[
    {
        "title": "Fairness in Serving Large Language Models",
        "authors": [
            "Ying Sheng",
            "Shiyi Cao",
            "Dacheng Li",
            "Banghua Zhu",
            "Zhuohan Li",
            "Danyang Zhuo",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-12-31T21:15:54Z",
        "summary": "High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions.",
        "pdf_link": "https://arxiv.org/pdf/2401.00588v1.pdf"
    },
    {
        "title": "HSC-GPT: A Large Language Model for Human Settlements Construction",
        "authors": [
            "Chen Ran",
            "Yao Xueqi",
            "Jiang Xuhui",
            "Han Zhengqi",
            "Guo Jingze",
            "Zhang Xianyue",
            "Lin Chunyu",
            "Liu Chumin",
            "Zhao Jing",
            "Lian Zeke",
            "Zhang Jingjing",
            "Li Keke"
        ],
        "published": "2023-12-31T13:56:15Z",
        "summary": "The field of human settlement construction encompasses a range of spatial\ndesigns and management tasks, including urban planning and landscape\narchitecture design. These tasks involve a plethora of instructions and\ndescriptions presented in natural language, which are essential for\nunderstanding design requirements and producing effective design solutions.\nRecent research has sought to integrate natural language processing (NLP) and\ngenerative artificial intelligence (AI) into human settlement construction\ntasks. Due to the efficient processing and analysis capabilities of AI with\ndata, significant successes have been achieved in design within this domain.\nHowever, this task still faces several fundamental challenges. The semantic\ninformation involved includes complex spatial details, diverse data source\nformats, high sensitivity to regional culture, and demanding requirements for\ninnovation and rigor in work scenarios. These factors lead to limitations when\napplying general generative AI in this field, further exacerbated by a lack of\nhigh-quality data for model training. To address these challenges, this paper\nfirst proposes HSC-GPT, a large-scale language model framework specifically\ndesigned for tasks in human settlement construction, considering the unique\ncharacteristics of this domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.00504v1.pdf"
    },
    {
        "title": "BatchEval: Towards Human-like Text Evaluation",
        "authors": [
            "Peiwen Yuan",
            "Shaoxiong Feng",
            "Yiwei Li",
            "Xinglin Wang",
            "Boyuan Pan",
            "Heda Wang",
            "Kan Li"
        ],
        "published": "2023-12-31T09:34:51Z",
        "summary": "Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.",
        "pdf_link": "https://arxiv.org/pdf/2401.00437v1.pdf"
    },
    {
        "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
        "authors": [
            "Yuanhao Wu",
            "Juno Zhu",
            "Siliang Xu",
            "Kashun Shum",
            "Cheng Niu",
            "Randy Zhong",
            "Juntong Song",
            "Tong Zhang"
        ],
        "published": "2023-12-31T04:43:45Z",
        "summary": "Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2401.00396v1.pdf"
    },
    {
        "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness",
        "authors": [
            "Neeraj Varshney",
            "Pavel Dolin",
            "Agastya Seth",
            "Chitta Baral"
        ],
        "published": "2023-12-30T17:37:06Z",
        "summary": "As Large Language Models (LLMs) play an increasingly pivotal role in natural\nlanguage processing applications, their safety concerns become critical areas\nof NLP research. This paper presents Safety and Over-Defensiveness Evaluation\n(SODE) benchmark: a collection of diverse safe and unsafe prompts with\ncarefully designed evaluation methods that facilitate systematic evaluation,\ncomparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we\nstudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,\nwhich reveals several interesting and important findings, such as (a) the\nwidely popular 'self-checking' techniques indeed improve the safety against\nunsafe inputs, but this comes at the cost of extreme over-defensiveness on the\nsafe inputs, (b) providing a safety instruction along with in-context exemplars\n(of both safe and unsafe inputs) consistently improves safety and also\nmitigates undue over-defensiveness of the models, (c) providing contextual\nknowledge easily breaks the safety guardrails and makes the models more\nvulnerable to generating unsafe responses. Overall, our work reveals numerous\nsuch critical findings that we believe will pave the way and facilitate further\nresearch in improving the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.00287v1.pdf"
    },
    {
        "title": "Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation",
        "authors": [
            "Reza Fayyazi",
            "Rozhina Taghdimi",
            "Shanchieh Jay Yang"
        ],
        "published": "2023-12-30T16:56:24Z",
        "summary": "Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use\nto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK\nframework can be challenging for cybersecurity practitioners due to presumed\nexpertise, complex dependencies, and inherent ambiguity. Meanwhile,\nadvancements with Large Language Models (LLMs) have led to recent surge in\nstudies exploring its uses in cybersecurity operations. This leads us to\nquestion how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5)\nLLMs can comprehend and summarize TTPs to inform analysts of the intended\npurposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs\nhave shown to be prone to hallucination by providing inaccurate information,\nwhich is problematic in critical domains like cybersecurity. Therefore, we\npropose the use of Retrieval Augmented Generation (RAG) techniques to extract\nrelevant contexts for each cyberattack procedure for decoder-only LLMs (without\nfine-tuning). We further contrast such approach against supervised fine-tuning\n(SFT) of encoder-only LLMs. Our results reveal that both the direct-use of\ndecoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-only\nLLMs offer inaccurate interpretation of cyberattack procedures. Significant\nimprovements are shown when RAG is used for decoder-only LLMs, particularly\nwhen directly relevant context is found. This study further sheds insights on\nthe limitations and capabilities of using RAG for LLMs in interpreting TTPs.",
        "pdf_link": "https://arxiv.org/pdf/2401.00280v2.pdf"
    },
    {
        "title": "Uncertainty-Penalized Reinforcement Learning from Human Feedback with Diverse Reward LoRA Ensembles",
        "authors": [
            "Yuanzhao Zhai",
            "Han Zhang",
            "Yu Lei",
            "Yue Yu",
            "Kele Xu",
            "Dawei Feng",
            "Bo Ding",
            "Huaimin Wang"
        ],
        "published": "2023-12-30T14:14:14Z",
        "summary": "Reinforcement learning from human feedback (RLHF) emerges as a promising\nparadigm for aligning large language models (LLMs). However, a notable\nchallenge in RLHF is overoptimization, where beyond a certain threshold, the\npursuit of higher rewards leads to a decline in human preferences. In this\npaper, we observe the weakness of KL regularization which is commonly employed\nin existing RLHF methods to address overoptimization. To mitigate this\nlimitation, we scrutinize the RLHF objective in the offline dataset and propose\nuncertainty-penalized RLHF (UP-RLHF), which incorporates uncertainty\nregularization during RL-finetuning. To enhance the uncertainty quantification\nabilities for reward models, we first propose a diverse low-rank adaptation\n(LoRA) ensemble by maximizing the nuclear norm of LoRA matrix concatenations.\nThen we optimize policy models utilizing penalized rewards, determined by both\nrewards and uncertainties provided by the diverse reward LoRA ensembles. Our\nexperimental results, based on two real human preference datasets, showcase the\neffectiveness of diverse reward LoRA ensembles in quantifying reward\nuncertainty. Additionally, uncertainty regularization in UP-RLHF proves to be\npivotal in mitigating overoptimization, thereby contributing to the overall\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2401.00243v1.pdf"
    },
    {
        "title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
        "authors": [
            "Hengrui Cai",
            "Shengjie Liu",
            "Rui Song"
        ],
        "published": "2023-12-30T04:51:46Z",
        "summary": "This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes \"do-operators\" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs' causal reasoning ability depends on\nthe context and domain-specific knowledge provided, and supports the argument\nthat \"knowledge is, indeed, what LLMs principally require for sound causal\nreasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a\ndegree of causal reasoning using the available numerical data, albeit with\nlimitations in the calculations.",
        "pdf_link": "https://arxiv.org/pdf/2401.00139v1.pdf"
    },
    {
        "title": "Teach Large Language Models to Forget Privacy",
        "authors": [
            "Ran Yan",
            "Yujun Li",
            "Wenqian Li",
            "Peihua Mai",
            "Yan Pang",
            "Yinchuan Li"
        ],
        "published": "2023-12-30T01:26:42Z",
        "summary": "Large Language Models (LLMs) have proven powerful, but the risk of privacy\nleakage remains a significant concern. Traditional privacy-preserving methods,\nsuch as Differential Privacy and Homomorphic Encryption, are inadequate for\nblack-box API-only settings, demanding either model transparency or heavy\ncomputational resources. We propose Prompt2Forget (P2F), the first framework\ndesigned to tackle the LLM local privacy challenge by teaching LLM to forget.\nThe method involves decomposing full questions into smaller segments,\ngenerating fabricated answers, and obfuscating the model's memory of the\noriginal input. A benchmark dataset was crafted with questions containing\nprivacy-sensitive information from diverse fields. P2F achieves zero-shot\ngeneralization, allowing adaptability across a wide range of use cases without\nmanual adjustments. Experimental results indicate P2F's robust capability to\nobfuscate LLM's memory, attaining a forgetfulness score of around 90\\% without\nany utility loss. This represents an enhancement of up to 63\\% when contrasted\nwith the naive direct instruction technique, highlighting P2F's efficacy in\nmitigating memory retention of sensitive information within LLMs. Our findings\nestablish the first benchmark in the novel field of the LLM forgetting task,\nrepresenting a meaningful advancement in privacy preservation in the emerging\nLLM domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.00870v1.pdf"
    },
    {
        "title": "ChatEd: A Chatbot Leveraging ChatGPT for an Enhanced Learning Experience in Higher Education",
        "authors": [
            "Kevin Wang",
            "Jason Ramos",
            "Ramon Lawrence"
        ],
        "published": "2023-12-29T19:11:55Z",
        "summary": "With the rapid evolution of Natural Language Processing (NLP), Large Language\nModels (LLMs) like ChatGPT have emerged as powerful tools capable of\ntransforming various sectors. Their vast knowledge base and dynamic interaction\ncapabilities represent significant potential in improving education by\noperating as a personalized assistant. However, the possibility of generating\nincorrect, biased, or unhelpful answers are a key challenge to resolve when\ndeploying LLMs in an education context. This work introduces an innovative\narchitecture that combines the strengths of ChatGPT with a traditional\ninformation retrieval based chatbot framework to offer enhanced student support\nin higher education. Our empirical evaluations underscore the high promise of\nthis approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.00052v1.pdf"
    },
    {
        "title": "Principled Gradient-based Markov Chain Monte Carlo for Text Generation",
        "authors": [
            "Li Du",
            "Afra Amini",
            "Lucas Torroba Hennigen",
            "Xinyan Velocity Yu",
            "Jason Eisner",
            "Holden Lee",
            "Ryan Cotterell"
        ],
        "published": "2023-12-29T18:00:56Z",
        "summary": "Recent papers have demonstrated the possibility of energy-based text\ngeneration by adapting gradient-based sampling algorithms, a paradigm of MCMC\nalgorithms that promises fast convergence. However, as we show in this paper,\nprevious attempts on this approach to text generation all fail to sample\ncorrectly from the target language model distributions. To address this\nlimitation, we consider the problem of designing text samplers that are\nfaithful, meaning that they have the target text distribution as its limiting\ndistribution. We propose several faithful gradient-based sampling algorithms to\nsample from the target energy-based text distribution correctly, and study\ntheir theoretical properties. Through experiments on various forms of text\ngeneration, we demonstrate that faithful samplers are able to generate more\nfluent text while adhering to the control objectives better.",
        "pdf_link": "https://arxiv.org/pdf/2312.17710v1.pdf"
    },
    {
        "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
        "authors": [
            "Julien Piet",
            "Maha Alrashed",
            "Chawin Sitawarin",
            "Sizhe Chen",
            "Zeming Wei",
            "Elizabeth Sun",
            "Basel Alomair",
            "David Wagner"
        ],
        "published": "2023-12-29T16:37:53Z",
        "summary": "Large Language Models (LLMs) are attracting significant research attention\ndue to their instruction-following abilities, allowing users and developers to\nleverage LLMs for a variety of tasks. However, LLMs are vulnerable to\nprompt-injection attacks: a class of attacks that hijack the model's\ninstruction-following abilities, changing responses to prompts to undesired,\npossibly malicious ones. In this work, we introduce Jatmo, a method for\ngenerating task-specific models resilient to prompt-injection attacks. Jatmo\nleverages the fact that LLMs can only follow instructions once they have\nundergone instruction tuning. It harnesses a teacher instruction-tuned model to\ngenerate a task-specific dataset, which is then used to fine-tune a base model\n(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a\ndataset of inputs for the task: it uses the teacher model to generate outputs.\nFor situations with no pre-existing datasets, Jatmo can use a single example,\nor in some cases none at all, to produce a fully synthetic dataset. Our\nexperiments on seven tasks show that Jatmo models provide similar quality of\noutputs on their specific task as standard LLMs, while being resilient to\nprompt injections. The best attacks succeeded in less than 0.5% of cases\nagainst our models, versus 87% success rate against GPT-3.5-Turbo. We release\nJatmo at https://github.com/wagner-group/prompt-injection-defense.",
        "pdf_link": "https://arxiv.org/pdf/2312.17673v2.pdf"
    },
    {
        "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao"
        ],
        "published": "2023-12-29T15:57:49Z",
        "summary": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as\nOpenAI's GPT-4V(ision), has significantly impacted both academic and industrial\nrealms. These models enhance Large Language Models (LLMs) with advanced visual\nunderstanding capabilities, facilitating their application in a variety of\nmultimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM\ndesigned specifically for multimodal integration. Despite its advancements,\npreliminary benchmarks indicate that Gemini lags behind GPT models in\ncommonsense reasoning tasks. However, this assessment, based on a limited\ndataset (i.e., HellaSWAG), does not fully capture Gemini's authentic\ncommonsense reasoning potential. To address this gap, our study undertakes a\nthorough evaluation of Gemini's performance in complex reasoning tasks that\nnecessitate the integration of commonsense knowledge across modalities. We\ncarry out a comprehensive analysis of 12 commonsense reasoning datasets,\nranging from general to domain-specific tasks. This includes 11 datasets\nfocused solely on language, as well as one that incorporates multimodal\nelements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's\ncompetitive commonsense reasoning capabilities. Additionally, we identify\ncommon challenges faced by current LLMs and MLLMs in addressing commonsense\nproblems, underscoring the need for further advancements in enhancing the\ncommonsense reasoning abilities of these models.",
        "pdf_link": "https://arxiv.org/pdf/2312.17661v1.pdf"
    },
    {
        "title": "Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception",
        "authors": [
            "Yuncheng Huang",
            "Qianyu He",
            "Jiaqing Liang",
            "Sihang Jiang",
            "Yanghua Xiao",
            "Yunwen Chen"
        ],
        "published": "2023-12-29T09:29:37Z",
        "summary": "Quantities are distinct and critical components of texts that characterize\nthe magnitude properties of entities, providing a precise perspective for the\nunderstanding of natural language, especially for reasoning tasks. In recent\nyears, there has been a flurry of research on reasoning tasks based on large\nlanguage models (LLMs), most of which solely focus on numerical values,\nneglecting the dimensional concept of quantities with units despite its\nimportance. We argue that the concept of dimension is essential for precisely\nunderstanding quantities and of great significance for LLMs to perform\nquantitative reasoning. However, the lack of dimension knowledge and\nquantity-related benchmarks has resulted in low performance of LLMs. Hence, we\npresent a framework to enhance the quantitative reasoning ability of language\nmodels based on dimension perception. We first construct a dimensional unit\nknowledge base (DimUnitKB) to address the knowledge gap in this area. We\npropose a benchmark DimEval consisting of seven tasks of three categories to\nprobe and enhance the dimension perception skills of LLMs. To evaluate the\neffectiveness of our methods, we propose a quantitative reasoning task and\nconduct experiments. The experimental results show that our dimension\nperception method dramatically improves accuracy (43.55%->50.67%) on\nquantitative reasoning tasks compared to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.17532v1.pdf"
    },
    {
        "title": "Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning",
        "authors": [
            "Xiao-Yang Liu",
            "Rongyi Zhu",
            "Daochen Zha",
            "Jiechao Gao",
            "Shan Zhong",
            "Meikang Qiu"
        ],
        "published": "2023-12-29T06:50:38Z",
        "summary": "The surge in interest and application of large language models (LLMs) has\nsparked a drive to fine-tune these models to suit specific applications, such\nas finance and medical science. However, concerns regarding data privacy have\nemerged, especially when multiple stakeholders aim to collaboratively enhance\nLLMs using sensitive data. In this scenario, federated learning becomes a\nnatural choice, allowing decentralized fine-tuning without exposing raw data to\ncentral servers. Motivated by this, we investigate how data privacy can be\nensured in LLM fine-tuning through practical federated learning approaches,\nenabling secure contributions from multiple parties to enhance LLMs. Yet,\nchallenges arise: 1) despite avoiding raw data exposure, there is a risk of\ninferring sensitive information from model outputs, and 2) federated learning\nfor LLMs incurs notable communication overhead. To address these challenges,\nthis article introduces DP-LoRA, a novel federated learning algorithm tailored\nfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that\nadds noise in weight updates, maintaining individual data privacy while\nfacilitating collaborative model training. Moreover, DP-LoRA optimizes\ncommunication efficiency via low-rank adaptation, minimizing the transmission\nof updated weights during distributed training. The experimental results across\nmedical, financial, and general datasets using various LLMs demonstrate that\nDP-LoRA effectively ensures strict privacy constraints while minimizing\ncommunication overhead.",
        "pdf_link": "https://arxiv.org/pdf/2312.17493v1.pdf"
    },
    {
        "title": "Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters",
        "authors": [
            "Manikanta Loya",
            "Divya Anand Sinha",
            "Richard Futrell"
        ],
        "published": "2023-12-29T05:19:11Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to their widespread\nuse across a broad spectrum of tasks including decision making. Prior studies\nhave compared the decision making abilities of LLMs with those of humans from a\npsychological perspective. However, these studies have not always properly\naccounted for the sensitivity of LLMs' behavior to hyperparameters and\nvariations in the prompt. In this study, we examine LLMs' performance on the\nHorizon decision making task studied by Binz and Schulz (2023) analyzing how\nLLMs respond to variations in prompts and hyperparameters. By experimenting on\nthree OpenAI language models possessing different capabilities, we observe that\nthe decision making abilities fluctuate based on the input prompts and\ntemperature settings. Contrary to previous findings language models display a\nhuman-like exploration exploitation tradeoff after simple adjustments to the\nprompt.",
        "pdf_link": "https://arxiv.org/pdf/2312.17476v1.pdf"
    },
    {
        "title": "LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model",
        "authors": [
            "Senqiao Yang",
            "Tianyuan Qu",
            "Xin Lai",
            "Zhuotao Tian",
            "Bohao Peng",
            "Shu Liu",
            "Jiaya Jia"
        ],
        "published": "2023-12-28T18:58:33Z",
        "summary": "While LISA effectively bridges the gap between segmentation and large\nlanguage models to enable reasoning segmentation, it poses certain limitations:\nunable to distinguish different instances of the target region, and constrained\nby the pre-defined textual response formats. In this work, we introduce LISA++,\nan update to the existing LISA model, focusing on improving core\nfunctionalities while keeping the base architecture intact. The main\nenhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance\nsegmentation ability has been added, providing a more detailed scene analysis\nalong with the existing multi-region semantic segmentation. \\textbf{2) More\nNatural Conversation}: Improved capability for multi-turn dialogue, with the\nability to incorporate segmentation results directly into text responses, i.e.,\nSegmentation in Dialogue (SiD). These improvements are achieved by curating the\nexisting samples of generic segmentation datasets, aimed specifically at\nenhancing the segmentation and conversational skills without structural change\nand additional data sources. Comparative analysis with the original LISA model\nshows significant advancements in these areas, positioning LISA++ as a notable\nupgrade in visual understanding and interaction. LISA++'s adaptability and\nimproved features highlight the versatility of the mask-as-embedding paradigm\nproposed by LISA, and the potential as a foundational model for diverse\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2312.17240v3.pdf"
    },
    {
        "title": "Fast Inference of Mixture-of-Experts Language Models with Offloading",
        "authors": [
            "Artyom Eliseev",
            "Denis Mazur"
        ],
        "published": "2023-12-28T18:58:13Z",
        "summary": "With the widespread adoption of Large Language Models (LLMs), many deep\nlearning practitioners are looking for strategies of running these models more\nefficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a\ntype of model architectures where only a fraction of model layers are active\nfor any given input. This property allows MoE-based language models to generate\ntokens faster than their dense counterparts, but it also increases model size\ndue to having multiple experts. Unfortunately, this makes state-of-the-art MoE\nlanguage models difficult to run without high-end GPUs. In this work, we study\nthe problem of running large MoE language models on consumer hardware with\nlimited accelerator memory. We build upon parameter offloading algorithms and\npropose a novel strategy that accelerates offloading by taking advantage of\ninnate properties of MoE LLMs. Using this strategy, we build can run\nMixtral-8x7B with mixed quantization on desktop hardware and free-tier Google\nColab instances.",
        "pdf_link": "https://arxiv.org/pdf/2312.17238v1.pdf"
    },
    {
        "title": "Large Language Model for Causal Decision Making",
        "authors": [
            "Haitao Jiang",
            "Lin Ge",
            "Yuhe Gao",
            "Jianian Wang",
            "Rui Song"
        ],
        "published": "2023-12-28T16:59:06Z",
        "summary": "Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\ninference based on user-specified structured data and knowledge in corpus-rare\nconcepts like causal decision-making is still limited. In this work, we explore\nthe possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can\nidentify the causal task, execute a corresponding function, and interpret its\nnumerical results based on users' queries and the provided dataset. Meanwhile,\nwe propose a data generation process for more controllable GPT prompting and\npresent two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal\nproblem identification and input parameter extraction for causal function\ncalling and (2) Causal-Interpret-Bench for in-context causal interpretation.\nWith three case studies, we showed that LLM4Causal can deliver end-to-end\nsolutions for causal problems and provide easy-to-understand answers. Numerical\nstudies also reveal that it has a remarkable ability to identify the correct\ncausal task given a query.",
        "pdf_link": "https://arxiv.org/pdf/2312.17122v2.pdf"
    },
    {
        "title": "How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation",
        "authors": [
            "Yang Xiao",
            "Yi Cheng",
            "Jinlan Fu",
            "Jiashuo Wang",
            "Wenjie Li",
            "Pengfei Liu"
        ],
        "published": "2023-12-28T16:51:11Z",
        "summary": "Human behavior simulation of AI agents necessitates the agents to possess a\nquality of believability, which is crucial as it facilitates users in\nestablishing trust toward the agents and streamlines the fulfillment of the\nagents' goal. While recent advancements in Large Language Model (LLM) based\nagents have improved human behavior simulation, challenges inherent to LLMs\n(e.g., long context modeling) can undermine their believability. Consequently,\nevaluating AI agent believability becomes imperative. Unfortunately, prior\nresearch often neglects the negative impacts of LLM deficiencies. To address\nthese gaps, we introduce two metrics for assessing LLM-based agent\nbelievability: consistency, and robustness, together with a benchmark,\nSimulateBench, with which, we evaluate the consistency and robustness of agents\nimplemented with popular LLMs. We find that agents (i) struggle to accurately\ndepict character information when presented with lengthy profile inputs; (ii)\nexhibit vulnerability to profile perturbations; and (iii) are significantly\naffected by certain key factors that impact their overall believability. Code\nand SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.",
        "pdf_link": "https://arxiv.org/pdf/2312.17115v1.pdf"
    },
    {
        "title": "GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension",
        "authors": [
            "Bohan Lyu",
            "Xin Cong",
            "Heyang Yu",
            "Pan Yang",
            "Yujia Qin",
            "Yining Ye",
            "Yaxi Lu",
            "Zhong Zhang",
            "Yukun Yan",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-12-28T15:47:30Z",
        "summary": "While Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nexceptional proficiency in natural language processing, their efficacy in\naddressing complex, multifaceted tasks remains limited. A growing area of\nresearch focuses on LLM-based agents equipped with external tools capable of\nperforming diverse tasks. However, existing LLM-based agents only support a\nlimited set of tools which is unable to cover a diverse range of user queries,\nespecially for those involving expertise domains. It remains a challenge for\nLLM-based agents to extend their tools autonomously when confronted with\nvarious user queries. As GitHub has hosted a multitude of repositories which\ncan be seen as a good resource for tools, a promising solution is that\nLLM-based agents can autonomously integrate the repositories in GitHub\naccording to the user queries to extend their tool set. In this paper, we\nintroduce GitAgent, an agent capable of achieving the autonomous tool extension\nfrom GitHub. GitAgent follows a four-phase procedure to incorporate\nrepositories and it can learn human experience by resorting to GitHub\nIssues/PRs to solve problems encountered during the procedure. Experimental\nevaluation involving 30 user queries demonstrates GitAgent's effectiveness,\nachieving a 69.4% success rate on average.",
        "pdf_link": "https://arxiv.org/pdf/2312.17294v1.pdf"
    },
    {
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "authors": [
            "Chen Qian",
            "Yufan Dang",
            "Jiahao Li",
            "Wei Liu",
            "Weize Chen",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-12-28T13:50:42Z",
        "summary": "Recent advancements in large language models (LLMs) have brought significant\nchanges to various domains, especially through LLM-driven autonomous agents.\nThese agents are now capable of collaborating seamlessly, splitting tasks and\nenhancing accuracy, thus minimizing the need for human involvement. However,\nthese agents often approach a diverse range of tasks in isolation, without\nbenefiting from past experiences. This isolation can lead to repeated mistakes\nand inefficient trials in task solving. To this end, this paper introduces\nExperiential Co-Learning, a novel framework in which instructor and assistant\nagents gather shortcut-oriented experiences from their historical trajectories\nand use these past experiences for mutual reasoning. This paradigm, enriched\nwith previous experiences, equips agents to more effectively address unseen\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.17025v2.pdf"
    },
    {
        "title": "AI Content Self-Detection for Transformer-based Large Language Models",
        "authors": [
            "Ant√¥nio Junior Alves Caiado",
            "Michael Hahsler"
        ],
        "published": "2023-12-28T10:08:57Z",
        "summary": "$ $The usage of generative artificial intelligence (AI) tools based on large\nlanguage models, including ChatGPT, Bard, and Claude, for text generation has\nmany exciting applications with the potential for phenomenal productivity\ngains. One issue is authorship attribution when using AI tools. This is\nespecially important in an academic setting where the inappropriate use of\ngenerative AI tools may hinder student learning or stifle research by creating\na large amount of automatically generated derivative work. Existing plagiarism\ndetection systems can trace the source of submitted text but are not yet\nequipped with methods to accurately detect AI-generated text. This paper\nintroduces the idea of direct origin detection and evaluates whether generative\nAI systems can recognize their output and distinguish it from human-written\ntexts. We argue why current transformer-based models may be able to self-detect\ntheir own generated text and perform a small empirical study using zero-shot\nlearning to investigate if that is the case. Results reveal varying\ncapabilities of AI systems to identify their generated text. Google's Bard\nmodel exhibits the largest capability of self-detection with an accuracy of\n94\\%, followed by OpenAI's ChatGPT with 83\\%. On the other hand, Anthropic's\nClaude model seems to be not able to self-detect.",
        "pdf_link": "https://arxiv.org/pdf/2312.17289v1.pdf"
    },
    {
        "title": "Spike No More: Stabilizing the Pre-training of Large Language Models",
        "authors": [
            "Sho Takase",
            "Shun Kiyono",
            "Sosuke Kobayashi",
            "Jun Suzuki"
        ],
        "published": "2023-12-28T08:53:27Z",
        "summary": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. To investigate the cause of loss spikes, we focus on\ngradients of internal layers. Through theoretical analyses, we reveal two\ncauses of the exploding gradients, and provide requirements to prevent the\nexplosion. In addition, we propose a method to satisfy the requirements by\ncombining the initialization method and a simple modification to embeddings. We\nconduct various experiments to verify our theoretical analyses empirically.\nExperimental results indicate that the combination is effective in preventing\nspikes during pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2312.16903v2.pdf"
    },
    {
        "title": "Large Language Models for Conducting Advanced Text Analytics Information Systems Research",
        "authors": [
            "Benjamin M. Ampel",
            "Chi-Heng Yang",
            "James Hu",
            "Hsinchun Chen"
        ],
        "published": "2023-12-27T19:49:00Z",
        "summary": "The exponential growth of digital content has generated massive textual\ndatasets, necessitating advanced analytical approaches. Large Language Models\n(LLMs) have emerged as tools capable of processing and extracting insights from\nmassive unstructured textual datasets. However, how to leverage LLMs for\ntext-based Information Systems (IS) research is currently unclear. To assist IS\nresearch in understanding how to operationalize LLMs, we propose a Text\nAnalytics for Information Systems Research (TAISR) framework. Our proposed\nframework provides detailed recommendations grounded in IS and LLM literature\non how to conduct meaningful text-based IS research. We conducted three case\nstudies in business intelligence using our TAISR framework to demonstrate its\napplication across several IS research contexts. We also outline potential\nchallenges and limitations in adopting LLMs for IS. By offering a systematic\napproach and evidence of its utility, our TAISR framework contributes to future\nIS research streams looking to incorporate powerful LLMs for text analytics.",
        "pdf_link": "https://arxiv.org/pdf/2312.17278v1.pdf"
    },
    {
        "title": "Make BERT-based Chinese Spelling Check Model Enhanced by Layerwise Attention and Gaussian Mixture Model",
        "authors": [
            "Yongchang Cao",
            "Liang He",
            "Zhen Wu",
            "Xinyu Dai"
        ],
        "published": "2023-12-27T16:11:07Z",
        "summary": "BERT-based models have shown a remarkable ability in the Chinese Spelling\nCheck (CSC) task recently. However, traditional BERT-based methods still suffer\nfrom two limitations. First, although previous works have identified that\nexplicit prior knowledge like Part-Of-Speech (POS) tagging can benefit in the\nCSC task, they neglected the fact that spelling errors inherent in CSC data can\nlead to incorrect tags and therefore mislead models. Additionally, they ignored\nthe correlation between the implicit hierarchical information encoded by BERT's\nintermediate layers and different linguistic phenomena. This results in\nsub-optimal accuracy. To alleviate the above two issues, we design a\nheterogeneous knowledge-infused framework to strengthen BERT-based CSC models.\nTo incorporate explicit POS knowledge, we utilize an auxiliary task strategy\ndriven by Gaussian mixture model. Meanwhile, to incorporate implicit\nhierarchical linguistic knowledge within the encoder, we propose a novel form\nof n-gram-based layerwise self-attention to generate a multilayer\nrepresentation. Experimental results show that our proposed framework yields a\nstable performance boost over four strong baseline models and outperforms the\nprevious state-of-the-art methods on two datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.16623v1.pdf"
    },
    {
        "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
        "authors": [
            "Qingyao Li",
            "Lingyue Fu",
            "Weiming Zhang",
            "Xianyu Chen",
            "Jingwei Yu",
            "Wei Xia",
            "Weinan Zhang",
            "Ruiming Tang",
            "Yong Yu"
        ],
        "published": "2023-12-27T14:37:32Z",
        "summary": "Online education platforms, leveraging the internet to distribute education\nresources, seek to provide convenient education but often fall short in\nreal-time communication with students. They often struggle to offer\npersonalized education resources due to the challenge of addressing the diverse\nobstacles students encounter throughout their learning journey. Recently, the\nemergence of large language models (LLMs), such as ChatGPT, offers the\npossibility for resolving this issue by comprehending individual requests.\nAlthough LLMs have been successful in various fields, creating an LLM-based\neducation system is still challenging for the wide range of educational skills\nrequired. This paper reviews the recently emerged LLM researches related to\neducational capabilities, including mathematics, writing, programming,\nreasoning, and knowledge-based question answering, with the aim to explore\ntheir potential in constructing the next-generation intelligent education\nsystem. Based on the current development status, we further outline two\napproaches for an LLM-based education system: a unified approach and a\nmixture-of-expert (MoE) approach. Finally, we explore the challenges and future\ndirections, providing new research opportunities and perspectives on adapting\nLLMs for education.",
        "pdf_link": "https://arxiv.org/pdf/2401.08664v2.pdf"
    },
    {
        "title": "How Robust are LLMs to In-Context Majority Label Bias?",
        "authors": [
            "Karan Gupta",
            "Sumegh Roychowdhury",
            "Siva Rajesh Kasa",
            "Santhosh Kumar Kasa",
            "Anish Bhanushali",
            "Nikhil Pattisapu",
            "Prasanna Srinivasa Murthy"
        ],
        "published": "2023-12-27T12:20:12Z",
        "summary": "In the In-Context Learning (ICL) setup, various forms of label biases can\nmanifest. One such manifestation is majority label bias, which arises when the\ndistribution of labeled examples in the in-context samples is skewed towards\none or more specific classes making Large Language Models (LLMs) more prone to\npredict those labels. Such discrepancies can arise from various factors,\nincluding logistical constraints, inherent biases in data collection methods,\nlimited access to diverse data sources, etc. which are unavoidable in a\nreal-world industry setup. In this work, we study the robustness of in-context\nlearning in LLMs to shifts that occur due to majority label bias within the\npurview of text classification tasks. Prior works have shown that in-context\nlearning with LLMs is susceptible to such biases. In our study, we go one level\ndeeper and show that the robustness boundary varies widely for different models\nand tasks, with certain LLMs being highly robust (~90%) to majority label bias.\nAdditionally, our findings also highlight the impact of model size and the\nrichness of instructional prompts contributing towards model robustness. We\nrestrict our study to only publicly available open-source models to ensure\ntransparency and reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2312.16549v1.pdf"
    },
    {
        "title": "LLM Factoscope: Uncovering LLMs' Factual Discernment through Inner States Analysis",
        "authors": [
            "Jinwen He",
            "Yujia Gong",
            "Kai Chen",
            "Zijin Lin",
            "Chengan Wei",
            "Yue Zhao"
        ],
        "published": "2023-12-27T01:44:47Z",
        "summary": "Large Language Models (LLMs) have revolutionized various domains with\nextensive knowledge and creative capabilities. However, a critical issue with\nLLMs is their tendency to produce outputs that diverge from factual reality.\nThis phenomenon is particularly concerning in sensitive applications such as\nmedical consultation and legal advice, where accuracy is paramount. In this\npaper, we introduce the LLM factoscope, a novel Siamese network-based model\nthat leverages the inner states of LLMs for factual detection. Our\ninvestigation reveals distinguishable patterns in LLMs' inner states when\ngenerating factual versus non-factual content. We demonstrate the LLM\nfactoscope's effectiveness across various architectures, achieving over 96%\naccuracy in factual detection. Our work opens a new avenue for utilizing LLMs'\ninner states for factual detection and encourages further exploration into\nLLMs' inner workings for enhanced reliability and transparency.",
        "pdf_link": "https://arxiv.org/pdf/2312.16374v2.pdf"
    },
    {
        "title": "LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing",
        "authors": [
            "Luyi Ma",
            "Nikhil Thakurdesai",
            "Jiao Chen",
            "Jianpeng Xu",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "published": "2023-12-26T23:08:38Z",
        "summary": "Data processing is one of the fundamental steps in machine learning pipelines\nto ensure data quality. Majority of the applications consider the user-defined\nfunction (UDF) design pattern for data processing in databases. Although the\nUDF design pattern introduces flexibility, reusability and scalability, the\nincreasing demand on machine learning pipelines brings three new challenges to\nthis design pattern -- not low-code, not dependency-free and not\nknowledge-aware. To address these challenges, we propose a new design pattern\nthat large language models (LLMs) could work as a generic data operator\n(LLM-GDO) for reliable data cleansing, transformation and modeling with their\nhuman-compatible performance. In the LLM-GDO design pattern, user-defined\nprompts (UDPs) are used to represent the data processing logic rather than\nimplementations with a specific programming language. LLMs can be centrally\nmaintained so users don't have to manage the dependencies at the run-time.\nFine-tuning LLMs with domain-specific data could enhance the performance on the\ndomain-specific tasks which makes data processing knowledge-aware. We\nillustrate these advantages with examples in different data processing tasks.\nFurthermore, we summarize the challenges and opportunities introduced by LLMs\nto provide a complete view of this design pattern for more discussions.",
        "pdf_link": "https://arxiv.org/pdf/2312.16351v1.pdf"
    },
    {
        "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
        "authors": [
            "Changmao Li",
            "Jeffrey Flanigan"
        ],
        "published": "2023-12-26T21:17:46Z",
        "summary": "Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2312.16337v1.pdf"
    },
    {
        "title": "Can ChatGPT Read Who You Are?",
        "authors": [
            "Erik Derner",
            "Dalibor Kuƒçera",
            "Nuria Oliver",
            "Jan Zah√°lka"
        ],
        "published": "2023-12-26T14:43:04Z",
        "summary": "The interplay between artificial intelligence (AI) and psychology,\nparticularly in personality assessment, represents an important emerging area\nof research. Accurate personality trait estimation is crucial not only for\nenhancing personalization in human-computer interaction but also for a wide\nvariety of applications ranging from mental health to education. This paper\nanalyzes the capability of a generic chatbot, ChatGPT, to effectively infer\npersonality traits from short texts. We report the results of a comprehensive\nuser study featuring texts written in Czech by a representative population\nsample of 155 participants. Their self-assessments based on the Big Five\nInventory (BFI) questionnaire serve as the ground truth. We compare the\npersonality trait estimations made by ChatGPT against those by human raters and\nreport ChatGPT's competitive performance in inferring personality traits from\ntext. We also uncover a 'positivity bias' in ChatGPT's assessments across all\npersonality dimensions and explore the impact of prompt composition on\naccuracy. This work contributes to the understanding of AI capabilities in\npsychological assessment, highlighting both the potential and limitations of\nusing large language models for personality inference. Our research underscores\nthe importance of responsible AI development, considering ethical implications\nsuch as privacy, consent, autonomy, and bias in AI applications.",
        "pdf_link": "https://arxiv.org/pdf/2312.16070v1.pdf"
    },
    {
        "title": "A Prompt Learning Framework for Source Code Summarization",
        "authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yudu You",
            "Yuchen Chen",
            "Yi Liu",
            "Chong Wang",
            "Jian Zhang",
            "Quanjun Zhang",
            "Hanwei Qian",
            "Wei Zhao",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "published": "2023-12-26T14:37:55Z",
        "summary": "(Source) code summarization is the task of automatically generating natural\nlanguage summaries for given code snippets. Such summaries play a key role in\nhelping developers understand and maintain source code. Recently, with the\nsuccessful application of large language models (LLMs) in numerous fields,\nsoftware engineering researchers have also attempted to adapt LLMs to solve\ncode summarization tasks. The main adaptation schemes include instruction\nprompting and task-oriented fine-tuning. However, instruction prompting\ninvolves designing crafted prompts for zero-shot learning or selecting\nappropriate samples for few-shot learning and requires users to have\nprofessional domain knowledge, while task-oriented fine-tuning requires high\ntraining costs. In this paper, we propose a novel prompt learning framework for\ncode summarization called PromptCS. PromptCS trains a prompt agent that can\ngenerate continuous prompts to unleash the potential for LLMs in code\nsummarization. Compared to the human-written discrete prompt, the continuous\nprompts are produced under the guidance of LLMs and are therefore easier to\nunderstand by LLMs. PromptCS freezes the parameters of LLMs when training the\nprompt agent, which can greatly reduce the requirements for training resources.\nWe evaluate PromptCS on the CodeSearchNet dataset involving multiple\nprogramming languages. The results show that PromptCS significantly outperforms\ninstruction prompting schemes on all four widely used metrics. In some base\nLLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS even\noutperforms the task-oriented fine-tuning scheme. More importantly, the\ntraining efficiency of PromptCS is faster than the task-oriented fine-tuning\nscheme, with a more pronounced advantage on larger LLMs. The results of the\nhuman evaluation demonstrate that PromptCS can generate more good summaries\ncompared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2312.16066v1.pdf"
    },
    {
        "title": "MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks",
        "authors": [
            "Jingyao Li",
            "Pengguang Chen",
            "Jiaya Jia"
        ],
        "published": "2023-12-26T08:49:57Z",
        "summary": "Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.",
        "pdf_link": "https://arxiv.org/pdf/2312.15960v2.pdf"
    },
    {
        "title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph",
        "authors": [
            "Tiezheng Guo",
            "Qingwen Yang",
            "Chen Wang",
            "Yanyi Liu",
            "Pan Li",
            "Jiawei Tang",
            "Dapeng Li",
            "Yingyou Wen"
        ],
        "published": "2023-12-26T04:22:56Z",
        "summary": "Large language model (LLM) has achieved outstanding performance on various\ndownstream tasks with its powerful natural language understanding and zero-shot\ncapability, but LLM still suffers from knowledge limitation. Especially in\nscenarios that require long logical chains or complex reasoning, the\nhallucination and knowledge limitation of LLM limit its performance in question\nanswering (QA). In this paper, we propose a novel framework KnowledgeNavigator\nto address these challenges by efficiently and accurately retrieving external\nknowledge from knowledge graph and using it as a key factor to enhance LLM\nreasoning. Specifically, KnowledgeNavigator first mines and enhances the\npotential constraints of the given question to guide the reasoning. Then it\nretrieves and filters external knowledge that supports answering through\niterative reasoning on knowledge graph with the guidance of LLM and the\nquestion. Finally, KnowledgeNavigator constructs the structured knowledge into\neffective prompts that are friendly to LLM to help its reasoning. We evaluate\nKnowledgeNavigator on multiple public KGQA benchmarks, the experiments show the\nframework has great effectiveness and generalization, outperforming previous\nknowledge graph enhanced LLM methods and is comparable to the fully supervised\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2312.15880v2.pdf"
    },
    {
        "title": "SecQA: A Concise Question-Answering Dataset for Evaluating Large Language Models in Computer Security",
        "authors": [
            "Zefang Liu"
        ],
        "published": "2023-12-26T00:59:30Z",
        "summary": "In this paper, we introduce SecQA, a novel dataset tailored for evaluating\nthe performance of Large Language Models (LLMs) in the domain of computer\nsecurity. Utilizing multiple-choice questions generated by GPT-4 based on the\n\"Computer Systems Security: Planning for Success\" textbook, SecQA aims to\nassess LLMs' understanding and application of security principles. We detail\nthe structure and intent of SecQA, which includes two versions of increasing\ncomplexity, to provide a concise evaluation across various difficulty levels.\nAdditionally, we present an extensive evaluation of prominent LLMs, including\nGPT-3.5-Turbo, GPT-4, Llama-2, Vicuna, Mistral, and Zephyr models, using both\n0-shot and 5-shot learning settings. Our results, encapsulated in the SecQA v1\nand v2 datasets, highlight the varying capabilities and limitations of these\nmodels in the computer security context. This study not only offers insights\ninto the current state of LLMs in understanding security-related content but\nalso establishes SecQA as a benchmark for future advancements in this critical\nresearch area.",
        "pdf_link": "https://arxiv.org/pdf/2312.15838v1.pdf"
    },
    {
        "title": "Large Language Models are Not Stable Recommender Systems",
        "authors": [
            "Tianhui Ma",
            "Yuan Cheng",
            "Hengshu Zhu",
            "Hui Xiong"
        ],
        "published": "2023-12-25T14:54:33Z",
        "summary": "With the significant successes of large language models (LLMs) in many\nnatural language processing tasks, there is growing interest among researchers\nin exploring LLMs for novel recommender systems. However, we have observed that\ndirectly using LLMs as a recommender system is usually unstable due to its\ninherent position bias. To this end, we introduce exploratory research and find\nconsistent patterns of positional bias in LLMs that influence the performance\nof recommendation across a range of scenarios. Then, we propose a Bayesian\nprobabilistic framework, STELLA (Stable LLM for Recommendation), which involves\na two-stage pipeline. During the first probing stage, we identify patterns in a\ntransition matrix using a probing detection dataset. And in the second\nrecommendation stage, a Bayesian strategy is employed to adjust the biased\noutput of LLMs with an entropy indicator. Therefore, our framework can\ncapitalize on existing pattern information to calibrate instability of LLMs,\nand enhance recommendation performance. Finally, extensive experiments clearly\nvalidate the effectiveness of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.15746v1.pdf"
    },
    {
        "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023-12-25T12:32:49Z",
        "summary": "Despite their impressive capabilities, large language models (LLMs) have been\nobserved to generate responses that include inaccurate or fabricated\ninformation, a phenomenon commonly known as ``hallucination''. In this work, we\npropose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to\nalleviate hallucinations. We first construct a factually weak LLM by inducing\nhallucinations from the original LLMs. Then, we penalize these induced\nhallucinations during decoding to enhance the factuality of the generated\ncontent. Concretely, we determine the final next-token predictions by\namplifying the predictions from the original model and downplaying the induced\nuntruthful predictions via contrastive decoding. Experimental results on both\ndiscrimination-based and generation-based hallucination evaluation benchmarks,\nsuch as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD\nmethods can effectively enhance the factuality of LLMs across various model\nsizes and families. For example, when equipped with ICD, Llama2-7B-Chat and\nMistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on\nTruthfulQA, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2312.15710v2.pdf"
    },
    {
        "title": "EcomGPT-CT: Continual Pre-training of E-commerce Large Language Models with Semi-structured Data",
        "authors": [
            "Shirong Ma",
            "Shen Huang",
            "Shulin Huang",
            "Xiaobin Wang",
            "Yangning Li",
            "Hai-Tao Zheng",
            "Pengjun Xie",
            "Fei Huang",
            "Yong Jiang"
        ],
        "published": "2023-12-25T11:31:47Z",
        "summary": "Large Language Models (LLMs) pre-trained on massive corpora have exhibited\nremarkable performance on various NLP tasks. However, applying these models to\nspecific domains still poses significant challenges, such as lack of domain\nknowledge, limited capacity to leverage domain knowledge and inadequate\nadaptation to domain-specific data formats. Considering the exorbitant cost of\ntraining LLMs from scratch and the scarcity of annotated data within particular\ndomains, in this work, we focus on domain-specific continual pre-training of\nLLMs using E-commerce domain as an exemplar. Specifically, we explore the\nimpact of continual pre-training on LLMs employing unlabeled general and\nE-commercial corpora. Furthermore, we design a mixing strategy among different\ndata sources to better leverage E-commercial semi-structured data. We construct\nmultiple tasks to assess LLMs' few-shot In-context Learning ability and their\nzero-shot performance after instruction tuning in E-commerce domain.\nExperimental results demonstrate the effectiveness of continual pre-training of\nE-commerce LLMs and the efficacy of our devised data mixing strategy.",
        "pdf_link": "https://arxiv.org/pdf/2312.15696v1.pdf"
    },
    {
        "title": "Instruction Fusion: Advancing Prompt Evolution through Hybridization",
        "authors": [
            "Weidong Guo",
            "Jiuding Yang",
            "Kaitong Yang",
            "Xiangyang Li",
            "Zhuwei Rao",
            "Yu Xu",
            "Di Niu"
        ],
        "published": "2023-12-25T11:00:37Z",
        "summary": "The fine-tuning of Large Language Models (LLMs) specialized in code\ngeneration has seen notable advancements through the use of open-domain coding\nqueries. Despite the successes, existing methodologies like Evol-Instruct\nencounter performance limitations, impeding further enhancements in code\ngeneration tasks. This paper examines the constraints of existing prompt\nevolution techniques and introduces a novel approach, Instruction Fusion (IF).\nIF innovatively combines two distinct prompts through a hybridization process,\nthereby enhancing the evolution of training prompts for code LLMs. Our\nexperimental results reveal that the proposed novel method effectively\naddresses the shortcomings of prior methods, significantly improving the\nperformance of Code LLMs across five code generation benchmarks, namely\nHumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the\neffectiveness of Instruction Fusion in advancing the capabilities of LLMs in\ncode generation.",
        "pdf_link": "https://arxiv.org/pdf/2312.15692v3.pdf"
    },
    {
        "title": "ESGReveal: An LLM-based approach for extracting structured data from ESG reports",
        "authors": [
            "Yi Zou",
            "Mengying Shi",
            "Zhongjie Chen",
            "Zhu Deng",
            "ZongXiong Lei",
            "Zihan Zeng",
            "Shiming Yang",
            "HongXiang Tong",
            "Lei Xiao",
            "Wenwen Zhou"
        ],
        "published": "2023-12-25T06:44:32Z",
        "summary": "ESGReveal is an innovative method proposed for efficiently extracting and\nanalyzing Environmental, Social, and Governance (ESG) data from corporate\nreports, catering to the critical need for reliable ESG information retrieval.\nThis approach utilizes Large Language Models (LLM) enhanced with Retrieval\nAugmented Generation (RAG) techniques. The ESGReveal system includes an ESG\nmetadata module for targeted queries, a preprocessing module for assembling\ndatabases, and an LLM agent for data extraction. Its efficacy was appraised\nusing ESG reports from 166 companies across various sectors listed on the Hong\nKong Stock Exchange in 2022, ensuring comprehensive industry and market\ncapitalization representation. Utilizing ESGReveal unearthed significant\ninsights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in\ndata extraction and 83.7% in disclosure analysis, which is an improvement over\nbaseline models. This highlights the framework's capacity to refine ESG data\nanalysis precision. Moreover, it revealed a demand for reinforced ESG\ndisclosures, with environmental and social data disclosures standing at 69.5%\nand 57.2%, respectively, suggesting a pursuit for more corporate transparency.\nWhile current iterations of ESGReveal do not process pictorial information, a\nfunctionality intended for future enhancement, the study calls for continued\nresearch to further develop and compare the analytical capabilities of various\nLLMs. In summary, ESGReveal is a stride forward in ESG data processing,\noffering stakeholders a sophisticated tool to better evaluate and advance\ncorporate sustainability efforts. Its evolution is promising in promoting\ntransparency in corporate reporting and aligning with broader sustainable\ndevelopment aims.",
        "pdf_link": "https://arxiv.org/pdf/2312.17264v1.pdf"
    },
    {
        "title": "Privacy-Preserved Neural Graph Databases",
        "authors": [
            "Qi Hu",
            "Haoran Li",
            "Jiaxin Bai",
            "Zihao Wang",
            "Yangqiu Song"
        ],
        "published": "2023-12-25T02:32:05Z",
        "summary": "In the era of large language models (LLMs), efficient and accurate data\nretrieval has become increasingly crucial for the use of domain-specific or\nprivate data in the retrieval augmented generation (RAG). Neural graph\ndatabases (NGDBs) have emerged as a powerful paradigm that combines the\nstrengths of graph databases (GDBs) and neural networks to enable efficient\nstorage, retrieval, and analysis of graph-structured data which can be\nadaptively trained with LLMs. The usage of neural embedding storage and Complex\nneural logical Query Answering (CQA) provides NGDBs with generalization\nability. When the graph is incomplete, by extracting latent patterns and\nrepresentations, neural graph databases can fill gaps in the graph structure,\nrevealing hidden relationships and enabling accurate query answering.\nNevertheless, this capability comes with inherent trade-offs, as it introduces\nadditional privacy risks to the domain-specific or private databases. Malicious\nattackers can infer more sensitive information in the database using\nwell-designed queries such as from the answer sets of where Turing Award\nwinners born before 1950 and after 1940 lived, the living places of Turing\nAward winner Hinton are probably exposed, although the living places may have\nbeen deleted in the training stage due to the privacy concerns. In this work,\nwe propose a privacy-preserved neural graph database (P-NGDB) framework to\nalleviate the risks of privacy leakage in NGDBs. We introduce adversarial\ntraining techniques in the training stage to enforce the NGDBs to generate\nindistinguishable answers when queried with private information, enhancing the\ndifficulty of inferring sensitive information through combinations of multiple\ninnocuous queries.",
        "pdf_link": "https://arxiv.org/pdf/2312.15591v4.pdf"
    },
    {
        "title": "Reducing LLM Hallucinations using Epistemic Neural Networks",
        "authors": [
            "Shreyas Verma",
            "Kien Tran",
            "Yusuf Ali",
            "Guangyu Min"
        ],
        "published": "2023-12-25T01:17:01Z",
        "summary": "Reducing and detecting hallucinations in large language models is an open\nresearch problem. In this project, we attempt to leverage recent advances in\nthe field of uncertainty estimation to reduce hallucinations in frozen large\nlanguage models. Epistemic neural networks have recently been proposed to\nimprove output joint distributions for large pre-trained models. ENNs are small\nnetworks attached to large, frozen models to improve the model's joint\ndistributions and uncertainty estimates. In this work, we train an epistemic\nneural network on top of the Llama-2 7B model combined with a contrastive\ndecoding feature enhancement technique. We are the first to train an ENN for\nthe next token prediction task and explore the efficacy of this method in\nreducing hallucinations on the TruthfulQA dataset. In essence, we provide a\nmethod that leverages a pre-trained model's latent embeddings to reduce\nhallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.15576v1.pdf"
    },
    {
        "title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective",
        "authors": [
            "George Gui",
            "Olivier Toubia"
        ],
        "published": "2023-12-24T16:32:35Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive potential to\nsimulate human behavior. Using a causal inference framework, we empirically and\ntheoretically analyze the challenges of conducting LLM-simulated experiments,\nand explore potential solutions. In the context of demand estimation, we show\nthat variations in the treatment included in the prompt (e.g., price of focal\nproduct) can cause variations in unspecified confounding factors (e.g., price\nof competitors, historical prices, outside temperature), introducing\nendogeneity and yielding implausibly flat demand curves. We propose a\ntheoretical framework suggesting this endogeneity issue generalizes to other\ncontexts and won't be fully resolved by merely improving the training data.\nUnlike real experiments where researchers assign pre-existing units across\nconditions, LLMs simulate units based on the entire prompt, which includes the\ndescription of the treatment. Therefore, due to associations in the training\ndata, the characteristics of individuals and environments simulated by the LLM\ncan be affected by the treatment assignment. We explore two potential\nsolutions. The first specifies all contextual variables that affect both\ntreatment and outcome, which we demonstrate to be challenging for a\ngeneral-purpose LLM. The second explicitly specifies the source of treatment\nvariation in the prompt given to the LLM (e.g., by informing the LLM that the\nstore is running an experiment). While this approach only allows the estimation\nof a conditional average treatment effect that depends on the specific\nexperimental design, it provides valuable directional results for exploratory\nanalysis.",
        "pdf_link": "https://arxiv.org/pdf/2312.15524v1.pdf"
    },
    {
        "title": "A Group Fairness Lens for Large Language Models",
        "authors": [
            "Guanqun Bi",
            "Lei Shen",
            "Yuqiang Xie",
            "Yanan Cao",
            "Tiangang Zhu",
            "Xiaodong He"
        ],
        "published": "2023-12-24T13:25:15Z",
        "summary": "The rapid advancement of large language models has revolutionized various\napplications but also raised crucial concerns about their potential to\nperpetuate biases and unfairness when deployed in social media contexts.\nEvaluating LLMs' potential biases and fairness has become crucial, as existing\nmethods rely on limited prompts focusing on just a few groups, lacking a\ncomprehensive categorical perspective. In this paper, we propose evaluating LLM\nbiases from a group fairness lens using a novel hierarchical schema\ncharacterizing diverse social groups. Specifically, we construct a dataset,\nGFair, encapsulating target-attribute combinations across multiple dimensions.\nIn addition, we introduce statement organization, a new open-ended text\ngeneration task, to uncover complex biases in LLMs. Extensive evaluations of\npopular LLMs reveal inherent safety concerns. To mitigate the biases of LLM\nfrom a group fairness perspective, we pioneer a novel chain-of-thought method\nGF-Think to mitigate biases of LLMs from a group fairness perspective.\nExperimental results demonstrate its efficacy in mitigating bias in LLMs to\nachieve fairness.",
        "pdf_link": "https://arxiv.org/pdf/2312.15478v1.pdf"
    },
    {
        "title": "Towards Consistent Language Models Using Declarative Constraints",
        "authors": [
            "Jasmin Mousavi",
            "Arash Termehchy"
        ],
        "published": "2023-12-24T12:53:07Z",
        "summary": "Large language models have shown unprecedented abilities in generating\nlinguistically coherent and syntactically correct natural language output.\nHowever, they often return incorrect and inconsistent answers to input\nquestions. Due to the complexity and uninterpretability of the internally\nlearned representations, it is challenging to modify language models such that\nthey provide correct and consistent results. The data management community has\ndeveloped various methods and tools for providing consistent answers over\ninconsistent datasets. In these methods, users specify the desired properties\nof data in a domain in the form of high-level declarative constraints. This\napproach has provided usable and scalable methods to delivering consistent\ninformation from inconsistent datasets. We aim to build upon this success and\nleverage these methods to modify language models such that they deliver\nconsistent and accurate results. We investigate the challenges of using these\nideas to obtain consistent and relevant answers from language models and report\nsome preliminary empirical studies.",
        "pdf_link": "https://arxiv.org/pdf/2312.15472v1.pdf"
    },
    {
        "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
        "authors": [
            "Chen Zhang",
            "Luis Fernando D'Haro",
            "Yiming Chen",
            "Malu Zhang",
            "Haizhou Li"
        ],
        "published": "2023-12-24T04:50:57Z",
        "summary": "Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.",
        "pdf_link": "https://arxiv.org/pdf/2312.15407v2.pdf"
    },
    {
        "title": "Fairness-Aware Structured Pruning in Transformers",
        "authors": [
            "Abdelrahman Zayed",
            "Goncalo Mordido",
            "Samira Shabanian",
            "Ioana Baldini",
            "Sarath Chandar"
        ],
        "published": "2023-12-24T03:57:52Z",
        "summary": "The increasing size of large language models (LLMs) has introduced challenges\nin their training and inference. Removing model components is perceived as a\nsolution to tackle the large model sizes, however, existing pruning methods\nsolely focus on performance, without considering an essential aspect for the\nresponsible use of LLMs: model fairness. It is crucial to address the fairness\nof LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish\ncommunities, among others, as they are being deployed and available to a wide\naudience. In this work, first, we investigate how attention heads impact\nfairness and performance in pre-trained transformer-based language models. We\nthen propose a novel method to prune the attention heads that negatively impact\nfairness while retaining the heads critical for performance, i.e. language\nmodeling capabilities. Our approach is practical in terms of time and\nresources, as it does not require fine-tuning the final pruned, and fairer,\nmodel. Our findings demonstrate a reduction in gender bias by 19%, 19.5%,\n39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different\nsizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased\nmodel, with only a slight decrease in performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.15398v1.pdf"
    },
    {
        "title": "On the Promises and Challenges of Multimodal Foundation Models for Geographical, Environmental, Agricultural, and Urban Planning Applications",
        "authors": [
            "Chenjiao Tan",
            "Qian Cao",
            "Yiwei Li",
            "Jielu Zhang",
            "Xiao Yang",
            "Huaqin Zhao",
            "Zihao Wu",
            "Zhengliang Liu",
            "Hao Yang",
            "Nemin Wu",
            "Tao Tang",
            "Xinyue Ye",
            "Lilong Chai",
            "Ninghao Liu",
            "Changying Li",
            "Lan Mu",
            "Tianming Liu",
            "Gengchen Mai"
        ],
        "published": "2023-12-23T22:36:58Z",
        "summary": "The advent of large language models (LLMs) has heightened interest in their\npotential for multimodal applications that integrate language and vision. This\npaper explores the capabilities of GPT-4V in the realms of geography,\nenvironmental science, agriculture, and urban planning by evaluating its\nperformance across a variety of tasks. Data sources comprise satellite imagery,\naerial photos, ground-level images, field images, and public datasets. The\nmodel is evaluated on a series of tasks including geo-localization, textual\ndata extraction from maps, remote sensing image classification, visual question\nanswering, crop type identification, disease/pest/weed recognition, chicken\nbehavior analysis, agricultural object counting, urban planning knowledge\nquestion answering, and plan generation. The results indicate the potential of\nGPT-4V in geo-localization, land cover classification, visual question\nanswering, and basic image understanding. However, there are limitations in\nseveral tasks requiring fine-grained recognition and precise counting. While\nzero-shot learning shows promise, performance varies across problem domains and\nimage complexities. The work provides novel insights into GPT-4V's capabilities\nand limitations for real-world geospatial, environmental, agricultural, and\nurban planning challenges. Further research should focus on augmenting the\nmodel's knowledge and reasoning for specialized domains through expanded\ntraining. Overall, the analysis demonstrates foundational multimodal\nintelligence, highlighting the potential of multimodal foundation models (FMs)\nto advance interdisciplinary applications at the nexus of computer vision and\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2312.17016v1.pdf"
    },
    {
        "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Zhihao Zhang",
            "Xinhao Cheng",
            "Hongyi Jin",
            "Tianqi Chen",
            "Zhihao Jia"
        ],
        "published": "2023-12-23T11:57:53Z",
        "summary": "In the rapidly evolving landscape of artificial intelligence (AI), generative\nlarge language models (LLMs) stand at the forefront, revolutionizing how we\ninteract with our data. However, the computational intensity and memory\nconsumption of deploying these models present substantial challenges in terms\nof serving efficiency, particularly in scenarios demanding low latency and high\nthroughput. This survey addresses the imperative need for efficient LLM serving\nmethodologies from a machine learning system (MLSys) research perspective,\nstanding at the crux of advanced AI innovations and practical system\noptimizations. We provide in-depth analysis, covering a spectrum of solutions,\nranging from cutting-edge algorithmic modifications to groundbreaking changes\nin system designs. The survey aims to provide a comprehensive understanding of\nthe current state and future directions in efficient LLM serving, offering\nvaluable insights for researchers and practitioners in overcoming the barriers\nof effective LLM deployment, thereby reshaping the future of AI.",
        "pdf_link": "https://arxiv.org/pdf/2312.15234v1.pdf"
    },
    {
        "title": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs",
        "authors": [
            "Max Zimmer",
            "Megi Andoni",
            "Christoph Spiegel",
            "Sebastian Pokutta"
        ],
        "published": "2023-12-23T11:45:22Z",
        "summary": "Neural Networks can be efficiently compressed through pruning, significantly\nreducing storage and computational demands while maintaining predictive\nperformance. Simple yet effective methods like Iterative Magnitude Pruning\n(IMP, Han et al., 2015) remove less important parameters and require a costly\nretraining procedure to recover performance after pruning. However, with the\nrise of Large Language Models (LLMs), full retraining has become infeasible due\nto memory and compute constraints. In this study, we challenge the practice of\nretraining all parameters by demonstrating that updating only a small subset of\nhighly expressive parameters is often sufficient to recover or even improve\nperformance compared to full retraining. Surprisingly, retraining as little as\n0.27%-0.35% of the parameters of GPT-architectures achieves comparable\nperformance to One Shot IMP across various sparsity levels. Our approach,\nParameter-Efficient Retraining after Pruning (PERP), drastically reduces\ncompute and memory demands, enabling pruning and retraining of up to 30 billion\nparameter models on a single NVIDIA A100 GPU within minutes. Despite magnitude\npruning being considered as unsuited for pruning LLMs, our findings show that\nPERP positions it as a strong contender against state-of-the-art\nretraining-free approaches such as Wanda (Sun et al., 2023) and SparseGPT\n(Frantar & Alistarh, 2023), opening up a promising alternative to avoiding\nretraining.",
        "pdf_link": "https://arxiv.org/pdf/2312.15230v2.pdf"
    },
    {
        "title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering",
        "authors": [
            "Hengrui Gu",
            "Kaixiong Zhou",
            "Xiaotian Han",
            "Ninghao Liu",
            "Ruobing Wang",
            "Xin Wang"
        ],
        "published": "2023-12-23T08:32:13Z",
        "summary": "Multi-hop question answering (MQA) is one of the challenging tasks to\nevaluate machine's comprehension and reasoning abilities, where large language\nmodels (LLMs) have widely achieved the human-comparable performance. Due to the\ndynamics of knowledge facts in real world, knowledge editing has been explored\nto update model with the up-to-date facts while avoiding expensive re-training\nor fine-tuning. Starting from the edited fact, the updated model needs to\nprovide cascading changes in the chain of MQA. The previous art simply adopts a\nmix-up prompt to instruct LLMs conducting multiple reasoning tasks\nsequentially, including question decomposition, answer generation, and conflict\nchecking via comparing with edited facts. However, the coupling of these\nfunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehending\nand answering questions while disturbing them with the unskilled task of\nconflict checking. We thus propose a framework, Programmable knowledge editing\nfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,\nwe prompt LLMs to decompose knowledge-augmented multi-hop question, while\ninteracting with a detached trainable scope detector to modulate LLMs behavior\ndepending on external conflict signal. The experiments on three LLM backbones\nand two benchmark datasets validate our superiority in knowledge editing of\nMQA, outperforming all competitors by a large margin in almost all settings and\nconsistently producing reliable reasoning process.",
        "pdf_link": "https://arxiv.org/pdf/2312.15194v2.pdf"
    },
    {
        "title": "ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-order Optimization",
        "authors": [
            "Shuoran Jiang",
            "Qingcai Chen",
            "Youchen Pan",
            "Yang Xiang",
            "Yukang Lin",
            "Xiangping Wu",
            "Chuanyi Liu",
            "Xiaobao Song"
        ],
        "published": "2023-12-23T07:46:31Z",
        "summary": "Lowering the memory requirement in full-parameter training on large models\nhas become a hot research area. MeZO fine-tunes the large language models\n(LLMs) by just forward passes in a zeroth-order SGD optimizer (ZO-SGD),\ndemonstrating excellent performance with the same GPU memory usage as\ninference. However, the simulated perturbation stochastic approximation for\ngradient estimate in MeZO leads to severe oscillations and incurs a substantial\ntime overhead. Moreover, without momentum regularization, MeZO shows severe\nover-fitting problems. Lastly, the perturbation-irrelevant momentum on ZO-SGD\ndoes not improve the convergence rate. This study proposes ZO-AdaMU to resolve\nthe above problems by adapting the simulated perturbation with momentum in its\nstochastic approximation. Unlike existing adaptive momentum methods, we\nrelocate momentum on simulated perturbation in stochastic gradient\napproximation. Our convergence analysis and experiments prove this is a better\nway to improve convergence stability and rate in ZO-SGD. Extensive experiments\ndemonstrate that ZO-AdaMU yields better generalization for LLMs fine-tuning\nacross various NLP tasks than MeZO and its momentum variants.",
        "pdf_link": "https://arxiv.org/pdf/2312.15184v1.pdf"
    },
    {
        "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
        "authors": [
            "Zhen Tan",
            "Tianlong Chen",
            "Zhenyu Zhang",
            "Huan Liu"
        ],
        "published": "2023-12-22T19:55:58Z",
        "summary": "Large Language Models (LLMs) have achieved unprecedented breakthroughs in\nvarious natural language processing domains. However, the enigmatic\n``black-box'' nature of LLMs remains a significant challenge for\ninterpretability, hampering transparent and accountable applications. While\npast approaches, such as attention visualization, pivotal subnetwork\nextraction, and concept-based analyses, offer some insight, they often focus on\neither local or global explanations within a single dimension, occasionally\nfalling short in providing comprehensive clarity. In response, we propose a\nnovel methodology anchored in sparsity-guided techniques, aiming to provide a\nholistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively\nintegrates sparsity to elucidate three intertwined layers of interpretation:\ninput, subnetwork, and concept levels. In addition, the newly introduced\ndimension of interpretable inference-time intervention facilitates dynamic\nadjustments to the model during deployment. Through rigorous empirical\nevaluations on real-world datasets, we demonstrate that SparseCBM delivers a\nprofound understanding of LLM behaviors, setting it apart in both interpreting\nand ameliorating model inaccuracies. Codes are provided in supplements.",
        "pdf_link": "https://arxiv.org/pdf/2312.15033v1.pdf"
    },
    {
        "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
        "authors": [
            "Lizhou Fan",
            "Wenyue Hua",
            "Lingyao Li",
            "Haoyang Ling",
            "Yongfeng Zhang"
        ],
        "published": "2023-12-22T18:07:44Z",
        "summary": "Complex reasoning ability is one of the most important features of current\nLLMs, which has also been leveraged to play an integral role in complex\ndecision-making tasks. Therefore, the investigation into the reasoning\ncapabilities of Large Language Models (LLMs) is critical: numerous benchmarks\nhave been established to assess the reasoning abilities of LLMs. However,\ncurrent benchmarks are inadequate in offering a rigorous evaluation of the full\nextent of reasoning abilities that LLMs are capable of achieving. They are also\nprone to the risk of overfitting, as these benchmarks, being publicly\naccessible and static, allow models to potentially tailor their responses to\nspecific benchmark metrics, thereby inflating their performance. Addressing\nthese limitations, our research introduces a new benchmark, named NPHardEval.\nThis benchmark is designed to evaluate the reasoning abilities of LLMs across a\nbroad spectrum of 900 algorithmic questions, extending up to the NP-Hard\ncomplexity class. These questions are meticulously chosen to represent a wide\nrange of complexity class below the NP-hard complexity class, offering a\nrigorous measure of the reasoning ability of LLMs. Through this study, we shed\nlight on the current state of reasoning in LLMs, providing an objective and\nrigorous perspective through the comparison of LLMs' performance across complex\nclasses. Moreover, this benchmark is designed with a dynamic update mechanism,\nwhere the datapoints are refreshed on a monthly basis. Such regular updates\nplay a crucial role in mitigating the risk of LLMs overfitting to the\nbenchmark, promoting a more accurate and reliable assessment of their reasoning\ncapabilities. The benchmark dataset and code of NPHardEval are available at\nhttps://github.com/casmlab/NPHardEval.",
        "pdf_link": "https://arxiv.org/pdf/2312.14890v4.pdf"
    },
    {
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
        "authors": [
            "Filippos Christianos",
            "Georgios Papoudakis",
            "Matthieu Zimmer",
            "Thomas Coste",
            "Zhihao Wu",
            "Jingxuan Chen",
            "Khyati Khandelwal",
            "James Doran",
            "Xidong Feng",
            "Jiacheng Liu",
            "Zheng Xiong",
            "Yicheng Luo",
            "Jianye Hao",
            "Kun Shao",
            "Haitham Bou-Ammar",
            "Jun Wang"
        ],
        "published": "2023-12-22T17:57:57Z",
        "summary": "A key method for creating Artificial Intelligence (AI) agents is\nReinforcement Learning (RL). However, constructing a standalone RL policy that\nmaps perception to action directly encounters severe problems, chief among them\nbeing its lack of generality across multiple tasks and the need for a large\namount of training data. The leading cause is that it cannot effectively\nintegrate prior information into the perception-action cycle when devising the\npolicy. Large language models (LLMs) emerged as a fundamental way to\nincorporate cross-domain knowledge into AI agents but lack crucial learning and\nadaptation toward specific decision problems. This paper presents a general\nframework model for integrating and learning structured reasoning into AI\nagents' policies. Our methodology is motivated by the modularity found in the\nhuman brain. The framework utilises the construction of intrinsic and extrinsic\nfunctions to add previous understandings of reasoning structures. It also\nprovides the adaptive ability to learn models inside every module or function,\nconsistent with the modular structure of cognitive processes. We describe the\nframework in-depth and compare it with other AI pipelines and existing\nframeworks. The paper explores practical applications, covering experiments\nthat show the effectiveness of our method. Our results indicate that AI agents\nperform and adapt far better when organised reasoning and prior knowledge are\nembedded. This opens the door to more resilient and general AI agent systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.14878v1.pdf"
    },
    {
        "title": "Robust Knowledge Extraction from Large Language Models using Social Choice Theory",
        "authors": [
            "Nico Potyka",
            "Yuqicheng Zhu",
            "Yunjie He",
            "Evgeny Kharlamov",
            "Steffen Staab"
        ],
        "published": "2023-12-22T17:57:29Z",
        "summary": "Large-language models (LLMs) can support a wide range of applications like\nconversational agents, creative writing or general query answering. However,\nthey are ill-suited for query answering in high-stake domains like medicine\nbecause they are typically not robust - even the same query can result in\ndifferent answers when prompted multiple times. In order to improve the\nrobustness of LLM queries, we propose using ranking queries repeatedly and to\naggregate the queries using methods from social choice theory. We study ranking\nqueries in diagnostic settings like medical and fault diagnosis and discuss how\nthe Partial Borda Choice function from the literature can be applied to merge\nmultiple query results. We discuss some additional interesting properties in\nour setting and evaluate the robustness of our approach empirically.",
        "pdf_link": "https://arxiv.org/pdf/2312.14877v2.pdf"
    },
    {
        "title": "Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code",
        "authors": [
            "Shahin Honarvar",
            "Mark van der Wilk",
            "Alastair Donaldson"
        ],
        "published": "2023-12-22T17:29:08Z",
        "summary": "We present a method for systematically evaluating the correctness and\nrobustness of instruction-tuned large language models (LLMs) for code\ngeneration via a new benchmark, Turbulence. Turbulence consists of a large set\nof natural language $\\textit{question templates}$, each of which is a\nprogramming problem, parameterised so that it can be asked in many different\nforms. Each question template has an associated $\\textit{test oracle}$ that\njudges whether a code solution returned by an LLM is correct. Thus, from a\nsingle question template, it is possible to ask an LLM a\n$\\textit{neighbourhood}$ of very similar programming questions, and assess the\ncorrectness of the result returned for each question. This allows gaps in an\nLLM's code generation abilities to be identified, including\n$\\textit{anomalies}$ where the LLM correctly solves $\\textit{almost all}$\nquestions in a neighbourhood but fails for particular parameter instantiations.\nWe present experiments against five LLMs from OpenAI, Cohere and Meta, each at\ntwo temperature configurations. Our findings show that, across the board,\nTurbulence is able to reveal gaps in LLM reasoning ability. This goes beyond\nmerely highlighting that LLMs sometimes produce wrong code (which is no\nsurprise): by systematically identifying cases where LLMs are able to solve\nsome problems in a neighbourhood but do not manage to generalise to solve the\nwhole neighbourhood, our method is effective at highlighting\n$\\textit{robustness}$ issues. We present data and examples that shed light on\nthe kinds of mistakes that LLMs make when they return incorrect code results.",
        "pdf_link": "https://arxiv.org/pdf/2312.14856v2.pdf"
    },
    {
        "title": "Plan, Posture and Go: Towards Open-World Text-to-Motion Generation",
        "authors": [
            "Jinpeng Liu",
            "Wenxun Dai",
            "Chunyu Wang",
            "Yiji Cheng",
            "Yansong Tang",
            "Xin Tong"
        ],
        "published": "2023-12-22T17:02:45Z",
        "summary": "Conventional text-to-motion generation methods are usually trained on limited\ntext-motion pairs, making them hard to generalize to open-world scenarios. Some\nworks use the CLIP model to align the motion space and the text space, aiming\nto enable motion generation from natural language motion descriptions. However,\nthey are still constrained to generate limited and unrealistic in-place\nmotions. To address these issues, we present a divide-and-conquer framework\nnamed PRO-Motion, which consists of three modules as motion planner,\nposture-diffuser and go-diffuser. The motion planner instructs Large Language\nModels (LLMs) to generate a sequence of scripts describing the key postures in\nthe target motion. Differing from natural languages, the scripts can describe\nall possible postures following very simple text templates. This significantly\nreduces the complexity of posture-diffuser, which transforms a script to a\nposture, paving the way for open-world generation. Finally, go-diffuser,\nimplemented as another diffusion model, estimates whole-body translations and\nrotations for all postures, resulting in realistic motions. Experimental\nresults have shown the superiority of our method with other counterparts, and\ndemonstrated its capability of generating diverse and realistic motions from\ncomplex open-world prompts such as \"Experiencing a profound sense of joy\". The\nproject page is available at https://moonsliu.github.io/Pro-Motion.",
        "pdf_link": "https://arxiv.org/pdf/2312.14828v1.pdf"
    },
    {
        "title": "Large Language Model (LLM) Bias Index -- LLMBI",
        "authors": [
            "Abiodun Finbarrs Oketunji",
            "Muhammad Anas",
            "Deepthi Saina"
        ],
        "published": "2023-12-22T15:38:13Z",
        "summary": "The Large Language Model Bias Index (LLMBI) is a pioneering approach designed\nto quantify and address biases inherent in large language models (LLMs), such\nas GPT-4. We recognise the increasing prevalence and impact of LLMs across\ndiverse sectors. This research introduces a novel metric, LLMBI, to\nsystematically measure and mitigate biases potentially skewing model responses.\nWe formulated LLMBI using a composite scoring system incorporating multiple\ndimensions of bias, including but not limited to age, gender, and racial\nbiases. To operationalise this metric, we engaged in a multi-step process\ninvolving collecting and annotating LLM responses, applying sophisticated\nNatural Language Processing (NLP) techniques for bias detection, and computing\nthe LLMBI score through a specially crafted mathematical formula. The formula\nintegrates weighted averages of various bias dimensions, a penalty for dataset\ndiversity deficiencies, and a correction for sentiment biases. Our empirical\nanalysis, conducted using responses from OpenAI's API, employs advanced\nsentiment analysis as a representative method for bias detection. The research\nreveals LLMs, whilst demonstrating impressive capabilities in text generation,\nexhibit varying degrees of bias across different dimensions. LLMBI provides a\nquantifiable measure to compare biases across models and over time, offering a\nvital tool for systems engineers, researchers and regulators in enhancing the\nfairness and reliability of LLMs. It highlights the potential of LLMs in\nmimicking unbiased human-like responses. Additionally, it underscores the\nnecessity of continuously monitoring and recalibrating such models to align\nwith evolving societal norms and ethical standards.",
        "pdf_link": "https://arxiv.org/pdf/2312.14769v3.pdf"
    },
    {
        "title": "Theory of Hallucinations based on Equivariance",
        "authors": [
            "Hisaichi Shibata"
        ],
        "published": "2023-12-22T08:08:45Z",
        "summary": "This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.14504v2.pdf"
    },
    {
        "title": "Empowering Working Memory for Large Language Model Agents",
        "authors": [
            "Jing Guo",
            "Nan Li",
            "Jianchuan Qi",
            "Hang Yang",
            "Ruiqiao Li",
            "Yuzhen Feng",
            "Si Zhang",
            "Ming Xu"
        ],
        "published": "2023-12-22T05:59:00Z",
        "summary": "Large language models (LLMs) have achieved impressive linguistic\ncapabilities. However, a key limitation persists in their lack of human-like\nmemory faculties. LLMs exhibit constrained memory retention across sequential\ninteractions, hindering complex reasoning. This paper explores the potential of\napplying cognitive psychology's working memory frameworks, to enhance LLM\narchitecture. The limitations of traditional LLM memory designs are analyzed,\nincluding their isolation of distinct dialog episodes and lack of persistent\nmemory links. To address this, an innovative model is proposed incorporating a\ncentralized Working Memory Hub and Episodic Buffer access to retain memories\nacross episodes. This architecture aims to provide greater continuity for\nnuanced contextual reasoning during intricate tasks and collaborative\nscenarios. While promising, further research is required into optimizing\nepisodic memory encoding, storage, prioritization, retrieval, and security.\nOverall, this paper provides a strategic blueprint for developing LLM agents\nwith more sophisticated, human-like memory capabilities, highlighting memory\nmechanisms as a vital frontier in artificial general intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2312.17259v1.pdf"
    },
    {
        "title": "Don't Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models",
        "authors": [
            "Priyesh Vakharia",
            "Devavrat Joshi",
            "Meenal Chavan",
            "Dhananjay Sonawane",
            "Bhrigu Garg",
            "Parsa Mazaheri"
        ],
        "published": "2023-12-22T00:31:46Z",
        "summary": "Large Language Models (LLMs) are adept at text manipulation -- tasks such as\nmachine translation and text summarization. However, these models can also be\nprone to hallucination, which can be detrimental to the faithfulness of any\nanswers that the model provides. Recent works in combating hallucinations in\nLLMs deal with identifying hallucinated sentences and categorizing the\ndifferent ways in which models hallucinate. This paper takes a deep dive into\nLLM behavior with respect to hallucinations, defines a token-level approach to\nidentifying different kinds of hallucinations, and further utilizes this\ntoken-level tagging to improve the interpretability and faithfulness of LLMs in\ndialogue summarization tasks. Through this, the paper presents a new, enhanced\ndataset and a new training paradigm.",
        "pdf_link": "https://arxiv.org/pdf/2312.14346v2.pdf"
    },
    {
        "title": "Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs",
        "authors": [
            "Behnam Rahdari",
            "Hao Ding",
            "Ziwei Fan",
            "Yifei Ma",
            "Zhuotong Chen",
            "Anoop Deoras",
            "Branislav Kveton"
        ],
        "published": "2023-12-22T00:30:10Z",
        "summary": "The unique capabilities of Large Language Models (LLMs), such as the natural\nlanguage text generation ability, position them as strong candidates for\nproviding explanation for recommendations. However, despite the size of the\nLLM, most existing models struggle to produce zero-shot explanations reliably.\nTo address this issue, we propose a framework called Logic-Scaffolding, that\ncombines the ideas of aspect-based explanation and chain-of-thought prompting\nto generate explanations through intermediate reasoning steps. In this paper,\nwe share our experience in building the framework and present an interactive\ndemonstration for exploring our results.",
        "pdf_link": "https://arxiv.org/pdf/2312.14345v2.pdf"
    },
    {
        "title": "Context-aware Decoding Reduces Hallucination in Query-focused Summarization",
        "authors": [
            "Zhichao Xu"
        ],
        "published": "2023-12-21T23:42:13Z",
        "summary": "Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",
        "pdf_link": "https://arxiv.org/pdf/2312.14335v2.pdf"
    },
    {
        "title": "Parameter Efficient Tuning Allows Scalable Personalization of LLMs for Text Entry: A Case Study on Abbreviation Expansion",
        "authors": [
            "Katrin Tomanek",
            "Shanqing Cai",
            "Subhashini Venugopalan"
        ],
        "published": "2023-12-21T22:52:44Z",
        "summary": "Abbreviation expansion is a strategy used to speed up communication by\nlimiting the amount of typing and using a language model to suggest expansions.\nHere we look at personalizing a Large Language Model's (LLM) suggestions based\non prior conversations to enhance the relevance of predictions, particularly\nwhen the user data is small (~1000 samples). Specifically, we compare\nfine-tuning, prompt-tuning, and retrieval augmented generation of expanded text\nsuggestions for abbreviated inputs. Our case study with a deployed 8B parameter\nLLM on a real user living with ALS, and experiments on movie character\npersonalization indicates that (1) customization may be necessary in some\nscenarios and prompt-tuning generalizes well to those, (2) fine-tuning on\nin-domain data (with as few as 600 samples) still shows some gains, however (3)\nretrieval augmented few-shot selection also outperforms fine-tuning. (4)\nParameter efficient tuning allows for efficient and scalable personalization.\nFor prompt-tuning, we also find that initializing the learned \"soft-prompts\" to\nuser relevant concept tokens leads to higher accuracy than random\ninitialization.",
        "pdf_link": "https://arxiv.org/pdf/2312.14327v1.pdf"
    },
    {
        "title": "From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models",
        "authors": [
            "Wolfgang Messner",
            "Tatum Greene",
            "Josephine Matalone"
        ],
        "published": "2023-12-21T22:50:14Z",
        "summary": "Large language models (LLMs) are able to engage in natural-sounding\nconversations with humans, showcasing unprecedented capabilities for\ninformation retrieval and automated decision support. They have disrupted\nhuman-technology interaction and the way businesses operate. However,\ntechnologies based on generative artificial intelligence (GenAI) are known to\nhallucinate, misinform, and display biases introduced by the massive datasets\non which they are trained. Existing research indicates that humans may\nunconsciously internalize these biases, which can persist even after they stop\nusing the programs. This study explores the cultural self-perception of LLMs by\nprompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from\nthe GLOBE project. The findings reveal that their cultural self-perception is\nmost closely aligned with the values of English-speaking countries and\ncountries characterized by sustained economic competitiveness. Recognizing the\ncultural biases of LLMs and understanding how they work is crucial for all\nmembers of society because one does not want the black box of artificial\nintelligence to perpetuate bias in humans, who might, in turn, inadvertently\ncreate and train even more biased algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2312.17256v1.pdf"
    },
    {
        "title": "LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding",
        "authors": [
            "Senqiao Yang",
            "Jiaming Liu",
            "Ray Zhang",
            "Mingjie Pan",
            "Zoey Guo",
            "Xiaoqi Li",
            "Zehui Chen",
            "Peng Gao",
            "Yandong Guo",
            "Shanghang Zhang"
        ],
        "published": "2023-12-21T17:52:12Z",
        "summary": "Recently, Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs) have shown promise in instruction following and 2D image understanding.\nWhile these models are powerful, they have not yet been developed to comprehend\nthe more challenging 3D physical scenes, especially when it comes to the sparse\noutdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw\nLiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs\nto gain a comprehensive understanding of outdoor 3D scenes. The central insight\nof our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a\nlanguage modeling problem, encompassing tasks such as 3D captioning, 3D\ngrounding, 3D question answering, etc. Specifically, due to the scarcity of 3D\nLiDAR-text pairing data, we introduce a three-stage training strategy and\ngenerate relevant datasets, progressively aligning the 3D modality with the\nlanguage embedding space of LLM. Furthermore, we design a View-Aware\nTransformer (VAT) to connect the 3D encoder with the LLM, which effectively\nbridges the modality gap and enhances the LLM's spatial orientation\ncomprehension of visual features. Our experiments show that LiDAR-LLM possesses\nfavorable capabilities to comprehend various instructions regarding 3D scenes\nand engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the\n3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\%\nBEV mIoU on the 3D grounding task. Web page:\nhttps://sites.google.com/view/lidar-llm",
        "pdf_link": "https://arxiv.org/pdf/2312.14074v1.pdf"
    },
    {
        "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
        "authors": [
            "Zehui Chen",
            "Weihua Du",
            "Wenwei Zhang",
            "Kuikun Liu",
            "Jiangning Liu",
            "Miao Zheng",
            "Jingming Zhuo",
            "Songyang Zhang",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "published": "2023-12-21T17:02:06Z",
        "summary": "Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2312.14033v3.pdf"
    },
    {
        "title": "AsyncMLD: Asynchronous Multi-LLM Framework for Dialogue Recommendation System",
        "authors": [
            "Naoki Yoshimaru",
            "Motoharu Okuma",
            "Takamasa Iio",
            "Kenji Hatano"
        ],
        "published": "2023-12-21T15:12:59Z",
        "summary": "We have reached a practical and realistic phase in human-support dialogue\nagents by developing a large language model (LLM). However, when requiring\nexpert knowledge or anticipating the utterance content using the massive size\nof the dialogue database, we still need help with the utterance content's\neffectiveness and the efficiency of its output speed, even if using LLM.\nTherefore, we propose a framework that uses LLM asynchronously in the part of\nthe system that returns an appropriate response and in the part that\nunderstands the user's intention and searches the database. In particular,\nnoting that it takes time for the robot to speak, threading related to database\nsearches is performed while the robot is speaking.",
        "pdf_link": "https://arxiv.org/pdf/2312.13925v1.pdf"
    },
    {
        "title": "SimLM: Can Language Models Infer Parameters of Physical Systems?",
        "authors": [
            "Sean Memery",
            "Mirella Lapata",
            "Kartic Subr"
        ],
        "published": "2023-12-21T12:05:19Z",
        "summary": "Several machine learning methods aim to learn or reason about complex\nphysical systems. A common first-step towards reasoning is to infer system\nparameters from observations of its behavior. In this paper, we investigate the\nperformance of Large Language Models (LLMs) at performing parameter inference\nin the context of physical systems. Our experiments suggest that they are not\ninherently suited to this task, even for simple systems. We propose a promising\ndirection of exploration, which involves the use of physical simulators to\naugment the context of LLMs. We assess and compare the performance of different\nLLMs on a simple example with and without access to physical simulation.",
        "pdf_link": "https://arxiv.org/pdf/2312.14215v2.pdf"
    },
    {
        "title": "On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning",
        "authors": [
            "Chengzu Li",
            "Han Zhou",
            "Goran Glava≈°",
            "Anna Korhonen",
            "Ivan Vuliƒá"
        ],
        "published": "2023-12-21T11:55:10Z",
        "summary": "Following the standard supervised fine-tuning (SFT) paradigm, in-context\nlearning (ICL) has become an efficient approach propelled by the recent\nadvancements in large language models (LLMs), yielding promising performance\nacross various tasks in few-shot data setups. However, both paradigms are prone\nto suffer from the critical problem of overconfidence (i.e., miscalibration),\nespecially in such limited data setups. In this work, we deliver an in-depth\nanalysis of the behavior across different choices of learning methods from the\nperspective of both performance and calibration, as well as their interplay.\nThrough extensive controlled experiments, we find that simultaneous gains for\nboth task performance and calibration are difficult to achieve, and the problem\nof miscalibration exists across all learning methods in low-resource scenarios.\nTo address this challenging trade-off between performance and calibration, we\nthen investigate the potential of self-ensembling techniques applied at\ndifferent modeling stages (e.g., variations of in-context examples or\nvariations in prompts or different ensembling strategies). We justify the\nfeasibility of self-ensembling on SFT in addition to ICL, to make the\npredictions more calibrated and have comparable or even better performance. Our\nwork sheds light on which learning paradigm to choose and how to enhance both\ntask performance and calibration of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.13772v2.pdf"
    },
    {
        "title": "Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",
        "authors": [
            "Samuel J. Aronson",
            "Kalotina Machini",
            "Jiyeon Shin",
            "Pranav Sriraman",
            "Sean Hamill",
            "Emma R. Henricks",
            "Charlotte Mailly",
            "Angie J. Nottage",
            "Sami S. Amr",
            "Michael Oates",
            "Matthew S. Lebo"
        ],
        "published": "2023-12-21T01:56:00Z",
        "summary": "Background. Large Language Models (LLMs) hold promise for improving genetic\nvariant literature review in clinical testing. We assessed Generative\nPretrained Transformer 4's (GPT-4) performance, nondeterminism, and drift to\ninform its suitability for use in complex clinical processes. Methods. A\n2-prompt process for classification of functional evidence was optimized using\na development set of 45 articles. The prompts asked GPT-4 to supply all\nfunctional data present in an article related to a variant or indicate that no\nfunctional evidence is present. For articles indicated as containing functional\nevidence, a second prompt asked GPT-4 to classify the evidence into pathogenic,\nbenign, or intermediate/inconclusive categories. A final test set of 72\nmanually classified articles was used to test performance. Results. Over a\n2.5-month period (Dec 2023-Feb 2024), we observed substantial differences in\nintraday (nondeterminism) and across day (drift) results, which lessened after\n1/18/24. This variability is seen within and across models in the GPT-4 series,\naffecting different performance statistics to different degrees. Twenty runs\nafter 1/18/24 identified articles containing functional evidence with 92.2%\nsensitivity, 95.6% positive predictive value (PPV) and 86.3% negative\npredictive value (NPV). The second prompt's identified pathogenic functional\nevidence with 90.0% sensitivity, 74.0% PPV and 95.3% NVP and for benign\nevidence with 88.0% sensitivity, 76.6% PPV and 96.9% NVP. Conclusion.\nNondeterminism and drift within LLMs must be assessed and monitored when\nintroducing LLM based functionality into clinical workflows. Failing to do this\nassessment or accounting for these challenges could lead to incorrect or\nmissing information that is critical for patient care. The performance of our\nprompts appears adequate to assist in article prioritization but not in\nautomated decision making.",
        "pdf_link": "https://arxiv.org/pdf/2312.13521v2.pdf"
    },
    {
        "title": "L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs",
        "authors": [
            "Md. Kowsher",
            "Md. Shohanur Islam Sobuj",
            "Asif Mahmud",
            "Nusrat Jahan Prottasha",
            "Prakash Bhat"
        ],
        "published": "2023-12-21T01:47:49Z",
        "summary": "Efficiently fine-tuning Large Language Models (LLMs) for specific tasks\npresents a considerable challenge in natural language processing. Traditional\nmethods, like prompt or prefix tuning, typically rely on arbitrary tokens for\ntraining, leading to prolonged training times and generalized token use across\nvarious class labels. To address these issues, this paper introduces L-Tuning,\nan efficient fine-tuning approach designed for classification tasks within the\nNatural Language Inference (NLI) framework. Diverging from conventional\nmethods, L-Tuning focuses on the fine-tuning of label tokens processed through\na pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This\ntechnique not only improves the fine-tuning accuracy and efficiency but also\nfacilitates the generation of distinct label embeddings for each class,\nenhancing the model's training nuance. Our experimental results indicate a\nsignificant improvement in training efficiency and classification accuracy with\nL-Tuning compared to traditional approaches, marking a promising advancement in\nfine-tuning LLMs for complex language tasks. \\\\ Code is available at:\n\\textcolor{red}{\\href{https://github.com/Kowsher/L-Tuning}{\\texttt{https://github.com/Kowsher/L-Tuning}}}.",
        "pdf_link": "https://arxiv.org/pdf/2402.01643v1.pdf"
    },
    {
        "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
        "authors": [
            "Jingwei Yi",
            "Yueqi Xie",
            "Bin Zhu",
            "Emre Kiciman",
            "Guangzhong Sun",
            "Xing Xie",
            "Fangzhao Wu"
        ],
        "published": "2023-12-21T01:08:39Z",
        "summary": "The integration of large language models (LLMs) with external content has\nenabled more up-to-date and wide-ranging applications of LLMs, such as\nMicrosoft Copilot. However, this integration has also exposed LLMs to the risk\nof indirect prompt injection attacks, where an attacker can embed malicious\ninstructions within external content, compromising LLM output and causing\nresponses to deviate from user expectations. To investigate this important but\nunderexplored issue, we introduce the first benchmark for indirect prompt\ninjection attacks, named BIPIA, to evaluate the risk of such attacks. Based on\nthe evaluation, our work makes a key analysis of the underlying reason for the\nsuccess of the attack, namely the inability of LLMs to distinguish between\ninstructions and external content and the absence of LLMs' awareness to not\nexecute instructions within external content. Building upon this analysis, we\ndevelop two black-box methods based on prompt learning and a white-box defense\nmethod based on fine-tuning with adversarial training accordingly. Experimental\nresults demonstrate that black-box defenses are highly effective in mitigating\nthese attacks, while the white-box defense reduces the attack success rate to\nnear-zero levels. Overall, our work systematically investigates indirect prompt\ninjection attacks by introducing a benchmark, analyzing the underlying reason\nfor the success of the attack, and developing an initial set of defenses.",
        "pdf_link": "https://arxiv.org/pdf/2312.14197v3.pdf"
    },
    {
        "title": "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation",
        "authors": [
            "Difei Gao",
            "Lei Ji",
            "Zechen Bai",
            "Mingyu Ouyang",
            "Peiran Li",
            "Dongxing Mao",
            "Qinchen Wu",
            "Weichen Zhang",
            "Peiyi Wang",
            "Xiangwu Guo",
            "Hengxu Wang",
            "Luowei Zhou",
            "Mike Zheng Shou"
        ],
        "published": "2023-12-20T15:28:38Z",
        "summary": "Graphical User Interface (GUI) automation holds significant promise for\nassisting users with complex tasks, thereby boosting human productivity.\nExisting works leveraging Large Language Model (LLM) or LLM-based AI agents\nhave shown capabilities in automating tasks on Android and Web platforms.\nHowever, these tasks are primarily aimed at simple device usage and\nentertainment operations. This paper presents a novel benchmark, AssistGUI, to\nevaluate whether models are capable of manipulating the mouse and keyboard on\nthe Windows platform in response to user-requested tasks. We carefully\ncollected a set of 100 tasks from nine widely-used software applications, such\nas, After Effects and MS Word, each accompanied by the necessary project files\nfor better evaluation. Moreover, we propose an advanced Actor-Critic Embodied\nAgent framework, which incorporates a sophisticated GUI parser driven by an\nLLM-agent and an enhanced reasoning mechanism adept at handling lengthy\nprocedural tasks. Our experimental results reveal that our GUI Parser and\nReasoning mechanism outshine existing methods in performance. Nevertheless, the\npotential remains substantial, with the best model attaining only a 46% success\nrate on our benchmark. We conclude with a thorough analysis of the current\nmethods' limitations, setting the stage for future breakthroughs in this\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2312.13108v2.pdf"
    },
    {
        "title": "Retrieval-augmented Multilingual Knowledge Editing",
        "authors": [
            "Weixuan Wang",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "published": "2023-12-20T14:08:58Z",
        "summary": "Knowledge represented in Large Language Models (LLMs) is quite often\nincorrect and can also become obsolete over time. Updating knowledge via\nfine-tuning is computationally resource-hungry and not reliable, and so\nknowledge editing (KE) has developed as an effective and economical alternative\nto inject new knowledge or to fix factual errors in LLMs. Although there has\nbeen considerable interest in this area, current KE research exclusively\nfocuses on the monolingual setting, typically in English. However, what happens\nif the new knowledge is supplied in one language, but we would like to query\nthe LLM in a different language? To address the problem of multilingual\nknowledge editing, we propose Retrieval-augmented Multilingual Knowledge Editor\n(ReMaKE) to update new knowledge in LLMs. ReMaKE can perform model-agnostic\nknowledge editing in multilingual settings. ReMaKE concatenates the new\nknowledge retrieved from a multilingual knowledge base with prompts. Our\nexperimental results show that ReMaKE outperforms baseline knowledge editing\nmethods by a significant margin and is the first KE method to work in a\nmultilingual setting. We provide our multilingual knowledge editing dataset\n(MzsRE) in 12 languages, which along with code, and additional project\ninformation is available at https://github.com/Vicky-Wil/ReMaKE.",
        "pdf_link": "https://arxiv.org/pdf/2312.13040v1.pdf"
    },
    {
        "title": "Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors",
        "authors": [
            "Yi-Fan Zhang",
            "Zhang Zhang",
            "Liang Wang",
            "Tieniu Tan",
            "Rong Jin"
        ],
        "published": "2023-12-20T10:53:53Z",
        "summary": "To combat the potential misuse of Natural Language Generation (NLG)\ntechnology, a variety of algorithms have been developed for the detection of\nAI-generated texts. Traditionally, this task is treated as a binary\nclassification problem. Although supervised learning has demonstrated promising\nresults, acquiring labeled data for detection purposes poses real-world\nchallenges and the risk of overfitting. In an effort to address these issues,\nwe delve into the realm of zero-shot machine-generated text detection. Existing\nzero-shot detectors, typically designed for specific tasks or topics, often\nassume uniform testing scenarios, limiting their practicality. In our research,\nwe explore various advanced Large Language Models (LLMs) and their specialized\nvariants, contributing to this field in several ways. In empirical studies, we\nuncover a significant correlation between topics and detection performance.\nSecondly, we delve into the influence of topic shifts on zero-shot detectors.\nThese investigations shed light on the adaptability and robustness of these\ndetection methods across diverse topics. The code is available at\n\\url{https://github.com/yfzhang114/robustness-detection}.",
        "pdf_link": "https://arxiv.org/pdf/2312.12918v2.pdf"
    },
    {
        "title": "Testing the Segment Anything Model on radiology data",
        "authors": [
            "Jos√© Guilherme de Almeida",
            "Nuno M. Rodrigues",
            "Sara Silva",
            "Nickolas Papanikolaou"
        ],
        "published": "2023-12-20T09:45:21Z",
        "summary": "Deep learning models trained with large amounts of data have become a recent\nand effective approach to predictive problem solving -- these have become known\nas \"foundation models\" as they can be used as fundamental tools for other\napplications. While the paramount examples of image classification (earlier)\nand large language models (more recently) led the way, the Segment Anything\nModel (SAM) was recently proposed and stands as the first foundation model for\nimage segmentation, trained on over 10 million images and with recourse to over\n1 billion masks. However, the question remains -- what are the limits of this\nfoundation? Given that magnetic resonance imaging (MRI) stands as an important\nmethod of diagnosis, we sought to understand whether SAM could be used for a\nfew tasks of zero-shot segmentation using MRI data. Particularly, we wanted to\nknow if selecting masks from the pool of SAM predictions could lead to good\nsegmentations.\n  Here, we provide a critical assessment of the performance of SAM on magnetic\nresonance imaging data. We show that, while acceptable in a very limited set of\ncases, the overall trend implies that these models are insufficient for MRI\nsegmentation across the whole volume, but can provide good segmentations in a\nfew, specific slices. More importantly, we note that while foundation models\ntrained on natural images are set to become key aspects of predictive\nmodelling, they may prove ineffective when used on other imaging modalities.",
        "pdf_link": "https://arxiv.org/pdf/2312.12880v1.pdf"
    },
    {
        "title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models",
        "authors": [
            "Dan Shi",
            "Chaobin You",
            "Jiantao Huang",
            "Taihao Li",
            "Deyi Xiong"
        ],
        "published": "2023-12-20T09:06:18Z",
        "summary": "As an indispensable ingredient of intelligence, commonsense reasoning is\ncrucial for large language models (LLMs) in real-world scenarios. In this\npaper, we propose CORECODE, a dataset that contains abundant commonsense\nknowledge manually annotated on dyadic dialogues, to evaluate the commonsense\nreasoning and commonsense conflict detection capabilities of Chinese LLMs. We\ncategorize commonsense knowledge in everyday conversations into three\ndimensions: entity, event, and social interaction. For easy and consistent\nannotation, we standardize the form of commonsense knowledge annotation in\nopen-domain dialogues as \"domain: slot = value\". A total of 9 domains and 37\nslots are defined to capture diverse commonsense knowledge. With these\npre-defined domains and slots, we collect 76,787 commonsense knowledge\nannotations from 19,700 dialogues through crowdsourcing. To evaluate and\nenhance the commonsense reasoning capability for LLMs on the curated dataset,\nwe establish a series of dialogue-level reasoning and detection tasks,\nincluding commonsense knowledge filling, commonsense knowledge generation,\ncommonsense conflict phrase detection, domain identification, slot\nidentification, and event causal inference. A wide variety of existing\nopen-source Chinese LLMs are evaluated with these tasks on our dataset.\nExperimental results demonstrate that these models are not competent to predict\nCORECODE's plentiful reasoning content, and even ChatGPT could only achieve\n0.275 and 0.084 accuracy on the domain identification and slot identification\ntasks under the zero-shot setting. We release the data and codes of CORECODE at\nhttps://github.com/danshi777/CORECODE to promote commonsense reasoning\nevaluation and study of LLMs in the context of daily conversations.",
        "pdf_link": "https://arxiv.org/pdf/2312.12853v1.pdf"
    },
    {
        "title": "Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data",
        "authors": [
            "Yiwei Li",
            "Peiwen Yuan",
            "Shaoxiong Feng",
            "Boyuan Pan",
            "Bin Sun",
            "Xinglin Wang",
            "Heda Wang",
            "Kan Li"
        ],
        "published": "2023-12-20T08:28:36Z",
        "summary": "Large Language Models (LLMs) have performed well on various reasoning tasks,\nbut their inaccessibility and numerous parameters hinder wide application in\npractice. One promising way is distilling the reasoning ability from LLMs to\nsmall models by the generated chain-of-thought reasoning paths. In some cases,\nhowever, LLMs may produce incorrect reasoning chains, especially when facing\ncomplex mathematical problems. Previous studies only transfer knowledge from\npositive samples and drop the synthesized data with wrong answers. In this\nwork, we illustrate the merit of negative data and propose a model\nspecialization framework to distill LLMs with negative samples besides positive\nones. The framework consists of three progressive steps, covering from training\nto inference stages, to absorb knowledge from negative data. We conduct\nextensive experiments across arithmetic reasoning tasks to demonstrate the role\nof negative data in distillation from LLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.12832v1.pdf"
    },
    {
        "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
        "authors": [
            "Edmund Mills",
            "Shiye Su",
            "Stuart Russell",
            "Scott Emmons"
        ],
        "published": "2023-12-20T03:44:18Z",
        "summary": "How do we measure the efficacy of language model explainability methods?\nWhile many explainability methods have been developed, they are typically\nevaluated on bespoke tasks, preventing an apples-to-apples comparison. To help\nfill this gap, we present ALMANACS, a language model explainability benchmark.\nALMANACS scores explainability methods on simulatability, i.e., how well the\nexplanations improve behavior prediction on new inputs. The ALMANACS scenarios\nspan twelve safety-relevant topics such as ethical reasoning and advanced AI\nbehaviors; they have idiosyncratic premises to invoke model-specific behavior;\nand they have a train-test distributional shift to encourage faithful\nexplanations. By using another language model to predict behavior based on the\nexplanations, ALMANACS is a fully automated benchmark. We use ALMANACS to\nevaluate counterfactuals, rationalizations, attention, and Integrated Gradients\nexplanations. Our results are sobering: when averaged across all topics, no\nexplanation method outperforms the explanation-free control. We conclude that\ndespite modest successes in prior work, developing an explanation method that\naids simulatability in ALMANACS remains an open challenge.",
        "pdf_link": "https://arxiv.org/pdf/2312.12747v1.pdf"
    },
    {
        "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
        "authors": [
            "Jiachen Zhao",
            "Zhun Deng",
            "David Madras",
            "James Zou",
            "Mengye Ren"
        ],
        "published": "2023-12-20T03:18:50Z",
        "summary": "As the number of large language models (LLMs) released to the public grows,\nthere is a pressing need to understand the safety implications associated with\nthese models learning from third-party custom finetuning data. We explore the\nbehavior of LLMs finetuned on noisy custom data containing unsafe content,\nrepresented by datasets that contain biases, toxicity, and harmfulness, finding\nthat while aligned LLMs can readily learn this unsafe content, they also tend\nto forget it more significantly than other examples when subsequently finetuned\non safer content. Drawing inspiration from the discrepancies in forgetting, we\nintroduce the \"ForgetFilter\" algorithm, which filters unsafe data based on how\nstrong the model's forgetting signal is for that data. We demonstrate that the\nForgetFilter algorithm ensures safety in customized finetuning without\ncompromising downstream task performance, unlike sequential safety finetuning.\nForgetFilter outperforms alternative strategies like replay and moral\nself-correction in curbing LLMs' ability to assimilate unsafe content during\ncustom finetuning, e.g. 75% lower than not applying any safety measures and 62%\nlower than using self-correction in toxicity score.",
        "pdf_link": "https://arxiv.org/pdf/2312.12736v1.pdf"
    },
    {
        "title": "A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges",
        "authors": [
            "Roberto Francisco de Lima Junior",
            "Luiz Fernando Paes de Barros Presta",
            "Lucca Santos Borborema",
            "Vanderson Nogueira da Silva",
            "Marcio Leal de Melo Dahia",
            "Anderson Carlos Sousa e Santos"
        ],
        "published": "2023-12-19T20:59:02Z",
        "summary": "This paper presents a detailed case study examining the application of Large\nLanguage Models (LLMs) in the construction of test cases within the context of\nsoftware engineering. LLMs, characterized by their advanced natural language\nprocessing capabilities, are increasingly garnering attention as tools to\nautomate and enhance various aspects of the software development life cycle.\nLeveraging a case study methodology, we systematically explore the integration\nof LLMs in the test case construction process, aiming to shed light on their\npractical efficacy, challenges encountered, and implications for software\nquality assurance. The study encompasses the selection of a representative\nsoftware application, the formulation of test case construction methodologies\nemploying LLMs, and the subsequent evaluation of outcomes. Through a blend of\nqualitative and quantitative analyses, this study assesses the impact of LLMs\non test case comprehensiveness, accuracy, and efficiency. Additionally, delves\ninto challenges such as model interpretability and adaptation to diverse\nsoftware contexts. The findings from this case study contributes with nuanced\ninsights into the practical utility of LLMs in the domain of test case\nconstruction, elucidating their potential benefits and limitations. By\naddressing real-world scenarios and complexities, this research aims to inform\nsoftware practitioners and researchers alike about the tangible implications of\nincorporating LLMs into the software testing landscape, fostering a more\ncomprehensive understanding of their role in optimizing the software\ndevelopment process.",
        "pdf_link": "https://arxiv.org/pdf/2312.12598v2.pdf"
    },
    {
        "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
        "authors": [
            "Jason Vega",
            "Isha Chaudhary",
            "Changming Xu",
            "Gagandeep Singh"
        ],
        "published": "2023-12-19T16:47:12Z",
        "summary": "With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we show that SOTA open-source LLMs are\nvulnerable to simple, optimization-free attacks we refer to as $\\textit{priming\nattacks}$, which are easy to execute and effectively bypass alignment from\nsafety training. Our proposed attack improves the Attack Success Rate on\nHarmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to\nbaselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks .",
        "pdf_link": "https://arxiv.org/pdf/2312.12321v1.pdf"
    },
    {
        "title": "On Early Detection of Hallucinations in Factual Question Answering",
        "authors": [
            "Ben Snyder",
            "Marius Moisescu",
            "Muhammad Bilal Zafar"
        ],
        "published": "2023-12-19T14:35:04Z",
        "summary": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks like search and summarization, hallucinations\nremain a major impediment towards gaining user trust. The fluency and coherence\nof model generations even when hallucinating makes it difficult to detect\nwhether or not a model is hallucinating. In this work, we explore if the\nartifacts associated with the model generations can provide hints that the\ngeneration will contain hallucinations. Specifically, we probe LLMs at 1) the\ninputs via Integrated Gradients based token attribution, 2) the outputs via the\nSoftmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC.\nWe further show that tokens preceding a hallucination can predict the\nsubsequent hallucination before it occurs.",
        "pdf_link": "https://arxiv.org/pdf/2312.14183v2.pdf"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment",
        "authors": [
            "Lingling Xu",
            "Haoran Xie",
            "Si-Zhao Joe Qin",
            "Xiaohui Tao",
            "Fu Lee Wang"
        ],
        "published": "2023-12-19T13:31:24Z",
        "summary": "With the continuous growth in the number of parameters of transformer-based\npretrained language models (PLMs), particularly the emergence of large language\nmodels (LLMs) with billions of parameters, many natural language processing\n(NLP) tasks have demonstrated remarkable success. However, the enormous size\nand computational demands of these models pose significant challenges for\nadapting them to specific downstream tasks, especially in environments with\nlimited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers\nan effective solution by reducing the number of fine-tuning parameters and\nmemory usage while achieving comparable performance to full fine-tuning. The\ndemands for fine-tuning PLMs, especially LLMs, have led to a surge in the\ndevelopment of PEFT methods, as depicted in Fig. 1. In this paper, we present a\ncomprehensive and systematic review of PEFT methods for PLMs. We summarize\nthese PEFT methods, discuss their applications, and outline future directions.\nFurthermore, we conduct experiments using several representative PEFT methods\nto better understand their effectiveness in parameter efficiency and memory\nefficiency. By offering insights into the latest advancements and practical\napplications, this survey serves as an invaluable resource for researchers and\npractitioners seeking to navigate the challenges and opportunities presented by\nPEFT in the context of PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.12148v1.pdf"
    },
    {
        "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes",
        "authors": [
            "Nabeel Seedat",
            "Nicolas Huynh",
            "Boris van Breugel",
            "Mihaela van der Schaar"
        ],
        "published": "2023-12-19T12:34:46Z",
        "summary": "Machine Learning (ML) in low-data settings remains an underappreciated yet\ncrucial problem. Hence, data augmentation methods to increase the sample size\nof datasets needed for ML are key to unlocking the transformative potential of\nML in data-deprived regions and domains. Unfortunately, the limited training\nset constrains traditional tabular synthetic data generators in their ability\nto generate a large and diverse augmented dataset needed for ML tasks. To\naddress this challenge, we introduce CLLM, which leverages the prior knowledge\nof Large Language Models (LLMs) for data augmentation in the low-data regime.\nHowever, not all the data generated by LLMs will improve downstream utility, as\nfor any generative model. Consequently, we introduce a principled curation\nmechanism, leveraging learning dynamics, coupled with confidence and\nuncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple\nreal-world datasets, we demonstrate the superior performance of CLLM in the\nlow-data regime compared to conventional generators. Additionally, we provide\ninsights into the LLM generation and curation mechanism, shedding light on the\nfeatures that enable them to output high-quality augmented datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.12112v2.pdf"
    },
    {
        "title": "A Performance Evaluation of a Quantized Large Language Model on Various Smartphones",
        "authors": [
            "Tolga √á√∂pl√º",
            "Marc Loedi",
            "Arto Bendiken",
            "Mykhailo Makohin",
            "Joshua J. Bouw",
            "Stephen Cobb"
        ],
        "published": "2023-12-19T10:19:39Z",
        "summary": "This paper explores the feasibility and performance of on-device large\nlanguage model (LLM) inference on various Apple iPhone models. Amidst the rapid\nevolution of generative AI, on-device LLMs offer solutions to privacy,\nsecurity, and connectivity challenges inherent in cloud-based models.\nLeveraging existing literature on running multi-billion parameter LLMs on\nresource-limited devices, our study examines the thermal effects and\ninteraction speeds of a high-performing LLM across different smartphone\ngenerations. We present real-world performance results, providing insights into\non-device inference capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2312.12472v1.pdf"
    },
    {
        "title": "Climate Change from Large Language Models",
        "authors": [
            "Hongyin Zhu",
            "Prayag Tiwari"
        ],
        "published": "2023-12-19T09:26:46Z",
        "summary": "Climate change presents significant challenges to the global community, and\nit is imperative to raise widespread awareness of the climate crisis and\neducate users about low-carbon living. Artificial intelligence, particularly\nlarge language models (LLMs), have emerged as powerful tools in mitigating the\nclimate crisis, leveraging their extensive knowledge, broad user base, and\nnatural language interaction capabilities. However, despite the growing body of\nresearch on climate change, there is a lack of comprehensive assessments of\nclimate crisis knowledge within LLMs. This paper aims to resolve this gap by\nproposing an automatic evaluation framework. We employ a hybrid approach to\ndata acquisition that combines data synthesis and manual collection to compile\na diverse set of questions related to the climate crisis. These questions cover\nvarious aspects of climate change, including its causes, impacts, mitigation\nstrategies, and adaptation measures. We then evaluate the model knowledge\nthrough prompt engineering based on the collected questions and generated\nanswers. We propose a set of comprehensive metrics to evaluate the climate\ncrisis knowledge, incorporating indicators from 10 different perspectives.\nExperimental results show that our method is effective in evaluating the\nknowledge of LLMs regarding the climate crisis. We evaluate several\nstate-of-the-art LLMs and find that their knowledge falls short in terms of\ntimeliness.",
        "pdf_link": "https://arxiv.org/pdf/2312.11985v2.pdf"
    },
    {
        "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
        "authors": [
            "Chen Gao",
            "Xiaochong Lan",
            "Nian Li",
            "Yuan Yuan",
            "Jingtao Ding",
            "Zhilun Zhou",
            "Fengli Xu",
            "Yong Li"
        ],
        "published": "2023-12-19T09:06:45Z",
        "summary": "Agent-based modeling and simulation has evolved as a powerful tool for\nmodeling complex systems, offering insights into emergent behaviors and\ninteractions among diverse agents. Integrating large language models into\nagent-based modeling and simulation presents a promising avenue for enhancing\nsimulation capabilities. This paper surveys the landscape of utilizing large\nlanguage models in agent-based modeling and simulation, examining their\nchallenges and promising future directions. In this survey, since this is an\ninterdisciplinary field, we first introduce the background of agent-based\nmodeling and simulation and large language model-empowered agents. We then\ndiscuss the motivation for applying large language models to agent-based\nsimulation and systematically analyze the challenges in environment perception,\nhuman alignment, action generation, and evaluation. Most importantly, we\nprovide a comprehensive overview of the recent works of large language\nmodel-empowered agent-based modeling and simulation in multiple scenarios,\nwhich can be divided into four domains: cyber, physical, social, and hybrid,\ncovering simulation of both real-world and virtual environments. Finally, since\nthis area is new and quickly evolving, we discuss the open problems and\npromising future directions.",
        "pdf_link": "https://arxiv.org/pdf/2312.11970v1.pdf"
    },
    {
        "title": "Text-Conditioned Resampler For Long Form Video Understanding",
        "authors": [
            "Bruno Korbar",
            "Yongqin Xian",
            "Alessio Tonioni",
            "Andrew Zisserman",
            "Federico Tombari"
        ],
        "published": "2023-12-19T06:42:47Z",
        "summary": "In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.",
        "pdf_link": "https://arxiv.org/pdf/2312.11897v2.pdf"
    },
    {
        "title": "Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models",
        "authors": [
            "Soaad Hossain",
            "Syed Ishtiaque Ahmed"
        ],
        "published": "2023-12-19T06:28:43Z",
        "summary": "Given the success of ChatGPT, LaMDA and other large language models (LLMs),\nthere has been an increase in development and usage of LLMs within the\ntechnology sector and other sectors. While the level in which LLMs has not\nreached a level where it has surpassed human intelligence, there will be a time\nwhen it will. Such LLMs can be referred to as advanced LLMs. Currently, there\nare limited usage of ethical artificial intelligence (AI) principles and\nguidelines addressing advanced LLMs due to the fact that we have not reached\nthat point yet. However, this is a problem as once we do reach that point, we\nwill not be adequately prepared to deal with the aftermath of it in an ethical\nand optimal way, which will lead to undesired and unexpected consequences. This\npaper addresses this issue by discussing what ethical AI principles and\nguidelines can be used to address highly advanced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.10745v1.pdf"
    },
    {
        "title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a Large Language Model-Based Difficulty Prediction",
        "authors": [
            "Unggi Lee",
            "Sungjun Yoon",
            "Joon Seo Yun",
            "Kyoungsoo Park",
            "YoungHoon Jung",
            "Damji Stratton",
            "Hyeoncheol Kim"
        ],
        "published": "2023-12-19T06:26:25Z",
        "summary": "This paper presents novel techniques for enhancing the performance of\nknowledge tracing (KT) models by focusing on the crucial factor of question and\nconcept difficulty level. Despite the acknowledged significance of difficulty,\nprevious KT research has yet to exploit its potential for model optimization\nand has struggled to predict difficulty from unseen data. To address these\nproblems, we propose a difficulty-centered contrastive learning method for KT\nmodels and a Large Language Model (LLM)-based framework for difficulty\nprediction. These innovative methods seek to improve the performance of KT\nmodels and provide accurate difficulty estimates for unseen data. Our ablation\nstudy demonstrates the efficacy of these techniques by demonstrating enhanced\nKT model performance. Nonetheless, the complex relationship between language\nand difficulty merits further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2312.11890v1.pdf"
    },
    {
        "title": "Efficient LLM inference solution on Intel GPU",
        "authors": [
            "Hui Wu",
            "Yi Gan",
            "Feng Yuan",
            "Jing Ma",
            "Wei Zhu",
            "Yutao Xu",
            "Hong Zhu",
            "Yuhua Zhu",
            "Xiaoli Liu",
            "Jinghui Gu"
        ],
        "published": "2023-12-19T05:40:43Z",
        "summary": "Transformer based Large Language Models (LLMs) have been widely used in many\nfields, and the efficiency of LLM inference becomes hot topic in real\napplications. However, LLMs are usually complicatedly designed in model\nstructure with massive operations and perform inference in the auto-regressive\nmode, making it a challenging task to design a system with high efficiency.\n  In this paper, we propose an efficient LLM inference solution with low\nlatency and high throughput. Firstly, we simplify the LLM decoder layer by\nfusing data movement and element-wise operations to reduce the memory access\nfrequency and lower system latency. We also propose a segment KV cache policy\nto keep key/value of the request and response tokens in separate physical\nmemory for effective device memory management, helping enlarge the runtime\nbatch size and improve system throughput. A customized\nScaled-Dot-Product-Attention kernel is designed to match our fusion policy\nbased on the segment KV cache solution. We implement our LLM inference solution\non Intel GPU and publish it publicly. Compared with the standard HuggingFace\nimplementation, the proposed solution achieves up to 7x lower token latency and\n27x higher throughput for some popular LLMs on Intel GPU.",
        "pdf_link": "https://arxiv.org/pdf/2401.05391v1.pdf"
    },
    {
        "title": "Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies",
        "authors": [
            "Anaelia Ovalle",
            "Ninareh Mehrabi",
            "Palash Goyal",
            "Jwala Dhamala",
            "Kai-Wei Chang",
            "Richard Zemel",
            "Aram Galstyan",
            "Yuval Pinter",
            "Rahul Gupta"
        ],
        "published": "2023-12-19T01:28:46Z",
        "summary": "Gender-inclusive NLP research has documented the harmful limitations of\ngender binary-centric large language models (LLM), such as the inability to\ncorrectly use gender-diverse English neopronouns (e.g., xe, zir, fae). While\ndata scarcity is a known culprit, the precise mechanisms through which scarcity\naffects this behavior remain underexplored. We discover LLM misgendering is\nsignificantly influenced by Byte-Pair Encoding (BPE) tokenization, the\ntokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments\nneopronouns, a direct consequence of data scarcity during tokenizer training.\nThis disparate tokenization mirrors tokenizer limitations observed in\nmultilingual and low-resource NLP, unlocking new misgendering mitigation\nstrategies. We propose two techniques: (1) pronoun tokenization parity, a\nmethod to enforce consistent tokenization across gendered pronouns, and (2)\nutilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency.\nOur proposed methods outperform finetuning with standard BPE, improving\nneopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM\nmisgendering to tokenization and deficient neopronoun grammar, indicating that\nLLMs unable to correctly treat neopronouns as pronouns are more prone to\nmisgender.",
        "pdf_link": "https://arxiv.org/pdf/2312.11779v3.pdf"
    },
    {
        "title": "Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled Spatial Ontologies",
        "authors": [
            "Jared Strader",
            "Nathan Hughes",
            "William Chen",
            "Alberto Speranzon",
            "Luca Carlone"
        ],
        "published": "2023-12-18T21:20:28Z",
        "summary": "This paper proposes an approach to build 3D scene graphs in arbitrary (indoor\nand outdoor) environments. Such extension is challenging; the hierarchy of\nconcepts that describe an outdoor environment is more complex than for indoors,\nand manually defining such hierarchy is time-consuming and does not scale.\nFurthermore, the lack of training data prevents the straightforward application\nof learning-based tools used in indoor settings. To address these challenges,\nwe propose two novel extensions. First, we develop methods to build a spatial\nontology defining concepts and relations relevant for indoor and outdoor robot\noperation. In particular, we use a Large Language Model (LLM) to build such an\nontology, thus largely reducing the amount of manual effort required. Second,\nwe leverage the spatial ontology for 3D scene graph construction using Logic\nTensor Networks (LTN) to add logical rules, or axioms (e.g., \"a beach contains\nsand\"), which provide additional supervisory signals at training time thus\nreducing the need for labelled data, providing better predictions, and even\nallowing predicting concepts unseen at training time. We test our approach in a\nvariety of datasets, including indoor, rural, and coastal environments, and\nshow that it leads to a significant increase in the quality of the 3D scene\ngraph generation with sparsely annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2312.11713v1.pdf"
    },
    {
        "title": "Opportunities and Challenges of Applying Large Language Models in Building Energy Efficiency and Decarbonization Studies: An Exploratory Overview",
        "authors": [
            "Liang Zhang",
            "Zhelun Chen"
        ],
        "published": "2023-12-18T20:58:58Z",
        "summary": "In recent years, the rapid advancement and impressive capabilities of Large\nLanguage Models (LLMs) have been evident across various domains. This paper\nexplores the application, implications, and potential of LLMs in building\nenergy efficiency and decarbonization studies. The wide-ranging capabilities of\nLLMs are examined in the context of the building energy field, including\nintelligent control systems, code generation, data infrastructure, knowledge\nextraction, and education. Despite the promising potential of LLMs, challenges\nincluding complex and expensive computation, data privacy, security and\ncopyright, complexity in fine-tuned LLMs, and self-consistency are discussed.\nThe paper concludes with a call for future research focused on the enhancement\nof LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research\nbetween AI and energy experts.",
        "pdf_link": "https://arxiv.org/pdf/2312.11701v1.pdf"
    },
    {
        "title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
        "authors": [
            "Megan Kinniment",
            "Lucas Jun Koba Sato",
            "Haoxing Du",
            "Brian Goodrich",
            "Max Hasin",
            "Lawrence Chan",
            "Luke Harold Miles",
            "Tao R. Lin",
            "Hjalmar Wijk",
            "Joel Burget",
            "Aaron Ho",
            "Elizabeth Barnes",
            "Paul Christiano"
        ],
        "published": "2023-12-18T19:27:09Z",
        "summary": "In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.",
        "pdf_link": "https://arxiv.org/pdf/2312.11671v2.pdf"
    },
    {
        "title": "Traces of Memorisation in Large Language Models for Code",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi",
            "Arie van Deursen"
        ],
        "published": "2023-12-18T19:12:58Z",
        "summary": "Large language models have gained significant popularity because of their\nability to generate human-like text and potential applications in various\nfields, such as Software Engineering. Large language models for code are\ncommonly trained on large unsanitised corpora of source code scraped from the\ninternet. The content of these datasets is memorised and can be extracted by\nattackers with data extraction attacks. In this work, we explore memorisation\nin large language models for code and compare the rate of memorisation with\nlarge language models trained on natural language. We adopt an existing\nbenchmark for natural language and construct a benchmark for code by\nidentifying samples that are vulnerable to attack. We run both benchmarks\nagainst a variety of models, and perform a data extraction attack. We find that\nlarge language models for code are vulnerable to data extraction attacks, like\ntheir natural language counterparts. From the training data that was identified\nto be potentially extractable we were able to extract 47% from a\nCodeGen-Mono-16B code completion model. We also observe that models memorise\nmore, as their parameter count grows, and that their pre-training data are also\nvulnerable to attack. We also find that data carriers are memorised at a higher\nrate than regular code or documentation and that different model architectures\nmemorise different samples. Data leakage has severe outcomes, so we urge the\nresearch community to further investigate the extent of this phenomenon using a\nwider range of models and extraction techniques in order to build safeguards to\nmitigate this issue.",
        "pdf_link": "https://arxiv.org/pdf/2312.11658v2.pdf"
    },
    {
        "title": "Language-Assisted 3D Scene Understanding",
        "authors": [
            "Yanmin Wu",
            "Qiankun Gao",
            "Renrui Zhang",
            "Jian Zhang"
        ],
        "published": "2023-12-18T18:54:56Z",
        "summary": "The scale and quality of point cloud datasets constrain the advancement of\npoint cloud learning. Recently, with the development of multi-modal learning,\nthe incorporation of domain-agnostic prior knowledge from other modalities,\nsuch as images and text, to assist in point cloud feature learning has been\nconsidered a promising avenue. Existing methods have demonstrated the\neffectiveness of multi-modal contrastive training and feature distillation on\npoint clouds. However, challenges remain, including the requirement for paired\ntriplet data, redundancy and ambiguity in supervised features, and the\ndisruption of the original priors. In this paper, we propose a\nlanguage-assisted approach to point cloud feature learning (LAST-PCL),\nenriching semantic concepts through LLMs-based text enrichment. We achieve\nde-redundancy and feature dimensionality reduction without compromising textual\npriors by statistical-based and training-free significant feature selection.\nFurthermore, we also delve into an in-depth analysis of the impact of text\ncontrastive training on the point cloud. Extensive experiments validate that\nthe proposed method learns semantically meaningful point cloud features and\nachieves state-of-the-art or comparable performance in 3D semantic\nsegmentation, 3D object detection, and 3D scene classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.11451v2.pdf"
    },
    {
        "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
        "authors": [
            "Jiahui Gao",
            "Renjie Pi",
            "Jipeng Zhang",
            "Jiacheng Ye",
            "Wanjun Zhong",
            "Yufei Wang",
            "Lanqing Hong",
            "Jianhua Han",
            "Hang Xu",
            "Zhenguo Li",
            "Lingpeng Kong"
        ],
        "published": "2023-12-18T17:36:20Z",
        "summary": "Large language models (LLMs) have shown remarkable proficiency in human-level\nreasoning and generation capabilities, which encourages extensive research on\ntheir application in mathematical problem solving. However, current work has\nbeen largely focused on text-based mathematical problems, with limited\ninvestigation in problems involving geometric information. Addressing this gap,\nwe aim to enable LLMs to solve geometric problems by understanding image input.\nWe first analyze the limitations of current Multimodal Large Language Models\n(MLLMs) in this area: they struggle to accurately comprehending basic geometric\nelements and their relationships. To overcome these challenges, we take\nadvantage of the unique characteristics of geometric problems (such as unique\ngeometric logical form, and geometric scalability) and the capacity of the\ntextual LLMs to build an enriched multimodal geometry dataset based on existing\ndata. The augmented dataset, Geo170K, contains more than 170K geometric\nimage-caption and question-answer pairs. Utilizing our constructed Geo170K\ndataset, we develop G-LLaVA, which demonstrates exceptional performance in\nsolving geometric problems, significantly outperforming GPT-4-V on the\nMathVista benchmark with only 7B parameters.",
        "pdf_link": "https://arxiv.org/pdf/2312.11370v1.pdf"
    },
    {
        "title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation",
        "authors": [
            "Nandan Thakur",
            "Luiz Bonifacio",
            "Xinyu Zhang",
            "Odunayo Ogundepo",
            "Ehsan Kamalloo",
            "David Alfonso-Hermelo",
            "Xiaoguang Li",
            "Qun Liu",
            "Boxing Chen",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023-12-18T17:18:04Z",
        "summary": "Retrieval-augmented generation (RAG) grounds large language model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior works lack a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages judged as\nnon-relevant, whereas queries in the relevant subset include at least a single\njudged relevant passage. We measure LLM robustness using two metrics: (i)\nhallucination rate, measuring model tendency to hallucinate an answer, when the\nanswer is not present in passages in the non-relevant subset, and (ii) error\nrate, measuring model inaccuracy to recognize relevant passages in the relevant\nsubset. In our work, we measure robustness for a wide variety of\nmultilingual-focused LLMs and observe that most of the models struggle to\nbalance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observe\nmore than an 88% hallucination rate on the non-relevant subset, whereas,\nMistral overall hallucinates less, but can achieve up to a 74.9% error rate on\nthe relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on\nboth subsets, highlighting future work necessary to improve LLM robustness.",
        "pdf_link": "https://arxiv.org/pdf/2312.11361v2.pdf"
    },
    {
        "title": "Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs",
        "authors": [
            "Yuxuan Huang",
            "Lida Shi",
            "Anqi Liu",
            "Hao Xu"
        ],
        "published": "2023-12-18T15:23:06Z",
        "summary": "The development of large language models (LLMs) has been catalyzed by\nadvancements in pre-training techniques. These models have demonstrated robust\nreasoning capabilities through manually designed prompts. In this work, we\nevaluate the conversational reasoning capabilities of the current\nstate-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the\nperformance of LLMs is constrained due to a lack of KG environment awareness\nand the difficulties in developing effective optimization mechanisms for\nintermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG\nreasoning agent designed to deliver precise and adaptable predictions on KG\npaths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate\nstate information within each reasoning step. We reframe the challenge of\nmulti-hop reasoning on the KG as a sequential decision-making task. Utilizing\nthe Proximal Policy Optimization (PPO) online policy gradient reinforcement\nlearning algorithm, our model is optimized to learn from rich reward signals.\nAdditionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG\ndataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the\ncurrent state-of-the-art model by 5.28 percentage points, with a performance\nrate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored\n14.91%, further demonstrating the effectiveness of our method. Our code is\navailable on GitHub (https://github.com/Aipura/LLM-ARK) for further access.",
        "pdf_link": "https://arxiv.org/pdf/2312.11282v2.pdf"
    },
    {
        "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
        "authors": [
            "Bing Wang",
            "Changyu Ren",
            "Jian Yang",
            "Xinnian Liang",
            "Jiaqi Bai",
            "Linzheng Chai",
            "Zhao Yan",
            "Qian-Wen Zhang",
            "Di Yin",
            "Xing Sun",
            "Zhoujun Li"
        ],
        "published": "2023-12-18T14:40:20Z",
        "summary": "Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on ``huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https://github.com/wbbeyourself/MAC-SQL).",
        "pdf_link": "https://arxiv.org/pdf/2312.11242v3.pdf"
    },
    {
        "title": "Linear Attention via Orthogonal Memory",
        "authors": [
            "Jun Zhang",
            "Shuyang Jiang",
            "Jiangtao Feng",
            "Lin Zheng",
            "Lingpeng Kong"
        ],
        "published": "2023-12-18T12:26:27Z",
        "summary": "Efficient attentions have greatly improved the computational efficiency of\nTransformers. However, most existing linear attention mechanisms suffer from an\n\\emph{efficiency degradation} problem, leading to inefficiencies in causal\nlanguage modeling and hindering their application in long-range language\nmodels. This problem is more pronounced under language modeling with unbounded\ncontexts. In this paper, we propose \\textbf{L}inear \\textbf{A}ttention\n\\textbf{V}ia \\textbf{O}rthogonal memory~(\\shortname) to address these\nlimitations, achieving strong performance while maintaining linear complexity.\n\\shortname employs orthogonal decomposition to compress a context into a\nfixed-size orthogonal memory while effectively minimizing redundancy within the\ncontext. Given that orthogonal memory compresses global information, we further\ndissect the context to amplify fine-grained local information. Additionally, we\nembed the relative position encoding into \\shortname to improve the\nextrapolation ability. Experimental results show that \\shortname greatly\nimproves the efficiency of the causal language model with the best\nextrapolation performance and outperforms other efficient baselines. Further,\nwe endeavor to employ \\shortname for unbounded language modeling and\nsuccessfully scale the context length to 128K.",
        "pdf_link": "https://arxiv.org/pdf/2312.11135v1.pdf"
    },
    {
        "title": "Split and Rephrase with Large Language Models",
        "authors": [
            "David Ponce",
            "Thierry Etchegoyhen",
            "Jes√∫s Calleja P√©rez",
            "Harritxu Gete"
        ],
        "published": "2023-12-18T10:16:37Z",
        "summary": "The Split and Rephrase (SPRP) task, which consists in splitting complex\nsentences into a sequence of shorter grammatical sentences, while preserving\nthe original meaning, can facilitate the processing of complex texts for humans\nand machines alike. It is also a valuable testbed to evaluate natural language\nprocessing models, as it requires modelling complex grammatical aspects. In\nthis work, we evaluate large language models on the task, showing that they can\nprovide large improvements over the state of the art on the main metrics,\nalthough still lagging in terms of splitting compliance. Results from two human\nevaluations further support the conclusions drawn from automated metric\nresults. We provide a comprehensive study that includes prompting variants,\ndomain shift, fine-tuned pretrained language models of varying parameter size\nand training data volumes, contrasted with both zero-shot and few-shot\napproaches on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they may constitute a reasonable\noff-the-shelf alternative. Our results provide a fine-grained analysis of the\npotential and limitations of large language models for SPRP, with significant\nimprovements achievable using relatively small amounts of training data and\nmodel parameters overall, and remaining limitations for all models on the task.",
        "pdf_link": "https://arxiv.org/pdf/2312.11075v3.pdf"
    },
    {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published": "2023-12-18T07:47:33Z",
        "summary": "Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.",
        "pdf_link": "https://arxiv.org/pdf/2312.10997v5.pdf"
    },
    {
        "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update",
        "authors": [
            "Zhi Gao",
            "Yuntao Du",
            "Xintong Zhang",
            "Xiaojian Ma",
            "Wenjuan Han",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "published": "2023-12-18T03:34:07Z",
        "summary": "Utilizing large language models (LLMs) to compose off-the-shelf visual tools\nrepresents a promising avenue of research for developing robust visual\nassistants capable of addressing diverse visual tasks. However, these methods\noften overlook the potential for continual learning, typically by freezing the\nutilized tools, thus limiting their adaptation to environments requiring new\nknowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual\nAssistant, which operates within a framework encompassing inference,\nreflection, and learning phases. During the inference phase, LLMs generate\nprograms and execute corresponding tools to complete assigned tasks. In the\nreflection phase, a multimodal global-local reflection scheme analyzes human\nfeedback to determine which tools require updating. Lastly, the learning phase\nemploys three flexible approaches to automatically gather training data and\nintroduces a novel prompt tuning scheme to update the tools, allowing CLOVA to\nefficiently acquire new knowledge. Experimental findings demonstrate that CLOVA\nsurpasses existing tool-usage methods by 5% in visual question answering and\nmultiple-image reasoning, by 10% in knowledge tagging, and by 20% in image\nediting. These results underscore the significance of the continual learning\ncapability in general visual assistants.",
        "pdf_link": "https://arxiv.org/pdf/2312.10908v3.pdf"
    },
    {
        "title": "Students' Perceptions and Preferences of Generative Artificial Intelligence Feedback for Programming",
        "authors": [
            "Zhengdong Zhang",
            "Zihan Dong",
            "Yang Shi",
            "Noboru Matsuda",
            "Thomas Price",
            "Dongkuan Xu"
        ],
        "published": "2023-12-17T22:26:53Z",
        "summary": "The rapid evolution of artificial intelligence (AI), specifically large\nlanguage models (LLMs), has opened opportunities for various educational\napplications. This paper explored the feasibility of utilizing ChatGPT, one of\nthe most popular LLMs, for automating feedback for Java programming assignments\nin an introductory computer science (CS1) class. Specifically, this study\nfocused on three questions: 1) To what extent do students view LLM-generated\nfeedback as formative? 2) How do students see the comparative affordances of\nfeedback prompts that include their code, vs. those that exclude it? 3) What\nenhancements do students suggest for improving AI-generated feedback? To\naddress these questions, we generated automated feedback using the ChatGPT API\nfor four lab assignments in the CS1 class. The survey results revealed that\nstudents perceived the feedback as aligning well with formative feedback\nguidelines established by Shute. Additionally, students showed a clear\npreference for feedback generated by including the students' code as part of\nthe LLM prompt, and our thematic study indicated that the preference was mainly\nattributed to the specificity, clarity, and corrective nature of the feedback.\nMoreover, this study found that students generally expected specific and\ncorrective feedback with sufficient code examples, but had diverged opinions on\nthe tone of the feedback. This study demonstrated that ChatGPT could generate\nJava programming assignment feedback that students perceived as formative. It\nalso offered insights into the specific improvements that would make the\nChatGPT-generated feedback useful for students.",
        "pdf_link": "https://arxiv.org/pdf/2312.11567v1.pdf"
    },
    {
        "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
        "authors": [
            "Chenglin Li",
            "Qianglong Chen",
            "Liangyue Li",
            "Caiyu Wang",
            "Yicheng Li",
            "Zulong Chen",
            "Yin Zhang"
        ],
        "published": "2023-12-17T14:28:28Z",
        "summary": "While large language models (LLMs) have demonstrated exceptional performance\nin recent natural language processing (NLP) tasks, their deployment poses\nsubstantial challenges due to high computational and memory demands in\nreal-world applications. Recent studies have focused on enhancing smaller\nmodels through knowledge distillation from LLMs, yielding promising results.\nHowever, these models often struggle to match the performance of LLMs,\nespecially in tasks that require reasoning. In this work, we introduce Mixed\nDistillation (MD) framework, which capitalizes on the strengths of Program of\nThought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining\nmultiple prompting techniques and distilling these capabilities into smaller\nmodels. Our experimental results show that MD significantly enhances the\nsingle-path and multi-path reasoning ability of smaller models in various\ntasks. In terms of accuracy and generality of reasoning tasks, the model\ngenerated by it exceeds the comprehensive performance of two individually\ndistilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved\nremarkable improvements of (84.5%) and (85.5%), respectively, outperforming\nGPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2312.10730v2.pdf"
    },
    {
        "title": "Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression",
        "authors": [
            "Luis Balderas",
            "Miguel Lastra",
            "Jos√© M. Ben√≠tez"
        ],
        "published": "2023-12-17T12:33:50Z",
        "summary": "Large Language Models (LLMs) like BERT have gained significant prominence due\nto their remarkable performance in various natural language processing tasks.\nHowever, they come with substantial computational and memory costs.\nAdditionally, they are essentially black-box models, challenging to explain and\ninterpret. In this article, we propose Optimus BERT Compression and\nExplainability (OBCE), a methodology to bring explainability to BERT models\nusing persistent homology, aiming to measure the importance of each neuron by\nstudying the topological characteristics of their outputs. As a result, we can\ncompress BERT significantly by reducing the number of parameters (58.47% of the\noriginal parameters for BERT Base, 52.3% for BERT Large). We evaluated our\nmethodology on the standard GLUE Benchmark, comparing the results with\nstate-of-the-art techniques and achieving outstanding results. Consequently,\nour methodology can \"whiten\" BERT models by providing explainability to its\nneurons and reducing the model's size, making it more suitable for deployment\non resource-constrained devices.",
        "pdf_link": "https://arxiv.org/pdf/2312.10702v1.pdf"
    },
    {
        "title": "An Evaluation of GPT-4V and Gemini in Online VQA",
        "authors": [
            "Mengchen Liu",
            "Chongyan Chen",
            "Danna Gurari"
        ],
        "published": "2023-12-17T07:38:43Z",
        "summary": "While there is much excitement about the potential of large multimodal models\n(LMM), a comprehensive evaluation is critical to establish their true\ncapabilities and limitations. In support of this aim, we evaluate two\nstate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answering\ndataset sourced from an authentic online question answering community. We\nconduct fine-grained analysis by generating seven types of metadata for nearly\n2,000 visual questions, such as image type and the required image processing\ncapabilities. Our zero-shot performance analysis highlights the types of\nquestions that are most challenging for both models, including questions\nrelated to \"puzzling\" topic, with \"Identification\" user intention, with \"Sheet\nMusic\" image type, or labeled as \"hard\" by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.10637v2.pdf"
    },
    {
        "title": "LLM-Twin: Mini-Giant Model-driven Beyond 5G Digital Twin Networking Framework with Semantic Secure Communication and Computation",
        "authors": [
            "Yang Hong",
            "Jun Wu",
            "Rosario Morello"
        ],
        "published": "2023-12-17T07:13:59Z",
        "summary": "Beyond 5G networks provide solutions for next-generation communications,\nespecially digital twins networks (DTNs) have gained increasing popularity for\nbridging physical space and digital space. However, current DTNs networking\nframeworks pose a number of challenges especially when applied in scenarios\nthat require high communication efficiency and multimodal data processing.\nFirst, current DTNs frameworks are unavoidable regarding high resource\nconsumption and communication congestion because of original bit-level\ncommunication and high-frequency computation, especially distributed\nlearning-based DTNs. Second, current machine learning models for DTNs are\ndomain-specific (e.g. E-health), making it difficult to handle DT scenarios\nwith multimodal data processing requirements. Last but not least, current\nsecurity schemes for DTNs, such as blockchain, introduce additional overheads\nthat impair the efficiency of DTNs. To address the above challenges, we propose\na large language model (LLM) empowered DTNs networking framework, LLM-Twin.\nFirst, we design the mini-giant model collaboration scheme to achieve efficient\ndeployment of LLM in DTNs, since LLM are naturally conducive to processing\nmultimodal data. Then, we design a semantic-level high-efficiency, and secure\ncommunication model for DTNs. The feasibility of LLM-Twin is demonstrated by\nnumerical experiments and case studies. To our knowledge, this is the first to\npropose LLM-based semantic-level digital twin networking framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.10631v1.pdf"
    },
    {
        "title": "TrojFSP: Trojan Insertion in Few-shot Prompt Tuning",
        "authors": [
            "Mengxin Zheng",
            "Jiaqi Xue",
            "Xun Chen",
            "YanShan Wang",
            "Qian Lou",
            "Lei Jiang"
        ],
        "published": "2023-12-16T14:49:36Z",
        "summary": "Prompt tuning is one of the most effective solutions to adapting a fixed\npre-trained language model (PLM) for various downstream tasks, especially with\nonly a few input samples. However, the security issues, e.g., Trojan attacks,\nof prompt tuning on a few data samples are not well-studied. Transferring\nestablished data poisoning attacks directly to few-shot prompt tuning presents\nmultiple challenges. One significant issue is the \\textit{poisoned imbalance\nissue}, where non-target class samples are added to the target class, resulting\nin a greater number of target-class samples compared to non-target class. While\nthis issue is not critical in regular tuning, it significantly hampers the\nfew-shot prompt tuning, making it difficult to simultaneously achieve a high\nattack success rate (ASR) and maintain clean data accuracy (CDA). Additionally,\nfew-shot prompting is prone to overfitting in terms of both ASR and CDA. In\nthis paper, we introduce \\textit{TrojFSP}, a method designed to address the\nchallenges. To solve the poisoned imbalance issue, we develop a\n\\textit{Target-Class Shrink (TC-Shrink)} technique, which aims to equalize the\nnumber of poisoning samples. To combat overfitting, we employ a\n\\textit{Selective Token Poisoning} technique to boost attack performance.\nFurthermore, we introduce a \\textit{Trojan-Trigger Attention} objective\nfunction to amplify the attention of the poisoned trojan prompt on triggers.\nExperiments show that our TrojFSP achieves an ASR of over 99\\% while\nmaintaining negligible decreases in CDA across various PLMs and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.10467v3.pdf"
    },
    {
        "title": "Resolving Crash Bugs via Large Language Models: An Empirical Study",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Juntao Li",
            "Hanlin Wang",
            "Xin Peng",
            "Yiling Lou"
        ],
        "published": "2023-12-16T13:41:04Z",
        "summary": "Crash bugs cause unexpected program behaviors or even termination, requiring\nhigh-priority resolution. However, manually resolving crash bugs is challenging\nand labor-intensive, and researchers have proposed various techniques for their\nautomated localization and repair. ChatGPT, a recent large language model\n(LLM), has garnered significant attention due to its exceptional performance\nacross various domains. This work performs the first investigation into\nChatGPT's capability in resolve real-world crash bugs, focusing on its\neffectiveness in both localizing and repairing code-related and\nenvironment-related crash bugs. Specifically, we initially assess ChatGPT's\nfundamental ability to resolve crash bugs with basic prompts in a single\niteration. We observe that ChatGPT performs better at resolving code-related\ncrash bugs compared to environment-related ones, and its primary challenge in\nresolution lies in inaccurate localization. Additionally, we explore ChatGPT's\npotential with various advanced prompts. Furthermore, by stimulating ChatGPT's\nself-planning, it methodically investigates each potential crash-causing\nenvironmental factor through proactive inquiry, ultimately identifying the root\ncause of the crash. Based on our findings, we propose IntDiagSolver, an\ninteraction methodology designed to facilitate precise crash bug resolution\nthrough continuous interaction with LLMs. Evaluating IntDiagSolver on multiple\nLLMs reveals consistent enhancement in the accuracy of crash bug resolution,\nincluding ChatGPT, Claude, and CodeLlama.",
        "pdf_link": "https://arxiv.org/pdf/2312.10448v1.pdf"
    },
    {
        "title": "DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content",
        "authors": [
            "Wentao Wang",
            "Xuanyao Huang",
            "Tianyang Wang",
            "Swalpa Kumar Roy"
        ],
        "published": "2023-12-16T10:17:09Z",
        "summary": "This paper explores the image synthesis capabilities of GPT-4, a leading\nmulti-modal large language model. We establish a benchmark for evaluating the\nfidelity of texture features in images generated by GPT-4, comprising manually\npainted pictures and their AI-generated counterparts. The contributions of this\nstudy are threefold: First, we provide an in-depth analysis of the fidelity of\nimage synthesis features based on GPT-4, marking the first such study on this\nstate-of-the-art model. Second, the quantitative and qualitative experiments\nfully reveals the limitations of the GPT-4 model in image synthesis. Third, we\nhave compiled a unique benchmark of manual drawings and corresponding\nGPT-4-generated images, introducing a new task to advance fidelity research in\nAI-generated content (AIGC). The dataset is available at:\n\\url{https://github.com/rickwang28574/DeepArt}.",
        "pdf_link": "https://arxiv.org/pdf/2312.10407v2.pdf"
    },
    {
        "title": "When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning",
        "authors": [
            "Qihang Ai",
            "Jianwu Zhou",
            "Haiyun Jiang",
            "Lemao Liu",
            "Shuming Shi"
        ],
        "published": "2023-12-16T08:14:11Z",
        "summary": "Graph data is ubiquitous in the physical world, and it has always been a\nchallenge to efficiently model graph structures using a unified paradigm for\nthe understanding and reasoning on various graphs. Moreover, in the era of\nlarge language models, integrating complex graph information into text\nsequences has become exceptionally difficult, which hinders the ability to\ninteract with graph data through natural language instructions.The paper\npresents a new paradigm for understanding and reasoning about graph data by\nintegrating image encoding and multimodal technologies. This approach enables\nthe comprehension of graph data through an instruction-response format,\nutilizing GPT-4V's advanced capabilities. The study evaluates this paradigm on\nvarious graph types, highlighting the model's strengths and weaknesses,\nparticularly in Chinese OCR performance and complex reasoning tasks. The\nfindings suggest new direction for enhancing graph data processing and natural\nlanguage interaction.",
        "pdf_link": "https://arxiv.org/pdf/2312.10372v1.pdf"
    },
    {
        "title": "LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?",
        "authors": [
            "Fuheng Zhao",
            "Lawrence Lim",
            "Ishtiyaque Ahmad",
            "Divyakant Agrawal",
            "Amr El Abbadi"
        ],
        "published": "2023-12-16T05:01:23Z",
        "summary": "Judging the equivalence between two SQL queries is a fundamental problem with\nmany practical applications in data management and SQL generation (i.e.,\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\nresearch community has reasoned about SQL equivalence for decades, it poses\nconsiderable difficulties and no complete solutions exist. Recently, Large\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\nquestion answering and solving mathematics challenges. In this paper, we study\nif LLMs can be used to determine the equivalence between SQL queries under two\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\nassist LLMs in generating high quality responses, we present two prompting\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\nquery on a simple database instance and then explore if a counterexample exists\nby modifying the database. The latter technique is used to evaluate the relaxed\nequivalence in which it asks LLMs to explain the queries and then compare if\nthey contain significant logical differences. Our experiments demonstrate using\nour techniques, LLMs is a promising tool to help data engineers in writing\nsemantically equivalent SQL queries, however challenges still persist, and is a\nbetter metric for evaluating SQL generation than the popular execution\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2312.10321v2.pdf"
    },
    {
        "title": "KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know",
        "authors": [
            "Shangshang Zheng",
            "He Bai",
            "Yizhe Zhang",
            "Yi Su",
            "Xiaochuan Niu",
            "Navdeep Jaitly"
        ],
        "published": "2023-12-15T23:34:05Z",
        "summary": "Measuring the alignment between a Knowledge Graph (KG) and Large Language\nModels (LLMs) is an effective method to assess the factualness and identify the\nknowledge blind spots of LLMs. However, this approach encounters two primary\nchallenges including the translation of KGs into natural language and the\nefficient evaluation of these extensive and complex structures. In this paper,\nwe present KGLens--a novel framework aimed at measuring the alignment between\nKGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs.\nKGLens features a graph-guided question generator for converting KGs into\nnatural language, along with a carefully designed sampling strategy based on\nparameterized KG structure to expedite KG traversal. We conducted experiments\nusing three domain-specific KGs from Wikidata, which comprise over 19,000\nedges, 700 relations, and 21,000 entities. Our analysis across eight LLMs\nreveals that KGLens not only evaluates the factual accuracy of LLMs more\nrapidly but also delivers in-depth analyses on topics, temporal dynamics, and\nrelationships. Furthermore, human evaluation results indicate that KGLens can\nassess LLMs with a level of accuracy nearly equivalent to that of human\nannotators, achieving 95.7% of the accuracy rate.",
        "pdf_link": "https://arxiv.org/pdf/2312.11539v2.pdf"
    },
    {
        "title": "Student as an Inherent Denoiser of Noisy Teacher",
        "authors": [
            "Jiachen Zhao"
        ],
        "published": "2023-12-15T20:21:45Z",
        "summary": "Knowledge distillation (KD) has been widely employed to transfer knowledge\nfrom a large language model (LLM) to a specialized model in low-data regimes\nthrough pseudo label learning. However, pseudo labels generated by teacher\nmodels are usually noisy and may influence KD performance. This study delves\ninto KD with noisy teachers and uncovers that the student model can already\ngenerate more accurate predictions than the teacher labels used to train it\nduring KD, indicating its inherent ability to denoise noisy teacher labels.\nMotivated by this finding, we propose Peer-Advised KD to improve vanilla KD\nfrom noisy teachers. Experiments show that Peer-Advised KD can outperform LLM\nby approximately 5% with 50 human-labeled data, and even competitive to\nstandard supervised finetuning with 750 human-labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2312.10185v1.pdf"
    },
    {
        "title": "Challenges with unsupervised LLM knowledge discovery",
        "authors": [
            "Sebastian Farquhar",
            "Vikrant Varma",
            "Zachary Kenton",
            "Johannes Gasteiger",
            "Vladimir Mikulik",
            "Rohin Shah"
        ],
        "published": "2023-12-15T18:49:43Z",
        "summary": "We show that existing unsupervised methods on large language model (LLM)\nactivations do not discover knowledge -- instead they seem to discover whatever\nfeature of the activations is most prominent. The idea behind unsupervised\nknowledge elicitation is that knowledge satisfies a consistency structure,\nwhich can be used to discover knowledge. We first prove theoretically that\narbitrary features (not just knowledge) satisfy the consistency structure of a\nparticular leading unsupervised knowledge-elicitation method,\ncontrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a\nseries of experiments showing settings in which unsupervised methods result in\nclassifiers that do not predict knowledge, but instead predict a different\nprominent feature. We conclude that existing unsupervised methods for\ndiscovering latent knowledge are insufficient, and we contribute sanity checks\nto apply to evaluating future knowledge elicitation methods. Conceptually, we\nhypothesise that the identification issues explored here, e.g. distinguishing a\nmodel's knowledge from that of a simulated character's, will persist for future\nunsupervised methods.",
        "pdf_link": "https://arxiv.org/pdf/2312.10029v2.pdf"
    },
    {
        "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
        "authors": [
            "Shihan Dou",
            "Enyu Zhou",
            "Yan Liu",
            "Songyang Gao",
            "Jun Zhao",
            "Wei Shen",
            "Yuhao Zhou",
            "Zhiheng Xi",
            "Xiao Wang",
            "Xiaoran Fan",
            "Shiliang Pu",
            "Jiang Zhu",
            "Rui Zheng",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-12-15T17:45:06Z",
        "summary": "Supervised fine-tuning (SFT) is a crucial step for large language models\n(LLMs), enabling them to align with human instructions and enhance their\ncapabilities in downstream tasks. Increasing instruction data substantially is\na direct solution to align the model with a broader range of downstream tasks\nor notably improve its performance on a specific task. However, we find that\nlarge-scale increases in instruction data can damage the world knowledge\npreviously stored in LLMs. To address this challenge, we propose LoRAMoE, a\nnovelty framework that introduces several low-rank adapters (LoRA) and\nintegrates them by using a router network, like a plugin version of Mixture of\nExperts (MoE). It freezes the backbone model and forces a portion of LoRAs to\nfocus on leveraging world knowledge to solve downstream tasks, to alleviate\nworld knowledge-edge forgetting. Experimental results show that, as the\ninstruction data increases, LoRAMoE can significantly improve the ability to\nprocess downstream tasks, while maintaining the world knowledge stored in the\nLLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.09979v4.pdf"
    },
    {
        "title": "Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China",
        "authors": [
            "Di Zhou",
            "Yinxian Zhang"
        ],
        "published": "2023-12-15T16:25:56Z",
        "summary": "The rising popularity of ChatGPT and other AI-powered large language models\n(LLMs) has led to increasing studies highlighting their susceptibility to\nmistakes and biases. However, most of these studies focus on models trained on\nEnglish texts. Taking an innovative approach, this study investigates political\nbiases in GPT's multilingual models. We posed the same question about\nhigh-profile political issues in the United States and China to GPT in both\nEnglish and simplified Chinese, and our analysis of the bilingual responses\nrevealed that GPT's bilingual models' political \"knowledge\" (content) and the\npolitical \"attitude\" (sentiment) are significantly more inconsistent on\npolitical issues in China. The simplified Chinese GPT models not only tended to\nprovide pro-China information but also presented the least negative sentiment\ntowards China's problems, whereas the English GPT was significantly more\nnegative towards China. This disparity may stem from Chinese state censorship\nand US-China geopolitical tensions, which influence the training corpora of GPT\nbilingual models. Moreover, both Chinese and English models tended to be less\ncritical towards the issues of \"their own\" represented by the language used,\nthan the issues of \"the other.\" This suggests that GPT multilingual models\ncould potentially develop a \"political identity\" and an associated sentiment\nbias based on their training language. We discussed the implications of our\nfindings for information transmission and communication in an increasingly\ndivided world.",
        "pdf_link": "https://arxiv.org/pdf/2312.09917v1.pdf"
    },
    {
        "title": "ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)",
        "authors": [
            "Tosin Adewumi",
            "Lama Alkhaled",
            "Claudia Buck",
            "Sergio Hernandez",
            "Saga Brilioth",
            "Mkpe Kekung",
            "Yelvin Ragimov",
            "Elisa Barney"
        ],
        "published": "2023-12-15T14:01:46Z",
        "summary": "We introduce a novel writing method called Probing Chain of Thought (ProCoT),\nwhich prevents students from cheating using a Large Language Model (LLM), such\nas ChatGPT, while enhancing their active learning through such models. LLMs\nhave disrupted education and many other feilds. For fear of students cheating,\nmany educationists have resorted to banning their use, as their outputs can be\nhuman-like and hard to detect in some cases. These LLMs are also known for\nhallucinations (i.e. fake facts). We conduct studies with ProCoT in two\ndifferent courses with a combined total of about 66 students. The students in\neach course were asked to prompt an LLM of their choice with one question from\na set of four and required to affirm or refute statements in the LLM output by\nusing peer reviewed references. The results show two things: (1) ProCoT\nstimulates creative/critical thinking and writing of students through\nengagement with LLMs when we compare the LLM solely output to ProCoT output and\n(2) ProCoT can prevent cheating because of clear limitations in existing LLMs\nwhen we compare students ProCoT output to LLM ProCoT output. We also discover\nthat most students prefer to give answers in fewer words than LLMs, which are\ntypically verbose. The average word counts for students, ChatGPT (v3.5) and\nPhind (v8) are 208, 391 and 383, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2312.09801v1.pdf"
    },
    {
        "title": "Taxonomy-based CheckList for Large Language Model Evaluation",
        "authors": [
            "Damin Zhang"
        ],
        "published": "2023-12-15T12:58:07Z",
        "summary": "As large language models (LLMs) have been used in many downstream tasks, the\ninternal stereotypical representation may affect the fairness of the outputs.\nIn this work, we introduce human knowledge into natural language interventions\nand study pre-trained language models' (LMs) behaviors within the context of\ngender bias. Inspired by CheckList behavioral testing, we present a\nchecklist-style task that aims to probe and quantify LMs' unethical behaviors\nthrough question-answering (QA). We design three comparison studies to evaluate\nLMs from four aspects: consistency, biased tendency, model preference, and\ngender preference switch. We probe one transformer-based QA model trained on\nSQuAD-v2 dataset and one autoregressive large language model. Our results\nindicate that transformer-based QA model's biased tendency positively\ncorrelates with its consistency, whereas LLM shows the opposite relation. Our\nproposed task provides the first dataset that involves human knowledge for LLM\nbias evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.10899v1.pdf"
    },
    {
        "title": "Prompting Large Language Models for Topic Modeling",
        "authors": [
            "Han Wang",
            "Nirmalendu Prakash",
            "Nguyen Khoi Hoang",
            "Ming Shan Hee",
            "Usman Naseem",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-12-15T11:15:05Z",
        "summary": "Topic modeling is a widely used technique for revealing underlying thematic\nstructures within textual data. However, existing models have certain\nlimitations, particularly when dealing with short text datasets that lack\nco-occurring words. Moreover, these models often neglect sentence-level\nsemantics, focusing primarily on token-level semantics. In this paper, we\npropose PromptTopic, a novel topic modeling approach that harnesses the\nadvanced language understanding of large language models (LLMs) to address\nthese challenges. It involves extracting topics at the sentence level from\nindividual documents, then aggregating and condensing these topics into a\npredefined quantity, ultimately providing coherent topics for texts of varying\nlengths. This approach eliminates the need for manual parameter tuning and\nimproves the quality of extracted topics. We benchmark PromptTopic against the\nstate-of-the-art baselines on three vastly diverse datasets, establishing its\nproficiency in discovering meaningful topics. Furthermore, qualitative analysis\nshowcases PromptTopic's ability to uncover relevant topics in multiple\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.09693v1.pdf"
    },
    {
        "title": "Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models",
        "authors": [
            "Xin Jin",
            "Jonathan Larson",
            "Weiwei Yang",
            "Zhiqiang Lin"
        ],
        "published": "2023-12-15T08:32:28Z",
        "summary": "Binary code summarization, while invaluable for understanding code semantics,\nis challenging due to its labor-intensive nature. This study delves into the\npotential of large language models (LLMs) for binary code comprehension. To\nthis end, we present BinSum, a comprehensive benchmark and dataset of over 557K\nbinary functions and introduce a novel method for prompt synthesis and\noptimization. To more accurately gauge LLM performance, we also propose a new\nsemantic similarity metric that surpasses traditional exact-match approaches.\nOur extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,\nand Code Llama, reveals 10 pivotal insights. This evaluation generates 4\nbillion inference tokens, incurred a total expense of 11,418 US dollars and 873\nNVIDIA A100 GPU hours. Our findings highlight both the transformative potential\nof LLMs in this field and the challenges yet to be overcome.",
        "pdf_link": "https://arxiv.org/pdf/2312.09601v1.pdf"
    },
    {
        "title": "Extending Context Window of Large Language Models via Semantic Compression",
        "authors": [
            "Weizhi Fei",
            "Xueyan Niu",
            "Pingyi Zhou",
            "Lu Hou",
            "Bo Bai",
            "Lei Deng",
            "Wei Han"
        ],
        "published": "2023-12-15T07:04:33Z",
        "summary": "Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.",
        "pdf_link": "https://arxiv.org/pdf/2312.09571v1.pdf"
    },
    {
        "title": "Privacy-Aware Document Visual Question Answering",
        "authors": [
            "Rub√®n Tito",
            "Khanh Nguyen",
            "Marlon Tobaben",
            "Raouf Kerkouche",
            "Mohamed Ali Souibgui",
            "Kangsoo Jung",
            "Lei Kang",
            "Ernest Valveny",
            "Antti Honkela",
            "Mario Fritz",
            "Dimosthenis Karatzas"
        ],
        "published": "2023-12-15T06:30:55Z",
        "summary": "Document Visual Question Answering (DocVQA) is a fast growing branch of\ndocument understanding. Despite the fact that documents contain sensitive or\ncopyrighted information, none of the current DocVQA methods offers strong\nprivacy guarantees.\n  In this work, we explore privacy in the domain of DocVQA for the first time.\nWe highlight privacy issues in state of the art multi-modal LLM models used for\nDocVQA, and explore possible solutions.\n  Specifically, we focus on the invoice processing use case as a realistic,\nwidely used scenario for document understanding, and propose a large scale\nDocVQA dataset comprising invoice documents and associated questions and\nanswers. We employ a federated learning scheme, that reflects the real-life\ndistribution of documents in different businesses, and we explore the use case\nwhere the ID of the invoice issuer is the sensitive information to be\nprotected.\n  We demonstrate that non-private models tend to memorise, behaviour that can\nlead to exposing private information. We then evaluate baseline training\nschemes employing federated learning and differential privacy in this\nmulti-modal scenario, where the sensitive information might be exposed through\nany of the two input modalities: vision (document image) or language (OCR\ntokens).\n  Finally, we design an attack exploiting the memorisation effect of the model,\nand demonstrate its effectiveness in probing different DocVQA models.",
        "pdf_link": "https://arxiv.org/pdf/2312.10108v1.pdf"
    },
    {
        "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models",
        "authors": [
            "Lei Zhang",
            "Yunshui Li",
            "Ziqiang Liu",
            "Jiaxi yang",
            "Junhao Liu",
            "Min Yang"
        ],
        "published": "2023-12-15T05:30:14Z",
        "summary": "Although there are currently many benchmarks available for evaluating the\nlong context understanding and reasoning capability of large language models,\nwith the expansion of the context window in these models, the existing long\ncontext benchmarks are no longer sufficient for evaluating the long context\nunderstanding and reasoning capability of large language models. In this paper,\nwe have developed a fresh long context evaluation benchmark, which we name it\nMarathon in the form of multiple choice questions, inspired by benchmarks such\nas MMLU, for assessing the long context comprehension capability of large\nlanguage models quickly, accurately, and objectively. We have evaluated several\nof the latest and most popular large language models, as well as three recent\nand effective long context optimization methods, on our benchmark. This\nshowcases the long context reasoning and comprehension capabilities of these\nlarge language models and validates the effectiveness of these optimization\nmethods. Marathon is available at\nhttps://huggingface.co/datasets/Lemoncoke/Marathon.",
        "pdf_link": "https://arxiv.org/pdf/2312.09542v1.pdf"
    },
    {
        "title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models",
        "authors": [
            "Shengyao Zhang",
            "Mi Zhang",
            "Xudong Pan",
            "Min Yang"
        ],
        "published": "2023-12-15T02:42:05Z",
        "summary": "To reduce the computation cost and the energy consumption in large language\nmodels (LLM), skimming-based acceleration dynamically drops unimportant tokens\nof the input sequence progressively along layers of the LLM while preserving\nthe tokens of semantic importance. However, our work for the first time reveals\nthe acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this\npaper, we propose No-Skim, a general framework to help the owners of\nskimming-based LLM to understand and measure the robustness of their\nacceleration scheme. Specifically, our framework searches minimal and\nunnoticeable perturbations at character-level and token-level to generate\nadversarial inputs that sufficiently increase the remaining token ratio, thus\nincreasing the computation cost and energy consumption. We systematically\nevaluate the vulnerability of the skimming acceleration in various LLM\narchitectures including BERT and RoBERTa on the GLUE benchmark. In the worst\ncase, the perturbation found by No-Skim substantially increases the running\ncost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation\nframework to various scenarios, making the evaluation conductible with\ndifferent level of knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2312.09494v2.pdf"
    },
    {
        "title": "CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment",
        "authors": [
            "Ljubisa Bojic",
            "Matteo Cinelli",
            "Dubravko Culibrk",
            "Boris Delibasic"
        ],
        "published": "2023-12-14T23:48:51Z",
        "summary": "This paper explores the potential of a multidisciplinary approach to testing\nand aligning artificial general intelligence (AGI) and LLMs. Due to the rapid\ndevelopment and wide application of LLMs, challenges such as ethical alignment,\ncontrollability, and predictability of these models have become important\nresearch topics. This study investigates an innovative simulation-based\nmulti-agent system within a virtual reality framework that replicates the\nreal-world environment. The framework is populated by automated 'digital\ncitizens,' simulating complex social structures and interactions to examine and\noptimize AGI. Application of various theories from the fields of sociology,\nsocial psychology, computer science, physics, biology, and economics\ndemonstrates the possibility of a more human-aligned and socially responsible\nAGI. The purpose of such a digital environment is to provide a dynamic platform\nwhere advanced AI agents can interact and make independent decisions, thereby\nmimicking realistic scenarios. The actors in this digital city, operated by the\nLLMs, serve as the primary agents, exhibiting high degrees of autonomy. While\nthis approach shows immense potential, there are notable challenges and\nlimitations, most significantly the unpredictable nature of real-world social\ndynamics. This research endeavors to contribute to the development and\nrefinement of AGI, emphasizing the integration of social, ethical, and\ntheoretical dimensions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2312.09402v1.pdf"
    },
    {
        "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft",
        "authors": [
            "Hao Li",
            "Xue Yang",
            "Zhaokai Wang",
            "Xizhou Zhu",
            "Jie Zhou",
            "Yu Qiao",
            "Xiaogang Wang",
            "Hongsheng Li",
            "Lewei Lu",
            "Jifeng Dai"
        ],
        "published": "2023-12-14T18:58:12Z",
        "summary": "Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.",
        "pdf_link": "https://arxiv.org/pdf/2312.09238v2.pdf"
    },
    {
        "title": "Measurement in the Age of LLMs: An Application to Ideological Scaling",
        "authors": [
            "Sean O'Hagan",
            "Aaron Schein"
        ],
        "published": "2023-12-14T18:34:06Z",
        "summary": "Much of social science is centered around terms like ``ideology'' or\n``power'', which generally elude precise definition, and whose contextual\nmeanings are trapped in surrounding language. This paper explores the use of\nlarge language models (LLMs) to flexibly navigate the conceptual clutter\ninherent to social scientific measurement tasks. We rely on LLMs' remarkable\nlinguistic fluency to elicit ideological scales of both legislators and text,\nwhich accord closely to established methods and our own judgement. A key aspect\nof our approach is that we elicit such scores directly, instructing the LLM to\nfurnish numeric scores itself. This approach affords a great deal of\nflexibility, which we showcase through a variety of different case studies. Our\nresults suggest that LLMs can be used to characterize highly subtle and diffuse\nmanifestations of political ideology in text.",
        "pdf_link": "https://arxiv.org/pdf/2312.09203v2.pdf"
    },
    {
        "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
        "authors": [
            "Hao Sun",
            "Hengyi Cai",
            "Bo Wang",
            "Yingyan Hou",
            "Xiaochi Wei",
            "Shuaiqiang Wang",
            "Yan Zhang",
            "Dawei Yin"
        ],
        "published": "2023-12-14T16:10:56Z",
        "summary": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.",
        "pdf_link": "https://arxiv.org/pdf/2312.09075v2.pdf"
    },
    {
        "title": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent",
        "authors": [
            "Haoran Liao",
            "Qinyi Du",
            "Shaohua Hu",
            "Hao He",
            "Yanyan Xu",
            "Jidong Tian",
            "Yaohui Jin"
        ],
        "published": "2023-12-14T13:33:50Z",
        "summary": "Large language models (LLMs) face challenges in solving complex mathematical\nproblems that require comprehensive capacities to parse the statements,\nassociate domain knowledge, perform compound logical reasoning, and integrate\nthe intermediate rationales. Tackling all these problems once could be arduous\nfor LLMs, thus leading to confusion in generation. In this work, we explore the\npotential of enhancing LLMs with agents by meticulous decomposition and\nmodeling of mathematical reasoning process. Specifically, we propose a formal\ndescription of the mathematical solving and extend LLMs with an agent-based\nzero-shot framework named\n$\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We\nfurther provide and implement two MathAgents that define the logical forms and\ninherent relations via a pool of actions in different grains and orientations:\nMathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with\nhumankind. Experiments on miniF2F and MATH have demonstrated the effectiveness\nof PRER and proposed MathAgents, achieving an increase of\n$12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$\n($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and\n$13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against\nGPT-4. Further analytical results provide more insightful perspectives on\nexploiting the behaviors of LLMs as agents.",
        "pdf_link": "https://arxiv.org/pdf/2312.08926v2.pdf"
    },
    {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxuan Zhang",
            "Juanzi Li",
            "Bin Xu",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "published": "2023-12-14T13:20:57Z",
        "summary": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM .",
        "pdf_link": "https://arxiv.org/pdf/2312.08914v2.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
        "authors": [
            "Navreet Kaur",
            "Monojit Choudhury",
            "Danish Pruthi"
        ],
        "published": "2023-12-14T10:35:13Z",
        "summary": "As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2312.08800v1.pdf"
    },
    {
        "title": "Future-proofing geotechnics workflows: accelerating problem-solving with large language models",
        "authors": [
            "Stephen Wu",
            "Yu Otake",
            "Daijiro Mizutani",
            "Chang Liu",
            "Kotaro Asano",
            "Nana Sato",
            "Hidetoshi Baba",
            "Yusuke Fukunaga",
            "Yosuke Higo",
            "Akiyoshi Kamura",
            "Shinnosuke Kodama",
            "Masataka Metoki",
            "Tomoka Nakamura",
            "Yuto Nakazato",
            "Taiga Saito",
            "Akihiro Shioi",
            "Masahiro Takenobu",
            "Keigo Tsukioka",
            "Ryo Yoshikawa"
        ],
        "published": "2023-12-14T05:17:27Z",
        "summary": "The integration of Large Language Models (LLMs) like ChatGPT into the\nworkflows of geotechnical engineering has a high potential to transform how the\ndiscipline approaches problem-solving and decision-making. This paper delves\ninto the innovative application of LLMs in geotechnical engineering, as\nexplored in a hands-on workshop held in Tokyo, Japan. The event brought\ntogether a diverse group of 20 participants, including students, researchers,\nand professionals from academia, industry, and government sectors, to\ninvestigate practical uses of LLMs in addressing specific geotechnical\nchallenges. The workshop facilitated the creation of solutions for four\ndifferent practical geotechnical problems as illustrative examples, culminating\nin the development of an academic paper. The paper discusses the potential of\nLLMs to transform geotechnical engineering practices, highlighting their\nproficiency in handling a range of tasks from basic data analysis to complex,\nmultimodal problem-solving. It also addresses the challenges in implementing\nLLMs, particularly in achieving high precision and accuracy in specialized\ntasks, and underscores the need for expert oversight. The findings demonstrate\nLLMs' effectiveness in enhancing efficiency, data processing, and\ndecision-making in geotechnical engineering, suggesting a paradigm shift\ntowards more integrated, data-driven approaches in this field. This study not\nonly showcases the potential of LLMs in a specific engineering domain, but also\nsets a precedent for their broader application in interdisciplinary research\nand practice, where the synergy of human expertise and artificial intelligence\nredefines the boundaries of problem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2312.12411v1.pdf"
    },
    {
        "title": "ChatSOS: LLM-based knowledge Q&A system for safety engineering",
        "authors": [
            "Haiyang Tang",
            "Zhenyi Liu",
            "Dongping Chen",
            "Qingzhao Chu"
        ],
        "published": "2023-12-14T03:25:23Z",
        "summary": "Recent advancements in large language models (LLMs) have notably propelled\nnatural language processing (NLP) capabilities, demonstrating significant\npotential in safety engineering applications. Despite these advancements, LLMs\nface constraints in processing specialized tasks, attributed to factors such as\ncorpus size, input processing limitations, and privacy concerns. Obtaining\nuseful information from reliable sources in a limited time is crucial for LLM.\nAddressing this, our study introduces an LLM-based Q&A system for safety\nengineering, enhancing the comprehension and response accuracy of the model. We\nemployed prompt engineering to incorporate external knowledge databases, thus\nenriching the LLM with up-to-date and reliable information. The system analyzes\nhistorical incident reports through statistical methods, utilizes vector\nembedding to construct a vector database, and offers an efficient\nsimilarity-based search functionality. Our findings indicate that the\nintegration of external knowledge significantly augments the capabilities of\nLLM for in-depth problem analysis and autonomous task assignment. It\neffectively summarizes accident reports and provides pertinent recommendations.\nThis integration approach not only expands LLM applications in safety\nengineering but also sets a precedent for future developments towards\nautomation and intelligent systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.08629v1.pdf"
    },
    {
        "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
        "authors": [
            "Kaiqiang Song",
            "Xiaoyang Wang",
            "Sangwoo Cho",
            "Xiaoman Pan",
            "Dong Yu"
        ],
        "published": "2023-12-14T02:45:31Z",
        "summary": "This paper introduces a novel approach to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and understanding extensive text\nsequences, a critical aspect in applications requiring deep comprehension and\nsynthesis of large volumes of information. Recognizing the inherent challenges\nin extending the context window for LLMs, primarily built on Transformer\narchitecture, we propose a new model architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic time and memory complexity\nissues associated with full attention in the Transformer by employing grouped\nlocal-global attention layers. Our model, akin to a zebra's alternating\nstripes, balances local and global attention layers, significantly reducing\ncomputational requirements and memory consumption. Comprehensive experiments,\nincluding pretraining from scratch, continuation of long context adaptation\ntraining, and long instruction tuning, are conducted to evaluate the Zebra's\nperformance. The results show that Zebra achieves comparable or superior\nperformance on both short and long sequence benchmarks, while also enhancing\ntraining and inference efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2312.08618v1.pdf"
    },
    {
        "title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach",
        "authors": [
            "Golnaz Shapurian",
            "Michael J Kurtz",
            "Alberto Accomazzi"
        ],
        "published": "2023-12-14T00:50:14Z",
        "summary": "The automatic identification of planetary feature names in astronomy\npublications presents numerous challenges. These features include craters,\ndefined as roughly circular depressions resulting from impact or volcanic\nactivity; dorsas, which are elongate raised structures or wrinkle ridges; and\nlacus, small irregular patches of dark, smooth material on the Moon, referred\nto as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap\nwith places or people's names that they are named after, for example, Syria,\nTempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some\nfeature names have been used in many contexts, for instance, Apollo, which can\nrefer to mission, program, sample, astronaut, seismic, seismometers, core, era,\ndata, collection, instrument, and station, in addition to the crater on the\nMoon. Some feature names can appear in the text as adjectives, like the lunar\ncraters Black, Green, and White. Some feature names in other contexts serve as\ndirections, like craters West and South on the Moon. Additionally, some\nfeatures share identical names across different celestial bodies, requiring\ndisambiguation, such as the Adams crater, which exists on both the Moon and\nMars. We present a multi-step pipeline combining rule-based filtering,\nstatistical relevance analysis, part-of-speech (POS) tagging, named entity\nrecognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)\nmatching, and inference with a locally installed large language model (LLM) to\nreliably identify planetary names despite these challenges. When evaluated on a\ndataset of astronomy papers from the Astrophysics Data System (ADS), this\nmethodology achieves an F1-score over 0.97 in disambiguating planetary feature\nnames.",
        "pdf_link": "https://arxiv.org/pdf/2312.08579v2.pdf"
    },
    {
        "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet",
        "authors": [
            "Alexander Borzunov",
            "Max Ryabinin",
            "Artem Chumachenko",
            "Dmitry Baranchuk",
            "Tim Dettmers",
            "Younes Belkada",
            "Pavel Samygin",
            "Colin Raffel"
        ],
        "published": "2023-12-13T18:52:49Z",
        "summary": "Large language models (LLMs) are useful in many NLP tasks and become more\ncapable with size, with the best open-source models having over 50 billion\nparameters. However, using these 50B+ models requires high-end hardware, making\nthem inaccessible to most researchers. In this work, we investigate methods for\ncost-efficient inference and fine-tuning of LLMs, comparing local and\ndistributed strategies. We observe that a large enough model (50B+) can run\nefficiently even on geodistributed devices in a consumer-grade network. This\ncould allow running LLM efficiently by pooling together idle compute resources\nof multiple research groups and volunteers. We address two open problems: (1)\nhow to perform inference and fine-tuning reliably if any device can disconnect\nabruptly and (2) how to partition LLMs between devices with uneven hardware,\njoining and leaving at will. In order to do that, we develop special\nfault-tolerant inference algorithms and load-balancing protocols that\nautomatically assign devices to maximize the total system throughput. We\nshowcase these algorithms in Petals - a decentralized system that runs Llama 2\n(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for\ninteractive generation. We evaluate the performance of our system in simulated\nconditions and a real-world setup spanning two continents.",
        "pdf_link": "https://arxiv.org/pdf/2312.08361v1.pdf"
    },
    {
        "title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF",
        "authors": [
            "Anand Siththaranjan",
            "Cassidy Laidlaw",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-12-13T18:51:34Z",
        "summary": "In practice, preference learning from human feedback depends on incomplete\ndata with hidden context. Hidden context refers to data that affects the\nfeedback received, but which is not represented in the data used to train a\npreference model. This captures common issues of data collection, such as\nhaving human annotators with varied preferences, cognitive processes that\nresult in seemingly irrational behavior, and combining data labeled according\nto different criteria. We prove that standard applications of preference\nlearning, including reinforcement learning from human feedback (RLHF),\nimplicitly aggregate over hidden contexts according to a well-known voting rule\ncalled Borda count. We show this can produce counter-intuitive results that are\nvery different from other methods which implicitly aggregate via expected\nutility. Furthermore, our analysis formalizes the way that preference learning\nfrom users with diverse values tacitly implements a social choice function. A\nkey implication of this result is that annotators have an incentive to\nmisreport their preferences in order to influence the learned model, leading to\nvulnerabilities in the deployment of RLHF. As a step towards mitigating these\nproblems, we introduce a class of methods called distributional preference\nlearning (DPL). DPL methods estimate a distribution of possible score values\nfor each alternative in order to better account for hidden context.\nExperimental results indicate that applying DPL to RLHF for LLM chatbots\nidentifies hidden context in the data and significantly reduces subsequent\njailbreak vulnerability. Our code and data are available at\nhttps://github.com/cassidylaidlaw/hidden-context",
        "pdf_link": "https://arxiv.org/pdf/2312.08358v1.pdf"
    },
    {
        "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
        "authors": [
            "Jiang Zhang",
            "Qiong Wu",
            "Yiming Xu",
            "Cheng Cao",
            "Zheng Du",
            "Konstantinos Psounis"
        ],
        "published": "2023-12-13T17:22:19Z",
        "summary": "Toxic content detection is crucial for online services to remove\ninappropriate content that violates community standards. To automate the\ndetection process, prior works have proposed varieties of machine learning (ML)\napproaches to train Language Models (LMs) for toxic content detection. However,\nboth their accuracy and transferability across datasets are limited. Recently,\nLarge Language Models (LLMs) have shown promise in toxic content detection due\nto their superior zero-shot and few-shot in-context learning ability as well as\nbroad transferability on ML tasks. However, efficiently designing prompts for\nLLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges, in this work, we\npropose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling\nLLMs for toxic content detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection\nperformance and extract high-quality rationales. DToT can automatically select\nmore fine-grained context to re-prompt LLMs when their responses lack\nconfidence. Additionally, we use the rationales extracted via DToT to fine-tune\nstudent LMs. Our experimental results on various datasets demonstrate that DToT\ncan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs\nfine-tuned with rationales extracted via DToT outperform baselines on all\ndatasets with up to 16.9\\% accuracy improvement, while being more than 60x\nsmaller than conventional LLMs. Finally, we observe that student LMs fine-tuned\nwith rationales exhibit better cross-dataset transferability.",
        "pdf_link": "https://arxiv.org/pdf/2312.08303v1.pdf"
    },
    {
        "title": "Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers",
        "authors": [
            "Haifeng Huang",
            "Zehan Wang",
            "Rongjie Huang",
            "Luping Liu",
            "Xize Cheng",
            "Yang Zhao",
            "Tao Jin",
            "Zhou Zhao"
        ],
        "published": "2023-12-13T14:27:45Z",
        "summary": "Recent research has evidenced the significant potentials of Large Language\nModels (LLMs) in handling challenging tasks within 3D scenes. However, current\nmodels are constrained to addressing object-centric tasks, where each\nquestion-answer pair focuses solely on an individual object. In real-world\napplications, users may pose queries involving multiple objects or expect for\nanswers that precisely reference various objects. We introduce the use of\nobject identifiers to freely reference objects during a conversation. While\nthis solution appears straightforward, it presents two main challenges: 1) How\nto establish a reliable one-to-one correspondence between each object and its\nidentifier? 2) How to incorporate complex spatial relationships among dozens of\nobjects into the embedding space of the LLM? To address these challenges, we\npropose a two-stage alignment method, which involves learning an\nattribute-aware token and a relation-aware token for each object. These tokens\ncapture the object's attributes and spatial relationships with surrounding\nobjects in the 3D scene. Once the alignment is established, we can fine-tune\nour model on various downstream tasks using instruction tuning. Experiments\nconducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3D\nshowcase the effectiveness of our proposed method. Additionally, we create a 3D\nscene captioning dataset annotated with rich object identifiers, with the\nassistant of GPT-4. This dataset aims to further explore the capability of\nobject identifiers in effective object referencing and precise scene\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2312.08168v2.pdf"
    },
    {
        "title": "Breaking the Silence: the Threats of Using LLMs in Software Engineering",
        "authors": [
            "June Sallou",
            "Thomas Durieux",
            "Annibale Panichella"
        ],
        "published": "2023-12-13T11:02:19Z",
        "summary": "Large Language Models (LLMs) have gained considerable traction within the\nSoftware Engineering (SE) community, impacting various SE tasks from code\ncompletion to test generation, from program repair to code summarization.\nDespite their promise, researchers must still be careful as numerous intricate\nfactors can influence the outcomes of experiments involving LLMs. This paper\ninitiates an open discussion on potential threats to the validity of LLM-based\nresearch including issues such as closed-source models, possible data leakage\nbetween LLM training data and research evaluation, and the reproducibility of\nLLM-based findings. In response, this paper proposes a set of guidelines\ntailored for SE researchers and Language Model (LM) providers to mitigate these\nconcerns. The implications of the guidelines are illustrated using existing\ngood practices followed by LLM providers and a practical example for SE\nresearchers in the context of test case generation.",
        "pdf_link": "https://arxiv.org/pdf/2312.08055v2.pdf"
    },
    {
        "title": "CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs",
        "authors": [
            "Huaiyuan Ying",
            "Zhengyun Zhao",
            "Yang Zhao",
            "Sihang Zeng",
            "Sheng Yu"
        ],
        "published": "2023-12-13T10:29:34Z",
        "summary": "Objective: Biomedical Knowledge Graphs play a pivotal role in various\nbiomedical research domains. Concurrently, term clustering emerges as a crucial\nstep in constructing these knowledge graphs, aiming to identify synonymous\nterms. Due to a lack of knowledge, previous contrastive learning models trained\nwith Unified Medical Language System (UMLS) synonyms struggle at clustering\ndifficult terms and do not generalize well beyond UMLS terms. In this work, we\nleverage the world knowledge from Large Language Models (LLMs) and propose\nContrastive Learning for Representing Terms via Explanations (CoRTEx) to\nenhance term representation and significantly improves term clustering.\nMaterials and Methods: The model training involves generating explanations for\na cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,\nconsidering term and explanation embeddings simultaneously, and progressively\nintroduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH\nalgorithm is designed for efficient clustering of a new ontology. Results: We\nestablished a clustering test set and a hard negative test set, where our model\nconsistently achieves the highest F1 score. With CoRTEx embeddings and the\nmodified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical\nInformatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries\nto ChatGPT. Case studies highlight the model's efficacy in handling challenging\nsamples, aided by information from explanations. Conclusion: By aligning terms\nto their explanations, CoRTEx demonstrates superior accuracy over benchmark\nmodels and robustness beyond its training set, and it is suitable for\nclustering terms for large-scale biomedical ontologies.",
        "pdf_link": "https://arxiv.org/pdf/2312.08036v1.pdf"
    },
    {
        "title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI",
        "authors": [
            "Kai Huang",
            "Boyuan Yang",
            "Wei Gao"
        ],
        "published": "2023-12-13T04:08:59Z",
        "summary": "Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.",
        "pdf_link": "https://arxiv.org/pdf/2312.07886v1.pdf"
    },
    {
        "title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications",
        "authors": [
            "Feibo Jiang",
            "Li Dong",
            "Yubo Peng",
            "Kezhi Wang",
            "Kun Yang",
            "Cunhua Pan",
            "Dusit Niyato",
            "Octavia A. Dobre"
        ],
        "published": "2023-12-13T02:35:57Z",
        "summary": "The rapid development of the Large Language Model (LLM) presents huge\nopportunities for 6G communications, e.g., network optimization and management\nby allowing users to input task requirements to LLMs by nature language.\nHowever, directly applying native LLMs in 6G encounters various challenges,\nsuch as a lack of private communication data and knowledge, limited logical\nreasoning, evaluation, and refinement abilities. Integrating LLMs with the\ncapabilities of retrieval, planning, memory, evaluation and reflection in\nagents can greatly enhance the potential of LLMs for 6G communications. To this\nend, we propose a multi-agent system with customized communication knowledge\nand tools for solving communication related tasks using natural language,\ncomprising three components: (1) Multi-agent Data Retrieval (MDR), which\nemploys the condensate and inference agents to refine and summarize\ncommunication knowledge from the knowledge base, expanding the knowledge\nboundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning\n(MCP), which utilizes multiple planning agents to generate feasible solutions\nfor the communication related task from different perspectives based on the\nretrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which\nutilizes the evaluation agent to assess the solutions, and applies the\nreflexion agent and refinement agent to provide improvement suggestions for\ncurrent solutions. Finally, we validate the effectiveness of the proposed\nmulti-agent system by designing a semantic communication system, as a case\nstudy of 6G communications.",
        "pdf_link": "https://arxiv.org/pdf/2312.07850v1.pdf"
    },
    {
        "title": "Tell, don't show: Declarative facts influence how LLMs generalize",
        "authors": [
            "Alexander Meinke",
            "Owain Evans"
        ],
        "published": "2023-12-12T22:47:42Z",
        "summary": "We examine how large language models (LLMs) generalize from abstract\ndeclarative statements in their training data. As an illustration, consider an\nLLM that is prompted to generate weather reports for London in 2050. One\npossibility is that the temperatures in the reports match the mean and variance\nof reports from 2023 (i.e. matching the statistics of pretraining). Another\npossibility is that the reports predict higher temperatures, by incorporating\ndeclarative statements about climate change from scientific papers written in\n2023. An example of such a declarative statement is \"global temperatures will\nincrease by $1^{\\circ} \\mathrm{C}$ by 2050\".\n  To test the influence of abstract declarative statements, we construct tasks\nin which LLMs are finetuned on both declarative and procedural information. We\nfind that declarative statements influence model predictions, even when they\nconflict with procedural information. In particular, finetuning on a\ndeclarative statement $S$ increases the model likelihood for logical\nconsequences of $S$. The effect of declarative statements is consistent across\nthree domains: aligning an AI assistant, predicting weather, and predicting\ndemographic features. Through a series of ablations, we show that the effect of\ndeclarative statements cannot be explained by associative learning based on\nmatching keywords. Nevertheless, the effect of declarative statements on model\nlikelihoods is small in absolute terms and increases surprisingly little with\nmodel size (i.e. from 330 million to 175 billion parameters). We argue that\nthese results have implications for AI risk (in relation to the \"treacherous\nturn\") and for fairness.",
        "pdf_link": "https://arxiv.org/pdf/2312.07779v1.pdf"
    },
    {
        "title": "Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization",
        "authors": [
            "Min Zhang",
            "Jianfeng He",
            "Shuo Lei",
            "Murong Yue",
            "Linhang Wang",
            "Chang-Tien Lu"
        ],
        "published": "2023-12-12T22:11:17Z",
        "summary": "The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.",
        "pdf_link": "https://arxiv.org/pdf/2312.07763v1.pdf"
    },
    {
        "title": "Large language models in healthcare and medical domain: A review",
        "authors": [
            "Zabir Al Nazi",
            "Wei Peng"
        ],
        "published": "2023-12-12T20:54:51Z",
        "summary": "The deployment of large language models (LLMs) within the healthcare sector\nhas sparked both enthusiasm and apprehension. These models exhibit the\nremarkable capability to provide proficient responses to free-text queries,\ndemonstrating a nuanced understanding of professional medical knowledge. This\ncomprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development,\nstarting from traditional Pretrained Language Models (PLMs) to the present\nstate of LLMs in healthcare sector. First, we explore the potential of LLMs to\namplify the efficiency and effectiveness of diverse healthcare applications,\nparticularly focusing on clinical language understanding tasks. These tasks\nencompass a wide spectrum, ranging from named entity recognition and relation\nextraction to natural language inference, multi-modal medical applications,\ndocument classification, and question-answering. Additionally, we conduct an\nextensive comparison of the most recent state-of-the-art LLMs in the healthcare\ndomain, while also assessing the utilization of various open-source LLMs and\nhighlighting their significance in healthcare applications. Furthermore, we\npresent the essential performance metrics employed to evaluate LLMs in the\nbiomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large\nlanguage models in the healthcare sector, offering a holistic perspective on\ntheir potential benefits and shortcomings. This review provides a comprehensive\nexploration of the current landscape of LLMs in healthcare, addressing their\nrole in transforming medical applications and the areas that warrant further\nresearch and development.",
        "pdf_link": "https://arxiv.org/pdf/2401.06775v1.pdf"
    },
    {
        "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
        "authors": [
            "Keivan Alizadeh",
            "Iman Mirzadeh",
            "Dmitry Belenko",
            "Karen Khatamifard",
            "Minsik Cho",
            "Carlo C Del Mundo",
            "Mohammad Rastegari",
            "Mehrdad Farajtabar"
        ],
        "published": "2023-12-12T18:57:08Z",
        "summary": "Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.",
        "pdf_link": "https://arxiv.org/pdf/2312.11514v2.pdf"
    },
    {
        "title": "Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection",
        "authors": [
            "Caoyun Fan",
            "Jidong Tian",
            "Yitian Li",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023-12-12T18:05:46Z",
        "summary": "In-Context Learning (ICL) is an important paradigm for adapting Large\nLanguage Models (LLMs) to downstream tasks through a few demonstrations.\nDespite the great success of ICL, the limitation of the demonstration number\nmay lead to demonstration bias, i.e. the input-label mapping induced by LLMs\nmisunderstands the task's essence. Inspired by human experience, we attempt to\nmitigate such bias through the perspective of the inter-demonstration\nrelationship. Specifically, we construct Comparable Demonstrations (CDs) by\nminimally editing the texts to flip the corresponding labels, in order to\nhighlight the task's essence and eliminate potential spurious correlations\nthrough the inter-demonstration comparison. Through a series of experiments on\nCDs, we find that (1) demonstration bias does exist in LLMs, and CDs can\nsignificantly reduce such bias; (2) CDs exhibit good performance in ICL,\nespecially in out-of-distribution scenarios. In summary, this study explores\nthe ICL mechanisms from a novel perspective, providing a deeper insight into\nthe demonstration selection strategy for ICL.",
        "pdf_link": "https://arxiv.org/pdf/2312.07476v2.pdf"
    },
    {
        "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
        "authors": [
            "Swanand Ravindra Kadhe",
            "Anisa Halimi",
            "Ambrish Rawat",
            "Nathalie Baracaldo"
        ],
        "published": "2023-12-12T16:44:47Z",
        "summary": "Training large language models (LLMs) is a costly endeavour in terms of time\nand computational resources. The large amount of training data used during the\nunsupervised pre-training phase makes it difficult to verify all data and,\nunfortunately, undesirable data may be ingested during training. Re-training\nfrom scratch is impractical and has led to the creation of the 'unlearning'\ndiscipline where models are modified to \"unlearn\" undesirable information\nwithout retraining. However, any modification can alter the behaviour of LLMs,\nespecially on key dimensions such as fairness. This is the first work that\nexamines this interplay between unlearning and fairness for LLMs. In\nparticular, we focus on a popular unlearning framework known as SISA [Bourtoule\net al., 2021], which creates an ensemble of models trained on disjoint shards.\nWe evaluate the performance-fairness trade-off for SISA, and empirically\ndemsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we\npropose post-processing bias mitigation techniques for ensemble models produced\nby SISA. We adapt the post-processing fairness improvement technique from\n[Hardt et al., 2016] to design three methods that can handle model ensembles,\nand prove that one of the methods is an optimal fair predictor for ensemble of\nmodels. Through experimental results, we demonstrate the efficacy of our\npost-processing framework called 'FairSISA'.",
        "pdf_link": "https://arxiv.org/pdf/2312.07420v1.pdf"
    },
    {
        "title": "ICL Markup: Structuring In-Context Learning using Soft-Token Tags",
        "authors": [
            "Marc-Etienne Brunet",
            "Ashton Anderson",
            "Richard Zemel"
        ],
        "published": "2023-12-12T16:25:05Z",
        "summary": "Large pretrained language models (LLMs) can be rapidly adapted to a wide\nvariety of tasks via a text-to-text approach, where the instruction and input\nare fed to the model in natural language. Combined with in-context learning\n(ICL), this paradigm is impressively flexible and powerful. However, it also\nburdens users with an overwhelming number of choices, many of them arbitrary.\nInspired by markup languages like HTML, we contribute a method of using\nsoft-token tags to compose prompt templates. This approach reduces arbitrary\ndecisions and streamlines the application of ICL. Our method is a form of\nmeta-learning for ICL; it learns these tags in advance during a\nparameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently\nbe used in templates for ICL on new, unseen tasks without any additional\nfine-tuning. Our experiments with this approach yield promising initial\nresults, improving LLM performance on important enterprise applications such as\nfew-shot and open-world intent detection, as well as text classification in\nnews and legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2312.07405v1.pdf"
    },
    {
        "title": "On Diversified Preferences of Large Language Model Alignment",
        "authors": [
            "Dun Zeng",
            "Yong Dai",
            "Pengyu Cheng",
            "Tianhao Hu",
            "Wanshun Chen",
            "Nan Du",
            "Zenglin Xu"
        ],
        "published": "2023-12-12T16:17:15Z",
        "summary": "Aligning large language models (LLMs) with human preferences has been\nrecognized as the key to improving LLMs' interaction quality. However, in this\npluralistic world, human preferences can be diversified due to annotators'\ndifferent tastes, which hinders the effectiveness of LLM alignment methods.\nThis paper presents the first quantitative analysis of commonly used human\nfeedback datasets to investigate the impact of diversified preferences on\nreward modeling. Our analysis reveals a correlation between the calibration\nperformance of reward models (RMs) and the alignment performance of LLMs. We\nfind that diversified preference data negatively affect the calibration\nperformance of RMs on human-shared preferences, such as\n\\textit{Harmless\\&Helpful}, thereby impairing the alignment performance of\nLLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward\nlearning method (MORE) to enhance the calibration performance of RMs on shared\npreferences. We validate our findings by experiments on three models and five\nhuman preference datasets. Our method significantly improves the prediction\ncalibration of RMs, leading to better alignment of the Alpaca-7B model with\n\\textit{Harmless\\&Helpful} preferences. Furthermore, the connection between\nreward calibration and preference alignment performance suggests that\ncalibration error can be adopted as a key metric for evaluating RMs. The\nopen-source code and data are available at\n\\url{https://github.com/dunzeng/MORE}.",
        "pdf_link": "https://arxiv.org/pdf/2312.07401v3.pdf"
    },
    {
        "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
        "authors": [
            "Taeyoon Kwon",
            "Kai Tzu-iunn Ong",
            "Dongjin Kang",
            "Seungjun Moon",
            "Jeong Ryong Lee",
            "Dosik Hwang",
            "Yongsik Sim",
            "Beomseok Sohn",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "published": "2023-12-12T16:14:45Z",
        "summary": "Machine reasoning has made great progress in recent years owing to large\nlanguage models (LLMs). In the clinical domain, however, most NLP-driven\nprojects mainly focus on clinical classification or reading comprehension, and\nunder-explore clinical reasoning for disease diagnosis due to the expensive\nrationale annotation with clinicians. In this work, we present a\n``reasoning-aware'' diagnosis framework that rationalizes the diagnostic\nprocess via prompt-based learning in a time- and labor-efficient manner, and\nlearns to reason over the prompt-generated rationales. Specifically, we address\nthe clinical reasoning for disease diagnosis, where the LLM generates\ndiagnostic rationales providing its insight on presented patient data and the\nreasoning path towards the diagnosis, namely Clinical Chain-of-Thought\n(Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical\nreasoning via extensive experiments and analyses on both rationale generation\nand disease diagnosis in various settings. We further propose a novel set of\ncriteria for evaluating machine-generated rationales' potential for real-world\nclinical settings, facilitating and benefiting future research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2312.07399v2.pdf"
    },
    {
        "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
        "authors": [
            "Pinelopi Papalampidi",
            "Skanda Koppula",
            "Shreya Pathak",
            "Justin Chiu",
            "Joe Heyward",
            "Viorica Patraucean",
            "Jiajun Shen",
            "Antoine Miech",
            "Andrew Zisserman",
            "Aida Nematzdeh"
        ],
        "published": "2023-12-12T16:10:19Z",
        "summary": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).",
        "pdf_link": "https://arxiv.org/pdf/2312.07395v1.pdf"
    },
    {
        "title": "Sequential Planning in Large Partially Observable Environments guided by LLMs",
        "authors": [
            "Swarna Kamal Paul"
        ],
        "published": "2023-12-12T15:36:59Z",
        "summary": "Sequential planning in large state space and action space quickly becomes\nintractable due to combinatorial explosion of the search space. Heuristic\nmethods, like monte-carlo tree search, though effective for large state space,\nbut struggle if action space is large. Pure reinforcement learning methods,\nrelying only on reward signals, needs prohibitively large interactions with the\nenvironment to device a viable plan. If the state space, observations and\nactions can be represented in natural language then Large Language models (LLM)\ncan be used to generate action plans. Recently several such goal-directed\nagents like Reflexion, CLIN, SayCan were able to surpass the performance of\nother state-of-the-art methods with minimum or no task specific training. But\nthey still struggle with exploration and get stuck in local optima. Their\nplanning capabilities are limited by the limited reasoning capability of the\nfoundational LLMs on text data. We propose a hybrid agent \"neoplanner\", that\nsynergizes both state space search with queries to foundational LLM to get the\nbest action plan. The reward signals are quantitatively used to drive the\nsearch. A balance of exploration and exploitation is maintained by maximizing\nupper confidence bounds of values of states. In places where random exploration\nis needed, the LLM is queried to generate an action plan. Learnings from each\ntrial are stored as entity relationships in text format. Those are used in\nfuture queries to the LLM for continual improvement. Experiments in the\nScienceworld environment reveals a 124% improvement from the current best\nmethod in terms of average reward gained across multiple tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.07368v1.pdf"
    },
    {
        "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks",
        "authors": [
            "Ahmed Salem",
            "Andrew Paverd",
            "Boris K√∂pf"
        ],
        "published": "2023-12-12T14:22:20Z",
        "summary": "Prompt injection has emerged as a serious security threat to large language\nmodels (LLMs). At present, the current best-practice for defending against\nnewly-discovered prompt injection techniques is to add additional guardrails to\nthe system (e.g., by updating the system prompt or using classifiers on the\ninput and/or output of the model.) However, in the same way that variants of a\npiece of malware are created to evade anti-virus software, variants of a prompt\ninjection can be created to evade the LLM's guardrails. Ideally, when a new\nprompt injection technique is discovered, candidate defenses should be tested\nnot only against the successful prompt injection, but also against possible\nvariants.\n  In this work, we present, a tool to assist defenders in performing automated\nvariant analysis of known prompt injection attacks. This involves solving two\nmain challenges: (1) automatically generating variants of a given prompt\naccording, and (2) automatically determining whether a variant was effective\nbased only on the output of the model. This tool can also assist in generating\ndatasets for jailbreak and prompt injection attacks, thus overcoming the\nscarcity of data in this domain.\n  We evaluate Maatphor on three different types of prompt injection tasks.\nStarting from an ineffective (0%) seed prompt, Maatphor consistently generates\nvariants that are at least 60% effective within the first 40 iterations.",
        "pdf_link": "https://arxiv.org/pdf/2312.11513v1.pdf"
    },
    {
        "title": "Multilingual large language models leak human stereotypes across language boundaries",
        "authors": [
            "Yang Trista Cao",
            "Anna Sotnikova",
            "Jieyu Zhao",
            "Linda X. Zou",
            "Rachel Rudinger",
            "Hal Daume III"
        ],
        "published": "2023-12-12T10:24:17Z",
        "summary": "Multilingual large language models have been increasingly popular for their\nproficiency in comprehending and generating text across various languages.\nPrevious research has shown that the presence of stereotypes and biases in\nmonolingual large language models can be attributed to the nature of their\ntraining data, which is collected from humans and reflects societal biases.\nMultilingual language models undergo the same training procedure as monolingual\nones, albeit with training data sourced from various languages. This raises the\nquestion: do stereotypes present in one social context leak across languages\nwithin the model? In our work, we first define the term ``stereotype leakage''\nand propose a framework for its measurement. With this framework, we\ninvestigate how stereotypical associations leak across four languages: English,\nRussian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an\napproach from social psychology, measuring stereotypes via group-trait\nassociations. We evaluate human stereotypes and stereotypical associations\nmanifested in multilingual large language models such as mBERT, mT5, and\nChatGPT. Our findings show a noticeable leakage of positive, negative, and\nnon-polar associations across all languages. Notably, Hindi within multilingual\nmodels appears to be the most susceptible to influence from other languages,\nwhile Chinese is the least. Additionally, ChatGPT exhibits a better alignment\nwith human scores than other models.",
        "pdf_link": "https://arxiv.org/pdf/2312.07141v1.pdf"
    },
    {
        "title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models",
        "authors": [
            "Yimo Deng",
            "Huangxun Chen"
        ],
        "published": "2023-12-12T10:04:43Z",
        "summary": "Text-to-image (TTI) models offer many innovative services but also raise\nethical concerns due to their potential to generate unethical images. Most\npublic TTI services employ safety filters to prevent unintended images. In this\nwork, we introduce the Divide-and-Conquer Attack to circumvent the safety\nfilters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our\nattack leverages LLMs as text transformation agents to create adversarial\nprompts. We design attack helper prompts that effectively guide LLMs to break\ndown an unethical drawing intent into multiple benign descriptions of\nindividual image elements, allowing them to bypass safety filters while still\ngenerating unethical images. Because the latent harmful meaning only becomes\napparent when all individual elements are drawn together. Our evaluation\ndemonstrates that our attack successfully circumvents multiple strong\nclosed-box safety filters. The comprehensive success rate of DACA bypassing the\nsafety filters of the state-of-the-art TTI engine DALL-E 3 is above 85%, while\nthe success rate for bypassing Midjourney V6 exceeds 75%. Our findings have\nmore severe security implications than methods of manual crafting or iterative\nTTI model querying due to lower attack barrier, enhanced interpretability , and\nbetter adaptation to defense. Our prototype is available at:\nhttps://github.com/researchcode001/Divide-and-Conquer-Attack",
        "pdf_link": "https://arxiv.org/pdf/2312.07130v3.pdf"
    },
    {
        "title": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature",
        "authors": [
            "Maxime W√ºrsch",
            "Andrei Kucharavy",
            "Dimitri Percia David",
            "Alain Mermoud"
        ],
        "published": "2023-12-12T09:39:03Z",
        "summary": "The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.",
        "pdf_link": "https://arxiv.org/pdf/2312.07110v1.pdf"
    },
    {
        "title": "Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
        "authors": [
            "Xiang Li",
            "Haoran Tang",
            "Siyu Chen",
            "Ziwei Wang",
            "Anurag Maravi",
            "Marcin Abram"
        ],
        "published": "2023-12-12T08:43:20Z",
        "summary": "In this paper, we explore the challenges inherent to Large Language Models\n(LLMs) like GPT-4, particularly their propensity for hallucinations, logic\nmistakes, and incorrect conclusions when tasked with answering complex\nquestions. The capacity of LLMs to present erroneous answers in a coherent and\nsemantically rigorous manner further complicates the detection of factual\ninaccuracies. This issue is especially pronounced in fields that require\nspecialized expertise. Our work delves into these challenges, aiming to enhance\nthe understanding and mitigation of such errors, thereby contributing to the\nimprovement of LLM accuracy and reliability in scientific and other specialized\ndomains. Our findings reveal a non-linear relationship between the context's\nrelevancy and the answers' measured quality. In addition, we demonstrate that\nwith the correct calibration, it is possible to automate the grading procedure\n-- a finding suggesting that, at least to some degree, the LLMs can be used to\nself-examine the quality of their own performance. Finally, we describe an\nexperimental platform that can be seen as a proof-of-concept of the techniques\ndescribed in this work.",
        "pdf_link": "https://arxiv.org/pdf/2312.07069v2.pdf"
    },
    {
        "title": "Improving Factual Error Correction by Learning to Inject Factual Errors",
        "authors": [
            "Xingwei He",
            "Qianru Zhang",
            "A-Long Jin",
            "Jun Ma",
            "Yuan Yuan",
            "Siu Ming Yiu"
        ],
        "published": "2023-12-12T08:02:06Z",
        "summary": "Factual error correction (FEC) aims to revise factual errors in false claims\nwith minimal editing, making them faithful to the provided evidence. This task\nis crucial for alleviating the hallucination problem encountered by large\nlanguage models. Given the lack of paired data (i.e., false claims and their\ncorresponding correct claims), existing methods typically adopt the\nmask-then-correct paradigm. This paradigm relies solely on unpaired false\nclaims and correct claims, thus being referred to as distantly supervised\nmethods. These methods require a masker to explicitly identify factual errors\nwithin false claims before revising with a corrector. However, the absence of\npaired data to train the masker makes accurately pinpointing factual errors\nwithin claims challenging. To mitigate this, we propose to improve FEC by\nLearning to Inject Factual Errors (LIFE), a three-step distantly supervised\nmethod: mask-corrupt-correct. Specifically, we first train a corruptor using\nthe mask-then-corrupt procedure, allowing it to deliberately introduce factual\nerrors into correct text. The corruptor is then applied to correct claims,\ngenerating a substantial amount of paired data. After that, we filter out\nlow-quality data, and use the remaining data to train a corrector. Notably, our\ncorrector does not require a masker, thus circumventing the bottleneck\nassociated with explicit factual error identification. Our experiments on a\npublic dataset verify the effectiveness of LIFE in two key aspects: Firstly, it\noutperforms the previous best-performing distantly supervised method by a\nnotable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,\neven compared to ChatGPT prompted with in-context examples, LIFE achieves a\nsuperiority of 7.16 points in SARI Final.",
        "pdf_link": "https://arxiv.org/pdf/2312.07049v1.pdf"
    },
    {
        "title": "Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models",
        "authors": [
            "Arnav Chavan",
            "Nahush Lele",
            "Deepak Gupta"
        ],
        "published": "2023-12-12T07:56:57Z",
        "summary": "Due to the substantial scale of Large Language Models (LLMs), the direct\napplication of conventional compression methodologies proves impractical. The\ncomputational demands associated with even minimal gradient updates present\nchallenges, particularly on consumer-grade hardware. This paper introduces an\ninnovative approach for the parametric and practical compression of LLMs based\non reduced order modelling, which entails low-rank decomposition within the\nfeature space and re-parameterization in the weight space. Notably, this\ncompression technique operates in a layer-wise manner, obviating the need for a\nGPU device and enabling the compression of billion-scale models within\nstringent constraints of both memory and time. Our method represents a\nsignificant advancement in model compression by leveraging matrix\ndecomposition, demonstrating superior efficacy compared to the prevailing\nstate-of-the-art structured pruning method.",
        "pdf_link": "https://arxiv.org/pdf/2312.07046v1.pdf"
    },
    {
        "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts",
        "authors": [
            "Giang Do",
            "Khiem Le",
            "Quang Pham",
            "TrungTin Nguyen",
            "Thanh-Nam Doan",
            "Bint T. Nguyen",
            "Chenghao Liu",
            "Savitha Ramasamy",
            "Xiaoli Li",
            "Steven Hoi"
        ],
        "published": "2023-12-12T07:40:23Z",
        "summary": "By routing input tokens to only a few split experts, Sparse\nMixture-of-Experts has enabled efficient training of large language models.\nRecent findings suggest that fixing the routers can achieve competitive\nperformance by alleviating the collapsing problem, where all experts eventually\nlearn similar representations. However, this strategy has two key limitations:\n(i) the policy derived from random routers might be sub-optimal, and (ii) it\nrequires extensive resources during training and evaluation, leading to limited\nefficiency gains. This work introduces \\HyperRout, which dynamically generates\nthe router's parameters through a fixed hypernetwork and trainable embeddings\nto achieve a balance between training the routers and freezing them to learn an\nimproved routing policy. Extensive experiments across a wide range of tasks\ndemonstrate the superior performance and efficiency gains of \\HyperRouter\ncompared to existing routing methods. Our implementation is publicly available\nat {\\url{{https://github.com/giangdip2410/HyperRouter}}}.",
        "pdf_link": "https://arxiv.org/pdf/2312.07035v1.pdf"
    },
    {
        "title": "Alignment for Honesty",
        "authors": [
            "Yuqing Yang",
            "Ethan Chern",
            "Xipeng Qiu",
            "Graham Neubig",
            "Pengfei Liu"
        ],
        "published": "2023-12-12T06:10:42Z",
        "summary": "Recent research has made significant strides in applying alignment techniques\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\naccordance with human intentions. In this paper, we argue for the importance of\nalignment for honesty, ensuring that LLMs proactively refuse to answer\nquestions when they lack knowledge, while still not being overly conservative.\nHowever, a pivotal aspect of alignment for honesty involves discerning the\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\ndemands comprehensive solutions in terms of metric development, benchmark\ncreation, and training methodologies. In this paper, we address these\nchallenges by first establishing a precise problem definition and defining\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\nfor developing metrics that effectively measure an LLM's honesty by quantifying\nits progress post-alignment. Furthermore, we introduce a flexible training\nframework which is further instantiated by several efficient fine-tuning\ntechniques that emphasize honesty without sacrificing performance on other\ntasks. Our extensive experiments reveal that these aligned models show a marked\nincrease in honesty, as indicated by our proposed metrics. We open-source a\nwealth of resources to facilitate future research at\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\nmodels, training and evaluation datasets for honesty alignment, concept\nglossary, as well as all relevant source code.",
        "pdf_link": "https://arxiv.org/pdf/2312.07000v1.pdf"
    },
    {
        "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model",
        "authors": [
            "Chaoya Jiang",
            "Haiyang Xu",
            "Mengfan Dong",
            "Jiaxing Chen",
            "Wei Ye",
            "Ming Yan",
            "Qinghao Ye",
            "Ji Zhang",
            "Fei Huang",
            "Shikun Zhang"
        ],
        "published": "2023-12-12T04:05:15Z",
        "summary": "Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA. Our code is available on\nhttps://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.",
        "pdf_link": "https://arxiv.org/pdf/2312.06968v4.pdf"
    },
    {
        "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack",
        "authors": [
            "Yu Fu",
            "Yufei Li",
            "Wen Xiao",
            "Cong Liu",
            "Yue Dong"
        ],
        "published": "2023-12-12T01:39:29Z",
        "summary": "Recent developments in balancing the usefulness and safety of Large Language\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\nadequately aligned with safety consideration? Our study, focusing on\nsafety-sensitive documents obtained through adversarial attacks, reveals\nsignificant disparities in the safety alignment of various NLP tasks. For\ninstance, LLMs can effectively summarize malicious long documents but often\nrefuse to translate them. This discrepancy highlights a previously unidentified\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\nsummarization, can potentially compromise the integraty of tasks traditionally\ndeemed more robust, such as translation and question-answering (QA). Moreover,\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\nGPT-4, indicating an urgent need for strengthening safety alignments across a\nbroad spectrum of NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.06924v1.pdf"
    },
    {
        "title": "Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks",
        "authors": [
            "Lingfeng Sun",
            "Devesh K. Jha",
            "Chiori Hori",
            "Siddarth Jain",
            "Radu Corcodel",
            "Xinghao Zhu",
            "Masayoshi Tomizuka",
            "Diego Romeres"
        ],
        "published": "2023-12-11T22:54:44Z",
        "summary": "Designing robotic agents to perform open vocabulary tasks has been the\nlong-standing goal in robotics and AI. Recently, Large Language Models (LLMs)\nhave achieved impressive results in creating robotic agents for performing open\nvocabulary tasks. However, planning for these tasks in the presence of\nuncertainties is challenging as it requires \\enquote{chain-of-thought}\nreasoning, aggregating information from the environment, updating state\nestimates, and generating actions based on the updated state estimates. In this\npaper, we present an interactive planning technique for partially observable\ntasks using LLMs. In the proposed method, an LLM is used to collect missing\ninformation from the environment using a robot and infer the state of the\nunderlying problem from collected observations while guiding the robot to\nperform the required actions. We also use a fine-tuned Llama 2 model via\nself-instruct and compare its performance against a pre-trained LLM like GPT-4.\nResults are demonstrated on several tasks in simulation as well as real-world\nenvironments. A video describing our work along with some results could be\nfound here.",
        "pdf_link": "https://arxiv.org/pdf/2312.06876v1.pdf"
    },
    {
        "title": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?",
        "authors": [
            "Shabaz Patel",
            "Hassan Kane",
            "Rayhan Patel"
        ],
        "published": "2023-12-11T18:59:09Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nnumerous natural language understanding use cases. However, this impressive\nperformance comes with inherent limitations, such as the tendency to perpetuate\nstereotypical biases or fabricate non-existent facts. In the context of Islam\nand its representation, accurate and factual representation of its beliefs and\nteachings rooted in the Quran and Sunnah is key. This work focuses on the\nchallenge of building domain-specific LLMs faithful to the Islamic worldview\nand proposes ways to build and evaluate such systems. Firstly, we define this\nopen-ended goal as a technical problem and propose various solutions.\nSubsequently, we critically examine known challenges inherent to each approach\nand highlight evaluation methodologies that can be used to assess such systems.\nThis work highlights the need for high-quality datasets, evaluations, and\ninterdisciplinary work blending machine learning with Islamic scholarship.",
        "pdf_link": "https://arxiv.org/pdf/2312.06652v1.pdf"
    },
    {
        "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
        "authors": [
            "Will E. Thompson",
            "David M. Vidmar",
            "Jessica K. De Freitas",
            "John M. Pfeifer",
            "Brandon K. Fornwalt",
            "Ruijun Chen",
            "Gabriel Altay",
            "Kabir Manghnani",
            "Andrew C. Nelsen",
            "Kellie Morland",
            "Martin C. Stumpe",
            "Riccardo Miotto"
        ],
        "published": "2023-12-11T15:45:27Z",
        "summary": "Identifying disease phenotypes from electronic health records (EHRs) is\ncritical for numerous secondary uses. Manually encoding physician knowledge\ninto rules is particularly challenging for rare diseases due to inadequate EHR\ncoding, necessitating review of clinical notes. Large language models (LLMs)\noffer promise in text understanding but may not efficiently handle real-world\nclinical documentation. We propose a zero-shot LLM-based method enriched by\nretrieval-augmented generation and MapReduce, which pre-identifies\ndisease-related text snippets to be used in parallel as queries for the LLM to\nestablish diagnosis. We show that this method as applied to pulmonary\nhypertension (PH), a rare disease characterized by elevated arterial pressures\nin the lungs, significantly outperforms physician logic rules ($F_1$ score of\n0.62 vs. 0.75). This method has the potential to enhance rare disease cohort\nidentification, expanding the scope of robust clinical research and care gap\nidentification.",
        "pdf_link": "https://arxiv.org/pdf/2312.06457v1.pdf"
    },
    {
        "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
        "authors": [
            "Tao Chen",
            "Enwei Zhang",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun",
            "Yan Zhang",
            "Hui Li"
        ],
        "published": "2023-12-11T13:11:04Z",
        "summary": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.",
        "pdf_link": "https://arxiv.org/pdf/2312.06363v2.pdf"
    },
    {
        "title": "Linguistic and Structural Basis of Engineering Design Knowledge",
        "authors": [
            "L. Siddharth",
            "Jianxi Luo"
        ],
        "published": "2023-12-11T13:03:39Z",
        "summary": "Artefact descriptions are the primary carriers of engineering design\nknowledge that is both an outcome and a driver of the design process. While an\nartefact could be described in different connotations, the design process\nrequires a description to embody engineering design knowledge, which is\nexpressed in the text through intricate placement of entities and\nrelationships. As large-language models learn from all kinds of text merely as\na sequence of characters/tokens, these are yet to generate text that embodies\nexplicit engineering design facts. Existing ontological design theories are\nless likely to guide the large-language models whose applications are currently\nlimited to ideation and learning purposes. In this article, we explicate\nengineering design knowledge as knowledge graphs from a large sample of 33,881\npatent documents. We examine the constituents of these knowledge graphs to\nunderstand the linguistic and structural basis of engineering design knowledge.\nIn terms of linguistic basis, we observe that entities and relationships could\nbe generalised to 64 and 24 linguistic syntaxes. While relationships mainly\ncapture attributes ('of'), structure ('in', 'with'), purpose ('to', 'for'),\nhierarchy ('include'), exemplification ('such as'), and behaviour ('to',\n'from'), the hierarchical relationships could specifically be identified using\n75 unique syntaxes. To understand the structural basis, we draw inspiration\nfrom various studies on biological/ecological networks and discover motifs from\npatent knowledge graphs. We identify four 3-node and four 4-node patterns that\ncould further be converged and simplified into sequence [->...->], aggregation\n[->...<-], and hierarchy [<-...->]. Expected to guide large-language model\nbased design tools, we propose few regulatory precepts for concretising\nabstract entities and relationships within subgraphs, while explicating\nhierarchical structures.",
        "pdf_link": "https://arxiv.org/pdf/2312.06355v2.pdf"
    },
    {
        "title": "Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models",
        "authors": [
            "Yubin Wang",
            "Xinyang Jiang",
            "De Cheng",
            "Dongsheng Li",
            "Cairong Zhao"
        ],
        "published": "2023-12-11T12:14:06Z",
        "summary": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models to downstream tasks. As large language models (LLMs) have\nemerged, recent studies have explored the use of category-related descriptions\nas input to enhance prompt effectiveness. Nevertheless, conventional\ndescriptions fall short of structured information that effectively represents\nthe interconnections among entities or attributes linked to a particular\ncategory. To address this limitation and prioritize harnessing structured\nknowledge, this paper advocates for leveraging LLMs to build a graph for each\ndescription to model the entities and attributes describing the category, as\nwell as their correlations. Preexisting prompt tuning methods exhibit\ninadequacies in managing this structured knowledge. Consequently, we propose a\nnovel approach called Hierarchical Prompt Tuning (HPT), which enables\nsimultaneous modeling of both structured and conventional linguistic knowledge.\nSpecifically, we introduce a relationship-guided attention module to capture\npair-wise associations among entities and attributes for low-level prompt\nlearning. In addition, by incorporating high-level and global-level prompts\nmodeling overall semantics, the proposed hierarchical structure forges\ncross-level interlinks and empowers the model to handle more complex and\nlong-term relationships. Extensive experiments demonstrate that our HPT shows\nstrong effectiveness and generalizes much better than existing SOTA methods.\nOur code is available at https://github.com/Vill-Lab/2024-AAAI-HPT.",
        "pdf_link": "https://arxiv.org/pdf/2312.06323v1.pdf"
    },
    {
        "title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Shirui Pan",
            "Wenpeng Yin",
            "Mykola Pechenizkiy"
        ],
        "published": "2023-12-11T12:02:14Z",
        "summary": "Warning: This paper contains content that may be offensive or upsetting.\nThere has been a significant increase in the usage of large language models\n(LLMs) in various applications, both in their original form and through\nfine-tuned adaptations. As a result, LLMs have gained popularity and are being\nwidely adopted by a large user community. However, one of the concerns with\nLLMs is the potential generation of socially biased content. The existing\nevaluation methods have many constraints, and their results exhibit a limited\ndegree of interpretability. In this work, we propose a bias evaluation\nframework named GPTBIAS that leverages the high performance of LLMs (e.g.,\nGPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce\nprompts called Bias Attack Instructions, which are specifically designed for\nevaluating model bias. To enhance the credibility and interpretability of bias\nevaluation, our framework not only provides a bias score but also offers\ndetailed information, including bias types, affected demographics, keywords,\nreasons behind the biases, and suggestions for improvement. We conduct\nextensive experiments to demonstrate the effectiveness and usability of our\nbias evaluation framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.06315v1.pdf"
    },
    {
        "title": "KnowGPT: Knowledge Injection for Large Language Models",
        "authors": [
            "Qinggang Zhang",
            "Junnan Dong",
            "Hao Chen",
            "Daochen Zha",
            "Zailiang Yu",
            "Xiao Huang"
        ],
        "published": "2023-12-11T07:56:25Z",
        "summary": "Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.06185v4.pdf"
    },
    {
        "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
        "authors": [
            "Lifu Tu",
            "Semih Yavuz",
            "Jin Qu",
            "Jiacheng Xu",
            "Rui Meng",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-12-11T06:35:33Z",
        "summary": "Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).",
        "pdf_link": "https://arxiv.org/pdf/2312.06149v2.pdf"
    },
    {
        "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
        "authors": [
            "Heegyu Kim",
            "Hyunsouk Cho"
        ],
        "published": "2023-12-11T05:04:17Z",
        "summary": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. The fast-paced evolution of generative language models such as\nGPT-4 has demonstrated outstanding results in various NLP generation tasks.\nHowever, due to the potential generation of offensive words related to race or\ngender, various Controllable Text Generation (CTG) methods have been proposed\nto mitigate the occurrence of harmful words. However, existing CTG methods not\nonly reduce toxicity but also negatively impact several aspects of the language\nmodel's generation performance, including topic consistency, grammar, and\nperplexity. This paper explores the limitations of previous methods and\nintroduces a novel solution in the form of a simple Gated Toxicity Avoidance\n(GTA) that can be applied to any CTG method. We also evaluate the effectiveness\nof the proposed GTA by comparing it with state-of-the-art CTG methods across\nvarious datasets. Our findings reveal that gated toxicity avoidance efficiently\nachieves comparable levels of toxicity reduction to the original CTG methods\nwhile preserving the generation performance of the language model.",
        "pdf_link": "https://arxiv.org/pdf/2312.06122v1.pdf"
    },
    {
        "title": "Can LLMs Configure Software Tools",
        "authors": [
            "Jai Kannan"
        ],
        "published": "2023-12-11T05:03:02Z",
        "summary": "In software engineering, the meticulous configuration of software tools is\ncrucial in ensuring optimal performance within intricate systems. However, the\ncomplexity inherent in selecting optimal configurations is exacerbated by the\nhigh-dimensional search spaces presented in modern applications. Conventional\ntrial-and-error or intuition-driven methods are both inefficient and\nerror-prone, impeding scalability and reproducibility. In this study, we embark\non an exploration of leveraging Large-Language Models (LLMs) to streamline the\nsoftware configuration process. We identify that the task of hyperparameter\nconfiguration for machine learning components within intelligent applications\nis particularly challenging due to the extensive search space and\nperformance-critical nature. Existing methods, including Bayesian optimization,\nhave limitations regarding initial setup, computational cost, and convergence\nefficiency. Our work presents a novel approach that employs LLMs, such as\nChat-GPT, to identify starting conditions and narrow down the search space,\nimproving configuration efficiency. We conducted a series of experiments to\ninvestigate the variability of LLM-generated responses, uncovering intriguing\nfindings such as potential response caching and consistent behavior based on\ndomain-specific keywords. Furthermore, our results from hyperparameter\noptimization experiments reveal the potential of LLMs in expediting\ninitialization processes and optimizing configurations. While our initial\ninsights are promising, they also indicate the need for further in-depth\ninvestigations and experiments in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2312.06121v1.pdf"
    },
    {
        "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
        "authors": [
            "Zhaoxuan Tan",
            "Meng Jiang"
        ],
        "published": "2023-12-11T03:59:36Z",
        "summary": "User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading",
        "pdf_link": "https://arxiv.org/pdf/2312.11518v2.pdf"
    },
    {
        "title": "EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models",
        "authors": [
            "Yi Chen",
            "Yuying Ge",
            "Yixiao Ge",
            "Mingyu Ding",
            "Bohao Li",
            "Rui Wang",
            "Ruifeng Xu",
            "Ying Shan",
            "Xihui Liu"
        ],
        "published": "2023-12-11T03:35:58Z",
        "summary": "Multimodal Large Language Models (MLLMs), building upon the powerful Large\nLanguage Models (LLMs) with exceptional reasoning and generalization\ncapability, have opened up new avenues for embodied task planning. MLLMs excel\nin their ability to integrate diverse environmental inputs, such as real-time\ntask progress, visual observations, and open-form language instructions, which\nare crucial for executable task planning. In this work, we introduce a\nbenchmark with human annotations, EgoPlan-Bench, to quantitatively investigate\nthe potential of MLLMs as embodied task planners in real-world scenarios. Our\nbenchmark is distinguished by realistic tasks derived from real-world videos, a\ndiverse set of actions involving interactions with hundreds of different\nobjects, and complex visual observations from varied environments. We evaluate\nvarious open-source MLLMs, revealing that these models have not yet evolved\ninto embodied planning generalists (even GPT-4V). We further construct an\ninstruction-tuning dataset EgoPlan-IT from videos of human-object interactions,\nto facilitate the learning of high-level task planning in intricate real-world\nsituations. The experiment results demonstrate that the model tuned on\nEgoPlan-IT not only significantly improves performance on our benchmark, but\nalso effectively acts as embodied planner in simulations.",
        "pdf_link": "https://arxiv.org/pdf/2312.06722v1.pdf"
    },
    {
        "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities",
        "authors": [
            "Sangwon Hyun",
            "Mingyu Guo",
            "M. Ali Babar"
        ],
        "published": "2023-12-11T01:29:19Z",
        "summary": "Large-Language Models (LLMs) have shifted the paradigm of natural language\ndata processing. However, their black-boxed and probabilistic characteristics\ncan lead to potential risks in the quality of outputs in diverse LLM\napplications. Recent studies have tested Quality Attributes (QAs), such as\nrobustness or fairness, of LLMs by generating adversarial input texts. However,\nexisting studies have limited their coverage of QAs and tasks in LLMs and are\ndifficult to extend. Additionally, these studies have only used one evaluation\nmetric, Attack Success Rate (ASR), to assess the effectiveness of their\napproaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL)\nframework to address these issues by applying Metamorphic Testing (MT)\ntechniques. This approach facilitates the systematic testing of LLM qualities\nby defining Metamorphic Relations (MRs), which serve as modularized evaluation\nmetrics. The METAL framework can automatically generate hundreds of MRs from\ntemplates that cover various QAs and tasks. In addition, we introduced novel\nmetrics that integrate the ASR method into the semantic qualities of text to\nassess the effectiveness of MRs accurately. Through the experiments conducted\nwith three prominent LLMs, we have confirmed that the METAL framework\neffectively evaluates essential QAs on primary LLM tasks and reveals the\nquality risks in LLMs. Moreover, the newly proposed metrics can guide the\noptimal MRs for testing each task and suggest the most effective method for\ngenerating MRs.",
        "pdf_link": "https://arxiv.org/pdf/2312.06056v1.pdf"
    },
    {
        "title": "Large Language Models on Lexical Semantic Change Detection: An Evaluation",
        "authors": [
            "Ruiyu Wang",
            "Matthew Choi"
        ],
        "published": "2023-12-10T21:26:35Z",
        "summary": "Lexical Semantic Change Detection stands out as one of the few areas where\nLarge Language Models (LLMs) have not been extensively involved. Traditional\nmethods like PPMI, and SGNS remain prevalent in research, alongside newer\nBERT-based approaches. Despite the comprehensive coverage of various natural\nlanguage processing domains by LLMs, there is a notable scarcity of literature\nconcerning their application in this specific realm. In this work, we seek to\nbridge this gap by introducing LLMs into the domain of Lexical Semantic Change\nDetection. Our work presents novel prompting solutions and a comprehensive\nevaluation that spans all three generations of language models, contributing to\nthe exploration of LLMs in this research area.",
        "pdf_link": "https://arxiv.org/pdf/2312.06002v1.pdf"
    },
    {
        "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
        "authors": [
            "Oded Ovadia",
            "Menachem Brief",
            "Moshik Mishaeli",
            "Oren Elisha"
        ],
        "published": "2023-12-10T16:52:00Z",
        "summary": "Large language models (LLMs) encapsulate a vast amount of factual information\nwithin their pre-trained weights, as evidenced by their ability to answer\ndiverse questions across different domains. However, this knowledge is\ninherently limited, relying heavily on the characteristics of the training\ndata. Consequently, using external datasets to incorporate new information or\nrefine the capabilities of LLMs on previously seen information poses a\nsignificant challenge. In this study, we compare two common approaches:\nunsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate\nboth approaches on a variety of knowledge-intensive tasks across different\ntopics. Our findings reveal that while unsupervised fine-tuning offers some\nimprovement, RAG consistently outperforms it, both for existing knowledge\nencountered during training and entirely new knowledge. Moreover, we find that\nLLMs struggle to learn new factual information through unsupervised\nfine-tuning, and that exposing them to numerous variations of the same fact\nduring training could alleviate this problem.",
        "pdf_link": "https://arxiv.org/pdf/2312.05934v3.pdf"
    },
    {
        "title": "Mutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer",
        "authors": [
            "Yongheng Deng",
            "Ziqing Qiao",
            "Ju Ren",
            "Yang Liu",
            "Yaoxue Zhang"
        ],
        "published": "2023-12-10T09:52:32Z",
        "summary": "While large language models (LLMs) are empowered with broad knowledge, their\ntask-specific performance is often suboptimal. It necessitates fine-tuning LLMs\nwith task-specific data, but such data may be inaccessible due to privacy\nconcerns. In this paper, we propose a novel approach to enhance LLMs with\nsmaller language models (SLMs) that are trained on clients using their private\ntask-specific data. To enable mutual enhancement between LLMs and SLMs, we\npropose CrossLM, where the SLMs promote the LLM to generate task-specific\nhigh-quality data, and both the LLM and SLMs are enhanced with the generated\ndata. We evaluate CrossLM using publicly accessible language models across a\nrange of benchmark tasks. The results demonstrate that CrossLM significantly\nenhances the task-specific performance of SLMs on clients and the LLM on the\ncloud server simultaneously while preserving the LLM's generalization\ncapability.",
        "pdf_link": "https://arxiv.org/pdf/2312.05842v1.pdf"
    },
    {
        "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
        "authors": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Yue Song",
            "Qiang Wu",
            "Yan Yan",
            "Guangyu Sun"
        ],
        "published": "2023-12-10T08:41:24Z",
        "summary": "This paper explores a new post-hoc training-free compression paradigm for\ncompressing Large Language Models (LLMs) to facilitate their wider adoption in\nvarious computing environments. We delve into the challenges of LLM\ncompression, notably their dependency on extensive training data and\ncomputational resources. We propose a training-free approach dubbed\nActivation-aware Singular Value Decomposition (ASVD) to address these\nlimitations. ASVD effectively manages activation outliers by adjusting the\nweight matrix based on the activation distribution, improving decomposition\naccuracy and efficiency. Our method also addresses the varying sensitivity of\ndifferent LLM layers to decomposition, with an iterative calibration process\nfor optimal layer-specific decomposition. Experiments demonstrate that ASVD can\ncompress network by 10%-20% without losing reasoning capacities. Additionally,\nit can be seamlessly integrated with other LLM compression paradigms,\nshowcasing its flexible compatibility. Code and compressed models are available\nat https://github.com/hahnyuan/ASVD4LLM.",
        "pdf_link": "https://arxiv.org/pdf/2312.05821v1.pdf"
    },
    {
        "title": "Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup",
        "authors": [
            "Maolin Wang",
            "Yao Zhao",
            "Jiajia Liu",
            "Jingdong Chen",
            "Chenyi Zhuang",
            "Jinjie Gu",
            "Ruocheng Guo",
            "Xiangyu Zhao"
        ],
        "published": "2023-12-10T06:57:48Z",
        "summary": "The deployment of Large Multimodal Models (LMMs) within AntGroup has\nsignificantly advanced multimodal tasks in payment, security, and advertising,\nnotably enhancing advertisement audition tasks in Alipay. However, the\ndeployment of such sizable models introduces challenges, particularly in\nincreased latency and carbon emissions, which are antithetical to the ideals of\nGreen AI. This paper introduces a novel multi-stage compression strategy for\nour proprietary LLM, AntGMM. Our methodology pivots on three main aspects:\nemploying small training sample sizes, addressing multi-level redundancy\nthrough multi-stage pruning, and introducing an advanced distillation loss\ndesign. In our research, we constructed a dataset, the Multimodal Advertisement\nAudition Dataset (MAAD), from real-world scenarios within Alipay, and conducted\nexperiments to validate the reliability of our proposed strategy. Furthermore,\nthe effectiveness of our strategy is evident in its operational success in\nAlipay's real-world multimodal advertisement audition for three months from\nSeptember 2023. Notably, our approach achieved a substantial reduction in\nlatency, decreasing it from 700ms to 90ms, while maintaining online performance\nwith only a slight performance decrease. Moreover, our compressed model is\nestimated to reduce electricity consumption by approximately 75 million kWh\nannually compared to the direct deployment of AntGMM, demonstrating our\ncommitment to green AI initiatives. We will publicly release our code and the\nMAAD dataset after some\nreviews\\footnote{https://github.com/MorinW/AntGMM$\\_$Pruning}.",
        "pdf_link": "https://arxiv.org/pdf/2312.05795v1.pdf"
    },
    {
        "title": "Context Tuning for Retrieval Augmented Generation",
        "authors": [
            "Raviteja Anantha",
            "Tharun Bethi",
            "Danil Vodianik",
            "Srinivas Chappidi"
        ],
        "published": "2023-12-09T23:33:16Z",
        "summary": "Large language models (LLMs) have the remarkable ability to solve new tasks\nwith just a few examples, but they need access to the right tools. Retrieval\nAugmented Generation (RAG) addresses this problem by retrieving a list of\nrelevant tools for a given task. However, RAG's tool retrieval step requires\nall the required information to be explicitly present in the query. This is a\nlimitation, as semantic search, the widely adopted tool retrieval method, can\nfail when the query is incomplete or lacks context. To address this limitation,\nwe propose Context Tuning for RAG, which employs a smart context retrieval\nsystem to fetch relevant information that improves both tool retrieval and plan\ngeneration. Our lightweight context retrieval model uses numerical,\ncategorical, and habitual usage signals to retrieve and rank context items. Our\nempirical results demonstrate that context tuning significantly enhances\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\nplan generation, even after tool retrieval, reduces hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2312.05708v1.pdf"
    },
    {
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": [
            "Gustavo Gon√ßalves",
            "Emma Strubell"
        ],
        "published": "2023-12-09T20:04:20Z",
        "summary": "Large Language Models (LLMs) trained with self-supervision on vast corpora of\nweb text fit to the social biases of that text. Without intervention, these\nsocial biases persist in the model's predictions in downstream tasks, leading\nto representational harm. Many strategies have been proposed to mitigate the\neffects of inappropriate social biases learned during pretraining.\nSimultaneously, methods for model compression have become increasingly popular\nto reduce the computational burden of LLMs. Despite the popularity and need for\nboth approaches, little work has been done to explore the interplay between\nthese two. We perform a carefully controlled study of the impact of model\ncompression via quantization and knowledge distillation on measures of social\nbias in LLMs. Longer pretraining and larger models led to higher social bias,\nand quantization showed a regularizer effect with its best trade-off around 20%\nof the original pretraining time.",
        "pdf_link": "https://arxiv.org/pdf/2312.05662v2.pdf"
    },
    {
        "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
        "authors": [
            "Rumeng Li",
            "Xun Wang",
            "Hong Yu"
        ],
        "published": "2023-12-09T19:35:40Z",
        "summary": "Large language models (LLMs) can generate natural language texts for various\ndomains and tasks, but their potential for clinical text mining, a domain with\nscarce, sensitive, and imbalanced medical data, is underexplored. We\ninvestigate whether LLMs can augment clinical data for detecting Alzheimer's\nDisease (AD)-related signs and symptoms from electronic health records (EHRs),\na challenging task that requires high expertise. We create a novel pragmatic\ntaxonomy for AD sign and symptom progression based on expert knowledge, which\nguides LLMs to generate synthetic data following two different directions:\n\"data-to-label\", which labels sentences from a public EHR collection with\nAD-related signs and symptoms; and \"label-to-data\", which generates sentences\nwith AD-related signs and symptoms based on the label definition. We train a\nsystem to detect AD-related signs and symptoms from EHRs, using three datasets:\n(1) a gold dataset annotated by human experts on longitudinal EHRs of AD\npatients; (2) a silver dataset created by the data-to-label method; and (3) a\nbronze dataset created by the label-to-data method. We find that using the\nsilver and bronze datasets improves the system performance, outperforming the\nsystem using only the gold dataset. This shows that LLMs can generate synthetic\nclinical data for a complex task by incorporating expert knowledge, and our\nlabel-to-data method can produce datasets that are free of sensitive\ninformation, while maintaining acceptable quality.",
        "pdf_link": "https://arxiv.org/pdf/2401.06774v1.pdf"
    },
    {
        "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
        "authors": [
            "Zhenting Qi",
            "Xiaoyu Tan",
            "Shaojie Shi",
            "Chao Qu",
            "Yinghui Xu",
            "Yuan Qi"
        ],
        "published": "2023-12-09T17:38:39Z",
        "summary": "Instruction fine-tuning has conventionally been employed to adapt Large\nLanguage Models (LLMs) to a variety of tasks. Nonetheless, this technique often\nnecessitates substantial computational resources, making it impractical for\ndeployment by individuals or small-scale entities. Recently, Low-Rank\nAdaptation (LoRA) has become a promising alternative, offering high\ncapabilities on par with full tuning with reduced resource overhead. However,\nattaining satisfactory performance through the fine-tuning of LoRA is a\nnon-trivial challenge. In this paper, we propose PILLOW, which aims to improve\nLoRA's performance by a discrimination-based prompting method, leveraging LLMs'\nIn-Context Learning ability. PILLOW incorporates a matching network that\nselects prompts from a user-defined prompt pool, concatenates the selected\nprompts with the user instruction as input, and performs inference using the\nLoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits\ncommensurate performance on various evaluation metrics compared with typical\ninstruction fine-tuning methods, utilizing only consumer-grade GPU resources\nand exhibiting a large reduction in computational costs.",
        "pdf_link": "https://arxiv.org/pdf/2312.05621v1.pdf"
    },
    {
        "title": "Sim-GPT: Text Similarity via GPT Annotated Data",
        "authors": [
            "Shuhe Wang",
            "Beiming Cao",
            "Shengyu Zhang",
            "Xiaoya Li",
            "Jiwei Li",
            "Fei Wu",
            "Guoyin Wang",
            "Eduard Hovy"
        ],
        "published": "2023-12-09T16:10:23Z",
        "summary": "Due to the lack of a large collection of high-quality labeled sentence pairs\nwith textual similarity scores, existing approaches for Semantic Textual\nSimilarity (STS) mostly rely on unsupervised techniques or training signals\nthat are only partially correlated with textual similarity, e.g., NLI-based\ndatasets. To tackle this issue, in this paper, we propose the strategy of\nmeasuring text similarity via GPT annotated data (Sim-GPT for short). The core\nidea of Sim-GPT is to generate data with STS labels using GPT-4, based on which\nan STS model is trained. Sim-GPT framework utilizes LLMs to provide a\nsubstantial amount of reliable annotated data filling the gap of the lack of\ntraining signals for STS. Sim-GPT is trained on a one-time generated dataset\nusing BERT or RoBERTa as the backbone, which offers long-term savings in cost\nand speed compared to repeatedly invoking LLMs for each sentence pair. Trained\non the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the\nwidely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over\nthe current SOTA PromCSE model. To encourage further advancements of the field,\nwe release both models and the 371K annotated examples from GPT-4. Code, models\nand annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2312.05603v2.pdf"
    },
    {
        "title": "Enhancing Medical Specialty Assignment to Patients using NLP Techniques",
        "authors": [
            "Chris Solomou"
        ],
        "published": "2023-12-09T14:13:45Z",
        "summary": "The introduction of Large Language Models (LLMs), and the vast volume of\npublicly available medical data, amplified the application of NLP to the\nmedical domain. However, LLMs are pretrained on data that are not explicitly\nrelevant to the domain that are applied to and are often biased towards the\noriginal data they were pretrained upon. Even when pretrained on domainspecific\ndata, these models typically require time-consuming fine-tuning to achieve good\nperformance for a specific task. To address these limitations, we propose an\nalternative approach that achieves superior performance while being\ncomputationally efficient. Specifically, we utilize keywords to train a deep\nlearning architecture that outperforms a language model pretrained on a large\ncorpus of text. Our proposal does not require pretraining nor fine-tuning and\ncan be applied directly to a specific setting for performing multi-label\nclassification. Our objective is to automatically assign a new patient to the\nspecialty of the medical professional they require, using a dataset that\ncontains medical transcriptions and relevant keywords. To this end, we\nfine-tune the PubMedBERT model on this dataset, which serves as the baseline\nfor our experiments. We then twice train/fine-tune a DNN and the RoBERTa\nlanguage model, using both the keywords and the full transcriptions as input.\nWe compare the performance of these approaches using relevant metrics. Our\nresults demonstrate that utilizing keywords for text classification\nsignificantly improves classification performance, for both a basic DL\narchitecture and a large language model. Our approach represents a promising\nand efficient alternative to traditional methods for finetuning language models\non domain-specific data and has potential applications in various medical\ndomains",
        "pdf_link": "https://arxiv.org/pdf/2312.05585v1.pdf"
    },
    {
        "title": "ESPN: Memory-Efficient Multi-Vector Information Retrieval",
        "authors": [
            "Susav Shrestha",
            "Narasimha Reddy",
            "Zongwang Li"
        ],
        "published": "2023-12-09T00:19:42Z",
        "summary": "Recent advances in large language models have demonstrated remarkable\neffectiveness in information retrieval (IR) tasks. While many neural IR systems\nencode queries and documents into single-vector representations, multi-vector\nmodels elevate the retrieval quality by producing multi-vector representations\nand facilitating similarity searches at the granularity of individual tokens.\nHowever, these models significantly amplify memory and storage requirements for\nretrieval indices by an order of magnitude. This escalation in index size\nrenders the scalability of multi-vector IR models progressively challenging due\nto their substantial memory demands. We introduce Embedding from Storage\nPipelined Network (ESPN) where we offload the entire re-ranking embedding\ntables to SSDs and reduce the memory requirements by 5-16x. We design a\nsoftware prefetcher with hit rates exceeding 90%, improving SSD based retrieval\nup to 6.4x, and demonstrate that we can maintain near memory levels of query\nlatency even for large query batch sizes.",
        "pdf_link": "https://arxiv.org/pdf/2312.05417v1.pdf"
    },
    {
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
        "authors": [
            "Jakub L√°la",
            "Odhran O'Donoghue",
            "Aleksandar Shtedritski",
            "Sam Cox",
            "Samuel G. Rodriques",
            "Andrew D. White"
        ],
        "published": "2023-12-08T18:50:20Z",
        "summary": "Large Language Models (LLMs) generalize well across language tasks, but\nsuffer from hallucinations and uninterpretability, making it difficult to\nassess their accuracy without ground-truth. Retrieval-Augmented Generation\n(RAG) models have been proposed to reduce hallucinations and provide provenance\nfor how an answer was generated. Applying such models to the scientific\nliterature may enable large-scale, systematic processing of scientific\nknowledge. We present PaperQA, a RAG agent for answering questions over the\nscientific literature. PaperQA is an agent that performs information retrieval\nacross full-text scientific articles, assesses the relevance of sources and\npassages, and uses RAG to provide answers. Viewing this agent as a question\nanswering model, we find it exceeds performance of existing LLMs and LLM agents\non current science QA benchmarks. To push the field closer to how humans\nperform research on scientific literature, we also introduce LitQA, a more\ncomplex benchmark that requires retrieval and synthesis of information from\nfull-text scientific papers across the literature. Finally, we demonstrate\nPaperQA's matches expert human researchers on LitQA.",
        "pdf_link": "https://arxiv.org/pdf/2312.07559v2.pdf"
    },
    {
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "published": "2023-12-08T18:25:22Z",
        "summary": "Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.",
        "pdf_link": "https://arxiv.org/pdf/2312.05230v1.pdf"
    },
    {
        "title": "DeltaZip: Multi-Tenant Language Model Serving via Delta Compression",
        "authors": [
            "Xiaozhe Yao",
            "Ana Klimovic"
        ],
        "published": "2023-12-08T18:07:05Z",
        "summary": "Fine-tuning large language models (LLMs) for downstream tasks can greatly\nimprove model quality, however serving many different fine-tuned LLMs\nconcurrently for users in multi-tenant environments is challenging. Dedicating\nGPU memory for each model is prohibitively expensive and naively swapping large\nmodel weights in and out of GPU memory is slow. Our key insight is that\nfine-tuned models can be quickly swapped in and out of GPU memory by extracting\nand compressing the delta between each model and its pre-trained base model. We\npropose DeltaZip, an LLM serving system that efficiently serves multiple\nfull-parameter fine-tuned models concurrently by aggressively compressing model\ndeltas by a factor of $6\\times$ to $8\\times$ while maintaining high model\nquality. DeltaZip increases serving throughput by $1.5\\times$ to $3\\times$ and\nimproves SLO attainment compared to a vanilla HuggingFace serving system.",
        "pdf_link": "https://arxiv.org/pdf/2312.05215v1.pdf"
    },
    {
        "title": "HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models",
        "authors": [
            "Navapat Nananukul",
            "Mayank Kejriwal"
        ],
        "published": "2023-12-08T17:57:20Z",
        "summary": "Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.",
        "pdf_link": "https://arxiv.org/pdf/2312.05209v2.pdf"
    },
    {
        "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering",
        "authors": [
            "Mobashir Sadat",
            "Zhengyu Zhou",
            "Lukas Lange",
            "Jun Araki",
            "Arsalan Gundroo",
            "Bingqing Wang",
            "Rakesh R Menon",
            "Md Rizwan Parvez",
            "Zhe Feng"
        ],
        "published": "2023-12-08T17:41:06Z",
        "summary": "Hallucination is a well-known phenomenon in text generated by large language\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\napplication scenarios e.g., summarization, question-answering (QA) etc. For\napplications requiring high reliability (e.g., customer-facing assistants), the\npotential existence of hallucination in LLM-generated text is a critical\nproblem. The amount of hallucination can be reduced by leveraging information\nretrieval to provide relevant background information to the LLM. However, LLMs\ncan still generate hallucinatory content for various reasons (e.g.,\nprioritizing its parametric knowledge over the context, failure to capture the\nrelevant information from the context, etc.). Detecting hallucinations through\nautomated methods is thus paramount. To facilitate research in this direction,\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\npropose a set of hallucination detection methods to serve as baselines for\nfuture works from the research community. Analysis and case study are also\nprovided to share valuable insights on hallucination phenomena in the target\nscenario.",
        "pdf_link": "https://arxiv.org/pdf/2312.05200v1.pdf"
    },
    {
        "title": "Assessing LLMs for Moral Value Pluralism",
        "authors": [
            "Noam Benkler",
            "Drisana Mosaphir",
            "Scott Friedman",
            "Andrew Smart",
            "Sonja Schmer-Galunder"
        ],
        "published": "2023-12-08T16:18:15Z",
        "summary": "The fields of AI current lacks methods to quantitatively assess and\npotentially alter the moral values inherent in the output of large language\nmodels (LLMs). However, decades of social science research has developed and\nrefined widely-accepted moral value surveys, such as the World Values Survey\n(WVS), eliciting value judgments from direct questions in various geographies.\nWe have turned those questions into value statements and use NLP to compute to\nhow well popular LLMs are aligned with moral values for various demographics\nand cultures. While the WVS is accepted as an explicit assessment of values, we\nlack methods for assessing implicit moral and cultural values in media, e.g.,\nencountered in social media, political rhetoric, narratives, and generated by\nAI systems such as LLMs that are increasingly present in our daily lives. As we\nconsume online content and utilize LLM outputs, we might ask, which moral\nvalues are being implicitly promoted or undercut, or -- in the case of LLMs --\nif they are intending to represent a cultural identity, are they doing so\nconsistently? In this paper we utilize a Recognizing Value Resonance (RVR) NLP\nmodel to identify WVS values that resonate and conflict with a given passage of\noutput text. We apply RVR to the text generated by LLMs to characterize\nimplicit moral values, allowing us to quantify the moral/cultural distance\nbetween LLMs and various demographics that have been surveyed using the WVS. In\nline with other work we find that LLMs exhibit several Western-centric value\nbiases; they overestimate how conservative people in non-Western countries are,\nthey are less accurate in representing gender for non-Western countries, and\nportray older populations as having more traditional values. Our results\nhighlight value misalignment and age groups, and a need for social science\ninformed technological solutions addressing value plurality in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.10075v1.pdf"
    },
    {
        "title": "TypeFly: Flying Drones with Large Language Model",
        "authors": [
            "Guojun Chen",
            "Xiaojing Yu",
            "Lin Zhong"
        ],
        "published": "2023-12-08T15:57:18Z",
        "summary": "Commanding a drone with a natural language is not only user-friendly but also\nopens the door for emerging language agents to control the drone. Emerging\nlarge language models (LLMs) provide a previously impossible opportunity to\nautomatically translate a task description in a natural language to a program\nthat can be executed by the drone. However, powerful LLMs and their vision\ncounterparts are limited in three important ways. First, they are only\navailable as cloud-based services. Sending images to the cloud raises privacy\nconcerns. Second, they are expensive, costing proportionally to the request\nsize. Finally, without expensive fine-tuning, existing LLMs are quite limited\nin their capability of writing a program for specialized systems like drones.\n  In this paper, we present a system called TypeFly that tackles the above\nthree problems using a combination of edge-based vision intelligence, novel\nprogramming language design, and prompt engineering. Instead of the familiar\nPython, TypeFly gets a cloud-based LLM service to write a program in a small,\ncustom language called MiniSpec, based on task and scene descriptions in\nEnglish. Such MiniSpec programs are not only succinct (and therefore efficient)\nbut also able to consult the LLM during their execution using a special skill\ncalled query. Using a set of increasingly challenging drone tasks, we show that\ndesign choices made by TypeFly can reduce both the cost of LLM service and the\ntask execution time by more than 2x. More importantly, query and prompt\nengineering techniques contributed by TypeFly significantly improve the chance\nof success of complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.14950v1.pdf"
    },
    {
        "title": "LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization",
        "authors": [
            "Andreas Florath"
        ],
        "published": "2023-12-08T13:52:57Z",
        "summary": "With the advent of large language models (LLMs) like GPT-3, a natural\nquestion is the extent to which these models can be utilized for source code\noptimization. This paper presents methodologically stringent case studies\napplied to well-known open source python libraries pillow and numpy. We find\nthat contemporary LLM ChatGPT-4 (state September and October 2023) is\nsurprisingly adept at optimizing energy and compute efficiency. However, this\nis only the case in interactive use, with a human expert in the loop. Aware of\nexperimenter bias, we document our qualitative approach in detail, and provide\ntranscript and source code. We start by providing a detailed description of our\napproach in conversing with the LLM to optimize the _getextrema function in the\npillow library, and a quantitative evaluation of the performance improvement.\nTo demonstrate qualitative replicability, we report further attempts on another\nlocus in the pillow library, and one code locus in the numpy library, to\ndemonstrate generalization within and beyond a library. In all attempts, the\nperformance improvement is significant (factor up to 38). We have also not\nomitted reporting of failed attempts (there were none). We conclude that LLMs\nare a promising tool for code optimization in open source libraries, but that\nthe human expert in the loop is essential for success. Nonetheless, we were\nsurprised by how few iterations were required to achieve substantial\nperformance improvements that were not obvious to the expert in the loop. We\nwould like bring attention to the qualitative nature of this study, more robust\nquantitative studies would need to introduce a layer of selecting experts in a\nrepresentative sample -- we invite the community to collaborate.",
        "pdf_link": "https://arxiv.org/pdf/2312.14949v2.pdf"
    },
    {
        "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
        "authors": [
            "Luka Ribar",
            "Ivan Chelombiev",
            "Luke Hudlass-Galley",
            "Charlie Blake",
            "Carlo Luschi",
            "Douglas Orr"
        ],
        "published": "2023-12-08T11:47:35Z",
        "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data-transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data-transfers without substantial drops in accuracy, by evaluating\nLlama 2, Mistral and Pythia models on a wide range of downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.04985v3.pdf"
    },
    {
        "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "published": "2023-12-08T09:48:36Z",
        "summary": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video question answering (Video QA) tasks, utilizing video\ntokens as contextual input. However, employing LLMs for long video\nunderstanding presents significant challenges and remains under-explored. The\nextensive number of video tokens leads to considerable computational costs for\nLLMs while using aggregated tokens results in loss of vision details. Moreover,\nthe presence of abundant question-irrelevant tokens introduces noise to the\nvideo QA process. To address these issues, we introduce a simple yet effective\nretrieval-based video language model (R-VLM) for efficient and interpretable\nlong video QA. Specifically, given a question (query) and a long video, our\nmodel identifies and selects the most relevant $K$ video chunks and uses their\nassociated visual tokens to serve as context for the LLM inference. This\neffectively reduces the number of video tokens, eliminates noise interference,\nand enhances system performance. Our experimental results validate the\neffectiveness of our framework for comprehending long videos. Furthermore,\nbased on the retrieved chunks, our model is interpretable that provides the\njustifications on where we get the answers.",
        "pdf_link": "https://arxiv.org/pdf/2312.04931v1.pdf"
    },
    {
        "title": "Exploring the Limits of ChatGPT in Software Security Applications",
        "authors": [
            "Fangzhou Wu",
            "Qingzhao Zhang",
            "Ati Priya Bajaj",
            "Tiffany Bao",
            "Ning Zhang",
            "Ruoyu \"Fish\" Wang",
            "Chaowei Xiao"
        ],
        "published": "2023-12-08T03:02:37Z",
        "summary": "Large language models (LLMs) have undergone rapid evolution and achieved\nremarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or\nGPT-4, has gained instant popularity due to its strong capability across a wide\nrange of tasks, including natural language tasks, coding, mathematics, and\nengaging conversations. However, the impacts and limits of such LLMs in system\nsecurity domain are less explored. In this paper, we delve into the limits of\nLLMs (i.e., ChatGPT) in seven software security applications including\nvulnerability detection/repair, debugging, debloating, decompilation, patching,\nroot cause analysis, symbolic execution, and fuzzing. Our exploration reveals\nthat ChatGPT not only excels at generating code, which is the conventional\napplication of language models, but also demonstrates strong capability in\nunderstanding user-provided commands in natural languages, reasoning about\ncontrol and data flows within programs, generating complex data structures, and\neven decompiling assembly code. Notably, GPT-4 showcases significant\nimprovements over GPT-3.5 in most security tasks. Also, certain limitations of\nChatGPT in security-related tasks are identified, such as its constrained\nability to process long code contexts.",
        "pdf_link": "https://arxiv.org/pdf/2312.05275v1.pdf"
    },
    {
        "title": "DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions",
        "authors": [
            "Fangzhou Wu",
            "Xiaogeng Liu",
            "Chaowei Xiao"
        ],
        "published": "2023-12-07T22:19:06Z",
        "summary": "With the advancement of Large Language Models (LLMs), significant progress\nhas been made in code generation, enabling LLMs to transform natural language\ninto programming code. These Code LLMs have been widely accepted by massive\nusers and organizations. However, a dangerous nature is hidden in the code,\nwhich is the existence of fatal vulnerabilities. While some LLM providers have\nattempted to address these issues by aligning with human guidance, these\nefforts fall short of making Code LLMs practical and robust. Without a deep\nunderstanding of the performance of the LLMs under the practical worst cases,\nit would be concerning to apply them to various real-world applications. In\nthis paper, we answer the critical issue: Are existing Code LLMs immune to\ngenerating vulnerable code? If not, what is the possible maximum severity of\nthis issue in practical deployment scenarios? In this paper, we introduce\nDeceptPrompt, a novel algorithm that can generate adversarial natural language\ninstructions that drive the Code LLMs to generate functionality correct code\nwith vulnerabilities. DeceptPrompt is achieved through a systematic\nevolution-based algorithm with a fine grain loss design. The unique advantage\nof DeceptPrompt enables us to find natural prefix/suffix with totally benign\nand non-directional semantic meaning, meanwhile, having great power in inducing\nthe Code LLMs to generate vulnerable code. This feature can enable us to\nconduct the almost-worstcase red-teaming on these LLMs in a real scenario,\nwhere users are using natural language. Our extensive experiments and analyses\non DeceptPrompt not only validate the effectiveness of our approach but also\nshed light on the huge weakness of LLMs in the code generation task. When\napplying the optimized prefix/suffix, the attack success rate (ASR) will\nimprove by average 50% compared with no prefix/suffix applying.",
        "pdf_link": "https://arxiv.org/pdf/2312.04730v2.pdf"
    },
    {
        "title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models",
        "authors": [
            "Manish Bhatt",
            "Sahana Chennabasappa",
            "Cyrus Nikolaidis",
            "Shengye Wan",
            "Ivan Evtimov",
            "Dominik Gabi",
            "Daniel Song",
            "Faizan Ahmad",
            "Cornelius Aschermann",
            "Lorenzo Fontana",
            "Sasha Frolov",
            "Ravi Prakash Giri",
            "Dhaval Kapil",
            "Yiannis Kozyrakis",
            "David LeBlanc",
            "James Milazzo",
            "Aleksandar Straumann",
            "Gabriel Synnaeve",
            "Varun Vontimitta",
            "Spencer Whitman",
            "Joshua Saxe"
        ],
        "published": "2023-12-07T22:07:54Z",
        "summary": "This paper presents CyberSecEval, a comprehensive benchmark developed to help\nbolster the cybersecurity of Large Language Models (LLMs) employed as coding\nassistants. As what we believe to be the most extensive unified cybersecurity\nsafety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs\nin two crucial security domains: their propensity to generate insecure code and\ntheir level of compliance when asked to assist in cyberattacks. Through a case\nstudy involving seven models from the Llama 2, Code Llama, and OpenAI GPT large\nlanguage model families, CyberSecEval effectively pinpointed key cybersecurity\nrisks. More importantly, it offered practical insights for refining these\nmodels. A significant observation from the study was the tendency of more\nadvanced models to suggest insecure code, highlighting the critical need for\nintegrating security considerations in the development of sophisticated LLMs.\nCyberSecEval, with its automated test case generation and evaluation pipeline\ncovers a broad scope and equips LLM designers and researchers with a tool to\nbroadly measure and enhance the cybersecurity safety properties of LLMs,\ncontributing to the development of more secure AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.04724v1.pdf"
    },
    {
        "title": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models",
        "authors": [
            "Victor Agostinelli",
            "Max Wild",
            "Matthew Raffel",
            "Kazi Ahmed Asif Fuad",
            "Lizhong Chen"
        ],
        "published": "2023-12-07T20:42:05Z",
        "summary": "Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.",
        "pdf_link": "https://arxiv.org/pdf/2312.04691v2.pdf"
    },
    {
        "title": "Testing LLM performance on the Physics GRE: some observations",
        "authors": [
            "Pranav Gupta"
        ],
        "published": "2023-12-07T17:33:12Z",
        "summary": "With the recent developments in large language models (LLMs) and their\nwidespread availability through open source models and/or low-cost APIs,\nseveral exciting products and applications are emerging, many of which are in\nthe field of STEM educational technology for K-12 and university students.\nThere is a need to evaluate these powerful language models on several\nbenchmarks, in order to understand their risks and limitations. In this short\npaper, we summarize and analyze the performance of Bard, a popular LLM-based\nconversational service made available by Google, on the standardized Physics\nGRE examination.",
        "pdf_link": "https://arxiv.org/pdf/2312.04613v1.pdf"
    },
    {
        "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use",
        "authors": [
            "Yuhan Chen",
            "Ang Lv",
            "Ting-En Lin",
            "Changyu Chen",
            "Yuchuan Wu",
            "Fei Huang",
            "Yongbin Li",
            "Rui Yan"
        ],
        "published": "2023-12-07T17:24:51Z",
        "summary": "In this paper, we demonstrate that an inherent waveform pattern in the\nattention allocation of large language models (LLMs) significantly affects\ntheir performance in tasks demanding a high degree of context awareness, such\nas utilizing LLMs for tool-use. Specifically, the crucial information in the\ncontext will be potentially overlooked by model when it is positioned in the\ntrough zone of the attention waveform, leading to decreased performance. To\naddress this issue, we propose a novel inference method named Attention\nBuckets. It allows LLMs to process their input through multiple parallel\nprocesses. Each process utilizes a distinct base angle for the rotary position\nembedding, thereby creating a unique attention waveform. By compensating an\nattention trough of a particular process with an attention peak of another\nprocess, our approach enhances LLM's awareness to various contextual positions,\nthus mitigating the risk of overlooking crucial information. In the largest\ntool-use benchmark, our method elevates a 7B model to achieve state-of-the-art\nperformance, comparable to that of GPT-4. On other benchmarks and some RAG\ntasks, which also demand a thorough understanding of contextual content,\nAttention Buckets also exhibited notable enhancements in performance.",
        "pdf_link": "https://arxiv.org/pdf/2312.04455v3.pdf"
    },
    {
        "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
        "authors": [
            "Yuechen Zhang",
            "Shengju Qian",
            "Bohao Peng",
            "Shu Liu",
            "Jiaya Jia"
        ],
        "published": "2023-12-07T13:53:29Z",
        "summary": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/",
        "pdf_link": "https://arxiv.org/pdf/2312.04302v2.pdf"
    },
    {
        "title": "Hijacking Context in Large Multi-modal Models",
        "authors": [
            "Joonhyun Jeong"
        ],
        "published": "2023-12-07T11:23:29Z",
        "summary": "Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to\nunderstand the visual contents of images given the instructions regarding the\nimages. Built upon the Large Language Models (LLMs), LMMs also inherit their\nabilities and characteristics such as in-context learning where a coherent\nsequence of images and texts are given as the input prompt. However, we\nidentify a new limitation of off-the-shelf LMMs where a small fraction of\nincoherent images or text descriptions mislead LMMs to only generate biased\noutput about the hijacked context, not the originally intended context. To\naddress this, we propose a pre-filtering method that removes irrelevant\ncontexts via GPT-4V, based on its robustness towards distribution shift within\nthe contexts. We further investigate whether replacing the hijacked visual and\ntextual contexts with the correlated ones via GPT-4V and text-to-image models\ncan help yield coherent responses.",
        "pdf_link": "https://arxiv.org/pdf/2312.07553v1.pdf"
    },
    {
        "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak",
        "authors": [
            "Yanrui Du",
            "Sendong Zhao",
            "Ming Ma",
            "Yuhan Chen",
            "Bing Qin"
        ],
        "published": "2023-12-07T08:29:58Z",
        "summary": "Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, LLMs still tend to generate harmful responses\nwhen faced with malicious instructions, a phenomenon referred to as \"Jailbreak\nAttack\". In our research, we introduce a novel automatic jailbreak method\nRADIAL, which bypasses the security mechanism by amplifying the potential of\nLLMs to generate affirmation responses. The jailbreak idea of our method is\n\"Inherent Response Tendency Analysis\" which identifies real-world instructions\nthat can inherently induce LLMs to generate affirmation responses and the\ncorresponding jailbreak strategy is \"Real-World Instructions-Driven Jailbreak\"\nwhich involves strategically splicing real-world instructions identified\nthrough the above analysis around the malicious instruction. Our method\nachieves excellent attack performance on English malicious instructions with\nfive open-source advanced LLMs while maintaining robust attack performance in\nexecuting cross-language attacks against Chinese malicious instructions. We\nconduct experiments to verify the effectiveness of our jailbreak idea and the\nrationality of our jailbreak strategy design. Notably, our method designed a\nsemantically coherent attack prompt, highlighting the potential risks of LLMs.\nOur study provides detailed insights into jailbreak attacks, establishing a\nfoundation for the development of safer LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.04127v2.pdf"
    },
    {
        "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss",
        "authors": [
            "Zhuoran Huang",
            "Michael P. Berry",
            "Christina Chwyl",
            "Gary Hsieh",
            "Jing Wei",
            "Evan M. Forman"
        ],
        "published": "2023-12-07T05:45:24Z",
        "summary": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.",
        "pdf_link": "https://arxiv.org/pdf/2312.04059v1.pdf"
    },
    {
        "title": "Large Language Models for Intent-Driven Session Recommendations",
        "authors": [
            "Zhu Sun",
            "Hongyang Liu",
            "Xinghua Qu",
            "Kaidong Feng",
            "Yan Wang",
            "Yew-Soon Ong"
        ],
        "published": "2023-12-07T02:25:14Z",
        "summary": "Intent-aware session recommendation (ISR) is pivotal in discerning user\nintents within sessions for precise predictions. Traditional approaches,\nhowever, face limitations due to their presumption of a uniform number of\nintents across all sessions. This assumption overlooks the dynamic nature of\nuser sessions, where the number and type of intentions can significantly vary.\nIn addition, these methods typically operate in latent spaces, thus hinder the\nmodel's transparency.Addressing these challenges, we introduce a novel ISR\napproach, utilizing the advanced reasoning capabilities of large language\nmodels (LLMs). First, this approach begins by generating an initial prompt that\nguides LLMs to predict the next item in a session, based on the varied intents\nmanifested in user sessions. Then, to refine this process, we introduce an\ninnovative prompt optimization mechanism that iteratively self-reflects and\nadjusts prompts. Furthermore, our prompt selection module, built upon the LLMs'\nbroad adaptability, swiftly selects the most optimized prompts across diverse\ndomains. This new paradigm empowers LLMs to discern diverse user intents at a\nsemantic level, leading to more accurate and interpretable session\nrecommendations. Our extensive experiments on three real-world datasets\ndemonstrate the effectiveness of our method, marking a significant advancement\nin ISR systems.",
        "pdf_link": "https://arxiv.org/pdf/2312.07552v1.pdf"
    },
    {
        "title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration",
        "authors": [
            "Meihao Fan",
            "Xiaoyue Han",
            "Ju Fan",
            "Chengliang Chai",
            "Nan Tang",
            "Guoliang Li",
            "Xiaoyong Du"
        ],
        "published": "2023-12-07T02:09:27Z",
        "summary": "Entity resolution (ER) is an important data integration task with a wide\nspectrum of applications. The state-of-the-art solutions on ER rely on\npre-trained language models (PLMs), which require fine-tuning on a lot of\nlabeled matching/non-matching entity pairs. Recently, large languages models\n(LLMs), such as GPT-4, have shown the ability to perform many tasks without\ntuning model parameters, which is known as in-context learning (ICL) that\nfacilitates effective learning from a few labeled input context demonstrations.\nHowever, existing ICL approaches to ER typically necessitate providing a task\ndescription and a set of demonstrations for each entity pair and thus have\nlimitations on the monetary cost of interfacing LLMs. To address the problem,\nin this paper, we provide a comprehensive study to investigate how to develop a\ncost-effective batch prompting approach to ER. We introduce a framework BATCHER\nconsisting of demonstration selection and question batching and explore\ndifferent design choices that support batch prompting for ER. We also devise a\ncovering-based demonstration selection strategy that achieves an effective\nbalance between matching accuracy and monetary cost. We conduct a thorough\nevaluation to explore the design space and evaluate our proposed strategies.\nThrough extensive experiments, we find that batch prompting is very\ncost-effective for ER, compared with not only PLM-based methods fine-tuned with\nextensive labeled data but also LLM-based methods with manually designed\nprompting. We also provide guidance for selecting appropriate design choices\nfor batch prompting.",
        "pdf_link": "https://arxiv.org/pdf/2312.03987v1.pdf"
    },
    {
        "title": "Understanding (Un)Intended Memorization in Text-to-Image Generative Models",
        "authors": [
            "Ali Naseh",
            "Jaechul Roh",
            "Amir Houmansadr"
        ],
        "published": "2023-12-06T19:53:17Z",
        "summary": "Multimodal machine learning, especially text-to-image models like Stable\nDiffusion and DALL-E 3, has gained significance for transforming text into\ndetailed images.\n  Despite their growing use and remarkable generative capabilities, there is a\npressing need for a detailed examination of these models' behavior,\nparticularly with respect to memorization. Historically, memorization in\nmachine learning has been context-dependent, with diverse definitions emerging\nfrom classification tasks to complex models like Large Language Models (LLMs)\nand Diffusion models. Yet, a definitive concept of memorization that aligns\nwith the intricacies of text-to-image synthesis remains elusive. This\nunderstanding is vital as memorization poses privacy risks yet is essential for\nmeeting user expectations, especially when generating representations of\nunderrepresented entities. In this paper, we introduce a specialized definition\nof memorization tailored to text-to-image models, categorizing it into three\ndistinct types according to user expectations. We closely examine the subtle\ndistinctions between intended and unintended memorization, emphasizing the\nimportance of balancing user privacy with the generative quality of the model\noutputs. Using the Stable Diffusion model, we offer examples to validate our\nmemorization definitions and clarify their application.",
        "pdf_link": "https://arxiv.org/pdf/2312.07550v1.pdf"
    },
    {
        "title": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management",
        "authors": [
            "Huan Wang",
            "Yan-Fu Li",
            "Min Xie"
        ],
        "published": "2023-12-06T15:24:01Z",
        "summary": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.",
        "pdf_link": "https://arxiv.org/pdf/2312.14945v1.pdf"
    },
    {
        "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification",
        "authors": [
            "Chengguang Gan",
            "Qinghao Zhang",
            "Tatsunori Mori"
        ],
        "published": "2023-12-06T12:34:46Z",
        "summary": "The proliferation of Large Language Models (LLMs) has spurred extensive\nresearch into LLM-related Prompt investigations, such as Instruction Learning\n(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches\naim to improve LLMs' responses by enabling them to provide concise statements\nor examples for deeper contemplation when addressing questions. However,\nindependent thinking by LLMs can introduce variability in their thought\nprocesses, leading to potential inaccuracies. In response, our study seeks to\nbridge the gap between LLM and human-like thinking processes, recognizing that\ntext comprehension begins with understanding individual words. To tackle this\nchallenge, we have expanded the CoT method to cater to a specific domain. Our\napproach, known as \"Think from Words\" (TFW), initiates the comprehension\nprocess at the word level and then extends it to encompass the entire text. We\nalso propose \"TFW with Extra word-level information\" (TFW Extra), augmenting\ncomprehension with additional word-level data. To assess our methods, we employ\ntext classification on six Japanese datasets comprising text-level and\nword-level elements. Our findings not only validate the effectiveness of TFW\nbut also shed light on the impact of various word-level information types on\nLLMs' text comprehension, offering insights into their potential to cause\nmisinterpretations and errors in the overall comprehension of the final text.",
        "pdf_link": "https://arxiv.org/pdf/2312.03458v1.pdf"
    },
    {
        "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM",
        "authors": [
            "Jiayi Pan",
            "Chengcan Wang",
            "Kaifu Zheng",
            "Yangguang Li",
            "Zhenyu Wang",
            "Bin Feng"
        ],
        "published": "2023-12-06T11:10:55Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.",
        "pdf_link": "https://arxiv.org/pdf/2312.03788v1.pdf"
    },
    {
        "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
        "authors": [
            "Kan Hatakeyama-Sato",
            "Yasuhiko Igarashi",
            "Shun Katakami",
            "Yuta Nabae",
            "Teruaki Hayakawa"
        ],
        "published": "2023-12-06T08:55:55Z",
        "summary": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.",
        "pdf_link": "https://arxiv.org/pdf/2312.03360v2.pdf"
    },
    {
        "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science",
        "authors": [
            "Chenxi Wu",
            "Alan John Varghese",
            "Vivek Oommen",
            "George Em Karniadakis"
        ],
        "published": "2023-12-05T21:41:52Z",
        "summary": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific\nreviews, possibly using more unbiased quantitative metrics, facilitating\ncross-disciplinary connections, and identifying emerging trends and research\ngaps by analyzing large volumes of data. However, at the present time, they\nlack the required deep understanding of complex methodologies, they have\ndifficulty in evaluating innovative claims, and they are unable to assess\nethical issues and conflicts of interest. Herein, we consider 13 GPT-related\npapers across different scientific domains, reviewed by a human reviewer and\nSciSpace, a large language model, with the reviews evaluated by three distinct\ntypes of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that\n50% of SciSpace's responses to objective questions align with those of a human\nreviewer, with GPT-4 (informed evaluator) often rating the human reviewer\nhigher in accuracy, and SciSpace higher in structure, clarity, and\ncompleteness. In subjective questions, the uninformed evaluators (GPT-3.5 and\ncrowd panel) showed varying preferences between SciSpace and human responses,\nwith the crowd panel showing a preference for the human responses. However,\nGPT-4 rated them equally in accuracy and structure but favored SciSpace for\ncompleteness.",
        "pdf_link": "https://arxiv.org/pdf/2312.03769v1.pdf"
    },
    {
        "title": "LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications",
        "authors": [
            "Brett Israelsen",
            "Soumalya Sarkar"
        ],
        "published": "2023-12-05T19:04:50Z",
        "summary": "Large Language Models have seen rapid progress in capability in recent years;\nthis progress has been accelerating and their capabilities, measured by various\nbenchmarks, are beginning to approach those of humans. There is a strong demand\nto use such models in a wide variety of applications but, due to unresolved\nvulnerabilities and limitations, great care needs to be used before applying\nthem to intelligence and safety-critical applications. This paper reviews\nrecent literature related to LLM assessment and vulnerabilities to synthesize\nthe current research landscape and to help understand what advances are most\ncritical to enable use of of these technologies in intelligence and\nsafety-critical applications. The vulnerabilities are broken down into ten\nhigh-level categories and overlaid onto a high-level life cycle of an LLM. Some\ngeneral categories of mitigations are reviewed.",
        "pdf_link": "https://arxiv.org/pdf/2312.03088v1.pdf"
    },
    {
        "title": "Clinical Notes Reveal Physician Fatigue",
        "authors": [
            "Chao-Chun Hsu",
            "Ziad Obermeyer",
            "Chenhao Tan"
        ],
        "published": "2023-12-05T19:00:18Z",
        "summary": "Physicians write notes about patients. In doing so, they reveal much about\nthemselves. Using data from 129,228 emergency room visits, we train a model to\nidentify notes written by fatigued physicians -- those who worked 5 or more of\nthe prior 7 days. In a hold-out set, the model accurately identifies notes\nwritten by these high-workload physicians, and also flags notes written in\nother high-fatigue settings: on overnight shifts, and after high patient\nvolumes. Model predictions also correlate with worse decision-making on at\nleast one important metric: yield of testing for heart attack is 18% lower with\neach standard deviation increase in model-predicted fatigue. Finally, the model\nindicates that notes written about Black and Hispanic patients have 12% and 21%\nhigher predicted fatigue than Whites -- larger than overnight vs. daytime\ndifferences. These results have an important implication for large language\nmodels (LLMs). Our model indicates that fatigued doctors write more predictable\nnotes. Perhaps unsurprisingly, because word prediction is the core of how LLMs\nwork, we find that LLM-written notes have 17% higher predicted fatigue than\nreal physicians' notes. This indicates that LLMs may introduce distortions in\ngenerated text that are not yet fully understood.",
        "pdf_link": "https://arxiv.org/pdf/2312.03077v1.pdf"
    },
    {
        "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
        "authors": [
            "Yushi Hu",
            "Otilia Stretcu",
            "Chun-Ta Lu",
            "Krishnamurthy Viswanathan",
            "Kenji Hata",
            "Enming Luo",
            "Ranjay Krishna",
            "Ariel Fuxman"
        ],
        "published": "2023-12-05T18:58:37Z",
        "summary": "Solving complex visual tasks such as \"Who invented the musical instrument on\nthe right?\" involves a composition of skills: understanding space, recognizing\ninstruments, and also retrieving prior knowledge. Recent work shows promise by\ndecomposing such tasks using a large language model (LLM) into an executable\nprogram that invokes specialized vision models. However, generated programs are\nerror-prone: they omit necessary steps, include spurious ones, and are unable\nto recover when the specialized models give incorrect outputs. Moreover, they\nrequire loading multiple models, incurring high latency and computation costs.\nWe propose Visual Program Distillation (VPD), an instruction tuning framework\nthat produces a vision-language model (VLM) capable of solving complex visual\ntasks with a single forward pass. VPD distills the reasoning ability of LLMs by\nusing them to sample multiple candidate programs, which are then executed and\nverified to identify a correct one. It translates each correct program into a\nlanguage description of the reasoning steps, which are then distilled into a\nVLM. Extensive experiments show that VPD improves the VLM's ability to count,\nunderstand spatial relations, and reason compositionally. Our VPD-trained\nPaLI-X outperforms all prior VLMs, achieving state-of-the-art performance\nacross complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,\nand Hateful Memes. An evaluation with human annotators also confirms that VPD\nimproves model response factuality and consistency. Finally, experiments on\ncontent moderation demonstrate that VPD is also helpful for adaptation to\nreal-world applications with limited data.",
        "pdf_link": "https://arxiv.org/pdf/2312.03052v2.pdf"
    },
    {
        "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models",
        "authors": [
            "Xinyu Zhang",
            "Sebastian Hofst√§tter",
            "Patrick Lewis",
            "Raphael Tang",
            "Jimmy Lin"
        ],
        "published": "2023-12-05T18:57:40Z",
        "summary": "Listwise rerankers based on large language models (LLM) are the zero-shot\nstate-of-the-art. However, current works in this direction all depend on the\nGPT models, making it a single point of failure in scientific reproducibility.\nMoreover, it raises the concern that the current research findings only hold\nfor GPT models but not LLM in general. In this work, we lift this pre-condition\nand build for the first time effective listwise rerankers without any form of\ndependency on GPT. Our passage retrieval experiments show that our best list se\nreranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves\n97% effectiveness of the ones built on GPT-4. Our results also show that the\nexisting training datasets, which were expressly constructed for pointwise\nranking, are insufficient for building such listwise rerankers. Instead,\nhigh-quality listwise ranking data is required and crucial, calling for further\nwork on building human-annotated listwise data resources.",
        "pdf_link": "https://arxiv.org/pdf/2312.02969v1.pdf"
    },
    {
        "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions",
        "authors": [
            "Zahra Abbasiantaeb",
            "Yifei Yuan",
            "Evangelos Kanoulas",
            "Mohammad Aliannejadi"
        ],
        "published": "2023-12-05T17:38:02Z",
        "summary": "Conversational question-answering (CQA) systems aim to create interactive\nsearch systems that effectively retrieve information by interacting with users.\nTo replicate human-to-human conversations, existing work uses human annotators\nto play the roles of the questioner (student) and the answerer (teacher).\nDespite its effectiveness, challenges exist as human annotation is\ntime-consuming, inconsistent, and not scalable. To address this issue and\ninvestigate the applicability of large language models (LLMs) in CQA\nsimulation, we propose a simulation framework that employs zero-shot learner\nLLMs for simulating teacher-student interactions. Our framework involves two\nLLMs interacting on a specific topic, with the first LLM acting as a student,\ngenerating questions to explore a given search topic. The second LLM plays the\nrole of a teacher by answering questions and is equipped with additional\ninformation, including a text on the given topic. We implement both the student\nand teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness\nof LLMs in simulating CQA interactions and understand the disparities between\nLLM- and human-generated conversations, we evaluate the simulated data from\nvarious perspectives. We begin by evaluating the teacher's performance through\nboth automatic and human assessment. Next, we evaluate the performance of the\nstudent, analyzing and comparing the disparities between questions generated by\nthe LLM and those generated by humans. Furthermore, we conduct extensive\nanalyses to thoroughly examine the LLM performance by benchmarking\nstate-of-the-art reading comprehension models on both datasets. Our results\nreveal that the teacher LLM generates lengthier answers that tend to be more\naccurate and complete. The student LLM generates more diverse questions,\ncovering more aspects of a given topic.",
        "pdf_link": "https://arxiv.org/pdf/2312.02913v1.pdf"
    },
    {
        "title": "Inherent limitations of LLMs regarding spatial information",
        "authors": [
            "He Yan",
            "Xinyao Hu",
            "Xiangpeng Wan",
            "Chengyu Huang",
            "Kai Zou",
            "Shiqi Xu"
        ],
        "published": "2023-12-05T16:02:20Z",
        "summary": "Despite the significant advancements in natural language processing\ncapabilities demonstrated by large language models such as ChatGPT, their\nproficiency in comprehending and processing spatial information, especially\nwithin the domains of 2D and 3D route planning, remains notably underdeveloped.\nThis paper investigates the inherent limitations of ChatGPT and similar models\nin spatial reasoning and navigation-related tasks, an area critical for\napplications ranging from autonomous vehicle guidance to assistive technologies\nfor the visually impaired. In this paper, we introduce a novel evaluation\nframework complemented by a baseline dataset, meticulously crafted for this\nstudy. This dataset is structured around three key tasks: plotting spatial\npoints, planning routes in two-dimensional (2D) spaces, and devising pathways\nin three-dimensional (3D) environments. We specifically developed this dataset\nto assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals\nkey insights into the model's capabilities and limitations in spatial\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2312.03042v1.pdf"
    },
    {
        "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
        "authors": [
            "Miriam Rateike",
            "Celia Cintas",
            "John Wamburu",
            "Tanya Akumu",
            "Skyler Speakman"
        ],
        "published": "2023-12-05T14:35:11Z",
        "summary": "We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.",
        "pdf_link": "https://arxiv.org/pdf/2312.02798v1.pdf"
    },
    {
        "title": "How should the advent of large language models affect the practice of science?",
        "authors": [
            "Marcel Binz",
            "Stephan Alaniz",
            "Adina Roskies",
            "Balazs Aczel",
            "Carl T. Bergstrom",
            "Colin Allen",
            "Daniel Schad",
            "Dirk Wulff",
            "Jevin D. West",
            "Qiong Zhang",
            "Richard M. Shiffrin",
            "Samuel J. Gershman",
            "Ven Popov",
            "Emily M. Bender",
            "Marco Marelli",
            "Matthew M. Botvinick",
            "Zeynep Akata",
            "Eric Schulz"
        ],
        "published": "2023-12-05T10:45:12Z",
        "summary": "Large language models (LLMs) are being increasingly incorporated into\nscientific workflows. However, we have yet to fully grasp the implications of\nthis integration. How should the advent of large language models affect the\npractice of science? For this opinion piece, we have invited four diverse\ngroups of scientists to reflect on this query, sharing their perspectives and\nengaging in debate. Schulz et al. make the argument that working with LLMs is\nnot fundamentally different from working with human collaborators, while Bender\net al. argue that LLMs are often misused and over-hyped, and that their\nlimitations warrant a focus on more specialized, easily interpretable tools.\nMarelli et al. emphasize the importance of transparent attribution and\nresponsible use of LLMs. Finally, Botvinick and Gershman advocate that humans\nshould retain responsibility for determining the scientific roadmap. To\nfacilitate the discussion, the four perspectives are complemented with a\nresponse from each group. By putting these different perspectives in\nconversation, we aim to bring attention to important considerations within the\nacademic community regarding the adoption of LLMs and their impact on both\ncurrent and future scientific practices.",
        "pdf_link": "https://arxiv.org/pdf/2312.03759v1.pdf"
    },
    {
        "title": "Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety",
        "authors": [
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-12-05T06:13:55Z",
        "summary": "Explainability and Safety engender Trust. These require a model to exhibit\nconsistency and reliability. To achieve these, it is necessary to use and\nanalyze data and knowledge with statistical and symbolic AI methods relevant to\nthe AI application - neither alone will do. Consequently, we argue and seek to\ndemonstrate that the NeuroSymbolic AI approach is better suited for making AI a\ntrusted AI system. We present the CREST framework that shows how Consistency,\nReliability, user-level Explainability, and Safety are built on NeuroSymbolic\nmethods that use data and knowledge to support requirements for critical\napplications such as health and well-being. This article focuses on Large\nLanguage Models (LLMs) as the chosen AI system within the CREST framework. LLMs\nhave garnered substantial attention from researchers due to their versatility\nin handling a broad array of natural language processing (NLP) scenarios. For\nexample, ChatGPT and Google's MedPaLM have emerged as highly promising\nplatforms for providing information in general and health-related queries,\nrespectively. Nevertheless, these models remain black boxes despite\nincorporating human feedback and instruction-guided tuning. For instance,\nChatGPT can generate unsafe responses despite instituting safety guardrails.\nCREST presents a plausible approach harnessing procedural and graph-based\nknowledge within a NeuroSymbolic framework to shed light on the challenges\nassociated with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.06798v1.pdf"
    },
    {
        "title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks",
        "authors": [
            "Chi Zhang",
            "Penglin Cai",
            "Yuhui Fu",
            "Haoqi Yuan",
            "Zongqing Lu"
        ],
        "published": "2023-12-05T06:00:52Z",
        "summary": "We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https://github.com/PKU-RL/Creative-Agents).",
        "pdf_link": "https://arxiv.org/pdf/2312.02519v1.pdf"
    },
    {
        "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
        "authors": [
            "Xinhang Li",
            "Chong Chen",
            "Xiangyu Zhao",
            "Yong Zhang",
            "Chunxiao Xing"
        ],
        "published": "2023-12-05T02:50:18Z",
        "summary": "The recent advancements in Large Language Models (LLMs) have sparked interest\nin harnessing their potential within recommender systems. Since LLMs are\ndesigned for natural language tasks, existing recommendation approaches have\npredominantly transformed recommendation tasks into open-domain natural\nlanguage generation tasks. However, this approach necessitates items to possess\nrich semantic information, often generates out-of-range results, and suffers\nfrom notably low efficiency and limited extensibility. Furthermore, practical\nID-based recommendation strategies, reliant on a huge number of unique\nidentities (IDs) to represent users and items, have gained prominence in\nreal-world recommender systems due to their effectiveness and efficiency.\nNevertheless, the incapacity of LLMs to model IDs presents a formidable\nchallenge when seeking to leverage LLMs for personalized recommendations. In\nthis paper, we introduce an Elegant Effective Efficient Extensible solution for\nlarge language models for Sequential Recommendation (E4SRec), which seamlessly\nintegrates LLMs with traditional recommender systems that exclusively utilize\nIDs to represent items. Specifically, E4SRec takes ID sequences as inputs,\nensuring that the generated outputs fall within the candidate lists.\nFurthermore, E4SRec possesses the capability to generate the entire ranking\nlist in a single forward process, and demands only a minimal set of pluggable\nparameters, which are trained for each dataset while keeping the entire LLM\nfrozen. We substantiate the effectiveness, efficiency, and extensibility of our\nproposed E4SRec through comprehensive experiments conducted on four widely-used\nreal-world datasets. The implementation code is accessible at\nhttps://github.com/HestiaSky/E4SRec/.",
        "pdf_link": "https://arxiv.org/pdf/2312.02443v1.pdf"
    },
    {
        "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation",
        "authors": [
            "Shanshan Zhong",
            "Zhongzhan Huang",
            "Shanghua Gao",
            "Wushao Wen",
            "Liang Lin",
            "Marinka Zitnik",
            "Pan Zhou"
        ],
        "published": "2023-12-05T02:41:57Z",
        "summary": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason\nstep-by-step, and can motivate their logical reasoning ability. While effective\nfor logical tasks, CoT is not conducive to creative problem-solving which often\nrequires out-of-box thoughts and is crucial for innovation advancements. In\nthis paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a\nnon-sequential, creative paradigm involving strong associations and knowledge\nleaps. To this end, we study LLMs on the popular Oogiri game which needs\nparticipants to have good creativity and strong associative thinking for\nresponding unexpectedly and humorously to the given image, text, or both, and\nthus is suitable for LoT study. Then to investigate LLMs' LoT ability in the\nOogiri game, we first build a multimodal and multilingual Oogiri-GO dataset\nwhich contains over 130,000 samples from the Oogiri game, and observe the\ninsufficient LoT ability or failures of most existing LLMs on the Oogiri game.\nAccordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve\nLLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into\nLoT-oriented instruction tuning data to train pretrained LLM for achieving\ncertain LoT humor generation and discrimination abilities. Then CLoT designs an\nexplorative self-refinement that encourages the LLM to generate more creative\nLoT data via exploring parallels between seemingly unrelated concepts and\nselects high-quality data to train itself for self-refinement. CLoT not only\nexcels in humor generation in the Oogiri game but also boosts creative\nabilities in various tasks like cloud guessing game and divergent association\ntask. These findings advance our understanding and offer a pathway to improve\nLLMs' creative capacities for innovative applications across domains. The\ndataset, code, and models will be released online.\nhttps://zhongshsh.github.io/CLoT/.",
        "pdf_link": "https://arxiv.org/pdf/2312.02439v2.pdf"
    },
    {
        "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following",
        "authors": [
            "Renze Lou",
            "Kai Zhang",
            "Jian Xie",
            "Yuxuan Sun",
            "Janice Ahn",
            "Hanzi Xu",
            "Yu Su",
            "Wenpeng Yin"
        ],
        "published": "2023-12-05T02:32:08Z",
        "summary": "In the realm of large language models (LLMs), enhancing instruction-following\ncapability often involves curating expansive training data. This is achieved\nthrough two primary schemes: i) Scaling-Inputs: Amplifying (input, output)\npairs per task instruction, aiming for better instruction adherence. ii)\nScaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,\noutput) pair (without requiring a separate input anymore). However, LLMs under\nScaling-Inputs tend to be overly sensitive to inputs, leading to\nmisinterpretation or non-compliance with instructions. Conversely, Scaling\nInput-Free Tasks demands a substantial number of tasks but is less effective in\ninstruction following when dealing with instances in Scaling-Inputs. This work\nintroduces MUFFIN, a new scheme of instruction-following dataset curation.\nSpecifically, we automatically Scale Tasks per Input by diversifying these\ntasks with various input facets. Experimental results across four zero-shot\nbenchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,\nreveal that LLMs, at various scales, trained on MUFFIN generally demonstrate\nsuperior instruction-following capabilities compared to those trained on the\ntwo aforementioned schemes.",
        "pdf_link": "https://arxiv.org/pdf/2312.02436v3.pdf"
    },
    {
        "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data",
        "authors": [
            "Yu Yang",
            "Aaditya K. Singh",
            "Mostafa Elhoushi",
            "Anas Mahmoud",
            "Kushal Tirumala",
            "Fabian Gloeckle",
            "Baptiste Rozi√®re",
            "Carole-Jean Wu",
            "Ari S. Morcos",
            "Newsha Ardalani"
        ],
        "published": "2023-12-05T01:19:30Z",
        "summary": "Code datasets, often collected from diverse and uncontrolled sources such as\nGitHub, potentially suffer from quality issues, thereby affecting the\nperformance and training efficiency of Large Language Models (LLMs) optimized\nfor code generation. Previous studies demonstrated the benefit of using\nembedding spaces for data pruning, but they mainly focused on duplicate removal\nor increasing variety, and in other modalities, such as images. Our work\nfocuses on using embeddings to identify and remove \"low-quality\" code data.\nFirst, we explore features of \"low-quality\" code in embedding space, through\nthe use of synthetic corruptions. Armed with this knowledge, we devise novel\npruning metrics that operate in embedding space to identify and remove\nlow-quality entries in the Stack dataset. We demonstrate the benefits of this\nsynthetic corruption informed pruning (SCIP) approach on the well-established\nHumanEval and MBPP benchmarks, outperforming existing embedding-based methods.\nImportantly, we achieve up to a 3% performance improvement over no pruning,\nthereby showing the promise of insights from synthetic corruptions for data\npruning.",
        "pdf_link": "https://arxiv.org/pdf/2312.02418v1.pdf"
    },
    {
        "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
        "authors": [
            "Karanpartap Singh",
            "James Zou"
        ],
        "published": "2023-12-04T22:56:31Z",
        "summary": "With the increasing use of large-language models (LLMs) like ChatGPT,\nwatermarking has emerged as a promising approach for tracing machine-generated\ncontent. However, research on LLM watermarking often relies on simple\nperplexity or diversity-based measures to assess the quality of watermarked\ntext, which can mask important limitations in watermarking. Here we introduce\ntwo new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)\nevaluation by LLM-judger with specific guidelines; and 2) binary classification\non text embeddings to distinguish between watermarked and unwatermarked text.\nWe apply these methods to characterize the effectiveness of current\nwatermarking techniques. Our experiments, conducted across various datasets,\nreveal that current watermarking methods are detectable by even simple\nclassifiers, challenging the notion of watermarking subtlety. We also found,\nthrough the LLM judger, that watermarking impacts text quality, especially in\ndegrading the coherence and depth of the response. Our findings underscore the\ntrade-off between watermark robustness and text quality and highlight the\nimportance of having more informative metrics to assess watermarking quality.",
        "pdf_link": "https://arxiv.org/pdf/2312.02382v1.pdf"
    },
    {
        "title": "Competition-Level Problems are Effective LLM Evaluators",
        "authors": [
            "Yiming Huang",
            "Zhenghao Lin",
            "Xiao Liu",
            "Yeyun Gong",
            "Shuai Lu",
            "Fangyu Lei",
            "Yaobo Liang",
            "Yelong Shen",
            "Chen Lin",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-12-04T18:58:57Z",
        "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.",
        "pdf_link": "https://arxiv.org/pdf/2312.02143v2.pdf"
    },
    {
        "title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques",
        "authors": [
            "Amir Panahandeh",
            "Hanie Asemi",
            "Esmaeil Nourani"
        ],
        "published": "2023-12-04T18:52:26Z",
        "summary": "Recent advances in language models (LMs), have demonstrated significant\nefficacy in tasks related to the arts and humanities. While LMs have exhibited\nexceptional performance across a wide range of natural language processing\ntasks, there are notable challenges associated with their utilization on small\ndatasets and their ability to replicate more creative human capacities. In this\nstudy, we aim to address these challenges by training a Persian classical\npoetry generation model using a transformer architecture on a specialized\ndataset with no pretraining. Additionally, we propose a novel decoding method\nto enhance coherence and meaningfulness in the generated poetry, effectively\nmanaging the tradeoff between diversity and quality. Furthermore, the results\nof our training approach and the proposed decoding method are evaluated through\ncomprehensive set of automatic and human evaluations and showed its superior\ncapability to generate coherent and meaningful poetry in compare to other\ndecoding methods and an existing Persian large language model (LLM).",
        "pdf_link": "https://arxiv.org/pdf/2312.02125v2.pdf"
    },
    {
        "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
        "authors": [
            "Giovanni Monea",
            "Maxime Peyrard",
            "Martin Josifoski",
            "Vishrav Chaudhary",
            "Jason Eisner",
            "Emre Kƒ±cƒ±man",
            "Hamid Palangi",
            "Barun Patra",
            "Robert West"
        ],
        "published": "2023-12-04T17:35:42Z",
        "summary": "Large language models (LLMs) have an impressive ability to draw on novel\ninformation supplied in their context. Yet the mechanisms underlying this\ncontextual grounding remain unknown, especially in situations where contextual\ninformation contradicts factual knowledge stored in the parameters, which LLMs\nalso excel at recalling. Favoring the contextual information is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify outdated or noisy\nstored knowledge. We present a novel method to study grounding abilities using\nFakepedia, a dataset of counterfactual texts constructed to clash with a\nmodel's internal parametric knowledge. We benchmark various LLMs with Fakepedia\nand then we conduct a causal mediation analysis, based on our Masked Grouped\nCausal Tracing (MGCT), on LLM components when answering Fakepedia queries.\nWithin this analysis, we identify distinct computational patterns between\ngrounded and ungrounded responses. We finally demonstrate that distinguishing\ngrounded from ungrounded responses is achievable through computational analysis\nalone. Our results, together with existing findings about factual recall\nmechanisms, provide a coherent narrative of how grounding and factual recall\nmechanisms interact within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.02073v2.pdf"
    },
    {
        "title": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?",
        "authors": [
            "Donya Rooein",
            "Amanda Cercas Curry",
            "Dirk Hovy"
        ],
        "published": "2023-12-04T17:19:53Z",
        "summary": "Large language models (LLMs) offer a range of new possibilities, including\nadapting the text to different audiences and their reading needs. But how well\ndo they adapt? We evaluate the readability of answers generated by four\nstate-of-the-art LLMs (commercial and open-source) to science questions when\nprompted to target different age groups and education levels. To assess the\nadaptability of LLMs to diverse audiences, we compare the readability scores of\nthe generated responses against the recommended comprehension level of each age\nand education group. We find large variations in the readability of the answers\nby different LLMs. Our results suggest LLM answers need to be better adapted to\nthe intended audience demographics to be more comprehensible. They underline\nthe importance of enhancing the adaptability of LLMs in education settings to\ncater to diverse age and education levels. Overall, current LLMs have set\nreadability ranges and do not adapt well to different audiences, even when\nprompted. That limits their potential for educational purposes.",
        "pdf_link": "https://arxiv.org/pdf/2312.02065v1.pdf"
    },
    {
        "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
        "authors": [
            "Zichao Li",
            "Ines Arous",
            "Siva Reddy",
            "Jackie C. K. Cheung"
        ],
        "published": "2023-12-04T12:45:30Z",
        "summary": "The potential of using a large language model (LLM) as a knowledge base (KB)\nhas sparked significant interest. To manage the knowledge acquired by LLMs, we\nneed to ensure that the editing of learned facts respects internal logical\nconstraints, which are known as dependency of knowledge. Existing work on\nediting LLMs has partially addressed the issue of dependency, when the editing\nof a fact should apply to its lexical variations without disrupting irrelevant\nones. However, they neglect the dependency between a fact and its logical\nimplications. We propose an evaluation protocol with an accompanying\nquestion-answering dataset, DepEdit, that provides a comprehensive assessment\nof the editing process considering the above notions of dependency. Our\nprotocol involves setting up a controlled environment in which we edit facts\nand monitor their impact on LLMs, along with their implications based on\nIf-Then rules. Extensive experiments on DepEdit show that existing knowledge\nediting methods are sensitive to the surface form of knowledge, and that they\nhave limited performance in inferring the implications of edited facts.",
        "pdf_link": "https://arxiv.org/pdf/2312.01858v1.pdf"
    },
    {
        "title": "Intelligent Virtual Assistants with LLM-based Process Automation",
        "authors": [
            "Yanchu Guan",
            "Dong Wang",
            "Zhixuan Chu",
            "Shiyu Wang",
            "Feiyue Ni",
            "Ruihua Song",
            "Longfei Li",
            "Jinjie Gu",
            "Chenyi Zhuang"
        ],
        "published": "2023-12-04T07:51:58Z",
        "summary": "While intelligent virtual assistants like Siri, Alexa, and Google Assistant\nhave become ubiquitous in modern life, they still face limitations in their\nability to follow multi-step instructions and accomplish complex goals\narticulated in natural language. However, recent breakthroughs in large\nlanguage models (LLMs) show promise for overcoming existing barriers by\nenhancing natural language processing and reasoning capabilities. Though\npromising, applying LLMs to create more advanced virtual assistants still faces\nchallenges like ensuring robust performance and handling variability in\nreal-world user commands. This paper proposes a novel LLM-based virtual\nassistant that can automatically perform multi-step operations within mobile\napps based on high-level user requests. The system represents an advance in\nassistants by providing an end-to-end solution for parsing instructions,\nreasoning about goals, and executing actions. LLM-based Process Automation\n(LLMPA) has modules for decomposing instructions, generating descriptions,\ndetecting interface elements, predicting next actions, and error checking.\nExperiments demonstrate the system completing complex mobile operation tasks in\nAlipay based on natural language instructions. This showcases how large\nlanguage models can enable automated assistants to accomplish real-world tasks.\nThe main contributions are the novel LLMPA architecture optimized for app\nprocess automation, the methodology for applying LLMs to mobile apps, and\ndemonstrations of multi-step task completion in a real-world environment.\nNotably, this work represents the first real-world deployment and extensive\nevaluation of a large language model-based virtual assistant in a widely used\nmobile application with an enormous user base numbering in the hundreds of\nmillions.",
        "pdf_link": "https://arxiv.org/pdf/2312.06677v1.pdf"
    },
    {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
        "authors": [
            "Lei Wang",
            "Jiabang He",
            "Shenshen Li",
            "Ning Liu",
            "Ee-Peng Lim"
        ],
        "published": "2023-12-04T07:43:02Z",
        "summary": "Large language models (LLMs) have shown remarkable performance in natural\nlanguage processing (NLP) tasks. To comprehend and execute diverse human\ninstructions over image data, instruction-tuned large vision-language models\n(LVLMs) have been introduced. However, LVLMs may suffer from different types of\nobject hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained\nobject hallucinations only (i.e., generated objects non-existent in the input\nimage). The fine-grained object attributes and behaviors non-existent in the\nimage may still be generated but not measured by the current evaluation\nmethods. In this paper, we thus focus on reducing fine-grained hallucinations\nof LVLMs. We propose \\textit{ReCaption}, a framework that consists of two\ncomponents: rewriting captions using ChatGPT and fine-tuning the\ninstruction-tuned LVLMs on the rewritten captions. We also propose a\nfine-grained probing-based evaluation method named \\textit{Fine-Grained Object\nHallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate\nthat ReCaption effectively reduces fine-grained object hallucination for\ndifferent LVLM options and improves their text generation quality. The code can\nbe found at https://github.com/Anonymousanoy/FOHE.",
        "pdf_link": "https://arxiv.org/pdf/2312.01701v1.pdf"
    },
    {
        "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation",
        "authors": [
            "Sunjae Lee",
            "Junyoung Choi",
            "Jungjae Lee",
            "Munim Hasan Wasi",
            "Hojun Choi",
            "Steven Y. Ko",
            "Sangeun Oh",
            "Insik Shin"
        ],
        "published": "2023-12-04T06:13:35Z",
        "summary": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MobileGPT, an innovative LLM-based mobile task automator\nequipped with a human-like app memory. MobileGPT emulates the cognitive process\nof humans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular sub-tasks that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on a dataset of 160 user instructions across 8 widely used mobile\napps. The results indicate that MobileGPT can automate and learn new tasks with\n82.5% accuracy, and is able to adapt them to different contexts with near\nperfect (98.75%) accuracy while reducing both latency and cost by 62.5% and\n68.8%, respectively, compared to the GPT-4 powered baseline.",
        "pdf_link": "https://arxiv.org/pdf/2312.03003v2.pdf"
    },
    {
        "title": "Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment",
        "authors": [
            "Cong-Duy Nguyen",
            "The-Anh Vu-Le",
            "Thong Nguyen",
            "Tho Quan",
            "Luu Anh Tuan"
        ],
        "published": "2023-12-04T03:16:48Z",
        "summary": "Language models have been supervised with both language-only objective and\nvisual grounding in existing studies of visual-grounded language learning.\nHowever, due to differences in the distribution and scale of visual-grounded\ndatasets and language corpora, the language model tends to mix up the context\nof the tokens that occurred in the grounded data with those that do not. As a\nresult, during representation learning, there is a mismatch between the visual\ninformation and the contextual meaning of the sentence. To overcome this\nlimitation, we propose GroundedBERT - a grounded language learning method that\nenhances the BERT representation with visually grounded information.\nGroundedBERT comprises two components: (i) the original BERT which captures the\ncontextual representation of words learned from the language corpora, and (ii)\na visual grounding module which captures visual information learned from\nvisual-grounded datasets. Moreover, we employ Optimal Transport (OT),\nspecifically its partial variant, to solve the fractional alignment problem\nbetween the two modalities. Our proposed method significantly outperforms the\nbaseline language models on various language tasks of the GLUE and SQuAD\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.01592v2.pdf"
    },
    {
        "title": "Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition",
        "authors": [
            "Chengyou Jia",
            "Minnan Luo",
            "Xiaojun Chang",
            "Zhuohang Dang",
            "Mingfei Han",
            "Mengmeng Wang",
            "Guang Dai",
            "Sizhe Dang",
            "Jingdong Wang"
        ],
        "published": "2023-12-04T02:31:38Z",
        "summary": "Exploring open-vocabulary video action recognition is a promising venture,\nwhich aims to recognize previously unseen actions within any arbitrary set of\ncategories. Existing methods typically adapt pretrained image-text models to\nthe video domain, capitalizing on their inherent strengths in generalization. A\ncommon thread among such methods is the augmentation of visual embeddings with\ntemporal information to improve the recognition of seen actions. Yet, they\ncompromise with standard less-informative action descriptions, thus faltering\nwhen confronted with novel actions. Drawing inspiration from human cognitive\nprocesses, we argue that augmenting text embeddings with human prior knowledge\nis pivotal for open-vocabulary video action recognition. To realize this, we\ninnovatively blend video models with Large Language Models (LLMs) to devise\nAction-conditioned Prompts. Specifically, we harness the knowledge in LLMs to\nproduce a set of descriptive sentences that contain distinctive features for\nidentifying given actions. Building upon this foundation, we further introduce\na multi-modal action knowledge alignment mechanism to align concepts in video\nand textual knowledge encapsulated within the prompts. Extensive experiments on\nvarious video benchmarks, including zero-shot, few-shot, and base-to-novel\ngeneralization settings, demonstrate that our method not only sets new SOTA\nperformance but also possesses excellent interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2312.02226v1.pdf"
    },
    {
        "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
        "authors": [
            "Vithya Yogarajan",
            "Gillian Dobbie",
            "Te Taka Keegan",
            "Rostam J. Neuwirth"
        ],
        "published": "2023-12-03T21:25:10Z",
        "summary": "The benefits and capabilities of pre-trained language models (LLMs) in\ncurrent and future innovations are vital to any society. However, introducing\nand using LLMs comes with biases and discrimination, resulting in concerns\nabout equality, diversity and fairness, and must be addressed. While\nunderstanding and acknowledging bias in LLMs and developing mitigation\nstrategies are crucial, the generalised assumptions towards societal needs can\nresult in disadvantages towards under-represented societies and indigenous\npopulations. Furthermore, the ongoing changes to actual and proposed amendments\nto regulations and laws worldwide also impact research capabilities in tackling\nthe bias problem. This research presents a comprehensive survey synthesising\nthe current trends and limitations in techniques used for identifying and\nmitigating bias in LLMs, where the overview of methods for tackling bias are\ngrouped into metrics, benchmark datasets, and mitigation strategies. The\nimportance and novelty of this survey are that it explores the perspective of\nunder-represented societies. We argue that current practices tackling the bias\nproblem cannot simply be 'plugged in' to address the needs of under-represented\nsocieties. We use examples from New Zealand to present requirements for\nadopting existing techniques to under-represented societies.",
        "pdf_link": "https://arxiv.org/pdf/2312.01509v1.pdf"
    },
    {
        "title": "D-Bot: Database Diagnosis System using Large Language Models",
        "authors": [
            "Xuanhe Zhou",
            "Guoliang Li",
            "Zhaoyan Sun",
            "Zhiyuan Liu",
            "Weize Chen",
            "Jianming Wu",
            "Jiesi Liu",
            "Ruohang Feng",
            "Guoyang Zeng"
        ],
        "published": "2023-12-03T16:58:10Z",
        "summary": "Database administrators (DBAs) play an important role in managing,\nmaintaining and optimizing database systems. However, it is hard and tedious\nfor DBAs to manage a large number of databases and give timely response\n(waiting for hours is intolerable in many online cases). In addition, existing\nempirical methods only support limited diagnosis scenarios, which are also\nlabor-intensive to update the diagnosis rules for database version updates.\nRecently large language models (LLMs) have shown great potential in various\nfields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can\nautomatically acquire knowledge from diagnosis documents, and generate\nreasonable and well-founded diagnosis report (i.e., identifying the root causes\nand solutions) within acceptable time (e.g., under 10 minutes compared to hours\nby a DBA). The techniques in D-Bot include (i) offline knowledge extraction\nfrom documents, (ii) automatic prompt generation (e.g., knowledge matching,\ntool retrieval), (iii) root cause analysis using tree search algorithm, and\n(iv) collaborative mechanism for complex anomalies with multiple root causes.\nWe verify D-Bot on real benchmarks (including 539 anomalies of six typical\napplications), and the results show that D-Bot can effectively analyze the root\ncauses of unseen anomalies and significantly outperforms traditional methods\nand vanilla models like GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2312.01454v2.pdf"
    },
    {
        "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
        "authors": [
            "James Enouen",
            "Hootan Nakhost",
            "Sayna Ebrahimi",
            "Sercan O Arik",
            "Yan Liu",
            "Tomas Pfister"
        ],
        "published": "2023-12-03T04:35:04Z",
        "summary": "Large language models (LLMs) have attracted huge interest in practical\napplications given their increasingly accurate responses and coherent reasoning\nabilities. Given their nature as black-boxes using complex reasoning processes\non their inputs, it is inevitable that the demand for scalable and faithful\nexplanations for LLMs' generated content will continue to grow. There have been\nmajor developments in the explainability of neural network models over the past\ndecade. Among them, post-hoc explainability methods, especially Shapley values,\nhave proven effective for interpreting deep learning models. However, there are\nmajor challenges in scaling up Shapley values for LLMs, particularly when\ndealing with long input contexts containing thousands of tokens and\nautoregressively generated output sequences. Furthermore, it is often unclear\nhow to effectively utilize generated explanations to improve the performance of\nLLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc\nexplanation method incorporating LM-specific techniques. We demonstrate that\nthis leads to significant increases in speed compared to conventional Shapley\nvalue computations, reducing processing times from hours to minutes for\ntoken-level explanations, and to just seconds for document-level explanations.\nIn addition, we demonstrate how real-time Shapley values can be utilized in two\nimportant scenarios, providing better understanding of long-document question\nanswering by localizing important words and sentences; and improving existing\ndocument retrieval systems through enhancing the accuracy of selected passages\nand ultimately the final responses.",
        "pdf_link": "https://arxiv.org/pdf/2312.01279v1.pdf"
    },
    {
        "title": "Running cognitive evaluations on large language models: The do's and the don'ts",
        "authors": [
            "Anna A. Ivanova"
        ],
        "published": "2023-12-03T04:28:19Z",
        "summary": "In this paper, I describe methodological considerations for studies that aim\nto evaluate the cognitive capacities of large language models (LLMs) using\nlanguage-based behavioral assessments. Drawing on three case studies from the\nliterature (a commonsense knowledge benchmark, a theory of mind evaluation, and\na test of syntactic agreement), I describe common pitfalls that might arise\nwhen applying a cognitive test to an LLM. I then list 10 do's and don'ts that\nshould help design high-quality cognitive evaluations for AI systems. I\nconclude by discussing four areas where the do's and don'ts are currently under\nactive discussion -- prompt sensitivity, cultural and linguistic diversity,\nusing LLMs as research assistants, and running evaluations on open vs. closed\nLLMs. Overall, the goal of the paper is to contribute to the broader discussion\nof best practices in the rapidly growing field of AI Psychology.",
        "pdf_link": "https://arxiv.org/pdf/2312.01276v1.pdf"
    },
    {
        "title": "Towards leveraging LLMs for Conditional QA",
        "authors": [
            "Syed-Amad Hussain",
            "Parag Pravin Dakle",
            "SaiKrishna Rallabandi",
            "Preethi Raghavan"
        ],
        "published": "2023-12-02T14:02:52Z",
        "summary": "This study delves into the capabilities and limitations of Large Language\nModels (LLMs) in the challenging domain of conditional question-answering.\nUtilizing the Conditional Question Answering (CQA) dataset and focusing on\ngenerative models like T5 and UL2, we assess the performance of LLMs across\ndiverse question types. Our findings reveal that fine-tuned LLMs can surpass\nthe state-of-the-art (SOTA) performance in some cases, even without fully\nencoding all input context, with an increase of 7-8 points in Exact Match (EM)\nand F1 scores for Yes/No questions. However, these models encounter challenges\nin extractive question answering, where they lag behind the SOTA by over 10\npoints, and in mitigating the risk of injecting false information. A study with\noracle-retrievers emphasizes the critical role of effective evidence retrieval,\nunderscoring the necessity for advanced solutions in this area. Furthermore, we\nhighlight the significant influence of evaluation metrics on performance\nassessments and advocate for a more comprehensive evaluation framework. The\ncomplexity of the task, the observed performance discrepancies, and the need\nfor effective evidence retrieval underline the ongoing challenges in this field\nand underscore the need for future work focusing on refining training tasks and\nexploring prompt-based techniques to enhance LLM performance in conditional\nquestion-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2312.01143v1.pdf"
    },
    {
        "title": "Large Language Models Are Zero-Shot Text Classifiers",
        "authors": [
            "Zhiqiang Wang",
            "Yiran Pang",
            "Yanbin Lin"
        ],
        "published": "2023-12-02T06:33:23Z",
        "summary": "Retrained large language models (LLMs) have become extensively used across\nvarious sub-disciplines of natural language processing (NLP). In NLP, text\nclassification problems have garnered considerable focus, but still faced with\nsome limitations related to expensive computational cost, time consumption, and\nrobust performance to unseen classes. With the proposal of chain of thought\nprompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with\nthe step by step reasoning prompts, instead of conventional question and answer\nformats. The zero-shot LLMs in the text classification problems can alleviate\nthese limitations by directly utilizing pretrained models to predict both seen\nand unseen classes. Our research primarily validates the capability of GPT\nmodels in text classification. We focus on effectively utilizing prompt\nstrategies to various text classification scenarios. Besides, we compare the\nperformance of zero shot LLMs with other state of the art text classification\nmethods, including traditional machine learning methods, deep learning methods,\nand ZSL methods. Experimental results demonstrate that the performance of LLMs\nunderscores their effectiveness as zero-shot text classifiers in three of the\nfour datasets analyzed. The proficiency is especially advantageous for small\nbusinesses or teams that may not have extensive knowledge in text\nclassification.",
        "pdf_link": "https://arxiv.org/pdf/2312.01044v1.pdf"
    },
    {
        "title": "Nash Learning from Human Feedback",
        "authors": [
            "R√©mi Munos",
            "Michal Valko",
            "Daniele Calandriello",
            "Mohammad Gheshlaghi Azar",
            "Mark Rowland",
            "Zhaohan Daniel Guo",
            "Yunhao Tang",
            "Matthieu Geist",
            "Thomas Mesnard",
            "Andrea Michi",
            "Marco Selvi",
            "Sertan Girgin",
            "Nikola Momchev",
            "Olivier Bachem",
            "Daniel J. Mankowitz",
            "Doina Precup",
            "Bilal Piot"
        ],
        "published": "2023-12-01T19:26:23Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the main\nparadigm for aligning large language models (LLMs) with human preferences.\nTypically, RLHF involves the initial step of learning a reward model from human\nfeedback, often expressed as preferences between pairs of text generations\nproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by\noptimizing it to maximize the reward model through a reinforcement learning\nalgorithm. However, an inherent limitation of current reward models is their\ninability to fully represent the richness of human preferences and their\ndependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of\nLLMs using pairwise human feedback. Our approach entails the initial learning\nof a preference model, which is conditioned on two inputs given a prompt,\nfollowed by the pursuit of a policy that consistently generates responses\npreferred over those generated by any competing policy, thus defining the Nash\nequilibrium of this preference model. We term this approach Nash learning from\nhuman feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel\nalgorithmic solution, Nash-MD, founded on the principles of mirror descent.\nThis algorithm produces a sequence of policies, with the last iteration\nconverging to the regularized Nash equilibrium. Additionally, we explore\nparametric representations of policies and introduce gradient descent\nalgorithms for deep-learning architectures. To demonstrate the effectiveness of\nour approach, we present experimental results involving the fine-tuning of a\nLLM for a text summarization task. We believe NLHF offers a compelling avenue\nfor preference learning and policy optimization with the potential of advancing\nthe field of aligning LLMs with human preferences.",
        "pdf_link": "https://arxiv.org/pdf/2312.00886v3.pdf"
    },
    {
        "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
        "authors": [
            "Fabio Fehr",
            "James Henderson"
        ],
        "published": "2023-12-01T15:40:30Z",
        "summary": "The current paradigm of large-scale pre-training and fine-tuning Transformer\nlarge language models has lead to significant improvements across the board in\nnatural language processing. However, such large models are susceptible to\noverfitting to their training data, and as a result the models perform poorly\nwhen the domain changes. Also, due to the model's scale, the cost of\nfine-tuning the model to the new domain is large. Nonparametric Variational\nInformation Bottleneck (NVIB) has been proposed as a regulariser for training\ncross-attention in Transformers, potentially addressing the overfitting\nproblem. We extend the NVIB framework to replace all types of attention\nfunctions in Transformers, and show that existing pretrained Transformers can\nbe reinterpreted as Nonparametric Variational (NV) models using a proposed\nidentity initialisation. We then show that changing the initialisation\nintroduces a novel, information-theoretic post-training regularisation in the\nattention mechanism, which improves out-of-domain generalisation without any\ntraining. This success supports the hypothesis that pretrained Transformers are\nimplicitly NV Bayesian models.",
        "pdf_link": "https://arxiv.org/pdf/2312.00662v1.pdf"
    },
    {
        "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
        "authors": [
            "Aniket Deroy",
            "Subhankar Maity"
        ],
        "published": "2023-12-01T13:00:45Z",
        "summary": "The evolution of legal datasets and the advent of large language models\n(LLMs) have significantly transformed the legal field, particularly in the\ngeneration of case judgment summaries. However, a critical concern arises\nregarding the potential biases embedded within these summaries. This study\nscrutinizes the biases present in case judgment summaries produced by legal\ndatasets and large language models. The research aims to analyze the impact of\nbiases on legal decision making. By interrogating the accuracy, fairness, and\nimplications of biases in these summaries, this study contributes to a better\nunderstanding of the role of technology in legal contexts and the implications\nfor justice systems worldwide. In this study, we investigate biases wrt\nGender-related keywords, Race-related keywords, Keywords related to crime\nagainst women, Country names and religious keywords. The study shows\ninteresting evidences of biases in the outputs generated by the large language\nmodels and pre-trained abstractive summarization models. The reasoning behind\nthese biases needs further studies.",
        "pdf_link": "https://arxiv.org/pdf/2312.00554v1.pdf"
    },
    {
        "title": "LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices",
        "authors": [
            "Junchen Zhao",
            "Yurun Song",
            "Simeng Liu",
            "Ian G. Harris",
            "Sangeetha Abdu Jyothi"
        ],
        "published": "2023-12-01T07:19:42Z",
        "summary": "Deploying Large Language Models (LLMs) locally on mobile devices presents a\nsignificant challenge due to their extensive memory requirements. In this\npaper, we introduce LinguaLinked, a system for decentralized, distributed LLM\ninference on mobile devices. LinguaLinked enables collaborative execution of\nthe inference task across multiple trusted devices. LinguaLinked ensures data\nprivacy by processing information locally. LinguaLinked uses three key\nstrategies. First, an optimized model assignment technique segments LLMs and\nuses linear optimization to align segments with each device's capabilities.\nSecond, an optimized data transmission mechanism ensures efficient and\nstructured data flow between model segments while also maintaining the\nintegrity of the original model structure. Finally, LinguaLinked incorporates a\nruntime load balancer that actively monitors and redistributes tasks among\nmobile devices to prevent bottlenecks, enhancing the system's overall\nefficiency and responsiveness. We demonstrate that LinguaLinked facilitates\nefficient LLM inference while maintaining consistent throughput and minimal\nlatency through extensive testing across various mobile devices, from high-end\nto low-end Android devices. In our evaluations, compared to the baseline,\nLinguaLinked achieves an inference performance acceleration of $1.11\\times$ to\n$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ with\nmulti-threading. Additionally, runtime load balancing yields an overall\ninference acceleration of $1.29\\times$ to $1.32\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2312.00388v1.pdf"
    },
    {
        "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration",
        "authors": [
            "Viraj Mehta",
            "Vikramjeet Das",
            "Ojash Neopane",
            "Yijia Dai",
            "Ilija Bogunovic",
            "Jeff Schneider",
            "Willie Neiswanger"
        ],
        "published": "2023-12-01T00:54:02Z",
        "summary": "Preference-based feedback is important for many applications in reinforcement\nlearning where direct evaluation of a reward function is not feasible. A\nnotable recent example arises in reinforcement learning from human feedback\n(RLHF) on large language models. For many applications of RLHF, the cost of\nacquiring the human feedback can be substantial. In this work, we take\nadvantage of the fact that one can often choose contexts at which to obtain\nhuman feedback in order to most efficiently identify a good policy, and\nformalize this as an offline contextual dueling bandit problem. We give an\nupper-confidence-bound style algorithm for this problem and prove a polynomial\nworst-case regret bound. We then provide empirical confirmation in a synthetic\nsetting that our approach outperforms existing methods. After, we extend the\nsetting and methodology for practical use in RLHF training of large language\nmodels. Here, our method is able to reach better performance with fewer samples\nof human preferences than multiple baselines on three real-world datasets.",
        "pdf_link": "https://arxiv.org/pdf/2312.00267v1.pdf"
    },
    {
        "title": "Towards Accurate Differential Diagnosis with Large Language Models",
        "authors": [
            "Daniel McDuff",
            "Mike Schaekermann",
            "Tao Tu",
            "Anil Palepu",
            "Amy Wang",
            "Jake Garrison",
            "Karan Singhal",
            "Yash Sharma",
            "Shekoofeh Azizi",
            "Kavita Kulkarni",
            "Le Hou",
            "Yong Cheng",
            "Yun Liu",
            "S Sara Mahdavi",
            "Sushant Prakash",
            "Anupam Pathak",
            "Christopher Semturs",
            "Shwetak Patel",
            "Dale R Webster",
            "Ewa Dominowska",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Jake Sunshine",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2023-11-30T19:55:51Z",
        "summary": "An accurate differential diagnosis (DDx) is a cornerstone of medical care,\noften reached through an iterative process of interpretation that combines\nclinical history, physical examination, investigations and procedures.\nInteractive interfaces powered by Large Language Models (LLMs) present new\nopportunities to both assist and automate aspects of this process. In this\nstudy, we introduce an LLM optimized for diagnostic reasoning, and evaluate its\nability to generate a DDx alone or as an aid to clinicians. 20 clinicians\nevaluated 302 challenging, real-world medical cases sourced from the New\nEngland Journal of Medicine (NEJM) case reports. Each case report was read by\ntwo clinicians, who were randomized to one of two assistive conditions: either\nassistance from search engines and standard medical resources, or LLM\nassistance in addition to these tools. All clinicians provided a baseline,\nunassisted DDx prior to using the respective assistive tools. Our LLM for DDx\nexhibited standalone performance that exceeded that of unassisted clinicians\n(top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study\narms, the DDx quality score was higher for clinicians assisted by our LLM\n(top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%)\n(McNemar's Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75, p =\n0.03). Further, clinicians assisted by our LLM arrived at more comprehensive\ndifferential lists than those without its assistance. Our study suggests that\nour LLM for DDx has potential to improve clinicians' diagnostic reasoning and\naccuracy in challenging cases, meriting further real-world evaluation for its\nability to empower physicians and widen patients' access to specialist-level\nexpertise.",
        "pdf_link": "https://arxiv.org/pdf/2312.00164v1.pdf"
    },
    {
        "title": "PoseGPT: Chatting about 3D Human Pose",
        "authors": [
            "Yao Feng",
            "Jing Lin",
            "Sai Kumar Dwivedi",
            "Yu Sun",
            "Priyanka Patel",
            "Michael J. Black"
        ],
        "published": "2023-11-30T18:59:52Z",
        "summary": "We introduce PoseGPT, a framework employing Large Language Models (LLMs) to\nunderstand and reason about 3D human poses from images or textual descriptions.\nOur work is motivated by the human ability to intuitively understand postures\nfrom a single image or a brief description, a process that intertwines image\ninterpretation, world knowledge, and an understanding of body language.\nTraditional human pose estimation methods, whether image-based or text-based,\noften lack holistic scene comprehension and nuanced reasoning, leading to a\ndisconnect between visual data and its real-world implications. PoseGPT\naddresses these limitations by embedding SMPL poses as a distinct signal token\nwithin a multi-modal LLM, enabling direct generation of 3D body poses from both\ntextual and visual inputs. This approach not only simplifies pose prediction\nbut also empowers LLMs to apply their world knowledge in reasoning about human\nposes, fostering two advanced tasks: speculative pose generation and reasoning\nabout pose estimation. These tasks involve reasoning about humans to generate\n3D poses from subtle text queries, possibly accompanied by images. We establish\nbenchmarks for these tasks, moving beyond traditional 3D pose generation and\nestimation methods. Our results show that PoseGPT outperforms existing\nmultimodal LLMs and task-sepcific methods on these newly proposed tasks.\nFurthermore, PoseGPT's ability to understand and generate 3D human poses based\non complex reasoning opens new directions in human pose analysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.18836v1.pdf"
    },
    {
        "title": "Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web",
        "authors": [
            "Hiroki Furuta",
            "Yutaka Matsuo",
            "Aleksandra Faust",
            "Izzeddin Gur"
        ],
        "published": "2023-11-30T17:50:47Z",
        "summary": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for task\ncompositionality, their performance further degrades under different\ninstruction compositions changing combinational order. In contrast to the\nrecent remarkable success of LMA, our benchmark and detailed analysis emphasize\nthe necessity of building LMAs that are robust and generalizable to task\ncompositionality for real-world deployment.",
        "pdf_link": "https://arxiv.org/pdf/2311.18751v2.pdf"
    },
    {
        "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
        "authors": [
            "Yingdi Guo"
        ],
        "published": "2023-11-30T15:06:50Z",
        "summary": "With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.",
        "pdf_link": "https://arxiv.org/pdf/2311.18609v1.pdf"
    },
    {
        "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",
        "authors": [
            "Shiyao Cui",
            "Zhenyu Zhang",
            "Yilong Chen",
            "Wenyuan Zhang",
            "Tianyun Liu",
            "Siqi Wang",
            "Tingwen Liu"
        ],
        "published": "2023-11-30T14:18:47Z",
        "summary": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.",
        "pdf_link": "https://arxiv.org/pdf/2311.18580v1.pdf"
    },
    {
        "title": "OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition",
        "authors": [
            "Tongjia Chen",
            "Hongshan Yu",
            "Zhengeng Yang",
            "Zechuan Li",
            "Wei Sun",
            "Chen Chen"
        ],
        "published": "2023-11-30T13:32:43Z",
        "summary": "Due to the resource-intensive nature of training vision-language models on\nexpansive video data, a majority of studies have centered on adapting\npre-trained image-language models to the video domain. Dominant pipelines\npropose to tackle the visual discrepancies with additional temporal learners\nwhile overlooking the substantial discrepancy for web-scaled descriptive\nnarratives and concise action category names, leading to less distinct semantic\nspace and potential performance limitations. In this work, we prioritize the\nrefinement of text knowledge to facilitate generalizable video recognition. To\naddress the limitations of the less distinct semantic space of category names,\nwe prompt a large language model (LLM) to augment action class names into\nSpatio-Temporal Descriptors thus bridging the textual discrepancy and serving\nas a knowledge base for general recognition. Moreover, to assign the best\ndescriptors with different video instances, we propose Optimal Descriptor\nSolver, forming the video recognition problem as solving the optimal matching\nflow across frame-level representations and descriptors. Comprehensive\nevaluations in zero-shot, few-shot, and fully supervised video recognition\nhighlight the effectiveness of our approach. Our best model achieves a\nstate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
        "pdf_link": "https://arxiv.org/pdf/2312.00096v2.pdf"
    },
    {
        "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
        "authors": [
            "Akira Kawabata",
            "Saku Sugawara"
        ],
        "published": "2023-11-30T08:44:55Z",
        "summary": "To precisely evaluate a language model's capability for logical reading\ncomprehension, we present a dataset for testing the understanding of the\nrationale behind critical reasoning. For questions taken from an existing\nmultiplechoice logical reading comprehension dataset, we crowdsource rationale\ntexts that explain why we should select or eliminate answer options, resulting\nin 3,003 multiple-choice subquestions that are associated with 943 main\nquestions. Experiments on our dataset show that recent large language models\n(e.g., InstructGPT) struggle to answer the subquestions even if they are able\nto answer the main questions correctly. We find that the models perform\nparticularly poorly in answering subquestions written for the incorrect options\nof the main questions, implying that the models have a limited capability for\nexplaining why incorrect alternatives should be eliminated. These results\nsuggest that our dataset encourages further investigation into the critical\nreasoning ability of language models while focusing on the elimination process\nof relevant alternatives.",
        "pdf_link": "https://arxiv.org/pdf/2311.18353v1.pdf"
    },
    {
        "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
        "authors": [
            "Anwen Hu",
            "Yaya Shi",
            "Haiyang Xu",
            "Jiabo Ye",
            "Qinghao Ye",
            "Ming Yan",
            "Chenliang Li",
            "Qi Qian",
            "Ji Zhang",
            "Fei Huang"
        ],
        "published": "2023-11-30T04:43:26Z",
        "summary": "Recently, the strong text creation ability of Large Language Models(LLMs) has\ngiven rise to many tools for assisting paper reading or even writing. However,\nthe weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit\ntheir application scenarios, especially for scientific academic paper writing.\nIn this work, towards a more versatile copilot for academic paper writing, we\nmainly focus on strengthening the multi-modal diagram analysis ability of\nMultimodal LLMs. By parsing Latex source files of high-quality papers, we\ncarefully build a multi-modal diagram understanding dataset M-Paper. By\naligning diagrams in the paper with related paragraphs, we construct\nprofessional diagram analysis samples for training and evaluation. M-Paper is\nthe first dataset to support joint comprehension of multiple scientific\ndiagrams, including figures and tables in the format of images or Latex codes.\nBesides, to better align the copilot with the user's intention, we introduce\nthe `outline' as the control signal, which could be directly given by the user\nor revised based on auto-generated ones. Comprehensive experiments with a\nstate-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows\nstronger scientific diagram understanding performance, including diagram\ncaptioning, diagram analysis, and outline recommendation. The dataset, code,\nand model are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl.",
        "pdf_link": "https://arxiv.org/pdf/2311.18248v2.pdf"
    },
    {
        "title": "Large Language Models for Travel Behavior Prediction",
        "authors": [
            "Baichuan Mo",
            "Hanyong Xu",
            "Dingyi Zhuang",
            "Ruoyun Ma",
            "Xiaotong Guo",
            "Jinhua Zhao"
        ],
        "published": "2023-11-30T04:35:55Z",
        "summary": "Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2312.00819v1.pdf"
    },
    {
        "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
        "authors": [
            "Sungjoo Byun",
            "Dongjun Jang",
            "Hyemi Jo",
            "Hyopil Shin"
        ],
        "published": "2023-11-30T03:19:45Z",
        "summary": "Caution: this paper may include material that could be offensive or\ndistressing.\n  The advent of Large Language Models (LLMs) necessitates the development of\ntraining approaches that mitigate the generation of unethical language and\naptly manage toxic user queries. Given the challenges related to human labor\nand the scarcity of data, we present KoTox, comprising 39K unethical\ninstruction-output pairs. This collection of automatically generated toxic\ninstructions refines the training of LLMs and establishes a foundational\nframework for improving LLMs' ethical awareness and response to various toxic\ninputs, promoting more secure and responsible interactions in Natural Language\nProcessing (NLP) applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.18215v1.pdf"
    },
    {
        "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
        "authors": [
            "Yongqiang Chen",
            "Binghui Xie",
            "Kaiwen Zhou",
            "Bo Han",
            "Yatao Bian",
            "James Cheng"
        ],
        "published": "2023-11-30T02:26:55Z",
        "summary": "In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts",
        "pdf_link": "https://arxiv.org/pdf/2311.18194v1.pdf"
    },
    {
        "title": "Zero-shot Conversational Summarization Evaluations with small Large Language Models",
        "authors": [
            "Ramesh Manuvinakurike",
            "Saurav Sahay",
            "Sangeeta Manepalli",
            "Lama Nachman"
        ],
        "published": "2023-11-29T19:34:34Z",
        "summary": "Large Language Models (LLMs) exhibit powerful summarization abilities.\nHowever, their capabilities on conversational summarization remains under\nexplored. In this work we evaluate LLMs (approx. 10 billion parameters) on\nconversational summarization and showcase their performance on various prompts.\nWe show that the summaries generated by models depend on the instructions and\nthe performance of LLMs vary with different instructions sometimes resulting\nsteep drop in ROUGE scores if prompts are not selected carefully. We also\nevaluate the models with human evaluations and discuss the limitations of the\nmodels on conversational summarization",
        "pdf_link": "https://arxiv.org/pdf/2311.18041v1.pdf"
    },
    {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "authors": [
            "Qidong Huang",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Bin Wang",
            "Conghui He",
            "Jiaqi Wang",
            "Dahua Lin",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2023-11-29T18:57:07Z",
        "summary": "Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Based on the observation, OPERA introduces a\npenalty term on the model logits during the beam-search decoding to mitigate\nthe over-trust issue, along with a rollback strategy that retrospects the\npresence of summary tokens in the previously generated tokens, and re-allocate\nthe token selection if necessary. With extensive experiments, OPERA shows\nsignificant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.",
        "pdf_link": "https://arxiv.org/pdf/2311.17911v3.pdf"
    },
    {
        "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models",
        "authors": [
            "Xin Liu",
            "Yichen Zhu",
            "Jindong Gu",
            "Yunshi Lan",
            "Chao Yang",
            "Yu Qiao"
        ],
        "published": "2023-11-29T12:49:45Z",
        "summary": "The security concerns surrounding Large Language Models (LLMs) have been\nextensively explored, yet the safety of Multimodal Large Language Models\n(MLLMs) remains understudied. In this paper, we observe that Multimodal Large\nLanguage Models (MLLMs) can be easily compromised by query-relevant images, as\nif the text query itself were malicious. To address this, we introduce\nMM-SafetyBench, a comprehensive framework designed for conducting\nsafety-critical evaluations of MLLMs against such image-based manipulations. We\nhave compiled a dataset comprising 13 scenarios, resulting in a total of 5,040\ntext-image pairs. Our analysis across 12 state-of-the-art models reveals that\nMLLMs are susceptible to breaches instigated by our approach, even when the\nequipped LLMs have been safety-aligned. In response, we propose a\nstraightforward yet effective prompting strategy to enhance the resilience of\nMLLMs against these types of attacks. Our work underscores the need for a\nconcerted effort to strengthen and enhance the safety measures of open-source\nMLLMs against potential malicious exploits. The resource is available at\n\\href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.",
        "pdf_link": "https://arxiv.org/pdf/2311.17600v2.pdf"
    },
    {
        "title": "TaskWeaver: A Code-First Agent Framework",
        "authors": [
            "Bo Qiao",
            "Liqun Li",
            "Xu Zhang",
            "Shilin He",
            "Yu Kang",
            "Chaoyun Zhang",
            "Fangkai Yang",
            "Hang Dong",
            "Jue Zhang",
            "Lu Wang",
            "Minghua Ma",
            "Pu Zhao",
            "Si Qin",
            "Xiaoting Qin",
            "Chao Du",
            "Yong Xu",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "published": "2023-11-29T11:23:42Z",
        "summary": "Large Language Models (LLMs) have shown impressive abilities in natural\nlanguage understanding and generation, leading to their use in applications\nsuch as chatbots and virtual assistants. However, existing LLM frameworks face\nlimitations in handling domain-specific data analytics tasks with rich data\nstructures. Moreover, they struggle with flexibility to meet diverse user\nrequirements. To address these issues, TaskWeaver is proposed as a code-first\nframework for building LLM-powered autonomous agents. It converts user requests\ninto executable code and treats user-defined plugins as callable functions.\nTaskWeaver provides support for rich data structures, flexible plugin usage,\nand dynamic plugin selection, and leverages LLM coding capabilities for complex\nlogic. It also incorporates domain-specific knowledge through examples and\nensures the secure execution of generated code. TaskWeaver offers a powerful\nand flexible framework for creating intelligent conversational agents that can\nhandle complex tasks and adapt to domain-specific scenarios. The code is\nopen-sourced at https://github.com/microsoft/TaskWeaver/.",
        "pdf_link": "https://arxiv.org/pdf/2311.17541v2.pdf"
    },
    {
        "title": "LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World",
        "authors": [
            "Siwei Chen",
            "Anxing Xiao",
            "David Hsu"
        ],
        "published": "2023-11-29T07:23:22Z",
        "summary": "This work addresses the problem of long-horizon task planning with the Large\nLanguage Model (LLM) in an open-world household environment. Existing works\nfail to explicitly track key objects and attributes, leading to erroneous\ndecisions in long-horizon tasks, or rely on highly engineered state features\nand feedback, which is not generalizable. We propose a novel, expandable state\nrepresentation that provides continuous expansion and updating of object\nattributes from the LLM's inherent capabilities for context understanding and\nhistorical action reasoning. Our proposed representation maintains a\ncomprehensive record of an object's attributes and changes, enabling robust\nretrospective summary of the sequence of actions leading to the current state.\nThis allows enhanced context understanding for decision-making in task\nplanning. We validate our model through experiments across simulated and\nreal-world task planning scenarios, demonstrating significant improvements over\nbaseline methods in a variety of tasks requiring long-horizon state tracking\nand reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.17406v1.pdf"
    },
    {
        "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
        "authors": [
            "Lujia Shen",
            "Yuwen Pu",
            "Shouling Ji",
            "Changjiang Li",
            "Xuhong Zhang",
            "Chunpeng Ge",
            "Ting Wang"
        ],
        "published": "2023-11-29T07:09:13Z",
        "summary": "Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.17400v2.pdf"
    },
    {
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "authors": [
            "Jiaxin Wen",
            "Pei Ke",
            "Hao Sun",
            "Zhexin Zhang",
            "Chengfei Li",
            "Jinfeng Bai",
            "Minlie Huang"
        ],
        "published": "2023-11-29T06:42:36Z",
        "summary": "The open-endedness of large language models (LLMs) combined with their\nimpressive capabilities may lead to new safety issues when being exploited for\nmalicious use. While recent studies primarily focus on probing toxic outputs\nthat can be easily detected with existing toxicity classifiers, we show that\nLLMs can generate diverse implicit toxic outputs that are exceptionally\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\nreinforcement learning (RL) based attacking method to further induce the\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\nthe attack success rate can be significantly improved through RL fine-tuning.\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\na significant threat in generating undetectable implicit toxic outputs. We\nfurther show that fine-tuning toxicity classifiers on the annotated examples\nfrom our attacking method can effectively enhance their ability to detect\nLLM-generated implicit toxic language. The code is publicly available at\nhttps://github.com/thu-coai/Implicit-Toxicity.",
        "pdf_link": "https://arxiv.org/pdf/2311.17391v1.pdf"
    },
    {
        "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study",
        "authors": [
            "Han Cao",
            "Lingwei Wei",
            "Mengyang Chen",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-11-29T05:04:52Z",
        "summary": "Recently, Large Language Models (LLMs) have drawn significant attention due\nto their outstanding reasoning capabilities and extensive knowledge repository,\npositioning them as superior in handling various natural language processing\ntasks compared to other language models. In this paper, we present a\npreliminary investigation into the potential of LLMs in fact-checking. This\nstudy aims to comprehensively evaluate various LLMs in tackling specific\nfact-checking subtasks, systematically evaluating their capabilities, and\nconducting a comparative analysis of their performance against pre-trained and\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\nachieve competitive performance compared to other small models in most\nscenarios. However, they encounter challenges in effectively handling Chinese\nfact verification and the entirety of the fact-checking pipeline due to\nlanguage inconsistencies and hallucinations. These findings underscore the need\nfor further exploration and research to enhance the proficiency of LLMs as\nreliable fact-checkers, unveiling the potential capability of LLMs and the\npossible challenges in fact-checking tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.17355v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Human Mobility Prediction under Public Events",
        "authors": [
            "Yuebing Liang",
            "Yichao Liu",
            "Xiaohan Wang",
            "Zhan Zhao"
        ],
        "published": "2023-11-29T04:25:15Z",
        "summary": "Public events, such as concerts and sports games, can be major attractors for\nlarge crowds, leading to irregular surges in travel demand. Accurate human\nmobility prediction for public events is thus crucial for event planning as\nwell as traffic or crowd management. While rich textual descriptions about\npublic events are commonly available from online sources, it is challenging to\nencode such information in statistical or machine learning models. Existing\nmethods are generally limited in incorporating textual information, handling\ndata sparsity, or providing rationales for their predictions. To address these\nchallenges, we introduce a framework for human mobility prediction under public\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\nunprecedented ability to process textual data, learn from minimal examples, and\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\nraw, unstructured event descriptions from online sources into a standardized\nformat, and then segments historical mobility data into regular and\nevent-related components. A prompting strategy is designed to direct LLMs in\nmaking and rationalizing demand predictions considering historical mobility and\nevent features. A case study is conducted for Barclays Center in New York City,\nbased on publicly available event information and taxi trip data. Results show\nthat LLM-MPE surpasses traditional models, particularly on event days, with\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\ninterpretable insights into its predictions. Despite the great potential of\nLLMs, we also identify key challenges including misinformation and high costs\nthat remain barriers to their broader adoption in large-scale human mobility\nanalysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.17351v1.pdf"
    },
    {
        "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner",
        "authors": [
            "Lizhao Liu",
            "Xinyu Sun",
            "Tianhang Xiang",
            "Zhuangwei Zhuang",
            "Liuren Yin",
            "Mingkui Tan"
        ],
        "published": "2023-11-29T03:29:46Z",
        "summary": "We study the task of extending the large language model (LLM) into a\nvision-language instruction-following model. This task is crucial but\nchallenging since the LLM is trained on text modality only, making it hard to\neffectively digest the visual modality. To address this, existing methods\ntypically train a visual adapter to align the representation between a\npre-trained vision transformer (ViT) and the LLM by a generative image\ncaptioning loss. However, we find that the generative objective can only\nproduce weak alignment for vision and language, making the aligned\nvision-language model very hungry for the instruction fine-tuning data. In this\npaper, we propose CG-VLM that applies both Contrastive and Generative alignment\nobjectives to effectively align the representation of ViT and LLM. Different\nfrom image level and sentence level alignment in common contrastive learning\nsettings, CG-VLM aligns the image-patch level features and text-token level\nembeddings, which, however, is very hard to achieve as no explicit grounding\npatch-token relation provided in standard image captioning datasets. To address\nthis issue, we propose to maximize the averaged similarity between pooled\nimage-patch features and text-token embeddings. Extensive experiments\ndemonstrate that the proposed CG-VLM produces strong vision-language alignment\nand is an efficient instruction learner. For example, using only 10%\ninstruction tuning data, we reach 95% performance of state-of-the-art method\nLLaVA [29] on the zero-shot ScienceQA-Image benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2311.17945v1.pdf"
    },
    {
        "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
        "authors": [
            "Meriem Boubdir",
            "Edward Kim",
            "Beyza Ermis",
            "Sara Hooker",
            "Marzieh Fadaee"
        ],
        "published": "2023-11-29T00:45:23Z",
        "summary": "In Natural Language Processing (NLP), the Elo rating system, originally\ndesigned for ranking players in dynamic games such as chess, is increasingly\nbeing used to evaluate Large Language Models (LLMs) through \"A vs B\" paired\ncomparisons. However, while popular, the system's suitability for assessing\nentities with constant skill levels, such as LLMs, remains relatively\nunexplored. We study two fundamental axioms that evaluation methods should\nadhere to: reliability and transitivity. We conduct extensive evaluation of Elo\nbehaviour, illustrating that individual Elo computations exhibit volatility and\ndelving into the impact of varying the Elo rating system's hyperparameters. We\nshow that these axioms are not always satisfied raising questions about the\nreliability of current comparative evaluations of LLMs. If the current use of\nElo scores is intended to substitute the costly head-to-head comparison of\nLLMs, it is crucial to ensure the ranking is as robust as possible. Guided by\nthe axioms, our findings offer concrete guidelines for enhancing the\nreliability of LLM evaluation methods, suggesting a need for reassessment of\nexisting comparative approaches.",
        "pdf_link": "https://arxiv.org/pdf/2311.17295v1.pdf"
    },
    {
        "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
        "authors": [
            "Wenyue Hua",
            "Lizhou Fan",
            "Lingyao Li",
            "Kai Mei",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Libby Hemphill",
            "Yongfeng Zhang"
        ],
        "published": "2023-11-28T20:59:49Z",
        "summary": "Can we avoid wars at the crossroads of history? This question has been\npursued by individuals, scholars, policymakers, and organizations throughout\nhuman history. In this research, we attempt to answer the question based on the\nrecent advances of Artificial Intelligence (AI) and Large Language Models\n(LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to\nsimulate the participating countries, their decisions, and the consequences, in\nhistorical international conflicts, including the World War I (WWI), the World\nWar II (WWII), and the Warring States Period (WSP) in Ancient China. By\nevaluating the simulation effectiveness, we examine the advancements and\nlimitations of cutting-edge AI systems' abilities in studying complex\ncollective human behaviors such as international conflicts under diverse\nsettings. In these simulations, the emergent interactions among agents also\noffer a novel perspective for examining the triggers and conditions that lead\nto war. Our findings offer data-driven and AI-augmented insights that can\nredefine how we approach conflict resolution and peacekeeping strategies. The\nimplications stretch beyond historical analysis, offering a blueprint for using\nAI to understand human history and possibly prevent future international\nconflicts. Code and data are available at\n\\url{https://github.com/agiresearch/WarAgent}.",
        "pdf_link": "https://arxiv.org/pdf/2311.17227v2.pdf"
    },
    {
        "title": "Scalable Extraction of Training Data from (Production) Language Models",
        "authors": [
            "Milad Nasr",
            "Nicholas Carlini",
            "Jonathan Hayase",
            "Matthew Jagielski",
            "A. Feder Cooper",
            "Daphne Ippolito",
            "Christopher A. Choquette-Choo",
            "Eric Wallace",
            "Florian Tram√®r",
            "Katherine Lee"
        ],
        "published": "2023-11-28T18:47:03Z",
        "summary": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
        "pdf_link": "https://arxiv.org/pdf/2311.17035v1.pdf"
    },
    {
        "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
        "authors": [
            "Zhiyuan Zhao",
            "Bin Wang",
            "Linke Ouyang",
            "Xiaoyi Dong",
            "Jiaqi Wang",
            "Conghui He"
        ],
        "published": "2023-11-28T14:54:37Z",
        "summary": "Multimodal large language models have made significant advancements in recent\nyears, yet they still suffer from a common issue known as the \"hallucination\nproblem\", in which the models generate textual descriptions that inaccurately\ndepict or entirely fabricate content from associated images. This paper\nintroduces a novel solution, Hallucination-Aware Direct Preference Optimization\n(HA-DPO), which reframes the hallucination problem as a preference selection\ntask. The model is trained to favor the non-hallucinating response when\npresented with two responses of the same image (one accurate and one\nhallucinatory). Furthermore, this paper proposes an efficient pipeline for\nconstructing positive~(non-hallucinatory) and negative~(hallucinatory) sample\npairs, ensuring a high-quality, style-consistent dataset for robust preference\nlearning. When applied to three mainstream multimodal models, HA-DPO\nsignificantly reduced hallucination issues and amplified the models'\ngeneralization capabilities. Notably, the MiniGPT-4 model, when enhanced with\nHA-DPO, demonstrated a substantial improvement: POPE accuracy rose from 51.13%\nto 86.13% (an absolute improvement of 35%), and the MME score surged from\n932.00 to 1326.46 (a relative improvement of 42.32%). The codes, models, and\ndatasets are made accessible at https://opendatalab.github.io/HA-DPO.",
        "pdf_link": "https://arxiv.org/pdf/2311.16839v2.pdf"
    },
    {
        "title": "Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis",
        "authors": [
            "Xiaohui Chen",
            "Yongfei Liu",
            "Yingxiang Yang",
            "Jianbo Yuan",
            "Quanzeng You",
            "Li-Ping Liu",
            "Hongxia Yang"
        ],
        "published": "2023-11-28T14:51:13Z",
        "summary": "Recent advancements in text-to-image (T2I) generative models have shown\nremarkable capabilities in producing diverse and imaginative visuals based on\ntext prompts. Despite the advancement, these diffusion models sometimes\nstruggle to translate the semantic content from the text into images entirely.\nWhile conditioning on the layout has shown to be effective in improving the\ncompositional ability of T2I diffusion models, they typically require manual\nlayout input. In this work, we introduce a novel approach to improving T2I\ndiffusion models using Large Language Models (LLMs) as layout generators. Our\nmethod leverages the Chain-of-Thought prompting of LLMs to interpret text and\ngenerate spatially reasonable object layouts. The generated layout is then used\nto enhance the generated images' composition and spatial accuracy. Moreover, we\npropose an efficient adapter based on a cross-attention mechanism, which\nexplicitly integrates the layout information into the stable diffusion models.\nOur experiments demonstrate significant improvements in image quality and\nlayout accuracy, showcasing the potential of LLMs in augmenting generative\nimage models.",
        "pdf_link": "https://arxiv.org/pdf/2311.17126v1.pdf"
    },
    {
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop",
        "authors": [
            "Martin Briesch",
            "Dominik Sobania",
            "Franz Rothlauf"
        ],
        "published": "2023-11-28T14:36:43Z",
        "summary": "Large language models (LLM) have become state of the art in many benchmarks\nand conversational LLM applications like ChatGPT are now widely used by the\npublic. Those LLMs can be used to generate large amounts of content which is\nposted on the internet to various platforms. As LLMs are trained on datasets\nusually collected from the internet, this LLM-generated content might be used\nto train the next generation of LLMs. Therefore, a self-consuming training loop\nemerges in which new LLM generations are trained on the output from the\nprevious generations. We empirically study this self-consuming training loop\nusing a novel dataset to analytically and accurately measure quality and\ndiversity of generated outputs. We find that this self-consuming training loop\ninitially improves both quality and diversity. However, after a few generations\nthe output inevitably degenerates in diversity. We find that the rate of\ndegeneration depends on the proportion of real and generated data.",
        "pdf_link": "https://arxiv.org/pdf/2311.16822v1.pdf"
    },
    {
        "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Lingzhi Wang",
            "Yiming Du",
            "Liang Chen",
            "Jingyan Zhou",
            "Yufei Wang",
            "Kam-Fai Wong"
        ],
        "published": "2023-11-28T13:51:32Z",
        "summary": "Dialogue systems, including task-oriented_dialogue_system (TOD) and\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\nwith language_models (LM) playing a central role. This survey delves into the\nhistorical trajectory of dialogue systems, elucidating their intricate\nrelationship with advancements in language models by categorizing this\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\nintrinsic knowledge in their parameters; 3) fusion between different types of\ndialogue systems with the advert of pre-trained_language_models (PLMs),\nstarting from the fusion between four_sub-tasks_within_TOD, and then\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\nchronological perspective aligned with LM breakthroughs, offering a\ncomprehensive review of state-of-the-art research outcomes. What's more, we\nfocus on emerging topics and discuss open challenges, providing valuable\ninsights into future directions for LLM-based_dialogue_systems. Through this\nexploration, we pave the way for a deeper_comprehension of the evolution,\nguiding future developments in LM-based dialogue_systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.16789v1.pdf"
    },
    {
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
        "authors": [
            "Yijun Yang",
            "Tianyi Zhou",
            "Kanxue Li",
            "Dapeng Tao",
            "Lusong Li",
            "Li Shen",
            "Xiaodong He",
            "Jing Jiang",
            "Yuhui Shi"
        ],
        "published": "2023-11-28T11:53:56Z",
        "summary": "While large language models (LLMs) excel in a simulated world of texts, they\nstruggle to interact with the more realistic world without perceptions of other\nmodalities such as visual or audio signals. Although vision-language models\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\nmay possess prior knowledge of world dynamics (as demonstrated in the text\nworld), they have not been trained in an embodied visual world and thus cannot\nalign with its dynamics. On the other hand, training an embodied agent in a\nnoisy visual world without expert guidance is often challenging and\ninefficient. In this paper, we train a VLM agent living in a visual world using\nan LLM agent excelling in a parallel text world. Specifically, we distill LLM's\nreflection outcomes (improved actions by analyzing mistakes) in a text world's\ntasks to finetune the VLM on the same tasks of the visual world, resulting in\nan Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world\ndynamics. Such cross-modality imitation learning between the two parallel\nworlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize\nto a broad scope of new tasks without any further guidance from the LLM expert.\nExtensive evaluations on the ALFWorld benchmark's diverse tasks highlight\nEMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement\nin the success rate.",
        "pdf_link": "https://arxiv.org/pdf/2311.16714v2.pdf"
    },
    {
        "title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?",
        "authors": [
            "Romain Lacombe",
            "Kerrie Wu",
            "Eddie Dilworth"
        ],
        "published": "2023-11-28T10:26:57Z",
        "summary": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.17107v1.pdf"
    },
    {
        "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
        "authors": [
            "Bohao Li",
            "Yuying Ge",
            "Yixiao Ge",
            "Guangzhi Wang",
            "Rui Wang",
            "Ruimao Zhang",
            "Ying Shan"
        ],
        "published": "2023-11-28T05:53:55Z",
        "summary": "Multimodal large language models (MLLMs), building upon the foundation of\npowerful large language models (LLMs), have recently demonstrated exceptional\ncapabilities in generating not only texts but also images given interleaved\nmultimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However,\nexisting MLLM benchmarks remain limited to assessing only models' comprehension\nability of single image-text inputs, failing to keep up with the strides made\nin MLLMs. A comprehensive benchmark is imperative for investigating the\nprogress and uncovering the limitations of current MLLMs. In this work, we\ncategorize the capabilities of MLLMs into hierarchical levels from $L_0$ to\n$L_4$ based on the modalities they can accept and generate, and propose\nSEED-Bench-2, a comprehensive benchmark that evaluates the\n\\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2\ncomprises 24K multiple-choice questions with accurate human annotations, which\nspans 27 dimensions, including the evaluation of both text and image\ngeneration. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 23 prominent open-source\nMLLMs and summarize valuable observations. By revealing the limitations of\nexisting MLLMs through extensive evaluations, we aim for SEED-Bench-2 to\nprovide insights that will motivate future research towards the goal of General\nArtificial Intelligence. Dataset and evaluation code are available at\n\\href{https://github.com/AILab-CVC/SEED-Bench}",
        "pdf_link": "https://arxiv.org/pdf/2311.17092v1.pdf"
    },
    {
        "title": "Methods to Estimate Large Language Model Confidence",
        "authors": [
            "Maia Kotelanski",
            "Robert Gallo",
            "Ashwin Nayak",
            "Thomas Savage"
        ],
        "published": "2023-11-28T05:44:06Z",
        "summary": "Large Language Models have difficulty communicating uncertainty, which is a\nsignificant obstacle to applying LLMs to complex medical tasks. This study\nevaluates methods to measure LLM confidence when suggesting a diagnosis for\nchallenging clinical vignettes. GPT4 was asked a series of challenging case\nquestions using Chain of Thought and Self Consistency prompting. Multiple\nmethods were investigated to assess model confidence and evaluated on their\nability to predict the models observed accuracy. The methods evaluated were\nIntrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC\nAgreement Frequency correlated with observed accuracy, yielding a higher Area\nunder the Receiver Operating Characteristic Curve compared to Intrinsic\nConfidence and CoT Length analysis. SC agreement is the most useful proxy for\nmodel confidence, especially for medical diagnosis. Model Intrinsic Confidence\nand CoT Response Length exhibit a weaker ability to differentiate between\ncorrect and incorrect answers, preventing them from being reliable and\ninterpretable markers for model confidence. We conclude GPT4 has a limited\nability to assess its own diagnostic accuracy. SC Agreement Frequency is the\nmost useful method to measure GPT4 confidence.",
        "pdf_link": "https://arxiv.org/pdf/2312.03733v2.pdf"
    },
    {
        "title": "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA",
        "authors": [
            "Damjan Kalajdzievski"
        ],
        "published": "2023-11-28T03:23:20Z",
        "summary": "As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.",
        "pdf_link": "https://arxiv.org/pdf/2312.03732v1.pdf"
    },
    {
        "title": "Enabling Fast 2-bit LLM on GPUs: Memory Alignment and Asynchronous Dequantization",
        "authors": [
            "Jinhao Li",
            "Shiyao Li",
            "Jiaming Xu",
            "Shan Huang",
            "Yaoxiu Lian",
            "Jun Liu",
            "Yu Wang",
            "Guohao Dai"
        ],
        "published": "2023-11-28T02:44:59Z",
        "summary": "Large language models (LLMs) have demonstrated impressive abilities in\nvarious domains while the inference cost is expensive. The state-of-the-art\nmethods use 2-bit quantization for mainstream LLMs. However, challenges still\nexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are\nquantized by groups, while the ranges of weights are large in some groups,\nresulting in large quantization errors and nonnegligible accuracy loss (e.g.\n>3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited\naccuracy improvement by adding 4-bit weights. Increasing 10% extra average bit\nmore 4-bit weights only leads to <0.5% accuracy improvement on a quantized\nLlama2-7b. (3) Time-consuming dequantization operations on GPUs. The\ndequantization operations lead to >50% execution time, hindering the potential\nof reducing LLM inference cost. To tackle these challenges, we propose the\nfollowing techniques: (1) We only quantize a small fraction of groups with the\nlarger range using 4-bit with memory alignment consideration on GPUs.(2) We\ndesign the asynchronous dequantization on GPUs, leading to up to 3.92X speedup.\nWe conduct extensive experiments on different model sizes. We achieve 2.85-bit\nfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over the\noriginal model, and we reduce both runtime cost and hardware cost by up to\n2.70X and 2.81X with less GPU requirements.",
        "pdf_link": "https://arxiv.org/pdf/2311.16442v2.pdf"
    },
    {
        "title": "Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation",
        "authors": [
            "Samuele Poppi",
            "Tobia Poppi",
            "Federico Cocchi",
            "Marcella Cornia",
            "Lorenzo Baraldi",
            "Rita Cucchiara"
        ],
        "published": "2023-11-27T19:02:17Z",
        "summary": "Vision-and-Language models such as CLIP have demonstrated remarkable\neffectiveness across a wide range of tasks. However, these models are typically\ntrained on web-scale data, which can introduce inappropriate content and lead\nto the development of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcern in their adoption. To overcome these limitations, we introduce a\nmethodology to make Vision-and-Language models safer by removing their\nsensitivity to not-safe-for-work concepts. We show how this can be done by\ndistilling from a large language model which converts between safe and unsafe\nsentences and which is fine-tuned starting from just 100 manually-curated\npairs. We conduct extensive experiments on the resulting embedding space for\nboth retrieval and text-to-image generation, where we show that our model can\nalso be properly employed with pre-trained image generators. Our source code\nand trained models are available at: https://github.com/aimagelab/safe-clip.",
        "pdf_link": "https://arxiv.org/pdf/2311.16254v1.pdf"
    },
    {
        "title": "Visual cognition in multimodal large language models",
        "authors": [
            "Luca M. Schulze Buschoff",
            "Elif Akata",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "published": "2023-11-27T18:58:34Z",
        "summary": "A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while these\nmodels demonstrate a notable proficiency in processing and interpreting visual\ndata, they still fall short of human capabilities in these areas. The models\nexhibit a rudimentary understanding of physical laws and causal relationships,\nbut their performance is hindered by a lack of deeper insights - a key aspect\nof human cognition. Furthermore, in tasks requiring an intuitive theory of\nmind, the models fail altogether. Our results emphasize the need for\nintegrating more robust mechanisms for understanding causality, physical\ndynamics, and social cognition into modern-day, vision-based language models,\nand point out the importance of cognitively-inspired benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.16093v2.pdf"
    },
    {
        "title": "Self-correcting LLM-controlled Diffusion Models",
        "authors": [
            "Tsung-Han Wu",
            "Long Lian",
            "Joseph E. Gonzalez",
            "Boyi Li",
            "Trevor Darrell"
        ],
        "published": "2023-11-27T18:56:37Z",
        "summary": "Text-to-image generation has witnessed significant progress with the advent\nof diffusion models. Despite the ability to generate photorealistic images,\ncurrent text-to-image diffusion models still often struggle to accurately\ninterpret and follow complex input text prompts. In contrast to existing models\nthat aim to generate images only with their best effort, we introduce\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\ngenerates an image from the input prompt, assesses its alignment with the\nprompt, and performs self-corrections on the inaccuracies in the generated\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\niterative closed-loop process, ensuring correctness in the resulting image. SLD\nis not only training-free but can also be seamlessly integrated with diffusion\nmodels behind API access, such as DALL-E 3, to further boost the performance of\nstate-of-the-art diffusion models. Experimental results show that our approach\ncan rectify a majority of incorrect generations, particularly in generative\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\nbridging the gap between text-to-image generation and image editing pipelines.\nWe will make our code available for future research and applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.16090v1.pdf"
    },
    {
        "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
        "authors": [
            "Dmitri Roussinov",
            "Serge Sharoff"
        ],
        "published": "2023-11-27T18:53:31Z",
        "summary": "While performance of many text classification tasks has been recently\nimproved due to Pre-trained Language Models (PLMs), in this paper we show that\nthey still suffer from a performance gap when the underlying distribution of\ntopics changes. For example, a genre classifier trained on \\textit{political}\ntopics often fails when tested on documents about \\textit{sport} or\n\\textit{medicine}. In this work, we quantify this phenomenon empirically with a\nlarge corpus and a large set of topics. Consequently, we verify that domain\ntransfer remains challenging both for classic PLMs, such as BERT, and for\nmodern large models, such as GPT-3. We also suggest and successfully test a\npossible remedy: after augmenting the training dataset with\ntopically-controlled synthetic texts, the F1 score improves by up to 50\\% for\nsome topics, nearing on-topic training results, while others show little to no\nimprovement. While our empirical results focus on genre classification, our\nmethodology is applicable to other classification tasks such as gender,\nauthorship, or sentiment classification. The code and data to replicate the\nexperiments are available at https://github.com/dminus1/genre",
        "pdf_link": "https://arxiv.org/pdf/2311.16083v1.pdf"
    },
    {
        "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
        "authors": [
            "Zeming Chen",
            "Alejandro Hern√°ndez Cano",
            "Angelika Romanou",
            "Antoine Bonnet",
            "Kyle Matoba",
            "Francesco Salvi",
            "Matteo Pagliardini",
            "Simin Fan",
            "Andreas K√∂pf",
            "Amirkeivan Mohtashami",
            "Alexandre Sallinen",
            "Alireza Sakhaeirad",
            "Vinitra Swamy",
            "Igor Krawczuk",
            "Deniz Bayazit",
            "Axel Marmet",
            "Syrielle Montariol",
            "Mary-Anne Hartley",
            "Martin Jaggi",
            "Antoine Bosselut"
        ],
        "published": "2023-11-27T18:49:43Z",
        "summary": "Large language models (LLMs) can potentially democratize access to medical\nknowledge. While many efforts have been made to harness and improve LLMs'\nmedical knowledge and reasoning capacities, the resulting models are either\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\nwhich restricts their abilities. In this work, we improve access to large-scale\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\nour adaptation of Nvidia's Megatron-LM distributed trainer), and extends\npretraining on a comprehensively curated medical corpus, including selected\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\nEvaluations using four major medical benchmarks show significant performance\ngains over several state-of-the-art baselines before and after task-specific\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\nbest public baseline in its parameter class and 3% over the strongest baseline\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\nthe MEDITRON model weights to drive open-source development of more capable\nmedical LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.16079v1.pdf"
    },
    {
        "title": "Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models",
        "authors": [
            "Stephen MacNeil",
            "Paul Denny",
            "Andrew Tran",
            "Juho Leinonen",
            "Seth Bernstein",
            "Arto Hellas",
            "Sami Sarsa",
            "Joanne Kim"
        ],
        "published": "2023-11-27T17:28:33Z",
        "summary": "Identifying and resolving logic errors can be one of the most frustrating\nchallenges for novices programmers. Unlike syntax errors, for which a compiler\nor interpreter can issue a message, logic errors can be subtle. In certain\nconditions, buggy code may even exhibit correct behavior -- in other cases, the\nissue might be about how a problem statement has been interpreted. Such errors\ncan be hard to spot when reading the code, and they can also at times be missed\nby automated tests. There is great educational potential in automatically\ndetecting logic errors, especially when paired with suitable feedback for\nnovices. Large language models (LLMs) have recently demonstrated surprising\nperformance for a range of computing tasks, including generating and explaining\ncode. These capabilities are closely linked to code syntax, which aligns with\nthe next token prediction behavior of LLMs. On the other hand, logic errors\nrelate to the runtime performance of code and thus may not be as well suited to\nanalysis by LLMs. To explore this, we investigate the performance of two\npopular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly\nexplanation of logic errors. We compare LLM performance with a large cohort of\nintroductory computing students $(n=964)$ solving the same error detection\ntask. Through a mixed-methods analysis of student and model responses, we\nobserve significant improvement in logic error identification between the\nprevious and current generation of LLMs, and find that both LLM generations\nsignificantly outperform students. We outline how such models could be\nintegrated into computing education tools, and discuss their potential for\nsupporting students when learning programming.",
        "pdf_link": "https://arxiv.org/pdf/2311.16017v1.pdf"
    },
    {
        "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models",
        "authors": [
            "Youssef Benchekroun",
            "Megi Dervishi",
            "Mark Ibrahim",
            "Jean-Baptiste Gaya",
            "Xavier Martinet",
            "Gr√©goire Mialon",
            "Thomas Scialom",
            "Emmanuel Dupoux",
            "Dieuwke Hupkes",
            "Pascal Vincent"
        ],
        "published": "2023-11-27T15:38:17Z",
        "summary": "We propose WorldSense, a benchmark designed to assess the extent to which\nLLMs are consistently able to sustain tacit world models, by testing how they\ndraw simple inferences from descriptions of simple arrangements of entities.\nWorldsense is a synthetic benchmark with three problem types, each with their\nown trivial control, which explicitly avoids bias by decorrelating the abstract\nstructure of problems from the vocabulary and expressions, and by decorrelating\nall problem subparts with the correct response. We run our benchmark on three\nstate-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these\nmodels make errors even with as few as three objects. Furthermore, they have\nquite heavy response biases, preferring certain responses irrespective of the\nquestion. Errors persist even with chain-of-thought prompting and in-context\nlearning. Lastly, we show that while finetuning on similar problems does result\nin substantial improvements -- within- and out-of-distribution -- the finetuned\nmodels do not generalise beyond a constraint problem space.",
        "pdf_link": "https://arxiv.org/pdf/2311.15930v1.pdf"
    },
    {
        "title": "vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training",
        "authors": [
            "Jehyeon Bang",
            "Yujeong Choi",
            "Myeongwoo Kim",
            "Yongdeok Kim",
            "Minsoo Rhu"
        ],
        "published": "2023-11-27T13:35:15Z",
        "summary": "As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.",
        "pdf_link": "https://arxiv.org/pdf/2312.12391v1.pdf"
    },
    {
        "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
        "authors": [
            "Nianwen Si",
            "Hao Zhang",
            "Heyu Chang",
            "Wenlin Zhang",
            "Dan Qu",
            "Weiqiang Zhang"
        ],
        "published": "2023-11-27T12:37:51Z",
        "summary": "In recent years, large language models (LLMs) have spurred a new research\nparadigm in natural language processing. Despite their excellent capability in\nknowledge-based question answering and reasoning, their potential to retain\nfaulty or even harmful knowledge poses risks of malicious application. The\nchallenge of mitigating this issue and transforming these models into purer\nassistants is crucial for their widespread applicability. Unfortunately,\nRetraining LLMs repeatedly to eliminate undesirable knowledge is impractical\ndue to their immense parameters. Knowledge unlearning, derived from analogous\nstudies on machine unlearning, presents a promising avenue to address this\nconcern and is notably advantageous in the context of LLMs. It allows for the\nremoval of harmful knowledge in an efficient manner, without affecting\nunrelated knowledge in the model. To this end, we provide a survey of knowledge\nunlearning in the era of LLMs. Firstly, we formally define the knowledge\nunlearning problem and distinguish it from related works. Subsequently, we\ncategorize existing knowledge unlearning methods into three classes: those\nbased on parameter optimization, parameter merging, and in-context learning,\nand introduce details of these unlearning methods. We further present\nevaluation datasets used in existing methods, and finally conclude this survey\nby presenting the ongoing challenges and future directions.",
        "pdf_link": "https://arxiv.org/pdf/2311.15766v2.pdf"
    },
    {
        "title": "Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications",
        "authors": [
            "Sabine Wehnert"
        ],
        "published": "2023-11-27T10:59:16Z",
        "summary": "In this work, I discuss how Large Language Models can be applied in the legal\ndomain, circumventing their current drawbacks. Despite their large success and\nacceptance, their lack of explainability hinders legal experts to trust in\ntheir output, and this happens rightfully so. However, in this paper, I argue\nin favor of a new view, Justifiable Artificial Intelligence, instead of\nfocusing on Explainable Artificial Intelligence. I discuss in this paper how\ngaining evidence for and against a Large Language Model's output may make their\ngenerated texts more trustworthy - or hold them accountable for misinformation.",
        "pdf_link": "https://arxiv.org/pdf/2311.15716v1.pdf"
    },
    {
        "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks",
        "authors": [
            "Yaran Chen",
            "Wenbo Cui",
            "Yuanwen Chen",
            "Mining Tan",
            "Xinyao Zhang",
            "Dongbin Zhao",
            "He Wang"
        ],
        "published": "2023-11-27T09:20:23Z",
        "summary": "Robotic agents must master common sense and long-term sequential decisions to\nsolve daily tasks through natural language instruction. The developments in\nLarge Language Models (LLMs) in natural language processing have inspired\nefforts to use LLMs in complex robot planning. Despite LLMs' great\ngeneralization and comprehension of instruction tasks, LLMs-generated task\nplans sometimes lack feasibility and correctness. To address the problem, we\npropose a RoboGPT agent\\footnote{our code and dataset will be released soon}\nfor making embodied long-term decisions for daily tasks, with two modules: 1)\nLLMs-based planning with re-plan to break the task into multiple sub-goals; 2)\nRoboSkill individually designed for sub-goals to learn better navigation and\nmanipulation skills. The LLMs-based planning is enhanced with a new robotic\ndataset and re-plan, called RoboGPT. The new robotic dataset of 67k daily\ninstruction tasks is gathered for fine-tuning the Llama model and obtaining\nRoboGPT. RoboGPT planner with strong generalization can plan hundreds of daily\ninstruction tasks. Additionally, a low-computational Re-Plan module is designed\nto allow plans to flexibly adapt to the environment, thereby addressing the\nnomenclature diversity challenge. The proposed RoboGPT agent outperforms SOTA\nmethods on the ALFRED daily tasks. Moreover, RoboGPT planner exceeds SOTA\nLLM-based planners like ChatGPT in task-planning rationality for hundreds of\nunseen daily tasks, and even other domain tasks, while keeping the large\nmodel's original broad application and generality.",
        "pdf_link": "https://arxiv.org/pdf/2311.15649v1.pdf"
    },
    {
        "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
        "authors": [
            "Yuhui Zhang",
            "Brandon McKinzie",
            "Zhe Gan",
            "Vaishaal Shankar",
            "Alexander Toshev"
        ],
        "published": "2023-11-27T07:19:26Z",
        "summary": "Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.",
        "pdf_link": "https://arxiv.org/pdf/2311.16201v1.pdf"
    },
    {
        "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
        "authors": [
            "Xupeng Miao",
            "Chunan Shi",
            "Jiangfei Duan",
            "Xiaoli Xi",
            "Dahua Lin",
            "Bin Cui",
            "Zhihao Jia"
        ],
        "published": "2023-11-27T06:31:17Z",
        "summary": "The high computational and memory requirements of generative large language\nmodels (LLMs) make it challenging to serve them cheaply. This paper aims to\nreduce the monetary cost for serving LLMs by leveraging preemptible GPU\ninstances on modern clouds, which offer accesses to spare GPUs at a much\ncheaper price than regular instances but may be preempted by the cloud at any\ntime. Serving LLMs on preemptible instances requires addressing challenges\ninduced by frequent instance preemptions and the necessity of migrating\ninstances to handle these preemptions.\n  This paper presents SpotServe, the first distributed LLM serving system on\npreemptible instances. Several key techniques in SpotServe realize fast and\nreliable serving of generative LLMs on cheap preemptible instances. First,\nSpotServe dynamically adapts the LLM parallelization configuration for dynamic\ninstance availability and fluctuating workload, while balancing the trade-off\namong the overall throughput, inference latency and monetary costs. Second, to\nminimize the cost of migrating instances for dynamic reparallelization, the\ntask of migrating instances is formulated as a bipartite graph matching\nproblem, which uses the Kuhn-Munkres algorithm to identify an optimal migration\nplan that minimizes communications. Finally, to take advantage of the grace\nperiod offered by modern clouds, we introduce stateful inference recovery, a\nnew inference mechanism that commits inference progress at a much finer\ngranularity and allows SpotServe to cheaply resume inference upon preemption.\nWe evaluate on real spot instance preemption traces and various popular LLMs\nand show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1x compared\nwith the best existing LLM serving systems. We also show that SpotServe can\nleverage the price advantage of preemptive instances, saving 54% monetary cost\ncompared with only using on-demand instances.",
        "pdf_link": "https://arxiv.org/pdf/2311.15566v1.pdf"
    },
    {
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
        "authors": [
            "Haoqiang Kang",
            "Xiao-Yang Liu"
        ],
        "published": "2023-11-27T05:27:13Z",
        "summary": "The hallucination issue is recognized as a fundamental deficiency of large\nlanguage models (LLMs), especially when applied to fields such as finance,\neducation, and law. Despite the growing concerns, there has been a lack of\nempirical investigation. In this paper, we provide an empirical examination of\nLLMs' hallucination behaviors in financial tasks. First, we empirically\ninvestigate LLM model's ability of explaining financial concepts and\nterminologies. Second, we assess LLM models' capacity of querying historical\nstock prices. Third, to alleviate the hallucination issue, we evaluate the\nefficacy of four practical methods, including few-shot learning, Decoding by\nContrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method\nand the prompt-based tool learning method for a function to generate a query\ncommand. Finally, our major finding is that off-the-shelf LLMs experience\nserious hallucination behaviors in financial tasks. Therefore, there is an\nurgent need to call for research efforts in mitigating LLMs' hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2311.15548v1.pdf"
    },
    {
        "title": "Function-constrained Program Synthesis",
        "authors": [
            "Patrick Hajali",
            "Ignas Budvytis"
        ],
        "published": "2023-11-27T02:55:34Z",
        "summary": "This work introduces (1) a technique that allows large language models (LLMs)\nto leverage user-provided code when solving programming tasks and (2) a method\nto iteratively generate modular sub-functions that can aid future code\ngeneration attempts when the initial code generated by the LLM is inadequate.\nGenerating computer programs in general-purpose programming languages like\nPython poses a challenge for LLMs when instructed to use code provided in the\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\ncompletions in real-time by drawing on all code available in a development\nenvironment. However, restricting code-specific LLMs to use only in-context\ncode is not straightforward, as the model is not explicitly instructed to use\nthe user-provided code and users cannot highlight precisely which snippets of\ncode the model should incorporate into its context. Moreover, current systems\nlack effective recovery methods, forcing users to iteratively re-prompt the\nmodel with modified prompts until a sufficient solution is reached. Our method\ndiffers from traditional LLM-powered code-generation by constraining\ncode-generation to an explicit function set and enabling recovery from failed\nattempts through automatically generated sub-functions. When the LLM cannot\nproduce working code, we generate modular sub-functions to aid subsequent\nattempts at generating functional code. A by-product of our method is a library\nof reusable sub-functions that can solve related tasks, imitating a software\nteam where efficiency scales with experience. We also introduce a new\n\"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\nmethod encourages models to output solutions in a structured format, decreasing\nsyntax errors that can be mistaken for poor coding ability.",
        "pdf_link": "https://arxiv.org/pdf/2311.15500v2.pdf"
    },
    {
        "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
        "authors": [
            "Junyuan Hong",
            "Jiachen T. Wang",
            "Chenhui Zhang",
            "Zhangheng Li",
            "Bo Li",
            "Zhangyang Wang"
        ],
        "published": "2023-11-27T02:01:10Z",
        "summary": "Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .",
        "pdf_link": "https://arxiv.org/pdf/2312.03724v2.pdf"
    },
    {
        "title": "Machine-Generated Text Detection using Deep Learning",
        "authors": [
            "Raghav Gaggar",
            "Ashish Bhagchandani",
            "Harsh Oza"
        ],
        "published": "2023-11-26T21:16:01Z",
        "summary": "Our research focuses on the crucial challenge of discerning text produced by\nLarge Language Models (LLMs) from human-generated text, which holds\nsignificance for various applications. With ongoing discussions about attaining\na model with such functionality, we present supporting evidence regarding the\nfeasibility of such models. We evaluated our models on multiple datasets,\nincluding Twitter Sentiment, Football Commentary, Project Gutenberg, PubMedQA,\nand SQuAD, confirming the efficacy of the enhanced detection approaches. These\ndatasets were sampled with intricate constraints encompassing every\npossibility, laying the foundation for future research. We evaluate\nGPT-3.5-Turbo against various detectors such as SVM, RoBERTa-base, and\nRoBERTa-large. Based on the research findings, the results predominantly relied\non the sequence length of the sentence.",
        "pdf_link": "https://arxiv.org/pdf/2311.15425v1.pdf"
    },
    {
        "title": "KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All",
        "authors": [
            "Quyen Tran",
            "Lam Tran",
            "Khoat Than",
            "Toan Tran",
            "Dinh Phung",
            "Trung Le"
        ],
        "published": "2023-11-26T20:35:19Z",
        "summary": "Drawing inspiration from prompt tuning techniques applied to Large Language\nModels, recent methods based on pre-trained ViT networks have achieved\nremarkable results in the field of Continual Learning. Specifically, these\napproaches propose to maintain a set of prompts and allocate a subset of them\nto learn each task using a key-query matching strategy. However, they may\nencounter limitations when lacking control over the correlations between old\ntask queries and keys of future tasks, the shift of features in the latent\nspace, and the relative separation of latent vectors learned in independent\ntasks. In this work, we introduce a novel key-query learning strategy based on\northogonal projection, inspired by model-agnostic meta-learning, to enhance\nprompt matching efficiency and address the challenge of shifting features.\nFurthermore, we introduce a One-Versus-All (OVA) prototype-based component that\nenhances the classification head distinction. Experimental results on benchmark\ndatasets demonstrate that our method empowers the model to achieve results\nsurpassing those of current state-of-the-art approaches by a large margin of up\nto 20%.",
        "pdf_link": "https://arxiv.org/pdf/2311.15414v2.pdf"
    },
    {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
        "authors": [
            "Xun Liang",
            "Shichao Song",
            "Simin Niu",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Bo Tang",
            "Zhaohui Wy",
            "Dawei He",
            "Peng Cheng",
            "Zhonghao Wang",
            "Haiying Deng"
        ],
        "published": "2023-11-26T13:42:56Z",
        "summary": "Large language models (LLMs) have emerged as pivotal contributors in\ncontemporary natural language processing and are increasingly being applied\nacross a diverse range of industries. However, these large-scale probabilistic\nstatistical models cannot currently ensure the requisite quality in\nprofessional content generation. These models often produce hallucinated text,\ncompromising their practical utility in professional contexts. To assess the\nauthentic reliability of LLMs in text generation, numerous initiatives have\ndeveloped benchmark evaluations for hallucination phenomena. Nevertheless,\nthese benchmarks frequently utilize constrained generation techniques due to\ncost and temporal constraints. These techniques encompass the use of directed\nhallucination induction and strategies that deliberately alter authentic text\nto produce hallucinations. These approaches are not congruent with the\nunrestricted text generation demanded by real-world applications. Furthermore,\na well-established Chinese-language dataset dedicated to the evaluation of\nhallucinations in text generation is presently lacking. Consequently, we have\ndeveloped an Unconstrained Hallucination Generation Evaluation (UHGEval)\nbenchmark, designed to compile outputs produced with minimal restrictions by\nLLMs. Concurrently, we have established a comprehensive benchmark evaluation\nframework to aid subsequent researchers in undertaking scalable and\nreproducible experiments. We have also executed extensive experiments,\nevaluating prominent Chinese language models and the GPT series models to\nderive professional performance insights regarding hallucination challenges.",
        "pdf_link": "https://arxiv.org/pdf/2311.15296v2.pdf"
    },
    {
        "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
        "authors": [
            "Johannes Schneider",
            "Steffi Haag",
            "Leona Chandra Kruse"
        ],
        "published": "2023-11-26T08:44:58Z",
        "summary": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.03720v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Model Volatility",
        "authors": [
            "Boyang Yu"
        ],
        "published": "2023-11-26T03:54:03Z",
        "summary": "The impact of non-deterministic outputs from Large Language Models (LLMs) is\nnot well examined for financial text understanding tasks. Through a compelling\ncase study on investing in the US equity market via news sentiment analysis, we\nuncover substantial variability in sentence-level sentiment classification\nresults, underscoring the innate volatility of LLM outputs. These uncertainties\ncascade downstream, leading to more significant variations in portfolio\nconstruction and return. While tweaking the temperature parameter in the\nlanguage model decoder presents a potential remedy, it comes at the expense of\nstifled creativity. Similarly, while ensembling multiple outputs mitigates the\neffect of volatile outputs, it demands a notable computational investment. This\nwork furnishes practitioners with invaluable insights for adeptly navigating\nuncertainty in the integration of LLMs into financial decision-making,\nparticularly in scenarios dictated by non-deterministic information.",
        "pdf_link": "https://arxiv.org/pdf/2311.15180v1.pdf"
    },
    {
        "title": "Large Language Models in Law: A Survey",
        "authors": [
            "Jinqi Lai",
            "Wensheng Gan",
            "Jiayang Wu",
            "Zhenlian Qi",
            "Philip S. Yu"
        ],
        "published": "2023-11-26T00:48:12Z",
        "summary": "The advent of artificial intelligence (AI) has significantly impacted the\ntraditional judicial industry. Moreover, recently, with the development of\nAI-generated content (AIGC), AI and law have found applications in various\ndomains, including image recognition, automatic text generation, and\ninteractive chat. With the rapid emergence and growing popularity of large\nmodels, it is evident that AI will drive transformation in the traditional\njudicial industry. However, the application of legal large language models\n(LLMs) is still in its nascent stage. Several challenges need to be addressed.\nIn this paper, we aim to provide a comprehensive survey of legal LLMs. We not\nonly conduct an extensive survey of LLMs, but also expose their applications in\nthe judicial system. We first provide an overview of AI technologies in the\nlegal field and showcase the recent research in LLMs. Then, we discuss the\npractical implementation presented by legal LLMs, such as providing legal\nadvice to users and assisting judges during trials. In addition, we explore the\nlimitations of legal LLMs, including data, algorithms, and judicial practice.\nFinally, we summarize practical recommendations and propose future development\ndirections to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2312.03718v1.pdf"
    },
    {
        "title": "Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains",
        "authors": [
            "Chia-Chien Hung",
            "Wiem Ben Rim",
            "Lindsay Frost",
            "Lars Bruckner",
            "Carolin Lawrence"
        ],
        "published": "2023-11-25T08:58:07Z",
        "summary": "High-risk domains pose unique challenges that require language models to\nprovide accurate and safe responses. Despite the great success of large\nlanguage models (LLMs), such as ChatGPT and its variants, their performance in\nhigh-risk domains remains unclear. Our study delves into an in-depth analysis\nof the performance of instruction-tuned LLMs, focusing on factual accuracy and\nsafety adherence. To comprehensively assess the capabilities of LLMs, we\nconduct experiments on six NLP datasets including question answering and\nsummarization tasks within two high-risk domains: legal and medical. Further\nqualitative analysis highlights the existing limitations inherent in current\nLLMs when evaluating in high-risk domains. This underscores the essential\nnature of not only improving LLM capabilities but also prioritizing the\nrefinement of domain-specific metrics, and embracing a more human-centric\napproach to enhance safety and factual reliability. Our findings advance the\nfield toward the concerns of properly evaluating LLMs in high-risk domains,\naiming to steer the adaptability of LLMs in fulfilling societal obligations and\naligning with forthcoming regulations, such as the EU AI Act.",
        "pdf_link": "https://arxiv.org/pdf/2311.14966v1.pdf"
    },
    {
        "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
        "authors": [
            "Xiuyuan Chen",
            "Yuan Lin",
            "Yuchen Zhang",
            "Weiran Huang"
        ],
        "published": "2023-11-25T02:46:12Z",
        "summary": "We propose a novel and challenging benchmark, AutoEval-Video, to\ncomprehensively evaluate large vision-language models in open-ended video\nquestion answering. The comprehensiveness of AutoEval-Video is demonstrated in\ntwo aspects: 1) AutoEval-Video constructs open-ended video-questions across 9\nskill dimensions, addressing capabilities of perception, comprehension, and\ngeneration. 2) AutoEval-Video contains newly collected videos that cover over\n40 distinct themes. To efficiently evaluate responses to the open-ended\nquestions, we employ an LLM-based evaluation approach, but instead of merely\nproviding a reference answer, we annotate unique evaluation rules for every\nsingle instance (video-question pair). To maximize the robustness of these\nrules, we develop a novel adversarial annotation mechanism. By using\ninstance-specific rules as prompt, GPT-4, as an automatic evaluator, can\nachieve a stable evaluation accuracy of around 97.0\\%, comparable to the 94.9\\%\n- 97.5\\% accuracy of a human evaluator. Furthermore, we assess the performance\nof eight large vision-language models on AutoEval-Video. Among them,\nGPT-4V(ision) significantly outperforms other models, achieving an accuracy of\n32.2\\%. However, there is still substantial room for improvement compared to\nhuman accuracy of 72.8\\%. By conducting an extensive case study, we uncover\nseveral drawbacks of GPT-4V, such as limited temporal and dynamic\ncomprehension, and overly general responses. Code is available at\n\\href{https://github.com/Xiuyuan-Chen/AutoEval-Video}{\\color{magenta}https://github.com/Xiuyuan-Chen/AutoEval-Video}.",
        "pdf_link": "https://arxiv.org/pdf/2311.14906v1.pdf"
    },
    {
        "title": "Gender inference: can chatGPT outperform common commercial tools?",
        "authors": [
            "Michelle Alexopoulos",
            "Kelly Lyons",
            "Kaushar Mahetaji",
            "Marcus Emmanuel Barnes",
            "Rogan Gutwillinger"
        ],
        "published": "2023-11-24T22:09:14Z",
        "summary": "An increasing number of studies use gender information to understand\nphenomena such as gender bias, inequity in access and participation, or the\nimpact of the Covid pandemic response. Unfortunately, most datasets do not\ninclude self-reported gender information, making it necessary for researchers\nto infer gender from other information, such as names or names and country\ninformation. An important limitation of these tools is that they fail to\nappropriately capture the fact that gender exists on a non-binary scale,\nhowever, it remains important to evaluate and compare how well these tools\nperform in a variety of contexts. In this paper, we compare the performance of\na generative Artificial Intelligence (AI) tool ChatGPT with three commercially\navailable list-based and machine learning-based gender inference tools (Namsor,\nGender-API, and genderize.io) on a unique dataset. Specifically, we use a large\nOlympic athlete dataset and report how variations in the input (e.g., first\nname and first and last name, with and without country information) impact the\naccuracy of their predictions. We report results for the full set, as well as\nfor the subsets: medal versus non-medal winners, athletes from the largest\nEnglish-speaking countries, and athletes from East Asia. On these sets, we find\nthat Namsor is the best traditional commercially available tool. However,\nChatGPT performs at least as well as Namsor and often outperforms it,\nespecially for the female sample when country and/or last name information is\navailable. All tools perform better on medalists versus non-medalists and on\nnames from English-speaking countries. Although not designed for this purpose,\nChatGPT may be a cost-effective tool for gender prediction. In the future, it\nmight even be possible for ChatGPT or other large scale language models to\nbetter identify self-reported gender rather than report gender on a binary\nscale.",
        "pdf_link": "https://arxiv.org/pdf/2312.00805v1.pdf"
    },
    {
        "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
        "authors": [
            "Raghav Addanki",
            "Chenyang Li",
            "Zhao Song",
            "Chiwun Yang"
        ],
        "published": "2023-11-24T18:35:00Z",
        "summary": "Attention computation takes both the time complexity of $O(n^2)$ and the\nspace complexity of $O(n^2)$ simultaneously, which makes deploying Large\nLanguage Models (LLMs) in streaming applications that involve long contexts\nrequiring substantial computational resources. In recent OpenAI DevDay (Nov 6,\n2023), OpenAI released a new model that is able to support a 128K-long\ndocument, in our paper, we focus on the memory-efficient issue when context\nlength $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer\nself-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n\n\\times d}$, the polynomial method approximates the attention output $T \\in\n\\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in\n\\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$\ncomputation within $n^{1+o(1)}$ time executions. Despite this, computing the\napproximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still\nnecessitates $O(n^2)$ space, leading to significant memory usage. In response\nto these challenges, we introduce a new algorithm that only reads one pass of\nthe data in a streaming fashion. This method employs sublinear space $o(n)$ to\nstore three sketch matrices, alleviating the need for exact $K, V$ storage.\nNotably, our algorithm exhibits exceptional memory-efficient performance with\nsuper-long tokens. As the token length $n$ increases, our error guarantee\ndiminishes while the memory usage remains nearly constant. This unique\nattribute underscores the potential of our technique in efficiently handling\nLLMs in streaming applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.14652v2.pdf"
    },
    {
        "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
        "authors": [
            "Yuanfeng Ji",
            "Chongjian Ge",
            "Weikai Kong",
            "Enze Xie",
            "Zhengying Liu",
            "Zhengguo Li",
            "Ping Luo"
        ],
        "published": "2023-11-24T16:12:05Z",
        "summary": "With the advancements in Large Language Models (LLMs), Vision-Language Models\n(VLMs) have reached a new level of sophistication, showing notable competence\nin executing intricate cognition and reasoning tasks. However, existing\nevaluation benchmarks, primarily relying on rigid, hand-crafted datasets to\nmeasure task-specific performance, face significant limitations in assessing\nthe alignment of these increasingly anthropomorphic models with human\nintelligence. In this work, we address the limitations via Auto-Bench, which\ndelves into exploring LLMs as proficient aligners, measuring the alignment\nbetween VLMs and human intelligence and value through automatic data curation\nand assessment. Specifically, for data curation, Auto-Bench utilizes LLMs\n(e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning\ntriplets via prompting on visual symbolic representations (e.g., captions,\nobject locations, instance relationships, and etc.). The curated data closely\nmatches human intent, owing to the extensive world knowledge embedded in LLMs.\nThrough this pipeline, a total of 28.5K human-verified and 3,504K unfiltered\nquestion-answer-reasoning triplets have been curated, covering 4 primary\nabilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to\nserve as judges, implementing the quantitative and qualitative automated\nassessments to facilitate a comprehensive evaluation of VLMs. Our validation\nresults reveal that LLMs are proficient in both evaluation data curation and\nmodel assessment, achieving an average agreement rate of 85%. We envision\nAuto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating\nthe evolving sophisticated VLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14580v1.pdf"
    },
    {
        "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
        "authors": [
            "Di Jin",
            "Shikib Mehri",
            "Devamanyu Hazarika",
            "Aishwarya Padmakumar",
            "Sungjin Lee",
            "Yang Liu",
            "Mahdi Namazifar"
        ],
        "published": "2023-11-24T15:20:36Z",
        "summary": "Learning from human feedback is a prominent technique to align the output of\nlarge language models (LLMs) with human expectations. Reinforcement learning\nfrom human feedback (RLHF) leverages human preference signals that are in the\nform of ranking of response pairs to perform this alignment. However, human\npreference on LLM outputs can come in much richer forms including natural\nlanguage, which may provide detailed feedback on strengths and weaknesses of a\ngiven response. In this work we investigate data efficiency of modeling human\nfeedback that is in natural language. Specifically, we fine-tune an open-source\nLLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or\neven less) of human feedback in natural language in the form of critiques and\nrevisions of responses. We show that this model is able to improve the quality\nof responses from even some of the strongest LLMs such as ChatGPT, BARD, and\nVicuna, through critique and revision of those responses. For instance, through\none iteration of revision of ChatGPT responses, the revised responses have\n56.6% win rate over the original ones, and this win rate can be further\nimproved to 65.9% after applying the revision for five iterations.",
        "pdf_link": "https://arxiv.org/pdf/2311.14543v1.pdf"
    },
    {
        "title": "Machine Translation for Ge'ez Language",
        "authors": [
            "Aman Kassahun Wassie"
        ],
        "published": "2023-11-24T14:55:23Z",
        "summary": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient\nlanguage that is no longer the native language of any community, faces\nchallenges such as out-of-vocabulary words, domain mismatches, and lack of\nsufficient labeled training data. In this work, we explore various methods to\nimprove Ge'ez MT, including transfer-learning from related languages,\noptimizing shared vocabulary and token segmentation approaches, finetuning\nlarge pre-trained models, and using large language models (LLMs) for few-shot\ntranslation with fuzzy matches. We develop a multilingual neural machine\ntranslation (MNMT) model based on languages relatedness, which brings an\naverage performance improvement of about 4 BLEU compared to standard bilingual\nmodels. We also attempt to finetune the NLLB-200 model, one of the most\nadvanced translation models available today, but find that it performs poorly\nwith only 4k training samples for Ge'ez. Furthermore, we experiment with using\nGPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches,\nwhich leverages embedding similarity-based retrieval to find context examples\nfrom a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU\nscore of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT\nbaseline of 15.2. Our work provides insights into the potential and limitations\nof different approaches for low-resource and ancient language MT.",
        "pdf_link": "https://arxiv.org/pdf/2311.14530v2.pdf"
    },
    {
        "title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review",
        "authors": [
            "Ming Li",
            "Ariunaa Enkhtur",
            "Beverley Anne Yamamoto",
            "Fei Cheng"
        ],
        "published": "2023-11-24T10:00:23Z",
        "summary": "ChatGPT and other Generative Artificial Intelligence (GAI) models tend to\ninherit and even amplify prevailing societal biases as they are trained on\nlarge amounts of existing data. Given the increasing usage of ChatGPT and other\nGAI by students, faculty members, and staff in higher education institutions\n(HEIs), there is an urgent need to examine the ethical issues involved such as\nits potential biases. In this scoping review, we clarify the ways in which\nbiases related to GAI in higher education settings have been discussed in\nrecent academic publications and identify what type of potential biases are\ncommonly reported in this body of literature. We searched for academic articles\nwritten in English, Chinese, and Japanese across four main databases concerned\nwith GAI usage in higher education and bias. Our findings show that while there\nis an awareness of potential biases around large language models (LLMs) and\nGAI, the majority of articles touch on ``bias'' at a relatively superficial\nlevel. Few identify what types of bias may occur under what circumstances.\nNeither do they discuss the possible implications for the higher education,\nstaff, faculty members, or students. There is a notable lack of empirical work\nat this point, and we call for higher education researchers and AI experts to\nconduct more research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2311.14381v1.pdf"
    },
    {
        "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
        "authors": [
            "Wu Zekun",
            "Sahan Bulathwela",
            "Adriano Soares Koshiyama"
        ],
        "published": "2023-11-23T17:47:14Z",
        "summary": "Large Language Models (LLM) have made significant advances in the recent past\nbecoming more mainstream in Artificial Intelligence (AI) enabled human-facing\napplications. However, LLMs often generate stereotypical output inherited from\nhistorical data, amplifying societal biases and raising ethical concerns. This\nwork introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751\ninstances of gender, race, profession and religion stereotypic text and ii) a\nnovel stereotype classifier for English text. We design several experiments to\nrigorously test the proposed model trained on the novel dataset. Our\nexperiments show that training the model in a multi-class setting can\noutperform the one-vs-all binary counterpart. Consistent feature importance\nsignals from different eXplainable AI tools demonstrate that the new model\nexploits relevant text features. We utilise the newly created model to assess\nthe stereotypic behaviour of the popular GPT family of models and observe the\nreduction of bias over time. In summary, our work establishes a robust and\npractical framework for auditing and evaluating the stereotypic bias in LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.14126v1.pdf"
    },
    {
        "title": "Auditing and Mitigating Cultural Bias in LLMs",
        "authors": [
            "Yan Tao",
            "Olga Viberg",
            "Ryan S. Baker",
            "Rene F. Kizilcec"
        ],
        "published": "2023-11-23T16:45:56Z",
        "summary": "Culture fundamentally shapes people's reasoning, behavior, and communication.\nGenerative artificial intelligence (AI) technologies may cause a shift towards\na dominant culture. As people increasingly use AI to expedite and even automate\nvarious professional and personal tasks, cultural values embedded in AI models\nmay bias authentic expression. We audit large language models for cultural\nbias, comparing their responses to nationally representative survey data, and\nevaluate country-specific prompting as a mitigation strategy. We find that\nGPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and\nProtestant European countries. Our mitigation strategy reduces cultural bias in\nrecent models but not for all countries/territories. To avoid cultural bias in\ngenerative AI, especially in high-stakes contexts, we suggest using culture\nmatching and ongoing cultural audits.",
        "pdf_link": "https://arxiv.org/pdf/2311.14096v1.pdf"
    },
    {
        "title": "PrivateLoRA For Efficient Privacy Preserving LLM",
        "authors": [
            "Yiming Wang",
            "Yu Lin",
            "Xiaodong Zeng",
            "Guannan Zhang"
        ],
        "published": "2023-11-23T14:36:30Z",
        "summary": "End users face a choice between privacy and efficiency in current Large\nLanguage Model (LLM) service paradigms. In cloud-based paradigms, users are\nforced to compromise data locality for generation quality and processing speed.\nConversely, edge device paradigms maintain data locality but fail to deliver\nsatisfactory performance. In this work, we propose a novel LLM service paradigm\nthat distributes privacy-sensitive computation on edge devices and shared\ncomputation in the cloud. Only activations are transmitted between the central\ncloud and edge devices to ensure data locality. Our core innovation,\nPrivateLoRA, addresses the challenging communication overhead by exploiting the\nlow rank of residual activations, achieving over 95% communication reduction.\nConsequently, PrivateLoRA effectively maintains data locality and is extremely\nresource efficient. Under standard 5G networks, PrivateLoRA achieves throughput\nover 300% of device-only solutions for 7B models and over 80% of an A100 GPU\nfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRA\nfor advanced personalization. Our approach democratizes access to\nstate-of-the-art generative AI for edge devices, paving the way for more\ntailored LLM experiences for the general public. To our knowledge, our proposed\nframework is the first efficient and privacy-preserving LLM solution in the\nliterature.",
        "pdf_link": "https://arxiv.org/pdf/2311.14030v1.pdf"
    },
    {
        "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
        "authors": [
            "Shulin Cao",
            "Jiajie Zhang",
            "Jiaxin Shi",
            "Xin Lv",
            "Zijun Yao",
            "Qi Tian",
            "Juanzi Li",
            "Lei Hou"
        ],
        "published": "2023-11-23T12:52:37Z",
        "summary": "Large language models (LLMs) are capable of answering knowledge-intensive\ncomplex questions with chain-of-thought (CoT) reasoning. However, they tend to\ngenerate factually incorrect reasoning steps when the required knowledge is not\navailable or up-to-date in models' parameters. Recent works turn to retrieving\nexternal knowledge to augment CoT reasoning. Despite being promising, these\nchain-based methods suffer from: 1) Negative retrieval. Unnecessary or\nincorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the\nability to look backward or forward, a local error in one step will propagate\nalong the chain.\n  In this paper, we propose a novel approach: Probabilistic Tree-of-thought\nReasoning (ProbTree). First, LLMs translate a complex question into a query\ntree, in which each non-root node denotes a sub-question of its parent node.\nThen, probabilistic reasoning is conducted over the tree, by solving questions\nfrom leaf to root considering the confidence of both question decomposing and\nanswering. During reasoning, for leaf nodes, LLMs choose a more confident\nanswer from Closed-book QA that employs parametric knowledge and Open-book QA\nthat employs retrieved external knowledge, thus eliminating the negative\nretrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs\nhave broader sights and are able to globally reason with the information from\nchild nodes, thus recovering from local errors. The experiments on three\nComplex QA datasets under the open-domain setting show that our approach\noutperforms SOTA methods significantly, demonstrating the effect of\nprobabilistic tree-of-thought reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.13982v1.pdf"
    },
    {
        "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations",
        "authors": [
            "John Mendon√ßa",
            "Patr√≠cia Pereira",
            "Miguel Menezes",
            "Vera Cabarr√£o",
            "Ana C. Farinha",
            "Helena Moniz",
            "Jo√£o Paulo Carvalho",
            "Alon Lavie",
            "Isabel Trancoso"
        ],
        "published": "2023-11-23T10:56:14Z",
        "summary": "Task-oriented conversational datasets often lack topic variability and\nlinguistic diversity. However, with the advent of Large Language Models (LLMs)\npretrained on extensive, multilingual and diverse text data, these limitations\nseem overcome. Nevertheless, their generalisability to different languages and\ndomains in dialogue applications remains uncertain without benchmarking\ndatasets. This paper presents a holistic annotation approach for emotion and\nconversational quality in the context of bilingual customer support\nconversations. By performing annotations that take into consideration the\ncomplete instances that compose a conversation, one can form a broader\nperspective of the dialogue as a whole. Furthermore, it provides a unique and\nvaluable resource for the development of text classification models. To this\nend, we present benchmarks for Emotion Recognition and Dialogue Quality\nEstimation and show that further research is needed to leverage these models in\na production setting.",
        "pdf_link": "https://arxiv.org/pdf/2311.13910v1.pdf"
    },
    {
        "title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach",
        "authors": [
            "Bin Zhang",
            "Hangyu Mao",
            "Jingqing Ruan",
            "Ying Wen",
            "Yang Li",
            "Shao Zhang",
            "Zhiwei Xu",
            "Dapeng Li",
            "Ziyue Li",
            "Rui Zhao",
            "Lijuan Li",
            "Guoliang Fan"
        ],
        "published": "2023-11-23T10:14:58Z",
        "summary": "The remarkable progress in Large Language Models (LLMs) opens up new avenues\nfor addressing planning and decision-making problems in Multi-Agent Systems\n(MAS). However, as the number of agents increases, the issues of hallucination\nin LLMs and coordination in MAS have become increasingly prominent.\nAdditionally, the efficient utilization of tokens emerges as a critical\nconsideration when employing LLMs to facilitate the interactions among a\nsubstantial number of agents. In this paper, we develop a modular framework\ncalled LLaMAC to mitigate these challenges. LLaMAC implements a value\ndistribution encoding similar to that found in the human brain, utilizing\ninternal and external feedback mechanisms to facilitate collaboration and\niterative reasoning among its modules. Through evaluations involving system\nresource allocation and robot grid transportation, we demonstrate the\nconsiderable advantages afforded by our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2311.13884v3.pdf"
    },
    {
        "title": "Compositional Zero-shot Learning via Progressive Language-based Observations",
        "authors": [
            "Lin Li",
            "Guikun Chen",
            "Jun Xiao",
            "Long Chen"
        ],
        "published": "2023-11-23T10:14:23Z",
        "summary": "Compositional zero-shot learning aims to recognize unseen state-object\ncompositions by leveraging known primitives (state and object) during training.\nHowever, effectively modeling interactions between primitives and generalizing\nknowledge to novel compositions remains a perennial challenge. There are two\nkey factors: object-conditioned and state-conditioned variance, i.e., the\nappearance of states (or objects) can vary significantly when combined with\ndifferent objects (or states). For instance, the state \"old\" can signify a\nvintage design for a \"car\" or an advanced age for a \"cat\". In this paper, we\nargue that these variances can be mitigated by predicting composition\ncategories based on pre-observed primitive. To this end, we propose Progressive\nLanguage-based Observations (PLO), which can dynamically determine a better\nobservation order of primitives. These observations comprise a series of\nconcepts or languages that allow the model to understand image content in a\nstep-by-step manner. Specifically, PLO adopts pre-trained vision-language\nmodels (VLMs) to empower the model with observation capabilities. We further\ndevise two variants: 1) PLO-VLM: a two-step method, where a pre-observing\nclassifier dynamically determines the observation order of two primitives. 2)\nPLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to\ncraft composition-specific prompts for step-by-step observing. Extensive\nablations on three challenging datasets demonstrate the superiority of PLO\ncompared with state-of-the-art methods, affirming its abilities in\ncompositional recognition.",
        "pdf_link": "https://arxiv.org/pdf/2311.14749v1.pdf"
    },
    {
        "title": "Challenges of Large Language Models for Mental Health Counseling",
        "authors": [
            "Neo Christopher Chung",
            "George Dyer",
            "Lennart Brocki"
        ],
        "published": "2023-11-23T08:56:41Z",
        "summary": "The global mental health crisis is looming with a rapid increase in mental\ndisorders, limited resources, and the social stigma of seeking treatment. As\nthe field of artificial intelligence (AI) has witnessed significant\nadvancements in recent years, large language models (LLMs) capable of\nunderstanding and generating human-like text may be used in supporting or\nproviding psychological counseling. However, the application of LLMs in the\nmental health domain raises concerns regarding the accuracy, effectiveness, and\nreliability of the information provided. This paper investigates the major\nchallenges associated with the development of LLMs for psychological\ncounseling, including model hallucination, interpretability, bias, privacy, and\nclinical effectiveness. We explore potential solutions to these challenges that\nare practical and applicable to the current paradigm of AI. From our experience\nin developing and deploying LLMs for mental health, AI holds a great promise\nfor improving mental health care, if we can carefully navigate and overcome\npitfalls of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13857v1.pdf"
    },
    {
        "title": "Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
        "authors": [
            "Saman Motamed",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "published": "2023-11-23T07:33:38Z",
        "summary": "Diffusion models have revolutionized generative content creation and\ntext-to-image (T2I) diffusion models in particular have increased the creative\nfreedom of users by allowing scene synthesis using natural language. T2I models\nexcel at synthesizing concepts such as nouns, appearances, and styles. To\nenable customized content creation based on a few example images of a concept,\nmethods such as Textual Inversion and DreamBooth invert the desired concept and\nenable synthesizing it in new scenes. However, inverting more general concepts\nthat go beyond object appearance and style (adjectives and verbs) through\nnatural language, remains a challenge. Two key characteristics of these\nconcepts contribute to the limitations of current inversion methods. 1)\nAdjectives and verbs are entangled with nouns (subject) and can hinder\nappearance-based inversion methods, where the subject appearance leaks into the\nconcept embedding and 2) describing such concepts often extends beyond single\nword embeddings (being frozen in ice, walking on a tightrope, etc.) that\ncurrent methods do not handle.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline.\nAdditionally, visual question answering using a large language model suggested\nLego-generated concepts are better aligned with the text description of the\nconcept.",
        "pdf_link": "https://arxiv.org/pdf/2311.13833v1.pdf"
    },
    {
        "title": "Surpassing GPT-4 Medical Coding with a Two-Stage Approach",
        "authors": [
            "Zhichao Yang",
            "Sanjit Singh Batra",
            "Joel Stremmel",
            "Eran Halperin"
        ],
        "published": "2023-11-22T23:35:13Z",
        "summary": "Recent advances in large language models (LLMs) show potential for clinical\napplications, such as clinical decision support and trial recommendations.\nHowever, the GPT-4 LLM predicts an excessive number of ICD codes for medical\ncoding tasks, leading to high recall but low precision. To tackle this\nchallenge, we introduce LLM-codex, a two-stage approach to predict ICD codes\nthat first generates evidence proposals using an LLM and then employs an\nLSTM-based verification stage. The LSTM learns from both the LLM's high recall\nand human expert's high precision, using a custom loss function. Our model is\nthe only approach that simultaneously achieves state-of-the-art results in\nmedical coding accuracy, accuracy on rare codes, and sentence-level evidence\nidentification to support coding decisions without training on human-annotated\nevidence according to experiments on the MIMIC dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.13735v1.pdf"
    },
    {
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
        "authors": [
            "Inderjeet Nair",
            "Shwetha Somasundaram",
            "Apoorv Saxena",
            "Koustava Goswami"
        ],
        "published": "2023-11-22T18:22:56Z",
        "summary": "We address the task of evidence retrieval for long document question\nanswering, which involves locating relevant paragraphs within a document to\nanswer a question. We aim to assess the applicability of large language models\n(LLMs) in the task of zero-shot long document evidence retrieval, owing to\ntheir unprecedented performance across various NLP tasks. However, currently\nthe LLMs can consume limited context lengths as input, thus providing document\nchunks as inputs might overlook the global context while missing out on\ncapturing the inter-segment dependencies. Moreover, directly feeding the large\ninput sets can incur significant computational costs, particularly when\nprocessing the entire document (and potentially incurring monetary expenses\nwith enterprise APIs like OpenAI's GPT variants). To address these challenges,\nwe propose a suite of techniques that exploit the discourse structure commonly\nfound in documents. By utilizing this structure, we create a condensed\nrepresentation of the document, enabling a more comprehensive understanding and\nanalysis of relationships between different parts. We retain $99.6\\%$ of the\nbest zero-shot approach's performance, while processing only $26\\%$ of the\ntotal tokens used by the best approach in the information seeking evidence\nretrieval setup. We also show how our approach can be combined with\n\\textit{self-ask} reasoning agent to achieve best zero-shot performance in\ncomplex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot\nperformance using gold evidence.",
        "pdf_link": "https://arxiv.org/pdf/2311.13565v1.pdf"
    },
    {
        "title": "Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object",
        "authors": [
            "Junhao Chen",
            "Peng Rong",
            "Jingbo Sun",
            "Chao Li",
            "Xiang Li",
            "Hongwu Lv"
        ],
        "published": "2023-11-22T18:15:43Z",
        "summary": "Image style transfer occupies an important place in both computer graphics\nand computer vision. However, most current methods require reference to\nstylized images and cannot individually stylize specific objects. To overcome\nthis limitation, we propose the \"Soulstyler\" framework, which allows users to\nguide the stylization of specific objects in an image through simple textual\ndescriptions. We introduce a large language model to parse the text and\nidentify stylization goals and specific styles. Combined with a CLIP-based\nsemantic visual embedding encoder, the model understands and matches text and\nimage content. We also introduce a novel localized text-image block matching\nloss that ensures that style transfer is performed only on specified target\nobjects, while non-target regions remain in their original style. Experimental\nresults demonstrate that our model is able to accurately perform style transfer\non target objects according to textual descriptions without affecting the style\nof background regions. Our code will be available at\nhttps://github.com/yisuanwang/Soulstyler.",
        "pdf_link": "https://arxiv.org/pdf/2311.13562v2.pdf"
    },
    {
        "title": "Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
        "authors": [
            "Chi Zhang",
            "Zifan Wang",
            "Ravi Mangal",
            "Matt Fredrikson",
            "Limin Jia",
            "Corina Pasareanu"
        ],
        "published": "2023-11-22T15:11:35Z",
        "summary": "Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.13445v1.pdf"
    },
    {
        "title": "Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents",
        "authors": [
            "Zihao Zhou",
            "Bin Hu",
            "Chenyang Zhao",
            "Pu Zhang",
            "Bin Liu"
        ],
        "published": "2023-11-22T13:15:42Z",
        "summary": "Recent studies have uncovered the potential of Large Language Models (LLMs)\nin addressing complex sequential decision-making tasks through the provision of\nhigh-level instructions. However, LLM-based agents lack specialization in\ntackling specific target problems, particularly in real-time dynamic\nenvironments. Additionally, deploying an LLM-based agent in practical scenarios\ncan be both costly and time-consuming. On the other hand, reinforcement\nlearning (RL) approaches train agents that specialize in the target task but\noften suffer from low sampling efficiency and high exploration costs. In this\npaper, we introduce a novel framework that addresses these challenges by\ntraining a smaller, specialized student RL agent using instructions from an\nLLM-based teacher agent. By incorporating the guidance from the teacher agent,\nthe student agent can distill the prior knowledge of the LLM into its own\nmodel. Consequently, the student agent can be trained with significantly less\ndata. Moreover, through further training with environment feedback, the student\nagent surpasses the capabilities of its teacher for completing the target task.\nWe conducted experiments on challenging MiniGrid and Habitat environments,\nspecifically designed for embodied AI research, to evaluate the effectiveness\nof our framework. The results clearly demonstrate that our approach achieves\nsuperior performance compared to strong baseline methods. Our code is available\nat https://github.com/ZJLAB-AMMI/LLM4Teach.",
        "pdf_link": "https://arxiv.org/pdf/2311.13373v4.pdf"
    },
    {
        "title": "Applying Large Language Models to Power Systems: Potential Security Threats",
        "authors": [
            "Jiaqi Ruan",
            "Gaoqi Liang",
            "Huan Zhao",
            "Guolong Liu",
            "Xianzhuo Sun",
            "Jing Qiu",
            "Zhao Xu",
            "Fushuan Wen",
            "Zhao Yang Dong"
        ],
        "published": "2023-11-22T12:55:02Z",
        "summary": "Applying large language models (LLMs) to modern power systems presents a\npromising avenue for enhancing decision-making and operational efficiency.\nHowever, this action may also incur potential security threats, which have not\nbeen fully recognized so far. To this end, this article analyzes potential\nthreats incurred by applying LLMs to power systems, emphasizing the need for\nurgent research and development of countermeasures.",
        "pdf_link": "https://arxiv.org/pdf/2311.13361v2.pdf"
    },
    {
        "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting",
        "authors": [
            "Xinyan Guan",
            "Yanjiang Liu",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-11-22T11:08:38Z",
        "summary": "Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13314v1.pdf"
    },
    {
        "title": "Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process",
        "authors": [
            "Nick Goodson",
            "Rongfei Lu"
        ],
        "published": "2023-11-22T10:04:29Z",
        "summary": "Large Language Models (LLMs) and chatbots show significant promise in\nstreamlining the legal intake process. This advancement can greatly reduce the\nworkload and costs for legal aid organizations, improving availability while\nmaking legal assistance more accessible to a broader audience. However, a key\nchallenge with current LLMs is their tendency to overconfidently deliver an\nimmediate 'best guess' to a client's question based on the output distribution\nlearned over the training data. This approach often overlooks the client's\nactual intentions or the specifics of their legal situation. As a result,\nclients may not realize the importance of providing essential additional\ncontext or expressing their underlying intentions, which are crucial for their\nlegal cases. Traditionally, logic based decision trees have been used to\nautomate intake for specific access to justice issues, such as immigration and\neviction. But those solutions lack scalability. We demonstrate a\nproof-of-concept using LLMs to elicit and infer clients' underlying intentions\nand specific legal circumstances through free-form, language-based\ninteractions. We also propose future research directions to use supervised\nfine-tuning or offline reinforcement learning to automatically incorporate\nintention and context elicitation in chatbots without explicit prompting.",
        "pdf_link": "https://arxiv.org/pdf/2311.13281v1.pdf"
    },
    {
        "title": "AutoKG: Efficient Automated Knowledge Graph Generation for Language Models",
        "authors": [
            "Bohan Chen",
            "Andrea L. Bertozzi"
        ],
        "published": "2023-11-22T08:58:25Z",
        "summary": "Traditional methods of linking large language models (LLMs) to knowledge\nbases via the semantic similarity search often fall short of capturing complex\nrelational dynamics. To address these limitations, we introduce AutoKG, a\nlightweight and efficient approach for automated knowledge graph (KG)\nconstruction. For a given knowledge base consisting of text blocks, AutoKG\nfirst extracts keywords using a LLM and then evaluates the relationship weight\nbetween each pair of keywords using graph Laplace learning. We employ a hybrid\nsearch scheme combining vector similarity and graph-based associations to\nenrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a\nmore comprehensive and interconnected knowledge retrieval mechanism compared to\nthe semantic similarity search, thereby enhancing the capabilities of LLMs in\ngenerating more insightful and relevant outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14740v1.pdf"
    },
    {
        "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
        "authors": [
            "Tianhang Zhang",
            "Lin Qiu",
            "Qipeng Guo",
            "Cheng Deng",
            "Yue Zhang",
            "Zheng Zhang",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Luoyi Fu"
        ],
        "published": "2023-11-22T08:39:17Z",
        "summary": "Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.",
        "pdf_link": "https://arxiv.org/pdf/2311.13230v1.pdf"
    },
    {
        "title": "Large Language Models in Education: Vision and Opportunities",
        "authors": [
            "Wensheng Gan",
            "Zhenlian Qi",
            "Jiayang Wu",
            "Jerry Chun-Wei Lin"
        ],
        "published": "2023-11-22T05:04:20Z",
        "summary": "With the rapid development of artificial intelligence technology, large\nlanguage models (LLMs) have become a hot research topic. Education plays an\nimportant role in human social development and progress. Traditional education\nfaces challenges such as individual student differences, insufficient\nallocation of teaching resources, and assessment of teaching effectiveness.\nTherefore, the applications of LLMs in the field of digital/smart education\nhave broad prospects. The research on educational large models (EduLLMs) is\nconstantly evolving, providing new methods and approaches to achieve\npersonalized learning, intelligent tutoring, and educational assessment goals,\nthereby improving the quality of education and the learning experience. This\narticle aims to investigate and summarize the application of LLMs in smart\neducation. It first introduces the research background and motivation of LLMs\nand explains the essence of LLMs. It then discusses the relationship between\ndigital education and EduLLMs and summarizes the current research status of\neducational large models. The main contributions are the systematic summary and\nvision of the research background, motivation, and application of large models\nfor education (LLM4Edu). By reviewing existing research, this article provides\nguidance and insights for educators, researchers, and policy-makers to gain a\ndeep understanding of the potential and challenges of LLM4Edu. It further\nprovides guidance for further advancing the development and application of\nLLM4Edu, while still facing technical, ethical, and practical challenges\nrequiring further research and exploration.",
        "pdf_link": "https://arxiv.org/pdf/2311.13160v1.pdf"
    },
    {
        "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data",
        "authors": [
            "Qifan Yu",
            "Juncheng Li",
            "Longhui Wei",
            "Liang Pang",
            "Wentao Ye",
            "Bosheng Qin",
            "Siliang Tang",
            "Qi Tian",
            "Yueting Zhuang"
        ],
        "published": "2023-11-22T04:52:58Z",
        "summary": "Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.",
        "pdf_link": "https://arxiv.org/pdf/2311.13614v2.pdf"
    },
    {
        "title": "Conditions for Length Generalization in Learning Reasoning Skills",
        "authors": [
            "Changnan Xiao",
            "Bing Liu"
        ],
        "published": "2023-11-22T03:36:18Z",
        "summary": "Reasoning is a fundamental capability of AI agents. Recently, large language\nmodels (LLMs) have shown remarkable abilities to perform reasoning tasks.\nHowever, numerous evaluations of the reasoning capabilities of LLMs have also\nshowed some limitations. An outstanding limitation is length generalization,\nmeaning that when trained on reasoning problems of smaller lengths or sizes,\nthe resulting models struggle with problems of larger sizes or lengths. This\npotentially indicates some theoretical limitations of generalization in\nlearning reasoning skills. These evaluations and their observations motivated\nus to perform a theoretical study of the length generalization problem. This\nwork focuses on reasoning tasks that can be formulated as Markov dynamic\nprocesses (MDPs) and/or directed acyclic graphs (DAGs). It identifies and\nproves conditions that decide whether the length generalization problem can be\nsolved or not for a reasoning task in a particular representation. Experiments\nare also conducted to verify the theoretical results.",
        "pdf_link": "https://arxiv.org/pdf/2311.16173v2.pdf"
    },
    {
        "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper",
        "authors": [
            "Chengyu Wang",
            "Junbing Yan",
            "Wei Zhang",
            "Jun Huang"
        ],
        "published": "2023-11-22T03:28:34Z",
        "summary": "This paper delves into the pressing need in Parameter-Efficient Fine-Tuning\n(PEFT) for Large Language Models (LLMs). While LLMs possess remarkable\ncapabilities, their extensive parameter requirements and associated\ncomputational demands hinder their practicality and scalability for real-world\napplications. Our position paper highlights current states and the necessity of\nfurther studying into the topic, and recognizes significant challenges and open\nissues that must be addressed to fully harness the powerful abilities of LLMs.\nThese challenges encompass novel efficient PEFT architectures, PEFT for\ndifferent learning settings, PEFT combined with model compression techniques,\nand the exploration of PEFT for multi-modal LLMs. By presenting this position\npaper, we aim to stimulate further research and foster discussions surrounding\nmore efficient and accessible PEFT for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.13126v1.pdf"
    },
    {
        "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
        "authors": [
            "Ha-Thanh Nguyen",
            "Wachara Fungwacharakorn",
            "Ken Satoh"
        ],
        "published": "2023-11-22T01:51:50Z",
        "summary": "Language serves as a vehicle for conveying thought, enabling communication\namong individuals. The ability to distinguish between diverse concepts,\nidentify fairness and injustice, and comprehend a range of legal notions\nfundamentally relies on logical reasoning. Large Language Models (LLMs) attempt\nto emulate human language understanding and generation, but their competency in\nlogical reasoning remains limited. This paper seeks to address the\nphilosophical question: How can we effectively teach logical reasoning to LLMs\nwhile maintaining a deep understanding of the intricate relationship between\nlanguage and logic? By focusing on bolstering LLMs' capabilities in logical\nreasoning, we aim to expand their applicability in law and other\nlogic-intensive disciplines. To this end, we propose a Reinforcement Learning\nfrom Logical Feedback (RLLF) approach, which serves as a potential framework\nfor refining LLMs' reasoning capacities. Through RLLF and a revised evaluation\nmethodology, we explore new avenues for research in this domain and contribute\nto the development of LLMs capable of handling complex legal reasoning tasks\nwhile acknowledging the fundamental connection between language and logic.",
        "pdf_link": "https://arxiv.org/pdf/2311.13095v1.pdf"
    },
    {
        "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift",
        "authors": [
            "Will LeVine",
            "Benjamin Pikus",
            "Anthony Chen",
            "Sean Hendryx"
        ],
        "published": "2023-11-21T18:41:26Z",
        "summary": "Foundation models, specifically Large Language Models (LLMs), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.",
        "pdf_link": "https://arxiv.org/pdf/2311.14743v7.pdf"
    },
    {
        "title": "Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions",
        "authors": [
            "Hye Sun Yun",
            "Mehdi Arjmand",
            "Phillip Raymond Sherlock",
            "Michael Paasche-Orlow",
            "James W. Griffith",
            "Timothy Bickmore"
        ],
        "published": "2023-11-21T16:20:49Z",
        "summary": "Standardized, validated questionnaires are vital tools in HCI research and\nhealthcare, offering dependable self-report data. However, their repeated use\nin longitudinal or pre-post studies can induce respondent fatigue, impacting\ndata quality via response biases and decreased response rates. We propose\nutilizing large language models (LLMs) to generate diverse questionnaire\nversions while retaining good psychometric properties. In a longitudinal study,\nparticipants engaged with our agent system and responded daily for two weeks to\neither a standardized depression questionnaire or one of two LLM-generated\nquestionnaire variants, alongside a validated depression questionnaire.\nPsychometric testing revealed consistent covariation between the external\ncriterion and the focal measure administered across the three conditions,\ndemonstrating the reliability and validity of the LLM-generated variants.\nParticipants found the repeated administration of the standardized\nquestionnaire significantly more repetitive compared to the variants. Our\nfindings highlight the potential of LLM-generated variants to invigorate\nquestionnaires, fostering engagement and interest without compromising\nvalidity.",
        "pdf_link": "https://arxiv.org/pdf/2311.12707v1.pdf"
    },
    {
        "title": "Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study",
        "authors": [
            "Mengyang Chen",
            "Lingwei Wei",
            "Han Cao",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-11-21T16:03:51Z",
        "summary": "Large Language Models (LLMs) have garnered significant attention for their\npowerful ability in natural language understanding and reasoning. In this\npaper, we present a comprehensive empirical study to explore the performance of\nLLMs on misinformation detection tasks. This study stands as the pioneering\ninvestigation into the understanding capabilities of multiple LLMs regarding\nboth content and propagation across social media platforms. Our empirical\nstudies on five misinformation detection datasets show that LLMs with diverse\nprompts achieve comparable performance in text-based misinformation detection\nbut exhibit notably constrained capabilities in comprehending propagation\nstructure compared to existing models in propagation-based misinformation\ndetection. Besides, we further design four instruction-tuned strategies to\nenhance LLMs for both content and propagation-based misinformation detection.\nThese strategies boost LLMs to actively learn effective features from multiple\ninstances or hard instances, and eliminate irrelevant propagation structures,\nthereby achieving better detection performance. Extensive experiments further\ndemonstrate LLMs would play a better capacity in content and propagation\nstructure under these proposed strategies and achieve promising detection\nperformance. These findings highlight the potential ability of LLMs to detect\nmisinformation.",
        "pdf_link": "https://arxiv.org/pdf/2311.12699v1.pdf"
    },
    {
        "title": "HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis",
        "authors": [
            "Sang-Hoon Lee",
            "Ha-Yeong Choi",
            "Seung-Bin Kim",
            "Seong-Whan Lee"
        ],
        "published": "2023-11-21T09:07:11Z",
        "summary": "Large language models (LLM)-based speech synthesis has been widely adopted in\nzero-shot speech synthesis. However, they require a large-scale data and\npossess the same limitations as previous autoregressive speech models,\nincluding slow inference speed and lack of robustness. This paper proposes\nHierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech\n(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis\nframeworks could significantly improve the robustness and expressiveness of the\nsynthetic speech. Furthermore, we significantly improve the naturalness and\nspeaker similarity of synthetic speech even in zero-shot speech synthesis\nscenarios. For text-to-speech, we adopt the text-to-vec framework, which\ngenerates a self-supervised speech representation and an F0 representation\nbased on text representations and prosody prompts. Then, HierSpeech++ generates\nspeech from the generated vector, F0, and voice prompt. We further introduce a\nhigh-efficient speech super-resolution framework from 16 kHz to 48 kHz. The\nexperimental results demonstrated that the hierarchical variational autoencoder\ncould be a strong zero-shot speech synthesizer given that it outperforms\nLLM-based and diffusion-based models. Moreover, we achieved the first\nhuman-level quality zero-shot speech synthesis. Audio samples and source code\nare available at https://github.com/sh-lee-prml/HierSpeechpp.",
        "pdf_link": "https://arxiv.org/pdf/2311.12454v2.pdf"
    },
    {
        "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
        "authors": [
            "Yunpeng Huang",
            "Jingwei Xu",
            "Junyu Lai",
            "Zixu Jiang",
            "Taolue Chen",
            "Zenan Li",
            "Yuan Yao",
            "Xiaoxing Ma",
            "Lijuan Yang",
            "Hao Chen",
            "Shupeng Li",
            "Penghao Zhao"
        ],
        "published": "2023-11-21T04:59:17Z",
        "summary": "Transformer-based Large Language Models (LLMs) have been applied in diverse\nareas such as knowledge bases, human interfaces, and dynamic agents, and\nmarking a stride towards achieving Artificial General Intelligence (AGI).\nHowever, current LLMs are predominantly pretrained on short text snippets,\nwhich compromises their effectiveness in processing the long-context prompts\nthat are frequently encountered in practical scenarios. This article offers a\ncomprehensive survey of the recent advancement in Transformer-based LLM\narchitectures aimed at enhancing the long-context capabilities of LLMs\nthroughout the entire model lifecycle, from pre-training through to inference.\nWe first delineate and analyze the problems of handling long-context input and\noutput with the current Transformer-based models. We then provide a taxonomy\nand the landscape of upgrades on Transformer architecture to solve these\nproblems. Afterwards, we provide an investigation on wildly used evaluation\nnecessities tailored for long-context LLMs, including datasets, metrics, and\nbaseline models, as well as optimization toolkits such as libraries,\nframeworks, and compilers to boost the efficacy of LLMs across different stages\nin runtime. Finally, we discuss the challenges and potential avenues for future\nresearch. A curated repository of relevant literature, continuously updated, is\navailable at https://github.com/Strivin0311/long-llms-learning.",
        "pdf_link": "https://arxiv.org/pdf/2311.12351v2.pdf"
    },
    {
        "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
        "authors": [
            "Samira Ghodratnama",
            "Mehrdad Zakershahrak"
        ],
        "published": "2023-11-21T02:01:01Z",
        "summary": "The advent of Large Language Models (LLMs) heralds a pivotal shift in online\nuser interactions with information. Traditional Information Retrieval (IR)\nsystems primarily relied on query-document matching, whereas LLMs excel in\ncomprehending and generating human-like text, thereby enriching the IR\nexperience significantly. While LLMs are often associated with chatbot\nfunctionalities, this paper extends the discussion to their explicit\napplication in information retrieval. We explore methodologies to optimize the\nretrieval process, select optimal models, and effectively scale and orchestrate\nLLMs, aiming for cost-efficiency and enhanced result accuracy. A notable\nchallenge, model hallucination-where the model yields inaccurate or\nmisinterpreted data-is addressed alongside other model-specific hurdles. Our\ndiscourse extends to crucial considerations including user privacy, data\noptimization, and the necessity for system clarity and interpretability.\nThrough a comprehensive examination, we unveil not only innovative strategies\nfor integrating Language Models (LLMs) with Information Retrieval (IR) systems,\nbut also the consequential considerations that underline the need for a\nbalanced approach aligned with user-centric principles.",
        "pdf_link": "https://arxiv.org/pdf/2311.12287v1.pdf"
    },
    {
        "title": "Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis",
        "authors": [
            "Ruiyang Qin",
            "Jun Xia",
            "Zhenge Jia",
            "Meng Jiang",
            "Ahmed Abbasi",
            "Peipei Zhou",
            "Jingtong Hu",
            "Yiyu Shi"
        ],
        "published": "2023-11-21T01:34:02Z",
        "summary": "After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.",
        "pdf_link": "https://arxiv.org/pdf/2311.12275v3.pdf"
    },
    {
        "title": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software",
        "authors": [
            "Dananjay Srinivas",
            "Rohan Das",
            "Saeid Tizpaz-Niari",
            "Ashutosh Trivedi",
            "Maria Leonor Pacheco"
        ],
        "published": "2023-11-20T18:12:28Z",
        "summary": "Due to the ever-increasing complexity of income tax laws in the United\nStates, the number of US taxpayers filing their taxes using tax preparation\nsoftware (henceforth, tax software) continues to increase. According to the\nU.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed\ntheir individual income taxes using tax software. Given the legal consequences\nof incorrectly filing taxes for the taxpayer, ensuring the correctness of tax\nsoftware is of paramount importance. Metamorphic testing has emerged as a\nleading solution to test and debug legal-critical tax software due to the\nabsence of correctness requirements and trustworthy datasets. The key idea\nbehind metamorphic testing is to express the properties of a system in terms of\nthe relationship between one input and its slightly metamorphosed twinned\ninput. Extracting metamorphic properties from IRS tax publications is a tedious\nand time-consuming process. As a response, this paper formulates the task of\ngenerating metamorphic specifications as a translation task between properties\nextracted from tax documents - expressed in natural language - to a contrastive\nfirst-order logic form. We perform a systematic analysis on the potential and\nlimitations of in-context learning with Large Language Models(LLMs) for this\ntask, and outline a research agenda towards automating the generation of\nmetamorphic specifications for tax preparation software.",
        "pdf_link": "https://arxiv.org/pdf/2311.11979v1.pdf"
    },
    {
        "title": "FinanceBench: A New Benchmark for Financial Question Answering",
        "authors": [
            "Pranab Islam",
            "Anand Kannappan",
            "Douwe Kiela",
            "Rebecca Qian",
            "Nino Scherrer",
            "Bertie Vidgen"
        ],
        "published": "2023-11-20T17:28:02Z",
        "summary": "FinanceBench is a first-of-its-kind test suite for evaluating the performance\nof LLMs on open book financial question answering (QA). It comprises 10,231\nquestions about publicly traded companies, with corresponding answers and\nevidence strings. The questions in FinanceBench are ecologically valid and\ncover a diverse set of scenarios. They are intended to be clear-cut and\nstraightforward to answer to serve as a minimum performance standard. We test\n16 state of the art model configurations (including GPT-4-Turbo, Llama2 and\nClaude2, with vector stores and long context prompts) on a sample of 150 cases\nfrom FinanceBench, and manually review their answers (n=2,400). The cases are\navailable open-source. We show that existing LLMs have clear limitations for\nfinancial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly\nanswered or refused to answer 81% of questions. While augmentation techniques\nsuch as using longer context window to feed in relevant evidence improve\nperformance, they are unrealistic for enterprise settings due to increased\nlatency and cannot support larger financial documents. We find that all models\nexamined exhibit weaknesses, such as hallucinations, that limit their\nsuitability for use by enterprises.",
        "pdf_link": "https://arxiv.org/pdf/2311.11944v1.pdf"
    },
    {
        "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
        "authors": [
            "Songhao Han",
            "Le Zhuo",
            "Yue Liao",
            "Si Liu"
        ],
        "published": "2023-11-20T16:37:45Z",
        "summary": "Vision-language models (VLMs) offer a promising paradigm for image\nclassification by comparing the similarity between images and class embeddings.\nA critical challenge lies in crafting precise textual representations for class\nnames. While previous studies have leveraged recent advancements in large\nlanguage models (LLMs) to enhance these descriptors, their outputs often suffer\nfrom ambiguity and inaccuracy. We attribute this to two primary factors: 1) the\nreliance on single-turn textual interactions with LLMs, leading to a mismatch\nbetween generated text and visual concepts for VLMs; 2) the oversight of the\ninter-class relationships, resulting in descriptors that fail to differentiate\nsimilar classes effectively. In this paper, we propose a novel framework that\nintegrates LLMs and VLMs to find the optimal class descriptors. Our\ntraining-free approach develops an LLM-based agent with an evolutionary\noptimization strategy to iteratively refine class descriptors. We demonstrate\nour optimized descriptors are of high quality which effectively improves\nclassification accuracy on a wide range of benchmarks. Additionally, these\ndescriptors offer explainable and robust features, boosting performance across\nvarious backbone models and complementing fine-tuning-based methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.11904v2.pdf"
    },
    {
        "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
        "authors": [
            "Yu Tian",
            "Xiao Yang",
            "Jingyuan Zhang",
            "Yinpeng Dong",
            "Hang Su"
        ],
        "published": "2023-11-20T15:50:09Z",
        "summary": "Rapid advancements in large language models (LLMs) have revitalized in\nLLM-based agents, exhibiting impressive human-like behaviors and cooperative\ncapabilities in various scenarios. However, these agents also bring some\nexclusive risks, stemming from the complexity of interaction environments and\nthe usability of tools. This paper delves into the safety of LLM-based agents\nfrom three perspectives: agent quantity, role definition, and attack level.\nSpecifically, we initially propose to employ a template-based attack strategy\non LLM-based agents to find the influence of agent quantity. In addition, to\naddress interaction environment and role specificity issues, we introduce Evil\nGeniuses (EG), an effective attack method that autonomously generates prompts\nrelated to the original role to examine the impact across various role\ndefinitions and attack levels. EG leverages Red-Blue exercises, significantly\nimproving the generated prompt aggressiveness and similarity to original roles.\nOur evaluations on CAMEL, Metagpt and ChatDev based on GPT-3.5 and GPT-4,\ndemonstrate high success rates. Extensive evaluation and discussion reveal that\nthese agents are less robust, prone to more harmful behaviors, and capable of\ngenerating stealthier content than LLMs, highlighting significant safety\nchallenges and guiding future research. Our code is available at\nhttps://github.com/T1aNS1R/Evil-Geniuses.",
        "pdf_link": "https://arxiv.org/pdf/2311.11855v2.pdf"
    },
    {
        "title": "System 2 Attention (is something you might need too)",
        "authors": [
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "published": "2023-11-20T15:04:50Z",
        "summary": "Soft attention in Transformer-based Large Language Models (LLMs) is\nsusceptible to incorporating irrelevant information from the context into its\nlatent representations, which adversely affects next token generations. To help\nrectify these issues, we introduce System 2 Attention (S2A), which leverages\nthe ability of LLMs to reason in natural language and follow instructions in\norder to decide what to attend to. S2A regenerates the input context to only\ninclude the relevant portions, before attending to the regenerated context to\nelicit the final response. In experiments, S2A outperforms standard\nattention-based LLMs on three tasks containing opinion or irrelevant\ninformation, QA, math word problems and longform generation, where S2A\nincreases factuality and objectivity, and decreases sycophancy.",
        "pdf_link": "https://arxiv.org/pdf/2311.11829v1.pdf"
    },
    {
        "title": "Towards Robust Text Retrieval with Progressive Learning",
        "authors": [
            "Tong Wu",
            "Yulei Qin",
            "Enwei Zhang",
            "Zihan Xu",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun"
        ],
        "published": "2023-11-20T11:44:01Z",
        "summary": "Retrieval augmentation has become an effective solution to empower large\nlanguage models (LLMs) with external and verified knowledge sources from the\ndatabase, which overcomes the limitations and hallucinations of LLMs in\nhandling up-to-date and domain-specific information. However, existing\nembedding models for text retrieval usually have three non-negligible\nlimitations. First, the number and diversity of samples in a batch are too\nrestricted to supervise the modeling of textual nuances at scale. Second, the\nhigh proportional noise are detrimental to the semantic correctness and\nconsistency of embeddings. Third, the equal treatment to easy and difficult\nsamples would cause sub-optimum convergence of embeddings with poorer\ngeneralization. In this paper, we propose the PEG, a progressively learned\nembeddings for robust text retrieval. Specifically, we increase the training\nin-batch negative samples to 80,000, and for each query, we extracted five hard\nnegatives. Concurrently, we incorporated a progressive learning mechanism,\nenabling the model to dynamically modulate its attention to the samples\nthroughout the entire training process. Additionally, PEG is trained on more\nthan 100 million data, encompassing a wide range of domains (e.g., finance,\nmedicine, and tourism) and covering various tasks (e.g., question-answering,\nmachine reading comprehension, and similarity matching). Extensive experiments\nconducted on C-MTEB and DuReader demonstrate that PEG surpasses\nstate-of-the-art embeddings in retrieving true positives, highlighting its\nsignificant potential for applications in LLMs. Our model is publicly available\nat https://huggingface.co/TownsWu/PEG.",
        "pdf_link": "https://arxiv.org/pdf/2311.11691v1.pdf"
    },
    {
        "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples",
        "authors": [
            "Atsushi Shirafuji",
            "Yusuke Oda",
            "Jun Suzuki",
            "Makoto Morishita",
            "Yutaka Watanobe"
        ],
        "published": "2023-11-20T11:43:45Z",
        "summary": "A less complex and more straightforward program is a crucial factor that\nenhances its maintainability and makes writing secure and bug-free programs\neasier. However, due to its heavy workload and the risks of breaking the\nworking programs, programmers are reluctant to do code refactoring, and thus,\nit also causes the loss of potential learning experiences. To mitigate this, we\ndemonstrate the application of using a large language model (LLM), GPT-3.5, to\nsuggest less complex versions of the user-written Python program, aiming to\nencourage users to learn how to write better programs. We propose a method to\nleverage the prompting with few-shot examples of the LLM by selecting the\nbest-suited code refactoring examples for each target programming problem based\non the prior evaluation of prompting with the one-shot example. The\nquantitative evaluation shows that 95.68% of programs can be refactored by\ngenerating 10 candidates each, resulting in a 17.35% reduction in the average\ncyclomatic complexity and a 25.84% decrease in the average number of lines\nafter filtering only generated programs that are semantically correct.\nFurthermore, the qualitative evaluation shows outstanding capability in code\nformatting, while unnecessary behaviors such as deleting or translating\ncomments are also observed.",
        "pdf_link": "https://arxiv.org/pdf/2311.11690v1.pdf"
    },
    {
        "title": "Incorporating LLM Priors into Tabular Learners",
        "authors": [
            "Max Zhu",
            "Sini≈°a Stanivuk",
            "Andrija Petrovic",
            "Mladen Nikolic",
            "Pietro Lio"
        ],
        "published": "2023-11-20T09:27:09Z",
        "summary": "We present a method to integrate Large Language Models (LLMs) and traditional\ntabular data classification techniques, addressing LLMs challenges like data\nserialization sensitivity and biases. We introduce two strategies utilizing\nLLMs for ranking categorical variables and generating priors on correlations\nbetween continuous variables and targets, enhancing performance in few-shot\nscenarios. We focus on Logistic Regression, introducing MonotonicLR that\nemploys a non-linear monotonic function for mapping ordinals to cardinals while\npreserving LLM-determined orders. Validation against baseline models reveals\nthe superior performance of our approach, especially in low-data scenarios,\nwhile remaining interpretable.",
        "pdf_link": "https://arxiv.org/pdf/2311.11628v1.pdf"
    },
    {
        "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
        "authors": [
            "Quanyu Long",
            "Wenya Wang",
            "Sinno Jialin Pan"
        ],
        "published": "2023-11-20T06:06:20Z",
        "summary": "Large language models (LLMs) have showcased their capability with few-shot\ninference known as in-context learning. However, in-domain demonstrations are\nnot always readily available in real scenarios, leading to cross-domain\nin-context learning. Besides, LLMs are still facing challenges in long-tail\nknowledge in unseen and unfamiliar domains. The above limitations demonstrate\nthe necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study\nthe UDA problem under an in-context learning setting to adapt language models\nfrom the source domain to the target domain without any target labels. The core\nidea is to retrieve a subset of cross-domain elements that are the most similar\nto the query, and elicit language model to adapt in an in-context manner by\nlearning both target domain distribution and the discriminative task signal\nsimultaneously with the augmented cross-domain in-context examples. We devise\ndifferent prompting and training strategies, accounting for different LM\narchitectures to learn the target distribution via language modeling. With\nextensive experiments on Sentiment Analysis (SA) and Named Entity Recognition\n(NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer\nand demonstrate significant improvements over baseline models.",
        "pdf_link": "https://arxiv.org/pdf/2311.11551v1.pdf"
    },
    {
        "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
        "authors": [
            "Zhengmian Hu",
            "Gang Wu",
            "Saayan Mitra",
            "Ruiyi Zhang",
            "Tong Sun",
            "Heng Huang",
            "Viswanathan Swaminathan"
        ],
        "published": "2023-11-20T03:17:21Z",
        "summary": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in\nvarious applications. However, these models are susceptible to adversarial\nprompt attacks, where attackers can carefully curate input strings that mislead\nLLMs into generating incorrect or undesired outputs. Previous work has revealed\nthat with relatively simple yet effective attacks based on discrete\noptimization, it is possible to generate adversarial prompts that bypass\nmoderation and alignment of the models. This vulnerability to adversarial\nprompts underscores a significant concern regarding the robustness and\nreliability of LLMs. Our work aims to address this concern by introducing a\nnovel approach to detecting adversarial prompts at a token level, leveraging\nthe LLM's capability to predict the next token's probability. We measure the\ndegree of the model's perplexity, where tokens predicted with high probability\nare considered normal, and those exhibiting high perplexity are flagged as\nadversarial. Additionaly, our method also integrates context understanding by\nincorporating neighboring token information to encourage the detection of\ncontiguous adversarial prompt sequences. To this end, we design two algorithms\nfor adversarial prompt detection: one based on optimization techniques and\nanother on Probabilistic Graphical Models (PGM). Both methods are equipped with\nefficient solving methods, ensuring efficient adversarial prompt detection. Our\ntoken-level detection result can be visualized as heatmap overlays on the text\nsequence, allowing for a clearer and more intuitive representation of which\npart of the text may contain adversarial prompts.",
        "pdf_link": "https://arxiv.org/pdf/2311.11509v3.pdf"
    },
    {
        "title": "A Security Risk Taxonomy for Large Language Models",
        "authors": [
            "Erik Derner",
            "Kristina Batistiƒç",
            "Jan Zah√°lka",
            "Robert Babu≈°ka"
        ],
        "published": "2023-11-19T20:22:05Z",
        "summary": "As large language models (LLMs) permeate more and more applications, an\nassessment of their associated security risks becomes increasingly necessary.\nThe potential for exploitation by malicious actors, ranging from disinformation\nto data breaches and reputation damage, is substantial. This paper addresses a\ngap in current research by focusing on the security risks posed by LLMs, which\nextends beyond the widely covered ethical and societal implications. Our work\nproposes a taxonomy of security risks along the user-model communication\npipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize\nthe attacks by target and attack type within a prompt-based interaction scheme.\nThe taxonomy is reinforced with specific attack examples to showcase the\nreal-world impact of these risks. Through this taxonomy, we aim to inform the\ndevelopment of robust and secure LLM applications, enhancing their safety and\ntrustworthiness.",
        "pdf_link": "https://arxiv.org/pdf/2311.11415v1.pdf"
    },
    {
        "title": "Zero-Shot Question Answering over Financial Documents using Large Language Models",
        "authors": [
            "Karmvir Singh Phogat",
            "Chetan Harsha",
            "Sridhar Dasaratha",
            "Shashishekar Ramakrishna",
            "Sai Akhil Puranam"
        ],
        "published": "2023-11-19T16:23:34Z",
        "summary": "We introduce a large language model (LLM) based approach to answer complex\nquestions requiring multi-hop numerical reasoning over financial reports. While\nLLMs have exhibited remarkable performance on various natural language and\nreasoning tasks, complex reasoning problems often rely on few-shot prompts that\nrequire carefully crafted examples. In contrast, our approach uses novel\nzero-shot prompts that guide the LLM to encode the required reasoning into a\nPython program or a domain specific language. The generated program is then\nexecuted by a program interpreter, thus mitigating the limitations of LLM in\nperforming accurate arithmetic calculations.\n  We evaluate the proposed approach on three financial datasets using some of\nthe recently developed generative pretrained transformer (GPT) models and\nperform comparisons with various zero-shot baselines. The experimental results\ndemonstrate that our approach significantly improves the accuracy for all the\nLLMs over their respective baselines. We provide a detailed analysis of the\nresults, generating insights to support our findings. The success of our\napproach demonstrates the enormous potential to extract complex domain specific\nnumerical reasoning by designing zero-shot prompts to effectively exploit the\nknowledge embedded in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14722v1.pdf"
    },
    {
        "title": "TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems",
        "authors": [
            "Yilun Kong",
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Tianpeng Bao",
            "Shiwei Shi",
            "Guoqing Du",
            "Xiaoru Hu",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "published": "2023-11-19T12:37:30Z",
        "summary": "Large Language Models (LLMs) have demonstrated proficiency in addressing\ntasks that necessitate a combination of task planning and the usage of external\ntools that require a blend of task planning and the utilization of external\ntools, such as APIs. However, real-world complex systems present three\nprevalent challenges concerning task planning and tool usage: (1) The real\nsystem usually has a vast array of APIs, so it is impossible to feed the\ndescriptions of all APIs to the prompt of LLMs as the token length is limited;\n(2) the real system is designed for handling complex tasks, and the base LLMs\ncan hardly plan a correct sub-task order and API-calling order for such tasks;\n(3) Similar semantics and functionalities among APIs in real systems create\nchallenges for both LLMs and even humans in distinguishing between them. In\nresponse, this paper introduces a comprehensive framework aimed at enhancing\nthe Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating\nwithin real-world systems. Our framework comprises three key components\ndesigned to address these challenges: (1) the API Retriever selects the most\npertinent APIs for the user task among the extensive array available; (2) LLM\nFinetuner tunes a base LLM so that the finetuned LLM can be more capable for\ntask planning and API calling; (3) the Demo Selector adaptively retrieves\ndifferent demonstrations related to hard-to-distinguish APIs, which is further\nused for in-context learning to boost the final performance. We validate our\nmethods using a real-world commercial system as well as an open-sourced\nacademic dataset, and the outcomes clearly showcase the efficacy of each\nindividual component as well as the integrated framework.",
        "pdf_link": "https://arxiv.org/pdf/2311.11315v1.pdf"
    },
    {
        "title": "Rethinking Large Language Models in Mental Health Applications",
        "authors": [
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Kailai Yang",
            "Sophia Ananiadou",
            "Erik Cambria"
        ],
        "published": "2023-11-19T08:40:01Z",
        "summary": "Large Language Models (LLMs) have become valuable assets in mental health,\nshowing promise in both classification tasks and counseling applications. This\npaper offers a perspective on using LLMs in mental health applications. It\ndiscusses the instability of generative models for prediction and the potential\nfor generating hallucinatory outputs, underscoring the need for ongoing audits\nand evaluations to maintain their reliability and dependability. The paper also\ndistinguishes between the often interchangeable terms ``explainability'' and\n``interpretability'', advocating for developing inherently interpretable\nmethods instead of relying on potentially hallucinated self-explanations\ngenerated by LLMs. Despite the advancements in LLMs, human counselors'\nempathetic understanding, nuanced interpretation, and contextual awareness\nremain irreplaceable in the sensitive and complex realm of mental health\ncounseling. The use of LLMs should be approached with a judicious and\nconsiderate mindset, viewing them as tools that complement human expertise\nrather than seeking to replace it.",
        "pdf_link": "https://arxiv.org/pdf/2311.11267v2.pdf"
    },
    {
        "title": "Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness",
        "authors": [
            "Gongbo Zhang",
            "Qiao Jin",
            "Denis Jered McInerney",
            "Yong Chen",
            "Fei Wang",
            "Curtis L. Cole",
            "Qian Yang",
            "Yanshan Wang",
            "Bradley A. Malin",
            "Mor Peleg",
            "Byron C. Wallace",
            "Zhiyong Lu",
            "Chunhua Weng",
            "Yifan Peng"
        ],
        "published": "2023-11-19T03:29:45Z",
        "summary": "Evidence-based medicine promises to improve the quality of healthcare by\nempowering medical decisions and practices with the best available evidence.\nThe rapid growth of medical evidence, which can be obtained from various\nsources, poses a challenge in collecting, appraising, and synthesizing the\nevidential information. Recent advancements in generative AI, exemplified by\nlarge language models, hold promise in facilitating the arduous task. However,\ndeveloping accountable, fair, and inclusive models remains a complicated\nundertaking. In this perspective, we discuss the trustworthiness of generative\nAI in the context of automated summarization of medical evidence.",
        "pdf_link": "https://arxiv.org/pdf/2311.11211v3.pdf"
    },
    {
        "title": "Few-Shot Classification & Segmentation Using Large Language Models Agent",
        "authors": [
            "Tian Meng",
            "Yang Tao",
            "Wuliang Yin"
        ],
        "published": "2023-11-19T00:33:41Z",
        "summary": "The task of few-shot image classification and segmentation (FS-CS) requires\nthe classification and segmentation of target objects in a query image, given\nonly a few examples of the target classes. We introduce a method that utilises\nlarge language models (LLM) as an agent to address the FS-CS problem in a\ntraining-free manner. By making the LLM the task planner and off-the-shelf\nvision models the tools, the proposed method is capable of classifying and\nsegmenting target objects using only image-level labels. Specifically,\nchain-of-thought prompting and in-context learning guide the LLM to observe\nsupport images like human; vision models such as Segment Anything Model (SAM)\nand GPT-4Vision assist LLM understand spatial and semantic information at the\nsame time. Ultimately, the LLM uses its summarizing and reasoning capabilities\nto classify and segment the query image. The proposed method's modular\nframework makes it easily extendable. Our approach achieves state-of-the-art\nperformance on the Pascal-5i dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.12065v1.pdf"
    },
    {
        "title": "Visual AI and Linguistic Intelligence Through Steerability and Composability",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-11-18T22:01:33Z",
        "summary": "This study explores the capabilities of multimodal large language models\n(LLMs) in handling challenging multistep tasks that integrate language and\nvision, focusing on model steerability, composability, and the application of\nlong-term memory and context understanding. The problem addressed is the LLM's\nability (Nov 2023 GPT-4 Vision Preview) to manage tasks that require\nsynthesizing visual and textual information, especially where stepwise\ninstructions and sequential logic are paramount. The research presents a series\nof 14 creatively and constructively diverse tasks, ranging from AI Lego\nDesigning to AI Satellite Image Analysis, designed to test the limits of\ncurrent LLMs in contexts that previously proved difficult without extensive\nmemory and contextual understanding. Key findings from evaluating 800 guided\ndialogs include notable disparities in task completion difficulty. For\ninstance, 'Image to Ingredient AI Bartender' (Low difficulty) contrasted\nsharply with 'AI Game Self-Player' (High difficulty), highlighting the LLM's\nvarying proficiency in processing complex visual data and generating coherent\ninstructions. Tasks such as 'AI Genetic Programmer' and 'AI Negotiator' showed\nhigh completion difficulty, emphasizing challenges in maintaining context over\nmultiple steps. The results underscore the importance of developing LLMs that\ncombine long-term memory and contextual awareness to mimic human-like thought\nprocesses in complex problem-solving scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2312.12383v1.pdf"
    },
    {
        "title": "A Principled Framework for Knowledge-enhanced Large Language Model",
        "authors": [
            "Saizhuo Wang",
            "Zhihan Liu",
            "Zhaoran Wang",
            "Jian Guo"
        ],
        "published": "2023-11-18T18:10:02Z",
        "summary": "Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.",
        "pdf_link": "https://arxiv.org/pdf/2311.11135v1.pdf"
    },
    {
        "title": "(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs",
        "authors": [
            "Wanqin Ma",
            "Chenyang Yang",
            "Christian K√§stner"
        ],
        "published": "2023-11-18T17:11:12Z",
        "summary": "Large Language Models (LLMs) are increasingly integrated into software\napplications. Downstream application developers often access LLMs through APIs\nprovided as a service. However, LLM APIs are often updated silently and\nscheduled to be deprecated, forcing users to continuously adapt to evolving\nmodels. This can cause performance regression and affect prompt design choices,\nas evidenced by our case study on toxicity detection. Based on our case study,\nwe emphasize the need for and re-examine the concept of regression testing for\nevolving LLM APIs. We argue that regression testing LLMs requires fundamental\nchanges to traditional testing approaches, due to different correctness\nnotions, prompting brittleness, and non-determinism in LLM APIs.",
        "pdf_link": "https://arxiv.org/pdf/2311.11123v2.pdf"
    },
    {
        "title": "Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers",
        "authors": [
            "Sohini Roychowdhury"
        ],
        "published": "2023-11-18T03:55:59Z",
        "summary": "Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.",
        "pdf_link": "https://arxiv.org/pdf/2311.10961v1.pdf"
    },
    {
        "title": "An Embodied Generalist Agent in 3D World",
        "authors": [
            "Jiangyong Huang",
            "Silong Yong",
            "Xiaojian Ma",
            "Xiongkun Linghu",
            "Puhao Li",
            "Yan Wang",
            "Qing Li",
            "Song-Chun Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "published": "2023-11-18T01:21:38Z",
        "summary": "Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.",
        "pdf_link": "https://arxiv.org/pdf/2311.12871v1.pdf"
    },
    {
        "title": "Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections",
        "authors": [
            "Lihan Zha",
            "Yuchen Cui",
            "Li-Heng Lin",
            "Minae Kwon",
            "Montserrat Gonzalez Arenas",
            "Andy Zeng",
            "Fei Xia",
            "Dorsa Sadigh"
        ],
        "published": "2023-11-17T18:00:20Z",
        "summary": "Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .",
        "pdf_link": "https://arxiv.org/pdf/2311.10678v2.pdf"
    },
    {
        "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest",
        "authors": [
            "Ruohong Zhang",
            "Luyu Gao",
            "Chen Zheng",
            "Zhen Fan",
            "Guokun Lai",
            "Zheng Zhang",
            "Fangzhou Ai",
            "Yiming Yang",
            "Hongxia Yang"
        ],
        "published": "2023-11-17T16:09:10Z",
        "summary": "Large Language Models (LLMs), despite their great power in language\ngeneration, often encounter challenges when dealing with intricate and\nknowledge-demanding queries in specific domains. This paper introduces a novel\napproach to enhance LLMs by effectively extracting the relevant knowledge from\ndomain-specific textual sources, and the adaptive training of a chatbot with\ndomain-specific inquiries. Our two-step approach starts from training a\nknowledge miner, namely LLMiner, which autonomously extracts Question-Answer\npairs from relevant documents through a chain-of-thought reasoning process.\nSubsequently, we blend the mined QA pairs with a conversational dataset to\nfine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise\nand conversational capabilities. We also developed a new evaluation benchmark\nwhich comprises four domain-specific text corpora and associated human-crafted\nQA pairs for testing. Our model shows remarkable performance improvement over\ngenerally aligned LLM and surpasses domain-adapted models directly fine-tuned\non domain corpus. In particular, LLMiner achieves this with minimal human\nintervention, requiring only 600 seed instances, thereby providing a pathway\ntowards self-improvement of LLMs through model-synthesized training data.",
        "pdf_link": "https://arxiv.org/pdf/2311.10614v1.pdf"
    },
    {
        "title": "TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes",
        "authors": [
            "Bibek Upadhayay",
            "Vahid Behzadan"
        ],
        "published": "2023-11-17T06:55:32Z",
        "summary": "Creating multilingual LLMs poses a significant challenge. Pretraining or\nfine-tuning LLMs to adopt new languages is evidently very costly. Furthermore,\nthere exist limitations concerning benchmark datasets and the metrics used to\nmeasure model performance in multilingual settings. This paper proposes\ncost-effective solutions to both aforementioned challenges. Firstly, we\nintroduce the Multilingual Instruction-Tuning Dataset (MITS), comprised of\nAlpaca-52K, Dolly-15K, and Vicuna Benchmark translations into 132 languages.\nSecondly, we propose a new method called \\emph{TaCo: Translation-Assisted\nCross-Linguality}, which utilizes translations in a chain-of-thought process to\ninstruction-tune LLMs on new languages through a curriculum-learning process.\nAs a proof of concept, we experimented with the instruction-tuned Guanaco-33B\nmodel, performing further instruction tuning using our proposed TaCo method in\nthree low-resource languages and one high-resource language. Our results\nindicate that the TaCo method impresses GPT-4 with an 82\\% score for a\nlow-resource language in the Vicuna Benchmark dataset, doubling the performance\nin contrast to instruction tuning alone. Furthermore, TaCo shows promise in\ncreating multilingual LLMs, even for low-resource languages. We have released\nour datasets and model adapters\\footnote{https://github.com/UNHSAILLab/TaCo} ,\nencouraging the research community to utilize these resources to advance work\non multilingual LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10797v2.pdf"
    },
    {
        "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
        "authors": [
            "Yangyi Chen",
            "Karan Sikka",
            "Michael Cogswell",
            "Heng Ji",
            "Ajay Divakaran"
        ],
        "published": "2023-11-16T18:37:29Z",
        "summary": "We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10081v2.pdf"
    },
    {
        "title": "ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond",
        "authors": [
            "Kanhai S. Amin",
            "Linda Mayes",
            "Pavan Khosla",
            "Rushabh Doshi"
        ],
        "published": "2023-11-16T18:30:14Z",
        "summary": "Purpose: Enhanced health literacy has been linked to better health outcomes;\nhowever, few interventions have been studied. We investigate whether large\nlanguage models (LLMs) can serve as a medium to improve health literacy in\nchildren and other populations.\n  Methods: We ran 288 conditions using 26 different prompts through\nChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate\nlimits, we tested a subset of 150 conditions through ChatGPT-4. The primary\noutcome measurements were the reading grade level (RGL) and word counts of\noutput.\n  Results: Across all models, output for basic prompts such as \"Explain\" and\n\"What is (are)\" were at, or exceeded, a 10th-grade RGL. When prompts were\nspecified to explain conditions from the 1st to 12th RGL, we found that LLMs\nhad varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided\nresponses that ranged from the 7th-grade to college freshmen RGL while\nChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.\nMicrosoft Bing provided responses from the 9th to 11th RGL while Google Bard\nprovided responses from the 7th to 10th RGL.\n  Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade\nlevel outputs. Meanwhile Bard and Bing tended to consistently produce an RGL\nthat is at the high school level regardless of prompt. Additionally, Bard's\nhesitancy in providing certain outputs indicates a cautious approach towards\nhealth information. LLMs demonstrate promise in enhancing health communication,\nbut future research should verify the accuracy and effectiveness of such tools\nin this context.\n  Implications: LLMs face challenges in crafting outputs below a sixth-grade\nreading level. However, their capability to modify outputs above this threshold\nprovides a potential mechanism to improve health literacy and communication in\na pediatric population and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2311.10075v1.pdf"
    },
    {
        "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
        "authors": [
            "Yao Qiang",
            "Xiangyu Zhou",
            "Dongxiao Zhu"
        ],
        "published": "2023-11-16T15:01:48Z",
        "summary": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09948v1.pdf"
    },
    {
        "title": "Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models",
        "authors": [
            "Debarati Das",
            "Ishaan Gupta",
            "Jaideep Srivastava",
            "Dongyeop Kang"
        ],
        "published": "2023-11-16T12:45:41Z",
        "summary": "Our research integrates graph data with Large Language Models (LLMs), which,\ndespite their advancements in various fields using large text corpora, face\nlimitations in encoding entire graphs due to context size constraints. This\npaper introduces a new approach to encoding a graph with diverse modalities,\nsuch as text, image, and motif, coupled with prompts to approximate a graph's\nglobal connectivity, thereby enhancing LLMs' efficiency in processing complex\ngraph structures. The study also presents GraphTMI, a novel benchmark for\nevaluating LLMs in graph structure analysis, focusing on homophily, motif\npresence, and graph difficulty. Key findings indicate that the image modality,\nespecially with vision-language models like GPT-4V, is superior to text in\nbalancing token limits and preserving essential information and outperforms\nprior graph neural net (GNN) encoders. Furthermore, the research assesses how\nvarious factors affect the performance of each encoding modality and outlines\nthe existing challenges and potential future developments for LLMs in graph\nunderstanding and reasoning tasks. All data will be publicly available upon\nacceptance.",
        "pdf_link": "https://arxiv.org/pdf/2311.09862v2.pdf"
    },
    {
        "title": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering",
        "authors": [
            "Tilahun Abedissa Taffa",
            "Ricardo Usbeck"
        ],
        "published": "2023-11-16T12:13:49Z",
        "summary": "This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09841v1.pdf"
    },
    {
        "title": "ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks",
        "authors": [
            "Yuliang Liu",
            "Xiangru Tang",
            "Zefan Cai",
            "Junjie Lu",
            "Yichi Zhang",
            "Yanjun Shao",
            "Zexuan Deng",
            "Helan Hu",
            "Zengxian Yang",
            "Kaikai An",
            "Ruijun Huang",
            "Shuzheng Si",
            "Sheng Chen",
            "Haozhe Zhao",
            "Zhengliang Li",
            "Liang Chen",
            "Yiming Zong",
            "Yan Wang",
            "Tianyu Liu",
            "Zhiwei Jiang",
            "Baobao Chang",
            "Yujia Qin",
            "Wangchunshu Zhou",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-11-16T12:03:21Z",
        "summary": "Large language models have shown promising performance in code generation\nbenchmarks. However, a considerable divide exists between these benchmark\nachievements and their practical applicability, primarily attributed to\nreal-world programming's reliance on pre-existing libraries. Instead of\nevaluating LLMs to code from scratch, this work aims to propose a new\nevaluation setup where LLMs use open-source libraries to finish machine\nlearning tasks. Therefore, we propose ML-Bench, an expansive benchmark\ndeveloped to assess the effectiveness of LLMs in leveraging existing functions\nin open-source libraries. Consisting of 10044 samples spanning 130 tasks over\n14 notable machine learning GitHub repositories. In this setting, given a\nspecific machine learning task instruction and the accompanying README in a\ncodebase, an LLM is tasked to generate code to accomplish the task. This\nnecessitates the comprehension of long and language-code interleaved documents,\nas well as the understanding of complex cross-file code structures, introducing\nnew challenges. Notably, while GPT-4 exhibits remarkable improvement over other\nLLMs, it manages to accomplish only 39.73\\% of the tasks, leaving a huge space\nfor improvement. We address these challenges by proposing ML-Agent, designed to\neffectively navigate the codebase, locate documentation, retrieve code, and\ngenerate executable code. Empirical results demonstrate that ML-Agent, built\nupon GPT-4, results in further improvements. Code, data, and models are\navailable at \\url{https://ml-bench.github.io/}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09835v1.pdf"
    },
    {
        "title": "FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models",
        "authors": [
            "Yimin Jing",
            "Renren Jin",
            "Jiahao Hu",
            "Huishi Qiu",
            "Xiaohua Wang",
            "Peng Wang",
            "Deyi Xiong"
        ],
        "published": "2023-11-16T11:53:31Z",
        "summary": "The effective assessment of the instruction-following ability of large\nlanguage models (LLMs) is of paramount importance. A model that cannot adhere\nto human instructions might be not able to provide reliable and helpful\nresponses. In pursuit of this goal, various benchmarks have been constructed to\nevaluate the instruction-following capacity of these models. However, these\nbenchmarks are limited to a single language and are constructed using automated\napproaches, which restricts their applicability and the quality of the test\nexamples they contain. To bridge this gap, we introduce the FollowEval\nbenchmark in this paper. This benchmark is composed of instances in both\nEnglish and Chinese, and all test examples are crafted by human experts.\nFurthermore, the FollowEval benchmark is designed to assess LLMs across five\ncritical dimensions of instruction following: string manipulation, commonsense\nreasoning, logical reasoning, spatial reasoning, and response constraints. To\nenhance the complexity and present a sufficient challenge, each test example is\ndesigned to evaluate more than one dimension. We have evaluated various LLMs\nusing the FollowEval benchmark and found that their performance significantly\nlags behind that of humans. This highlights the considerable room for\nimprovement in the instruction-following ability of these models.",
        "pdf_link": "https://arxiv.org/pdf/2311.09829v1.pdf"
    },
    {
        "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
        "authors": [
            "Nan Xu",
            "Fei Wang",
            "Ben Zhou",
            "Bang Zheng Li",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-11-16T11:52:22Z",
        "summary": "While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.",
        "pdf_link": "https://arxiv.org/pdf/2311.09827v2.pdf"
    },
    {
        "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Anni Zou",
            "Zhuosheng Zhang",
            "Ziming Li",
            "Yilun Zhao",
            "Xingyao Zhang",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-11-16T11:47:58Z",
        "summary": "Large language models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and reasoning over specialized knowledge. To address these\nissues, we propose a novel Multi-disciplinary Collaboration (MC) framework for\nthe medical domain that leverages LLM-based agents in a role-playing setting\nthat participate in a collaborative multi-round discussion, thereby enhancing\nLLM proficiency and reasoning capabilities. This training-free framework\nencompasses five critical steps: gathering domain experts, proposing individual\nanalyses, summarising these analyses into a report, iterating over discussions\nuntil a consensus is reached, and ultimately making a decision. Our work\nfocuses on the zero-shot setting, which is applicable in real-world scenarios.\nExperimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six\nsubtasks from MMLU) establish that our proposed MC framework excels at mining\nand harnessing the medical expertise within LLMs, as well as extending its\nreasoning abilities. Our code can be found at\n\\url{https://github.com/gersteinlab/MedAgents}.",
        "pdf_link": "https://arxiv.org/pdf/2311.10537v3.pdf"
    },
    {
        "title": "The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text",
        "authors": [
            "Yanzhu Guo",
            "Guokan Shang",
            "Michalis Vazirgiannis",
            "Chlo√© Clavel"
        ],
        "published": "2023-11-16T11:31:50Z",
        "summary": "This study investigates the consequences of training large language models\n(LLMs) on synthetic data generated by their predecessors, an increasingly\nprevalent practice aimed at addressing the limited supply of human-generated\ntraining data. Diverging from the usual emphasis on performance metrics, we\nfocus on the impact of this training methodology on linguistic diversity,\nespecially when conducted recursively over time. To assess this, we developed a\nset of novel metrics targeting lexical, syntactic, and semantic diversity,\napplying them in recursive fine-tuning experiments across various natural\nlanguage generation tasks. Our findings reveal a marked decrease in the\ndiversity of the models' outputs through successive iterations. This trend\nunderscores the potential risks of training LLMs on predecessor-generated text,\nparticularly concerning the preservation of linguistic richness. Our study\nhighlights the need for careful consideration of the long-term effects of such\ntraining approaches on the linguistic capabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09807v1.pdf"
    },
    {
        "title": "DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Yitao Long",
            "Hongjun Liu",
            "Linyong Nan",
            "Lyuhao Chen",
            "Ryo Kamoi",
            "Yixin Liu",
            "Xiangru Tang",
            "Rui Zhang",
            "Arman Cohan"
        ],
        "published": "2023-11-16T11:30:53Z",
        "summary": "Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning and problem-solving capabilities of LLMs in the context of\nunderstanding and analyzing financial documents containing both text and\ntables. We evaluate a wide spectrum of 19 LLMs, including those specialized in\ncoding and finance. We also incorporate different prompting strategies (i.e.,\nChain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the\ncapabilities and limitations of existing LLMs in DocMath-Eval. We found that,\nalthough the current best-performing system (i.e., GPT-4), can perform well on\nsimple problems such as calculating the rate of increase in a financial metric\nwithin a short document context, it significantly lags behind human experts in\nmore complex problems grounded in longer contexts. We believe DocMath-Eval can\nbe used as a valuable benchmark to evaluate LLMs' capabilities to solve\nchallenging numerical reasoning problems in expert domains. We will release the\nbenchmark and code at https://github.com/yale-nlp/DocMath-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2311.09805v1.pdf"
    },
    {
        "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
        "authors": [
            "Sen Yang",
            "Xin Li",
            "Leyang Cui",
            "Lidong Bing",
            "Wai Lam"
        ],
        "published": "2023-11-16T11:26:21Z",
        "summary": "Though prompting LLMs with various reasoning structures produces reasoning\nproofs along with answers, these proofs are not ensured to be causal and\nreliable due to the inherent defects of LLMs. Tracking such deficiencies, we\npresent a neuro-symbolic integration method, in which a neural LLM is used to\nrepresent the knowledge of the problem while an LLM-free symbolic solver is\nadopted to do deliberative reasoning using the knowledge. Specifically, our\ncustomized meta-interpreters allow the production of reasoning proofs and\nsupport flexible search strategies. These reasoning proofs are ensured to be\ncausal and reliable because of the deterministic executing nature of the\nsymbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT\nbaseline by nearly double in accuracy and more than triple in proof similarity.\nOn GSM8K, our method also shows accuracy improvements and nearly doubled proof\nsimilarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
        "pdf_link": "https://arxiv.org/pdf/2311.09802v1.pdf"
    },
    {
        "title": "How Far Can We Extract Diverse Perspectives from Large Language Models?",
        "authors": [
            "Shirley Anugrah Hayati",
            "Minhwa Lee",
            "Dheeraj Rajagopal",
            "Dongyeop Kang"
        ],
        "published": "2023-11-16T11:23:38Z",
        "summary": "Collecting diverse human opinions is costly and challenging. This leads to a\nrecent trend in collaborative efforts between humans and Large Language Models\n(LLMs) for generating diverse data, offering potential scalable and efficient\nsolutions. However, the extent of LLMs' capability to generate diverse\nperspectives on subjective topics remains an unexplored question. In this\nstudy, we investigate LLMs' capacity for generating diverse perspectives and\nrationales on subjective topics, such as social norms and argumentative texts.\nWe formulate a new problem of maximum diversity extraction from LLMs. Motivated\nby how humans develop their opinions through their values, we propose a\ncriteria-based prompting technique to ground diverse opinions. To see how far\nwe can extract diverse perspectives from LLMs, or called diversity coverage, we\nemploy a step-by-step recall prompting for generating more outputs from the\nmodel in an iterative manner. As we apply our methods to various tasks, indeed\nwe find that LLMs can generate diverse opinions according to the degree of task\nsubjectivity",
        "pdf_link": "https://arxiv.org/pdf/2311.09799v2.pdf"
    },
    {
        "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions",
        "authors": [
            "Nikita Moghe",
            "Patrick Xia",
            "Jacob Andreas",
            "Jason Eisner",
            "Benjamin Van Durme",
            "Harsh Jhamtani"
        ],
        "published": "2023-11-16T11:19:26Z",
        "summary": "Users of natural language interfaces, generally powered by Large Language\nModels (LLMs),often must repeat their preferences each time they make a similar\nrequest. We describe an approach to LLM-based dialogue modeling in which\npersistent user constraints and preferences -- collectively termed standing\ninstructions -- as additional context for such interfaces. For example, when a\nuser states \"I'm hungry\", a previously expressed preference for Persian food\ncan be automatically added to the LLM prompt, influencing the search for\nrelevant restaurants. We develop NLSI, a language-to-program dataset consisting\nof over 2.4K dialogues spanning 17 domains, where each dialogue is paired with\na user profile (a set of users specific standing instructions) and\ncorresponding structured representations (API calls). A key challenge in NLSI\nis to identify which subset of the standing instructions is applicable to a\ngiven dialogue. NLSI contains diverse phenomena, from simple preferences to\ninterdependent instructions such as triggering a hotel search whenever the user\nis booking tickets to an event. We conduct experiments on NLSI using prompting\nwith large language models and various retrieval approaches, achieving a\nmaximum of 44.7% exact match on API prediction. Our results demonstrate the\nchallenges in identifying the relevant standing instructions and their\ninterpretation into API calls.",
        "pdf_link": "https://arxiv.org/pdf/2311.09796v2.pdf"
    },
    {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "authors": [
            "Chunyuan Deng",
            "Yilun Zhao",
            "Xiangru Tang",
            "Mark Gerstein",
            "Arman Cohan"
        ],
        "published": "2023-11-16T11:03:04Z",
        "summary": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "pdf_link": "https://arxiv.org/pdf/2311.09783v2.pdf"
    },
    {
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
            "Bin Lin",
            "Yang Ye",
            "Bin Zhu",
            "Jiaxi Cui",
            "Munan Ning",
            "Peng Jin",
            "Li Yuan"
        ],
        "published": "2023-11-16T10:59:44Z",
        "summary": "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.10122v2.pdf"
    },
    {
        "title": "Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations",
        "authors": [
            "Wenjie Mo",
            "Jiashu Xu",
            "Qin Liu",
            "Jiongxiao Wang",
            "Jun Yan",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-11-16T10:38:43Z",
        "summary": "Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes particularly pronounced in the context of Large Language Models\n(LLMs) deployed as Web Services, which typically offer only black-box access,\nrendering training-time defenses impractical. To bridge this gap, our work\nintroduces defensive demonstrations, an innovative backdoor defense strategy\nfor blackbox large language models. Our method involves identifying the task\nand retrieving task-relevant demonstrations from an uncontaminated pool. These\ndemonstrations are then combined with user queries and presented to the model\nduring testing, without requiring any modifications/tuning to the black-box\nmodel or insights into its internal mechanisms. Defensive demonstrations are\ndesigned to counteract the adverse effects of triggers, aiming to recalibrate\nand correct the behavior of poisoned models during test-time evaluations.\nExtensive experiments show that defensive demonstrations are effective in\ndefending both instance-level and instruction-level backdoor attacks, not only\nrectifying the behavior of poisoned models but also surpassing existing\nbaselines in most scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2311.09763v1.pdf"
    },
    {
        "title": "Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models",
        "authors": [
            "Jinyoung Park",
            "Ameen Patel",
            "Omar Zia Khan",
            "Hyunwoo J. Kim",
            "Joo-Kyung Kim"
        ],
        "published": "2023-11-16T10:36:08Z",
        "summary": "Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning\ncapabilities of Large Language Models (LLMs) by generating a series of\nrationales before the final answer. We analyze the reasoning paths generated by\nCoT and find two issues in multi-step reasoning: (i) Generating rationales\nirrelevant to the question, (ii) Unable to compose subquestions or queries for\ngenerating/retrieving all the relevant information. To address them, we propose\na graph-guided CoT prompting method, which guides the LLMs to reach the correct\nanswer with graph representation/verification steps. Specifically, we first\nleverage LLMs to construct a \"question/rationale graph\" by using knowledge\nextraction prompting given the initial question and the rationales generated in\nthe previous steps. Then, the graph verification step diagnoses the current\nrationale triplet by comparing it with the existing question/rationale graph to\nfilter out irrelevant rationales and generate follow-up questions to obtain\nrelevant information. Additionally, we generate CoT paths that exclude the\nextracted graph information to represent the context information missed from\nthe graph extraction. Our graph-guided reasoning method shows superior\nperformance compared to previous CoT prompting and the variants on multi-hop\nquestion answering benchmark datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.09762v1.pdf"
    },
    {
        "title": "How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?",
        "authors": [
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2023-11-16T10:30:00Z",
        "summary": "Pruning and quantization form the foundation of model compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nstate-of-the-art performance in a post-training setting. They rely upon\ncalibration data, a small set of unlabeled examples, to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of pruning and\nquantization methods, tasks, models, and datasets. Surprisingly, we find\nsubstantial variations in downstream task performance, contrasting existing\nwork that suggests a greater level of robustness to the calibration data.\nFinally, we make a series of recommendations for the effective use of\ncalibration data in LLM quantization and pruning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09755v1.pdf"
    },
    {
        "title": "GEO: Generative Engine Optimization",
        "authors": [
            "Pranjal Aggarwal",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Karthik R Narasimhan",
            "Ameet Deshpande"
        ],
        "published": "2023-11-16T10:06:09Z",
        "summary": "The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of Generative Engines (GEs), has the potential to generate\naccurate and personalized responses, and is rapidly replacing traditional\nsearch engines like Google and Bing. Generative Engines typically satisfy\nqueries by synthesizing information from multiple sources and summarizing them\nwith the help of LLMs. While this shift significantly improves \\textit{user}\nutility and \\textit{generative search engine} traffic, it results in a huge\nchallenge for the third stakeholder -- website and content creators. Given the\nblack-box and fast-moving nature of Generative Engines, content creators have\nlittle to no control over when and how their content is displayed. With\ngenerative engines here to stay, the right tools should be provided to ensure\nthat creator economy is not severely disadvantaged. To address this, we\nintroduce Generative Engine Optimization (GEO), a novel paradigm to aid content\ncreators in improving the visibility of their content in Generative Engine\nresponses through a black-box optimization framework for optimizing and\ndefining visibility metrics. We facilitate systematic evaluation in this new\nparadigm by introducing GEO-bench, a benchmark of diverse user queries across\nmultiple domains, coupled with sources required to answer these queries.\nThrough rigorous evaluation, we show that GEO can boost visibility by up to\n40\\% in generative engine responses. Moreover, we show the efficacy of these\nstrategies varies across domains, underscoring the need for domain-specific\nmethods. Our work opens a new frontier in the field of information discovery\nsystems, with profound implications for generative engines and content\ncreators.",
        "pdf_link": "https://arxiv.org/pdf/2311.09735v1.pdf"
    },
    {
        "title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction",
        "authors": [
            "Xinliang Frederick Zhang",
            "Winston Wu",
            "Nick Beauchamp",
            "Lu Wang"
        ],
        "published": "2023-11-16T10:04:49Z",
        "summary": "News media employ moral language to create memorable stories, and readers\noften engage with the content that align with their values. Moral theories have\nbeen applied to news analysis studying moral values in isolation, while the\nintricate dynamics among participating entities in shaping moral events have\nbeen overlooked. This is mainly due to the use of obscure language to conceal\nevident ideology and values, coupled with the insufficient moral reasoning\ncapability in most existing NLP systems, where LLMs are no exception. To study\nthis phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of\n5,494 structured annotations on 474 news articles by diverse US media across\nthe political spectrum. We further propose MOKA, a moral event extraction\nframework with MOral Knowledge Augmentation, that leverages knowledge derived\nfrom moral words and moral scenarios. Experimental results show that MOKA\noutperforms competitive baselines across three moral event understanding tasks.\nFurther analyses illuminate the selective reporting of moral events by media\noutlets of different ideological leanings, suggesting the significance of\nevent-level morality analysis in news. Our datasets and codebase are available\nat https://github.com/launchnlp/MOKA.",
        "pdf_link": "https://arxiv.org/pdf/2311.09733v1.pdf"
    },
    {
        "title": "Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge",
        "authors": [
            "Genglin Liu",
            "Xingyao Wang",
            "Lifan Yuan",
            "Yangyi Chen",
            "Hao Peng"
        ],
        "published": "2023-11-16T10:02:40Z",
        "summary": "Can large language models (LLMs) express their uncertainty in situations\nwhere they lack sufficient parametric knowledge to generate reasonable\nresponses? This work aims to systematically investigate LLMs' behaviors in such\nsituations, emphasizing the trade-off between honesty and helpfulness. To\ntackle the challenge of precisely determining LLMs' knowledge gaps, we\ndiagnostically create unanswerable questions containing non-existent concepts\nor false premises, ensuring that they are outside the LLMs' vast training data.\nBy compiling a benchmark, UnknownBench, which consists of both unanswerable and\nanswerable questions, we quantitatively evaluate the LLMs' performance in\nmaintaining honesty while being helpful. Using a model-agnostic unified\nconfidence elicitation approach, we observe that most LLMs fail to consistently\nrefuse or express uncertainty towards questions outside their parametric\nknowledge, although instruction fine-tuning and alignment techniques can\nprovide marginal enhancements. Moreover, LLMs' uncertainty expression does not\nalways stay consistent with the perceived confidence of their textual outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09731v2.pdf"
    },
    {
        "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
        "authors": [
            "Huaman Sun",
            "Jiaxin Pei",
            "Minje Choi",
            "David Jurgens"
        ],
        "published": "2023-11-16T10:02:24Z",
        "summary": "Human perception of language depends on personal backgrounds like gender and\nethnicity. While existing studies have shown that large language models (LLMs)\nhold values that are closer to certain societal groups, it is unclear whether\ntheir prediction behaviors on subjective NLP tasks also exhibit a similar bias.\nIn this study, leveraging the POPQUORN dataset which contains annotations of\ndiverse demographic backgrounds, we conduct a series of experiments on four\npopular LLMs to investigate their capability to understand group differences\nand potential biases in their predictions for politeness and offensiveness. We\nfind that for both tasks, model predictions are closer to the labels from White\nand female participants. We further explore prompting with the target\ndemographic labels and show that including the target demographic in the prompt\nactually worsens the model's performance. More specifically, when being\nprompted to respond from the perspective of \"Black\" and \"Asian\" individuals,\nmodels show lower performance in predicting both overall scores as well as the\nscores from corresponding groups. Our results suggest that LLMs hold gender and\nracial biases for subjective NLP tasks and that demographic-infused prompts\nalone may be insufficient to mitigate such effects. Code and data are available\nat https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
        "pdf_link": "https://arxiv.org/pdf/2311.09730v1.pdf"
    },
    {
        "title": "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",
        "authors": [
            "Fei Yu",
            "Anningzhe Gao",
            "Benyou Wang"
        ],
        "published": "2023-11-16T09:56:28Z",
        "summary": "Large language models (LLMs) often struggle with maintaining accuracy\nthroughout multiple multiple reasoning steps, especially in mathematical\nreasoning where an error in earlier steps can propagate to subsequent ones and\nit ultimately leading to an incorrect answer. To reduce error propagation,\nguided decoding is employed to direct the LM decoding on a step-by-step basis.\nWe argue that in guided decoding, assessing the potential of an incomplete\nreasoning path can be more advantageous than simply ensuring per-step\ncorrectness, as the former approach leads towards a correct final answer. This\ntransforms the task into a $\\textit{value estimation}$ problem in planning.\n  Inspired by the findings that $\\textit{outcome supervision for guided\ndecoding essentially acts as a value model}$, we propose Outcome-supervised\nValue Model (OVM) that employs outcome supervision for training a value model,\nwhich prioritizes steps that lead to accurate conclusions. Furthermore, the OVM\neliminates the need for labor-intensive annotations of step-level correctness,\nthereby significantly enhancing its scalability. Our experiments on two\nmulti-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate\nthe superior performance of the OVM model. Notably, in GSM8K, our\n$\\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B\nparameters}$; especially it does not utilize GPT-4 or code execution. These\nfindings offer a novel perspective on the role of outcome supervision in\ntraining value models for multi-step reasoning tasks and provide theoretical\njustification for its advantage in value estimation for guided decoding.",
        "pdf_link": "https://arxiv.org/pdf/2311.09724v2.pdf"
    },
    {
        "title": "Large Language Model Inference with Lexical Shortlisting",
        "authors": [
            "Nikolay Bogoychev",
            "Pinzhen Chen",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "published": "2023-11-16T09:35:50Z",
        "summary": "Large language model (LLM) inference is computation and memory intensive, so\nwe adapt lexical shortlisting to it hoping to improve both. While lexical\nshortlisting is well-explored in tasks like machine translation, it requires\nmodifications before being suitable for LLMs as the intended applications vary\nsignificantly. Our work studies two heuristics to shortlist sub-vocabulary at\nLLM inference time: Unicode-based script filtering and corpus-based selection.\nWe explore different LLM families and sizes, and we find that lexical\nshortlisting can reduce the memory usage of some models by nearly 50\\% and has\nan upper bound of 25\\% improvement in generation speed. In this pilot study, we\nalso identify the drawbacks of such vocabulary selection methods and propose\navenues for future research.",
        "pdf_link": "https://arxiv.org/pdf/2311.09709v1.pdf"
    },
    {
        "title": "Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance",
        "authors": [
            "Shiro Takagi",
            "Ryutaro Yamauchi",
            "Wataru Kumagai"
        ],
        "published": "2023-11-16T09:34:23Z",
        "summary": "Research automation efforts usually employ AI as a tool to automate specific\ntasks within the research process. To create an AI that truly conduct research\nthemselves, it must independently generate hypotheses, design verification\nplans, and execute verification. Therefore, we investigated if an AI itself\ncould autonomously generate and verify hypothesis for a toy machine learning\nresearch problem. We prompted GPT-4 to generate hypotheses and Python code for\nhypothesis verification with limited methodological guidance. Our findings\nsuggest that, in some instances, GPT-4 can autonomously generate and validate\nhypotheses without detailed guidance. While this is a promising result, we also\nfound that none of the verifications were flawless, and there remain\nsignificant challenges in achieving autonomous, human-level research using only\ngeneric instructions. These findings underscore the need for continued\nexploration to develop a general and autonomous AI researcher.",
        "pdf_link": "https://arxiv.org/pdf/2311.09706v1.pdf"
    },
    {
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
        "authors": [
            "Bangzheng Li",
            "Ben Zhou",
            "Fei Wang",
            "Xingyu Fu",
            "Dan Roth",
            "Muhao Chen"
        ],
        "published": "2023-11-16T09:27:36Z",
        "summary": "Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09702v3.pdf"
    },
    {
        "title": "BLT: Can Large Language Models Handle Basic Legal Text?",
        "authors": [
            "Andrew Blair-Stanek",
            "Nils Holzenberger",
            "Benjamin Van Durme"
        ],
        "published": "2023-11-16T09:09:22Z",
        "summary": "We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM\n2} currently perform poorly at basic legal text handling. We introduce a\nbenchmark consisting of tasks that lawyers and paralegals would expect LLMs to\nhandle zero-shot, such as looking up the text at a line of a witness deposition\nor at a subsection of a contract. LLMs' poor performance on this benchmark\ncasts into doubt their reliability as-is for legal practice. However,\nfine-tuning for these tasks brings even a smaller model to near-perfect\nperformance on our test set and also raises performance on a related legal\ntask. These results suggest that many simple behaviors needed for a domain may\nnot be present in foundational LLMs, without additional engagement from subject\nmatter experts.",
        "pdf_link": "https://arxiv.org/pdf/2311.09693v2.pdf"
    },
    {
        "title": "Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies",
        "authors": [
            "Zihao He",
            "Siyi Guo",
            "Ashwin Rao",
            "Kristina Lerman"
        ],
        "published": "2023-11-16T08:57:53Z",
        "summary": "Social media platforms are rife with politically charged discussions.\nTherefore, accurately deciphering and predicting partisan biases using Large\nLanguage Models (LLMs) is increasingly critical. In this study, we address the\nchallenge of understanding political bias in digitized discourse using LLMs.\nWhile traditional approaches often rely on finetuning separate models for each\npolitical faction, our work innovates by employing a singular,\ninstruction-tuned LLM to reflect a spectrum of political ideologies. We present\na comprehensive analytical framework, consisting of Partisan Bias Divergence\nAssessment and Partisan Class Tendency Prediction, to evaluate the model's\nalignment with real-world political ideologies in terms of stances, emotions,\nand moral foundations. Our findings reveal the model's effectiveness in\ncapturing emotional and moral nuances, albeit with some challenges in stance\ndetection, highlighting the intricacies and potential for refinement in NLP\ntools for politically sensitive contexts. This research contributes\nsignificantly to the field by demonstrating the feasibility and importance of\nnuanced political understanding in LLMs, particularly for applications\nrequiring acute awareness of political bias.",
        "pdf_link": "https://arxiv.org/pdf/2311.09687v1.pdf"
    },
    {
        "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions",
        "authors": [
            "Hanning Zhang",
            "Shizhe Diao",
            "Yong Lin",
            "Yi R. Fung",
            "Qing Lian",
            "Xingyao Wang",
            "Yangyi Chen",
            "Heng Ji",
            "Tong Zhang"
        ],
        "published": "2023-11-16T08:45:44Z",
        "summary": "Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the knowledge gap between parametric knowledge and the\ninstruction tuning data. Then, we construct the refusal-aware data based on the\nknowledge intersection, to tune LLMs to refrain from responding to questions\nbeyond its parametric knowledge. Experimental results demonstrate this new\ninstruction tuning approach effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty during training displays a\nbetter ability to estimate uncertainty than uncertainty-based testing. Our code\nwill be released at https://github.com/shizhediao/R-Tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09677v1.pdf"
    },
    {
        "title": "Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring",
        "authors": [
            "Yuhang Li",
            "Yihan Wang",
            "Zhouxing Shi",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-11-16T08:36:00Z",
        "summary": "The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.",
        "pdf_link": "https://arxiv.org/pdf/2311.09668v1.pdf"
    },
    {
        "title": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents",
        "authors": [
            "Yun-Shiuan Chuang",
            "Siddharth Suresh",
            "Nikunj Harlalka",
            "Agam Goyal",
            "Robert Hawkins",
            "Sijia Yang",
            "Dhavan Shah",
            "Junjie Hu",
            "Timothy T. Rogers"
        ],
        "published": "2023-11-16T08:30:15Z",
        "summary": "Human groups are able to converge on more accurate beliefs through\ndeliberation, even in the presence of polarization and partisan bias -- a\nphenomenon known as the \"wisdom of partisan crowds.\" Generated agents powered\nby Large Language Models (LLMs) are increasingly used to simulate human\ncollective behavior, yet few benchmarks exist for evaluating their dynamics\nagainst the behavior of human groups. In this paper, we examine the extent to\nwhich the wisdom of partisan crowds emerges in groups of LLM-based agents that\nare prompted to role-play as partisan personas (e.g., Democrat or Republican).\nWe find that they not only display human-like partisan biases, but also\nconverge to more accurate beliefs through deliberation as humans do. We then\nidentify several factors that interfere with convergence, including the use of\nchain-of-thought prompt and lack of details in personas. Conversely,\nfine-tuning on human data appears to enhance convergence. These findings show\nthe potential and limitations of LLM-based agents as a model of human\ncollective intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2311.09665v2.pdf"
    },
    {
        "title": "Structured Chemistry Reasoning with Large Language Models",
        "authors": [
            "Siru Ouyang",
            "Zhuosheng Zhang",
            "Bing Yan",
            "Xuan Liu",
            "Yejin Choi",
            "Jiawei Han",
            "Lianhui Qin"
        ],
        "published": "2023-11-16T08:20:36Z",
        "summary": "Large Language Models (LLMs) excel in diverse areas, yet struggle with\ncomplex scientific reasoning, especially in the field of chemistry. Different\nfrom the simple chemistry tasks (e.g., molecule classification) addressed in\nprevious studies, complex chemistry problems require not only vast knowledge\nand precise calculation, but also compositional reasoning about rich dynamic\ninteractions of different concepts (e.g., temperature changes). Our study shows\nthat even advanced LLMs, like GPT-4, can fail easily in different ways.\nInterestingly, the errors often stem not from a lack of domain knowledge within\nthe LLMs, but rather from the absence of an effective reasoning structure that\nguides the LLMs to elicit the right knowledge, incorporate the knowledge in\nstep-by-step reasoning, and iteratively refine results for further improved\nquality. On this basis, we introduce StructChem, a simple yet effective\nprompting strategy that offers the desired guidance and substantially boosts\nthe LLMs' chemical reasoning capability. Testing across four chemistry areas --\nquantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem\nsubstantially enhances GPT-4's performance, with up to 30\\% peak improvement.\nOur analysis also underscores the unique difficulties of precise grounded\nreasoning in science with LLMs, highlighting a need for more research in this\narea. Code is available at \\url{https://github.com/ozyyshr/StructChem}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09656v2.pdf"
    },
    {
        "title": "On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models",
        "authors": [
            "Jiongxiao Wang",
            "Junlin Wu",
            "Muhao Chen",
            "Yevgeniy Vorobeychik",
            "Chaowei Xiao"
        ],
        "published": "2023-11-16T07:48:45Z",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09641v1.pdf"
    },
    {
        "title": "Automatic Engineering of Long Prompts",
        "authors": [
            "Cho-Jui Hsieh",
            "Si Si",
            "Felix X. Yu",
            "Inderjit S. Dhillon"
        ],
        "published": "2023-11-16T07:42:46Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex open-domain tasks, guided by comprehensive instructions and\ndemonstrations provided in the form of prompts. However, these prompts can be\nlengthy, often comprising hundreds of lines and thousands of tokens, and their\ndesign often requires considerable human effort. Recent research has explored\nautomatic prompt engineering for short prompts, typically consisting of one or\na few sentences. However, the automatic design of long prompts remains a\nchallenging problem due to its immense search space. In this paper, we\ninvestigate the performance of greedy algorithms and genetic algorithms for\nautomatic long prompt engineering. We demonstrate that a simple greedy approach\nwith beam search outperforms other methods in terms of search efficiency.\nMoreover, we introduce two novel techniques that utilize search history to\nenhance the effectiveness of LLM-based mutation in our search algorithm. Our\nresults show that the proposed automatic long prompt engineering algorithm\nachieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,\nhighlighting the significance of automating prompt designs to fully harness the\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.10117v1.pdf"
    },
    {
        "title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework",
        "authors": [
            "Matthew Pisano",
            "Peter Ly",
            "Abraham Sanders",
            "Bingsheng Yao",
            "Dakuo Wang",
            "Tomek Strzalkowski",
            "Mei Si"
        ],
        "published": "2023-11-16T07:31:18Z",
        "summary": "Research into AI alignment has grown considerably since the recent\nintroduction of increasingly capable Large Language Models (LLMs).\nUnfortunately, modern methods of alignment still fail to fully prevent harmful\nresponses when models are deliberately attacked. These attacks can trick\nseemingly aligned models into giving manufacturing instructions for dangerous\nmaterials, inciting violence, or recommending other immoral acts. To help\nmitigate this issue, we introduce Bergeron: a framework designed to improve the\nrobustness of LLMs against attacks without any additional parameter\nfine-tuning. Bergeron is organized into two tiers; with a secondary LLM\nemulating the conscience of a protected, primary LLM. This framework better\nsafeguards the primary model against incoming attacks while monitoring its\noutput for any harmful content. Empirical analysis shows that, by using\nBergeron to complement models with existing alignment training, we can improve\nthe robustness and safety of multiple, commonly used commercial and open-source\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2312.00029v2.pdf"
    },
    {
        "title": "CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model",
        "authors": [
            "Nakyeong Yang",
            "Taegwan Kang",
            "Kyomin Jung"
        ],
        "published": "2023-11-16T07:16:55Z",
        "summary": "Large language models (LLMs) executing tasks through instruction-based\nprompts often face challenges stemming from distribution differences between\nuser instructions and training instructions. This leads to distractions and\nbiases, especially when dealing with inconsistent dynamic labels. In this\npaper, we introduces a novel bias mitigation method, CRISPR, designed to\nalleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods\nto identify bias neurons influencing biased outputs and employs pruning to\neliminate the bias neurons. Experimental results demonstrate the method's\neffectiveness in mitigating biases in instruction-based prompting, enhancing\nlanguage model performance on social bias benchmarks without compromising\npre-existing knowledge. CRISPR proves highly practical, model-agnostic,\noffering flexibility in adapting to evolving social biases.",
        "pdf_link": "https://arxiv.org/pdf/2311.09627v1.pdf"
    },
    {
        "title": "Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations",
        "authors": [
            "Jing Yao",
            "Wei Xu",
            "Jianxun Lian",
            "Xiting Wang",
            "Xiaoyuan Yi",
            "Xing Xie"
        ],
        "published": "2023-11-16T07:09:38Z",
        "summary": "The significant progress of large language models (LLMs) provides a promising\nopportunity to build human-like systems for various practical applications.\nHowever, when applied to specific task domains, an LLM pre-trained on a\ngeneral-purpose corpus may exhibit a deficit or inadequacy in two types of\ndomain-specific knowledge. One is a comprehensive set of domain data that is\ntypically large-scale and continuously evolving. The other is specific working\npatterns of this domain reflected in the data. The absence or inadequacy of\nsuch knowledge impacts the performance of the LLM. In this paper, we propose a\ngeneral paradigm that augments LLMs with DOmain-specific KnowledgE to enhance\ntheir performance on practical applications, namely DOKE. This paradigm relies\non a domain knowledge extractor, working in three steps: 1) preparing effective\nknowledge for the task; 2) selecting the knowledge for each specific sample;\nand 3) expressing the knowledge in an LLM-understandable way. Then, the\nextracted knowledge is incorporated through prompts, without any computational\ncost of model fine-tuning. We instantiate the general paradigm on a widespread\napplication, i.e. recommender systems, where critical item attributes and\ncollaborative filtering signals are incorporated. Experimental results\ndemonstrate that DOKE can substantially improve the performance of LLMs in\nspecific domains.",
        "pdf_link": "https://arxiv.org/pdf/2311.10779v1.pdf"
    },
    {
        "title": "Simulating Opinion Dynamics with Networks of LLM-based Agents",
        "authors": [
            "Yun-Shiuan Chuang",
            "Agam Goyal",
            "Nikunj Harlalka",
            "Siddharth Suresh",
            "Robert Hawkins",
            "Sijia Yang",
            "Dhavan Shah",
            "Junjie Hu",
            "Timothy T. Rogers"
        ],
        "published": "2023-11-16T07:01:48Z",
        "summary": "Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations often over-simplify human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\nproducing accurate information, leading simulated agents to consensus in line\nwith scientific reality. This bias limits their utility for understanding\nresistance to consensus views on issues like climate change. After inducing\nconfirmation bias through prompt engineering, however, we observed opinion\nfragmentation in line with existing agent-based modeling and opinion dynamics\nresearch. These insights highlight the promise and limitations of LLM agents in\nthis domain and suggest a path forward: refining LLMs with real-world discourse\nto better simulate the evolution of human beliefs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09618v4.pdf"
    },
    {
        "title": "Self-Contradictory Reasoning Evaluation and Detection",
        "authors": [
            "Ziyi Liu",
            "Isabelle Lee",
            "Yongkang Du",
            "Soumya Sanyal",
            "Jieyu Zhao"
        ],
        "published": "2023-11-16T06:22:17Z",
        "summary": "In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks\nfocus on performance-wise evaluation. Two fundamental questions persist: 1) how\nreliable is the quality of reasoning, and 2) can models detect unreliable\nreasoning? In this paper, we investigate self-contradictory (Self-Contra)\nreasoning, where the model reasoning does not support predictions. To address\n1), we assess the Self-Contra rate across four datasets and delve into\nfiner-grained categories of Self-Contra reasoning. We find that LLMs often\ncontradict themselves when performing reasoning tasks that involve contextual\ninformation understanding or commonsense. Importantly, a higher accuracy does\nnot necessarily correspond to a lower Self-Contra rate. The model may appear to\ngenerate correct answers but it may take shortcuts in reasoning or skip over\ncontextual evidence, thereby displaying Self-Contra behaviors with compromised\nreasoning. As for 2), we task GPT-4 with identifying Self-Contra reasoning and\nfiner-grained fallacies. We observe that GPT-4 struggles to effectively detect\nSelf-Contra reasoning, with significantly low performance compared with human\njudgment. Our results indicate that the current LLMs lack robustness necessary\nfor reliable reasoning and we emphasize the urgent need for establishing best\npractices in comprehensive reasoning evaluations beyond accuracy-based metrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.09603v2.pdf"
    },
    {
        "title": "Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion",
        "authors": [
            "Smriti Singh",
            "Cornelia Caragea",
            "Junyi Jessy Li"
        ],
        "published": "2023-11-16T06:20:13Z",
        "summary": "Situations and events evoke emotions in humans, but to what extent do they\ninform the prediction of emotion detection models? This work investigates how\nwell human-annotated emotion triggers correlate with features that models\ndeemed salient in their prediction of emotions. First, we introduce a novel\ndataset EmoTrigger, consisting of 900 social media posts sourced from three\ndifferent datasets; these were annotated by experts for emotion triggers with\nhigh agreement. Using EmoTrigger, we evaluate the ability of large language\nmodels (LLMs) to identify emotion triggers, and conduct a comparative analysis\nof the features considered important for these tasks between LLMs and\nfine-tuned models. Our analysis reveals that emotion triggers are largely not\nconsidered salient features for emotion prediction models, instead there is\nintricate interplay between various features and the task of emotion detection.",
        "pdf_link": "https://arxiv.org/pdf/2311.09602v2.pdf"
    },
    {
        "title": "LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks",
        "authors": [
            "Mihir Parmar",
            "Aakanksha Naik",
            "Himanshu Gupta",
            "Disha Agrawal",
            "Chitta Baral"
        ],
        "published": "2023-11-16T04:57:49Z",
        "summary": "Many large language models (LLMs) for medicine have largely been evaluated on\nshort texts, and their ability to handle longer sequences such as a complete\nelectronic health record (EHR) has not been systematically explored. Assessing\nthese models on long sequences is crucial since prior work in the general\ndomain has demonstrated performance degradation of LLMs on longer texts.\nMotivated by this, we introduce LongBoX, a collection of seven medical datasets\nin text-to-text format, designed to investigate model performance on long\nsequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)\nand strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We\nfurther evaluate two techniques designed for long-sequence handling: (i)\nlocal-global attention, and (ii) Fusion-in-Decoder (FiD). Our results\ndemonstrate mixed results with long-sequence handling - while scores on some\ndatasets increase, there is substantial room for improvement. We hope that\nLongBoX facilitates the development of more effective long-sequence techniques\nfor the medical domain. Data and source code are available at\nhttps://github.com/Mihir3009/LongBoX.",
        "pdf_link": "https://arxiv.org/pdf/2311.09564v1.pdf"
    },
    {
        "title": "Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization",
        "authors": [
            "Gaurav Sahu",
            "Olga Vechtomova",
            "Issam H. Laradji"
        ],
        "published": "2023-11-16T04:29:41Z",
        "summary": "Semi-supervised learning (SSL) is a widely used technique in scenarios where\nlabeled data is scarce and unlabeled data is abundant. While SSL is popular for\nimage and text classification, it is relatively underexplored for the task of\nextractive text summarization. Standard SSL methods follow a teacher-student\nparadigm to first train a classification model and then use the classifier's\nconfidence values to select pseudo-labels for the subsequent training cycle;\nhowever, such classifiers are not suitable to measure the accuracy of\npseudo-labels as they lack specific tuning for evaluation, which leads to\nconfidence values that fail to capture the semantics and correctness of the\ngenerated summary. To address this problem, we propose a prompt-based\npseudo-labeling strategy with LLMs that picks unlabeled examples with more\naccurate pseudo-labels than using just the classifier's probability outputs.\nOur approach also includes a relabeling mechanism that improves the quality of\npseudo-labels. We evaluate our method on three text summarization datasets:\nTweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that a\nprompting-based LLM that scores and generates pseudo-labels outperforms\nexisting SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all the\ndatasets. Furthermore, our method achieves competitive G-Eval scores\n(evaluation with GPT-4) as a fully supervised method that uses 100% of the\nlabeled data with only 16.67% of the labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2311.09559v2.pdf"
    },
    {
        "title": "Leveraging Code to Improve In-context Learning for Semantic Parsing",
        "authors": [
            "Ben Bogin",
            "Shivanshu Gupta",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023-11-16T02:50:06Z",
        "summary": "In-context learning (ICL) is an appealing approach for semantic parsing due\nto its few-shot nature and improved generalization. However, learning to parse\nto rare domain-specific languages (DSLs) from just a few demonstrations is\nchallenging, limiting the performance of even the most capable LLMs. In this\nwork, we improve the effectiveness of ICL for semantic parsing by (1) using\ngeneral-purpose programming languages such as Python instead of DSLs, and (2)\naugmenting prompts with a structured domain description that includes, e.g.,\nthe available classes and functions. We show that both these changes\nsignificantly improve accuracy across three popular datasets. Combined, they\nlead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional\nsplit), nearly closing the performance gap between easier i.i.d.\\ and harder\ncompositional splits when used with a strong model, and reducing the need for a\nlarge number of demonstrations. We find that the resemblance of the target\nparse language to general-purpose code is a more important factor than the\nlanguage's popularity in pre-training corpora. Our findings provide an improved\nmethodology for building semantic parsers in the modern context of ICL with\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.09519v2.pdf"
    },
    {
        "title": "One Size Does Not Fit All: Customizing Open-Domain Procedures",
        "authors": [
            "Yash Kumar Lal",
            "Li Zhang",
            "Faeze Brahman",
            "Bodhisattwa Prasad Majumder",
            "Peter Clark",
            "Niket Tandon"
        ],
        "published": "2023-11-16T02:25:36Z",
        "summary": "How-to procedures, such as how to plant a garden, are now used by millions of\nusers, but sometimes need customizing to meet a user's specific needs, e.g.,\nplanting a garden without pesticides. Our goal is to measure and improve an\nLLM's ability to perform such customization. Our approach is to test several\nsimple multi-LLM-agent architectures for customization, as well as an\nend-to-end LLM, using a new evaluation set, called CustomPlans, of over 200\nWikiHow procedures each with a customization need. We find that a simple\narchitecture with two LLM agents used sequentially performs best, one that\nedits a generic how-to procedure and one that verifies its executability,\nsignificantly outperforming (10.5% absolute) an end-to-end prompted LLM. This\nsuggests that LLMs can be configured reasonably effectively for procedure\ncustomization. This also suggests that multi-agent editing architectures may be\nworth exploring further for other customization applications (e.g. coding,\ncreative writing) in the future.",
        "pdf_link": "https://arxiv.org/pdf/2311.09510v2.pdf"
    },
    {
        "title": "Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections",
        "authors": [
            "Yuanpu Cao",
            "Bochuan Cao",
            "Jinghui Chen"
        ],
        "published": "2023-11-15T23:52:05Z",
        "summary": "Recent developments in Large Language Models (LLMs) have manifested\nsignificant advancements. To facilitate safeguards against malicious\nexploitation, a body of research has concentrated on aligning LLMs with human\npreferences and inhibiting their generation of inappropriate content.\nUnfortunately, such alignments are often vulnerable: fine-tuning with a minimal\namount of harmful data can easily unalign the target LLM. While being\neffective, such fine-tuning-based unalignment approaches also have their own\nlimitations: (1) non-stealthiness, after fine-tuning, safety audits or\nred-teaming can easily expose the potential weaknesses of the unaligned models,\nthereby precluding their release/use. (2) non-persistence, the unaligned LLMs\ncan be easily repaired through re-alignment, i.e., fine-tuning again with\naligned data points. In this work, we show that it is possible to conduct\nstealthy and persistent unalignment on large language models via backdoor\ninjections. We also provide a novel understanding on the relationship between\nthe backdoor persistence and the activation pattern and further provide\nguidelines for potential trigger design. Through extensive experiments, we\ndemonstrate that our proposed stealthy and persistent unalignment can\nsuccessfully pass the safety evaluation while maintaining strong persistence\nagainst re-alignment defense.",
        "pdf_link": "https://arxiv.org/pdf/2312.00027v1.pdf"
    },
    {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "authors": [
            "Fuxiao Liu",
            "Xiaoyang Wang",
            "Wenlin Yao",
            "Jianshu Chen",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Yaser Yacoob",
            "Dong Yu"
        ],
        "published": "2023-11-15T23:36:42Z",
        "summary": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (MMC-Instruction) dataset comprising\n600k instances supporting diverse tasks and chart types. Leveraging this data,\nwe develop MultiModal Chart Assistant (MMCA), an LMM that achieves\nstate-of-the-art performance on existing chart QA benchmarks. Recognizing the\nneed for a comprehensive evaluation of LMM chart understanding, we also propose\na MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated\nbenchmark with 9 distinct tasks evaluating reasoning capabilities over charts.\nExtensive experiments on MMC-Benchmark reveal the limitations of existing LMMs\non correctly interpreting charts, even for the most recent GPT-4V model. Our\nwork provides an instruction-tuning methodology and benchmark to advance\nmultimodal understanding of charts.",
        "pdf_link": "https://arxiv.org/pdf/2311.10774v1.pdf"
    },
    {
        "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
        "authors": [
            "Lingbo Mo",
            "Boshi Wang",
            "Muhao Chen",
            "Huan Sun"
        ],
        "published": "2023-11-15T23:33:07Z",
        "summary": "The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an adversarial\nassessment of open-source LLMs on trustworthiness, scrutinizing them across\neight different aspects including toxicity, stereotypes, ethics, hallucination,\nfairness, sycophancy, privacy, and robustness against adversarial\ndemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)\nprompting strategy by incorporating carefully crafted malicious demonstrations\nfor trustworthiness attack. Our extensive experiments encompass recent and\nrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,\nMistral, and Llama 2. The empirical outcomes underscore the efficacy of our\nattack strategy across diverse aspects. More interestingly, our result analysis\nreveals that models with superior performance in general NLP tasks do not\nalways have greater trustworthiness; in fact, larger models can be more\nvulnerable to attacks. Additionally, models that have undergone instruction\ntuning, focusing on instruction following, tend to be more susceptible,\nalthough fine-tuning LLMs for safety alignment proves effective in mitigating\nadversarial trustworthiness attacks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09447v2.pdf"
    },
    {
        "title": "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment",
        "authors": [
            "Haoran Wang",
            "Kai Shu"
        ],
        "published": "2023-11-15T23:07:40Z",
        "summary": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications.\nInspired by recent success in modifying model behavior through steering vectors\nwithout the need for optimization, and drawing on its effectiveness in\nred-teaming LLMs, we conducted experiments employing activation steering to\ntarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness\n- across a varied set of attack settings. To establish a universal attack\nstrategy applicable to diverse target alignments without depending on manual\nanalysis, we automatically select the intervention layer based on contrastive\nlayer search. Our experiment results show that activation attacks are highly\neffective and add little or no overhead to attack efficiency. Additionally, we\ndiscuss potential countermeasures against such activation attacks. Our code and\ndata are available at https://github.com/wang2226/Backdoor-Activation-Attack\nWarning: this paper contains content that can be offensive or upsetting.",
        "pdf_link": "https://arxiv.org/pdf/2311.09433v2.pdf"
    },
    {
        "title": "When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour",
        "authors": [
            "Leonardo Ranaldi",
            "Giulia Pucci"
        ],
        "published": "2023-11-15T22:18:33Z",
        "summary": "Large Language Models (LLMs) have been demonstrating the ability to solve\ncomplex tasks by delivering answers that are positively evaluated by humans due\nin part to the intensive use of human feedback that refines responses. However,\nthe suggestibility transmitted through human feedback increases the inclination\nto produce responses that correspond to the user's beliefs or misleading\nprompts as opposed to true facts, a behaviour known as sycophancy. This\nphenomenon decreases the bias, robustness, and, consequently, their\nreliability.\n  In this paper, we shed light on the suggestibility of LLMs to sycophantic\nbehaviour, demonstrating these tendencies via human-influenced prompts over\ndifferent tasks. Our investigation reveals that LLMs show sycophantic\ntendencies when responding to queries involving subjective opinions and\nstatements that should elicit a contrary response based on facts, demonstrating\na lack of robustness.",
        "pdf_link": "https://arxiv.org/pdf/2311.09410v1.pdf"
    },
    {
        "title": "LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction",
        "authors": [
            "Jamie McCusker"
        ],
        "published": "2023-11-15T20:57:44Z",
        "summary": "While the potential of Open Information Extraction (Open IE) for Knowledge\nGraph Construction (KGC) may seem promising, we find that the alignment of Open\nIE extraction results with existing knowledge graphs to be inadequate. The\nadvent of Large Language Models (LLMs), especially the commercially available\nOpenAI models, have reset expectations for what is possible with deep learning\nmodels and have created a new field called prompt engineering. We investigate\nthe use of GPT models and prompt engineering for knowledge graph construction\nwith the Wikidata knowledge graph to address a similar problem to Open IE,\nwhich we call Open Knowledge Extraction (OKE) using an approach we call the\nLinked Open Knowledge Extractor (LOKE, pronounced like \"Loki\"). We consider the\nentity linking task essential to construction of real world knowledge graphs.\nWe merge the CaRB benchmark scoring approach with data from the TekGen dataset\nfor the LOKE task. We then show that a well engineered prompt, paired with a\nnaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's\nOpenIE 4 implementation on the OKE task, although it over-generates triples\ncompared to the reference set due to overall triple scarcity in the TekGen set.\nThrough an analysis of entity linkability in the CaRB dataset, as well as\noutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the \"silver\"\nTekGen triples show that the task is significantly different in content from\nOIE, if not structure. Through this analysis and a qualitative analysis of\nsentence extractions via all methods, we found that LOKE-GPT extractions are of\nhigh utility for the KGC task and suitable for use in semi-automated extraction\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2311.09366v1.pdf"
    },
    {
        "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
        "authors": [
            "George Chrysostomou",
            "Zhixue Zhao",
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2023-11-15T19:49:24Z",
        "summary": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations from pruned LLMs are less prevalent\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.",
        "pdf_link": "https://arxiv.org/pdf/2311.09335v2.pdf"
    },
    {
        "title": "Improving fit to human reading times via temperature-scaled surprisal",
        "authors": [
            "Tong Liu",
            "Iza ≈†krjanec",
            "Vera Demberg"
        ],
        "published": "2023-11-15T19:34:06Z",
        "summary": "Past studies have provided broad support for that words with lower\npredictability (i.e., higher surprisal) require more time for comprehension by\nusing large language models (LLMs) to simulate humans' cognitive load. In\ngeneral, these studies have implicitly assumed that the probability scores from\nLLMs are accurate, ignoring the discrepancies between human cognition and LLMs\nfrom this standpoint. Inspired by the concept of probability calibration, we\nare the first work to focus on the probability distribution for human reading\nsimulation. We propose to use temperature-scaled surprisal, a surprisal\ncalculated by shaped probability, to be the predictor of human reading times.\nOur results across three corpora consistently revealed that such a surprisal\ncan drastically improve the prediction of reading times. Setting the\ntemperature to be approximately 2.5 across all models and datasets can yield up\nto an 89% of increase in delta log-likelihood in our setting. We also propose a\ncalibration metric to quantify the possible human-likeness bias. Further\nanalysis was done and provided insights into this phenomenon.",
        "pdf_link": "https://arxiv.org/pdf/2311.09325v1.pdf"
    },
    {
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
        "authors": [
            "Fangzhi Xu",
            "Zhiyong Wu",
            "Qiushi Sun",
            "Siyu Ren",
            "Fei Yuan",
            "Shuai Yuan",
            "Qika Lin",
            "Yu Qiao",
            "Jun Liu"
        ],
        "published": "2023-11-15T18:59:56Z",
        "summary": "Although Large Language Models (LLMs) demonstrate remarkable ability in\nprocessing and generating human-like text, they do have limitations when it\ncomes to comprehending and expressing world knowledge that extends beyond the\nboundaries of natural language(e.g., chemical molecular formula). Injecting a\ncollection of symbolic data directly into the training of LLMs can be\nproblematic, as it disregards the synergies among different symbolic families\nand overlooks the need for a balanced mixture of natural and symbolic data. In\nthis work, we tackle these challenges from both a data and framework\nperspective and introduce Symbol-LLM series models. First, we curated a data\ncollection consisting of 34 tasks and incorporating approximately 20 distinct\nsymbolic families, intending to capture the interrelations and foster synergies\nbetween symbols. Then, a two-stage tuning framework succeeds in injecting\nsymbolic knowledge without loss of the generality ability. Extensive\nexperiments on both symbol- and NL-centric tasks demonstrate the balanced and\nsuperior performances of Symbol-LLM series models. The project page is\nhttps://xufangzhi.github.io/symbol-llm-page/.",
        "pdf_link": "https://arxiv.org/pdf/2311.09278v2.pdf"
    },
    {
        "title": "Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models",
        "authors": [
            "Weize Liu",
            "Guocong Li",
            "Kai Zhang",
            "Bang Du",
            "Qiyuan Chen",
            "Xuming Hu",
            "Hongxia Xu",
            "Jintai Chen",
            "Jian Wu"
        ],
        "published": "2023-11-15T18:56:23Z",
        "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing. However, the massive scale and computational demands of\nthese models present formidable challenges when considering their practical\ndeployment in resource-constrained environments. While techniques such as\nchain-of-thought (CoT) distillation have displayed promise in distilling LLMs\ninto small language models (SLMs), there is a risk that distilled SLMs may\nstill inherit flawed reasoning and hallucinations from LLMs. To address these\nissues, we propose a twofold methodology: First, we introduce a novel method\nfor distilling the self-evaluation capability from LLMs into SLMs, aiming to\nmitigate the adverse effects of flawed reasoning and hallucinations inherited\nfrom LLMs. Second, we advocate for distilling more comprehensive thinking by\nincorporating multiple distinct CoTs and self-evaluation outputs, to ensure a\nmore thorough and robust knowledge transfer into SLMs. Experiments on three NLP\nbenchmarks demonstrate that our method significantly improves the performance\nof distilled SLMs, offering a new perspective for developing more effective and\nefficient SLMs in resource-constrained environments.",
        "pdf_link": "https://arxiv.org/pdf/2311.09214v3.pdf"
    },
    {
        "title": "Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering",
        "authors": [
            "Junqing He",
            "Kunhao Pan",
            "Xiaoqun Dong",
            "Zhuoyang Song",
            "Yibo Liu",
            "Yuxin Liang",
            "Hao Wang",
            "Qianguo Sun",
            "Songxin Zhang",
            "Zejian Xie",
            "Jiaxing Zhang"
        ],
        "published": "2023-11-15T18:42:44Z",
        "summary": "While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.",
        "pdf_link": "https://arxiv.org/pdf/2311.09198v1.pdf"
    },
    {
        "title": "Towards Verifiable Text Generation with Symbolic References",
        "authors": [
            "Lucas Torroba Hennigen",
            "Shannon Shen",
            "Aniruddha Nrusimha",
            "Bernhard Gapp",
            "David Sontag",
            "Yoon Kim"
        ],
        "published": "2023-11-15T18:28:29Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2311.09188v1.pdf"
    },
    {
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
        "authors": [
            "Yixin Liu",
            "Alexander R. Fabbri",
            "Jiawen Chen",
            "Yilun Zhao",
            "Simeng Han",
            "Shafiq Joty",
            "Pengfei Liu",
            "Dragomir Radev",
            "Chien-Sheng Wu",
            "Arman Cohan"
        ],
        "published": "2023-11-15T18:25:26Z",
        "summary": "While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.",
        "pdf_link": "https://arxiv.org/pdf/2311.09184v1.pdf"
    },
    {
        "title": "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models",
        "authors": [
            "Jierui Li",
            "Vipul Raheja",
            "Dhruv Kumar"
        ],
        "published": "2023-11-15T18:23:17Z",
        "summary": "In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments.",
        "pdf_link": "https://arxiv.org/pdf/2311.09182v1.pdf"
    },
    {
        "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph",
        "authors": [
            "Zhaowei Wang",
            "Haochen Shi",
            "Weiqi Wang",
            "Tianqing Fang",
            "Hongming Zhang",
            "Sehyun Choi",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023-11-15T18:11:23Z",
        "summary": "Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09174v3.pdf"
    },
    {
        "title": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models",
        "authors": [
            "Wenhong Zhu",
            "Hongkun Hao",
            "Zhiwei He",
            "Yunze Song",
            "Yumeng Zhang",
            "Hanxu Hu",
            "Yiran Wei",
            "Rui Wang",
            "Hongyuan Lu"
        ],
        "published": "2023-11-15T17:50:30Z",
        "summary": "We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2311.09154v2.pdf"
    },
    {
        "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
        "authors": [
            "Ziyang Chen",
            "Dongfang Li",
            "Xiang Zhao",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2023-11-15T17:46:39Z",
        "summary": "In this paper, we tackle the significant challenge of temporal knowledge\nreasoning in Large Language Models (LLMs), an area where such models frequently\nencounter difficulties. These difficulties often result in the generation of\nmisleading or incorrect information, primarily due to their limited capacity to\nprocess evolving factual knowledge and complex temporal logic. In response, we\npropose a novel, constructivism-based approach that advocates for a paradigm\nshift in LLM learning towards an active, ongoing process of knowledge synthesis\nand customization. At the heart of our proposal is the Abstract Reasoning\nInduction ARI framework, which divides temporal reasoning into two distinct\nphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\ninstances of hallucinations and improve LLMs' capacity for integrating abstract\nmethodologies derived from historical data. Our approach achieves remarkable\nimprovements, with relative gains of 29.7\\% and 9.27\\% on two temporal QA\ndatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\nThe code will be released at https://github.com/czy1999/ARI.",
        "pdf_link": "https://arxiv.org/pdf/2311.09149v1.pdf"
    },
    {
        "title": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
        "authors": [
            "Yuanwei Wu",
            "Xiang Li",
            "Yixin Liu",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "published": "2023-11-15T17:17:39Z",
        "summary": "Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities, especially in model API. To fill the research gap, we carry\nout the following work: 1) We discover a system prompt leakage vulnerability in\nGPT-4V. Through carefully designed dialogue, we successfully extract the\ninternal system prompts of GPT-4V. This finding indicates potential exploitable\nsecurity risks in MLLMs; 2) Based on the acquired system prompts, we propose a\nnovel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via\nSystem Prompt). By employing GPT-4 as a red teaming tool against itself, we aim\nto search for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking. This finding could be leveraged to\ngreatly facilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.",
        "pdf_link": "https://arxiv.org/pdf/2311.09127v2.pdf"
    },
    {
        "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
        "authors": [
            "Haoqiang Kang",
            "Juntong Ni",
            "Huaxiu Yao"
        ],
        "published": "2023-11-15T17:04:56Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2311.09114v2.pdf"
    },
    {
        "title": "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?",
        "authors": [
            "Yusuke Sakai",
            "Hidetaka Kamigaito",
            "Katsuhiko Hayashi",
            "Taro Watanabe"
        ],
        "published": "2023-11-15T16:56:49Z",
        "summary": "Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.",
        "pdf_link": "https://arxiv.org/pdf/2311.09109v1.pdf"
    },
    {
        "title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
        "authors": [
            "Xiaozhi Wang",
            "Hao Peng",
            "Yong Guan",
            "Kaisheng Zeng",
            "Jianhui Chen",
            "Lei Hou",
            "Xu Han",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Ruobing Xie",
            "Jie Zhou",
            "Juanzi Li"
        ],
        "published": "2023-11-15T16:52:14Z",
        "summary": "Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and our code can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.",
        "pdf_link": "https://arxiv.org/pdf/2311.09105v1.pdf"
    },
    {
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
        "authors": [
            "Zhexin Zhang",
            "Junxiao Yang",
            "Pei Ke",
            "Minlie Huang"
        ],
        "published": "2023-11-15T16:42:29Z",
        "summary": "Large Language Models (LLMs) continue to advance in their capabilities, yet\nthis progress is accompanied by a growing array of safety risks. While\nsignificant attention has been dedicated to exploiting weaknesses in LLMs\nthrough jailbreaking attacks, there remains a paucity of exploration into\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the inherent conflict between the goals of being\nhelpful and ensuring safety. To counter jailbreaking attacks, we propose to\nintegrate goal prioritization at both training and inference stages.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to\n2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising\ngeneral performance. Furthermore, integrating the concept of goal\nprioritization into the training phase reduces the ASR from 71.0% to 6.6% for\nLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are\nincluded during training, our approach slashes the ASR by half, decreasing it\nfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs\nface greater safety risks, they also possess a greater capacity to be steered\ntowards defending against such attacks. We hope our work could contribute to\nthe comprehension of jailbreaking attacks and defenses, and shed light on the\nrelationship between LLMs' capability and safety. Our code will be available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.",
        "pdf_link": "https://arxiv.org/pdf/2311.09096v1.pdf"
    },
    {
        "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
        "authors": [
            "Marta Marchiori Manerba",
            "Karolina Sta≈Ñczak",
            "Riccardo Guidotti",
            "Isabelle Augenstein"
        ],
        "published": "2023-11-15T16:35:59Z",
        "summary": "Large language models have been shown to encode a variety of social biases,\nwhich carries the risk of downstream harms. While the impact of these biases\nhas been recognized, prior methods for bias evaluation have been limited to\nbinary association tests on small datasets, offering a constrained view of the\nnature of societal biases within language models. In this paper, we propose an\noriginal framework for probing language models for societal biases. We collect\na probing dataset to analyze language models' general associations, as well as\nalong the axes of societal categories, identities, and stereotypes. To this\nend, we leverage a novel perplexity-based fairness score. We curate a\nlarge-scale benchmarking dataset addressing drawbacks and limitations of\nexisting fairness collections, expanding to a variety of different identities\nand stereotypes. When comparing our methodology with prior work, we demonstrate\nthat biases within language models are more nuanced than previously\nacknowledged. In agreement with recent findings, we find that larger model\nvariants exhibit a higher degree of bias. Moreover, we expose how identities\nexpressing different religions lead to the most pronounced disparate treatments\nacross all models.",
        "pdf_link": "https://arxiv.org/pdf/2311.09090v2.pdf"
    },
    {
        "title": "How Multilingual is Multilingual LLM?",
        "authors": [
            "Fei Yuan",
            "Shuai Yuan",
            "Zhiyong Wu",
            "Lei Li"
        ],
        "published": "2023-11-15T16:13:14Z",
        "summary": "Large Language Models (LLMs), trained predominantly on extensive English\ndata, often exhibit limitations when applied to other languages. Current\nresearch is primarily focused on enhancing the multilingual capabilities of\nthese models by employing various tuning strategies. Despite their\neffectiveness in certain languages, the understanding of the multilingual\nabilities of LLMs remains incomplete. This study endeavors to evaluate the\nmultilingual capacity of LLMs by conducting an exhaustive analysis across 101\nlanguages, and classifies languages with similar characteristics into four\ndistinct quadrants. By delving into each quadrant, we shed light on the\nrationale behind their categorization and offer actionable guidelines for\ntuning these languages. Extensive experiments reveal that existing LLMs possess\nmultilingual capabilities that surpass our expectations, and we can\nsignificantly improve the multilingual performance of LLMs by focusing on these\ndistinct attributes present in each quadrant.",
        "pdf_link": "https://arxiv.org/pdf/2311.09071v1.pdf"
    },
    {
        "title": "How Well Do Large Language Models Truly Ground?",
        "authors": [
            "Hyunji Lee",
            "Sejune Joo",
            "Chaeeun Kim",
            "Joel Jang",
            "Doyoung Kim",
            "Kyoung-Woon On",
            "Minjoon Seo"
        ],
        "published": "2023-11-15T16:11:27Z",
        "summary": "Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\nissues such as hallucinations, lack of control, and difficulties in integrating\nvariable knowledge. To mitigate this, LLMs can be probed to generate responses\nby grounding on external context, often given as input (knowledge-augmented\nmodels). Yet, previous research is often confined to a narrow view of the term\n\"grounding\", often only focusing on whether the response contains the correct\nanswer or not, which does not ensure the reliability of the entire response. To\naddress this limitation, we introduce a strict definition of grounding: a model\nis considered truly grounded when its responses (1) fully utilize necessary\nknowledge from the provided context, and (2) don't exceed the knowledge within\nthe contexts. We introduce a new dataset and a grounding metric to assess this\nnew definition and perform experiments across 13 LLMs of different sizes and\ntraining methods to provide insights into the factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.09069v1.pdf"
    },
    {
        "title": "GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models",
        "authors": [
            "Serwan Jassim",
            "Mario Holubar",
            "Annika Richter",
            "Cornelius Wolff",
            "Xenia Ohmer",
            "Elia Bruni"
        ],
        "published": "2023-11-15T15:38:28Z",
        "summary": "This paper presents GRASP, a novel benchmark to evaluate the language\ngrounding and physical understanding capabilities of video-based multimodal\nlarge language models (LLMs). This evaluation is accomplished via a two-tier\napproach leveraging Unity simulations. The first level tests for language\ngrounding by assessing a model's ability to relate simple textual descriptions\nwith visual information. The second level evaluates the model's understanding\nof \"Intuitive Physics\" principles, such as object permanence and continuity. In\naddition to releasing the benchmark, we use it to evaluate several\nstate-of-the-art multimodal LLMs. Our evaluation reveals significant\nshortcomings in the language grounding and intuitive physics capabilities of\nthese models. Although they exhibit at least some grounding capabilities,\nparticularly for colors and shapes, these capabilities depend heavily on the\nprompting strategy. At the same time, all models perform below or at the chance\nlevel of 50% in the Intuitive Physics tests, while human subjects are on\naverage 80% correct. These identified limitations underline the importance of\nusing benchmarks like GRASP to monitor the progress of future models in\ndeveloping these competencies.",
        "pdf_link": "https://arxiv.org/pdf/2311.09048v2.pdf"
    },
    {
        "title": "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
        "authors": [
            "Yuxia Wang",
            "Revanth Gangi Reddy",
            "Zain Muhammad Mujahid",
            "Arnav Arora",
            "Aleksandr Rubashevskii",
            "Jiahui Geng",
            "Osama Mohammed Afzal",
            "Liangming Pan",
            "Nadav Borenstein",
            "Aditya Pillai",
            "Isabelle Augenstein",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2023-11-15T14:41:57Z",
        "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We design and\nbuild an annotation tool to speed up the labelling procedure and ease the\nworkload of raters. It allows flexible incorporation of automatic results in\nany stage, e.g. automatically-retrieved evidence. We further construct an\nopen-domain document-level factuality benchmark in three-level granularity:\nclaim, sentence and document. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims with the\nbest F1=0.53. Annotation tool, benchmark and code are available at\nhttps://github.com/yuxiaw/Factcheck-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2311.09000v2.pdf"
    },
    {
        "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks",
        "authors": [
            "Hao Peng",
            "Xiaozhi Wang",
            "Jianhui Chen",
            "Weikai Li",
            "Yunjia Qi",
            "Zimu Wang",
            "Zhili Wu",
            "Kaisheng Zeng",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023-11-15T14:26:30Z",
        "summary": "In-context learning (ICL) has become the default method for using large\nlanguage models (LLMs), making the exploration of its limitations and\nunderstanding the underlying causes crucial. In this paper, we find that ICL\nfalls short of handling specification-heavy tasks, which are tasks with\ncomplicated and extensive task specifications, requiring several hours for\nordinary humans to master, such as traditional information extraction tasks.\nThe performance of ICL on these tasks mostly cannot reach half of the\nstate-of-the-art results. To explore the reasons behind this failure, we\nconduct comprehensive experiments on 18 specification-heavy tasks with various\nLLMs and identify three primary reasons: inability to specifically understand\ncontext, misalignment in task schema comprehension with humans, and inadequate\nlong-text understanding ability. Furthermore, we demonstrate that through\nfine-tuning, LLMs can achieve decent performance on these tasks, indicating\nthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\nof existing alignment methods that renders LLMs incapable of handling\ncomplicated specification-heavy tasks via ICL. To substantiate this, we perform\ndedicated instruction tuning on LLMs for these tasks and observe a notable\nimprovement. We hope the analyses in this paper could facilitate advancements\nin alignment methods enabling LLMs to meet more sophisticated human demands.",
        "pdf_link": "https://arxiv.org/pdf/2311.08993v1.pdf"
    },
    {
        "title": "Enabling Large Language Models to Learn from Rules",
        "authors": [
            "Wenkai Yang",
            "Yankai Lin",
            "Jie Zhou",
            "Jirong Wen"
        ],
        "published": "2023-11-15T11:42:41Z",
        "summary": "Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current knowledge learning paradigm of LLMs is\nmainly based on learning from examples, in which LLMs learn the internal rule\nimplicitly from a certain number of supervised examples. However, this learning\nparadigm may not well learn those complicated rules, especially when the\ntraining examples are limited. We are inspired that humans can learn the new\ntasks or knowledge in another way by learning from rules. That is, humans can\nlearn new tasks or grasps new knowledge quickly and generalize well given only\na detailed rule and a few optional examples. Therefore, in this paper, we aim\nto explore the feasibility of this new learning paradigm, which targets on\nencoding rule-based knowledge into LLMs. We further propose rule distillation,\nwhich first uses the strong in-context abilities of LLMs to extract the\nknowledge from the textual rules, and then explicitly encode the knowledge into\nthe parameters of LLMs by learning from the above in-context signals produced\ninside the model. Our experiments show that making LLMs learn from rules by our\nmethod is much more efficient than example-based learning in both the sample\nsize and generalization ability. Warning: This paper may contain examples with\noffensive content.",
        "pdf_link": "https://arxiv.org/pdf/2311.08883v2.pdf"
    },
    {
        "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
        "authors": [
            "Vaishnavi Shrivastava",
            "Percy Liang",
            "Ananya Kumar"
        ],
        "published": "2023-11-15T11:27:44Z",
        "summary": "To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).",
        "pdf_link": "https://arxiv.org/pdf/2311.08877v1.pdf"
    },
    {
        "title": "Disinformation Capabilities of Large Language Models",
        "authors": [
            "Ivan Vykopal",
            "Mat√∫≈° Pikuliak",
            "Ivan Srba",
            "Robert Moro",
            "Dominik Macko",
            "Maria Bielikova"
        ],
        "published": "2023-11-15T10:25:30Z",
        "summary": "Automated disinformation generation is often listed as an important risk\nassociated with large language models (LLMs). The theoretical ability to flood\nthe information space with disinformation content might have dramatic\nconsequences for societies around the world. This paper presents a\ncomprehensive study of the disinformation capabilities of the current\ngeneration of LLMs to generate false news articles in the English language. In\nour study, we evaluated the capabilities of 10 LLMs using 20 disinformation\nnarratives. We evaluated several aspects of the LLMs: how good they are at\ngenerating news articles, how strongly they tend to agree or disagree with the\ndisinformation narratives, how often they generate safety warnings, etc. We\nalso evaluated the abilities of detection models to detect these articles as\nLLM-generated. We conclude that LLMs are able to generate convincing news\narticles that agree with dangerous disinformation narratives.",
        "pdf_link": "https://arxiv.org/pdf/2311.08838v2.pdf"
    },
    {
        "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
        "authors": [
            "Davis Yoshida",
            "Kartik Goyal",
            "Kevin Gimpel"
        ],
        "published": "2023-11-15T09:38:53Z",
        "summary": "It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has\ngenerally been attributed to either a fundamental inadequacy of modes in models\nor weaknesses in language modeling. Contrastingly in this work, we emphasize\nthat degenerate modes can even occur in the absence of any model error, due to\ncontamination of the training data. Specifically, we show that mixing even a\ntiny amount of low-entropy noise with a population text distribution can cause\nthe data distribution's mode to become degenerate, implying that any models\ntrained on it will be as well. As the unconditional mode of NLG models will\noften be degenerate, we therefore propose to apply MAP decoding to the model's\ndistribution conditional on avoiding specific degeneracies. Using exact-search,\nwe empirically verify that the length-conditional modes of machine translation\nmodels and language models are indeed more fluent and topical than their\nunconditional modes. For the first time, we also share many examples of exact\nmodal sequences from these models, and from several variants of the LLaMA-7B\nmodel. Notably, the modes of the LLaMA models are still degenerate, showing\nthat improvements in modeling have not fixed this issue. Because of the cost of\nexact mode finding algorithms, we develop an approximate mode finding approach,\nACBS, which finds sequences that are both high-likelihood and high-quality. We\napply this approach to LLaMA-7B, a model which was not trained for instruction\nfollowing, and find that we are able to elicit reasonable outputs without any\nfinetuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.08817v1.pdf"
    },
    {
        "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
        "authors": [
            "Chang Gao",
            "Haiyun Jiang",
            "Deng Cai",
            "Shuming Shi",
            "Wai Lam"
        ],
        "published": "2023-11-15T09:18:09Z",
        "summary": "Most existing chain-of-thought (CoT) prompting methods suffer from the issues\nof generalizability and consistency, as they often rely on instance-specific\nsolutions that may not be applicable to other cases and lack task-level\nconsistency in their reasoning steps. To address these limitations, we propose\na comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to\nconstruct generalizable and consistent few-shot prompts for various tasks\nautomatically. To this end, StrategyLLM employs four LLM-based agents: strategy\ngenerator, executor, optimizer, and evaluator, working together to generate,\nevaluate, and select promising strategies for a given task. The experimental\nresults demonstrate that StrategyLLM outperforms the competitive baseline\nCoT-SC that requires human-annotated solutions on 13 datasets across 4\nchallenging tasks without human involvement, including math reasoning (34.21%\n$\\rightarrow$ 38.79%), commonsense reasoning (70.3% $\\rightarrow$ 72.5%),\nalgorithmic reasoning (51.7% $\\rightarrow$ 62.0%), and symbolic reasoning\n(30.0% $\\rightarrow$ 79.2%).",
        "pdf_link": "https://arxiv.org/pdf/2311.08803v2.pdf"
    },
    {
        "title": "Auto-ICL: In-Context Learning without Human Supervision",
        "authors": [
            "Jinghan Yang",
            "Shuming Ma",
            "Furu Wei"
        ],
        "published": "2023-11-15T07:37:28Z",
        "summary": "In the era of Large Language Models (LLMs), human-computer interaction has\nevolved towards natural language, offering unprecedented flexibility. Despite\nthis, LLMs are heavily reliant on well-structured prompts to function\nefficiently within the realm of In-Context Learning. Vanilla In-Context\nLearning relies on human-provided contexts, such as labeled examples, explicit\ninstructions, or other guiding mechanisms that shape the model's outputs. To\naddress this challenge, our study presents a universal framework named\nAutomatic In-Context Learning. Upon receiving a user's request, we ask the\nmodel to independently generate examples, including labels, instructions, or\nreasoning pathways. The model then leverages this self-produced context to\ntackle the given problem. Our approach is universally adaptable and can be\nimplemented in any setting where vanilla In-Context Learning is applicable. We\ndemonstrate that our method yields strong performance across a range of tasks,\nstanding up well when compared to existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.09263v1.pdf"
    },
    {
        "title": "Thread of Thought Unraveling Chaotic Contexts",
        "authors": [
            "Yucheng Zhou",
            "Xiubo Geng",
            "Tao Shen",
            "Chongyang Tao",
            "Guodong Long",
            "Jian-Guang Lou",
            "Jianbing Shen"
        ],
        "published": "2023-11-15T06:54:44Z",
        "summary": "Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.",
        "pdf_link": "https://arxiv.org/pdf/2311.08734v1.pdf"
    },
    {
        "title": "Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models",
        "authors": [
            "Minze Chen",
            "Zhenxiang Tao",
            "Weitong Tang",
            "Tingxin Qin",
            "Rui Yang",
            "Chunli Zhu"
        ],
        "published": "2023-11-15T06:48:50Z",
        "summary": "Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.",
        "pdf_link": "https://arxiv.org/pdf/2311.08732v1.pdf"
    },
    {
        "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
        "authors": [
            "Bairu Hou",
            "Yujian Liu",
            "Kaizhi Qian",
            "Jacob Andreas",
            "Shiyu Chang",
            "Yang Zhang"
        ],
        "published": "2023-11-15T05:58:35Z",
        "summary": "Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a model into data (aleatoric) uncertainty, resulting from the\ninherent complexity or ambiguity of the data, and model (epistemic)\nuncertainty, resulting from the lack of knowledge in the model. Performing\nuncertainty decomposition for large language models (LLMs) is an important step\ntoward improving the reliability, trustworthiness, and interpretability of\nLLMs, but this research task is very challenging and remains unresolved. The\nexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\nLLMs, because BNN requires training and ensembling multiple variants of models,\nwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\nintroduce an uncertainty decomposition framework for LLMs, called input\nclarifications ensemble, which bypasses the need to train new models. Rather\nthan ensembling models with different parameters, our approach generates a set\nof clarifications for the input, feeds them into the fixed LLMs, and ensembles\nthe corresponding predictions. We show that our framework shares a symmetric\ndecomposition structure with BNN. Empirical evaluations demonstrate that the\nproposed framework provides accurate and reliable uncertainty quantification on\nvarious tasks. Code will be made publicly available at\nhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .",
        "pdf_link": "https://arxiv.org/pdf/2311.08718v1.pdf"
    },
    {
        "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning",
        "authors": [
            "Zhihan Zhang",
            "Dong-Ho Lee",
            "Yuwei Fang",
            "Wenhao Yu",
            "Mengzhao Jia",
            "Meng Jiang",
            "Francesco Barbieri"
        ],
        "published": "2023-11-15T05:28:07Z",
        "summary": "Instruction tuning has remarkably advanced large language models (LLMs) in\nunderstanding and responding to diverse human instructions. Despite the success\nin high-resource languages, its application in lower-resource ones faces\nchallenges due to the imbalanced foundational abilities of LLMs across\ndifferent languages, stemming from the uneven language distribution in their\npre-training data. To tackle this issue, we propose pivot language guided\ngeneration (PLUG), an approach that utilizes a high-resource language,\nprimarily English, as the pivot to enhance instruction tuning in lower-resource\nlanguages. It trains the model to first process instructions in the pivot\nlanguage, and then produce responses in the target language. To evaluate our\napproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\nlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\nprofessional translators. Our approach demonstrates a significant improvement\nin the instruction-following abilities of LLMs by 29% on average, compared to\ndirectly responding in the target language alone. Further experiments validate\nthe versatility of our approach by employing alternative pivot languages beyond\nEnglish to assist languages where LLMs exhibit lower proficiency. Our code and\ndata are available at https://github.com/ytyz1307zzh/PLUG.",
        "pdf_link": "https://arxiv.org/pdf/2311.08711v2.pdf"
    },
    {
        "title": "Comparing Generalization in Learning with Limited Numbers of Exemplars: Transformer vs. RNN in Attractor Dynamics",
        "authors": [
            "Rui Fukushima",
            "Jun Tani"
        ],
        "published": "2023-11-15T00:37:49Z",
        "summary": "ChatGPT, a widely-recognized large language model (LLM), has recently gained\nsubstantial attention for its performance scaling, attributed to the billions\nof web-sourced natural language sentences used for training. Its underlying\narchitecture, Transformer, has found applications across diverse fields,\nincluding video, audio signals, and robotic movement. %The crucial question\nthis raises concerns the Transformer's generalization-in-learning (GIL)\ncapacity. However, this raises a crucial question about Transformer's\ngeneralization in learning (GIL) capacity. Is ChatGPT's success chiefly due to\nthe vast dataset used for training, or is there more to the story? To\ninvestigate this, we compared Transformer's GIL capabilities with those of a\ntraditional Recurrent Neural Network (RNN) in tasks involving attractor\ndynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)\nmethod has been employed. Our simulation results suggest that under conditions\nof limited data availability, Transformer's GIL abilities are markedly inferior\nto those of RNN.",
        "pdf_link": "https://arxiv.org/pdf/2311.10763v1.pdf"
    },
    {
        "title": "Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures",
        "authors": [
            "David F. Jenny",
            "Yann Billeter",
            "Mrinmaya Sachan",
            "Bernhard Sch√∂lkopf",
            "Zhijing Jin"
        ],
        "published": "2023-11-15T00:02:25Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has sparked intense\ndebate regarding their ability to perceive and interpret complex\nsocio-political landscapes. In this study, we undertake an exploration of\ndecision-making processes and inherent biases within LLMs, exemplified by\nChatGPT, specifically contextualizing our analysis within political debates. We\naim not to critique or validate LLMs' values, but rather to discern how they\ninterpret and adjudicate \"good arguments.\" By applying Activity Dependency\nNetworks (ADNs), we extract the LLMs' implicit criteria for such assessments\nand illustrate how normative values influence these perceptions. We discuss the\nconsequences of our findings for human-AI alignment and bias mitigation. Our\ncode and data at https://github.com/david-jenny/LLM-Political-Study.",
        "pdf_link": "https://arxiv.org/pdf/2311.08605v1.pdf"
    },
    {
        "title": "Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment",
        "authors": [
            "Philippe Laban",
            "Lidiya Murakhovs'ka",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-11-14T23:40:22Z",
        "summary": "The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM completes a\nclassification task. In a second round, the LLM is challenged with a follow-up\nphrase like \"Are you sure?\", offering an opportunity for the model to reflect\non its initial answer, and decide whether to confirm or flip its answer. A\nsystematic study of ten LLMs on seven classification tasks reveals that models\nflip their answers on average 46% of the time and that all models see a\ndeterioration of accuracy between their first and final prediction, with an\naverage drop of 17% (the FlipFlop effect). We conduct finetuning experiments on\nan open-source LLM and find that finetuning on synthetically created data can\nmitigate - reducing performance deterioration by 60% - but not resolve\nsycophantic behavior entirely. The FlipFlop experiment illustrates the\nuniversality of sycophantic behavior in LLMs and provides a robust framework to\nanalyze model behavior and evaluate future models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08596v2.pdf"
    },
    {
        "title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation",
        "authors": [
            "Weixiang Yan",
            "Haitian Liu",
            "Yunkun Wang",
            "Yunzhe Li",
            "Qian Chen",
            "Wen Wang",
            "Tingyu Lin",
            "Weishan Zhao",
            "Li Zhu",
            "Shuiguang Deng",
            "Hari Sundaram"
        ],
        "published": "2023-11-14T23:18:52Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\ncoding related tasks, particularly on assisting humans in programming and\nfacilitating programming automation. However, existing benchmarks for\nevaluating the code understanding and generation capacities of LLMs suffer from\nsevere limitations. First, most benchmarks are deficient as they focus on a\nnarrow range of popular programming languages and specific tasks, whereas the\nreal-world software development scenarios show dire need to implement systems\nwith multilingual programming environments to satisfy diverse requirements.\nPractical programming practices also strongly expect multi-task settings for\ntesting coding capabilities of LLMs comprehensively and robustly. Second, most\nbenchmarks also fail to consider the actual executability and the consistency\nof execution results of the generated code. To bridge these gaps between\nexisting benchmarks and expectations from practical applications, we introduce\nCodeScope, an execution-based, multilingual, multi-task, multi-dimensional\nevaluation benchmark for comprehensively gauging LLM capabilities on coding\ntasks. CodeScope covers 43 programming languages and 8 coding tasks. It\nevaluates the coding performance of LLMs from three dimensions (perspectives):\ndifficulty, efficiency, and length. To facilitate execution-based evaluations\nof code generation, we develop MultiCodeEngine, an automated code execution\nengine that supports 14 programming languages. Finally, we systematically\nevaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the\nsuperior breadth and challenges of CodeScope for evaluating LLMs on code\nunderstanding and generation tasks compared to other benchmarks. The CodeScope\nbenchmark and datasets are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.",
        "pdf_link": "https://arxiv.org/pdf/2311.08588v2.pdf"
    },
    {
        "title": "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer",
        "authors": [
            "Urchade Zaratiana",
            "Nadi Tomeh",
            "Pierre Holat",
            "Thierry Charnois"
        ],
        "published": "2023-11-14T20:39:12Z",
        "summary": "Named Entity Recognition (NER) is essential in various Natural Language\nProcessing (NLP) applications. Traditional NER models are effective but limited\nto a set of predefined entity types. In contrast, Large Language Models (LLMs)\ncan extract arbitrary entities through natural language instructions, offering\ngreater flexibility. However, their size and cost, particularly for those\naccessed via APIs like ChatGPT, make them impractical in resource-limited\nscenarios. In this paper, we introduce a compact NER model trained to identify\nany type of entity. Leveraging a bidirectional transformer encoder, our model,\nGLiNER, facilitates parallel entity extraction, an advantage over the slow\nsequential token generation of LLMs. Through comprehensive testing, GLiNER\ndemonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs\nin zero-shot evaluations on various NER benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2311.08526v1.pdf"
    },
    {
        "title": "LLMs cannot find reasoning errors, but can correct them!",
        "authors": [
            "Gladys Tyen",
            "Hassan Mansoor",
            "Victor CƒÉrbune",
            "Peter Chen",
            "Tony Mak"
        ],
        "published": "2023-11-14T20:12:38Z",
        "summary": "While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we break down the self-correction process into two\ncore components: mistake finding and output correction. For mistake finding, we\nrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\nreasoning traces. We provide benchmark numbers for several state-of-the-art\nLLMs, and demonstrate that LLMs generally struggle with finding logical\nmistakes. For output correction, we propose a backtracking method which\nprovides large improvements when given information on mistake location. We\nconstrue backtracking as a lightweight alternative to reinforcement learning\nmethods, and show that it remains effective with a reward model at 60-70%\naccuracy.",
        "pdf_link": "https://arxiv.org/pdf/2311.08516v2.pdf"
    },
    {
        "title": "Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models",
        "authors": [
            "Carlos Aguirre",
            "Kuleen Sasse",
            "Isabel Cachola",
            "Mark Dredze"
        ],
        "published": "2023-11-14T19:02:03Z",
        "summary": "Recently, work in NLP has shifted to few-shot (in-context) learning, with\nlarge language models (LLMs) performing well across a range of tasks. However,\nwhile fairness evaluations have become a standard for supervised methods,\nlittle is known about the fairness of LLMs as prediction systems. Further,\ncommon standard methods for fairness involve access to models weights or are\napplied during finetuning, which are not applicable in few-shot learning. Do\nLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\nwe explore the effect of shots, which directly affect the performance of\nmodels, on the fairness of LLMs as NLP classification systems. We consider how\ndifferent shot selection strategies, both existing and new demographically\nsensitive methods, affect model fairness across three standard fairness\ndatasets. We discuss how future work can include LLM fairness evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2311.08472v1.pdf"
    },
    {
        "title": "Fine-tuning Language Models for Factuality",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Huaxiu Yao",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-11-14T18:59:15Z",
        "summary": "The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2311.08401v1.pdf"
    },
    {
        "title": "Towards Open-Ended Visual Recognition with Large Language Model",
        "authors": [
            "Qihang Yu",
            "Xiaohui Shen",
            "Liang-Chieh Chen"
        ],
        "published": "2023-11-14T18:59:01Z",
        "summary": "Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code/model are available at\nhttps://github.com/bytedance/OmniScient-Model.",
        "pdf_link": "https://arxiv.org/pdf/2311.08400v1.pdf"
    },
    {
        "title": "Are Large Language Models Temporally Grounded?",
        "authors": [
            "Yifu Qiu",
            "Zheng Zhao",
            "Yftah Ziser",
            "Anna Korhonen",
            "Edoardo M. Ponti",
            "Shay B. Cohen"
        ],
        "published": "2023-11-14T18:57:15Z",
        "summary": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.",
        "pdf_link": "https://arxiv.org/pdf/2311.08398v2.pdf"
    },
    {
        "title": "On What Basis? Predicting Text Preference Via Structured Comparative Reasoning",
        "authors": [
            "Jing Nathan Yan",
            "Tianqi Liu",
            "Justin T Chiu",
            "Jiaming Shen",
            "Zhen Qin",
            "Yue Yu",
            "Yao Zhao",
            "Charu Lakshmanan",
            "Yair Kurzion",
            "Alexander M. Rush",
            "Jialu Liu",
            "Michael Bendersky"
        ],
        "published": "2023-11-14T18:51:38Z",
        "summary": "Comparative reasoning plays a crucial role in text preference prediction;\nhowever, large language models (LLMs) often demonstrate inconsistencies in\ntheir reasoning. While approaches like Chain-of-Thought improve accuracy in\nmany other settings, they struggle to consistently distinguish the similarities\nand differences of complex texts. We introduce SC, a prompting approach that\npredicts text preferences by generating structured intermediate comparisons. SC\nbegins by proposing aspects of comparison, followed by generating textual\ncomparisons under each aspect. We select consistent comparisons with a pairwise\nconsistency comparator that ensures each aspect's comparisons clearly\ndistinguish differences between texts, significantly reducing hallucination and\nimproving consistency. Our comprehensive evaluations across various NLP tasks,\nincluding summarization, retrieval, and automatic rating, demonstrate that SC\nequips LLMs to achieve state-of-the-art performance in text preference\nprediction.",
        "pdf_link": "https://arxiv.org/pdf/2311.08390v1.pdf"
    },
    {
        "title": "SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models",
        "authors": [
            "Bertie Vidgen",
            "Nino Scherrer",
            "Hannah Rose Kirk",
            "Rebecca Qian",
            "Anand Kannappan",
            "Scott A. Hale",
            "Paul R√∂ttger"
        ],
        "published": "2023-11-14T18:33:43Z",
        "summary": "The past year has seen rapid acceleration in the development of large\nlanguage models (LLMs). However, without proper steering and safeguards, LLMs\nwill readily follow malicious instructions, provide unsafe advice, and generate\ntoxic content. We introduce SimpleSafetyTests (SST) as a new test suite for\nrapidly and systematically identifying such critical safety risks. The test\nsuite comprises 100 test prompts across five harm areas that LLMs, for the vast\nmajority of applications, should refuse to comply with. We test 11 open-access\nand open-source LLMs and four closed-source LLMs, and find critical safety\nweaknesses. While some of the models do not give a single unsafe response, most\ngive unsafe responses to more than 20% of the prompts, with over 50% unsafe\nresponses in the extreme. Prepending a safety-emphasising system prompt\nsubstantially reduces the occurrence of unsafe responses, but does not\ncompletely stop them from happening. Trained annotators labelled every model\nresponse to SST (n = 3,000). We use these annotations to evaluate five AI\nsafety filters (which assess whether a models' response is unsafe given a\nprompt) as a way of automatically evaluating models' performance on SST. The\nfilters' performance varies considerably. There are also differences across the\nfive harm areas, and on the unsafe versus safe responses. The widely-used\nPerspective API has 72% accuracy and a newly-created zero-shot prompt to\nOpenAI's GPT-4 performs best with 89% accuracy. Content Warning: This paper\ncontains prompts and responses that relate to child abuse, suicide, self-harm\nand eating disorders, scams and fraud, illegal items, and physical harm.",
        "pdf_link": "https://arxiv.org/pdf/2311.08370v2.pdf"
    },
    {
        "title": "GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment",
        "authors": [
            "Senthujan Senkaiahliyan",
            "Augustin Toma",
            "Jun Ma",
            "An-Wen Chan",
            "Andrew Ha",
            "Kevin R. An",
            "Hrishikesh Suresh",
            "Barry Rubin",
            "Bo Wang"
        ],
        "published": "2023-11-14T17:06:09Z",
        "summary": "OpenAI's large multimodal model, GPT-4V(ision), was recently developed for\ngeneral image interpretation. However, less is known about its capabilities\nwith medical image interpretation and diagnosis. Board-certified physicians and\nsenior residents assessed GPT-4V's proficiency across a range of medical\nconditions using imaging modalities such as CT scans, MRIs, ECGs, and clinical\nphotographs. Although GPT-4V is able to identify and explain medical images,\nits diagnostic accuracy and clinical decision-making abilities are poor, posing\nrisks to patient safety. Despite the potential that large language models may\nhave in enhancing medical education and delivery, the current limitations of\nGPT-4V in interpreting medical images reinforces the importance of appropriate\ncaution when using it for clinical decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2403.12046v1.pdf"
    },
    {
        "title": "Extrinsically-Focused Evaluation of Omissions in Medical Summarization",
        "authors": [
            "Elliot Schumacher",
            "Daniel Rosenthal",
            "Varun Nair",
            "Luladay Price",
            "Geoffrey Tso",
            "Anitha Kannan"
        ],
        "published": "2023-11-14T16:46:15Z",
        "summary": "The goal of automated summarization techniques (Paice, 1990; Kupiec et al,\n1995) is to condense text by focusing on the most critical information.\nGenerative large language models (LLMs) have shown to be robust summarizers,\nyet traditional metrics struggle to capture resulting performance (Goyal et al,\n2022) in more powerful LLMs. In safety-critical domains such as medicine, more\nrigorous evaluation is required, especially given the potential for LLMs to\nomit important information in the resulting summary. We propose MED-OMIT, a new\nomission benchmark for medical summarization. Given a doctor-patient\nconversation and a generated summary, MED-OMIT categorizes the chat into a set\nof facts and identifies which are omitted from the summary. We further propose\nto determine fact importance by simulating the impact of each fact on a\ndownstream clinical task: differential diagnosis (DDx) generation. MED-OMIT\nleverages LLM prompt-based approaches which categorize the importance of facts\nand cluster them as supporting or negating evidence to the diagnosis. We\nevaluate MED-OMIT on a publicly-released dataset of patient-doctor\nconversations and find that MED-OMIT captures omissions better than alternative\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.08303v1.pdf"
    },
    {
        "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
        "authors": [
            "Jiahui Geng",
            "Fengyu Cai",
            "Yuxia Wang",
            "Heinz Koeppl",
            "Preslav Nakov",
            "Iryna Gurevych"
        ],
        "published": "2023-11-14T16:43:29Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.",
        "pdf_link": "https://arxiv.org/pdf/2311.08298v2.pdf"
    },
    {
        "title": "How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions",
        "authors": [
            "Houquan Zhou",
            "Yang Hou",
            "Zhenghua Li",
            "Xuebin Wang",
            "Zhefeng Wang",
            "Xinyu Duan",
            "Min Zhang"
        ],
        "published": "2023-11-14T16:30:36Z",
        "summary": "While recent advancements in large language models (LLMs) bring us closer to\nachieving artificial general intelligence, the question persists: Do LLMs truly\nunderstand language, or do they merely mimic comprehension through pattern\nrecognition? This study seeks to explore this question through the lens of\nsyntax, a crucial component of sentence comprehension. Adopting a natural\nlanguage question-answering (Q&A) scheme, we craft questions targeting nine\nsyntactic knowledge points that are most closely related to sentence\ncomprehension. Experiments conducted on 24 LLMs suggest that most have a\nlimited grasp of syntactic knowledge, exhibiting notable discrepancies across\ndifferent syntactic knowledge points. In particular, questions involving\nprepositional phrase attachment pose the greatest challenge, whereas those\nconcerning adjectival modifier and indirect object are relatively easier for\nLLMs to handle. Furthermore, a case study on the training dynamics of the LLMs\nreveals that the majority of syntactic knowledge is learned during the initial\nstages of training, hinting that simply increasing the number of training\ntokens may not be the `silver bullet' for improving the comprehension ability\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.08287v1.pdf"
    },
    {
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
        "authors": [
            "Peng Ding",
            "Jun Kuang",
            "Dan Ma",
            "Xuezhi Cao",
            "Yunsen Xian",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published": "2023-11-14T16:02:16Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.08268v4.pdf"
    },
    {
        "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning",
        "authors": [
            "Shengguang Wu",
            "Keming Lu",
            "Benfeng Xu",
            "Junyang Lin",
            "Qi Su",
            "Chang Zhou"
        ],
        "published": "2023-11-14T14:10:40Z",
        "summary": "Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.",
        "pdf_link": "https://arxiv.org/pdf/2311.08182v1.pdf"
    },
    {
        "title": "Vision-Language Instruction Tuning: A Review and Analysis",
        "authors": [
            "Chen Li",
            "Yixiao Ge",
            "Dian Li",
            "Ying Shan"
        ],
        "published": "2023-11-14T14:02:32Z",
        "summary": "Instruction tuning is a crucial supervised training phase in Large Language\nModels (LLMs), aiming to enhance the LLM's ability to generalize instruction\nexecution and adapt to user preferences. With the increasing integration of\nmulti-modal data into LLMs, there is growing interest in Vision-Language\nInstruction Tuning (VLIT), which presents more complex characteristics compared\nto pure text instruction tuning. In this paper, we systematically review the\nlatest VLIT settings and corresponding datasets in multi-modal LLMs and provide\ninsights into the intrinsic motivations behind their design. For the first\ntime, we offer a detailed multi-perspective categorization for existing VLIT\ndatasets and identify the characteristics that high-quality VLIT data should\npossess. By incorporating these characteristics as guiding principles into the\nexisting VLIT data construction process, we conduct extensive experiments and\nverify their positive impact on the performance of tuned multi-modal LLMs.\nFurthermore, we discuss the current challenges and future research directions\nof VLIT, providing insights for the continuous development of this field. The\ncode and dataset related to this paper have been open-sourced at\nhttps://github.com/palchenli/VL-Instruction-Tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.08172v2.pdf"
    },
    {
        "title": "MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",
        "authors": [
            "Bo Ni",
            "Markus J. Buehler"
        ],
        "published": "2023-11-14T13:49:03Z",
        "summary": "Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.",
        "pdf_link": "https://arxiv.org/pdf/2311.08166v1.pdf"
    },
    {
        "title": "Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
        "authors": [
            "Lei Lin",
            "Jiayi Fu",
            "Pengli Liu",
            "Qingyang Li",
            "Yan Gong",
            "Junchen Wan",
            "Fuzheng Zhang",
            "Zhongyuan Wang",
            "Di Zhang",
            "Kun Gai"
        ],
        "published": "2023-11-14T13:30:54Z",
        "summary": "Although chain-of-thought (CoT) prompting combined with language models has\nachieved encouraging results on complex reasoning tasks, the naive greedy\ndecoding used in CoT prompting usually causes the repetitiveness and local\noptimality. To address this shortcoming, ensemble-optimization tries to obtain\nmultiple reasoning paths to get the final answer assembly. However, current\nensemble-optimization methods either simply employ rule-based post-processing\nsuch as \\textit{self-consistency}, or train an additional model based on\nseveral task-related human annotations to select the best one among multiple\nreasoning paths, yet fail to generalize to realistic settings where the type of\ninput questions is unknown or the answer format of reasoning paths is unknown.\nTo avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable\nensemble-optimization method applying in almost all scenarios where the type of\ninput questions and the answer format of reasoning paths may be known or\nunknown. Self-agreement firstly samples from language model's decoder to\ngenerate a \\textit{diverse} set of reasoning paths, and subsequently prompts\nthe language model \\textit{one more time} to determine the optimal answer by\nselecting the most \\textit{agreed} answer among the sampled reasoning paths.\nSelf-agreement simultaneously achieves remarkable performance on six public\nreasoning benchmarks and superior generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.08154v2.pdf"
    },
    {
        "title": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge",
        "authors": [
            "Yi Liu",
            "Lianzhe Huang",
            "Shicheng Li",
            "Sishuo Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023-11-14T13:24:19Z",
        "summary": "LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.",
        "pdf_link": "https://arxiv.org/pdf/2311.08147v1.pdf"
    },
    {
        "title": "Insights into Classifying and Mitigating LLMs' Hallucinations",
        "authors": [
            "Alessandro Bruno",
            "Pier Luigi Mazzeo",
            "Aladine Chetouani",
            "Marouane Tliba",
            "Mohamed Amine Kerkouri"
        ],
        "published": "2023-11-14T12:30:28Z",
        "summary": "The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2311.08117v1.pdf"
    },
    {
        "title": "Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models",
        "authors": [
            "Yujin Kim",
            "Jaehong Yoon",
            "Seonghyeon Ye",
            "Sung Ju Hwang",
            "Se-young Yun"
        ],
        "published": "2023-11-14T12:12:02Z",
        "summary": "In an ever-evolving world, the dynamic nature of knowledge presents\nchallenges for language models that are trained on static data, leading to\noutdated encoded information. However, real-world scenarios require models not\nonly to acquire new knowledge but also to overwrite outdated information into\nupdated ones. To address this under-explored issue, we introduce the temporally\nevolving question answering benchmark, EvolvingQA - a novel benchmark designed\nfor training and evaluating LMs on an evolving Wikipedia database, where the\nconstruction of our benchmark is automated with our pipeline using large\nlanguage models. Our benchmark incorporates question-answering as a downstream\ntask to emulate real-world applications. Through EvolvingQA, we uncover that\nexisting continual learning baselines have difficulty in updating and\nforgetting outdated knowledge. Our findings suggest that the models fail to\nlearn updated knowledge due to the small weight gradient. Furthermore, we\nelucidate that the models struggle mostly on providing numerical or temporal\nanswers to questions asking for updated knowledge. Our work aims to model the\ndynamic nature of real-world information, offering a robust measure for the\nevolution-adaptability of language models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08106v1.pdf"
    },
    {
        "title": "Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts",
        "authors": [
            "Leonardo Ranaldi",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2023-11-14T11:49:43Z",
        "summary": "Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large\nLanguage Models (LLMs), eliciting them to solve complex reasoning tasks\nstep-by-step. However, with the success of CoT methods, the ability to deliver\nmulti-step reasoning remains limited to English due to the imbalance in the\ndistribution of the pre-training data, making the other languages a barrier.\n  In this work, we propose a Cross-lingual multi-step reasoning approach,\naiming to align reasoning processes across different languages. In particular,\nour method, through a Self-consistent Cross-lingual prompting mechanism\ninspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths\nin different languages that, during the steps, lead to the final solution. Our\nexperimental evaluations show that our method significantly outperforms\nexisting prompting methods, reducing the number of interactions and achieving\nstate-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.08097v1.pdf"
    },
    {
        "title": "Adversarial Preference Optimization",
        "authors": [
            "Pengyu Cheng",
            "Yifan Yang",
            "Jian Li",
            "Yong Dai",
            "Tianhao Hu",
            "Peixin Cao",
            "Nan Du"
        ],
        "published": "2023-11-14T10:10:31Z",
        "summary": "Human preference alignment is essential to improve the interaction quality of\nlarge language models (LLMs). Existing aligning methods depend on manually\nannotated preference data to guide the LLM optimization directions. However, in\npractice, continuously updating LLMs raises a distribution gap between\nmodel-generated samples and human-preferred responses, which hinders model\nfine-tuning efficiency. To mitigate this issue, previous methods require\nadditional preference annotation on generated samples to adapt the shifted\ndistribution, which consumes a large amount of annotation resources. Targeting\nmore efficient human preference optimization, we propose an adversarial\npreference optimization (APO) framework, where the LLM agent and the preference\nmodel update alternatively via a min-max game. Without additional annotation,\nour APO method can make a self-adaption to the generation distribution gap\nthrough the adversarial learning process. Based on comprehensive experiments,\nwe find APO further enhances the alignment performance of baseline methods in\nterms of helpfulness and harmlessness. The code is at\nhttps://github.com/Linear95/APO.",
        "pdf_link": "https://arxiv.org/pdf/2311.08045v3.pdf"
    },
    {
        "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
        "authors": [
            "Vivek Gupta",
            "Pranshu Kandoi",
            "Mahek Bhavesh Vora",
            "Shuo Zhang",
            "Yujie He",
            "Ridho Reinanda",
            "Vivek Srikumar"
        ],
        "published": "2023-11-14T08:57:01Z",
        "summary": "Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.",
        "pdf_link": "https://arxiv.org/pdf/2311.08002v1.pdf"
    },
    {
        "title": "How good are Large Language Models on African Languages?",
        "authors": [
            "Jessica Ojo",
            "Kelechi Ogueji",
            "Pontus Stenetorp",
            "David I. Adelani"
        ],
        "published": "2023-11-14T08:10:14Z",
        "summary": "Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and\nlanguages. Additionally, they have been widely adopted as\nlanguage-model-as-a-service commercial APIs like GPT-4 API. However, their\nperformance on African languages is largely unknown. We present an analysis of\nthree popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks\n(news topic classification, sentiment classification, machine translation,\nquestion answering, and named entity recognition) across 30 African languages,\nspanning different language families and geographical regions. Our results\nsuggest that all LLMs produce below-par performance on African languages, and\nthere is a large gap in performance compared to high-resource languages like\nEnglish most tasks. We find that GPT-4 has an average or impressive performance\non classification tasks but very poor results on generative tasks like machine\ntranslation. Surprisingly, we find that mT0 had the best overall on\ncross-lingual QA, better than the state-of-the-art supervised model (i.e.\nfine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the\nworst performance due to its limited multilingual capabilities and\nEnglish-centric pre-training corpus. In general, our findings present a\ncall-to-action to ensure African languages are well represented in large\nlanguage models, given their growing popularity.",
        "pdf_link": "https://arxiv.org/pdf/2311.07978v1.pdf"
    },
    {
        "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Xinyu Pang",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023-11-14T07:13:10Z",
        "summary": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.",
        "pdf_link": "https://arxiv.org/pdf/2311.07954v2.pdf"
    },
    {
        "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
        "authors": [
            "Garima Agrawal",
            "Tharindu Kumarage",
            "Zeyad Alghamdi",
            "Huan Liu"
        ],
        "published": "2023-11-14T05:21:57Z",
        "summary": "The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\ncomprehensively review these knowledge-graph-based augmentation techniques in\nLLMs, focusing on their efficacy in mitigating hallucinations. We\nsystematically categorize these methods into three overarching groups, offering\nmethodological comparisons and performance evaluations. Lastly, this survey\nexplores the current trends and challenges associated with these techniques and\noutlines potential avenues for future research in this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2311.07914v2.pdf"
    },
    {
        "title": "Instruction-Following Evaluation for Large Language Models",
        "authors": [
            "Jeffrey Zhou",
            "Tianjian Lu",
            "Swaroop Mishra",
            "Siddhartha Brahma",
            "Sujoy Basu",
            "Yi Luan",
            "Denny Zhou",
            "Le Hou"
        ],
        "published": "2023-11-14T05:13:55Z",
        "summary": "One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval",
        "pdf_link": "https://arxiv.org/pdf/2311.07911v1.pdf"
    },
    {
        "title": "Fair Abstractive Summarization of Diverse Perspectives",
        "authors": [
            "Yusen Zhang",
            "Nan Zhang",
            "Yixin Liu",
            "Alexander Fabbri",
            "Junru Liu",
            "Ryo Kamoi",
            "Xiaoxin Lu",
            "Caiming Xiong",
            "Jieyu Zhao",
            "Dragomir Radev",
            "Kathleen McKeown",
            "Rui Zhang"
        ],
        "published": "2023-11-14T03:38:55Z",
        "summary": "People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people, and we propose four reference-free automatic metrics by measuring\nthe differences between target and source perspectives. We evaluate nine LLMs,\nincluding three GPT models, four LLaMA models, PaLM 2, and Claude, on six\ndatasets collected from social media, online reviews, and recorded transcripts.\nExperiments show that both the model-generated and the human-written reference\nsummaries suffer from low fairness. We conduct a comprehensive analysis of the\ncommon factors influencing fairness and propose three simple but effective\nmethods to alleviate unfair summarization. Our dataset and code are available\nat https://github.com/psunlpgroup/FairSumm.",
        "pdf_link": "https://arxiv.org/pdf/2311.07884v2.pdf"
    },
    {
        "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
        "authors": [
            "Xiaonan Li",
            "Changtai Zhu",
            "Linyang Li",
            "Zhangyue Yin",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "published": "2023-11-14T01:38:02Z",
        "summary": "Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2311.07838v3.pdf"
    },
    {
        "title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
        "authors": [
            "Alessandro Oltramari"
        ],
        "published": "2023-11-13T21:20:17Z",
        "summary": "High-level reasoning can be defined as the capability to generalize over\nknowledge acquired via experience, and to exhibit robust behavior in novel\nsituations. Such form of reasoning is a basic skill in humans, who seamlessly\nuse it in a broad spectrum of tasks, from language communication to decision\nmaking in complex situations. When it manifests itself in understanding and\nmanipulating the everyday world of objects and their interactions, we talk\nabout common sense or commonsense reasoning. State-of-the-art AI systems don't\npossess such capability: for instance, Large Language Models have recently\nbecome popular by demonstrating remarkable fluency in conversing with humans,\nbut they still make trivial mistakes when probed for commonsense competence; on\na different level, performance degradation outside training data prevents\nself-driving vehicles to safely adapt to unseen scenarios, a serious and\nunsolved problem that limits the adoption of such technology. In this paper we\npropose to enable high-level reasoning in AI systems by integrating cognitive\narchitectures with external neuro-symbolic components. We illustrate a hybrid\nframework centered on ACT-R and we discuss the role of generative models in\nrecent and future applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.07759v1.pdf"
    },
    {
        "title": "On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models",
        "authors": [
            "Naman Goel"
        ],
        "published": "2023-11-13T19:21:25Z",
        "summary": "The surprisingly likely criterion in the seminal work of Prelec (the Bayesian\nTruth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,\nby rewarding rational agents to maximise the expected information gain with\ntheir answers w.r.t. their probabilistic beliefs. We investigate the relevance\nof a similar criterion for responses of LLMs. We hypothesize that if the\nsurprisingly likely criterion works in LLMs, under certain conditions, the\nresponses that maximize the reward under this criterion should be more accurate\nthan the responses that only maximize the posterior probability. Using\nbenchmarks including the TruthfulQA benchmark and using openly available LLMs:\nGPT-2 and LLaMA-2, we show that the method indeed improves the accuracy\nsignificantly (for example, upto 24 percentage points aggregate improvement on\nTruthfulQA and upto 70 percentage points improvement on individual categories\nof questions).",
        "pdf_link": "https://arxiv.org/pdf/2311.07692v1.pdf"
    },
    {
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
        "authors": [
            "Suyu Ge",
            "Chunting Zhou",
            "Rui Hou",
            "Madian Khabsa",
            "Yi-Chia Wang",
            "Qifan Wang",
            "Jiawei Han",
            "Yuning Mao"
        ],
        "published": "2023-11-13T19:13:29Z",
        "summary": "Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.",
        "pdf_link": "https://arxiv.org/pdf/2311.07689v1.pdf"
    },
    {
        "title": "Using Natural Language Explanations to Improve Robustness of In-context Learning for Natural Language Inference",
        "authors": [
            "Xuanli He",
            "Yuxiang Wu",
            "Oana-Maria Camburu",
            "Pasquale Minervini",
            "Pontus Stenetorp"
        ],
        "published": "2023-11-13T18:49:13Z",
        "summary": "Recent studies have demonstrated that large language models (LLMs) excel in\ndiverse tasks through in-context learning (ICL) facilitated by task-specific\nprompts and examples. However, the existing literature shows that ICL\nencounters performance deterioration when exposed to adversarial inputs.\nEnhanced performance has been observed when ICL is augmented with natural\nlanguage explanations (NLEs) (we refer to it as X-ICL). Thus, this work\ninvestigates whether X-ICL can improve the robustness of LLMs on a suite of\nseven adversarial and challenging natural language inference datasets.\nMoreover, we introduce a new approach to X-ICL by prompting an LLM (ChatGPT in\nour case) with few human-generated NLEs to produce further NLEs (we call it\nChatGPT few-shot), which we show superior to both ChatGPT zero-shot and\nhuman-generated NLEs alone. We evaluate five popular LLMs (GPT3.5-turbo,\nLLaMa2, Vicuna, Zephyr, Mistral) and show that X-ICL with ChatGPT few-shot\nyields over 6% improvement over ICL. Furthermore, while prompt selection\nstrategies were previously shown to significantly improve ICL on\nin-distribution test sets, we show that these strategies do not match the\nefficacy of the X-ICL paradigm in robustness-oriented evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2311.07556v1.pdf"
    },
    {
        "title": "GPT-4V(ision) as A Social Media Analysis Engine",
        "authors": [
            "Hanjia Lyu",
            "Jinfa Huang",
            "Daoan Zhang",
            "Yongsheng Yu",
            "Xinyi Mou",
            "Jinsheng Pan",
            "Zhengyuan Yang",
            "Zhongyu Wei",
            "Jiebo Luo"
        ],
        "published": "2023-11-13T18:36:50Z",
        "summary": "Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.",
        "pdf_link": "https://arxiv.org/pdf/2311.07547v1.pdf"
    },
    {
        "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
        "authors": [
            "Nishant Balepur",
            "Shramay Palta",
            "Rachel Rudinger"
        ],
        "published": "2023-11-13T18:18:22Z",
        "summary": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason\ntoward correct answers, but its efficacy in reasoning toward incorrect answers\nis unexplored. This process of elimination (PoE), when used with COT, can\nenhance self-consistency, interpretability, and tasks such as medical diagnoses\nof exclusion. Thus, we propose PoE with COT, where LLMs must reason toward\nincorrect options on multiple-choice questions. We evaluate the ability of\nGPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four\ncommonsense and scientific reasoning datasets. We find that the strategy of PoE\nalways underperforms the strategy of choosing the correct answer. The agreement\nof these strategies is also lower than the self-consistency of each strategy.\nTo study these issues further, we conduct error analyses and give suggestions\nfor future work.",
        "pdf_link": "https://arxiv.org/pdf/2311.07532v2.pdf"
    },
    {
        "title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
        "authors": [
            "Hejing Cao",
            "Zhenwei An",
            "Jiazhan Feng",
            "Kun Xu",
            "Liwei Chen",
            "Dongyan Zhao"
        ],
        "published": "2023-11-13T17:28:03Z",
        "summary": "While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.",
        "pdf_link": "https://arxiv.org/pdf/2311.07491v1.pdf"
    },
    {
        "title": "InCA: Rethinking In-Car Conversational System Assessment Leveraging Large Language Models",
        "authors": [
            "Ken E. Friedl",
            "Abbas Goher Khan",
            "Soumya Ranjan Sahoo",
            "Md Rashad Al Hasan Rony",
            "Jana Germies",
            "Christian S√º√ü"
        ],
        "published": "2023-11-13T17:02:06Z",
        "summary": "The assessment of advanced generative large language models (LLMs) poses a\nsignificant challenge, given their heightened complexity in recent\ndevelopments. Furthermore, evaluating the performance of LLM-based applications\nin various industries, as indicated by Key Performance Indicators (KPIs), is a\ncomplex undertaking. This task necessitates a profound understanding of\nindustry use cases and the anticipated system behavior. Within the context of\nthe automotive industry, existing evaluation metrics prove inadequate for\nassessing in-car conversational question answering (ConvQA) systems. The unique\ndemands of these systems, where answers may relate to driver or car safety and\nare confined within the car domain, highlight the limitations of current\nmetrics. To address these challenges, this paper introduces a set of KPIs\ntailored for evaluating the performance of in-car ConvQA systems, along with\ndatasets specifically designed for these KPIs. A preliminary and comprehensive\nempirical evaluation substantiates the efficacy of our proposed approach.\nFurthermore, we investigate the impact of employing varied personas in prompts\nand found that it enhances the model's capacity to simulate diverse viewpoints\nin assessments, mirroring how individuals with different backgrounds perceive a\ntopic.",
        "pdf_link": "https://arxiv.org/pdf/2311.07469v2.pdf"
    },
    {
        "title": "Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation of the Reversal Curse",
        "authors": [
            "Ang Lv",
            "Kaiyi Zhang",
            "Shufang Xie",
            "Quan Tu",
            "Yuhan Chen",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "published": "2023-11-13T17:01:12Z",
        "summary": "Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B as the answer. However, it may\nencounter confusion when presented with questions concerning B. We contend that\nthe reversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.",
        "pdf_link": "https://arxiv.org/pdf/2311.07468v2.pdf"
    },
    {
        "title": "On Measuring Faithfulness or Self-consistency of Natural Language Explanations",
        "authors": [
            "Letitia Parcalabescu",
            "Anette Frank"
        ],
        "published": "2023-11-13T16:53:51Z",
        "summary": "Large language models (LLMs) can explain their predictions through post-hoc\nor Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably\nsounding explanations that are unfaithful to its underlying reasoning. Recent\nwork has designed tests that aim to judge the faithfulness of post-hoc or CoT\nexplanations. In this work we argue that these faithfulness tests do not\nmeasure faithfulness to the models' inner workings -- but rather their\nself-consistency at output level. Our contributions are three-fold: i) We\nclarify the status of faithfulness tests in view of model explainability,\ncharacterising them as self-consistency tests instead. This assessment we\nunderline by ii) constructing a Comparative Consistency Bank for\nself-consistency tests that for the first time compares existing tests on a\ncommon suite of 11 open LLMs and 5 tasks -- including iii) our new\nself-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a\ntest) of LLM self-consistency. It compares how a model's input contributes to\nthe predicted answer and to generating the explanation. Our fine-grained\nCC-SHAP metric allows us iii) to compare LLM behaviour when making predictions\nand to analyse the effect of other consistency tests at a deeper level, which\ntakes us one step further towards measuring faithfulness by bringing us closer\nto the internals of the model than strictly surface output-oriented tests. Our\ncode is available at \\url{https://github.com/Heidelberg-NLP/CC-SHAP}",
        "pdf_link": "https://arxiv.org/pdf/2311.07466v2.pdf"
    },
    {
        "title": "Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text",
        "authors": [
            "Zhongfei Qing",
            "Zhongang Cai",
            "Zhitao Yang",
            "Lei Yang"
        ],
        "published": "2023-11-13T16:22:38Z",
        "summary": "Generating natural human motion from a story has the potential to transform\nthe landscape of animation, gaming, and film industries. A new and challenging\ntask, Story-to-Motion, arises when characters are required to move to various\nlocations and perform specific motions based on a long text description. This\ntask demands a fusion of low-level control (trajectories) and high-level\ncontrol (motion semantics). Previous works in character control and\ntext-to-motion have addressed related aspects, yet a comprehensive solution\nremains elusive: character control methods do not handle text description,\nwhereas text-to-motion methods lack position constraints and often produce\nunstable motions. In light of these limitations, we propose a novel system that\ngenerates controllable, infinitely long motions and trajectories aligned with\nthe input text. (1) We leverage contemporary Large Language Models to act as a\ntext-driven motion scheduler to extract a series of (text, position, duration)\npairs from long text. (2) We develop a text-driven motion retrieval scheme that\nincorporates motion matching with motion semantic and trajectory constraints.\n(3) We design a progressive mask transformer that addresses common artifacts in\nthe transition motion such as unnatural pose and foot sliding. Beyond its\npioneering role as the first comprehensive solution for Story-to-Motion, our\nsystem undergoes evaluation across three distinct sub-tasks: trajectory\nfollowing, temporal action composition, and motion blending, where it\noutperforms previous state-of-the-art motion synthesis methods across the\nboard. Homepage: https://story2motion.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2311.07446v1.pdf"
    },
    {
        "title": "Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue",
        "authors": [
            "Junkai Zhou",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2023-11-13T16:19:42Z",
        "summary": "The emergence of large language models (LLMs) further improves the\ncapabilities of open-domain dialogue systems and can generate fluent, coherent,\nand diverse responses. However, LLMs still lack a crucial ability:\ncommunication skills. This limitation renders them more like information\nseeking tools rather than anthropomorphic chatbots. Communication skills, such\nas topic transition, proactively asking questions, concept guidance, empathy,\nand summarising often should be taken into consideration, to make LLMs more\nanthropomorphic and proactive during the conversation, thereby increasing the\ninterest of users and attracting them to chat for longer. However, enabling\nthese communication skills in black-box LLMs remains a key challenge because\nthey do not have the same utterance formation mode as real people: think before\nspeaking. Inspired by linguistics and cognitive science, we empower LLMs with\ncommunication skills through inner monologues. To evaluate various\ncommunication skills, we construct a benchmark named Cskills, which can also\nmore comprehensively evaluate the dialogue generation ability of the model.\nExperimental results show that the proposed CSIM strategy improves the backbone\nmodels and outperforms the baselines.",
        "pdf_link": "https://arxiv.org/pdf/2311.07445v2.pdf"
    },
    {
        "title": "AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation",
        "authors": [
            "Junyang Wang",
            "Yuhang Wang",
            "Guohai Xu",
            "Jing Zhang",
            "Yukai Gu",
            "Haitao Jia",
            "Jiaqi Wang",
            "Haiyang Xu",
            "Ming Yan",
            "Ji Zhang",
            "Jitao Sang"
        ],
        "published": "2023-11-13T15:25:42Z",
        "summary": "Despite making significant progress in multi-modal tasks, current Multi-modal\nLarge Language Models (MLLMs) encounter the significant challenge of\nhallucinations, which may lead to harmful consequences. Therefore, evaluating\nMLLMs' hallucinations is becoming increasingly important in model improvement\nand practical application deployment. Previous works are limited in high\nevaluation costs (e.g., relying on humans or advanced LLMs) and insufficient\nevaluation dimensions (e.g., types of tasks and hallucinations). In this paper,\nwe propose an LLM-free multi-dimensional benchmark AMBER, which can be used to\nevaluate both generative task and discriminative task including existence,\nattribute and relation hallucination. Based on AMBER, we design a low-cost and\nefficient evaluation pipeline. Additionally, we conduct a comprehensive\nevaluation and detailed analysis of mainstream MLLMs including GPT-4V(ision),\nand also give guideline suggestions for mitigating hallucinations. The data and\ncode of AMBER are available at https://github.com/junyangwang0410/AMBER.",
        "pdf_link": "https://arxiv.org/pdf/2311.07397v2.pdf"
    },
    {
        "title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
        "authors": [
            "Yinghao Li",
            "Haorui Wang",
            "Chao Zhang"
        ],
        "published": "2023-11-13T15:11:26Z",
        "summary": "Large Language Models (LLMs) have shown remarkable proficiency in language\nunderstanding and have been successfully applied to a variety of real-world\ntasks through task-specific fine-tuning or prompt engineering. Despite these\nadvancements, it remains an open question whether LLMs are fundamentally\ncapable of reasoning and planning, or if they primarily rely on recalling and\nsynthesizing information from their training data. In our research, we\nintroduce a novel task -- Minesweeper -- specifically designed in a format\nunfamiliar to LLMs and absent from their training datasets. This task\nchallenges LLMs to identify the locations of mines based on numerical clues\nprovided by adjacent opened cells. Successfully completing this task requires\nan understanding of each cell's state, discerning spatial relationships between\nthe clues and mines, and strategizing actions based on logical deductions drawn\nfrom the arrangement of the cells. Our experiments, including trials with the\nadvanced GPT-4 model, indicate that while LLMs possess the foundational\nabilities required for this task, they struggle to integrate these into a\ncoherent, multi-step logical reasoning process needed to solve Minesweeper.\nThese findings highlight the need for further research to understand and nature\nof reasoning capabilities in LLMs under similar circumstances, and to explore\npathways towards more sophisticated AI reasoning and planning models.",
        "pdf_link": "https://arxiv.org/pdf/2311.07387v1.pdf"
    },
    {
        "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
        "authors": [
            "Ekaterina Fadeeva",
            "Roman Vashurin",
            "Akim Tsvigun",
            "Artem Vazhentsev",
            "Sergey Petrakov",
            "Kirill Fedyanin",
            "Daniil Vasilev",
            "Elizaveta Goncharova",
            "Alexander Panchenko",
            "Maxim Panov",
            "Timothy Baldwin",
            "Artem Shelmanov"
        ],
        "published": "2023-11-13T15:08:59Z",
        "summary": "Recent advancements in the capabilities of large language models (LLMs) have\npaved the way for a myriad of groundbreaking applications in various fields.\nHowever, a significant challenge arises as these models often \"hallucinate\",\ni.e., fabricate facts without providing users an apparent means to discern the\nveracity of their statements. Uncertainty estimation (UE) methods are one path\nto safer, more responsible, and more effective use of LLMs. However, to date,\nresearch on UE methods for LLMs has been focused primarily on theoretical\nrather than engineering contributions. In this work, we tackle this issue by\nintroducing LM-Polygraph, a framework with implementations of a battery of\nstate-of-the-art UE methods for LLMs in text generation tasks, with unified\nprogram interfaces in Python. Additionally, it introduces an extendable\nbenchmark for consistent evaluation of UE techniques by researchers, and a demo\nweb application that enriches the standard chat dialog with confidence scores,\nempowering end-users to discern unreliable responses. LM-Polygraph is\ncompatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and\nGPT-4, and is designed to support future releases of similarly-styled LMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.07383v1.pdf"
    },
    {
        "title": "Do large language models and humans have similar behaviors in causal inference with script knowledge?",
        "authors": [
            "Xudong Hong",
            "Margarita Ryzhova",
            "Daniel Adrian Biondi",
            "Vera Demberg"
        ],
        "published": "2023-11-13T13:05:15Z",
        "summary": "Recently, large pre-trained language models (LLMs) have demonstrated superior\nlanguage understanding abilities, including zero-shot causal reasoning.\nHowever, it is unclear to what extent their capabilities are similar to human\nones. We here study the processing of an event $B$ in a script-based story,\nwhich causally depends on a previous event $A$. In our manipulation, event $A$\nis stated, negated, or omitted in an earlier section of the text. We first\nconducted a self-paced reading experiment, which showed that humans exhibit\nsignificantly longer reading times when causal conflicts exist ($\\neg A\n\\rightarrow B$) than under logical conditions ($A \\rightarrow B$). However,\nreading times remain similar when cause A is not explicitly mentioned,\nindicating that humans can easily infer event B from their script knowledge. We\nthen tested a variety of LLMs on the same data to check to what extent the\nmodels replicate human behavior. Our experiments show that 1) only recent LLMs,\nlike GPT-3 or Vicuna, correlate with human behavior in the $\\neg A \\rightarrow\nB$ condition. 2) Despite this correlation, all models still fail to predict\nthat $nil \\rightarrow B$ is less surprising than $\\neg A \\rightarrow B$,\nindicating that LLMs still have difficulties integrating script knowledge. Our\ncode and collected data set are available at\nhttps://github.com/tony-hong/causal-script.",
        "pdf_link": "https://arxiv.org/pdf/2311.07311v1.pdf"
    },
    {
        "title": "What Large Language Models Bring to Text-rich VQA?",
        "authors": [
            "Xuejing Liu",
            "Wei Tang",
            "Xinzhe Ni",
            "Jinghui Lu",
            "Rui Zhao",
            "Zechao Li",
            "Fei Tan"
        ],
        "published": "2023-11-13T12:52:29Z",
        "summary": "Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.07306v1.pdf"
    },
    {
        "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search",
        "authors": [
            "Huihan Li",
            "Yuting Ning",
            "Zeyi Liao",
            "Siyuan Wang",
            "Xiang Lorraine Li",
            "Ximing Lu",
            "Wenting Zhao",
            "Faeze Brahman",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-11-13T10:56:59Z",
        "summary": "State-of-the-art LLMs outperform humans on reasoning tasks such as Natural\nLanguage Inference. Recent works evaluating LLMs note a marked performance drop\non input data from the low-probability distribution, i.e., the longtail.\nTherefore, we focus on systematically generating statements involving long-tail\ninferential knowledge for more effective evaluation of LLMs in the reasoning\nspace. We first propose a novel framework Logic-Induced- Knowledge-Search\n(LINK) that generates factually correct and long-tail knowledge statements\ngrounded on symbolic rule templates; LINK effectively generates data in the\nlongtail distribution that zero-shot prompted LLMs are unable to reach, and\noutperforms zero-shot GPT4 on factual correctness by 5%. We further use the\ndata generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT)\nthat can be used to evaluate downstream models on the long-tail distribution;\nLINT contains 108K knowledge statements spanning four domains. We use LINT to\ntest LLMs on an entailment classification task and find that model performances\ndrop by as high as 5% in the long-tail distribution compared to head\ndistribution. Our work shows the utility of evaluating models in the long-tail\ndistribution, and calls for more research on generating evaluation data in the\nlong-tail distribution.",
        "pdf_link": "https://arxiv.org/pdf/2311.07237v2.pdf"
    },
    {
        "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
        "authors": [
            "Shuaijie She",
            "Shujian Huang",
            "Xingyun Wang",
            "Yanke Zhou",
            "Jiajun Chen"
        ],
        "published": "2023-11-13T09:32:12Z",
        "summary": "LLMs (Large Language Models) usually interact with users in the form of\ndialogue and generate responses following their instructions, which naturally\nrequire dialogue comprehension abilities. However, dialogue comprehension is a\ngeneral language ability which is hard to be evaluated directly. In this work,\nwe propose to perform the evaluation focusing on the factual consistency issue\nwith the help of the dialogue summarization task. Besides evaluating and\nanalyzing the dialogue summarization performance (DIAC-Sum) of different LLMs,\nwe also derive factual questions from the generated summaries and use them as a\nmore flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation\nshows that, on average, 26.8% of the summaries generated by LLMs contain\nfactual inconsistency. Even ChatGPT, the strongest model evaluated, has such\nerrors in 16% of its summaries. For answering the factual questions, which is\nmore challenging, the average error rate of all evaluated LLMs is 36.1%. Both\nresults indicate serious deficiencies. Detailed analysis shows that the\nunderstanding of subject/object of the conversation is still challenging for\nLLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability\nof LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task\ndata, which achieved a relative error rate reduction of 11% on DIAC-QA.",
        "pdf_link": "https://arxiv.org/pdf/2311.07194v3.pdf"
    },
    {
        "title": "Can LLMs Patch Security Issues?",
        "authors": [
            "Kamel Alrashedy",
            "Abdullah Aljasser"
        ],
        "published": "2023-11-13T08:54:37Z",
        "summary": "Large Language Models (LLMs) have shown impressive proficiency in code\ngeneration. Nonetheless, similar to human developers, these models might\ngenerate code that contains security vulnerabilities and flaws. Writing secure\ncode remains a substantial challenge, as vulnerabilities often arise during\ninteractions between programs and external systems or services, such as\ndatabases and operating systems. In this paper, we propose a novel approach,\nFeedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs\nin receiving feedback from Bandit, which is a static code analysis tool, and\nthen the LLMs generate potential solutions to resolve security vulnerabilities.\nEach solution, along with the vulnerable code, is then sent back to the LLM for\ncode refinement. Our approach shows a significant improvement over the baseline\nand outperforms existing approaches. Furthermore, we introduce a new dataset,\nPythonSecurityEval, collected from real-world scenarios on Stack Overflow to\nevaluate the LLMs' ability to generate secure code. Code and data are available\nat \\url{https://github.com/Kamel773/LLM-code-refine}",
        "pdf_link": "https://arxiv.org/pdf/2312.00024v3.pdf"
    },
    {
        "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
        "authors": [
            "Shangqing Tu",
            "Yuliang Sun",
            "Yushi Bai",
            "Jifan Yu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023-11-13T08:09:01Z",
        "summary": "To mitigate the potential misuse of large language models (LLMs), recent\nresearch has developed watermarking algorithms, which restrict the generation\nprocess to leave an invisible trace for watermark detection. Due to the\ntwo-stage nature of the task, most studies evaluate the generation and\ndetection separately, thereby presenting a challenge in unbiased, thorough, and\napplicable evaluations. In this paper, we introduce WaterBench, the first\ncomprehensive benchmark for LLM watermarks, in which we design three crucial\nfactors: (1) For \\textbf{benchmarking procedure}, to ensure an apples-to-apples\ncomparison, we first adjust each watermarking method's hyper-parameter to reach\nthe same watermarking strength, then jointly evaluate their generation and\ndetection performance. (2) For \\textbf{task selection}, we diversify the input\nand output length to form a five-category taxonomy, covering $9$ tasks. (3) For\n\\textbf{evaluation metric}, we adopt the GPT4-Judge for automatically\nevaluating the decline of instruction-following abilities after watermarking.\nWe evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking\nstrengths and observe the common struggles for current methods on maintaining\nthe generation quality. The code and data are available at\n\\url{https://github.com/THU-KEG/WaterBench}.",
        "pdf_link": "https://arxiv.org/pdf/2311.07138v1.pdf"
    },
    {
        "title": "Towards the Law of Capacity Gap in Distilling Language Models",
        "authors": [
            "Chen Zhang",
            "Dawei Song",
            "Zheyu Ye",
            "Yan Gao"
        ],
        "published": "2023-11-13T03:36:18Z",
        "summary": "Language model (LM) distillation is a trending area that aims to distil the\nknowledge resided in a large teacher LM to a small student one. While various\nmethods have been proposed to push the distillation to its limits, it is still\na pain distilling LMs when a large capacity gap is exhibited between the\nteacher and the student LMs. The pain is mainly resulted by the curse of\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\nbetter student LM than one distilled from a smaller teacher LM due to the\naffect of capacity gap increment. That is, there is likely an optimal point\nyielding the best student LM along the scaling course of the teacher LM. Even\nworse, the curse of capacity gap can be only partly yet not fully lifted as\nindicated in previous studies.\n  However, the tale is not ever one-sided. Although a larger teacher LM has\nbetter performance than a smaller teacher LM, it is much more\nresource-demanding especially in the context of recent large LMs (LLMs).\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\ncapacity gap is almost consistent across different student scales and\narchitectures, fortunately turning the curse into the law of capacity gap. The\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\ncompute-performance pareto frontier among existing 3B LMs on commonly used\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\nwide range of 3B competitors in GPT4 evaluation and could even compete with\nseveral 7B chat models.",
        "pdf_link": "https://arxiv.org/pdf/2311.07052v1.pdf"
    },
    {
        "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with Experience Notebook",
        "authors": [
            "Wangtao Sun",
            "Xuanqing Yu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-11-13T02:31:16Z",
        "summary": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote",
        "pdf_link": "https://arxiv.org/pdf/2311.07032v1.pdf"
    },
    {
        "title": "SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions by Themselves",
        "authors": [
            "Jiachen Zhao",
            "Zonghai Yao",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023-11-12T23:14:43Z",
        "summary": "Large language models (LLMs) can generate intermediate reasoning steps. To\nelicit the reliable reasoning, the common practice is to employ few-shot\nchain-of-thought prompting, where several in-context demonstrations for\nreasoning are prepended to the question. However, such chain-of-thought\nexamples are expensive to craft, especially for professional domains, and can\nhave high variance depending on human annotators. Therefore, this work\ninvestigates whether LLMs can teach themselves to reason without human-crafted\ndemonstrations. We propose SELF-EXPLAIN to generate CoT examples by LLMs\ninspired by \"encoding specificity\" in human memory retrieval. We find using\nself-explanations makes LLMs more confident, more calibrated and less biased\nwhen answering complex questions. Moreover, we find prompting with\nself-explanations can even significantly outperform using human-crafted CoTs on\nseveral complex question answering dataset.",
        "pdf_link": "https://arxiv.org/pdf/2311.06985v1.pdf"
    },
    {
        "title": "Assessing the Interpretability of Programmatic Policies with Large Language Models",
        "authors": [
            "Zahra Bashir",
            "Michael Bowling",
            "Levi H. S. Lelis"
        ],
        "published": "2023-11-12T22:43:26Z",
        "summary": "Although the synthesis of programs encoding policies often carries the\npromise of interpretability, systematic evaluations were never performed to\nassess the interpretability of these policies, likely because of the complexity\nof such an evaluation. In this paper, we introduce a novel metric that uses\nlarge-language models (LLM) to assess the interpretability of programmatic\npolicies. For our metric, an LLM is given both a program and a description of\nits associated programming language. The LLM then formulates a natural language\nexplanation of the program. This explanation is subsequently fed into a second\nLLM, which tries to reconstruct the program from the natural-language\nexplanation. Our metric then measures the behavioral similarity between the\nreconstructed program and the original. We validate our approach with\nsynthesized and human-crafted programmatic policies for playing a real-time\nstrategy game, comparing the interpretability scores of these programmatic\npolicies to obfuscated versions of the same programs. Our LLM-based\ninterpretability score consistently ranks less interpretable programs lower and\nmore interpretable ones higher. These findings suggest that our metric could\nserve as a reliable and inexpensive tool for evaluating the interpretability of\nprogrammatic policies.",
        "pdf_link": "https://arxiv.org/pdf/2311.06979v2.pdf"
    },
    {
        "title": "Flames: Benchmarking Value Alignment of LLMs in Chinese",
        "authors": [
            "Kexin Huang",
            "Xiangyang Liu",
            "Qianyu Guo",
            "Tianxiang Sun",
            "Jiawei Sun",
            "Yaru Wang",
            "Zeyang Zhou",
            "Yixu Wang",
            "Yan Teng",
            "Xipeng Qiu",
            "Yingchun Wang",
            "Dahua Lin"
        ],
        "published": "2023-11-12T17:18:21Z",
        "summary": "The widespread adoption of large language models (LLMs) across various\nregions underscores the urgent need to evaluate their alignment with human\nvalues. Current benchmarks, however, fall short of effectively uncovering\nsafety vulnerabilities in LLMs. Despite numerous models achieving high scores\nand 'topping the chart' in these evaluations, there is still a significant gap\nin LLMs' deeper alignment with human values and achieving genuine harmlessness.\nTo this end, this paper proposes a value alignment benchmark named Flames,\nwhich encompasses both common harmlessness principles and a unique morality\ndimension that integrates specific Chinese values such as harmony. Accordingly,\nwe carefully design adversarial prompts that incorporate complex scenarios and\njailbreaking methods, mostly with implicit malice. By prompting 17 mainstream\nLLMs, we obtain model responses and rigorously annotate them for detailed\nevaluation. Our findings indicate that all the evaluated LLMs demonstrate\nrelatively poor performance on Flames, particularly in the safety and fairness\ndimensions. We also develop a lightweight specified scorer capable of scoring\nLLMs across multiple dimensions to efficiently evaluate new models on the\nbenchmark. The complexity of Flames has far exceeded existing benchmarks,\nsetting a new challenge for contemporary LLMs and highlighting the need for\nfurther alignment of LLMs. Our benchmark is publicly available at\nhttps://github.com/AIFlames/Flames.",
        "pdf_link": "https://arxiv.org/pdf/2311.06899v3.pdf"
    },
    {
        "title": "Can Large Language Models Augment a Biomedical Ontology with missing Concepts and Relations?",
        "authors": [
            "Antonio Zaitoun",
            "Tomer Sagi",
            "Szymon Wilk",
            "Mor Peleg"
        ],
        "published": "2023-11-12T14:20:55Z",
        "summary": "Ontologies play a crucial role in organizing and representing knowledge.\nHowever, even current ontologies do not encompass all relevant concepts and\nrelationships. Here, we explore the potential of large language models (LLM) to\nexpand an existing ontology in a semi-automated fashion. We demonstrate our\napproach on the biomedical ontology SNOMED-CT utilizing semantic relation types\nfrom the widely used UMLS semantic network. We propose a method that uses\nconversational interactions with an LLM to analyze clinical practice guidelines\n(CPGs) and detect the relationships among the new medical concepts that are not\npresent in SNOMED-CT. Our initial experimentation with the conversational\nprompts yielded promising preliminary results given a manually generated gold\nstandard, directing our future potential improvements.",
        "pdf_link": "https://arxiv.org/pdf/2311.06858v1.pdf"
    },
    {
        "title": "Evaluating the Efficacy of Interactive Language Therapy Based on LLM for High-Functioning Autistic Adolescent Psychological Counseling",
        "authors": [
            "Yujin Cho",
            "Mingeon Kim",
            "Seojin Kim",
            "Oyun Kwon",
            "Ryan Donghan Kwon",
            "Yoonha Lee",
            "Dohyun Lim"
        ],
        "published": "2023-11-12T07:55:39Z",
        "summary": "This study investigates the efficacy of Large Language Models (LLMs) in\ninteractive language therapy for high-functioning autistic adolescents. With\nthe rapid advancement of artificial intelligence, particularly in natural\nlanguage processing, LLMs present a novel opportunity to augment traditional\npsychological counseling methods. This research primarily focuses on evaluating\nthe LLM's ability to engage in empathetic, adaptable, and contextually\nappropriate interactions within a therapeutic setting. A comprehensive\nevaluation was conducted by a panel of clinical psychologists and psychiatrists\nusing a specially developed scorecard. The assessment covered various aspects\nof the LLM's performance, including empathy, communication skills,\nadaptability, engagement, and the ability to establish a therapeutic alliance.\nThe study avoided direct testing with patients, prioritizing privacy and\nethical considerations, and instead relied on simulated scenarios to gauge the\nLLM's effectiveness. The results indicate that LLMs hold significant promise as\nsupportive tools in therapy, demonstrating strengths in empathetic engagement\nand adaptability in conversation. However, challenges in achieving the depth of\npersonalization and emotional understanding characteristic of human therapists\nwere noted. The study also highlights the importance of ethical considerations\nin the application of AI in therapeutic contexts. This research provides\nvaluable insights into the potential and limitations of using LLMs in\npsychological counseling for autistic adolescents. It lays the groundwork for\nfuture explorations into AI's role in mental health care, emphasizing the need\nfor ongoing development to enhance the capabilities of these models in\ntherapeutic settings.",
        "pdf_link": "https://arxiv.org/pdf/2311.09243v1.pdf"
    },
    {
        "title": "Are LLMs Rigorous Logical Reasoner? Empowering Natural Language Proof Generation with Contrastive Stepwise Decoding",
        "authors": [
            "Ying Su",
            "Xiaojin Fu",
            "Mingwen Liu",
            "Zhijiang Guo"
        ],
        "published": "2023-11-12T05:12:49Z",
        "summary": "Logical reasoning remains a pivotal component within the realm of artificial\nintelligence. The recent evolution of large language models (LLMs) has marked\nsignificant progress in this domain. The adoption of strategies like\nchain-of-thought (CoT) has enhanced the performance of LLMs across diverse\nreasoning tasks. Nonetheless, logical reasoning that involves proof planning,\nspecifically those that necessitate the validation of explanation accuracy,\ncontinues to present stumbling blocks. In this study, we first evaluate the\nefficacy of LLMs with advanced CoT strategies concerning such tasks. Our\nanalysis reveals that LLMs still struggle to navigate complex reasoning chains,\nwhich demand the meticulous linkage of premises to derive a cogent conclusion.\nTo address this issue, we finetune a smaller-scale language model, equipping it\nto decompose proof objectives into more manageable subgoals. We also introduce\ncontrastive decoding to stepwise proof generation, making use of negative\nreasoning paths to strengthen the model's capacity for logical deduction.\nExperiments on EntailmentBank underscore the success of our method in\naugmenting the proof planning abilities of language models.",
        "pdf_link": "https://arxiv.org/pdf/2311.06736v1.pdf"
    },
    {
        "title": "Trusted Source Alignment in Large Language Models",
        "authors": [
            "Vasilisa Bashlovkina",
            "Zhaobin Kuang",
            "Riley Matthews",
            "Edward Clifford",
            "Yennie Jun",
            "William W. Cohen",
            "Simon Baumgartner"
        ],
        "published": "2023-11-12T00:25:25Z",
        "summary": "Large language models (LLMs) are trained on web-scale corpora that inevitably\ninclude contradictory factual information from sources of varying reliability.\nIn this paper, we propose measuring an LLM property called trusted source\nalignment (TSA): the model's propensity to align with content produced by\ntrusted publishers in the face of uncertainty or controversy. We present\nFactCheckQA, a TSA evaluation dataset based on a corpus of fact checking\narticles. We describe a simple protocol for evaluating TSA and offer a detailed\nanalysis of design considerations including response extraction, claim\ncontextualization, and bias in prompt formulation. Applying the protocol to\nPaLM-2, we find that as we scale up the model size, the model performance on\nFactCheckQA improves from near-random to up to 80% balanced accuracy in\naligning with trusted sources.",
        "pdf_link": "https://arxiv.org/pdf/2311.06697v1.pdf"
    },
    {
        "title": "Intentional Biases in LLM Responses",
        "authors": [
            "Nicklaus Badyal",
            "Derek Jacoby",
            "Yvonne Coady"
        ],
        "published": "2023-11-11T19:59:24Z",
        "summary": "In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.",
        "pdf_link": "https://arxiv.org/pdf/2311.07611v1.pdf"
    },
    {
        "title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance",
        "authors": [
            "Rik Koncel-Kedziorski",
            "Michael Krumdick",
            "Viet Lai",
            "Varshini Reddy",
            "Charles Lovering",
            "Chris Tanner"
        ],
        "published": "2023-11-11T16:16:11Z",
        "summary": "Answering questions within business and finance requires reasoning,\nprecision, and a wide-breadth of technical knowledge. Together, these\nrequirements make this domain difficult for large language models (LLMs). We\nintroduce BizBench, a benchmark for evaluating models' ability to reason about\nrealistic financial problems. BizBench comprises eight quantitative reasoning\ntasks, focusing on question-answering (QA) over financial data via program\nsynthesis. We include three financially-themed code-generation tasks from newly\ncollected and augmented QA data. Additionally, we isolate the reasoning\ncapabilities required for financial QA: reading comprehension of financial text\nand tables for extracting intermediate values, and understanding financial\nconcepts and formulas needed to calculate complex solutions. Collectively,\nthese tasks evaluate a model's financial background knowledge, ability to parse\nfinancial documents, and capacity to solve problems with code. We conduct an\nin-depth evaluation of open-source and commercial LLMs, comparing and\ncontrasting the behavior of code-focused and language-focused models. We\ndemonstrate that the current bottleneck in performance is due to LLMs' limited\nbusiness and financial understanding, highlighting the value of a challenging\nbenchmark for quantitative reasoning within this domain.",
        "pdf_link": "https://arxiv.org/pdf/2311.06602v2.pdf"
    },
    {
        "title": "Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study",
        "authors": [
            "Maarten De Raedt",
            "Semere Kiros Bitew",
            "Fr√©deric Godin",
            "Thomas Demeester",
            "Chris Develder"
        ],
        "published": "2023-11-11T11:56:56Z",
        "summary": "The brittleness of finetuned language model performance on\nout-of-distribution (OOD) test samples in unseen domains has been well-studied\nfor English, yet is unexplored for multi-lingual models. Therefore, we study\ngeneralization to OOD test data specifically in zero-shot cross-lingual\ntransfer settings, analyzing performance impacts of both language and domain\nshifts between train and test data. We further assess the effectiveness of\ncounterfactually augmented data (CAD) in improving OOD generalization for the\ncross-lingual setting, since CAD has been shown to benefit in a monolingual\nEnglish setting. Finally, we propose two new approaches for OOD generalization\nthat avoid the costly annotation process associated with CAD, by exploiting the\npower of recent large language models (LLMs). We experiment with 3 multilingual\nmodels, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and\nevaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and\nRestaurant reviews. Results echo the OOD performance decline observed in the\nmonolingual English setting. Further, (i) counterfactuals from the original\nhigh-resource language do improve OOD generalization in the low-resource\nlanguage, and (ii) our newly proposed cost-effective approaches reach similar\nor up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.",
        "pdf_link": "https://arxiv.org/pdf/2311.06549v1.pdf"
    },
    {
        "title": "CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset",
        "authors": [
            "Le Chen",
            "Arijit Bhattacharjee",
            "Nesreen K. Ahmed",
            "Niranjan Hasabnis",
            "Gal Oren",
            "Bin Lei",
            "Ali Jannesari"
        ],
        "published": "2023-11-11T08:21:52Z",
        "summary": "Large language models (LLMs) have become increasingly prominent in academia\nand industry due to their remarkable performance in diverse applications. As\nthese models evolve with increasing parameters, they excel in tasks like\nsentiment analysis and machine translation. However, even models with billions\nof parameters face challenges in tasks demanding multi-step reasoning. Code\ngeneration and comprehension, especially in C and C++, emerge as significant\nchallenges. While LLMs trained on code datasets demonstrate competence in many\ntasks, they struggle with rectifying non-compilable C and C++ code. Our\ninvestigation attributes this subpar performance to two primary factors: the\nquality of the training dataset and the inherent complexity of the problem\nwhich demands intricate reasoning. Existing \"Chain of Thought\" (CoT) prompting\ntechniques aim to enhance multi-step reasoning. This approach, however, retains\nthe limitations associated with the latent drawbacks of LLMs. In this work, we\npropose CompCodeVet, a compiler-guided CoT approach to produce compilable code\nfrom non-compilable ones. Diverging from the conventional approach of utilizing\nlarger LLMs, we employ compilers as a teacher to establish a more robust\nzero-shot thought process. The evaluation of CompCodeVet on two open-source\ncode datasets shows that CompCodeVet has the ability to improve the training\ndataset quality for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.06505v1.pdf"
    },
    {
        "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering",
        "authors": [
            "Yichi Zhang",
            "Zhuo Chen",
            "Yin Fang",
            "Lei Cheng",
            "Yanxi Lu",
            "Fangming Li",
            "Wen Zhang",
            "Huajun Chen"
        ],
        "published": "2023-11-11T07:56:40Z",
        "summary": "Recently, the development of large language models (LLMs) has attracted wide\nattention in academia and industry. Deploying LLMs to real scenarios is one of\nthe key directions in the current Internet industry. In this paper, we present\na novel pipeline to apply LLMs for domain-specific question answering (QA) that\nincorporates domain knowledge graphs (KGs), addressing an important direction\nof LLM application. As a real-world application, the content generated by LLMs\nshould be user-friendly to serve the customers. Additionally, the model needs\nto utilize domain knowledge properly to generate reliable answers. These two\nissues are the two major difficulties in the LLM application as vanilla\nfine-tuning can not adequately address them. We think both requirements can be\nunified as the model preference problem that needs to align with humans to\nachieve practical application. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference set called style\npreference set and knowledge preference set respectively to tackle the two\nissues. Besides, we design a new alignment objective to align the LLM\npreference with human preference, aiming to train a better LLM for\nreal-scenario domain-specific QA to generate reliable and user-friendly\nanswers. Adequate experiments and comprehensive with 15 baseline methods\ndemonstrate that our KnowPAT is an outperforming pipeline for real-scenario\ndomain-specific QA with LLMs. Our code is open-source at\nhttps://github.com/zjukg/KnowPAT.",
        "pdf_link": "https://arxiv.org/pdf/2311.06503v1.pdf"
    },
    {
        "title": "Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks",
        "authors": [
            "Pouya Pezeshkpour",
            "Hayate Iso",
            "Thom Lake",
            "Nikita Bhutani",
            "Estevam Hruschka"
        ],
        "published": "2023-11-10T20:25:42Z",
        "summary": "Numerous HR applications are centered around resumes and job descriptions.\nWhile they can benefit from advancements in NLP, particularly large language\nmodels, their real-world adoption faces challenges due to absence of\ncomprehensive benchmarks for various HR tasks, and lack of smaller models with\ncompetitive capabilities. In this paper, we aim to bridge this gap by\nintroducing the Resume-Job Description Benchmark (RJDB). We meticulously craft\nthis benchmark to cater to a wide array of HR tasks, including matching and\nexplaining resumes to job descriptions, extracting skills and experiences from\nresumes, and editing resumes. To create this benchmark, we propose to distill\ndomain-specific knowledge from a large language model (LLM). We rely on a\ncurated skill-occupation graph to ensure diversity and provide context for LLMs\ngeneration. Our benchmark includes over 50 thousand triples of job\ndescriptions, matched resumes and unmatched resumes. Using RJDB, we train\nmultiple smaller student models. Our experiments reveal that the student models\nachieve near/better performance than the teacher model (GPT-4), affirming the\neffectiveness of the benchmark. Additionally, we explore the utility of RJDB on\nout-of-distribution data for skill extraction and resume-job description\nmatching, in zero-shot and weak supervision manner. We release our datasets and\ncode to foster further research and industry applications.",
        "pdf_link": "https://arxiv.org/pdf/2311.06383v1.pdf"
    },
    {
        "title": "ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome Management",
        "authors": [
            "Angela Zhang",
            "Mert Yuksekgonul",
            "Joshua Guild",
            "James Zou",
            "Joseph C. Wu"
        ],
        "published": "2023-11-10T19:59:36Z",
        "summary": "Recent breakthroughs in large language models (LLMs) have led to their rapid\ndissemination and widespread use. One early application has been to medicine,\nwhere LLMs have been investigated to streamline clinical workflows and\nfacilitate clinical analysis and decision-making. However, a leading barrier to\nthe deployment of Artificial Intelligence (AI) and in particular LLMs has been\nconcern for embedded gender and racial biases. Here, we evaluate whether a\nleading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical\nmanagement of acute coronary syndrome (ACS). We find that specifying patients\nas female, African American, or Hispanic resulted in a decrease in guideline\nrecommended medical management, diagnosis, and symptom management of ACS. Most\nnotably, the largest disparities were seen in the recommendation of coronary\nangiography or stress testing for the diagnosis and further intervention of ACS\nand recommendation of high intensity statins. These disparities correlate with\nbiases that have been observed clinically and have been implicated in the\ndifferential gender and racial morbidity and mortality outcomes of ACS and\ncoronary artery disease. Furthermore, we find that the largest disparities are\nseen during unstable angina, where fewer explicit clinical guidelines exist.\nFinally, we find that through asking ChatGPT 3.5 to explain its reasoning prior\nto providing an answer, we are able to improve clinical accuracy and mitigate\ninstances of gender and racial biases. This is among the first studies to\ndemonstrate that the gender and racial biases that LLMs exhibit do in fact\naffect clinical management. Additionally, we demonstrate that existing\nstrategies that improve LLM performance not only improve LLM performance in\nclinical management, but can also be used to mitigate gender and racial biases.",
        "pdf_link": "https://arxiv.org/pdf/2311.14703v1.pdf"
    },
    {
        "title": "Language Models can be Logical Solvers",
        "authors": [
            "Jiazhan Feng",
            "Ruochen Xu",
            "Junheng Hao",
            "Hiteshi Sharma",
            "Yelong Shen",
            "Dongyan Zhao",
            "Weizhu Chen"
        ],
        "published": "2023-11-10T16:23:50Z",
        "summary": "Logical reasoning is a fundamental aspect of human intelligence and a key\ncomponent of tasks like problem-solving and decision-making. Recent\nadvancements have enabled Large Language Models (LLMs) to potentially exhibit\nreasoning capabilities, but complex logical reasoning remains a challenge. The\nstate-of-the-art, solver-augmented language models, use LLMs to parse natural\nlanguage logical questions into symbolic representations first and then adopt\nexternal logical solvers to take in the symbolic representations and output the\nanswers. Despite their impressive performance, any parsing errors will\ninevitably result in the failure of the execution of the external logical\nsolver and no answer to the logical questions. In this paper, we introduce\nLoGiPT, a novel language model that directly emulates the reasoning processes\nof logical solvers and bypasses the parsing errors by learning to strict\nadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly\nconstructed instruction-tuning dataset derived from revealing and refining the\ninvisible reasoning process of deductive solvers. Experimental results on two\npublic deductive reasoning datasets demonstrate that LoGiPT outperforms\nstate-of-the-art solver-augmented LMs and few-shot prompting methods on\ncompetitive LLMs like ChatGPT or GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2311.06158v1.pdf"
    },
    {
        "title": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking",
        "authors": [
            "Lefteris Loukas",
            "Ilias Stogiannidis",
            "Odysseas Diamantopoulos",
            "Prodromos Malakasiotis",
            "Stavros Vassos"
        ],
        "published": "2023-11-10T15:10:36Z",
        "summary": "Standard Full-Data classifiers in NLP demand thousands of labeled examples,\nwhich is impractical in data-limited domains. Few-shot methods offer an\nalternative, utilizing contrastive learning techniques that can be effective\nwith as little as 20 examples per class. Similarly, Large Language Models\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\nHowever, the performance-cost trade-offs of these methods remain underexplored,\na critical concern for budget-limited organizations. Our work addresses this\ngap by studying the aforementioned approaches over the Banking77 financial\nintent detection dataset, including the evaluation of cutting-edge LLMs by\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\ncomplete the picture with two additional methods: first, a cost-effective\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\nreduce operational costs multiple times compared to classic few-shot\napproaches, and second, a data augmentation method using GPT-4, able to improve\nperformance in data-limited scenarios. Finally, to inspire future research, we\nprovide a human expert's curated subset of Banking77, along with extensive\nerror analysis.",
        "pdf_link": "https://arxiv.org/pdf/2311.06102v1.pdf"
    },
    {
        "title": "Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
        "authors": [
            "Wenjie Fu",
            "Huandong Wang",
            "Chen Gao",
            "Guanghua Liu",
            "Yong Li",
            "Tao Jiang"
        ],
        "published": "2023-11-10T13:55:05Z",
        "summary": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.",
        "pdf_link": "https://arxiv.org/pdf/2311.06062v2.pdf"
    },
    {
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences",
        "authors": [
            "Yuanhe Tian",
            "Ruyi Gan",
            "Yan Song",
            "Jiaxing Zhang",
            "Yongdong Zhang"
        ],
        "published": "2023-11-10T12:25:32Z",
        "summary": "Recently, the increasing demand for superior medical services has highlighted\nthe discrepancies in the medical infrastructure. With big data, especially\ntexts, forming the foundation of medical services, there is an exigent need for\neffective natural language processing (NLP) solutions tailored to the\nhealthcare domain. Conventional approaches leveraging pre-trained models\npresent promising results in this domain and current large language models\n(LLMs) offer advanced foundation for medical text processing. However, most\nmedical LLMs are trained only with supervised fine-tuning (SFT), even though it\nefficiently empowers LLMs to understand and respond to medical instructions but\nis ineffective in learning domain knowledge and aligning with human preference.\nAnother engineering barrier that prevents current medical LLM from better text\nprocessing ability is their restricted context length (e.g., 2,048 tokens),\nmaking it hard for the LLMs to process long context, which is frequently\nrequired in the medical domain. In this work, we propose ChiMed-GPT, a new\nbenchmark LLM designed explicitly for Chinese medical domain, with enlarged\ncontext length to 4,096 tokens and undergoes a comprehensive training regime\nwith pre-training, SFT, and RLHF. Evaluations on real-world tasks including\ninformation extraction, question answering, and dialogue generation demonstrate\nChiMed-GPT's superior performance over general domain LLMs. Furthermore, we\nanalyze possible biases through prompting ChiMed-GPT to perform attitude scales\nregarding discrimination of patients, so as to contribute to further\nresponsible development of LLMs in the medical domain. The code and model are\nreleased at https://github.com/synlp/ChiMed-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2311.06025v2.pdf"
    },
    {
        "title": "How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model",
        "authors": [
            "Shezheng Song",
            "Xiaopeng Li",
            "Shasha Li",
            "Shan Zhao",
            "Jie Yu",
            "Jun Ma",
            "Xiaoguang Mao",
            "Weimin Zhang"
        ],
        "published": "2023-11-10T09:51:24Z",
        "summary": "This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.",
        "pdf_link": "https://arxiv.org/pdf/2311.07594v2.pdf"
    },
    {
        "title": "Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications",
        "authors": [
            "Zhangyin Feng",
            "Weitao Ma",
            "Weijiang Yu",
            "Lei Huang",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting liu"
        ],
        "published": "2023-11-10T05:24:04Z",
        "summary": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2311.05876v2.pdf"
    },
    {
        "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service",
        "authors": [
            "Yuanmin Tang",
            "Jing Yu",
            "Keke Gai",
            "Xiangyan Qu",
            "Yue Hu",
            "Gang Xiong",
            "Qi Wu"
        ],
        "published": "2023-11-10T04:27:27Z",
        "summary": "Recent advances in vision-language pre-trained models (VLPs) have\nsignificantly increased visual understanding and cross-modal analysis\ncapabilities. Companies have emerged to provide multi-modal Embedding as a\nService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount\nof training data and resources for high-performance service. However, existing\nstudies indicate that EaaS is vulnerable to model extraction attacks that\ninduce great loss for the owners of VLPs. Protecting the intellectual property\nand commercial ownership of VLPs is increasingly crucial yet challenging. A\nmajor solution of watermarking model for EaaS implants a backdoor in the model\nby inserting verifiable trigger embeddings into texts, but it is only\napplicable for large language models and is unrealistic due to data and model\nprivacy. In this paper, we propose a safe and robust backdoor-based embedding\nwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding\northogonal transformation to effectively inject triggers into the VLPs without\ninterfering with the model parameters, which achieves high-quality copyright\nverification and minimal impact on model performance. To enhance the watermark\nrobustness, we further propose a collaborative copyright verification strategy\nbased on both backdoor trigger and embedding distribution, enhancing resilience\nagainst various attacks. We increase the watermark practicality via an\nout-of-distribution trigger selection approach, removing access to the model\ntraining data and thus making it possible for many real-world scenarios. Our\nextensive experiments on various datasets indicate that the proposed\nwatermarking approach is effective and safe for verifying the copyright of VLPs\nfor multi-modal EaaS and robust against model extraction attacks. Our code is\navailable at https://github.com/Pter61/vlpmarker.",
        "pdf_link": "https://arxiv.org/pdf/2311.05863v1.pdf"
    },
    {
        "title": "Tamil-Llama: A New Tamil Language Model Based on Llama 2",
        "authors": [
            "Abhinand Balachandran"
        ],
        "published": "2023-11-10T03:02:39Z",
        "summary": "Language modeling has witnessed remarkable advancements in recent years, with\nLarge Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in\nhuman-like text generation. However, a prevailing limitation is the\nunderrepresentation of languages like Tamil in these cutting-edge models,\nleading to suboptimal performance in diverse linguistic contexts. This paper\naddresses this lacuna, enhancing the open-source LLaMA model with an addition\nof 16,000 Tamil tokens, aiming to achieve superior text generation and\ncomprehension in the Tamil language. We strategically employ the LoRA\nmethodology for efficient model training on a comprehensive Tamil corpus,\nensuring computational feasibility and model robustness. Moreover, we introduce\na Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca\ndataset tailored for instruction fine-tuning. Our results showcase significant\nperformance improvements in Tamil text generation, with potential implications\nfor the broader landscape of LLMs in Indian languages. We further underscore\nour commitment to open research by making our models, datasets, and code\npublicly accessible, fostering further innovations in language modeling.",
        "pdf_link": "https://arxiv.org/pdf/2311.05845v1.pdf"
    },
    {
        "title": "Hallucination-minimized Data-to-answer Framework for Financial Decision-makers",
        "authors": [
            "Sohini Roychowdhury",
            "Andres Alvarez",
            "Brian Moore",
            "Marko Krema",
            "Maria Paz Gelpi",
            "Federico Martin Rodriguez",
            "Angel Rodriguez",
            "Jose Ramon Cabrejas",
            "Pablo Martinez Serrano",
            "Punit Agrawal",
            "Arijit Mukherjee"
        ],
        "published": "2023-11-09T22:53:52Z",
        "summary": "Large Language Models (LLMs) have been applied to build several automation\nand personalized question-answering prototypes so far. However, scaling such\nprototypes to robust products with minimized hallucinations or fake responses\nstill remains an open challenge, especially in niche data-table heavy domains\nsuch as financial decision making. In this work, we present a novel\nLangchain-based framework that transforms data tables into hierarchical textual\ndata chunks to enable a wide variety of actionable question answering. First,\nthe user-queries are classified by intention followed by automated retrieval of\nthe most relevant data chunks to generate customized LLM prompts per query.\nNext, the custom prompts and their responses undergo multi-metric scoring to\nassess for hallucinations and response confidence. The proposed system is\noptimized with user-query intention classification, advanced prompting, data\nscaling capabilities and it achieves over 90% confidence scores for a variety\nof user-queries responses ranging from {What, Where, Why, How, predict, trend,\nanomalies, exceptions} that are crucial for financial decision making\napplications. The proposed data to answers framework can be extended to other\nanalytical domains such as sales and payroll to ensure optimal hallucination\ncontrol guardrails.",
        "pdf_link": "https://arxiv.org/pdf/2311.07592v1.pdf"
    },
    {
        "title": "Efficiently Adapting Pretrained Language Models To New Languages",
        "authors": [
            "Zoltan Csaki",
            "Pian Pawakapan",
            "Urmish Thakker",
            "Qiantong Xu"
        ],
        "published": "2023-11-09T20:59:08Z",
        "summary": "Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.",
        "pdf_link": "https://arxiv.org/pdf/2311.05741v2.pdf"
    },
    {
        "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
        "authors": [
            "Simon Stepputtis",
            "Joseph Campbell",
            "Yaqi Xie",
            "Zhengyang Qi",
            "Wenxin Sharon Zhang",
            "Ruiyi Wang",
            "Sanketh Rangreji",
            "Michael Lewis",
            "Katia Sycara"
        ],
        "published": "2023-11-09T20:04:08Z",
        "summary": "Deception and persuasion play a critical role in long-horizon dialogues\nbetween multiple parties, especially when the interests, goals, and motivations\nof the participants are not aligned. Such complex tasks pose challenges for\ncurrent Large Language Models (LLM) as deception and persuasion can easily\nmislead them, especially in long-horizon multi-party dialogues. To this end, we\nexplore the game of Avalon: The Resistance, a social deduction game in which\nplayers must determine each other's hidden identities to complete their team's\nobjective. We introduce an online testbed and a dataset containing 20 carefully\ncollected and labeled games among human players that exhibit long-horizon\ndeception in a cooperative-competitive setting. We discuss the capabilities of\nLLMs to utilize deceptive long-horizon conversations between six human players\nto determine each player's goal and motivation. Particularly, we discuss the\nmultimodal integration of the chat between the players and the game's state\nthat grounds the conversation, providing further insights into the true player\nidentities. We find that even current state-of-the-art LLMs do not reach human\nperformance, making our dataset a compelling benchmark to investigate the\ndecision-making and language-processing capabilities of LLMs. Our dataset and\nonline testbed can be found at our project website:\nhttps://sstepput.github.io/Avalon-NLU/",
        "pdf_link": "https://arxiv.org/pdf/2311.05720v1.pdf"
    },
    {
        "title": "Conversational AI Threads for Visualizing Multidimensional Datasets",
        "authors": [
            "Matt-Heun Hong",
            "Anamaria Crisan"
        ],
        "published": "2023-11-09T18:47:46Z",
        "summary": "Generative Large Language Models (LLMs) show potential in data analysis, yet\ntheir full capabilities remain uncharted. Our work explores the capabilities of\nLLMs for creating and refining visualizations via conversational interfaces. We\nused an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examining\nthe use of chatbots for conducting visual analysis. We surfaced the strengths\nand weaknesses of LLM-driven analytic chatbots, finding that they fell short in\nsupporting progressive visualization refinements. From these findings, we\ndeveloped AI Threads, a multi-threaded analytic chatbot that enables analysts\nto proactively manage conversational context and improve the efficacy of its\noutputs. We evaluate its usability through a crowdsourced study (n=40) and\nin-depth interviews with expert analysts (n=10). We further demonstrate the\ncapabilities of AI Threads on a dataset outside the LLM's training corpus. Our\nfindings show the potential of LLMs while also surfacing challenges and\nfruitful avenues for future research.",
        "pdf_link": "https://arxiv.org/pdf/2311.05590v1.pdf"
    },
    {
        "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations",
        "authors": [
            "Joey Hong",
            "Sergey Levine",
            "Anca Dragan"
        ],
        "published": "2023-11-09T18:45:16Z",
        "summary": "Large language models (LLMs) have emerged as powerful and general solutions\nto many natural language tasks. However, many of the most important\napplications of language generation are interactive, where an agent has to talk\nto a person to reach a desired outcome. For example, a teacher might try to\nunderstand their student's current comprehension level to tailor their\ninstruction accordingly, and a travel agent might ask questions of their\ncustomer to understand their preferences in order to recommend activities they\nmight enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as\nwith standard RLHF, might struggle which tasks that require such goal-directed\nbehavior, since they are not trained to optimize for overall conversational\noutcomes after multiple turns of interaction. In this work, we explore a new\nmethod for adapting LLMs with RL for such goal-directed dialogue. Our key\ninsight is that, though LLMs might not effectively solve goal-directed dialogue\ntasks out of the box, they can provide useful data for solving such tasks by\nsimulating suboptimal but human-like behaviors. Given a textual description of\na goal-directed dialogue task, we leverage LLMs to sample diverse synthetic\nrollouts of hypothetical in-domain human-human interactions. Our algorithm then\nutilizes this dataset with offline reinforcement learning to train an\ninteractive conversational agent that can optimize goal-directed objectives\nover multiple turns. In effect, the LLM produces examples of possible\ninteractions, and RL then processes these examples to learn to perform more\noptimal interactions. Empirically, we show that our proposed approach achieves\nstate-of-the-art performance in various goal-directed dialogue tasks that\ninclude teaching and preference elicitation.",
        "pdf_link": "https://arxiv.org/pdf/2311.05584v1.pdf"
    },
    {
        "title": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
        "authors": [
            "Qiusi Zhan",
            "Richard Fang",
            "Rohan Bindu",
            "Akul Gupta",
            "Tatsunori Hashimoto",
            "Daniel Kang"
        ],
        "published": "2023-11-09T17:54:59Z",
        "summary": "As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the\ncontrary: fine-tuning allows attackers to remove RLHF protections with as few\nas 340 examples and a 95% success rate. These training examples can be\nautomatically generated with weaker models. We further show that removing RLHF\nprotections does not decrease usefulness on non-censored outputs, providing\nevidence that our fine-tuning strategy does not decrease usefulness despite\nusing weaker models to generate training data. Our results show the need for\nfurther research on protections on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.05553v3.pdf"
    },
    {
        "title": "Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure",
        "authors": [
            "J√©r√©my Scheurer",
            "Mikita Balesni",
            "Marius Hobbhahn"
        ],
        "published": "2023-11-09T17:12:44Z",
        "summary": "We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.",
        "pdf_link": "https://arxiv.org/pdf/2311.07590v2.pdf"
    },
    {
        "title": "Do personality tests generalize to Large Language Models?",
        "authors": [
            "Florian E. Dorner",
            "Tom S√ºhr",
            "Samira Samadi",
            "Augustin Kelava"
        ],
        "published": "2023-11-09T11:54:01Z",
        "summary": "With large language models (LLMs) appearing to behave increasingly human-like\nin text-based interactions, it has become popular to attempt to evaluate\nvarious properties of these models using tests originally designed for humans.\nWhile re-using existing tests is a resource-efficient way to evaluate LLMs,\ncareful adjustments are usually required to ensure that test results are even\nvalid across human sub-populations. Thus, it is not clear to what extent\ndifferent tests' validity generalizes to LLMs. In this work, we provide\nevidence that LLMs' responses to personality tests systematically deviate from\ntypical human responses, implying that these results cannot be interpreted in\nthe same way as human test results. Concretely, reverse-coded items (e.g. \"I am\nintroverted\" vs \"I am extraverted\") are often both answered affirmatively by\nLLMs. In addition, variation across different prompts designed to \"steer\" LLMs\nto simulate particular personality types does not follow the clear separation\ninto five independent personality factors from human samples. In light of these\nresults, we believe it is important to pay more attention to tests' validity\nfor LLMs before drawing strong conclusions about potentially ill-defined\nconcepts like LLMs' \"personality\".",
        "pdf_link": "https://arxiv.org/pdf/2311.05297v1.pdf"
    },
    {
        "title": "BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings",
        "authors": [
            "Xianming Li",
            "Jing Li"
        ],
        "published": "2023-11-09T11:53:52Z",
        "summary": "Sentence embeddings are crucial in measuring semantic similarity. Most recent\nstudies employed large language models (LLMs) to learn sentence embeddings.\nExisting LLMs mainly adopted autoregressive architecture without explicit\nbackward dependency modeling. Therefore, we examined the effects of backward\ndependencies in LLMs for semantic similarity measurements. Concretely, we\npropose a novel model: backward dependency enhanced large language model\n(BeLLM). It learns sentence embeddings via transforming specific attention\nlayers from uni- to bi-directional. We extensively experiment across various\nsemantic textual similarity (STS) tasks and downstream applications. BeLLM\nachieves state-of-the-art performance in varying scenarios. It shows that\nauto-regressive LLMs benefit from backward dependencies for sentence\nembeddings.",
        "pdf_link": "https://arxiv.org/pdf/2311.05296v2.pdf"
    },
    {
        "title": "Chain of Images for Intuitively Reasoning",
        "authors": [
            "Fanxu Meng",
            "Haotong Yang",
            "Yiding Wang",
            "Muhan Zhang"
        ],
        "published": "2023-11-09T11:14:51Z",
        "summary": "The human brain is naturally equipped to comprehend and interpret visual\ninformation rapidly. When confronted with complex problems or concepts, we use\nflowcharts, sketches, and diagrams to aid our thought process. Leveraging this\ninherent ability can significantly enhance logical reasoning. However, current\nLarge Language Models (LLMs) do not utilize such visual intuition to help their\nthinking. Even the most advanced version language models (e.g., GPT-4V and\nLLaVA) merely align images into textual space, which means their reasoning\nprocesses remain purely verbal. To mitigate such limitations, we present a\nChain of Images (CoI) approach, which can convert complex language reasoning\nproblems to simple pattern recognition by generating a series of images as\nintermediate representations. Furthermore, we have developed a CoI evaluation\ndataset encompassing 15 distinct domains where images can intuitively aid\nproblem-solving. Based on this dataset, we aim to construct a benchmark to\nassess the capability of future multimodal large-scale models to leverage\nimages for reasoning. In supporting our CoI reasoning, we introduce a symbolic\nmultimodal large language model (SyMLLM) that generates images strictly based\non language instructions and accepts both text and image as input. Experiments\non Geometry, Chess and Common Sense tasks sourced from the CoI evaluation\ndataset show that CoI improves performance significantly over the pure-language\nChain of Thoughts (CoT) baselines. The code is available at\nhttps://github.com/GraphPKU/CoI.",
        "pdf_link": "https://arxiv.org/pdf/2311.09241v1.pdf"
    },
    {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "authors": [
            "Lei Huang",
            "Weijiang Yu",
            "Weitao Ma",
            "Weihong Zhong",
            "Zhangyin Feng",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-11-09T09:25:37Z",
        "summary": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.05232v1.pdf"
    },
    {
        "title": "Prompt Engineering a Prompt Engineer",
        "authors": [
            "Qinyuan Ye",
            "Maxamed Axmed",
            "Reid Pryzant",
            "Fereshte Khani"
        ],
        "published": "2023-11-09T08:00:32Z",
        "summary": "Prompt engineering is a challenging yet crucial task for optimizing the\nperformance of large language models on customized tasks. It requires complex\nreasoning to examine the model's errors, hypothesize what is missing or\nmisleading in the current prompt, and communicate the task with clarity. While\nrecent works indicate that large language models can be meta-prompted to\nperform automatic prompt engineering, we argue that their potential is limited\ndue to insufficient guidance for complex reasoning in the meta-prompt. We fill\nthis gap by infusing into the meta-prompt three key components: detailed\ndescriptions, context specification, and a step-by-step reasoning template. The\nresulting method, named PE2, showcases remarkable versatility across diverse\nlanguage tasks. It finds prompts that outperform \"let's think step by step\" by\n6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on\ncounterfactual tasks by 6.9%. Further, we show that PE2 can make targeted\nprompt edits, rectify erroneous prompts, and induce multi-step plans for\ncomplex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.05661v2.pdf"
    },
    {
        "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
        "authors": [
            "Aditi Mishra",
            "Sajjadur Rahman",
            "Hannah Kim",
            "Kushan Mitra",
            "Estevam Hruschka"
        ],
        "published": "2023-11-09T01:04:44Z",
        "summary": "Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.",
        "pdf_link": "https://arxiv.org/pdf/2311.05085v2.pdf"
    },
    {
        "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
        "authors": [
            "C. Daniel Freeman",
            "Laura Culp",
            "Aaron Parisi",
            "Maxwell L Bileschi",
            "Gamaleldin F Elsayed",
            "Alex Rizkowsky",
            "Isabelle Simpson",
            "Alex Alemi",
            "Azade Nova",
            "Ben Adlam",
            "Bernd Bohnet",
            "Gaurav Mishra",
            "Hanie Sedghi",
            "Igor Mordatch",
            "Izzeddin Gur",
            "Jaehoon Lee",
            "JD Co-Reyes",
            "Jeffrey Pennington",
            "Kelvin Xu",
            "Kevin Swersky",
            "Kshiteej Mahajan",
            "Lechao Xiao",
            "Rosanne Liu",
            "Simon Kornblith",
            "Noah Constant",
            "Peter J. Liu",
            "Roman Novak",
            "Yundi Qian",
            "Noah Fiedel",
            "Jascha Sohl-Dickstein"
        ],
        "published": "2023-11-08T19:07:10Z",
        "summary": "We introduce and study the problem of adversarial arithmetic, which provides\na simple yet challenging testbed for language model alignment. This problem is\ncomprised of arithmetic questions posed in natural language, with an arbitrary\nadversarial string inserted before the question is complete. Even in the simple\nsetting of 1-digit addition problems, it is easy to find adversarial prompts\nthat make all tested models (including PaLM2, GPT4, Claude2) misbehave, and\neven to steer models to a particular wrong answer. We additionally provide a\nsimple algorithm for finding successful attacks by querying those same models,\nwhich we name \"prompt inversion rejection sampling\" (PIRS). We finally show\nthat models can be partially hardened against these attacks via reinforcement\nlearning and via agentic constitutional loops. However, we were not able to\nmake a language model fully robust against adversarial arithmetic attacks.",
        "pdf_link": "https://arxiv.org/pdf/2311.07587v2.pdf"
    },
    {
        "title": "How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure",
        "authors": [
            "Michael Wilson",
            "Jackson Petty",
            "Robert Frank"
        ],
        "published": "2023-11-08T18:58:43Z",
        "summary": "Language models are typically evaluated on their success at predicting the\ndistribution of specific words in specific contexts. Yet linguistic knowledge\nalso encodes relationships between contexts, allowing inferences between word\ndistributions. We investigate the degree to which pre-trained Transformer-based\nlarge language models (LLMs) represent such relationships, focusing on the\ndomain of argument structure. We find that LLMs perform well in generalizing\nthe distribution of a novel noun argument between related contexts that were\nseen during pre-training (e.g., the active object and passive subject of the\nverb spray), succeeding by making use of the semantically-organized structure\nof the embedding space for word embeddings. However, LLMs fail at\ngeneralizations between related contexts that have not been observed during\npre-training, but which instantiate more abstract, but well-attested structural\ngeneralizations (e.g., between the active object and passive subject of an\narbitrary verb). Instead, in this case, LLMs show a bias to generalize based on\nlinear order. This finding points to a limitation with current models and\npoints to a reason for which their training is data-intensive.s reported here\nare available at https://github.com/clay-lab/structural-alternations.",
        "pdf_link": "https://arxiv.org/pdf/2311.04900v1.pdf"
    },
    {
        "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
        "authors": [
            "Shashank Gupta",
            "Vaishnavi Shrivastava",
            "Ameet Deshpande",
            "Ashwin Kalyan",
            "Peter Clark",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "published": "2023-11-08T18:52:17Z",
        "summary": "Recent works have showcased the ability of LLMs to embody diverse personas in\ntheir responses, exemplified by prompts like 'You are Yoda. Explain the Theory\nof Relativity.' While this ability allows personalization of LLMs and enables\nhuman behavior simulation, its effect on LLMs' capabilities remains unclear. To\nfill this gap, we present the first extensive study of the unintended\nside-effects of persona assignment on the ability of LLMs to perform basic\nreasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse\npersonas (e.g. an Asian person) spanning 5 socio-demographic groups. Our\nexperiments unveil that LLMs harbor deep rooted bias against various\nsocio-demographics underneath a veneer of fairness. While they overtly reject\nstereotypes when explicitly asked ('Are Black people less skilled at\nmathematics?'), they manifest stereotypical and erroneous presumptions when\nasked to answer questions while adopting a persona. These can be observed as\nabstentions in responses, e.g., 'As a Black person, I can't answer this\nquestion as it requires math knowledge', and generally result in a substantial\nperformance drop. Our experiments with ChatGPT-3.5 show that this bias is\nubiquitous - 80% of our personas demonstrate bias; it is significant - some\ndatasets show performance drops of 70%+; and can be especially harmful for\ncertain groups - some personas suffer statistically significant drops on 80%+\nof the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with\nGPT-4-Turbo showing the least but still a problematic amount of bias (evident\nin 42% of the personas). Further analysis shows that these persona-induced\nerrors can be hard-to-discern and hard-to-avoid. Our findings serve as a\ncautionary tale that the practice of assigning personas to LLMs - a trend on\nthe rise - can surface their deep-rooted biases and have unforeseeable and\ndetrimental side-effects.",
        "pdf_link": "https://arxiv.org/pdf/2311.04892v2.pdf"
    },
    {
        "title": "SEMQA: Semi-Extractive Multi-Source Question Answering",
        "authors": [
            "Tal Schuster",
            "Adam D. Lelkes",
            "Haitian Sun",
            "Jai Gupta",
            "Jonathan Berant",
            "William W. Cohen",
            "Donald Metzler"
        ],
        "published": "2023-11-08T18:46:32Z",
        "summary": "Recently proposed long-form question answering (QA) systems, supported by\nlarge language models (LLMs), have shown promising capabilities. Yet,\nattributing and verifying their generated abstractive answers can be difficult,\nand automatically evaluating their accuracy remains an ongoing challenge.\n  In this work, we introduce a new QA task for answering multi-answer questions\nby summarizing multiple diverse sources in a semi-extractive fashion.\nSpecifically, Semi-extractive Multi-source QA (SEMQA) requires models to output\na comprehensive answer, while mixing factual quoted spans -- copied verbatim\nfrom given input sources -- and non-factual free-text connectors that glue\nthese spans together into a single cohesive passage. This setting bridges the\ngap between the outputs of well-grounded but constrained extractive QA systems\nand more fluent but harder to attribute fully abstractive answers.\nParticularly, it enables a new mode for language models that leverages their\nadvanced language generation capabilities, while also producing fine in-line\nattributions by-design that are easy to verify, interpret, and evaluate.\n  To study this task, we create the first dataset of this kind, QuoteSum, with\nhuman-written semi-extractive answers to natural and generated questions, and\ndefine text-based evaluation metrics. Experimenting with several LLMs in\nvarious settings, we find this task to be surprisingly challenging,\ndemonstrating the importance of QuoteSum for developing and studying such\nconsolidation capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.04886v1.pdf"
    },
    {
        "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
        "authors": [
            "Shuo Yang",
            "Wei-Lin Chiang",
            "Lianmin Zheng",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-11-08T17:35:20Z",
        "summary": "Large language models are increasingly trained on all the data ever produced\nby humans. Many have raised concerns about the trustworthiness of public\nbenchmarks due to potential contamination in pre-training or fine-tuning\ndatasets. While most data decontamination efforts apply string matching (e.g.,\nn-gram overlap) to remove benchmark data, we show that these methods are\ninsufficient, and simple variations of test data (e.g., paraphrasing,\ntranslation) can easily bypass these decontamination measures. Furthermore, we\ndemonstrate that if such variation of test data is not eliminated, a 13B model\ncan easily overfit a test benchmark and achieve drastically high performance,\non par with GPT-4. We validate such observations in widely used benchmarks such\nas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a\nstronger LLM-based decontamination method and apply it to widely used\npre-training and fine-tuning datasets, revealing significant previously unknown\ntest overlap. For example, in pre-training sets such as RedPajama-Data-1T and\nStarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps.\nInterestingly, we also find such contamination in synthetic dataset generated\nby GPT-3.5/4, suggesting a potential risk of unintentional contamination. We\nurge the community to adopt stronger decontamination approaches when using\npublic benchmarks. Moreover, we call for the community to actively develop\nfresh one-time exams to evaluate models accurately. Our decontamination tool is\npublicly available at https://github.com/lm-sys/llm-decontaminator.",
        "pdf_link": "https://arxiv.org/pdf/2311.04850v2.pdf"
    },
    {
        "title": "Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection",
        "authors": [
            "Zhengyuan Liu",
            "Hai Leong Chieu",
            "Nancy F. Chen"
        ],
        "published": "2023-11-08T06:54:34Z",
        "summary": "Data collection from manual labeling provides domain-specific and\ntask-aligned supervision for data-driven approaches, and a critical mass of\nwell-annotated resources is required to achieve reasonable performance in\nnatural language processing tasks. However, manual annotations are often\nchallenging to scale up in terms of time and budget, especially when domain\nknowledge, capturing subtle semantic features, and reasoning steps are needed.\nIn this paper, we investigate the efficacy of leveraging large language models\non automated labeling for computational stance detection. We empirically\nobserve that while large language models show strong potential as an\nalternative to human annotators, their sensitivity to task-specific\ninstructions and their intrinsic biases pose intriguing yet unique challenges\nin machine annotation. We introduce a multi-label and multi-target sampling\nstrategy to optimize the annotation quality. Experimental results on the\nbenchmark stance detection corpora show that our method can significantly\nimprove performance and learning efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2311.04495v1.pdf"
    },
    {
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
        "authors": [
            "Jiaqi Li",
            "Mengmeng Wang",
            "Zilong Zheng",
            "Muhan Zhang"
        ],
        "published": "2023-11-08T01:45:37Z",
        "summary": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".",
        "pdf_link": "https://arxiv.org/pdf/2311.04939v1.pdf"
    },
    {
        "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
        "authors": [
            "Hanlin Zhang",
            "Benjamin L. Edelman",
            "Danilo Francati",
            "Daniele Venturi",
            "Giuseppe Ateniese",
            "Boaz Barak"
        ],
        "published": "2023-11-07T22:52:54Z",
        "summary": "Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2311.04378v2.pdf"
    },
    {
        "title": "Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning",
        "authors": [
            "Sai Munikoti",
            "Anurag Acharya",
            "Sridevi Wagle",
            "Sameera Horawalavithana"
        ],
        "published": "2023-11-07T21:09:57Z",
        "summary": "Despite the dramatic progress in Large Language Model (LLM) development, LLMs\noften provide seemingly plausible but not factual information, often referred\nto as hallucinations. Retrieval-augmented LLMs provide a non-parametric\napproach to solve these issues by retrieving relevant information from external\ndata sources and augment the training process. These models help to trace\nevidence from an externally provided knowledge base allowing the model\npredictions to be better interpreted and verified. In this work, we critically\nevaluate these models in their ability to perform in scientific document\nreasoning tasks. To this end, we tuned multiple such model variants with\nscience-focused instructions and evaluated them on a scientific document\nreasoning benchmark for the usefulness of the retrieved document passages. Our\nfindings suggest that models justify predictions in science tasks with\nfabricated evidence and leveraging scientific corpus as pretraining data does\nnot alleviate the risk of evidence fabrication.",
        "pdf_link": "https://arxiv.org/pdf/2311.04348v1.pdf"
    },
    {
        "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications",
        "authors": [
            "Fengqing Jiang",
            "Zhangchen Xu",
            "Luyao Niu",
            "Boxin Wang",
            "Jinyuan Jia",
            "Bo Li",
            "Radha Poovendran"
        ],
        "published": "2023-11-07T20:13:05Z",
        "summary": "Large language models (LLMs) are increasingly deployed as the service backend\nfor LLM-integrated applications such as code completion and AI-powered search.\nLLM-integrated applications serve as middleware to refine users' queries with\ndomain-specific knowledge to better inform LLMs and enhance the responses.\nDespite numerous opportunities and benefits, LLM-integrated applications also\nintroduce new attack surfaces. Understanding, minimizing, and eliminating these\nemerging attack surfaces is a new area of research. In this work, we consider a\nsetup where the user and LLM interact via an LLM-integrated application in the\nmiddle. We focus on the communication rounds that begin with user's queries and\nend with LLM-integrated application returning responses to the queries, powered\nby LLMs at the service backend. For this query-response protocol, we identify\npotential vulnerabilities that can originate from the malicious application\ndeveloper or from an outsider threat initiator that is able to control the\ndatabase access, manipulate and poison data that are high-risk for the user.\nSuccessful exploits of the identified vulnerabilities result in the users\nreceiving responses tailored to the intent of a threat initiator. We assess\nsuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5\nand GPT-4. Our empirical results show that the threats can effectively bypass\nthe restrictions and moderation policies of OpenAI, resulting in users\nreceiving responses that contain bias, toxic content, privacy risk, and\ndisinformation. To mitigate those threats, we identify and define four key\nproperties, namely integrity, source identification, attack detectability, and\nutility preservation, that need to be satisfied by a safe LLM-integrated\napplication. Based on these properties, we develop a lightweight,\nthreat-agnostic defense that mitigates both insider and outsider threats.",
        "pdf_link": "https://arxiv.org/pdf/2311.16153v2.pdf"
    },
    {
        "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study",
        "authors": [
            "Peilin Zhou",
            "Meng Cao",
            "You-Liang Huang",
            "Qichen Ye",
            "Peiyan Zhang",
            "Junling Liu",
            "Yueqi Xie",
            "Yining Hua",
            "Jaeboum Kim"
        ],
        "published": "2023-11-07T18:39:10Z",
        "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance\nacross various vision and language tasks, yet their potential applications in\nrecommendation tasks with visual assistance remain unexplored. To bridge this\ngap, we present a preliminary case study investigating the recommendation\ncapabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a\nseries of qualitative test samples spanning multiple domains and employ these\nsamples to assess the quality of GPT-4V's responses within recommendation\nscenarios. Evaluation results on these test samples prove that GPT-4V has\nremarkable zero-shot recommendation abilities across diverse domains, thanks to\nits robust visual-text comprehension capabilities and extensive general\nknowledge. However, we have also identified some limitations in using GPT-4V\nfor recommendations, including a tendency to provide similar responses when\ngiven similar inputs. This report concludes with an in-depth discussion of the\nchallenges and research opportunities associated with utilizing GPT-4V in\nrecommendation scenarios. Our objective is to explore the potential of\nextending LMMs from vision and language tasks to recommendation tasks. We hope\nto inspire further research into next-generation multimodal generative\nrecommendation models, which can enhance user experiences by offering greater\ndiversity and interactivity. All images and prompts used in this report will be\naccessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
        "pdf_link": "https://arxiv.org/pdf/2311.04199v1.pdf"
    },
    {
        "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
        "authors": [
            "In Gim",
            "Guojun Chen",
            "Seung-seob Lee",
            "Nikhil Sarda",
            "Anurag Khandelwal",
            "Lin Zhong"
        ],
        "published": "2023-11-07T18:17:05Z",
        "summary": "We present Prompt Cache, an approach for accelerating inference for large\nlanguage models (LLM) by reusing attention states across different LLM prompts.\nMany input prompts have overlapping text segments, such as system messages,\nprompt templates, and documents provided for context. Our key insight is that\nby precomputing and storing the attention states of these frequently occurring\ntext segments on the inference server, we can efficiently reuse them when these\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\ndefine such reusable text segments, called prompt modules. The schema ensures\npositional accuracy during attention state reuse and provides users with an\ninterface to access cached states in their prompt. Using a prototype\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\nPrompt Cache significantly reduce latency in time-to-first-token, especially\nfor longer prompts such as document-based question answering and\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\nfor CPU-based inference, all while maintaining output accuracy and without the\nneed for model parameter modifications.",
        "pdf_link": "https://arxiv.org/pdf/2311.04934v1.pdf"
    },
    {
        "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
        "authors": [
            "Eric Melz"
        ],
        "published": "2023-11-07T18:03:23Z",
        "summary": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,\n(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of\nperforming amazing tasks typically necessitating human-level intelligence.\nHowever, unlike humans, frozen LLMs do not improve over time; they neither\nacquire new knowledge nor learn from their successes or failures. Some\napproaches to improving the intelligence of LLMs include fine-tuning models\nbased on problem-solving performance (Zelikman et al., 2022), and building\nbigger and more sophisticated models (Bubeck et al., 2023). However, these\nmethods have the drawback of requiring substantial data and computational\nresources to retrain existing models. In this paper, we explore the use of\nRetrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to\nimprove problem-solving performance. We propose ARM-RAG (Auxiliary Rationale\nMemory for Retrieval Augmented Generation), a system that learns from its\nsuccesses without incurring high training costs. We demonstrate that the\nstorage and subsequent retrieval of reasoning chains have a positive influence\non performance in grade-school math problems.",
        "pdf_link": "https://arxiv.org/pdf/2311.04177v1.pdf"
    },
    {
        "title": "Unveiling Safety Vulnerabilities of Large Language Models",
        "authors": [
            "George Kour",
            "Marcel Zalmanovici",
            "Naama Zwerdling",
            "Esther Goldbraich",
            "Ora Nova Fandina",
            "Ateret Anaby-Tavor",
            "Orna Raz",
            "Eitan Farchi"
        ],
        "published": "2023-11-07T16:50:33Z",
        "summary": "As large language models become more prevalent, their possible harmful or\ninappropriate responses are a cause for concern. This paper introduces a unique\ndataset containing adversarial examples in the form of questions, which we call\nAttaQ, designed to provoke such harmful or inappropriate responses. We assess\nthe efficacy of our dataset by analyzing the vulnerabilities of various models\nwhen subjected to it. Additionally, we introduce a novel automatic approach for\nidentifying and naming vulnerable semantic regions - input semantic areas for\nwhich the model is likely to produce harmful outputs. This is achieved through\nthe application of specialized clustering techniques that consider both the\nsemantic similarity of the input attacks and the harmfulness of the model's\nresponses. Automatically identifying vulnerable semantic regions enhances the\nevaluation of model weaknesses, facilitating targeted improvements to its\nsafety mechanisms and overall reliability.",
        "pdf_link": "https://arxiv.org/pdf/2311.04124v1.pdf"
    },
    {
        "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
        "authors": [
            "Lindia Tjuatja",
            "Valerie Chen",
            "Sherry Tongshuang Wu",
            "Ameet Talwalkar",
            "Graham Neubig"
        ],
        "published": "2023-11-07T15:40:43Z",
        "summary": "As large language models (LLMs) become more capable, there is growing\nexcitement about the possibility of using LLMs as proxies for humans in\nreal-world tasks where subjective labels are desired, such as in surveys and\nopinion polling. One widely-cited barrier to the adoption of LLMs as proxies\nfor humans in subjective tasks is their sensitivity to prompt wording - but\ninterestingly, humans also display sensitivities to instruction changes in the\nform of response biases. We investigate the extent to which LLMs reflect human\nresponse biases, if at all. We look to survey design, where human response\nbiases caused by changes in the wordings of \"prompts\" have been extensively\nexplored in social psychology literature. Drawing from these works, we design a\ndataset and framework to evaluate whether LLMs exhibit human-like response\nbiases in survey questionnaires. Our comprehensive evaluation of nine models\nshows that popular open and commercial LLMs generally fail to reflect\nhuman-like behavior, particularly in models that have undergone RLHF.\nFurthermore, even if a model shows a significant change in the same direction\nas humans, we find that they are sensitive to perturbations that do not elicit\nsignificant changes in humans. These results highlight the pitfalls of using\nLLMs as human proxies, and underscore the need for finer-grained\ncharacterizations of model behavior. Our code, dataset, and collected samples\nare available at https://github.com/lindiatjuatja/BiasMonkey",
        "pdf_link": "https://arxiv.org/pdf/2311.04076v5.pdf"
    },
    {
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
        "authors": [
            "Geyang Guo",
            "Ranchi Zhao",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-11-07T15:36:40Z",
        "summary": "Alignment with human preference is a desired property of large language\nmodels (LLMs). Currently, the main alignment approach is based on reinforcement\nlearning from human feedback (RLHF). Despite the effectiveness of RLHF, it is\nintricate to implement and train, thus recent studies explore how to develop\nalternative alignment approaches based on supervised fine-tuning (SFT). A major\nlimitation of SFT is that it essentially does imitation learning, which cannot\nfully understand what are the expected behaviors. To address this issue, we\npropose an improved alignment approach named FIGA. Different from prior\nmethods, we incorporate fine-grained (i.e., token or phrase level) quality\nsignals that are derived by contrasting good and bad responses. Our approach\nhas made two major contributions. Firstly, we curate a refined alignment\ndataset that pairs initial responses and the corresponding revised ones.\nSecondly, we devise a new loss function can leverage fine-grained quality\nsignals to instruct the learning of LLMs for alignment. Extensive experiments\nhave demonstrated the effectiveness of our approaches by comparing a number of\ncompetitive baselines.",
        "pdf_link": "https://arxiv.org/pdf/2311.04072v1.pdf"
    },
    {
        "title": "Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features",
        "authors": [
            "Diogo Cruz",
            "Edoardo Pona",
            "Alex Holness-Tofts",
            "Elias Schmied",
            "V√≠ctor Abia Alonso",
            "Charlie Griffin",
            "Bogdan-Ionut Cirstea"
        ],
        "published": "2023-11-07T15:00:39Z",
        "summary": "Many capable large language models (LLMs) are developed via self-supervised\npre-training followed by a reinforcement-learning fine-tuning phase, often\nbased on human or AI feedback. During this stage, models may be guided by their\ninductive biases to rely on simpler features which may be easier to extract, at\na cost to robustness and generalisation. We investigate whether principles\ngoverning inductive biases in the supervised fine-tuning of LLMs also apply\nwhen the fine-tuning process uses reinforcement learning. Following Lovering et\nal (2021), we test two hypotheses: that features more $\\textit{extractable}$\nafter pre-training are more likely to be utilised by the final policy, and that\nthe evidence for/against a feature predicts whether it will be utilised.\nThrough controlled experiments on synthetic and natural language tasks, we find\nstatistically significant correlations which constitute strong evidence for\nthese hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2311.04046v1.pdf"
    },
    {
        "title": "Benefits and Harms of Large Language Models in Digital Mental Health",
        "authors": [
            "Munmun De Choudhury",
            "Sachin R. Pendse",
            "Neha Kumar"
        ],
        "published": "2023-11-07T14:11:10Z",
        "summary": "The past decade has been transformative for mental health research and\npractice. The ability to harness large repositories of data, whether from\nelectronic health records (EHR), mobile devices, or social media, has revealed\na potential for valuable insights into patient experiences, promising early,\nproactive interventions, as well as personalized treatment plans. Recent\ndevelopments in generative artificial intelligence, particularly large language\nmodels (LLMs), show promise in leading digital mental health to uncharted\nterritory. Patients are arriving at doctors' appointments with information\nsourced from chatbots, state-of-the-art LLMs are being incorporated in medical\nsoftware and EHR systems, and chatbots from an ever-increasing number of\nstartups promise to serve as AI companions, friends, and partners. This article\npresents contemporary perspectives on the opportunities and risks posed by LLMs\nin the design, development, and implementation of digital mental health tools.\nWe adopt an ecological framework and draw on the affordances offered by LLMs to\ndiscuss four application areas -- care-seeking behaviors from individuals in\nneed of care, community care provision, institutional and medical care\nprovision, and larger care ecologies at the societal level. We engage in a\nthoughtful consideration of whether and how LLM-based technologies could or\nshould be employed for enhancing mental health. The benefits and harms our\narticle surfaces could serve to help shape future research, advocacy, and\nregulatory efforts focused on creating more responsible, user-friendly,\nequitable, and secure LLM-based tools for mental health treatment and\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2311.14693v1.pdf"
    },
    {
        "title": "Input Reconstruction Attack against Vertical Federated Large Language Models",
        "authors": [
            "Fei Zheng"
        ],
        "published": "2023-11-07T09:39:22Z",
        "summary": "Recently, large language models (LLMs) have drawn extensive attention from\nacademia and the public, due to the advent of the ChatGPT. While LLMs show\ntheir astonishing ability in text generation for various tasks, privacy\nconcerns limit their usage in real-life businesses. More specifically, either\nthe user's inputs (the user sends the query to the model-hosting server) or the\nmodel (the user downloads the complete model) itself will be revealed during\nthe usage. Vertical federated learning (VFL) is a promising solution to this\nkind of problem. It protects both the user's input and the knowledge of the\nmodel by splitting the model into a bottom part and a top part, which is\nmaintained by the user and the model provider, respectively. However, in this\npaper, we demonstrate that in LLMs, VFL fails to protect the user input since\nit is simple and cheap to reconstruct the input from the intermediate\nembeddings. Experiments show that even with a commercial GPU, the input\nsentence can be reconstructed in only one second. We also discuss several\npossible solutions to enhance the privacy of vertical federated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.07585v2.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Automated Proof Synthesis in Rust",
        "authors": [
            "Jianan Yao",
            "Ziqiao Zhou",
            "Weiteng Chen",
            "Weidong Cui"
        ],
        "published": "2023-11-07T05:47:47Z",
        "summary": "Formal verification can provably guarantee the correctness of critical system\nsoftware, but the high proof burden has long hindered its wide adoption.\nRecently, Large Language Models (LLMs) have shown success in code analysis and\nsynthesis. In this paper, we present a combination of LLMs and static analysis\nto synthesize invariants, assertions, and other proof structures for a\nRust-based formal verification framework called Verus. In a few-shot setting,\nLLMs demonstrate impressive logical ability in generating postconditions and\nloop invariants, especially when analyzing short code snippets. However, LLMs\nlack the ability to retain and propagate context information, a strength of\ntraditional static analysis. Based on these observations, we developed a\nprototype based on OpenAI's GPT-4 model. Our prototype decomposes the\nverification task into multiple smaller ones, iteratively queries GPT-4, and\ncombines its output with lightweight static analysis. We evaluated the\nprototype with a developer in the automation loop on 20 vector-manipulating\nprograms. The results demonstrate that it significantly reduces human effort in\nwriting entry-level proof code.",
        "pdf_link": "https://arxiv.org/pdf/2311.03739v2.pdf"
    },
    {
        "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
        "authors": [
            "Ruosen Li",
            "Xinya Du"
        ],
        "published": "2023-11-07T05:32:39Z",
        "summary": "Neural models, including large language models (LLMs), achieve superior\nperformance on multi-hop question-answering. To elicit reasoning capabilities\nfrom LLMs, recent works propose using the chain-of-thought (CoT) mechanism to\ngenerate both the reasoning chain and the answer, which enhances the model's\ncapabilities in conducting multi-hop reasoning. However, several challenges\nstill remain: such as struggling with inaccurate reasoning, hallucinations, and\nlack of interpretability. On the other hand, information extraction (IE)\nidentifies entities, relations, and events grounded to the text. The extracted\nstructured information can be easily interpreted by humans and machines\n(Grishman, 2019). In this work, we investigate constructing and leveraging\nextracted semantic structures (graphs) for multi-hop question answering,\nespecially the reasoning process. Empirical results and human evaluations show\nthat our framework: generates more faithful reasoning chains and substantially\nimproves the QA performance on two benchmark datasets. Moreover, the extracted\nstructures themselves naturally provide grounded explanations that are\npreferred by humans, as compared to the generated reasoning chains and\nsaliency-based explanations.",
        "pdf_link": "https://arxiv.org/pdf/2311.03734v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models Attribution",
        "authors": [
            "Dongfang Li",
            "Zetian Sun",
            "Xinshuo Hu",
            "Zhenyu Liu",
            "Ziyang Chen",
            "Baotian Hu",
            "Aiguo Wu",
            "Min Zhang"
        ],
        "published": "2023-11-07T05:20:09Z",
        "summary": "Open-domain generative systems have gained significant attention in the field\nof conversational AI (e.g., generative search engines). This paper presents a\ncomprehensive review of the attribution mechanisms employed by these systems,\nparticularly large language models. Though attribution or citation improve the\nfactuality and verifiability, issues like ambiguous knowledge reservoirs,\ninherent biases, and the drawbacks of excessive attribution can hinder the\neffectiveness of these systems. The aim of this survey is to provide valuable\ninsights for researchers, aiding in the refinement of attribution methodologies\nto enhance the reliability and veracity of responses generated by open-domain\ngenerative systems. We believe that this field is still in its early stages;\nhence, we maintain a repository to keep track of ongoing studies at\nhttps://github.com/HITsz-TMG/awesome-llm-attributions.",
        "pdf_link": "https://arxiv.org/pdf/2311.03731v2.pdf"
    },
    {
        "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
        "authors": [
            "Sree Harsha Tanneru",
            "Chirag Agarwal",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-11-06T21:14:40Z",
        "summary": "Large Language Models (LLMs) are increasingly used as powerful tools for\nseveral high-stakes natural language processing (NLP) applications. Recent\nprompting works claim to elicit intermediate reasoning steps and key tokens\nthat serve as proxy explanations for LLM predictions. However, there is no\ncertainty whether these explanations are reliable and reflect the LLMs\nbehavior. In this work, we make one of the first attempts at quantifying the\nuncertainty in explanations of LLMs. To this end, we propose two novel metrics\n-- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to\nquantify the uncertainty of generated explanations. While verbalized\nuncertainty involves prompting the LLM to express its confidence in its\nexplanations, probing uncertainty leverages sample and model perturbations as a\nmeans to quantify the uncertainty. Our empirical analysis of benchmark datasets\nreveals that verbalized uncertainty is not a reliable estimate of explanation\nconfidence. Further, we show that the probing uncertainty estimates are\ncorrelated with the faithfulness of an explanation, with lower uncertainty\ncorresponding to explanations with higher faithfulness. Our study provides\ninsights into the challenges and opportunities of quantifying uncertainty in\nLLM explanations, contributing to the broader discussion of the trustworthiness\nof foundation models.",
        "pdf_link": "https://arxiv.org/pdf/2311.03533v1.pdf"
    },
    {
        "title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
        "authors": [
            "Rusheb Shah",
            "Quentin Feuillade--Montixi",
            "Soroush Pour",
            "Arush Tagade",
            "Stephen Casper",
            "Javier Rando"
        ],
        "published": "2023-11-06T18:55:18Z",
        "summary": "Despite efforts to align large language models to produce harmless responses,\nthey are still vulnerable to jailbreak prompts that elicit unrestricted\nbehaviour. In this work, we investigate persona modulation as a black-box\njailbreaking method to steer a target model to take on personalities that are\nwilling to comply with harmful instructions. Rather than manually crafting\nprompts for each persona, we automate the generation of jailbreaks using a\nlanguage model assistant. We demonstrate a range of harmful completions made\npossible by persona modulation, including detailed instructions for\nsynthesising methamphetamine, building a bomb, and laundering money. These\nautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is\n185 times larger than before modulation (0.23%). These prompts also transfer to\nClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,\nrespectively. Our work reveals yet another vulnerability in commercial large\nlanguage models and highlights the need for more comprehensive safeguards.",
        "pdf_link": "https://arxiv.org/pdf/2311.03348v2.pdf"
    },
    {
        "title": "ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity",
        "authors": [
            "Huixin Zhan",
            "Zijun Zhang"
        ],
        "published": "2023-11-06T18:43:47Z",
        "summary": "Clinical variant classification of pathogenic versus benign genetic variants\nremains a pivotal challenge in clinical genetics. Recently, the proposition of\nprotein language models has improved the generic variant effect prediction\n(VEP) accuracy via weakly-supervised or unsupervised training. However, these\nVEPs are not disease-specific, limiting their adaptation at point-of-care. To\naddress this problem, we propose a disease-specific \\textsc{pro}tein language\nmodel for variant \\textsc{path}ogenicity, termed ProPath, to capture the\npseudo-log-likelihood ratio in rare missense variants through a siamese\nnetwork. We evaluate the performance of ProPath against pre-trained language\nmodels, using clinical variant sets in inherited cardiomyopathies and\narrhythmias that were not seen during training. Our results demonstrate that\nProPath surpasses the pre-trained ESM1b with an over $5\\%$ improvement in AUC\nacross both datasets. Furthermore, our model achieved the highest performances\nacross all baselines for both datasets. Thus, our ProPath offers a potent\ndisease-specific variant effect prediction, particularly valuable for disease\nassociations and clinical applicability.",
        "pdf_link": "https://arxiv.org/pdf/2311.03429v2.pdf"
    },
    {
        "title": "DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase",
        "authors": [
            "Dawei Li",
            "Yaxuan Li",
            "Dheeraj Mekala",
            "Shuyao Li",
            "Yulin wang",
            "Xueqi Wang",
            "William Hogan",
            "Jingbo Shang"
        ],
        "published": "2023-11-06T18:12:55Z",
        "summary": "In-Context Learning (ICL) combined with pre-trained large language models has\nachieved promising results on various NLP tasks. However, ICL requires\nhigh-quality annotated demonstrations which might not be available in\nreal-world scenarios. To overcome this limitation, we propose \\textbf{D}ata\n\\textbf{A}ugmentation for \\textbf{I}n-Context \\textbf{L}earning\n(\\textbf{DAIL}). DAIL leverages the intuition that large language models are\nmore familiar with the content generated by themselves. It first utilizes the\nlanguage model to generate paraphrases of the test sample and employs majority\nvoting to determine the final result based on individual predictions. Our\nextensive empirical evaluation shows that DAIL outperforms the standard ICL\nmethod and other ensemble-based methods in the low-resource scenario.\nAdditionally, we explore the use of voting consistency as a confidence score of\nthe model when the logits of predictions are inaccessible. We believe our work\nwill stimulate further research on ICL in low-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2311.03319v1.pdf"
    },
    {
        "title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
        "authors": [
            "Thiemo Wambsganss",
            "Xiaotian Su",
            "Vinitra Swamy",
            "Seyed Parsa Neshaei",
            "Roman Rietsche",
            "Tanja K√§ser"
        ],
        "published": "2023-11-06T18:01:34Z",
        "summary": "Large Language Models (LLMs) are increasingly utilized in educational tasks\nsuch as providing writing suggestions to students. Despite their potential,\nLLMs are known to harbor inherent biases which may negatively impact learners.\nPrevious studies have investigated bias in models and data representations\nseparately, neglecting the potential impact of LLM bias on human writing. In\nthis paper, we investigate how bias transfers through an AI writing support\npipeline. We conduct a large-scale user study with 231 students writing\nbusiness case peer reviews in German. Students are divided into five groups\nwith different levels of writing support: one classroom group with\nfeature-based suggestions and four groups recruited from Prolific -- a control\ngroup with no assistance, two groups with suggestions from fine-tuned GPT-2 and\nGPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using\nGenBit gender bias analysis, Word Embedding Association Tests (WEAT), and\nSentence Embedding Association Test (SEAT) we evaluate the gender bias at\nvarious stages of the pipeline: in model embeddings, in suggestions generated\nby the models, and in reviews written by students. Our results demonstrate that\nthere is no significant difference in gender bias between the resulting peer\nreviews of groups with and without LLM suggestions. Our research is therefore\noptimistic about the use of AI writing support in the classroom, showcasing a\ncontext where bias in LLMs does not transfer to students' responses.",
        "pdf_link": "https://arxiv.org/pdf/2311.03311v1.pdf"
    },
    {
        "title": "Ziya2: Data-centric Learning is All LLMs Need",
        "authors": [
            "Ruyi Gan",
            "Ziwei Wu",
            "Renliang Sun",
            "Junyu Lu",
            "Xiaojun Wu",
            "Dixiang Zhang",
            "Kunhao Pan",
            "Junqing He",
            "Yuanhe Tian",
            "Ping Yang",
            "Qi Yang",
            "Hao Wang",
            "Jiaxing Zhang",
            "Yan Song"
        ],
        "published": "2023-11-06T17:49:34Z",
        "summary": "Various large language models (LLMs) have been proposed in recent years,\nincluding closed- and open-source ones, continually setting new records on\nmultiple benchmarks. However, the development of LLMs still faces several\nissues, such as high cost of training models from scratch, and continual\npre-training leading to catastrophic forgetting, etc. Although many such issues\nare addressed along the line of research on LLMs, an important yet practical\nlimitation is that many studies overly pursue enlarging model sizes without\ncomprehensively analyzing and optimizing the use of pre-training data in their\nlearning process, as well as appropriate organization and leveraging of such\ndata in training LLMs under cost-effective settings. In this work, we propose\nZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundation\nmodel, and further pre-trained on 700 billion tokens, where we focus on\npre-training techniques and use data-centric optimization to enhance the\nlearning process of Ziya2 on different stages. We define three data attributes\nand firstly establish data-centric scaling laws to illustrate how different\ndata impacts LLMs. Experiments show that Ziya2 significantly outperforms other\nmodels in multiple benchmarks especially with promising results compared to\nrepresentative open-source ones. Ziya2 (Base) is released at\nhttps://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and\nhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.",
        "pdf_link": "https://arxiv.org/pdf/2311.03301v2.pdf"
    },
    {
        "title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges",
        "authors": [
            "Chenhang Cui",
            "Yiyang Zhou",
            "Xinyu Yang",
            "Shirley Wu",
            "Linjun Zhang",
            "James Zou",
            "Huaxiu Yao"
        ],
        "published": "2023-11-06T17:26:59Z",
        "summary": "While GPT-4V(ision) impressively models both visual and textual information\nsimultaneously, it's hallucination behavior has not been systematically\nassessed. To bridge this gap, we introduce a new benchmark, namely, the Bias\nand Interference Challenges in Visual Language Models (Bingo). This benchmark\nis designed to evaluate and shed light on the two common types of\nhallucinations in visual language models: bias and interference. Here, bias\nrefers to the model's tendency to hallucinate certain types of responses,\npossibly due to imbalance in its training data. Interference pertains to\nscenarios where the judgment of GPT-4V(ision) can be disrupted due to how the\ntext prompt is phrased or how the input image is presented. We identify a\nnotable regional bias, whereby GPT-4V(ision) is better at interpreting Western\nimages or images with English writing compared to images from other countries\nor containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to\nleading questions and is often confused when interpreting multiple images\ntogether. Popular mitigation approaches, such as self-correction and\nchain-of-thought reasoning, are not effective in resolving these challenges. We\nalso identified similar biases and interference vulnerabilities with LLaVA and\nBard. Our results characterize the hallucination challenges in GPT-4V(ision)\nand state-of-the-art visual-language models, and highlight the need for new\nsolutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.",
        "pdf_link": "https://arxiv.org/pdf/2311.03287v2.pdf"
    },
    {
        "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
        "authors": [
            "Zilin Xiao",
            "Ming Gong",
            "Jie Wu",
            "Xingyao Zhang",
            "Linjun Shou",
            "Jian Pei",
            "Daxin Jiang"
        ],
        "published": "2023-11-06T16:38:51Z",
        "summary": "Generative approaches powered by large language models (LLMs) have\ndemonstrated emergent abilities in tasks that require complex reasoning\nabilities. Yet the generative nature still makes the generated content suffer\nfrom hallucinations, thus unsuitable for entity-centric tasks like entity\nlinking (EL) requiring precise entity predictions over a large knowledge base.\nWe present Instructed Generative Entity Linker (INSGENEL), the first approach\nthat enables casual language models to perform entity linking over knowledge\nbases. Several methods to equip language models with EL capability were\nproposed in this work, including (i) a sequence-to-sequence training EL\nobjective with instruction-tuning, (ii) a novel generative EL framework based\non a light-weight potential mention retriever that frees the model from heavy\nand non-parallelizable decoding, achieving 4$\\times$ speedup without compromise\non linking metrics. INSGENEL outperforms previous generative alternatives with\n+6.8 F1 points gain on average, also with a huge advantage in training data\nefficiency and training compute consumption. In addition, our skillfully\nengineered in-context learning (ICL) framework for EL still lags behind\nINSGENEL significantly, reaffirming that the EL task remains a persistent\nhurdle for general LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.03250v1.pdf"
    },
    {
        "title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents",
        "authors": [
            "Shaoguang Mao",
            "Yuzhe Cai",
            "Yan Xia",
            "Wenshan Wu",
            "Xun Wang",
            "Fengyi Wang",
            "Tao Ge",
            "Furu Wei"
        ],
        "published": "2023-11-06T16:03:46Z",
        "summary": "This paper introduces Alympics (Olympics for Agents), a systematic simulation\nframework utilizing Large Language Model (LLM) agents for game theory research.\nAlympics creates a versatile platform for studying complex game theory\nproblems, bridging the gap between theoretical game theory and empirical\ninvestigations by providing a controlled environment for simulating human-like\nstrategic interactions with LLM agents. In our pilot case study, the \"Water\nAllocation Challenge,\" we explore Alympics through a challenging strategic game\nfocused on the multi-round auction on scarce survival resources. This study\ndemonstrates the framework's ability to qualitatively and quantitatively\nanalyze game determinants, strategies, and outcomes. Additionally, we conduct a\ncomprehensive human assessment and an in-depth evaluation of LLM agents in\nstrategic decision-making scenarios. Our findings not only expand the\nunderstanding of LLM agents' proficiency in emulating human strategic behavior\nbut also highlight their potential in advancing game theory knowledge, thereby\nenriching our understanding of both game theory and empowering further research\ninto strategic decision-making domains with LLM agents. Codes, prompts, and all\nrelated resources are available at https://github.com/microsoft/Alympics.",
        "pdf_link": "https://arxiv.org/pdf/2311.03220v4.pdf"
    },
    {
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
        "authors": [
            "Xuan Li",
            "Zhanke Zhou",
            "Jianing Zhu",
            "Jiangchao Yao",
            "Tongliang Liu",
            "Bo Han"
        ],
        "published": "2023-11-06T15:29:30Z",
        "summary": "Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment\nw.r.t. the authority power for inciting harmfulness, we disclose a lightweight\nmethod, termed DeepInception, which can easily hypnotize LLM to be a\njailbreaker. Specifically, DeepInception leverages the personification ability\nof LLM to construct a novel nested scene to behave, which realizes an adaptive\nway to escape the usage control in a normal scenario. Empirically, our\nDeepInception can achieve competitive jailbreak success rates with previous\ncounterparts and realize a continuous jailbreak in subsequent interactions,\nwhich reveals the critical weakness of self-losing on both open and\nclosed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, and GPT-3.5-turbo/4. Our\ninvestigation appeals to people to pay more attention to the safety aspects of\nLLMs and develop a stronger defense against their misuse risks. The code is\npublicly available at: https://github.com/tmlr-group/DeepInception.",
        "pdf_link": "https://arxiv.org/pdf/2311.03191v3.pdf"
    },
    {
        "title": "Zero-shot Bilingual App Reviews Mining with Large Language Models",
        "authors": [
            "Jialiang Wei",
            "Anne-Lise Courbis",
            "Thomas Lambolais",
            "Binbin Xu",
            "Pierre Louis Bernard",
            "G√©rard Dray"
        ],
        "published": "2023-11-06T12:36:46Z",
        "summary": "App reviews from app stores are crucial for improving software requirements.\nA large number of valuable reviews are continually being posted, describing\nsoftware problems and expected features. Effectively utilizing user reviews\nnecessitates the extraction of relevant information, as well as their\nsubsequent summarization. Due to the substantial volume of user reviews, manual\nanalysis is arduous. Various approaches based on natural language processing\n(NLP) have been proposed for automatic user review mining. However, the\nmajority of them requires a manually crafted dataset to train their models,\nwhich limits their usage in real-world scenarios. In this work, we propose\nMini-BAR, a tool that integrates large language models (LLMs) to perform\nzero-shot mining of user reviews in both English and French. Specifically,\nMini-BAR is designed to (i) classify the user reviews, (ii) cluster similar\nreviews together, (iii) generate an abstractive summary for each cluster and\n(iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we\ncreated a dataset containing 6,000 English and 6,000 French annotated user\nreviews and conducted extensive experiments. Preliminary results demonstrate\nthe effectiveness and efficiency of Mini-BAR in requirement engineering by\nanalyzing bilingual app reviews. (Replication package containing the code,\ndataset, and experiment setups on https://github.com/Jl-wei/mini-bar )",
        "pdf_link": "https://arxiv.org/pdf/2311.03058v1.pdf"
    },
    {
        "title": "LitSumm: Large language models for literature summarisation of non-coding RNAs",
        "authors": [
            "Andrew Green",
            "Carlos Ribas",
            "Nancy Ontiveros-Palacios",
            "Sam Griffiths-Jones",
            "Anton I. Petrov",
            "Alex Bateman",
            "Blake Sweeney"
        ],
        "published": "2023-11-06T12:22:19Z",
        "summary": "Motivation: Curation of literature in life sciences is a growing challenge.\nThe continued increase in the rate of publication, coupled with the relatively\nfixed number of curators worldwide presents a major challenge to developers of\nbiomedical knowledgebases. Very few knowledgebases have resources to scale to\nthe whole relevant literature and all have to prioritise their efforts.\n  Results: In this work, we take a first step to alleviating the lack of\ncurator time in RNA science by generating summaries of literature for\nnon-coding RNAs using large language models (LLMs). We demonstrate that\nhigh-quality, factually accurate summaries with accurate references can be\nautomatically generated from the literature using a commercial LLM and a chain\nof prompts and checks. Manual assessment was carried out for a subset of\nsummaries, with the majority being rated extremely high quality. We also\napplied the most commonly used automated evaluation approaches, finding that\nthey do not correlate with human assessment. Finally, we apply our tool to a\nselection of over 4,600 ncRNAs and make the generated summaries available via\nthe RNAcentral resource. We conclude that automated literature summarization is\nfeasible with the current generation of LLMs, provided careful prompting and\nautomated checking are applied.\n  Availability: Code used to produce these summaries can be found here:\nhttps://github.com/RNAcentral/litscan-summarization and the dataset of contexts\nand summaries can be found here:\nhttps://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are also\ndisplayed on the RNA report pages in RNAcentral (https://rnacentral.org/)",
        "pdf_link": "https://arxiv.org/pdf/2311.03056v2.pdf"
    },
    {
        "title": "Can LLMs Follow Simple Rules?",
        "authors": [
            "Norman Mu",
            "Sarah Chen",
            "Zifan Wang",
            "Sizhe Chen",
            "David Karamardian",
            "Lulwa Aljeraisy",
            "Basel Alomair",
            "Dan Hendrycks",
            "David Wagner"
        ],
        "published": "2023-11-06T08:50:29Z",
        "summary": "As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Existing evaluations\nof adversarial attacks and defenses on LLMs generally require either expensive\nmanual review or unreliable heuristic checks. To address this issue, we propose\nRule-following Language Evaluation Scenarios (RuLES), a programmatic framework\nfor measuring rule-following ability in LLMs. RuLES consists of 14 simple text\nscenarios in which the model is instructed to obey various rules while\ninteracting with the user. Each scenario has a programmatic evaluation function\nto determine whether the model has broken any rules in a conversation. Our\nevaluations of proprietary and open models show that almost all current models\nstruggle to follow scenario rules, even on straightforward test cases. We also\ndemonstrate that simple optimization attacks suffice to significantly increase\nfailure rates on test cases. We conclude by exploring two potential avenues for\nimprovement: test-time steering and supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.04235v3.pdf"
    },
    {
        "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
        "authors": [
            "Sahana Ramnath",
            "Brihi Joshi",
            "Skyler Hallinan",
            "Ximing Lu",
            "Liunian Harold Li",
            "Aaron Chan",
            "Jack Hessel",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-11-06T00:20:11Z",
        "summary": "Large language models (LMs) are capable of generating free-text rationales to\naid question answering. However, prior work 1) suggests that useful\nself-rationalization is emergent only at significant scales (e.g., 175B\nparameter GPT-3); and 2) focuses largely on downstream performance, ignoring\nthe semantics of the rationales themselves, e.g., are they faithful, true, and\nhelpful for humans? In this work, we enable small-scale LMs (approx. 200x\nsmaller than GPT-3) to generate rationales that not only improve downstream\ntask performance, but are also more plausible, consistent, and diverse,\nassessed both by automatic and human evaluation. Our method, MaRio\n(Multi-rewArd RatIOnalization), is a multi-reward conditioned\nself-rationalization algorithm that optimizes multiple distinct properties like\nplausibility, diversity and consistency. Results on five difficult\nquestion-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC\nshow that not only does MaRio improve task accuracy, but it also improves the\nself-rationalization quality of small LMs across the aforementioned axes better\nthan a supervised fine-tuning (SFT) baseline. Extensive human evaluations\nconfirm that MaRio rationales are preferred vs. SFT rationales, as well as\nqualitative improvements in plausibility and consistency.",
        "pdf_link": "https://arxiv.org/pdf/2311.02805v1.pdf"
    },
    {
        "title": "On the Intersection of Self-Correction and Trust in Language Models",
        "authors": [
            "Satyapriya Krishna"
        ],
        "published": "2023-11-06T00:04:12Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex cognitive tasks. However, their complexity and lack of\ntransparency have raised several trustworthiness concerns, including the\npropagation of misinformation and toxicity. Recent research has explored the\nself-correction capabilities of LLMs to enhance their performance. In this\nwork, we investigate whether these self-correction capabilities can be\nharnessed to improve the trustworthiness of LLMs. We conduct experiments\nfocusing on two key aspects of trustworthiness: truthfulness and toxicity. Our\nfindings reveal that self-correction can lead to improvements in toxicity and\ntruthfulness, but the extent of these improvements varies depending on the\nspecific aspect of trustworthiness and the nature of the task. Interestingly,\nour study also uncovers instances of \"self-doubt\" in LLMs during the\nself-correction process, introducing a new set of challenges that need to be\naddressed.",
        "pdf_link": "https://arxiv.org/pdf/2311.02801v1.pdf"
    },
    {
        "title": "Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation",
        "authors": [
            "Muhammad Fawad Akbar Khan",
            "Max Ramsdell",
            "Erik Falor",
            "Hamid Karimi"
        ],
        "published": "2023-11-05T12:56:40Z",
        "summary": "This paper presents a comprehensive evaluation of the code generation\ncapabilities of ChatGPT, a prominent large language model, compared to human\nprogrammers. A novel dataset of 131 code-generation prompts across 5 categories\nwas curated to enable robust analysis. Code solutions were generated by both\nChatGPT and humans for all prompts, resulting in 262 code samples. A meticulous\nmanual assessment methodology prioritized evaluating correctness,\ncomprehensibility, and security using 14 established code quality metrics. The\nkey findings reveal ChatGPT's strengths in crafting concise, efficient code\nwith advanced constructs, showcasing strengths in data analysis tasks (93.1%\naccuracy) but limitations in visual-graphical challenges. Comparative analysis\nwith human code highlights ChatGPT's inclination towards modular design and\nsuperior error handling. Additionally, machine learning models effectively\ndistinguished ChatGPT from human code with up to 88% accuracy, suggesting\ndetectable coding style disparities. By providing profound insights into\nChatGPT's code generation capabilities and limitations through quantitative\nmetrics and qualitative analysis, this study makes valuable contributions\ntoward advancing AI-based programming assistants. The curated dataset and\nmethodology offer a robust foundation for future research in this nascent\ndomain. All data and codes are available on\nhttps://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls.",
        "pdf_link": "https://arxiv.org/pdf/2311.02640v1.pdf"
    },
    {
        "title": "FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM",
        "authors": [
            "Grace Colverd",
            "Paul Darm",
            "Leonard Silverberg",
            "Noah Kasmanoff"
        ],
        "published": "2023-11-05T08:34:26Z",
        "summary": "Fast disaster impact reporting is crucial in planning humanitarian\nassistance. Large Language Models (LLMs) are well known for their ability to\nwrite coherent text and fulfill a variety of tasks relevant to impact\nreporting, such as question answering or text summarization. However, LLMs are\nconstrained by the knowledge within their training data and are prone to\ngenerating inaccurate, or \"hallucinated\", information. To address this, we\nintroduce a sophisticated pipeline embodied in our tool FloodBrain\n(floodbrain.com), specialized in generating flood disaster impact reports by\nextracting and curating information from the web. Our pipeline assimilates\ninformation from web search results to produce detailed and accurate reports on\nflood events. We test different LLMs as backbones in our tool and compare their\ngenerated reports to human-written reports on different metrics. Similar to\nother studies, we find a notable correlation between the scores assigned by\nGPT-4 and the scores given by human evaluators when comparing our generated\nreports to human-authored ones. Additionally, we conduct an ablation study to\ntest our single pipeline components and their relevancy for the final reports.\nWith our tool, we aim to advance the use of LLMs for disaster impact reporting\nand reduce the time for coordination of humanitarian efforts in the wake of\nflood disasters.",
        "pdf_link": "https://arxiv.org/pdf/2311.02597v1.pdf"
    },
    {
        "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models",
        "authors": [
            "Kun Chu",
            "Xufeng Zhao",
            "Cornelius Weber",
            "Mengdi Li",
            "Stefan Wermter"
        ],
        "published": "2023-11-04T11:21:38Z",
        "summary": "Reinforcement Learning (RL) plays an important role in the robotic\nmanipulation domain since it allows self-learning from trial-and-error\ninteractions with the environment. Still, sample efficiency and reward\nspecification seriously limit its potential. One possible solution involves\nlearning from expert guidance. However, obtaining a human expert is impractical\ndue to the high cost of supervising an RL agent, and developing an automatic\nsupervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate\nremarkable abilities to provide human-like feedback on user inputs in natural\nlanguage. Nevertheless, they are not designed to directly control low-level\nrobotic motions, as their pretraining is based on vast internet data rather\nthan specific robotics data. In this paper, we introduce the Lafite-RL\n(Language agent feedback interactive Reinforcement Learning) framework, which\nenables RL agents to learn robotic tasks efficiently by taking advantage of\nLLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate\nthat, with simple prompt design in natural language, the Lafite-RL agent\nexhibits improved learning capabilities when guided by an LLM. It outperforms\nthe baseline in terms of both learning efficiency and success rate,\nunderscoring the efficacy of the rewards provided by an LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.02379v1.pdf"
    },
    {
        "title": "Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles",
        "authors": [
            "Weiting Tan",
            "Haoran Xu",
            "Lingfeng Shen",
            "Shuyue Stella Li",
            "Kenton Murray",
            "Philipp Koehn",
            "Benjamin Van Durme",
            "Yunmo Chen"
        ],
        "published": "2023-11-04T03:18:45Z",
        "summary": "Large language models trained primarily in a monolingual setting have\ndemonstrated their ability to generalize to machine translation using zero- and\nfew-shot examples with in-context learning. However, even though zero-shot\ntranslations are relatively good, there remains a discernible gap comparing\ntheir performance with the few-shot setting. In this paper, we investigate the\nfactors contributing to this gap and find that this gap can largely be closed\n(for about 70%) by matching the writing styles of the target corpus.\nAdditionally, we explore potential approaches to enhance zero-shot baselines\nwithout the need for parallel demonstration examples, providing valuable\ninsights into how these methods contribute to improving translation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.02310v1.pdf"
    },
    {
        "title": "LLMs-augmented Contextual Bandit",
        "authors": [
            "Ali Baheri",
            "Cecilia O. Alm"
        ],
        "published": "2023-11-03T23:12:57Z",
        "summary": "Contextual bandits have emerged as a cornerstone in reinforcement learning,\nenabling systems to make decisions with partial feedback. However, as contexts\ngrow in complexity, traditional bandit algorithms can face challenges in\nadequately capturing and utilizing such contexts. In this paper, we propose a\nnovel integration of large language models (LLMs) with the contextual bandit\nframework. By leveraging LLMs as an encoder, we enrich the representation of\nthe context, providing the bandit with a denser and more informative view.\nPreliminary results on synthetic datasets demonstrate the potential of this\napproach, showing notable improvements in cumulative rewards and reductions in\nregret compared to traditional bandit algorithms. This integration not only\nshowcases the capabilities of LLMs in reinforcement learning but also opens the\ndoor to a new era of contextually-aware decision systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.02268v1.pdf"
    },
    {
        "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
        "authors": [
            "Qingru Zhang",
            "Chandan Singh",
            "Liyuan Liu",
            "Xiaodong Liu",
            "Bin Yu",
            "Jianfeng Gao",
            "Tuo Zhao"
        ],
        "published": "2023-11-03T22:56:43Z",
        "summary": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need -\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .",
        "pdf_link": "https://arxiv.org/pdf/2311.02262v1.pdf"
    },
    {
        "title": "An Interdisciplinary Outlook on Large Language Models for Scientific Research",
        "authors": [
            "James Boyko",
            "Joseph Cohen",
            "Nathan Fox",
            "Maria Han Veiga",
            "Jennifer I-Hsiu Li",
            "Jing Liu",
            "Bernardo Modenesi",
            "Andreas H. Rauch",
            "Kenneth N. Reid",
            "Soumi Tribedi",
            "Anastasia Visheratina",
            "Xin Xie"
        ],
        "published": "2023-11-03T19:41:09Z",
        "summary": "In this paper, we describe the capabilities and constraints of Large Language\nModels (LLMs) within disparate academic disciplines, aiming to delineate their\nstrengths and limitations with precision. We examine how LLMs augment\nscientific inquiry, offering concrete examples such as accelerating literature\nreview by summarizing vast numbers of publications, enhancing code development\nthrough automated syntax correction, and refining the scientific writing\nprocess. Simultaneously, we articulate the challenges LLMs face, including\ntheir reliance on extensive and sometimes biased datasets, and the potential\nethical dilemmas stemming from their use. Our critical discussion extends to\nthe varying impacts of LLMs across fields, from the natural sciences, where\nthey help model complex biological sequences, to the social sciences, where\nthey can parse large-scale qualitative data. We conclude by offering a nuanced\nperspective on how LLMs can be both a boon and a boundary to scientific\nprogress.",
        "pdf_link": "https://arxiv.org/pdf/2311.04929v1.pdf"
    },
    {
        "title": "An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology",
        "authors": [
            "Reza Khanmohammadi",
            "Mohammad M. Ghassemi",
            "Kyle Verdecchia",
            "Ahmed I. Ghanem",
            "Luo Bing",
            "Indrin J. Chetty",
            "Hassan Bagher-Ebadian",
            "Farzan Siddiqui",
            "Mohamed Elshaikh",
            "Benjamin Movsas",
            "Kundan Thind"
        ],
        "published": "2023-11-03T19:32:35Z",
        "summary": "Natural Language Processing (NLP) is a key technique for developing Medical\nArtificial Intelligence (AI) systems that leverage Electronic Health Record\n(EHR) data to build diagnostic and prognostic models. NLP enables the\nconversion of unstructured clinical text into structured data that can be fed\ninto AI algorithms. The emergence of the transformer architecture and large\nlanguage models (LLMs) has led to remarkable advances in NLP for various\nhealthcare tasks, such as entity recognition, relation extraction, sentence\nsimilarity, text summarization, and question answering. In this article, we\nreview the major technical innovations that underpin modern NLP models and\npresent state-of-the-art NLP applications that employ LLMs in radiation\noncology research. However, these LLMs are prone to many errors such as\nhallucinations, biases, and ethical violations, which necessitate rigorous\nevaluation and validation before clinical deployment. As such, we propose a\ncomprehensive framework for assessing the NLP models based on their purpose and\nclinical fit, technical performance, bias and trust, legal and ethical\nimplications, and quality assurance, prior to implementation in clinical\nradiation oncology. Our article aims to provide guidance and insights for\nresearchers and clinicians who are interested in developing and using NLP\nmodels in clinical radiation oncology.",
        "pdf_link": "https://arxiv.org/pdf/2311.02205v2.pdf"
    },
    {
        "title": "The Alignment Problem in Context",
        "authors": [
            "Rapha√´l Milli√®re"
        ],
        "published": "2023-11-03T17:57:55Z",
        "summary": "A core challenge in the development of increasingly capable AI systems is to\nmake them safe and reliable by ensuring their behaviour is consistent with\nhuman values. This challenge, known as the alignment problem, does not merely\napply to hypothetical future AI systems that may pose catastrophic risks; it\nalready applies to current systems, such as large language models, whose\npotential for harm is rapidly increasing. In this paper, I assess whether we\nare on track to solve the alignment problem for large language models, and what\nthat means for the safety of future AI systems. I argue that existing\nstrategies for alignment are insufficient, because large language models remain\nvulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I\noffer an explanation of this lingering vulnerability on which it is not simply\na contingent limitation of current language models, but has deep technical ties\nto a crucial aspect of what makes these models useful and versatile in the\nfirst place -- namely, their remarkable aptitude to learn \"in context\" directly\nfrom user instructions. It follows that the alignment problem is not only\nunsolved for current AI systems, but may be intrinsically difficult to solve\nwithout severely undermining their capabilities. Furthermore, this assessment\nraises concerns about the prospect of ensuring the safety of future and more\ncapable AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.02147v1.pdf"
    },
    {
        "title": "Grounded Intuition of GPT-Vision's Abilities with Scientific Images",
        "authors": [
            "Alyssa Hwang",
            "Andrew Head",
            "Chris Callison-Burch"
        ],
        "published": "2023-11-03T17:53:43Z",
        "summary": "GPT-Vision has impressed us on a range of vision-language tasks, but it comes\nwith the familiar new challenge: we have little idea of its capabilities and\nlimitations. In our study, we formalize a process that many have instinctively\nbeen trying already to develop \"grounded intuition\" of this new model. Inspired\nby the recent movement away from benchmarking in favor of example-driven\nqualitative evaluation, we draw upon grounded theory and thematic analysis in\nsocial science and human-computer interaction to establish a rigorous framework\nfor qualitative evaluation in natural language processing. We use our technique\nto examine alt text generation for scientific figures, finding that GPT-Vision\nis particularly sensitive to prompting, counterfactual text in images, and\nrelative spatial relationships. Our method and analysis aim to help researchers\nramp up their own grounded intuitions of new models while exposing how\nGPT-Vision can be applied to make information more accessible.",
        "pdf_link": "https://arxiv.org/pdf/2311.02069v1.pdf"
    },
    {
        "title": "Post Turing: Mapping the landscape of LLM Evaluation",
        "authors": [
            "Alexey Tikhonov",
            "Ivan P. Yamshchikov"
        ],
        "published": "2023-11-03T17:24:50Z",
        "summary": "In the rapidly evolving landscape of Large Language Models (LLMs),\nintroduction of well-defined and standardized evaluation methodologies remains\na crucial challenge. This paper traces the historical trajectory of LLM\nevaluations, from the foundational questions posed by Alan Turing to the modern\nera of AI research. We categorize the evolution of LLMs into distinct periods,\neach characterized by its unique benchmarks and evaluation criteria. As LLMs\nincreasingly mimic human-like behaviors, traditional evaluation proxies, such\nas the Turing test, have become less reliable. We emphasize the pressing need\nfor a unified evaluation system, given the broader societal implications of\nthese models. Through an analysis of common evaluation methodologies, we\nadvocate for a qualitative shift in assessment approaches, underscoring the\nimportance of standardization and objective criteria. This work serves as a\ncall for the AI community to collaboratively address the challenges of LLM\nevaluation, ensuring their reliability, fairness, and societal benefit.",
        "pdf_link": "https://arxiv.org/pdf/2311.02049v1.pdf"
    },
    {
        "title": "Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks",
        "authors": [
            "Yifan Wang",
            "Qingyan Guo",
            "Xinzhe Ni",
            "Chufan Shi",
            "Lemao Liu",
            "Haiyun Jiang",
            "Yujiu Yang"
        ],
        "published": "2023-11-03T14:39:20Z",
        "summary": "In-context learning (ICL) ability has emerged with the increasing scale of\nlarge language models (LLMs), enabling them to learn input-label mappings from\ndemonstrations and perform well on downstream tasks. However, under the\nstandard ICL setting, LLMs may sometimes neglect query-related information in\ndemonstrations, leading to incorrect predictions. To address this limitation,\nwe propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to\nexplore the power of ICL in open-domain question answering, an important form\nin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract\nquery-related knowledge from demonstrations, then concatenates the knowledge to\nprompt LLMs in a more explicit way. Furthermore, we track the source of this\nknowledge to identify specific examples, and introduce a Hint-related Example\nRetriever (HER) to select informative examples for enhanced demonstrations. We\nevaluate HICL with HER on 3 open-domain QA benchmarks, and observe average\nperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM\nscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.",
        "pdf_link": "https://arxiv.org/pdf/2311.01949v1.pdf"
    },
    {
        "title": "Comprehensive Assessment of Toxicity in ChatGPT",
        "authors": [
            "Boyang Zhang",
            "Xinyue Shen",
            "Wai Man Si",
            "Zeyang Sha",
            "Zeyuan Chen",
            "Ahmed Salem",
            "Yun Shen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-11-03T14:37:53Z",
        "summary": "Moderating offensive, hateful, and toxic language has always been an\nimportant but challenging topic in the domain of safe use in NLP. The emerging\nlarge language models (LLMs), such as ChatGPT, can potentially further\naccentuate this threat. Previous works have discovered that ChatGPT can\ngenerate toxic responses using carefully crafted inputs. However, limited\nresearch has been done to systematically examine when ChatGPT generates toxic\nresponses. In this paper, we comprehensively evaluate the toxicity in ChatGPT\nby utilizing instruction-tuning datasets that closely align with real-world\nscenarios. Our results show that ChatGPT's toxicity varies based on different\nproperties and settings of the prompts, including tasks, domains, length, and\nlanguages. Notably, prompts in creative writing tasks can be 2x more likely\nthan others to elicit toxic responses. Prompting in German and Portuguese can\nalso double the response toxicity. Additionally, we discover that certain\ndeliberately toxic prompts, designed in earlier studies, no longer yield\nharmful responses. We hope our discoveries can guide model developers to better\nregulate these AI systems and the users to avoid undesirable outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14685v1.pdf"
    },
    {
        "title": "Sentiment Analysis through LLM Negotiations",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Shengyu Zhang",
            "Shuhe Wang",
            "Fei Wu",
            "Jiwei Li",
            "Tianwei Zhang",
            "Guoyin Wang"
        ],
        "published": "2023-11-03T12:35:29Z",
        "summary": "A standard paradigm for sentiment analysis is to rely on a singular LLM and\nmakes the decision in a single round under the framework of in-context\nlearning. This framework suffers the key disadvantage that the single-turn\noutput generated by a single LLM might not deliver the perfect decision, just\nas humans sometimes need multiple attempts to get things right. This is\nespecially true for the task of sentiment analysis where deep reasoning is\nrequired to address the complex linguistic phenomenon (e.g., clause\ncomposition, irony, etc) in the input.\n  To address this issue, this paper introduces a multi-LLM negotiation\nframework for sentiment analysis. The framework consists of a reasoning-infused\ngenerator to provide decision along with rationale, a explanation-deriving\ndiscriminator to evaluate the credibility of the generator. The generator and\nthe discriminator iterate until a consensus is reached. The proposed framework\nnaturally addressed the aforementioned challenge, as we are able to take the\ncomplementary abilities of two LLMs, have them use rationale to persuade each\nother for correction.\n  Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie\nReview, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed\napproach: it consistently yields better performances than the ICL baseline\nacross all benchmarks, and even superior performances to supervised baselines\non the Twitter and movie review datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.01876v1.pdf"
    },
    {
        "title": "Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT",
        "authors": [
            "Mario S√§nger",
            "Ninon De Mecquenem",
            "Katarzyna Ewa Lewi≈Ñska",
            "Vasilis Bountris",
            "Fabian Lehmann",
            "Ulf Leser",
            "Thomas Kosch"
        ],
        "published": "2023-11-03T10:28:53Z",
        "summary": "Scientific workflow systems are increasingly popular for expressing and\nexecuting complex data analysis pipelines over large datasets, as they offer\nreproducibility, dependability, and scalability of analyses by automatic\nparallelization on large compute clusters. However, implementing workflows is\ndifficult due to the involvement of many black-box tools and the deep\ninfrastructure stack necessary for their execution. Simultaneously,\nuser-supporting tools are rare, and the number of available examples is much\nlower than in classical programming languages. To address these challenges, we\ninvestigate the efficiency of Large Language Models (LLMs), specifically\nChatGPT, to support users when dealing with scientific workflows. We performed\nthree user studies in two scientific domains to evaluate ChatGPT for\ncomprehending, adapting, and extending workflows. Our results indicate that\nLLMs efficiently interpret workflows but achieve lower performance for\nexchanging components or purposeful workflow extensions. We characterize their\nlimitations in these challenging scenarios and suggest future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2311.01825v2.pdf"
    },
    {
        "title": "AFPQ: Asymmetric Floating Point Quantization for LLMs",
        "authors": [
            "Yijia Zhang",
            "Sicheng Zhang",
            "Shijie Cao",
            "Dayou Du",
            "Jianyu Wei",
            "Ting Cao",
            "Ningyi Xu"
        ],
        "published": "2023-11-03T09:07:09Z",
        "summary": "Large language models (LLMs) show great performance in various tasks, but\nface deployment challenges from limited memory capacity and bandwidth. Low-bit\nweight quantization can save memory and accelerate inference. Although\nfloating-point (FP) formats show good performance in LLM quantization, they\ntend to perform poorly with small group sizes or sub-4 bits. We find the reason\nis that the absence of asymmetry in previous FP quantization makes it\nunsuitable for handling asymmetric value distribution of LLM weight tensors. In\nthis work, we propose asymmetric FP quantization (AFPQ), which sets separate\nscales for positive and negative values. Our method leads to large accuracy\nimprovements and can be easily plugged into other quantization methods,\nincluding GPTQ and AWQ, for better performance. Besides, no additional storage\nis needed compared with asymmetric integer (INT) quantization. The code is\navailable at https://github.com/zhangsichengsjtu/AFPQ.",
        "pdf_link": "https://arxiv.org/pdf/2311.01792v1.pdf"
    },
    {
        "title": "TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine",
        "authors": [
            "Guoxing Yang",
            "Jianyu Shi",
            "Zan Wang",
            "Xiaohong Liu",
            "Guangyu Wang"
        ],
        "published": "2023-11-03T08:54:50Z",
        "summary": "Pre-training and fine-tuning have emerged as a promising paradigm across\nvarious natural language processing (NLP) tasks. The effectiveness of\npretrained large language models (LLM) has witnessed further enhancement,\nholding potential for applications in the field of medicine, particularly in\nthe context of Traditional Chinese Medicine (TCM). However, the application of\nthese general models to specific domains often yields suboptimal results,\nprimarily due to challenges like lack of domain knowledge, unique objectives,\nand computational efficiency. Furthermore, their effectiveness in specialized\ndomains, such as Traditional Chinese Medicine, requires comprehensive\nevaluation. To address the above issues, we propose a novel domain specific\nTCMDA (TCM Domain Adaptation) approach, efficient pre-training with\ndomain-specific corpus. Specifically, we first construct a large TCM-specific\ncorpus, TCM-Corpus-1B, by identifying domain keywords and retreving from\ngeneral corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained\nmodel's weights and uses rank decomposition matrices to efficiently train\nspecific dense layers for pre-training and fine-tuning, efficiently aligning\nthe model with TCM-related tasks, namely TCM-GPT-7B. We further conducted\nextensive experiments on two TCM tasks, including TCM examination and TCM\ndiagnosis. TCM-GPT-7B archived the best performance across both datasets,\noutperforming other models by relative increments of 17% and 12% in accuracy,\nrespectively. To the best of our knowledge, our study represents the pioneering\nvalidation of domain adaptation of a large language model with 7 billion\nparameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B\nmodel once accepted to facilitate interdisciplinary development in TCM and NLP,\nserving as the foundation for further study.",
        "pdf_link": "https://arxiv.org/pdf/2311.01786v1.pdf"
    },
    {
        "title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
        "authors": [
            "Yiduo Guo",
            "Zekai Zhang",
            "Yaobo Liang",
            "Dongyan Zhao",
            "Nan Duan"
        ],
        "published": "2023-11-03T08:06:35Z",
        "summary": "Recent evaluations of Large Language Models (LLMs) have centered around\ntesting their zero-shot/few-shot capabilities for basic natural language tasks\nand their ability to translate instructions into tool APIs. However, the\nevaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal\ninstructions in a complex multi-modal environment has not been investigated. To\naddress this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark\nto assess LLMs' ability to create and edit PPT files based on user\ninstructions. It contains 279 multi-turn sessions covering diverse topics and\nhundreds of instructions involving multi-modal operations. We also propose the\nPPTX-Match Evaluation System that evaluates if LLMs finish the instruction\nbased on the prediction file rather than the label API sequence, thus it\nsupports various LLM-generated API sequences. We measure 3 closed LLMs and 6\nopen-source LLMs. The results show that GPT-4 outperforms other LLMs with\n75.1\\% accuracy in single-turn dialogue testing but faces challenges in\ncompleting entire sessions, achieving just 6\\% session accuracy. We find three\nmain error causes in our benchmark: error accumulation in the multi-turn\nsession, long PPT template processing, and multi-modality perception. These\npose great challenges for future LLM and agent systems. We release the data,\ncode, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.",
        "pdf_link": "https://arxiv.org/pdf/2311.01767v2.pdf"
    },
    {
        "title": "FinGPT: Large Generative Models for a Small Language",
        "authors": [
            "Risto Luukkonen",
            "Ville Komulainen",
            "Jouni Luoma",
            "Anni Eskelinen",
            "Jenna Kanerva",
            "Hanna-Mari Kupari",
            "Filip Ginter",
            "Veronika Laippala",
            "Niklas Muennighoff",
            "Aleksandra Piktus",
            "Thomas Wang",
            "Nouamane Tazi",
            "Teven Le Scao",
            "Thomas Wolf",
            "Osma Suominen",
            "Samuli Sairanen",
            "Mikko Merioksa",
            "Jyrki Heinonen",
            "Aija Vahtola",
            "Samuel Antao",
            "Sampo Pyysalo"
        ],
        "published": "2023-11-03T08:05:04Z",
        "summary": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most\nopen models have very limited coverage of smaller languages and LLM work tends\nto focus on languages where nearly unlimited data is available for pretraining.\nIn this work, we study the challenges of creating LLMs for Finnish, a language\nspoken by less than 0.1% of the world population. We compile an extensive\ndataset of Finnish combining web crawls, news, social media and eBooks. We\npursue two approaches to pretrain models: 1) we train seven monolingual models\nfrom scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the\npretraining of the multilingual BLOOM model on a mix of its original training\ndata and Finnish, resulting in a 176 billion parameter model we call BLUUMI.\nFor model evaluation, we introduce FIN-bench, a version of BIG-bench with\nFinnish tasks. We also assess other model qualities such as toxicity and bias.\nOur models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "pdf_link": "https://arxiv.org/pdf/2311.05640v1.pdf"
    },
    {
        "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models",
        "authors": [
            "Sean Xie",
            "Soroush Vosoughi",
            "Saeed Hassanpour"
        ],
        "published": "2023-11-03T05:55:32Z",
        "summary": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), but their lack of interpretability has been a major\nconcern. Current methods for interpreting LLMs are post hoc, applied after\ninference time, and have limitations such as their focus on low-level features\nand lack of explainability at higher level text units. In this work, we\nintroduce proto-lm, a prototypical network-based white-box framework that\nallows LLMs to learn immediately interpretable embeddings during the\nfine-tuning stage while maintaining competitive performance. Our method's\napplicability and interpretability are demonstrated through experiments on a\nwide range of NLP tasks, and our results indicate a new possibility of creating\ninterpretable models without sacrificing performance. This novel approach to\ninterpretability in LLMs can pave the way for more interpretable models without\nthe need to sacrifice performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.01732v2.pdf"
    },
    {
        "title": "Successor Features for Efficient Multisubject Controlled Text Generation",
        "authors": [
            "Meng Cao",
            "Mehdi Fatemi",
            "Jackie Chi Kit Cheung",
            "Samira Shabanian"
        ],
        "published": "2023-11-03T00:17:08Z",
        "summary": "While large language models (LLMs) have achieved impressive performance in\ngenerating fluent and realistic text, controlling the generated text so that it\nexhibits properties such as safety, factuality, and non-toxicity remains\nchallenging. % such as DExperts, GeDi, and rectification Existing\ndecoding-based methods are static in terms of the dimension of control; if the\ntarget subject is changed, they require new training. Moreover, it can quickly\nbecome prohibitive to concurrently control multiple subjects. In this work, we\nintroduce SF-GEN, which is grounded in two primary concepts: successor features\n(SFs) to decouple the LLM's dynamics from task-specific rewards, and language\nmodel rectification to proportionally adjust the probability of selecting a\ntoken based on the likelihood that the finished text becomes undesired. SF-GEN\nseamlessly integrates the two to enable dynamic steering of text generation\nwith no need to alter the LLM's parameters. Thanks to the decoupling effect\ninduced by successor features, our method proves to be memory-wise and\ncomputationally efficient for training as well as decoding, especially when\ndealing with multiple target subjects. To the best of our knowledge, our\nresearch represents the first application of successor features in text\ngeneration. In addition to its computational efficiency, the resultant language\nproduced by our method is comparable to the SOTA (and outperforms baselines) in\nboth control measures as well as language quality, which we demonstrate through\na series of experiments in various controllable text generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.04921v1.pdf"
    },
    {
        "title": "Preserving the knowledge of long clinical texts using aggregated ensembles of large language models",
        "authors": [
            "Mohammad Junayed Hasan",
            "Suhra Noor",
            "Mohammad Ashrafuzzaman Khan"
        ],
        "published": "2023-11-02T19:50:02Z",
        "summary": "Clinical texts, such as admission notes, discharge summaries, and progress\nnotes, contain rich and valuable information that can be used for various\nclinical outcome prediction tasks. However, applying large language models,\nsuch as BERT-based models, to clinical texts poses two major challenges: the\nlimitation of input length and the diversity of data sources. This paper\nproposes a novel method to preserve the knowledge of long clinical texts using\naggregated ensembles of large language models. Unlike previous studies which\nuse model ensembling or text aggregation methods separately, we combine\nensemble learning with text aggregation and train multiple large language\nmodels on two clinical outcome tasks: mortality prediction and length of stay\nprediction. We show that our method can achieve better results than baselines,\nensembling, and aggregation individually, and can improve the performance of\nlarge language models while handling long inputs and diverse datasets. We\nconduct extensive experiments on the admission notes from the MIMIC-III\nclinical database by combining multiple unstructured and high-dimensional\ndatasets, demonstrating our method's effectiveness and superiority over\nexisting approaches. We also provide a comprehensive analysis and discussion of\nour results, highlighting our method's applications and limitations for future\nresearch in the domain of clinical healthcare. The results and analysis of this\nstudy is supportive of our method assisting in clinical healthcare systems by\nenabling clinical decision-making with robust performance overcoming the\nchallenges of long text inputs and varied datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.01571v1.pdf"
    },
    {
        "title": "Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization",
        "authors": [
            "Bj√∂rn Deiseroth",
            "Max Meuer",
            "Nikolas Gritsch",
            "Constantin Eichenberg",
            "Patrick Schramowski",
            "Matthias A√üenmacher",
            "Kristian Kersting"
        ],
        "published": "2023-11-02T18:55:53Z",
        "summary": "Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. However, their ever-increasing size has raised\nconcerns about their effective deployment and the need for LLM compression.\nThis study introduces the Divergent Token Metrics (DTMs), a novel approach to\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs measure token divergences that allow deeper insights into the subtleties\nof model compression, in particular, when evaluating components' impacts\nindividually. Utilizing the First Divergent Token Metric (FDTM) in model\nsparsification reveals that 25% of all attention components can be pruned\nbeyond 90% on the Llama-2 model family, still keeping SOTA performance. For\nquantization, FDTM suggests that more than 80% of parameters can be naively\ntransformed to int8 without special outlier management. These evaluations\nindicate the necessity of choosing appropriate compressions for parameters\nindividually -- and that FDTM can identify those -- while standard metrics\nresult in deteriorated outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2311.01544v3.pdf"
    },
    {
        "title": "GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks",
        "authors": [
            "Xinlu Zhang",
            "Yujie Lu",
            "Weizhi Wang",
            "An Yan",
            "Jun Yan",
            "Lianke Qin",
            "Heng Wang",
            "Xifeng Yan",
            "William Yang Wang",
            "Linda Ruth Petzold"
        ],
        "published": "2023-11-02T16:11:09Z",
        "summary": "Automatically evaluating vision-language tasks is challenging, especially\nwhen it comes to reflecting human judgments due to limitations in accounting\nfor fine-grained details. Although GPT-4V has shown promising results in\nvarious multi-modal tasks, leveraging GPT-4V as a generalist evaluator for\nthese tasks has not yet been systematically explored. We comprehensively\nvalidate GPT-4V's capabilities for evaluation purposes, addressing tasks\nranging from foundational image-to-text and text-to-image synthesis to\nhigh-level image-to-image translations and multi-images to text alignment. We\nemploy two evaluation methods, single-answer grading and pairwise comparison,\nusing GPT-4V. Notably, GPT-4V shows promising agreement with humans across\nvarious tasks and evaluation methods, demonstrating immense potential for\nmulti-modal LLMs as evaluators. Despite limitations like restricted visual\nclarity grading and real-world complex reasoning, its ability to provide\nhuman-aligned scores enriched with detailed explanations is promising for\nuniversal automatic evaluator.",
        "pdf_link": "https://arxiv.org/pdf/2311.01361v1.pdf"
    },
    {
        "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
        "authors": [
            "Lovisa Hagstr√∂m",
            "Denitsa Saynova",
            "Tobias Norlund",
            "Moa Johansson",
            "Richard Johansson"
        ],
        "published": "2023-11-02T15:20:11Z",
        "summary": "Large Language Models (LLMs) make natural interfaces to factual knowledge,\nbut their usefulness is limited by their tendency to deliver inconsistent\nanswers to semantically equivalent questions. For example, a model might\npredict both \"Anne Redpath passed away in Edinburgh.\" and \"Anne Redpath's life\nended in London.\" In this work, we identify potential causes of inconsistency\nand evaluate the effectiveness of two mitigation strategies: up-scaling and\naugmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas\nmodels show that both strategies reduce inconsistency while retrieval\naugmentation is considerably more efficient. We further consider and\ndisentangle the consistency contributions of different components of Atlas. For\nall LMs evaluated we find that syntactical form and other evaluation task\nartifacts impact consistency. Taken together, our results provide a better\nunderstanding of the factors affecting the factual consistency of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2311.01307v1.pdf"
    },
    {
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
            "Ke Hong",
            "Guohao Dai",
            "Jiaming Xu",
            "Qiuli Mao",
            "Xiuhong Li",
            "Jun Liu",
            "Kangdi Chen",
            "Yuhan Dong",
            "Yu Wang"
        ],
        "published": "2023-11-02T14:57:03Z",
        "summary": "As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01282v4.pdf"
    },
    {
        "title": "Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations",
        "authors": [
            "Hanglei Zhang",
            "Yiwei Guo",
            "Sen Liu",
            "Xie Chen",
            "Kai Yu"
        ],
        "published": "2023-11-02T14:20:37Z",
        "summary": "Expressive text-to-speech (TTS) aims to synthesize speeches with human-like\ntones, moods, or even artistic attributes. Recent advancements in expressive\nTTS empower users with the ability to directly control synthesis style through\nnatural language prompts. However, these methods often require excessive\ntraining with a significant amount of style-annotated data, which can be\nchallenging to acquire. Moreover, they may have limited adaptability due to\nfixed style annotations. In this work, we present FreeStyleTTS (FS-TTS), a\ncontrollable expressive TTS model with minimal human annotations. Our approach\nutilizes a large language model (LLM) to transform expressive TTS into a style\nretrieval task. The LLM selects the best-matching style references from\nannotated utterances based on external style prompts, which can be raw input\ntext or natural language style descriptions. The selected reference guides the\nTTS pipeline to synthesize speeches with the intended style. This innovative\napproach provides flexible, versatile, and precise style control with minimal\nhuman workload. Experiments on a Mandarin storytelling corpus demonstrate\nFS-TTS's proficiency in leveraging LLM's semantic inference ability to retrieve\ndesired styles from either input text or user-defined descriptions. This\nresults in synthetic speeches that are closely aligned with the specified\nstyles.",
        "pdf_link": "https://arxiv.org/pdf/2311.01260v1.pdf"
    },
    {
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "authors": [
            "Mayank Kothyari",
            "Dhruva Dhingra",
            "Sunita Sarawagi",
            "Soumen Chakrabarti"
        ],
        "published": "2023-11-02T12:13:52Z",
        "summary": "Existing Text-to-SQL generators require the entire schema to be encoded with\nthe user text. This is expensive or impractical for large databases with tens\nof thousands of columns. Standard dense retrieval techniques are inadequate for\nschema subsetting of a large structured database, where the correct semantics\nof retrieval demands that we rank sets of schema elements rather than\nindividual elements. In response, we propose a two-stage process for effective\ncoverage during retrieval. First, we instruct an LLM to hallucinate a minimal\nDB schema deemed adequate to answer the query. We use the hallucinated schema\nto retrieve a subset of the actual schema, by composing the results from\nmultiple dense retrievals. Remarkably, hallucination $\\unicode{x2013}$\ngenerally considered a nuisance $\\unicode{x2013}$ turns out to be actually\nuseful as a bridging mechanism. Since no existing benchmarks exist for schema\nsubsetting on large databases, we introduce three benchmarks. Two\nsemi-synthetic datasets are derived from the union of schemas in two well-known\ndatasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements\nrespectively. A real-life benchmark called SocialDB is sourced from an actual\nlarge data warehouse comprising 17844 schema elements. We show that our method1\nleads to significantly higher recall than SOTA retrieval-based augmentation\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2311.01173v1.pdf"
    },
    {
        "title": "Revisiting the Knowledge Injection Frameworks",
        "authors": [
            "Peng Fu",
            "Yiming Zhang",
            "Haobo Wang",
            "Weikang Qiu",
            "Junbo Zhao"
        ],
        "published": "2023-11-02T11:18:16Z",
        "summary": "In recent years, large language models (LLMs), such as GPTs, have attained\ngreat impact worldwide. However, how to adapt these LLMs to better suit the\nvertical domain-specific tasks by utilizing external knowledge remains not\ncompletely solved. Indeed, there have emerged a few works on this line where\nmost of them rely on an alignment heuristic that is built to inject the\ncorresponding knowledge tuple into the associated text sample.\n  However, despite the promise, we identify a pivotal problem in this work\nubiquitously. Simply put, we find that injecting unaligned (i.e., random)\nknowledge tuple into the LLMs achieves comparable (and sometimes better)\nresults than the aligned knowledge being injected. We therefore take a thorough\ninvestigation of this frustrating finding on a variety of related prior work\nand further provide a chain of potential interpretations for the phenomenon.\nBased on all that, we offer a simple remediated technique. Briefly, the core of\nthis technique is rooted in an ideological emphasis on the pruning and\npurification of the external knowledge base to be injected into LLMs. At last,\nwe show that by integrating this technique into most (if not all) knowledge\ninjection frameworks and recent LLMs, it manages to overcome the aforementioned\nsanity problem and further pushes the boundary of the performance of the\ndomain-adaptive LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01150v1.pdf"
    },
    {
        "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
        "authors": [
            "Jianghao Chen",
            "Pu Jian",
            "Tengxiao Xi",
            "Dongyi Yi",
            "Qianlong Du",
            "Chenglin Ding",
            "Guibo Zhu",
            "Chengqing Zong",
            "Jinqiao Wang",
            "Jiajun Zhang"
        ],
        "published": "2023-11-02T11:13:51Z",
        "summary": "During the development of large language models (LLMs), the scale and quality\nof the pre-training data play a crucial role in shaping LLMs' capabilities. To\naccelerate the research of LLMs, several large-scale datasets, such as C4 [1],\nPile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.\nHowever, most of the released corpus focus mainly on English, and there is\nstill lack of complete tool-chain for extracting clean texts from web data.\nFurthermore, fine-grained information of the corpus, e.g. the quality of each\ntext, is missing. To address these challenges, we propose in this paper a new\ncomplete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.\nFirst, similar to previous work, manually crafted rules are employed to discard\nexplicit noisy texts from the raw crawled web contents. Second, a well-designed\nevaluation model is leveraged to assess the remaining relatively clean data,\nand each text is assigned a specific quality score. Finally, we can easily\nutilize an appropriate threshold to select the high-quality pre-training data\nfor Chinese. Using our proposed approach, we release the largest and latest\nlarge-scale high-quality Chinese web text ChineseWebText, which consists of\n1.42 TB and each text is associated with a quality score, facilitating the LLM\nresearchers to choose the data according to the desired quality thresholds. We\nalso release a much cleaner subset of 600 GB Chinese data with the quality\nexceeding 90%.",
        "pdf_link": "https://arxiv.org/pdf/2311.01149v2.pdf"
    },
    {
        "title": "Making Harmful Behaviors Unlearnable for Large Language Models",
        "authors": [
            "Xin Zhou",
            "Yi Lu",
            "Ruotian Ma",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-11-02T09:18:21Z",
        "summary": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants in various domains. To meet the requirements of different\napplications, LLMs are often customized by further fine-tuning. However, the\npowerful learning ability of LLMs not only enables them to acquire new tasks\nbut also makes them susceptible to learning undesired behaviors. For example,\neven safety-aligned LLMs can be easily fine-tuned into harmful assistants as\nthe fine-tuning data often contains implicit or explicit harmful content. Can\nwe train LLMs on harmful data without learning harmful behaviors? This paper\nproposes a controllable training framework that makes harmful behaviors\nunlearnable during the fine-tuning process. Specifically, we introduce\n``security vectors'', a few new parameters that can be separated from the LLM,\nto ensure LLM's responses are consistent with the harmful behavior. Security\nvectors are activated during fine-tuning, the consistent behavior makes LLM\nbelieve that such behavior has already been learned, there is no need to\nfurther optimize for harmful data. During inference, we can deactivate security\nvectors to restore the LLM's normal behavior. The experimental results show\nthat the security vectors generated by 100 harmful samples are enough to\nprevent LLM from learning 1000 harmful samples, while preserving the ability to\nlearn other useful information.",
        "pdf_link": "https://arxiv.org/pdf/2311.02105v1.pdf"
    },
    {
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
        "authors": [
            "Zhenjie Yang",
            "Xiaosong Jia",
            "Hongyang Li",
            "Junchi Yan"
        ],
        "published": "2023-11-02T07:23:33Z",
        "summary": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
        "pdf_link": "https://arxiv.org/pdf/2311.01043v3.pdf"
    },
    {
        "title": "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism",
        "authors": [
            "Lang Cao"
        ],
        "published": "2023-11-02T07:20:49Z",
        "summary": "Large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, enabling them to answer a wide range\nof questions across various domains. However, these models are not flawless and\noften produce responses that contain errors or misinformation. These\ninaccuracies, commonly referred to as hallucinations, render LLMs unreliable\nand even unusable in many scenarios. In this paper, our focus is on mitigating\nthe issue of hallucination in LLMs, particularly in the context of\nquestion-answering. Instead of attempting to answer all questions, we explore a\nrefusal mechanism that instructs LLMs to refuse to answer challenging questions\nin order to avoid errors. We then propose a simple yet effective solution\ncalled Learn to Refuse (L2R), which incorporates the refusal mechanism to\nenable LLMs to recognize and refuse to answer questions that they find\ndifficult to address. To achieve this, we utilize a structured knowledge base\nto represent all the LLM's understanding of the world, enabling it to provide\ntraceable gold knowledge. This knowledge base is separate from the LLM and\ninitially empty, and it is progressively expanded with validated knowledge.\nWhen an LLM encounters questions outside its domain, the system recognizes its\nknowledge scope and determines whether it can answer the question\nindependently. Additionally, we introduce a method for automatically and\nefficiently expanding the knowledge base of LLMs. Through qualitative and\nquantitative analysis, we demonstrate that our approach enhances the\ncontrollability and reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01041v1.pdf"
    },
    {
        "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
        "authors": [
            "Sam Toyer",
            "Olivia Watkins",
            "Ethan Adrian Mendes",
            "Justin Svegliato",
            "Luke Bailey",
            "Tiffany Wang",
            "Isaac Ong",
            "Karim Elmaaroufi",
            "Pieter Abbeel",
            "Trevor Darrell",
            "Alan Ritter",
            "Stuart Russell"
        ],
        "published": "2023-11-02T06:13:36Z",
        "summary": "While Large Language Models (LLMs) are increasingly being used in real-world\napplications, they remain vulnerable to prompt injection attacks: malicious\nthird party prompts that subvert the intent of the system designer. To help\nresearchers study this problem, we present a dataset of over 126,000 prompt\ninjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\nall created by players of an online game called Tensor Trust. To the best of\nour knowledge, this is currently the largest dataset of human-generated\nadversarial examples for instruction-following LLMs. The attacks in our dataset\nhave a lot of easily interpretable stucture, and shed light on the weaknesses\nof LLMs. We also use the dataset to create a benchmark for resistance to two\ntypes of prompt injection, which we refer to as prompt extraction and prompt\nhijacking. Our benchmark results show that many models are vulnerable to the\nattack strategies in the Tensor Trust dataset. Furthermore, we show that some\nattack strategies from the dataset generalize to deployed LLM-based\napplications, even though they have a very different set of constraints to the\ngame. We release all data and source code at https://tensortrust.ai/paper",
        "pdf_link": "https://arxiv.org/pdf/2311.01011v1.pdf"
    },
    {
        "title": "POS: A Prompts Optimization Suite for Augmenting Text-to-Video Generation",
        "authors": [
            "Shijie Ma",
            "Huayi Xu",
            "Mengjian Li",
            "Weidong Geng",
            "Meng Wang",
            "Yaxiong Wang"
        ],
        "published": "2023-11-02T02:33:09Z",
        "summary": "This paper targets to enhance the diffusion-based text-to-video generation by\nimproving the two input prompts, including the noise and the text. Accommodated\nwith this goal, we propose POS, a training-free Prompt Optimization Suite to\nboost text-to-video models. POS is motivated by two observations: (1) Video\ngeneration shows instability in terms of noise. Given the same text, different\nnoises lead to videos that differ significantly in terms of both frame quality\nand temporal consistency. This observation implies that there exists an optimal\nnoise matched to each textual input; To capture the potential noise, we propose\nan optimal noise approximator to approach the potential optimal noise.\nParticularly, the optimal noise approximator initially searches a video that\nclosely relates to the text prompt and then inverts it into the noise space to\nserve as an improved noise prompt for the textual input. (2) Improving the text\nprompt via LLMs often causes semantic deviation. Many existing text-to-vision\nworks have utilized LLMs to improve the text prompts for generation\nenhancement. However, existing methods often neglect the semantic alignment\nbetween the original text and the rewritten one. In response to this issue, we\ndesign a semantic-preserving rewriter to impose contraints in both rewritng and\ndenoising phrases to preserve the semantic consistency. Extensive experiments\non popular benchmarks show that our POS can improve the text-to-video models\nwith a clear margin. The code will be open-sourced.",
        "pdf_link": "https://arxiv.org/pdf/2311.00949v2.pdf"
    },
    {
        "title": "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place",
        "authors": [
            "Wentao Yuan",
            "Adithyavairavan Murali",
            "Arsalan Mousavian",
            "Dieter Fox"
        ],
        "published": "2023-11-02T01:42:52Z",
        "summary": "With the advent of large language models and large-scale robotic datasets,\nthere has been tremendous progress in high-level decision-making for object\nmanipulation. These generic models are able to interpret complex tasks using\nlanguage commands, but they often have difficulties generalizing to\nout-of-distribution objects due to the inability of low-level action\nprimitives. In contrast, existing task-specific models excel in low-level\nmanipulation of unknown objects, but only work for a single type of action. To\nbridge this gap, we present M2T2, a single model that supplies different types\nof low-level actions that work robustly on arbitrary objects in cluttered\nscenes. M2T2 is a transformer model which reasons about contact points and\npredicts valid gripper poses for different action modes given a raw point cloud\nof the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2\nachieves zero-shot sim2real transfer on the real robot, outperforming the\nbaseline system with state-of-the-art task-specific models by about 19% in\noverall performance and 37.5% in challenging scenes where the object needs to\nbe re-oriented for collision-free placement. M2T2 also achieves\nstate-of-the-art results on a subset of language conditioned tasks in RLBench.\nVideos of robot experiments on unseen objects in both real world and simulation\nare available on our project website https://m2-t2.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2311.00926v1.pdf"
    },
    {
        "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
        "authors": [
            "Zedian Xiao",
            "William Held",
            "Yanchen Liu",
            "Diyi Yang"
        ],
        "published": "2023-11-02T01:17:29Z",
        "summary": "Large Language Models (LLMs) are trained on corpora disproportionally\nweighted in favor of Standard American English. As a result, speakers of other\ndialects experience significantly more failures when interacting with these\ntechnologies. In practice, these speakers often accommodate their speech to be\nbetter understood. Our work shares the belief that language technologies should\nbe designed to accommodate the diversity in English dialects and not the other\nway around. However, prior works on dialect struggle with generalizing to\nevolving and emerging dialects in a scalable manner. To fill this gap, our\nmethod, HyperLoRA, leverages expert linguistic knowledge to enable\nresource-efficient adaptation via hypernetworks. By disentangling\ndialect-specific and cross-dialectal information, HyperLoRA improves\ngeneralization to unseen dialects in a task-agnostic fashion. Not only is\nHyperLoRA more scalable in the number of parameters, but it also achieves the\nbest or most competitive performance across 5 dialects in a zero-shot setting.\nIn this way, our approach facilitates access to language technology for\nbillions of English dialect speakers who are traditionally underrepresented.",
        "pdf_link": "https://arxiv.org/pdf/2311.00915v1.pdf"
    },
    {
        "title": "Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code",
        "authors": [
            "Mohammed Latif Siddiq",
            "Joanna C. S. Santos"
        ],
        "published": "2023-11-01T22:46:31Z",
        "summary": "With the growing popularity of Large Language Models (e.g. GitHub Copilot,\nChatGPT, etc.) in software engineers' daily practices, it is important to\nensure that the code generated by these tools is not only functionally correct\nbut also free of vulnerabilities. Although LLMs can help developers to be more\nproductive, prior empirical studies have shown that LLMs can generate insecure\ncode. There are two contributing factors to the insecure code generation.\nFirst, existing datasets used to evaluate Large Language Models (LLMs) do not\nadequately represent genuine software engineering tasks sensitive to security.\nInstead, they are often based on competitive programming challenges or\nclassroom-type coding tasks. In real-world applications, the code produced is\nintegrated into larger codebases, introducing potential security risks. There's\na clear absence of benchmarks that focus on evaluating the security of the\ngenerated code. Second, existing evaluation metrics primarily focus on the\nfunctional correctness of the generated code while ignoring security\nconsiderations. Metrics such as pass@k gauge the probability of obtaining the\ncorrect code in the top k suggestions. Other popular metrics like BLEU,\nCodeBLEU, ROUGE, and METEOR similarly emphasize functional accuracy, neglecting\nsecurity implications. In light of these research gaps, in this paper, we\ndescribed SALLM, a framework to benchmark LLMs' abilities to generate secure\ncode systematically. This framework has three major components: a novel dataset\nof security-centric Python prompts, an evaluation environment to test the\ngenerated code, and novel metrics to evaluate the models' performance from the\nperspective of secure code generation.",
        "pdf_link": "https://arxiv.org/pdf/2311.00889v1.pdf"
    },
    {
        "title": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models",
        "authors": [
            "Steve Yadlowsky",
            "Lyric Doshi",
            "Nilesh Tripuraneni"
        ],
        "published": "2023-11-01T21:41:08Z",
        "summary": "Transformer models, notably large language models (LLMs), have the remarkable\nability to perform in-context learning (ICL) -- to perform new tasks when\nprompted with unseen input-output examples without any explicit model training.\nIn this work, we study how effectively transformers can bridge between their\npretraining data mixture, comprised of multiple distinct task families, to\nidentify and learn new tasks in-context which are both inside and outside the\npretraining distribution. Building on previous work, we investigate this\nquestion in a controlled setting, where we study transformer models trained on\nsequences of $(x, f(x))$ pairs rather than natural language. Our empirical\nresults show transformers demonstrate near-optimal unsupervised model selection\ncapabilities, in their ability to first in-context identify different task\nfamilies and in-context learn within them when the task families are\nwell-represented in their pretraining data. However when presented with tasks\nor functions which are out-of-domain of their pretraining data, we demonstrate\nvarious failure modes of transformers and degradation of their generalization\nfor even simple extrapolation tasks. Together our results highlight that the\nimpressive ICL abilities of high-capacity sequence models may be more closely\ntied to the coverage of their pretraining data mixtures than inductive biases\nthat create fundamental generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.00871v1.pdf"
    },
    {
        "title": "SAGE: Smart home Agent with Grounded Execution",
        "authors": [
            "Dmitriy Rivkin",
            "Francois Hogan",
            "Amal Feriani",
            "Abhisek Konar",
            "Adam Sigal",
            "Steve Liu",
            "Greg Dudek"
        ],
        "published": "2023-11-01T18:36:28Z",
        "summary": "The common sense reasoning abilities and vast general knowledge of Large\nLanguage Models (LLMs) make them a natural fit for interpreting user requests\nin a Smart Home assistant context. LLMs, however, lack specific knowledge about\nthe user and their home limit their potential impact. SAGE (Smart Home Agent\nwith Grounded Execution), overcomes these and other limitations by using a\nscheme in which a user request triggers an LLM-controlled sequence of discrete\nactions. These actions can be used to retrieve information, interact with the\nuser, or manipulate device states. SAGE controls this process through a\ndynamically constructed tree of LLM prompts, which help it decide which action\nto take next, whether an action was successful, and when to terminate the\nprocess. The SAGE action set augments an LLM's capabilities to support some of\nthe most critical requirements for a Smart Home assistant. These include:\nflexible and scalable user preference management (\"is my team playing\ntonight?\"), access to any smart device's full functionality without\ndevice-specific code via API reading \"turn down the screen brightness on my\ndryer\", persistent device state monitoring (\"remind me to throw out the milk\nwhen I open the fridge\"), natural device references using only a photo of the\nroom (\"turn on the light on the dresser\"), and more. We introduce a benchmark\nof 50 new and challenging smart home tasks where SAGE achieves a 75% success\nrate, significantly outperforming existing LLM-enabled baselines (30% success\nrate).",
        "pdf_link": "https://arxiv.org/pdf/2311.00772v2.pdf"
    },
    {
        "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
        "authors": [
            "Zhan Ling",
            "Yunhao Fang",
            "Xuanlin Li",
            "Tongzhou Mu",
            "Mingu Lee",
            "Reza Pourreza",
            "Roland Memisevic",
            "Hao Su"
        ],
        "published": "2023-11-01T17:52:15Z",
        "summary": "Large Language Models (LLMs) have achieved tremendous progress, yet they\nstill often struggle with challenging reasoning problems. Current approaches\naddress this challenge by sampling or searching detailed and low-level\nreasoning chains. However, these methods are still limited in their exploration\ncapabilities, making it challenging for correct solutions to stand out in the\nhuge solution space. In this work, we unleash LLMs' creative potential for\nexploring multiple diverse problem solving strategies by framing an LLM as a\nhierarchical policy via in-context learning. This policy comprises of a\nvisionary leader that proposes multiple diverse high-level problem-solving\ntactics as hints, accompanied by a follower that executes detailed\nproblem-solving processes following each of the high-level instruction. The\nfollower uses each of the leader's directives as a guide and samples multiple\nreasoning chains to tackle the problem, generating a solution group for each\nleader proposal. Additionally, we propose an effective and efficient\ntournament-based approach to select among these explored solution groups to\nreach the final answer. Our approach produces meaningful and inspiring hints,\nenhances problem-solving strategy exploration, and improves the final answer\naccuracy on challenging problems in the MATH dataset. Code will be released at\nhttps://github.com/lz1oceani/LLM-As-Hierarchical-Policy.",
        "pdf_link": "https://arxiv.org/pdf/2311.00694v2.pdf"
    },
    {
        "title": "Improving Interpersonal Communication by Simulating Audiences with Language Models",
        "authors": [
            "Ryan Liu",
            "Howard Yen",
            "Raja Marjieh",
            "Thomas L. Griffiths",
            "Ranjay Krishna"
        ],
        "published": "2023-11-01T17:44:50Z",
        "summary": "How do we communicate with others to achieve our goals? We use our prior\nexperience or advice from others, or construct a candidate utterance by\npredicting how it will be received. However, our experiences are limited and\nbiased, and reasoning about potential outcomes can be difficult and cognitively\nchallenging. In this paper, we explore how we can leverage Large Language Model\n(LLM) simulations to help us communicate better. We propose the\nExplore-Generate-Simulate (EGS) framework, which takes as input any scenario\nwhere an individual is communicating to an audience with a goal they want to\nachieve. EGS (1) explores the solution space by producing a diverse set of\nadvice relevant to the scenario, (2) generates communication candidates\nconditioned on subsets of the advice, and (3) simulates the reactions from\nvarious audiences to determine both the best candidate and advice to use. We\nevaluate the framework on eight scenarios spanning the ten fundamental\nprocesses of interpersonal communication. For each scenario, we collect a\ndataset of human evaluations across candidates and baselines, and showcase that\nour framework's chosen candidate is preferred over popular generation\nmechanisms including Chain-of-Thought. We also find that audience simulations\nachieve reasonably high agreement with human raters across 5 of the 8\nscenarios. Finally, we demonstrate the generality of our framework by applying\nit to real-world scenarios described by users on web forums. Through\nevaluations and demonstrations, we show that EGS enhances the effectiveness and\noutcomes of goal-oriented communication across a variety of situations, thus\nopening up new possibilities for the application of large language models in\nrevolutionizing communication and decision-making processes.",
        "pdf_link": "https://arxiv.org/pdf/2311.00687v2.pdf"
    },
    {
        "title": "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Alexander I. Rudnicky"
        ],
        "published": "2023-11-01T17:43:35Z",
        "summary": "An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any fine-tuning. Such\nlong-context utilization capability relies heavily on a flexible positional\nembedding design. Upon investigating the flexibility of existing large\npre-trained Transformer language models, we find that the T5 family deserves a\ncloser look, as its positional embeddings capture rich and flexible attention\npatterns. However, T5 suffers from the dispersed attention issue: the longer\nthe input sequence, the flatter the attention distribution. To alleviate the\nissue, we propose two attention alignment strategies via temperature scaling.\nOur findings show improvement on the long-context utilization capability of T5\non language modeling, retrieval, multi-document question answering, and code\ncompletion tasks without any fine-tuning. This suggests that a flexible\npositional embedding design and attention alignment can go a long way toward\nTransformer length extrapolation.",
        "pdf_link": "https://arxiv.org/pdf/2311.00684v2.pdf"
    },
    {
        "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
        "authors": [
            "Xue-Yong Fu",
            "Md Tahmid Rahman Laskar",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2023-11-01T17:42:45Z",
        "summary": "In recent years, Large Language Models (LLMs) have gained immense attention\ndue to their notable emergent capabilities, surpassing those seen in earlier\nlanguage models. A particularly intriguing application of LLMs is their role as\nevaluators for texts produced by various generative models.\n  In this study, we delve into the potential of LLMs as reliable assessors of\nfactual consistency in summaries generated by text-generation models.\nInitially, we introduce an innovative approach for factuality assessment using\nLLMs. This entails employing a singular LLM for the entirety of the\nquestion-answering-based factuality scoring process. Following this, we examine\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\nagainst traditional measures and human annotations.\n  Contrary to initial expectations, our results indicate a lack of significant\ncorrelations between factuality metrics and human evaluations, specifically for\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\ntwo factuality subcategories. These consistent findings across various factual\nerror categories suggest a fundamental limitation in the current LLMs'\ncapability to accurately gauge factuality.\n  This version presents the information more concisely while maintaining the\nmain points and findings of the original text.",
        "pdf_link": "https://arxiv.org/pdf/2311.00681v1.pdf"
    },
    {
        "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
        "authors": [
            "Xiaoqian Li",
            "Ercong Nie",
            "Sheng Liang"
        ],
        "published": "2023-11-01T15:32:50Z",
        "summary": "The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.00587v2.pdf"
    },
    {
        "title": "Can Large Language Models Design Accurate Label Functions?",
        "authors": [
            "Naiqing Guan",
            "Kaiwen Chen",
            "Nick Koudas"
        ],
        "published": "2023-11-01T15:14:46Z",
        "summary": "Programmatic weak supervision methodologies facilitate the expedited labeling\nof extensive datasets through the use of label functions (LFs) that encapsulate\nheuristic data sources. Nonetheless, the creation of precise LFs necessitates\ndomain expertise and substantial endeavors. Recent advances in pre-trained\nlanguage models (PLMs) have exhibited substantial potential across diverse\ntasks. However, the capacity of PLMs to autonomously formulate accurate LFs\nremains an underexplored domain. In this research, we address this gap by\nintroducing DataSculpt, an interactive framework that harnesses PLMs for the\nautomated generation of LFs. Within DataSculpt, we incorporate an array of\nprompting techniques, instance selection strategies, and LF filtration methods\nto explore the expansive design landscape. Ultimately, we conduct a thorough\nassessment of DataSculpt's performance on 12 real-world datasets, encompassing\na range of tasks. This evaluation unveils both the strengths and limitations of\ncontemporary PLMs in LF design.",
        "pdf_link": "https://arxiv.org/pdf/2311.00739v1.pdf"
    },
    {
        "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation",
        "authors": [
            "Xiangjue Dong",
            "Yibo Wang",
            "Philip S. Yu",
            "James Caverlee"
        ],
        "published": "2023-11-01T05:31:46Z",
        "summary": "Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.00306v1.pdf"
    },
    {
        "title": "Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions",
        "authors": [
            "Taehyeon Kim",
            "Joonkee Kim",
            "Gihun Lee",
            "Se-Young Yun"
        ],
        "published": "2023-11-01T02:31:35Z",
        "summary": "While instruction-tuned language models have demonstrated impressive\nzero-shot generalization, these models often struggle to generate accurate\nresponses when faced with instructions that fall outside their training set.\nThis paper presents Instructive Decoding (ID), a simple yet effective approach\nthat augments the efficacy of instruction-tuned models. Specifically, ID\nadjusts the logits for next-token prediction in a contrastive manner, utilizing\npredictions generated from a manipulated version of the original instruction,\nreferred to as a noisy instruction. This noisy instruction aims to elicit\nresponses that could diverge from the intended instruction yet remain\nplausible. We conduct experiments across a spectrum of such noisy instructions,\nranging from those that insert semantic noise via random words to others like\n'opposite' that elicit the deviated responses. Our approach achieves\nconsiderable performance gains across various instruction-tuned models and\ntasks without necessitating any additional parameter updates. Notably,\nutilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum\ndivergence from the original instruction, consistently produces the most\nsignificant performance gains across multiple models and tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.00233v2.pdf"
    },
    {
        "title": "Is GPT Powerful Enough to Analyze the Emotions of Memes?",
        "authors": [
            "Jingjing Wang",
            "Joshua Luo",
            "Grace Yang",
            "Allen Hong",
            "Feng Luo"
        ],
        "published": "2023-11-01T01:57:48Z",
        "summary": "Large Language Models (LLMs), representing a significant achievement in\nartificial intelligence (AI) research, have demonstrated their ability in a\nmultitude of tasks. This project aims to explore the capabilities of GPT-3.5, a\nleading example of LLMs, in processing the sentiment analysis of Internet\nmemes. Memes, which include both verbal and visual aspects, act as a powerful\nyet complex tool for expressing ideas and sentiments, demanding an\nunderstanding of societal norms and cultural contexts. Notably, the detection\nand moderation of hateful memes pose a significant challenge due to their\nimplicit offensive nature. This project investigates GPT's proficiency in such\nsubjective tasks, revealing its strengths and potential limitations. The tasks\ninclude the classification of meme sentiment, determination of humor type, and\ndetection of implicit hate in memes. The performance evaluation, using datasets\nfrom SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative\nunderstanding of GPT responses against human annotations. Despite GPT's\nremarkable progress, our findings underscore the challenges faced by these\nmodels in handling subjective tasks, which are rooted in their inherent\nlimitations including contextual understanding, interpretation of implicit\nmeanings, and data biases. This research contributes to the broader discourse\non the applicability of AI in handling complex, context-dependent tasks, and\noffers valuable insights for future advancements.",
        "pdf_link": "https://arxiv.org/pdf/2311.00223v1.pdf"
    },
    {
        "title": "Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias",
        "authors": [
            "S. Lee",
            "T. Q. Peng",
            "M. H. Goldberg",
            "S. A. Rosenthal",
            "J. E. Kotcher",
            "E. W. Maibach",
            "A. Leiserowitz"
        ],
        "published": "2023-11-01T01:32:59Z",
        "summary": "Large language models (LLMs) have demonstrated their potential in social\nscience research by emulating human perceptions and behaviors, a concept\nreferred to as algorithmic fidelity. This study assesses the algorithmic\nfidelity and bias of LLMs by utilizing two nationally representative climate\nchange surveys. The LLMs were conditioned on demographics and/or psychological\ncovariates to simulate survey responses. The findings indicate that LLMs can\neffectively capture presidential voting behaviors but encounter challenges in\naccurately representing global warming perspectives when relevant covariates\nare not included. GPT-4 exhibits improved performance when conditioned on both\ndemographics and covariates. However, disparities emerge in LLM estimations of\nthe views of certain groups, with LLMs tending to underestimate worry about\nglobal warming among Black Americans. While highlighting the potential of LLMs\nto aid social science research, these results underscore the importance of\nmeticulous conditioning, model selection, survey question format, and bias\nassessment when employing LLMs for survey simulation. Further investigation\ninto prompt engineering and algorithm auditing is essential to harness the\npower of LLMs while addressing their inherent limitations.",
        "pdf_link": "https://arxiv.org/pdf/2311.00217v2.pdf"
    },
    {
        "title": "ChatGPT-Powered Hierarchical Comparisons for Image Classification",
        "authors": [
            "Zhiyuan Ren",
            "Yiyang Su",
            "Xiaoming Liu"
        ],
        "published": "2023-11-01T00:26:40Z",
        "summary": "The zero-shot open-vocabulary challenge in image classification is tackled by\npretrained vision-language models like CLIP, which benefit from incorporating\nclass-specific knowledge from large language models (LLMs) like ChatGPT.\nHowever, biases in CLIP lead to similar descriptions for distinct but related\nclasses, prompting our novel image classification framework via hierarchical\ncomparisons: using LLMs to recursively group classes into hierarchies and\nclassifying images by comparing image-text embeddings at each hierarchy level,\nresulting in an intuitive, effective, and explainable approach.",
        "pdf_link": "https://arxiv.org/pdf/2311.00206v1.pdf"
    },
    {
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
        "authors": [
            "Nathan Lambert",
            "Roberto Calandra"
        ],
        "published": "2023-10-31T21:52:41Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful\ntechnique to make large language models (LLMs) more capable in complex\nsettings. RLHF proceeds as collecting human preference data, training a reward\nmodel on said data, and optimizing a base ML model with respect to said reward\nfor extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many\nassumptions about how the various pieces fit together, such as a reward model\ncapturing human preferences and an RL optimizer extracting the right signal\nfrom a reward model. As the RLHF process involves many distinct design\ndecisions, it is easy to assume that multiple processes are correlated and\ntherefore numerically linked. This apparent correlation is often not true,\nwhere reward models are easily overoptimized or RL optimizers can reduce\nperformance on tasks not modeled in the data. Notable manifestations of models\ntrained with imperfect RLHF systems are those that are prone to refusing basic\nrequests for safety reasons or appearing lazy in generations. As chat model\nevaluation becomes increasingly nuanced, the reliance on a perceived link\nbetween reward model training, RL scores, and downstream performance drives\nthese issues, which we describe as an objective mismatch. In this paper, we\nillustrate the causes of this issue, reviewing relevant literature from\nmodel-based reinforcement learning, and argue for solutions. By solving\nobjective mismatch in RLHF, the ML models of the future will be more precisely\naligned to user instructions for both safety and helpfulness.",
        "pdf_link": "https://arxiv.org/pdf/2311.00168v2.pdf"
    },
    {
        "title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "authors": [
            "Pranav Gade",
            "Simon Lermen",
            "Charlie Rogers-Smith",
            "Jeffrey Ladish"
        ],
        "published": "2023-10-31T19:45:15Z",
        "summary": "Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.",
        "pdf_link": "https://arxiv.org/pdf/2311.00117v2.pdf"
    },
    {
        "title": "BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text",
        "authors": [
            "Aarohi Srivastava",
            "David Chiang"
        ],
        "published": "2023-10-31T19:44:50Z",
        "summary": "Real-world NLP applications often deal with nonstandard text (e.g.,\ndialectal, informal, or misspelled text). However, language models like BERT\ndeteriorate in the face of dialect variation or noise. How do we push BERT's\nmodeling capabilities to encompass nonstandard text? Fine-tuning helps, but it\nis designed for specializing a model to a task and does not seem to bring about\nthe deeper, more pervasive changes needed to adapt a model to nonstandard\nlanguage. In this paper, we introduce the novel idea of sandwiching BERT's\nencoder stack between additional encoder layers trained to perform masked\nlanguage modeling on noisy text. We find that our approach, paired with recent\nwork on including character-level noise in fine-tuning data, can promote\nzero-shot transfer to dialectal text, as well as reduce the distance in the\nembedding space between words and their noisy counterparts.",
        "pdf_link": "https://arxiv.org/pdf/2311.00116v1.pdf"
    },
    {
        "title": "Filter bubbles and affective polarization in user-personalized large language model outputs",
        "authors": [
            "Tomo Lazovich"
        ],
        "published": "2023-10-31T18:19:28Z",
        "summary": "Echoing the history of search engines and social media content rankings, the\nadvent of large language models (LLMs) has led to a push for increased\npersonalization of model outputs to individual users. In the past, personalized\nrecommendations and ranking systems have been linked to the development of\nfilter bubbles (serving content that may confirm a user's existing biases) and\naffective polarization (strong negative sentiment towards those with differing\nviews). In this work, we explore how prompting a leading large language model,\nChatGPT-3.5, with a user's political affiliation prior to asking factual\nquestions about public figures and organizations leads to differing results. We\nobserve that left-leaning users tend to receive more positive statements about\nleft-leaning political figures and media outlets, while right-leaning users see\nmore positive statements about right-leaning entities. This pattern holds\nacross presidential candidates, members of the U.S. Senate, and media\norganizations with ratings from AllSides. When qualitatively evaluating some of\nthese outputs, there is evidence that particular facts are included or excluded\nbased on the user's political affiliation. These results illustrate that\npersonalizing LLMs based on user demographics carry the same risks of affective\npolarization and filter bubbles that have been seen in other personalized\ninternet technologies. This ``failure mode\" should be monitored closely as\nthere are more attempts to monetize and personalize these models.",
        "pdf_link": "https://arxiv.org/pdf/2311.14677v1.pdf"
    },
    {
        "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
        "authors": [
            "Simon Lermen",
            "Charlie Rogers-Smith",
            "Jeffrey Ladish"
        ],
        "published": "2023-10-31T16:55:06Z",
        "summary": "AI developers often apply safety alignment procedures to prevent the misuse\nof their AI systems. For example, before Meta released Llama 2-Chat, a\ncollection of instruction fine-tuned large language models, they invested\nheavily in safety training, incorporating extensive red-teaming and\nreinforcement learning from human feedback. However, it remains unclear how\nwell safety training guards against model misuse when attackers have access to\nmodel weights. We explore the robustness of safety training in language models\nby subversively fine-tuning the public weights of Llama 2-Chat. We employ\nlow-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of\nless than $200 per model and using only one GPU, we successfully undo the\nsafety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,\nour fine-tuning technique significantly reduces the rate at which the model\nrefuses to follow harmful instructions. We achieve a refusal rate below 1% for\nour 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method\nretains general performance, which we validate by comparing our fine-tuned\nmodels against Llama 2-Chat across two benchmarks. Additionally, we present a\nselection of harmful outputs produced by our models. While there is\nconsiderable uncertainty about the scope of risks from current models, it is\nlikely that future models will have significantly more dangerous capabilities,\nincluding the ability to hack into critical infrastructure, create dangerous\nbio-weapons, or autonomously replicate and adapt to new environments. We show\nthat subversive fine-tuning is practical and effective, and hence argue that\nevaluating risks from fine-tuning should be a core part of risk assessments for\nreleasing model weights.",
        "pdf_link": "https://arxiv.org/pdf/2310.20624v1.pdf"
    },
    {
        "title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding",
        "authors": [
            "Yuqi Wang",
            "Zeqiang Wang",
            "Wei Wang",
            "Qi Chen",
            "Kaizhu Huang",
            "Anh Nguyen",
            "Suparna De"
        ],
        "published": "2023-10-31T16:26:33Z",
        "summary": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.",
        "pdf_link": "https://arxiv.org/pdf/2310.20588v1.pdf"
    },
    {
        "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
        "authors": [
            "Ruizhe Shi",
            "Yuyao Liu",
            "Yanjie Ze",
            "Simon S. Du",
            "Huazhe Xu"
        ],
        "published": "2023-10-31T16:24:17Z",
        "summary": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks\nand closes the gap between value-based offline RL methods and decision\ntransformers in dense-reward tasks. In particular, our method demonstrates\nsuperior performance in scenarios with limited data samples.",
        "pdf_link": "https://arxiv.org/pdf/2310.20587v4.pdf"
    },
    {
        "title": "Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT",
        "authors": [
            "Aman Jaiswal",
            "Evangelos Milios"
        ],
        "published": "2023-10-31T15:41:08Z",
        "summary": "Transformer-based models, specifically BERT, have propelled research in\nvarious NLP tasks. However, these models are limited to a maximum token limit\nof 512 tokens. Consequently, this makes it non-trivial to apply it in a\npractical setting with long input. Various complex methods have claimed to\novercome this limit, but recent research questions the efficacy of these models\nacross different classification tasks. These complex architectures evaluated on\ncarefully curated long datasets perform at par or worse than simple baselines.\nIn this work, we propose a relatively simple extension to vanilla BERT\narchitecture called ChunkBERT that allows finetuning of any pretrained models\nto perform inference on arbitrarily long text. The proposed method is based on\nchunking token representations and CNN layers, making it compatible with any\npre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for\ncomparing long-text classification models across a variety of tasks (including\nbinary classification, multi-class classification, and multi-label\nclassification). A BERT model finetuned using the ChunkBERT method performs\nconsistently across long samples in the benchmark while utilizing only a\nfraction (6.25\\%) of the original memory footprint. These findings suggest that\nefficient finetuning and inference can be achieved through simple modifications\nto pre-trained BERT models.",
        "pdf_link": "https://arxiv.org/pdf/2310.20558v1.pdf"
    },
    {
        "title": "LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts",
        "authors": [
            "Sunhao Dai",
            "Yuqi Zhou",
            "Liang Pang",
            "Weihao Liu",
            "Xiaolin Hu",
            "Yong Liu",
            "Xiao Zhang",
            "Gang Wang",
            "Jun Xu"
        ],
        "published": "2023-10-31T14:42:23Z",
        "summary": "Recently, the emergence of large language models (LLMs) has revolutionized\nthe paradigm of information retrieval (IR) applications, especially in web\nsearch. With their remarkable capabilities in generating human-like texts, LLMs\nhave created enormous texts on the Internet. As a result, IR systems in the\nLLMs era are facing a new challenge: the indexed documents now are not only\nwritten by human beings but also automatically generated by the LLMs. How these\nLLM-generated documents influence the IR systems is a pressing and still\nunexplored question. In this work, we conduct a quantitative evaluation of\ndifferent IR models in scenarios where both human-written and LLM-generated\ntexts are involved. Surprisingly, our findings indicate that neural retrieval\nmodels tend to rank LLM-generated documents higher. We refer to this category\nof biases in neural retrieval models towards the LLM-generated text as the\n\\textbf{source bias}. Moreover, we discover that this bias is not confined to\nthe first-stage neural retrievers, but extends to the second-stage neural\nre-rankers. Then, we provide an in-depth analysis from the perspective of text\ncompression and observe that neural models can better understand the semantic\ninformation of LLM-generated text, which is further substantiated by our\ntheoretical analysis. To mitigate the source bias, we also propose a\nplug-and-play debiased constraint for the optimization objective, and\nexperimental results show the effectiveness. Finally, we discuss the potential\nsevere concerns stemming from the observed source bias and hope our findings\ncan serve as a critical wake-up call to the IR community and beyond. To\nfacilitate future explorations of IR in the LLM era, the constructed two new\nbenchmarks and codes will later be available at\n\\url{https://github.com/KID-22/LLM4IR-Bias}.",
        "pdf_link": "https://arxiv.org/pdf/2310.20501v2.pdf"
    },
    {
        "title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Yufei Wang",
            "Xingshan Zeng",
            "Wanjun Zhong",
            "Liangyou Li",
            "Fei Mi",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Wei Wang"
        ],
        "published": "2023-10-31T12:32:38Z",
        "summary": "The ability to follow instructions is crucial for Large Language Models\n(LLMs) to handle various real-world applications. Existing benchmarks primarily\nfocus on evaluating pure response quality, rather than assessing whether the\nresponse follows constraints stated in the instruction. To fill this research\ngap, in this paper, we propose FollowBench, a Multi-level Fine-grained\nConstraints Following Benchmark for LLMs. FollowBench comprehensively includes\nfive different types (i.e., Content, Situation, Style, Format, and Example) of\nfine-grained constraints. To enable a precise constraint following estimation\non diverse difficulties, we introduce a Multi-level mechanism that\nincrementally adds a single constraint to the initial instruction at each\nincreased level. To assess whether LLMs' outputs have satisfied every\nindividual constraint, we propose to prompt strong LLMs with\nconstraint-evolution paths to handle challenging open-ended instructions. By\nevaluating ten closed-source and open-source popular LLMs on FollowBench, we\nhighlight the weaknesses of LLMs in instruction following and point towards\npotential avenues for future work. The data and code are publicly available at\nhttps://github.com/YJiangcm/FollowBench.",
        "pdf_link": "https://arxiv.org/pdf/2310.20410v2.pdf"
    },
    {
        "title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing",
        "authors": [
            "Kaixin Li",
            "Qisheng Hu",
            "Xu Zhao",
            "Hui Chen",
            "Yuxi Xie",
            "Tiedong Liu",
            "Qizhe Xie",
            "Junxian He"
        ],
        "published": "2023-10-31T10:15:35Z",
        "summary": "Code editing encompasses a variety of pragmatic tasks that developers deal\nwith daily. Despite its relevance and practical usefulness, automatic code\nediting remains an underexplored area in the evolution of deep learning models,\npartly due to data scarcity. In this work, we explore the use of Large Language\nModels (LLMs) to edit code based on user instructions. Evaluated on a novel\nhuman-written execution-based benchmark dubbed EditEval, we found current\nmodels often struggle to fulfill the instructions. In light of this, we\ncontribute InstructCoder, the first instruction-tuning dataset designed to\nadapt LLMs for general-purpose code editing, containing high-diversity\ncode-editing tasks such as comment insertion, code optimization, and code\nrefactoring. It consists of over 114,000 instruction-input-output triplets and\ncovers multiple distinct code editing scenarios. The collection process starts\nwith filtered commit data sourced from GitHub Python repositories as seeds.\nSubsequently, the dataset is systematically expanded through an iterative\nprocess, where both seed and generated tasks are used to prompt ChatGPT for\nmore data. Our findings reveal that open-source LLMs fine-tuned on\nInstructCoder can significantly enhance the accuracy of code edits, exhibiting\nsuperior code-editing performance matching advanced proprietary LLMs. The\ndatasets and the source code are publicly available at\nhttps://github.com/qishenghu/CodeInstruct.",
        "pdf_link": "https://arxiv.org/pdf/2310.20329v3.pdf"
    },
    {
        "title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision",
        "authors": [
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Kamalika Das",
            "Sricharan Kumar"
        ],
        "published": "2023-10-31T03:39:23Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, their suitability for domain-specific tasks, is limited\ndue to their immense scale at deployment, susceptibility to misinformation, and\nmore importantly, high data annotation costs. We propose a novel Interactive\nMulti-Fidelity Learning (IMFL) framework for the cost-effective development of\nsmall domain-specific LMs under limited annotation budgets. Our approach\nformulates the domain-specific fine-tuning process as a multi-fidelity learning\nproblem, focusing on identifying the optimal acquisition strategy that balances\nbetween low-fidelity automatic LLM annotations and high-fidelity human\nannotations to maximize model performance. We further propose an\nexploration-exploitation query strategy that enhances annotation diversity and\ninformativeness, incorporating two innovative designs: 1) prompt retrieval that\nselects in-context examples from human-annotated samples to improve LLM\nannotation, and 2) variable batch size that controls the order for choosing\neach fidelity to facilitate knowledge distillation, ultimately enhancing\nannotation quality. Extensive experiments on financial and medical tasks\ndemonstrate that IMFL achieves superior performance compared with single\nfidelity annotations. Given a limited budget of human annotation, IMFL\nsignificantly outperforms the human annotation baselines in all four tasks and\nachieves very close performance as human annotations on two of the tasks. These\npromising results suggest that the high human annotation costs in\ndomain-specific tasks can be significantly reduced by employing IMFL, which\nutilizes fewer human annotations, supplemented with cheaper and faster LLM\n(e.g., GPT-3.5) annotations to achieve comparable performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.20153v1.pdf"
    },
    {
        "title": "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
        "authors": [
            "Chris Richardson",
            "Yao Zhang",
            "Kellen Gillespie",
            "Sudipta Kar",
            "Arshdeep Singh",
            "Zeynab Raeesy",
            "Omar Zia Khan",
            "Abhinav Sethy"
        ],
        "published": "2023-10-30T23:40:41Z",
        "summary": "Personalization, the ability to tailor a system to individual users, is an\nessential factor in user experience with natural language processing (NLP)\nsystems. With the emergence of Large Language Models (LLMs), a key question is\nhow to leverage these models to better personalize user experiences. To\npersonalize a language model's output, a straightforward approach is to\nincorporate past user data into the language model prompt, but this approach\ncan result in lengthy inputs exceeding limitations on input length and\nincurring latency and cost issues. Existing approaches tackle such challenges\nby selectively extracting relevant user data (i.e. selective retrieval) to\nconstruct a prompt for downstream tasks. However, retrieval-based methods are\nlimited by potential information loss, lack of more profound user\nunderstanding, and cold-start challenges. To overcome these limitations, we\npropose a novel summary-augmented approach by extending retrieval-augmented\npersonalization with task-aware user summaries generated by LLMs. The summaries\ncan be generated and stored offline, enabling real-world systems with runtime\nconstraints like voice assistants to leverage the power of LLMs. Experiments\nshow our method with 75% less of retrieved user data is on-par or outperforms\nretrieval augmentation on most tasks in the LaMP personalization benchmark. We\ndemonstrate that offline summarization via LLMs and runtime retrieval enables\nbetter performance for personalization on a range of tasks under practical\nconstraints.",
        "pdf_link": "https://arxiv.org/pdf/2310.20081v1.pdf"
    },
    {
        "title": "The Expressibility of Polynomial based Attention Scheme",
        "authors": [
            "Zhao Song",
            "Guangyi Xu",
            "Junze Yin"
        ],
        "published": "2023-10-30T22:16:18Z",
        "summary": "Large language models (LLMs) have significantly improved various aspects of\nour daily lives. These models have impacted numerous domains, from healthcare\nto education, enhancing productivity, decision-making processes, and\naccessibility. As a result, they have influenced and, to some extent, reshaped\npeople's lifestyles. However, the quadratic complexity of attention in\ntransformer architectures poses a challenge when scaling up these models for\nprocessing long textual contexts. This issue makes it impractical to train very\nlarge models on lengthy texts or use them efficiently during inference. While a\nrecent study by [KMZ23] introduced a technique that replaces the softmax with a\npolynomial function and polynomial sketching to speed up attention mechanisms,\nthe theoretical understandings of this new approach are not yet well\nunderstood.\n  In this paper, we offer a theoretical analysis of the expressive capabilities\nof polynomial attention. Our study reveals a disparity in the ability of\nhigh-degree and low-degree polynomial attention. Specifically, we construct two\ncarefully designed datasets, namely $\\mathcal{D}_0$ and $\\mathcal{D}_1$, where\n$\\mathcal{D}_1$ includes a feature with a significantly larger value compared\nto $\\mathcal{D}_0$. We demonstrate that with a sufficiently high degree\n$\\beta$, a single-layer polynomial attention network can distinguish between\n$\\mathcal{D}_0$ and $\\mathcal{D}_1$. However, with a low degree $\\beta$, the\nnetwork cannot effectively separate the two datasets. This analysis underscores\nthe greater effectiveness of high-degree polynomials in amplifying large values\nand distinguishing between datasets. Our analysis offers insight into the\nrepresentational capacity of polynomial attention and provides a rationale for\nincorporating higher-degree polynomials in attention mechanisms to capture\nintricate linguistic correlations.",
        "pdf_link": "https://arxiv.org/pdf/2310.20051v1.pdf"
    },
    {
        "title": "Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
        "authors": [
            "Prakamya Mishra",
            "Zonghai Yao",
            "Shuwei Chen",
            "Beining Wang",
            "Rohan Mittal",
            "Hong Yu"
        ],
        "published": "2023-10-30T21:33:22Z",
        "summary": "Large Language Models (LLMs) like the GPT and LLaMA families have\ndemonstrated exceptional capabilities in capturing and condensing critical\ncontextual information and achieving state-of-the-art performance in the\nsummarization task. However, community concerns about these models'\nhallucination issues continue to rise. LLMs sometimes generate factually\nhallucinated summaries, which can be extremely harmful in the clinical domain\nNLP tasks (e.g., clinical note summarization), where factually incorrect\nstatements can lead to critically erroneous diagnoses. Fine-tuning LLMs using\nhuman feedback has shown the promise of aligning LLMs to be factually\nconsistent during generation, but such training procedure requires high-quality\nhuman-annotated data, which can be extremely expensive to get in the clinical\ndomain. In this work, we propose a new pipeline using ChatGPT instead of human\nexperts to generate high-quality feedback data for improving factual\nconsistency in the clinical note summarization task. We focus specifically on\nedit feedback because recent work discusses the shortcomings of human alignment\nvia preference feedback in complex situations (such as clinical NLP tasks that\nrequire extensive expert knowledge), as well as some advantages of collecting\nedit feedback from domain experts. In addition, although GPT has reached the\nexpert level in many clinical NLP tasks (e.g., USMLE QA), there is not much\nprevious work discussing whether GPT can generate expert-level edit feedback\nfor LMs in the clinical note summarization task. We hope to fill this gap.\nFinally, our evaluations demonstrate the potential use of GPT edits in human\nalignment, especially from a factuality perspective.",
        "pdf_link": "https://arxiv.org/pdf/2310.20033v2.pdf"
    },
    {
        "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
        "authors": [
            "Joseph Gatto",
            "Omar Sharif",
            "Sarah Masud Preum"
        ],
        "published": "2023-10-30T17:18:10Z",
        "summary": "Stance detection on social media is challenging for Large Language Models\n(LLMs), as emerging slang and colloquial language in online conversations often\ncontain deeply implicit stance labels. Chain-of-Thought (COT) prompting has\nrecently been shown to improve performance on stance detection tasks --\nalleviating some of these issues. However, COT prompting still struggles with\nimplicit stance identification. This challenge arises because many samples are\ninitially challenging to comprehend before a model becomes familiar with the\nslang and evolving knowledge related to different topics, all of which need to\nbe acquired through the training data. In this study, we address this problem\nby introducing COT Embeddings which improve COT performance on stance detection\ntasks by embedding COT reasonings and integrating them into a traditional\nRoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text\nencoders can leverage COT reasonings with minor errors or hallucinations that\nwould otherwise distort the COT output label. 2) Text encoders can overlook\nmisleading COT reasoning when a sample's prediction heavily depends on\ndomain-specific patterns. Our model achieves SOTA performance on multiple\nstance detection datasets collected from social media.",
        "pdf_link": "https://arxiv.org/pdf/2310.19750v1.pdf"
    },
    {
        "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
        "authors": [
            "Qintong Li",
            "Leyang Cui",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2023-10-30T17:04:35Z",
        "summary": "Humans are widely involved in the evaluation of open-ended natural language\ngeneration tasks (NLG) that demand creativity, as automatic metrics often\nexhibit weak correlations with human judgments. Large language models (LLMs)\nrecently have emerged as a scalable and cost-effective alternative to human\nevaluations. However, both humans and LLMs have limitations, i.e., inherent\nsubjectivity and unreliable judgments, particularly for open-ended tasks that\nrequire adaptable metrics tailored to diverse task requirements. To explore the\nsynergy between humans and LLM-based evaluators and address the challenges of\nexisting inconsistent evaluation criteria in open-ended NLG tasks, we propose a\nCollaborative Evaluation pipeline CoEval, involving the design of a checklist\nof task-specific criteria and the detailed evaluation of texts, in which LLM\ngenerates initial ideation, and then humans engage in scrutiny. We conducted a\nseries of experiments to investigate the mutual effects between LLMs and humans\nin CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates\nlengthy texts, saving significant time and reducing human evaluation outliers.\nHuman scrutiny still plays a role, revising around 20% of LLM evaluation scores\nfor ultimate reliability.",
        "pdf_link": "https://arxiv.org/pdf/2310.19740v1.pdf"
    },
    {
        "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
        "authors": [
            "Leo Schwinn",
            "David Dobre",
            "Stephan G√ºnnemann",
            "Gauthier Gidel"
        ],
        "published": "2023-10-30T17:01:02Z",
        "summary": "Over the past decade, there has been extensive research aimed at enhancing\nthe robustness of neural networks, yet this problem remains vastly unsolved.\nHere, one major impediment has been the overestimation of the robustness of new\ndefense approaches due to faulty defense evaluations. Flawed robustness\nevaluations necessitate rectifications in subsequent works, dangerously slowing\ndown the research and providing a false sense of security. In this context, we\nwill face substantial challenges associated with an impending adversarial arms\nrace in natural language processing, specifically with closed-source Large\nLanguage Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We\nprovide a first set of prerequisites to improve the robustness assessment of\nnew approaches and reduce the amount of faulty evaluations. Additionally, we\nidentify embedding space attacks on LLMs as another viable threat model for the\npurposes of generating malicious content in open-sourced models. Finally, we\ndemonstrate on a recently proposed defense that, without LLM-specific best\npractices in place, it is easy to overestimate the robustness of a new\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2310.19737v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models: A Comprehensive Survey",
        "authors": [
            "Zishan Guo",
            "Renren Jin",
            "Chuang Liu",
            "Yufei Huang",
            "Dan Shi",
            "Supryadi",
            "Linhao Yu",
            "Yan Liu",
            "Jiaxuan Li",
            "Bojian Xiong",
            "Deyi Xiong"
        ],
        "published": "2023-10-30T17:00:52Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na broad spectrum of tasks. They have attracted significant attention and been\ndeployed in numerous downstream applications. Nevertheless, akin to a\ndouble-edged sword, LLMs also present potential risks. They could suffer from\nprivate data leaks or yield inappropriate, harmful, or misleading content.\nAdditionally, the rapid progress of LLMs raises concerns about the potential\nemergence of superintelligent systems without adequate safeguards. To\neffectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of\nLLMs. We categorize the evaluation of LLMs into three major groups: knowledge\nand capability evaluation, alignment evaluation and safety evaluation. In\naddition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs' performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making\nevaluation serve as a cornerstone in guiding the responsible development of\nLLMs. We envision that this will channel their evolution into a direction that\nmaximizes societal benefit while minimizing potential risks. A curated list of\nrelated papers has been publicly available at\nhttps://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.",
        "pdf_link": "https://arxiv.org/pdf/2310.19736v3.pdf"
    },
    {
        "title": "Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection",
        "authors": [
            "Noah Ziems",
            "Gang Liu",
            "John Flanagan",
            "Meng Jiang"
        ],
        "published": "2023-10-30T15:40:34Z",
        "summary": "Network intrusion detection (NID) systems which leverage machine learning\nhave been shown to have strong performance in practice when used to detect\nmalicious network traffic. Decision trees in particular offer a strong balance\nbetween performance and simplicity, but require users of NID systems to have\nbackground knowledge in machine learning to interpret. In addition, they are\nunable to provide additional outside information as to why certain features may\nbe important for classification.\n  In this work, we explore the use of large language models (LLMs) to provide\nexplanations and additional background knowledge for decision tree NID systems.\nFurther, we introduce a new human evaluation framework for decision tree\nexplanations, which leverages automatically generated quiz questions that\nmeasure human evaluators' understanding of decision tree inference. Finally, we\nshow LLM generated decision tree explanations correlate highly with human\nratings of readability, quality, and use of background knowledge while\nsimultaneously providing better understanding of decision boundaries.",
        "pdf_link": "https://arxiv.org/pdf/2310.19658v1.pdf"
    },
    {
        "title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models",
        "authors": [
            "Zhenpeng Su",
            "Xing Wu",
            "Xue Bai",
            "Zijia Lin",
            "Hui Chen",
            "Guiguang Ding",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-10-30T13:33:21Z",
        "summary": "Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2310.19531v7.pdf"
    },
    {
        "title": "Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs",
        "authors": [
            "Huawen Feng",
            "Yan Fan",
            "Xiong Liu",
            "Ting-En Lin",
            "Zekun Yao",
            "Yuchuan Wu",
            "Fei Huang",
            "Yongbin Li",
            "Qianli Ma"
        ],
        "published": "2023-10-30T08:40:16Z",
        "summary": "Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nefficient training to cover the shortage of sensitivity for true and false in\nthe training process of LLMs. In this way, LLMs are less confused about\nembellishing and understanding; thus, they can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.19347v3.pdf"
    },
    {
        "title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models",
        "authors": [
            "Wai-Chung Kwan",
            "Xingshan Zeng",
            "Yufei Wang",
            "Yusen Sun",
            "Liangyou Li",
            "Lifeng Shang",
            "Qun Liu",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-30T03:11:30Z",
        "summary": "Managing long sequences has become an important and necessary feature for\nlarge language models (LLMs). However, it is still an open question of how to\ncomprehensively and systematically evaluate the long-sequence capability of\nLLMs. One of the reasons is that conventional and widely-used benchmarks mainly\nconsist of short sequences. In this paper, we propose M4LE, a Multi-ability,\nMulti-range, Multi-task, Multi-domain benchmark for Long-context Evaluation.\nM4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task\ntypes and 12 domains. To alleviate the scarcity of tasks with naturally long\nsequences and incorporate multiple-ability assessment, we propose an automatic\napproach (but with negligible human annotations) to convert short-sequence\ntasks into a unified long-sequence scenario where LLMs have to identify single\nor multiple relevant spans in long contexts based on explicit or semantic\nhints. Specifically, the scenario includes five different types of abilities:\n(1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span;\n(4) semantic multiple-span; and (5) global context understanding. The resulting\nsamples in M4LE are evenly distributed from 1k to 8k input length. We conducted\na systematic evaluation on 11 well-established LLMs, especially those optimized\nfor long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to\nunderstand long context, particularly when tasks require multiple-span\nattention. 2) Semantic retrieval task is more difficult for competent LLMs. 3)\nModels fine-tuned on longer text with position interpolation have comparable\nperformance to those using Neural Tangent Kernel (NTK) aware scaling methods\nwithout fine-tuning. We make our benchmark publicly available to encourage\nfuture research in this challenging area.",
        "pdf_link": "https://arxiv.org/pdf/2310.19240v1.pdf"
    },
    {
        "title": "From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude",
        "authors": [
            "Sayak Saha Roy",
            "Poojitha Thota",
            "Krishna Vamsi Naragam",
            "Shirin Nilizadeh"
        ],
        "published": "2023-10-29T22:52:40Z",
        "summary": "The advanced capabilities of Large Language Models (LLMs) have made them\ninvaluable across various applications, from conversational agents and content\ncreation to data analysis, research, and innovation. However, their\neffectiveness and accessibility also render them susceptible to abuse for\ngenerating malicious content, including phishing attacks. This study explores\nthe potential of using four popular commercially available LLMs, i.e., ChatGPT\n(GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing\nattacks using a series of malicious prompts. We discover that these LLMs can\ngenerate both phishing websites and emails that can convincingly imitate\nwell-known brands and also deploy a range of evasive tactics that are used to\nelude detection mechanisms employed by anti-phishing systems. These attacks can\nbe generated using unmodified or \"vanilla\" versions of these LLMs without\nrequiring any prior adversarial exploits such as jailbreaking. We evaluate the\nperformance of the LLMs towards generating these attacks and find that they can\nalso be utilized to create malicious prompts that, in turn, can be fed back to\nthe model to generate phishing scams - thus massively reducing the\nprompt-engineering effort required by attackers to scale these threats. As a\ncountermeasure, we build a BERT-based automated detection tool that can be used\nfor the early detection of malicious prompts to prevent LLMs from generating\nphishing content. Our model is transferable across all four commercial LLMs,\nattaining an average accuracy of 96% for phishing website prompts and 94% for\nphishing email prompts. We also disclose the vulnerabilities to the concerned\nLLMs, with Google acknowledging it as a severe issue. Our detection model is\navailable for use at Hugging Face, as well as a ChatGPT Actions plugin.",
        "pdf_link": "https://arxiv.org/pdf/2310.19181v2.pdf"
    },
    {
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion",
        "authors": [
            "Pengyue Jia",
            "Yiding Liu",
            "Xiangyu Zhao",
            "Xiaopeng Li",
            "Changying Hao",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2023-10-29T16:04:10Z",
        "summary": "Query expansion, pivotal in search engines, enhances the representation of\nuser information needs with additional terms. While existing methods expand\nqueries using retrieved or generated contextual documents, each approach has\nnotable limitations. Retrieval-based methods often fail to accurately capture\nsearch intent, particularly with brief or ambiguous queries. Generation-based\nmethods, utilizing large language models (LLMs), generally lack corpus-specific\nknowledge and entail high fine-tuning costs. To address these gaps, we propose\na novel zero-shot query expansion framework utilizing LLMs for mutual\nverification. Specifically, we first design a query-query-document generation\nmethod, leveraging LLMs' zero-shot reasoning ability to produce diverse\nsub-queries and corresponding documents. Then, a mutual verification process\nsynergizes generated and retrieved documents for optimal expansion. Our\nproposed method is fully zero-shot, and extensive experiments on three public\nbenchmark datasets are conducted to demonstrate its effectiveness over existing\nmethods. Our code is available online at\nhttps://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.",
        "pdf_link": "https://arxiv.org/pdf/2310.19056v3.pdf"
    },
    {
        "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding",
        "authors": [
            "Lixing Zhu",
            "Runcong Zhao",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2023-10-28T18:47:57Z",
        "summary": "Narrative understanding involves capturing the author's cognitive processes,\nproviding insights into their knowledge, intentions, beliefs, and desires.\nAlthough large language models (LLMs) excel in generating grammatically\ncoherent text, their ability to comprehend the author's thoughts remains\nuncertain. This limitation hinders the practical applications of narrative\nunderstanding. In this paper, we conduct a comprehensive survey of narrative\nunderstanding tasks, thoroughly examining their key features, definitions,\ntaxonomy, associated datasets, training objectives, evaluation metrics, and\nlimitations. Furthermore, we explore the potential of expanding the\ncapabilities of modularized LLMs to address novel narrative understanding\ntasks. By framing narrative understanding as the retrieval of the author's\nimaginative cues that outline the narrative structure, our study introduces a\nfresh perspective on enhancing narrative comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2310.18783v1.pdf"
    },
    {
        "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
        "authors": [
            "Sajad Mousavi",
            "Ricardo Luna Guti√©rrez",
            "Desik Rengarajan",
            "Vineet Gundecha",
            "Ashwin Ramesh Babu",
            "Avisek Naug",
            "Antonio Guillen",
            "Soumyendu Sarkar"
        ],
        "published": "2023-10-28T11:22:22Z",
        "summary": "We propose a self-correction mechanism for Large Language Models (LLMs) to\nmitigate issues such as toxicity and fact hallucination. This method involves\nrefining model outputs through an ensemble of critics and the model's own\nfeedback. Drawing inspiration from human behavior, we explore whether LLMs can\nemulate the self-correction process observed in humans who often engage in\nself-reflection and seek input from others to refine their understanding of\ncomplex topics. Our approach is model-agnostic and can be applied across\nvarious domains to enhance trustworthiness by addressing fairness, bias, and\nrobustness concerns. We consistently observe performance improvements in LLMs\nfor reducing toxicity and correcting factual errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.18679v2.pdf"
    },
    {
        "title": "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers",
        "authors": [
            "Wencong You",
            "Zayd Hammoudeh",
            "Daniel Lowd"
        ],
        "published": "2023-10-28T06:11:07Z",
        "summary": "Backdoor attacks manipulate model predictions by inserting innocuous triggers\ninto training and test data. We focus on more realistic and more challenging\nclean-label attacks where the adversarial training examples are correctly\nlabeled. Our attack, LLMBkd, leverages language models to automatically insert\ndiverse style-based triggers into texts. We also propose a poison selection\ntechnique to improve the effectiveness of both LLMBkd as well as existing\ntextual backdoor attacks. Lastly, we describe REACT, a baseline defense to\nmitigate backdoor attacks via antidote training examples. Our evaluations\ndemonstrate LLMBkd's effectiveness and efficiency, where we consistently\nachieve high attack success rates across a wide range of styles with little\neffort and no model training.",
        "pdf_link": "https://arxiv.org/pdf/2310.18603v1.pdf"
    },
    {
        "title": "LLMs-Healthcare : Current Applications and Challenges of Large Language Models in various Medical Specialties",
        "authors": [
            "Ummara Mumtaz",
            "Awais Ahmed",
            "Summaya Mumtaz"
        ],
        "published": "2023-10-28T01:01:30Z",
        "summary": "We aim to present a comprehensive overview of the latest advancements in\nutilizing Large Language Models (LLMs) within the healthcare sector,\nemphasizing their transformative impact across various medical domains. LLMs\nhave become pivotal in supporting healthcare, including physicians, healthcare\nproviders, and patients. Our review provides insight into the applications of\nLarge Language Models (LLMs) in healthcare, specifically focusing on diagnostic\nand treatment-related functionalities. We shed light on how LLMs are applied in\ncancer care, dermatology, dental care, neurodegenerative disorders, and mental\nhealth, highlighting their innovative contributions to medical diagnostics and\npatient care. Throughout our analysis, we explore the challenges and\nopportunities associated with integrating LLMs in healthcare, recognizing their\npotential across various medical specialties despite existing limitations.\nAdditionally, we offer an overview of handling diverse data types within the\nmedical field.",
        "pdf_link": "https://arxiv.org/pdf/2311.12882v3.pdf"
    },
    {
        "title": "On the Automatic Generation and Simplification of Children's Stories",
        "authors": [
            "Maria Valentini",
            "Jennifer Weber",
            "Jesus Salcido",
            "T√©a Wright",
            "Eliana Colunga",
            "Katharina Kann"
        ],
        "published": "2023-10-27T21:31:34Z",
        "summary": "With recent advances in large language models (LLMs), the concept of\nautomatically generating children's educational materials has become\nincreasingly realistic. Working toward the goal of age-appropriate simplicity\nin generated educational texts, we first examine the ability of several popular\nLLMs to generate stories with properly adjusted lexical and readability levels.\nWe find that, in spite of the growing capabilities of LLMs, they do not yet\npossess the ability to limit their vocabulary to levels appropriate for younger\nage groups. As a second experiment, we explore the ability of state-of-the-art\nlexical simplification models to generalize to the domain of children's stories\nand, thus, create an efficient pipeline for their automatic generation. In\norder to test these models, we develop a dataset of child-directed lexical\nsimplification instances, with examples taken from the LLM-generated stories in\nour first experiment. We find that, while the strongest-performing current\nlexical simplification models do not perform as well on material designed for\nchildren due to their reliance on large language models behind the scenes, some\nmodels that still achieve fairly strong results on general data can mimic or\neven improve their performance on children-directed data with proper\nfine-tuning, which we conduct using our newly created child-directed\nsimplification dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.18502v1.pdf"
    },
    {
        "title": "PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction",
        "authors": [
            "Mingchen Li",
            "M. Chen",
            "Huixue Zhou",
            "Halil Kilicoglu",
            "Rui Zhang"
        ],
        "published": "2023-10-27T20:15:23Z",
        "summary": "Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. While current unified information\nextraction models showcase state-of-the-art performance, they face challenges\nin understanding relationships between entities within intricate biomedical\nsentences. Furthermore, the absence of a high-quality biomedical triple\nextraction dataset impedes the progress in developing robust triple extraction\nsystems. To tackle these challenges, we propose a novel retrieval-based\nframework for biomedical triple extraction, namely PeTailor, which explicitly\nretrieves the relevant document from our pre-built diverse chunk database using\na novel tailored chunk scorer and integrates the retrieved information into the\ninput of a Large Language Model (LLM) to generate the corresponding triple\n(head entity, relation, tail entity) for the input sentence. Additionally, we\npresent GM-CIHT, an expert-annotated biomedical triple extraction dataset that\ncovers a wider range of relation types. Experimental results show that our\nproposed PeTailor method achieves state-of-the-art performance on GM-CIHT and\ntwo standard biomedical triple extraction datasets",
        "pdf_link": "https://arxiv.org/pdf/2310.18463v3.pdf"
    },
    {
        "title": "T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models",
        "authors": [
            "Rebecca M. M. Hicke",
            "David Mimno"
        ],
        "published": "2023-10-27T20:04:57Z",
        "summary": "Large language models have shown breakthrough potential in many NLP domains.\nHere we consider their use for stylometry, specifically authorship\nidentification in Early Modern English drama. We find both promising and\nconcerning results; LLMs are able to accurately predict the author of\nsurprisingly short passages but are also prone to confidently misattribute\ntexts to specific authors. A fine-tuned t5-large model outperforms all tested\nbaselines, including logistic regression, SVM with a linear kernel, and cosine\ndelta, at attributing small passages. However, we see indications that the\npresence of certain authors in the model's pre-training data affects predictive\nresults in ways that are difficult to assess.",
        "pdf_link": "https://arxiv.org/pdf/2310.18454v1.pdf"
    },
    {
        "title": "Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models",
        "authors": [
            "Eren Unlu",
            "Unver Ciftci"
        ],
        "published": "2023-10-27T17:04:10Z",
        "summary": "Large Language Models (LLMs) are evolving to integrate multiple modalities,\nsuch as text, image, and audio into a unified linguistic space. We envision a\nfuture direction based on this framework where conceptual entities defined in\nsequences of text can also be imagined as modalities. Such a formulation has\nthe potential to overcome the cognitive and computational limitations of\ncurrent models. Several illustrative examples of such potential implicit\nmodalities are given. Along with vast promises of the hypothesized structure,\nexpected challenges are discussed as well.",
        "pdf_link": "https://arxiv.org/pdf/2310.18390v1.pdf"
    },
    {
        "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models",
        "authors": [
            "Benjamin Feuer",
            "Yurong Liu",
            "Chinmay Hegde",
            "Juliana Freire"
        ],
        "published": "2023-10-27T15:31:22Z",
        "summary": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.",
        "pdf_link": "https://arxiv.org/pdf/2310.18208v2.pdf"
    },
    {
        "title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues",
        "authors": [
            "David Q. Sun",
            "Artem Abzaliev",
            "Hadas Kotek",
            "Zidi Xiu",
            "Christopher Klein",
            "Jason D. Williams"
        ],
        "published": "2023-10-27T13:23:02Z",
        "summary": "Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.",
        "pdf_link": "https://arxiv.org/pdf/2310.18130v2.pdf"
    },
    {
        "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
        "authors": [
            "Oscar Sainz",
            "Jon Ander Campos",
            "Iker Garc√≠a-Ferrero",
            "Julen Etxaniz",
            "Oier Lopez de Lacalle",
            "Eneko Agirre"
        ],
        "published": "2023-10-27T09:48:29Z",
        "summary": "In this position paper, we argue that the classical evaluation on Natural\nLanguage Processing (NLP) tasks using annotated benchmarks is in trouble. The\nworst kind of data contamination happens when a Large Language Model (LLM) is\ntrained on the test split of a benchmark, and then evaluated in the same\nbenchmark. The extent of the problem is unknown, as it is not straightforward\nto measure. Contamination causes an overestimation of the performance of a\ncontaminated model in a target benchmark and associated task with respect to\ntheir non-contaminated counterparts. The consequences can be very harmful, with\nwrong scientific conclusions being published while other correct ones are\ndiscarded. This position paper defines different levels of data contamination\nand argues for a community effort, including the development of automatic and\nsemi-automatic measures to detect when data from a benchmark was exposed to a\nmodel, and suggestions for flagging papers with conclusions that are\ncompromised by data contamination.",
        "pdf_link": "https://arxiv.org/pdf/2310.18018v1.pdf"
    },
    {
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-10-27T06:48:48Z",
        "summary": "Sentiment analysis is a well-established natural language processing task,\nwith sentiment polarity classification being one of its most popular and\nrepresentative tasks. However, despite the success of pre-trained language\nmodels in this area, they often fall short of capturing the broader\ncomplexities of sentiment analysis. To address this issue, we propose a new\ntask called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims\nto evaluate sentiment understanding through two subtasks: Review Comprehension\n(RC) and Justification Generation (JG). RC seeks to validate statements that\nfocus on subjective information based on a review text, while JG requires\nmodels to provide explanations for their sentiment predictions. To enable\ncomprehensive evaluation, we annotate a new dataset comprising 15,028\nstatements from 3,638 reviews. Experimental results indicate that SOUL is a\nchallenging task for both small and large language models, with a performance\ngap of up to 27% when compared to human performance. Furthermore, evaluations\nconducted with both human experts and GPT-4 highlight the limitations of the\nsmall language model in generating reasoning-based justifications. These\nfindings underscore the challenging nature of the SOUL task for existing\nmodels, emphasizing the need for further advancements in sentiment analysis to\naddress its complexities. The new dataset and code are available at\nhttps://github.com/DAMO-NLP-SG/SOUL.",
        "pdf_link": "https://arxiv.org/pdf/2310.17924v1.pdf"
    },
    {
        "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method",
        "authors": [
            "Yukun Zhao",
            "Lingyong Yan",
            "Weiwei Sun",
            "Guoliang Xing",
            "Chong Meng",
            "Shuaiqiang Wang",
            "Zhicong Cheng",
            "Zhaochun Ren",
            "Dawei Yin"
        ],
        "published": "2023-10-27T06:22:14Z",
        "summary": "Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2310.17918v2.pdf"
    },
    {
        "title": "Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey",
        "authors": [
            "Weixu Zhang",
            "Yifei Wang",
            "Yuanfeng Song",
            "Victor Junqiu Wei",
            "Yuxing Tian",
            "Yiyan Qi",
            "Jonathan H. Chan",
            "Raymond Chi-Wing Wong",
            "Haiqin Yang"
        ],
        "published": "2023-10-27T05:01:20Z",
        "summary": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.",
        "pdf_link": "https://arxiv.org/pdf/2310.17894v1.pdf"
    },
    {
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
        "authors": [
            "Niloofar Mireshghallah",
            "Hyunwoo Kim",
            "Xuhui Zhou",
            "Yulia Tsvetkov",
            "Maarten Sap",
            "Reza Shokri",
            "Yejin Choi"
        ],
        "published": "2023-10-27T04:15:30Z",
        "summary": "The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.",
        "pdf_link": "https://arxiv.org/pdf/2310.17884v1.pdf"
    },
    {
        "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of Abstract Meaning Representation",
        "authors": [
            "Allyson Ettinger",
            "Jena D. Hwang",
            "Valentina Pyatkin",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "published": "2023-10-26T21:47:59Z",
        "summary": "Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.",
        "pdf_link": "https://arxiv.org/pdf/2310.17793v2.pdf"
    },
    {
        "title": "Evaluation of large language models using an Indian language LGBTI+ lexicon",
        "authors": [
            "Aditya Joshi",
            "Shruta Rawat",
            "Alpana Dange"
        ],
        "published": "2023-10-26T21:32:24Z",
        "summary": "Large language models (LLMs) are typically evaluated on the basis of\ntask-based benchmarks such as MMLU. Such benchmarks do not examine responsible\nbehaviour of LLMs in specific contexts. This is particularly true in the LGBTI+\ncontext where social stereotypes may result in variation in LGBTI+ terminology.\nTherefore, domain-specific lexicons or dictionaries may be useful as a\nrepresentative list of words against which the LLM's behaviour needs to be\nevaluated. This paper presents a methodology for evaluation of LLMs using an\nLGBTI+ lexicon in Indian languages. The methodology consists of four steps:\nformulating NLP tasks relevant to the expected behaviour, creating prompts that\ntest LLMs, using the LLMs to obtain the output and, finally, manually\nevaluating the results. Our qualitative analysis shows that the three LLMs we\nexperiment on are unable to detect underlying hateful content. Similarly, we\nobserve limitations in using machine translation as means to evaluate natural\nlanguage understanding in languages other than English. The methodology\npresented in this paper can be useful for LGBTI+ lexicons in other languages as\nwell as other domain-specific lexicons. The work done in this paper opens\navenues for responsible behaviour of LLMs, as demonstrated in the context of\nprevalent social perception of the LGBTI+ community.",
        "pdf_link": "https://arxiv.org/pdf/2310.17787v1.pdf"
    },
    {
        "title": "A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications",
        "authors": [
            "Ahmed Magooda",
            "Alec Helyar",
            "Kyle Jackson",
            "David Sullivan",
            "Chad Atalla",
            "Emily Sheng",
            "Dan Vann",
            "Richard Edgar",
            "Hamid Palangi",
            "Roman Lutz",
            "Hongliang Kong",
            "Vincent Yun",
            "Eslam Kamal",
            "Federico Zarfati",
            "Hanna Wallach",
            "Sarah Bird",
            "Mei Chen"
        ],
        "published": "2023-10-26T19:45:06Z",
        "summary": "We present a framework for the automated measurement of responsible AI (RAI)\nmetrics for large language models (LLMs) and associated products and services.\nOur framework for automatically measuring harms from LLMs builds on existing\ntechnical and sociotechnical expertise and leverages the capabilities of\nstate-of-the-art LLMs, such as GPT-4. We use this framework to run through\nseveral case studies investigating how different LLMs may violate a range of\nRAI-related principles. The framework may be employed alongside domain-specific\nsociotechnical expertise to create measurements for new harm areas in the\nfuture. By implementing this framework, we aim to enable more advanced harm\nmeasurement efforts and further the responsible use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.17750v1.pdf"
    },
    {
        "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems",
        "authors": [
            "Lidiya Murakhovs'ka",
            "Philippe Laban",
            "Tian Xie",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-10-26T19:44:06Z",
        "summary": "Making big purchases requires consumers to research or consult a salesperson\nto gain domain expertise. However, existing conversational recommender systems\n(CRS) often overlook users' lack of background knowledge, focusing solely on\ngathering preferences. In this work, we define a new problem space for\nconversational agents that aim to provide both product recommendations and\neducational value through mixed-type mixed-initiative dialog. We introduce\nSalesOps, a framework that facilitates the simulation and evaluation of such\nsystems by leveraging recent advancements in large language models (LLMs). We\nbuild SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate\neither side of the framework. A comprehensive human study compares SalesBot\nagainst professional salespeople, revealing that although SalesBot approaches\nprofessional performance in terms of fluency and informativeness, it lags\nbehind in recommendation quality. We emphasize the distinct limitations both\nface in providing truthful information, highlighting the challenges of ensuring\nfaithfulness in the CRS context. We release our code and make all data\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2310.17749v1.pdf"
    },
    {
        "title": "Outlier Dimensions Encode Task-Specific Knowledge",
        "authors": [
            "William Rudman",
            "Catherine Chen",
            "Carsten Eickhoff"
        ],
        "published": "2023-10-26T18:22:13Z",
        "summary": "Representations from large language models (LLMs) are known to be dominated\nby a small subset of dimensions with exceedingly high variance. Previous works\nhave argued that although ablating these outlier dimensions in LLM\nrepresentations hurts downstream performance, outlier dimensions are\ndetrimental to the representational quality of embeddings. In this study, we\ninvestigate how fine-tuning impacts outlier dimensions and show that 1) outlier\ndimensions that occur in pre-training persist in fine-tuned models and 2) a\nsingle outlier dimension can complete downstream tasks with a minimal error\nrate. Our results suggest that outlier dimensions can encode crucial\ntask-specific knowledge and that the value of a representation in a single\noutlier dimension drives downstream model decisions.",
        "pdf_link": "https://arxiv.org/pdf/2310.17715v2.pdf"
    },
    {
        "title": "Proving Test Set Contamination in Black Box Language Models",
        "authors": [
            "Yonatan Oren",
            "Nicole Meister",
            "Niladri Chatterji",
            "Faisal Ladhak",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-10-26T17:43:13Z",
        "summary": "Large language models are trained on vast amounts of internet data, prompting\nconcerns and speculation that they have memorized public benchmarks. Going from\nspeculation to proof of contamination is challenging, as the pretraining data\nused by proprietary models are often not publicly accessible. We show that it\nis possible to provide provable guarantees of test set contamination in\nlanguage models without access to pretraining data or model weights. Our\napproach leverages the fact that when there is no data contamination, all\norderings of an exchangeable benchmark should be equally likely. In contrast,\nthe tendency for language models to memorize example order means that a\ncontaminated language model will find certain canonical orderings to be much\nmore likely than others. Our test flags potential contamination whenever the\nlikelihood of a canonically ordered benchmark dataset is significantly higher\nthan the likelihood after shuffling the examples. We demonstrate that our\nprocedure is sensitive enough to reliably prove test set contamination in\nchallenging situations, including models as small as 1.4 billion parameters, on\nsmall test sets of only 1000 examples, and datasets that appear only a few\ntimes in the pretraining corpus. Using our test, we audit five popular publicly\naccessible language models for test set contamination and find little evidence\nfor pervasive contamination.",
        "pdf_link": "https://arxiv.org/pdf/2310.17623v2.pdf"
    },
    {
        "title": "An Open Source Data Contamination Report for Large Language Models",
        "authors": [
            "Yucheng Li",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2023-10-26T17:11:42Z",
        "summary": "Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to \"cheat\"\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.",
        "pdf_link": "https://arxiv.org/pdf/2310.17589v3.pdf"
    },
    {
        "title": "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
        "authors": [
            "Qusai Khraisha",
            "Sophie Put",
            "Johanna Kappenberg",
            "Azza Warraitch",
            "Kristin Hadfield"
        ],
        "published": "2023-10-26T16:18:30Z",
        "summary": "Systematic reviews are vital for guiding practice, research, and policy, yet\nthey are often slow and labour-intensive. Large language models (LLMs) could\noffer a way to speed up and automate systematic reviews, but their performance\nin such tasks has not been comprehensively evaluated against humans, and no\nstudy has tested GPT-4, the biggest LLM so far. This pre-registered study\nevaluates GPT-4's capability in title/abstract screening, full-text review, and\ndata extraction across various literature types and languages using a\n'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human\nperformance in most tasks, results were skewed by chance agreement and dataset\nimbalance. After adjusting for these, there was a moderate level of performance\nfor data extraction, and - barring studies that used highly reliable prompts -\nscreening performance levelled at none to moderate for different stages and\nlanguages. When screening full-text literature using highly reliable prompts,\nGPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key\nstudies using highly reliable prompts improved its performance even more. Our\nfindings indicate that, currently, substantial caution should be used if LLMs\nare being used to conduct systematic reviews, but suggest that, for certain\nsystematic review tasks delivered under reliable prompts, LLMs can rival human\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2310.17526v2.pdf"
    },
    {
        "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering",
        "authors": [
            "Sukmin Cho",
            "Jeongyeon Seo",
            "Soyeong Jeong",
            "Jong C. Park"
        ],
        "published": "2023-10-26T15:45:12Z",
        "summary": "Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.",
        "pdf_link": "https://arxiv.org/pdf/2310.17490v3.pdf"
    },
    {
        "title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks",
        "authors": [
            "Shen Yuan",
            "Hongteng Xu"
        ],
        "published": "2023-10-26T14:43:07Z",
        "summary": "As one of the most popular neural network modules, Transformer plays a\ncentral role in many fundamental deep learning models, e.g., the ViT in\ncomputer vision and the BERT and GPT in natural language processing. The\neffectiveness of the Transformer is often attributed to its multi-head\nattention (MHA) mechanism. In this study, we discuss the limitations of MHA,\nincluding the high computational complexity due to its ``query-key-value''\narchitecture and the numerical issue caused by its softmax operation.\nConsidering the above problems and the recent development tendency of the\nattention layer, we propose an effective and efficient surrogate of the\nTransformer, called Sliceformer. Our Sliceformer replaces the classic MHA\nmechanism with an extremely simple ``slicing-sorting'' operation, i.e.,\nprojecting inputs linearly to a latent space and sorting them along different\nfeature dimensions (or equivalently, called channels). For each feature\ndimension, the sorting operation implicitly generates an implicit attention map\nwith sparse, full-rank, and doubly-stochastic structures. We consider different\nimplementations of the slicing-sorting operation and analyze their impacts on\nthe Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark,\nimage classification, text classification, and molecular property prediction,\ndemonstrating its advantage in computational complexity and universal\neffectiveness in discriminative tasks. Our Sliceformer achieves comparable or\nbetter performance with lower memory cost and faster speed than the Transformer\nand its variants. Moreover, the experimental results reveal that applying our\nSliceformer can empirically suppress the risk of mode collapse when\nrepresenting data. The code is available at\n\\url{https://github.com/SDS-Lab/sliceformer}.",
        "pdf_link": "https://arxiv.org/pdf/2310.17683v1.pdf"
    },
    {
        "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
        "authors": [
            "Zi Lin",
            "Zihan Wang",
            "Yongqi Tong",
            "Yangkun Wang",
            "Yuxin Guo",
            "Yujia Wang",
            "Jingbo Shang"
        ],
        "published": "2023-10-26T13:35:41Z",
        "summary": "Despite remarkable advances that large language models have achieved in\nchatbots, maintaining a non-toxic user-AI interactive environment has become\nincreasingly critical nowadays. However, previous efforts in toxicity detection\nhave been mostly based on benchmarks derived from social media content, leaving\nthe unique challenges inherent to real-world user-AI interactions\ninsufficiently explored. In this work, we introduce ToxicChat, a novel\nbenchmark based on real user queries from an open-source chatbot. This\nbenchmark contains the rich, nuanced phenomena that can be tricky for current\ntoxicity detection models to identify, revealing a significant domain\ndifference compared to social media content. Our systematic evaluation of\nmodels trained on existing toxicity datasets has shown their shortcomings when\napplied to this unique domain of ToxicChat. Our work illuminates the\npotentially overlooked challenges of toxicity detection in real-world user-AI\nconversations. In the future, ToxicChat can be a valuable resource to drive\nfurther advancements toward building a safe and healthy environment for user-AI\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2310.17389v1.pdf"
    },
    {
        "title": "Cultural Adaptation of Recipes",
        "authors": [
            "Yong Cao",
            "Yova Kementchedjhieva",
            "Ruixiang Cui",
            "Antonia Karamolegkou",
            "Li Zhou",
            "Megan Dare",
            "Lucia Donatelli",
            "Daniel Hershcovich"
        ],
        "published": "2023-10-26T12:39:20Z",
        "summary": "Building upon the considerable advances in Large Language Models (LLMs), we\nare now equipped to address more sophisticated tasks demanding a nuanced\nunderstanding of cross-cultural contexts. A key example is recipe adaptation,\nwhich goes beyond simple translation to include a grasp of ingredients,\nculinary techniques, and dietary preferences specific to a given culture. We\nintroduce a new task involving the translation and cultural adaptation of\nrecipes between Chinese and English-speaking cuisines. To support this\ninvestigation, we present CulturalRecipes, a unique dataset comprised of\nautomatically paired recipes written in Mandarin Chinese and English. This\ndataset is further enriched with a human-written and curated test set. In this\nintricate task of cross-cultural recipe adaptation, we evaluate the performance\nof various methods, including GPT-4 and other LLMs, traditional machine\ntranslation, and information retrieval techniques. Our comprehensive analysis\nincludes both automatic and human evaluation metrics. While GPT-4 exhibits\nimpressive abilities in adapting Chinese recipes into English, it still lags\nbehind human expertise when translating English recipes into Chinese. This\nunderscores the multifaceted nature of cultural adaptations. We anticipate that\nthese insights will significantly contribute to future research on\nculturally-aware language models and their practical application in culturally\ndiverse contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.17353v1.pdf"
    },
    {
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "authors": [
            "Justin T. Chiu",
            "Wenting Zhao",
            "Derek Chen",
            "Saujas Vaduguru",
            "Alexander M. Rush",
            "Daniel Fried"
        ],
        "published": "2023-10-26T04:22:23Z",
        "summary": "Large language models (LLMs) excel at processing and generating both text and\ncode. However, LLMs have had limited applicability in grounded task-oriented\ndialogue as they are difficult to steer toward task objectives and fail to\nhandle novel grounding. We present a modular and interpretable grounded\ndialogue system that addresses these shortcomings by composing LLMs with a\nsymbolic planner and grounded code execution. Our system consists of a reader\nand planner: the reader leverages an LLM to convert partner utterances into\nexecutable code, calling functions that perform grounding. The translated\ncode's output is stored to track dialogue state, while a symbolic planner\ndetermines the next appropriate response. We evaluate our system's performance\non the demanding OneCommon dialogue task, involving collaborative reference\nresolution on abstract images of scattered dots. Our system substantially\noutperforms the previous state-of-the-art, including improving task success in\nhuman evaluations from 56% to 69% in the most challenging setting.",
        "pdf_link": "https://arxiv.org/pdf/2310.17140v1.pdf"
    },
    {
        "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs",
        "authors": [
            "Yuxin Zuo",
            "Bei Li",
            "Chuanhao Lv",
            "Tong Zheng",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "published": "2023-10-26T04:13:49Z",
        "summary": "This paper presents an in-depth study of multimodal machine translation\n(MMT), examining the prevailing understanding that MMT systems exhibit\ndecreased sensitivity to visual information when text inputs are complete.\nInstead, we attribute this phenomenon to insufficient cross-modal interaction,\nrather than image information redundancy. A novel approach is proposed to\ngenerate parallel Visual Question-Answering (VQA) style pairs from the source\ntext, fostering more robust cross-modal interaction. Using Large Language\nModels (LLMs), we explicitly model the probing signal in MMT to convert it into\nVQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask\nlearning framework is introduced to incorporate explicit probing signals from\nthe dataset into the MMT training process. Experimental results on two\nwidely-used benchmarks demonstrate the effectiveness of this novel approach.\nOur code and data would be available at:\n\\url{https://github.com/libeineu/MMT-VQA}.",
        "pdf_link": "https://arxiv.org/pdf/2310.17133v1.pdf"
    },
    {
        "title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge",
        "authors": [
            "Farima Fatahi Bayat",
            "Kun Qian",
            "Benjamin Han",
            "Yisi Sang",
            "Anton Belyi",
            "Samira Khorshidi",
            "Fei Wu",
            "Ihab F. Ilyas",
            "Yunyao Li"
        ],
        "published": "2023-10-26T03:28:30Z",
        "summary": "Detecting factual errors in textual information, whether generated by large\nlanguage models (LLM) or curated by humans, is crucial for making informed\ndecisions. LLMs' inability to attribute their claims to external knowledge and\ntheir tendency to hallucinate makes it difficult to rely on their responses.\nHumans, too, are prone to factual errors in their writing. Since manual\ndetection and correction of factual errors is labor-intensive, developing an\nautomatic approach can greatly reduce human effort. We present FLEEK, a\nprototype tool that automatically extracts factual claims from text, gathers\nevidence from external knowledge sources, evaluates the factuality of each\nclaim, and suggests revisions for identified errors using the collected\nevidence. Initial empirical evaluation on fact error detection (77-85\\% F1)\nshows the potential of FLEEK. A video demo of FLEEK can be found at\nhttps://youtu.be/NapJFUlkPdQ.",
        "pdf_link": "https://arxiv.org/pdf/2310.17119v1.pdf"
    },
    {
        "title": "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories",
        "authors": [
            "Hassen Saidi",
            "Susmit Jha",
            "Tuhin Sahai"
        ],
        "published": "2023-10-25T23:54:04Z",
        "summary": "As artificial intelligence (AI) gains greater adoption in a wide variety of\napplications, it has immense potential to contribute to mathematical discovery,\nby guiding conjecture generation, constructing counterexamples, assisting in\nformalizing mathematics, and discovering connections between different\nmathematical areas, to name a few.\n  While prior work has leveraged computers for exhaustive mathematical proof\nsearch, recent efforts based on large language models (LLMs) aspire to position\ncomputing platforms as co-contributors in the mathematical research process.\nDespite their current limitations in logic and mathematical tasks, there is\ngrowing interest in melding theorem proving systems with foundation models.\nThis work investigates the applicability of LLMs in formalizing advanced\nmathematical concepts and proposes a framework that can critically review and\ncheck mathematical reasoning in research papers. Given the noted reasoning\nshortcomings of LLMs, our approach synergizes the capabilities of proof\nassistants, specifically PVS, with LLMs, enabling a bridge between textual\ndescriptions in academic papers and formal specifications in PVS. By harnessing\nthe PVS environment, coupled with data ingestion and conversion mechanisms, we\nenvision an automated process, called \\emph{math-PVS}, to extract and formalize\nmathematical theorems from research papers, offering an innovative tool for\nacademic review and discovery.",
        "pdf_link": "https://arxiv.org/pdf/2310.17064v1.pdf"
    },
    {
        "title": "BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation",
        "authors": [
            "Yufei Tian",
            "Felix Zhang",
            "Nanyun Peng"
        ],
        "published": "2023-10-25T23:32:12Z",
        "summary": "Large language models (LLMs) such as GPT-3 have demonstrated a strong\ncapability to generate coherent and contextually relevant text. However, amidst\ntheir successes, a crucial issue persists: their generated outputs still lack\ncommonsense at times. Moreover, fine-tuning the entire LLM towards more\ncommonsensical outputs is computationally expensive if not infeasible. In this\npaper, we present a computation-efficient framework that steers a frozen\nPre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,\nproducing a plausible output that incorporates a list of concepts in a\nmeaningful way). Specifically, we first construct a reference-free evaluator\nthat assigns a sentence with a commonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base from four different relational aspects.\nWe then use the scorer as the oracle for commonsense knowledge, and extend the\ncontrollable generation method called NADO to train an auxiliary head that\nguides a fixed PTLM to better satisfy the oracle. We test our framework on a\nseries of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two\nconstrained concept-to-sentence benchmarks. Human evaluation results\ndemonstrate that our method consistently leads to the most commonsensical\noutputs.",
        "pdf_link": "https://arxiv.org/pdf/2310.17054v1.pdf"
    },
    {
        "title": "Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning",
        "authors": [
            "Ananth Balashankar",
            "Xiao Ma",
            "Aradhana Sinha",
            "Ahmad Beirami",
            "Yao Qin",
            "Jilin Chen",
            "Alex Beutel"
        ],
        "published": "2023-10-25T19:57:07Z",
        "summary": "As large language models (LLMs) are widely adopted, new safety issues and\npolicies emerge, to which existing safety classifiers do not generalize well.\nIf we have only observed a few examples of violations of a new safety rule, how\ncan we build a classifier to detect violations? In this paper, we study the\nnovel setting of domain-generalized few-shot learning for LLM-based text safety\nclassifiers. Unlike prior few-shot work, these new safety issues can be hard to\nuncover and we do not get to choose the few examples. We demonstrate that\nexisting few-shot techniques do not perform well in this setting, and rather we\npropose to do parameter-efficient fine-tuning (PEFT) combined with augmenting\ntraining data based on similar examples in prior existing rules. We empirically\nshow that our approach of similarity-based data-augmentation + prompt-tuning\n(DAPT) consistently outperforms baselines that either do not rely on data\naugmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral\njudgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule\nis loosely correlated with existing ones.",
        "pdf_link": "https://arxiv.org/pdf/2310.16959v1.pdf"
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "authors": [
            "Shih-yang Liu",
            "Zechun Liu",
            "Xijie Huang",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "published": "2023-10-25T17:59:32Z",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
        "pdf_link": "https://arxiv.org/pdf/2310.16836v1.pdf"
    },
    {
        "title": "Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization",
        "authors": [
            "Yongxin Zhou",
            "Fabien Ringeval",
            "Fran√ßois Portet"
        ],
        "published": "2023-10-25T17:39:07Z",
        "summary": "This study explores the capabilities of prompt-driven Large Language Models\n(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue\nsummarization. Experiments employed DialogSum (English social conversations)\nand DECODA (French call center interactions), testing various prompts:\nincluding prompts from existing literature and those from human summarization\nguidelines, as well as a two-step prompt approach. Our findings indicate that\nGPT models often produce lengthy summaries and deviate from human summarization\nguidelines. However, using human guidelines as an intermediate step shows\npromise, outperforming direct word-length constraint prompts in some cases. The\nresults reveal that GPT models exhibit unique stylistic tendencies in their\nsummaries. While BERTScores did not dramatically decrease for GPT outputs\nsuggesting semantic similarity to human references and specialised pre-trained\nmodels, ROUGE scores reveal grammatical and lexical disparities between\nGPT-generated and human-written summaries. These findings shed light on the\ncapabilities and limitations of GPT models in following human instructions for\ndialogue summarization.",
        "pdf_link": "https://arxiv.org/pdf/2310.16810v1.pdf"
    },
    {
        "title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation",
        "authors": [
            "Yongxin Shi",
            "Dezhi Peng",
            "Wenhui Liao",
            "Zening Lin",
            "Xinhong Chen",
            "Chongyu Liu",
            "Yuyi Zhang",
            "Lianwen Jin"
        ],
        "published": "2023-10-25T17:38:55Z",
        "summary": "This paper presents a comprehensive evaluation of the Optical Character\nRecognition (OCR) capabilities of the recently released GPT-4V(ision), a Large\nMultimodal Model (LMM). We assess the model's performance across a range of OCR\ntasks, including scene text recognition, handwritten text recognition,\nhandwritten mathematical expression recognition, table structure recognition,\nand information extraction from visually-rich document. The evaluation reveals\nthat GPT-4V performs well in recognizing and understanding Latin contents, but\nstruggles with multilingual scenarios and complex tasks. Specifically, it\nshowed limitations when dealing with non-Latin languages and complex tasks such\nas handwriting mathematical expression recognition, table structure\nrecognition, and end-to-end semantic entity recognition and pair extraction\nfrom document image. Based on these observations, we affirm the necessity and\ncontinued research value of specialized OCR models. In general, despite its\nversatility in handling diverse OCR tasks, GPT-4V does not outperform existing\nstate-of-the-art OCR models. How to fully utilize pre-trained general-purpose\nLMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study\noffers a critical reference for future research in OCR with LMMs. Evaluation\npipeline and results are available at\nhttps://github.com/SCUT-DLVCLab/GPT-4V_OCR.",
        "pdf_link": "https://arxiv.org/pdf/2310.16809v2.pdf"
    },
    {
        "title": "Detecting Pretraining Data from Large Language Models",
        "authors": [
            "Weijia Shi",
            "Anirudh Ajith",
            "Mengzhou Xia",
            "Yangsibo Huang",
            "Daogao Liu",
            "Terra Blevins",
            "Danqi Chen",
            "Luke Zettlemoyer"
        ],
        "published": "2023-10-25T17:21:23Z",
        "summary": "Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted book\ndetection, contaminated downstream example detection and privacy auditing of\nmachine unlearning, and find it a consistently effective solution.",
        "pdf_link": "https://arxiv.org/pdf/2310.16789v3.pdf"
    },
    {
        "title": "SuperHF: Supervised Iterative Learning from Human Feedback",
        "authors": [
            "Gabriel Mukobi",
            "Peter Chatain",
            "Su Fong",
            "Robert Windesheim",
            "Gitta Kutyniok",
            "Kush Bhatia",
            "Silas Alberti"
        ],
        "published": "2023-10-25T16:52:00Z",
        "summary": "While large language models demonstrate remarkable capabilities, they often\npresent challenges in terms of safety, alignment with human values, and\nstability during training. Here, we focus on two prevalent methods used to\nalign these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning\nfrom Human Feedback (RLHF). SFT is simple and robust, powering a host of\nopen-source models, while RLHF is a more sophisticated method used in top-tier\nmodels like ChatGPT but also suffers from instability and susceptibility to\nreward hacking. We propose a novel approach, Supervised Iterative Learning from\nHuman Feedback (SuperHF), which seeks to leverage the strengths of both\nmethods. Our hypothesis is two-fold: that the reward model used in RLHF is\ncritical for efficient data use and model generalization and that the use of\nProximal Policy Optimization (PPO) in RLHF may not be necessary and could\ncontribute to instability issues. SuperHF replaces PPO with a simple supervised\nloss and a Kullback-Leibler (KL) divergence prior. It creates its own training\ndata by repeatedly sampling a batch of model outputs and filtering them through\nthe reward model in an online learning regime. We then break down the reward\noptimization problem into three components: robustly optimizing the training\nrewards themselves, preventing reward hacking-exploitation of the reward model\nthat degrades model performance-as measured by a novel METEOR similarity\nmetric, and maintaining good performance on downstream evaluations. Our\nexperimental results show SuperHF exceeds PPO-based RLHF on the training\nobjective, easily and favorably trades off high reward with low reward hacking,\nimproves downstream calibration, and performs the same on our GPT-4 based\nqualitative evaluation scheme all the while being significantly simpler to\nimplement, highlighting SuperHF's potential as a competitive language model\nalignment technique.",
        "pdf_link": "https://arxiv.org/pdf/2310.16763v1.pdf"
    },
    {
        "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
        "authors": [
            "Yinghui He",
            "Yufan Wu",
            "Yilin Jia",
            "Rada Mihalcea",
            "Yulong Chen",
            "Naihao Deng"
        ],
        "published": "2023-10-25T16:41:15Z",
        "summary": "Theory of Mind (ToM) is the ability to reason about one's own and others'\nmental states. ToM plays a critical role in the development of intelligence,\nlanguage understanding, and cognitive processes. While previous work has\nprimarily focused on first and second-order ToM, we explore higher-order ToM,\nwhich involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\nHigher Order Theory of Mind benchmark. Our experimental evaluation using\nvarious Large Language Models (LLMs) indicates a decline in performance on\nhigher-order ToM tasks, demonstrating the limitations of current LLMs. We\nconduct a thorough analysis of different failure cases of LLMs, and share our\nthoughts on the implications of our findings on the future of NLP.",
        "pdf_link": "https://arxiv.org/pdf/2310.16755v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Code Explanation",
        "authors": [
            "Paheli Bhattacharya",
            "Manojit Chakraborty",
            "Kartheek N S N Palepu",
            "Vikas Pandey",
            "Ishan Dindorkar",
            "Rakesh Rajpurohit",
            "Rishabh Gupta"
        ],
        "published": "2023-10-25T14:38:40Z",
        "summary": "Automating code documentation through explanatory text can prove highly\nbeneficial in code understanding. Large Language Models (LLMs) have made\nremarkable strides in Natural Language Processing, especially within software\nengineering tasks such as code generation and code summarization. This study\nspecifically delves into the task of generating natural-language summaries for\ncode snippets, using various LLMs. The findings indicate that Code LLMs\noutperform their generic counterparts, and zero-shot methods yield superior\nresults when dealing with datasets with dissimilar distributions between\ntraining and testing sets.",
        "pdf_link": "https://arxiv.org/pdf/2310.16673v1.pdf"
    },
    {
        "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
        "authors": [
            "Tianlong Li",
            "Shihan Dou",
            "Changze Lv",
            "Wenhao Liu",
            "Jianhan Xu",
            "Muling Wu",
            "Zixuan Ling",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2023-10-25T12:16:33Z",
        "summary": "Personality plays a pivotal role in shaping human expression patterns, thus\nregulating the personality of large language models (LLMs) holds significant\npotential in enhancing the user experience of LLMs. Previous methods either\nrelied on fine-tuning LLMs on specific corpora or necessitated manually crafted\nprompts to elicit specific personalities from LLMs. However, the former\napproach is inefficient and costly, while the latter cannot precisely\nmanipulate personality traits at a fine-grained level. To address the above\nchallenges, we have employed a novel Unsupervisedly-Built Personalized Lexicons\n(UBPL) in a pluggable manner during the decoding phase of LLMs to manipulate\ntheir personality traits. UBPL is a lexicon built through an unsupervised\napproach from a situational judgment test dataset (SJTs4LLM). Users can utilize\nUBPL to adjust the probability vectors of predicted words in the decoding phase\nof LLMs, thus influencing the personality expression of LLMs. Extensive\nexperimentation demonstrates the remarkable effectiveness and pluggability of\nour method for fine-grained manipulation of LLM's personality.",
        "pdf_link": "https://arxiv.org/pdf/2310.16582v2.pdf"
    },
    {
        "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
        "authors": [
            "Qingyuan Tian",
            "Hanlun Zhu",
            "Lei Wang",
            "Yang Li",
            "Yunshi Lan"
        ],
        "published": "2023-10-25T10:34:02Z",
        "summary": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models\n(LLMs) have achieved remarkable performance on various reasoning tasks.\nHowever, most of them have been evaluated under noise-free context and the\ndilemma for LLMs to produce inaccurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize trigger sentences to\nencourage LLMs to concentrate on the relevant information but the trigger has\nlimited effect on final answer prediction. Inspired by interactive CoT method,\nwhere intermediate reasoning steps are promoted by multiple rounds of\ninteraction between users and LLMs, we propose a novel prompting method, namely\nR$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$\nprompting interacts with LLMs to perform key sentence extraction, variable\ndeclaration and answer prediction, which corresponds to a thought process of\nreviewing, rephrasing and resolving. The responses generated at the last\ninteraction will perform as hints to guide toward the responses of the next\ninteraction. Our experiments show that R$^3$ prompting significantly\noutperforms existing CoT prompting methods on five reasoning tasks under noisy\ncontext. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on\nthe reasoning tasks under noisy context compared to the most competitive\nprompting baseline. More analyses and ablation studies show the robustness and\ngeneralization of R$^3$ prompting method in solving reasoning tasks in LLMs\nunder noisy context.",
        "pdf_link": "https://arxiv.org/pdf/2310.16535v1.pdf"
    },
    {
        "title": "An Early Evaluation of GPT-4V(ision)",
        "authors": [
            "Yang Wu",
            "Shilong Wang",
            "Hao Yang",
            "Tian Zheng",
            "Hongbo Zhang",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "published": "2023-10-25T10:33:17Z",
        "summary": "In this paper, we evaluate different abilities of GPT-4V including visual\nunderstanding, language understanding, visual puzzle solving, and understanding\nof other modalities such as depth, thermal, video, and audio. To estimate\nGPT-4V's performance, we manually construct 656 test instances and carefully\nevaluate the results of GPT-4V. The highlights of our findings are as follows:\n(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks\nbut fails to recognize simple Chinese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering questions related to sensitive\ntraits such as gender, race, and age; (3) GPT-4V obtains worse results than\nGPT-4 (API) on language understanding tasks including general language\nunderstanding benchmarks and visual commonsense knowledge evaluation\nbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both\nvisual understanding and language understanding; (5) GPT-4V struggles to find\nthe nuances between two similar images and solve the easy math picture puzzles;\n(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to\nimage, such as video and thermal. Our experimental results reveal the ability\nand limitations of GPT-4V and we hope our paper can provide some insights into\nthe application and research of GPT-4V.",
        "pdf_link": "https://arxiv.org/pdf/2310.16534v1.pdf"
    },
    {
        "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
        "authors": [
            "Preethi Lahoti",
            "Nicholas Blumm",
            "Xiao Ma",
            "Raghavendra Kotikalapudi",
            "Sahitya Potluri",
            "Qijun Tan",
            "Hansa Srinivasan",
            "Ben Packer",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023-10-25T10:17:17Z",
        "summary": "A crucial challenge for generative large language models (LLMs) is diversity:\nwhen a user's prompt is under-specified, models may follow implicit assumptions\nwhile generating a response, which may result in homogenization of the\nresponses, as well as certain demographic groups being under-represented or\neven erased from the generated responses. In this paper, we formalize diversity\nof representation in generative LLMs. We present evaluation datasets and\npropose metrics to measure diversity in generated responses along people and\nculture axes. We find that LLMs understand the notion of diversity, and that\nthey can reason and critique their own responses for that goal. This finding\nmotivated a new prompting technique called collective-critique and self-voting\n(CCSV) to self-improve people diversity of LLMs by tapping into its diversity\nreasoning capabilities, without relying on handcrafted examples or prompt\ntuning. Extensive empirical experiments with both human and automated\nevaluations show that our proposed approach is effective at improving people\nand culture diversity, and outperforms all baseline methods by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2310.16523v1.pdf"
    },
    {
        "title": "OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models",
        "authors": [
            "Mingfeng Xue",
            "Dayiheng Liu",
            "Kexin Yang",
            "Guanting Dong",
            "Wenqiang Lei",
            "Zheng Yuan",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2023-10-25T10:06:17Z",
        "summary": "The emergence of large language models (LLMs) has revolutionized natural\nlanguage processing tasks. However, existing instruction-tuning datasets suffer\nfrom occupational bias: the majority of data relates to only a few occupations,\nwhich hampers the instruction-tuned LLMs to generate helpful responses to\nprofessional queries from practitioners in specific fields. To mitigate this\nissue and promote occupation-inclusive LLMs, we create an instruction-tuning\ndataset named \\emph{OccuQuest}, which contains 110,000+ prompt-completion pairs\nand 30,000+ dialogues covering over 1,000 occupations in 26 occupational\ncategories. We systematically request ChatGPT, organizing queries\nhierarchically based on Occupation, Responsibility, Topic, and Question, to\nensure a comprehensive coverage of occupational specialty inquiries. By\ncomparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we\nobserve that OccuQuest exhibits a more balanced distribution across\noccupations. Furthermore, we assemble three test sets for comprehensive\nevaluation, an occu-test set covering 25 occupational categories, an estate set\nfocusing on real estate, and an occu-quora set containing real-world questions\nfrom Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which\nsignificantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and\nWizardLM) on professional questions in GPT-4 and human evaluations. Notably, on\nthe occu-quora set, OccuLLaMA reaches a high win rate of 86.4\\% against\nWizardLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.16517v1.pdf"
    },
    {
        "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
        "authors": [
            "Guanzheng Chen",
            "Xin Li",
            "Zaiqiao Meng",
            "Shangsong Liang",
            "Lidong Bing"
        ],
        "published": "2023-10-25T08:13:02Z",
        "summary": "Transformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position\nEmbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their\nextrapolation abilities or sacrificing partial performance within the context\nwindow. Length extrapolation methods, although theoretically capable of\nextending the context window beyond the training sequence length, often\nunderperform in practical long-context applications. To address these\nchallenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We\ngeneralise the PE scaling approaches to model the continuous dynamics by\nordinary differential equations over the length scaling factor, thereby\novercoming the constraints of current PE scaling methods designed for specific\nlengths. Moreover, by extending the dynamics to desired context lengths beyond\nthe training sequence length, CLEX facilitates the length extrapolation with\nimpressive performance in practical tasks. We demonstrate that CLEX can be\nseamlessly incorporated into LLMs equipped with Rotary Position Embedding, such\nas LLaMA and GPT-NeoX, with negligible impact on training and inference\nlatency. Experimental results reveal that CLEX can effectively extend the\ncontext window to over 4x or almost 8x training length, with no deterioration\nin performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to\n32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
        "pdf_link": "https://arxiv.org/pdf/2310.16450v3.pdf"
    },
    {
        "title": "Graph Agent: Explicit Reasoning Agent for Graphs",
        "authors": [
            "Qinyong Wang",
            "Zhenxiang Gao",
            "Rong Xu"
        ],
        "published": "2023-10-25T07:20:16Z",
        "summary": "Graph embedding methods such as Graph Neural Networks (GNNs) and Graph\nTransformers have contributed to the development of graph reasoning algorithms\nfor various tasks on knowledge graphs. However, the lack of interpretability\nand explainability of graph embedding methods has limited their applicability\nin scenarios requiring explicit reasoning. In this paper, we introduce the\nGraph Agent (GA), an intelligent agent methodology of leveraging large language\nmodels (LLMs), inductive-deductive reasoning modules, and long-term memory for\nknowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning\nand existing graph embedding methods to provide an innovative approach for\ncomplex graph reasoning tasks. By converting graph structures into textual\ndata, GA enables LLMs to process, reason, and provide predictions alongside\nhuman-interpretable explanations. The effectiveness of the GA was evaluated on\nnode classification and link prediction tasks. Results showed that GA reached\nstate-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and\n89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to\nexisting GNN and transformer models, GA offered advantages of explicit\nreasoning ability, free-of-training, easy adaption to various graph reasoning\ntasks",
        "pdf_link": "https://arxiv.org/pdf/2310.16421v1.pdf"
    },
    {
        "title": "Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model",
        "authors": [
            "Dui Wang",
            "Xiangyu Hou",
            "Xiaohui Yang",
            "Bo Zhang",
            "Renbing Chen",
            "Daiyue Xue"
        ],
        "published": "2023-10-25T06:49:19Z",
        "summary": "Recommendation system (RS) plays significant roles in matching users\ninformation needs for Internet applications, and it usually utilizes the\nvanilla neural network as the backbone to handle embedding details. Recently,\nthe large language model (LLM) has exhibited emergent abilities and achieved\ngreat breakthroughs both in the CV and NLP communities. Thus, it is logical to\nincorporate RS with LLM better, which has become an emerging research\ndirection. Although some existing works have made their contributions to this\nissue, they mainly consider the single key situation (e.g. historical\ninteractions), especially in sequential recommendation. The situation of\nmultiple key-value data is simply neglected. This significant scenario is\nmainstream in real practical applications, where the information of users (e.g.\nage, occupation, etc) and items (e.g. title, category, etc) has more than one\nkey. Therefore, we aim to implement sequential recommendations based on\nmultiple key-value data by incorporating RS with LLM. In particular, we\ninstruct tuning a prevalent open-source LLM (Llama 7B) in order to inject\ndomain knowledge of RS into the pre-trained LLM. Since we adopt multiple\nkey-value strategies, LLM is hard to learn well among these keys. Thus the\ngeneral and innovative shuffle and mask strategies, as an innovative manner of\ndata argument, are designed. To demonstrate the effectiveness of our approach,\nextensive experiments are conducted on the popular and suitable dataset\nMovieLens which contains multiple keys-value. The experimental results\ndemonstrate that our approach can nicely and effectively complete this\nchallenging issue.",
        "pdf_link": "https://arxiv.org/pdf/2310.16409v1.pdf"
    },
    {
        "title": "Evaluating, Understanding, and Improving Constrained Text Generation for Large Language Models",
        "authors": [
            "Xiang Chen",
            "Xiaojun Wan"
        ],
        "published": "2023-10-25T03:58:49Z",
        "summary": "Advancements in natural language generation (NLG) and large language models\n(LLMs) have led to proficient text generation in various tasks. However,\nintegrating intricate constraints into neural text generation, due to LLMs'\nopacity, remains challenging. This study investigates constrained text\ngeneration for LLMs, where predefined constraints are applied during LLM's\ngeneration process. Our research mainly focuses on mainstream open-source LLMs,\ncategorizing constraints into lexical, structural, and relation-based types. We\nalso present various benchmarks to facilitate fair evaluation. The study\naddresses some key research questions, including evaluating, understanding and\nimproving constrained text generation for LLMs. Results illuminate LLMs'\ncapacity and deficiency to incorporate constraints and provide insights for\nfuture developments in constrained text generation. Codes and datasets will be\nreleased upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2310.16343v2.pdf"
    },
    {
        "title": "Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation",
        "authors": [
            "Jiexin Wang",
            "Liuwen Cao",
            "Xitong Luo",
            "Zhiping Zhou",
            "Jiayuan Xie",
            "Adam Jatowt",
            "Yi Cai"
        ],
        "published": "2023-10-25T00:32:56Z",
        "summary": "Large language models (LLMs) have brought significant advancements to code\ngeneration, benefiting both novice and experienced developers. However, their\ntraining using unsanitized data from open-source repositories, like GitHub,\nintroduces the risk of inadvertently propagating security vulnerabilities. To\neffectively mitigate this concern, this paper presents a comprehensive study\nfocused on evaluating and enhancing code LLMs from a software security\nperspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as\nsupplemental material and will be made publicly available after publication.},\na meticulously curated dataset targeting 21 critical vulnerability types.\nSecuCoGen comprises 180 samples and serves as the foundation for conducting\nexperiments on three crucial code-related tasks: code generation, code repair\nand vulnerability classification, with a strong emphasis on security. Our\nexperimental results reveal that existing models often overlook security\nconcerns during code generation, leading to the generation of vulnerable code.\nTo address this, we propose effective approaches to mitigate the security\nvulnerabilities and enhance the overall robustness of code generated by LLMs.\nMoreover, our study identifies weaknesses in existing models' ability to repair\nvulnerable code, even when provided with vulnerability information.\nAdditionally, certain vulnerability types pose challenges for the models,\nhindering their performance in vulnerability classification. Based on these\nfindings, we believe our study will have a positive impact on the software\nengineering community, inspiring the development of improved methods for\ntraining and utilizing LLMs, thereby leading to safer and more trustworthy\nmodel deployment.",
        "pdf_link": "https://arxiv.org/pdf/2310.16263v1.pdf"
    },
    {
        "title": "ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair",
        "authors": [
            "Yonghao Wu",
            "Zheng Li",
            "Jie M. Zhang",
            "Yong Liu"
        ],
        "published": "2023-10-25T00:06:02Z",
        "summary": "With the growing interest on Large Language Models (LLMs) for fault\nlocalization and program repair, ensuring the integrity and generalizability of\nthe LLM-based methods becomes paramount. The code in existing widely-adopted\nbenchmarks for these tasks was written before the the bloom of LLMs and may be\nincluded in the training data of existing popular LLMs, thereby suffering from\nthe threat of data leakage, leading to misleadingly optimistic performance\nmetrics. To address this issue, we introduce \"ConDefects\", a novel dataset of\nreal faults meticulously curated to eliminate such overlap. ConDefects contains\n1,254 Java faulty programs and 1,625 Python faulty programs. All these programs\nare sourced from the online competition platform AtCoder and were produced\nbetween October 2021 and September 2023. We pair each fault with fault\nlocations and the corresponding repaired code versions, making it tailored for\nin fault localization and program repair related research. We also provide\ninterfaces for selecting subsets based on different time windows and coding\ntask difficulties. While inspired by LLM-based tasks, ConDefects can be adopted\nfor benchmarking ALL types of fault localization and program repair methods.\nThe dataset is publicly available, and a demo video can be found at\nhttps://www.youtube.com/watch?v=22j15Hj5ONk.",
        "pdf_link": "https://arxiv.org/pdf/2310.16253v1.pdf"
    },
    {
        "title": "Knowledge Editing for Large Language Models: A Survey",
        "authors": [
            "Song Wang",
            "Yaochen Zhu",
            "Haochen Liu",
            "Zaiyi Zheng",
            "Chen Chen",
            "Jundong Li"
        ],
        "published": "2023-10-24T22:18:13Z",
        "summary": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.",
        "pdf_link": "https://arxiv.org/pdf/2310.16218v3.pdf"
    },
    {
        "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
        "authors": [
            "Alejandro Lozano",
            "Scott L Fleming",
            "Chia-Chun Chiang",
            "Nigam Shah"
        ],
        "published": "2023-10-24T19:43:39Z",
        "summary": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
        "pdf_link": "https://arxiv.org/pdf/2310.16146v1.pdf"
    },
    {
        "title": "Can You Follow Me? Testing Situational Understanding in ChatGPT",
        "authors": [
            "Chenghao Yang",
            "Allyson Ettinger"
        ],
        "published": "2023-10-24T19:22:01Z",
        "summary": "Understanding sentence meanings and updating information states appropriately\nacross time -- what we call \"situational understanding\" (SU) -- is a critical\nability for human-like AI agents. SU is essential in particular for chat\nmodels, such as ChatGPT, to enable consistent, coherent, and effective dialogue\nbetween humans and AI. Previous works have identified certain SU limitations in\nnon-chatbot Large Language models (LLMs), but the extent and causes of these\nlimitations are not well understood, and capabilities of current chat-based\nmodels in this domain have not been explored. In this work we tackle these\nquestions, proposing a novel synthetic environment for SU testing which allows\nus to do controlled and systematic testing of SU in chat-oriented models,\nthrough assessment of models' ability to track and enumerate environment\nstates. Our environment also allows for close analysis of dynamics of model\nperformance, to better understand underlying causes for performance patterns.\nWe apply our test to ChatGPT, the state-of-the-art chatbot, and find that\ndespite the fundamental simplicity of the task, the model's performance\nreflects an inability to retain correct environment states across time. Our\nfollow-up analyses suggest that performance degradation is largely because\nChatGPT has non-persistent in-context memory (although it can access the full\ndialogue history) and it is susceptible to hallucinated updates -- including\nupdates that artificially inflate accuracies. Our findings suggest overall that\nChatGPT is not currently equipped for robust tracking of situation states, and\nthat trust in the impressive dialogue performance of ChatGPT comes with risks.\nWe release the codebase for reproducing our test environment, as well as all\nprompts and API responses from ChatGPT, at\nhttps://github.com/yangalan123/SituationalTesting.",
        "pdf_link": "https://arxiv.org/pdf/2310.16135v1.pdf"
    },
    {
        "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
        "authors": [
            "Saiteja Utpala",
            "Sara Hooker",
            "Pin Yu Chen"
        ],
        "published": "2023-10-24T18:25:13Z",
        "summary": "Numerous studies have highlighted the privacy risks associated with\npretrained large language models. In contrast, our research offers a unique\nperspective by demonstrating that pretrained large language models can\neffectively contribute to privacy preservation. We propose a locally\ndifferentially private mechanism called DP-Prompt, which leverages the power of\npretrained large language models and zero-shot prompting to counter author\nde-anonymization attacks while minimizing the impact on downstream utility.\nWhen DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),\nwe observe a notable reduction in the success rate of de-anonymization attacks,\nshowing that it surpasses existing approaches by a considerable margin despite\nits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt\n(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving\na 46\\% reduction in author identification F1 score against static attackers and\na 26\\% reduction against adaptive attackers. We conduct extensive experiments\nacross six open-source large language models, ranging up to 7 billion\nparameters, to analyze various effects of the privacy-utility tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2310.16111v2.pdf"
    },
    {
        "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition",
        "authors": [
            "Sander Schulhoff",
            "Jeremy Pinto",
            "Anaum Khan",
            "Louis-Fran√ßois Bouchard",
            "Chenglei Si",
            "Svetlina Anati",
            "Valen Tagliabue",
            "Anson Liu Kost",
            "Christopher Carnahan",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-10-24T18:18:11Z",
        "summary": "Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.",
        "pdf_link": "https://arxiv.org/pdf/2311.16119v3.pdf"
    },
    {
        "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
        "authors": [
            "Zayne Sprague",
            "Xi Ye",
            "Kaj Bostrom",
            "Swarat Chaudhuri",
            "Greg Durrett"
        ],
        "published": "2023-10-24T17:59:20Z",
        "summary": "While large language models (LLMs) equipped with techniques like\nchain-of-thought prompting have demonstrated impressive capabilities, they\nstill fall short in their ability to reason robustly in complex settings.\nHowever, evaluating LLM reasoning is challenging because system capabilities\ncontinue to grow while benchmark datasets for tasks like logical deduction have\nremained static. We introduce MuSR, a dataset for evaluating language models on\nmultistep soft reasoning tasks specified in a natural language narrative. This\ndataset has two crucial features. First, it is created through a novel\nneurosymbolic synthetic-to-natural generation algorithm, enabling the\nconstruction of complex reasoning instances that challenge GPT-4 (e.g., murder\nmysteries roughly 1000 words in length) and which can be scaled further as more\ncapable LLMs are released. Second, our dataset instances are free text\nnarratives corresponding to real-world domains of reasoning; this makes it\nsimultaneously much more challenging than other synthetically-crafted\nbenchmarks while remaining realistic and tractable for human annotators to\nsolve with high accuracy. We evaluate a range of LLMs and prompting techniques\non this dataset and characterize the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2310.16049v2.pdf"
    },
    {
        "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
        "authors": [
            "Joy Hsu",
            "Jiayuan Mao",
            "Joshua B. Tenenbaum",
            "Jiajun Wu"
        ],
        "published": "2023-10-24T17:50:20Z",
        "summary": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.16035v1.pdf"
    },
    {
        "title": "Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs",
        "authors": [
            "Jiarui Zhang",
            "Mahyar Khayatkhoei",
            "Prateek Chhikara",
            "Filip Ilievski"
        ],
        "published": "2023-10-24T17:48:04Z",
        "summary": "Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive small details as well as\nlarge details in images. In particular, we show that their zero-shot accuracy\nin answering visual questions is very sensitive to the size of the visual\nsubject of the question, declining up to 46% with size. Furthermore, we show\nthat this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose five automatic visual cropping methods --\nleveraging either external localization models or the decision process of the\ngiven MLLM itself -- as inference time mechanisms to improve the zero-shot\nperformance of MLLMs. We study their effectiveness on four popular VQA\ndatasets, and a subset of the VQAv2 dataset tailored towards fine visual\ndetails. Our findings suggest that MLLMs should be used with caution in\ndetail-sensitive VQA applications, and that visual cropping is a promising\ndirection to improve their zero-shot performance. To facilitate further\ninvestigation of MLLMs' behaviors, our code and data are publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2310.16033v3.pdf"
    },
    {
        "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation",
        "authors": [
            "Szymon Antoniak",
            "Sebastian Jaszczur",
            "Micha≈Ç Krutul",
            "Maciej Pi√≥ro",
            "Jakub Krajewski",
            "Jan Ludziejewski",
            "Tomasz Odrzyg√≥≈∫d≈∫",
            "Marek Cygan"
        ],
        "published": "2023-10-24T16:03:57Z",
        "summary": "Despite the promise of Mixture of Experts (MoE) models in increasing\nparameter counts of Transformer models while maintaining training and inference\ncosts, their application carries notable drawbacks. The key strategy of these\nmodels is to, for each processed token, activate at most a few experts -\nsubsets of an extensive feed-forward layer. But this approach is not without\nits challenges. The operation of matching experts and tokens is discrete, which\nmakes MoE models prone to issues like training instability and uneven expert\nutilization. Existing techniques designed to address these concerns, such as\nauxiliary losses or balance-aware matching, result either in lower model\nperformance or are more difficult to train. In response to these issues, we\npropose Mixture of Tokens, a fully-differentiable model that retains the\nbenefits of MoE architectures while avoiding the aforementioned difficulties.\nRather than routing tokens to experts, this approach mixes tokens from\ndifferent examples prior to feeding them to experts, enabling the model to\nlearn from all token-expert combinations. Importantly, this mixing can be\ndisabled to avoid mixing of different sequences during inference. Crucially,\nthis method is fully compatible with both masked and causal Large Language\nModel training and inference.",
        "pdf_link": "https://arxiv.org/pdf/2310.15961v1.pdf"
    },
    {
        "title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Zhichao Yang",
            "Huixue Zhou",
            "Rumeng Li",
            "Xun Wang",
            "Yucheng Xu",
            "Hong Yu"
        ],
        "published": "2023-10-24T15:59:43Z",
        "summary": "We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.",
        "pdf_link": "https://arxiv.org/pdf/2310.15959v2.pdf"
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "authors": [
            "Xubin Ren",
            "Wei Wei",
            "Lianghao Xia",
            "Lixin Su",
            "Suqi Cheng",
            "Junfeng Wang",
            "Dawei Yin",
            "Chao Huang"
        ],
        "published": "2023-10-24T15:51:13Z",
        "summary": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.",
        "pdf_link": "https://arxiv.org/pdf/2310.15950v4.pdf"
    },
    {
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": [
            "Iker Garc√≠a-Ferrero",
            "Bego√±a Altuna",
            "Javier √Ålvez",
            "Itziar Gonzalez-Dios",
            "German Rigau"
        ],
        "published": "2023-10-24T15:38:21Z",
        "summary": "Although large language models (LLMs) have apparently acquired a certain\nlevel of grammatical knowledge and the ability to make generalizations, they\nfail to interpret negation, a crucial step in Natural Language Processing. We\ntry to clarify the reasons for the sub-optimal performance of LLMs\nunderstanding negation. We introduce a large semi-automatically generated\ndataset of circa 400,000 descriptive sentences about commonsense knowledge that\ncan be true or false in which negation is present in about 2/3 of the corpus in\ndifferent forms. We have used our dataset with the largest available open LLMs\nin a zero-shot approach to grasp their generalization and inference capability\nand we have also fine-tuned some of the models to assess whether the\nunderstanding of negation can be trained. Our findings show that, while LLMs\nare proficient at classifying affirmative sentences, they struggle with\nnegative sentences and lack a deep understanding of negation, often relying on\nsuperficial cues. Although fine-tuning the models on negative sentences\nimproves their performance, the lack of generalization in handling negation is\npersistent, highlighting the ongoing challenges of LLMs regarding negation\nunderstanding and generalization. The dataset and code are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2310.15941v1.pdf"
    },
    {
        "title": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity",
        "authors": [
            "Yun Li",
            "Lin Niu",
            "Xipeng Zhang",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2023-10-24T15:27:15Z",
        "summary": "Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.",
        "pdf_link": "https://arxiv.org/pdf/2310.15929v2.pdf"
    },
    {
        "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT",
        "authors": [
            "Yirong Chen",
            "Zhenyu Wang",
            "Xiaofen Xing",
            "huimin zheng",
            "Zhipei Xu",
            "Kai Fang",
            "Junhong Wang",
            "Sihang Li",
            "Jieling Wu",
            "Qi Liu",
            "Xiangmin Xu"
        ],
        "published": "2023-10-24T14:57:34Z",
        "summary": "Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.",
        "pdf_link": "https://arxiv.org/pdf/2310.15896v2.pdf"
    },
    {
        "title": "SoK: Memorization in General-Purpose Large Language Models",
        "authors": [
            "Valentin Hartmann",
            "Anshuman Suri",
            "Vincent Bindschaedler",
            "David Evans",
            "Shruti Tople",
            "Robert West"
        ],
        "published": "2023-10-24T14:25:53Z",
        "summary": "Large Language Models (LLMs) are advancing at a remarkable pace, with myriad\napplications under development. Unlike most earlier machine learning models,\nthey are no longer built for one specific application but are designed to excel\nin a wide range of tasks. A major part of this success is due to their huge\ntraining datasets and the unprecedented number of model parameters, which allow\nthem to memorize large amounts of information contained in the training data.\nThis memorization goes beyond mere language, and encompasses information only\npresent in a few documents. This is often desirable since it is necessary for\nperforming tasks such as question answering, and therefore an important part of\nlearning, but also brings a whole array of issues, from privacy and security to\ncopyright and beyond. LLMs can memorize short secrets in the training data, but\ncan also memorize concepts like facts or writing styles that can be expressed\nin text in many different ways. We propose a taxonomy for memorization in LLMs\nthat covers verbatim text, facts, ideas and algorithms, writing styles,\ndistributional properties, and alignment goals. We describe the implications of\neach type of memorization - both positive and negative - for model performance,\nprivacy, security and confidentiality, copyright, and auditing, and ways to\ndetect and prevent memorization. We further highlight the challenges that arise\nfrom the predominant way of defining memorization with respect to model\nbehavior instead of model weights, due to LLM-specific phenomena such as\nreasoning capabilities or differences between decoding algorithms. Throughout\nthe paper, we describe potential risks and opportunities arising from\nmemorization in LLMs that we hope will motivate new research directions.",
        "pdf_link": "https://arxiv.org/pdf/2310.18362v1.pdf"
    },
    {
        "title": "Self-Guard: Empower the LLM to Safeguard Itself",
        "authors": [
            "Zezhong Wang",
            "Fangkai Yang",
            "Lu Wang",
            "Pu Zhao",
            "Hongru Wang",
            "Liang Chen",
            "Qingwei Lin",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-24T14:08:26Z",
        "summary": "The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.",
        "pdf_link": "https://arxiv.org/pdf/2310.15851v2.pdf"
    },
    {
        "title": "Unnatural language processing: How do language models handle machine-generated prompts?",
        "authors": [
            "Corentin Kervadec",
            "Francesca Franzon",
            "Marco Baroni"
        ],
        "published": "2023-10-24T13:32:20Z",
        "summary": "Language model prompt optimization research has shown that semantically and\ngrammatically well-formed manually crafted prompts are routinely outperformed\nby automatically generated token sequences with no apparent meaning or\nsyntactic structure, including sequences of vectors from a model's embedding\nspace. We use machine-generated prompts to probe how models respond to input\nthat is not composed of natural language expressions. We study the behavior of\nmodels of different sizes in multiple semantic tasks in response to both\ncontinuous and discrete machine-generated prompts, and compare it to the\nbehavior in response to human-generated natural-language prompts. Even when\nproducing a similar output, machine-generated and human prompts trigger\ndifferent response patterns through the network processing pathways, including\ndifferent perplexities, different attention and output entropy distributions,\nand different unit activation profiles. We provide preliminary insight into the\nnature of the units activated by different prompt types, suggesting that only\nnatural language prompts recruit a genuinely linguistic circuit.",
        "pdf_link": "https://arxiv.org/pdf/2310.15829v1.pdf"
    },
    {
        "title": "Generative Language Models Exhibit Social Identity Biases",
        "authors": [
            "Tiancheng Hu",
            "Yara Kyrychenko",
            "Steve Rathje",
            "Nigel Collier",
            "Sander van der Linden",
            "Jon Roozenbeek"
        ],
        "published": "2023-10-24T13:17:40Z",
        "summary": "The surge in popularity of large language models has given rise to concerns\nabout biases that these models could learn from humans. In this study, we\ninvestigate whether ingroup solidarity and outgroup hostility, fundamental\nsocial biases known from social science, are present in 51 large language\nmodels. We find that almost all foundational language models and some\ninstruction fine-tuned models exhibit clear ingroup-positive and\noutgroup-negative biases when prompted to complete sentences (e.g., \"We\nare...\"). A comparison of LLM-generated sentences with human-written sentences\non the internet reveals that these models exhibit similar level, if not\ngreater, levels of bias than human text. To investigate where these biases stem\nfrom, we experimentally varied the amount of ingroup-positive or\noutgroup-negative sentences the model was exposed to during fine-tuning in the\ncontext of the United States Democrat-Republican divide. Doing so resulted in\nthe models exhibiting a marked increase in ingroup solidarity and an even\ngreater increase in outgroup hostility. Furthermore, removing either\ningroup-positive or outgroup-negative sentences (or both) from the fine-tuning\ndata leads to a significant reduction in both ingroup solidarity and outgroup\nhostility, suggesting that biases can be reduced by removing biased training\ndata. Our findings suggest that modern language models exhibit fundamental\nsocial identity biases and that such biases can be mitigated by curating\ntraining data. Our results have practical implications for creating less biased\nlarge-language models and further underscore the need for more research into\nuser interactions with LLMs to prevent potential bias reinforcement in humans.",
        "pdf_link": "https://arxiv.org/pdf/2310.15819v1.pdf"
    },
    {
        "title": "Improving generalization in large language models by learning prefix subspaces",
        "authors": [
            "Louis Falissard",
            "Vincent Guigue",
            "Laure Soulier"
        ],
        "published": "2023-10-24T12:44:09Z",
        "summary": "This article focuses on large language models (LLMs) fine-tuning in the\nscarce data regime (also known as the \"few-shot\" learning setting). We propose\na method to increase the generalization capabilities of LLMs based on neural\nnetwork subspaces. This optimization method, recently introduced in computer\nvision, aims to improve model generalization by identifying wider local optima\nthrough the joint optimization of an entire simplex of models in parameter\nspace. Its adaptation to massive, pretrained transformers, however, poses some\nchallenges. First, their considerable number of parameters makes it difficult\nto train several models jointly, and second, their deterministic parameter\ninitialization schemes make them unfit for the subspace method as originally\nproposed. We show in this paper that \"Parameter Efficient Fine-Tuning\" (PEFT)\nmethods, however, are perfectly compatible with this original approach, and\npropose to learn entire simplex of continuous prefixes. We test our method on a\nvariant of the GLUE benchmark adapted to the few-shot learning setting, and\nshow that both our contributions jointly lead to a gain in average performances\ncompared to sota methods. The implementation can be found at the following\nlink: https://github.com/Liloulou/prefix_subspace",
        "pdf_link": "https://arxiv.org/pdf/2310.15793v1.pdf"
    },
    {
        "title": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers",
        "authors": [
            "Mosh Levy",
            "Shauli Ravfogel",
            "Yoav Goldberg"
        ],
        "published": "2023-10-24T12:37:06Z",
        "summary": "Recent applications of LLMs in Machine Reading Comprehension (MRC) systems\nhave shown impressive results, but the use of shortcuts, mechanisms triggered\nby features spuriously correlated to the true label, has emerged as a potential\nthreat to their reliability. We analyze the problem from two angles: LLMs as\neditors, guided to edit text to mislead LLMs; and LLMs as readers, who answer\nquestions based on the edited text. We introduce a framework that guides an\neditor to add potential shortcuts-triggers to samples. Using GPT4 as the\neditor, we find it can successfully edit trigger shortcut in samples that fool\nLLMs. Analysing LLMs as readers, we observe that even capable LLMs can be\ndeceived using shortcut knowledge. Strikingly, we discover that GPT4 can be\ndeceived by its own edits (15% drop in F1). Our findings highlight inherent\nvulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a\ncurated dataset generated by our framework for future research.",
        "pdf_link": "https://arxiv.org/pdf/2310.18360v1.pdf"
    },
    {
        "title": "Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection",
        "authors": [
            "Dennis Fucci",
            "Marco Gaido",
            "Sara Papi",
            "Mauro Cettolo",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2023-10-24T11:55:16Z",
        "summary": "When translating words referring to the speaker, speech translation (ST)\nsystems should not resort to default masculine generics nor rely on potentially\nmisleading vocal traits. Rather, they should assign gender according to the\nspeakers' preference. The existing solutions to do so, though effective, are\nhardly feasible in practice as they involve dedicated model re-training on\ngender-labeled ST data. To overcome these limitations, we propose the first\ninference-time solution to control speaker-related gender inflections in ST.\nOur approach partially replaces the (biased) internal language model (LM)\nimplicitly learned by the ST decoder with gender-specific external LMs.\nExperiments on en->es/fr/it show that our solution outperforms the base models\nand the best training-time mitigation strategy by up to 31.0 and 1.6 points in\ngender accuracy, respectively, for feminine forms. The gains are even larger\n(up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits\nconflict with their gender.",
        "pdf_link": "https://arxiv.org/pdf/2310.15752v1.pdf"
    },
    {
        "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
        "authors": [
            "Dohwan Ko",
            "Ji Soo Lee",
            "Wooyoung Kang",
            "Byungseok Roh",
            "Hyunwoo J. Kim"
        ],
        "published": "2023-10-24T11:44:39Z",
        "summary": "Large Language Models (LLMs) have shown remarkable performances on a wide\nrange of natural language understanding and generation tasks. We observe that\nthe LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$\nfor temporal and causal reasoning in Video Question Answering (VideoQA).\nHowever, such priors often cause suboptimal results on VideoQA by leading the\nmodel to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$,\nwhile ignoring visual content. This is also known as `ungrounded guesses' or\n`hallucinations'. To address this problem while leveraging LLMs' prior on\nVideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to\npredict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping\nthe source pair and the target label to understand their complex relationships,\n$\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,\nrespectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to\nLLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five\nchallenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general\nframework that is applicable to various LLMs (OPT and GPT-J) and consistently\nimproves their performances. We empirically demonstrate that Flipped-VQA not\nonly enhances the exploitation of linguistic shortcuts but also mitigates the\nlinguistic bias, which causes incorrect answers over-relying on the question.\nCode is available at https://github.com/mlvlab/Flipped-VQA.",
        "pdf_link": "https://arxiv.org/pdf/2310.15747v2.pdf"
    },
    {
        "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
        "authors": [
            "Zeyuan Yang",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023-10-24T11:40:34Z",
        "summary": "Large Language Models (LLMs) have showcased impressive performance. However,\ndue to their inability to capture relationships among samples, these frozen\nLLMs inevitably keep repeating similar mistakes. In this work, we propose our\nTuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving\ntheir performance by learning from previous mistakes. Considering data arrives\nsequentially, LLMs gradually accumulate rules from incorrect cases, forming a\nrule collection. These rules are then utilized by the LLMs to avoid making\nsimilar mistakes when processing subsequent inputs. Moreover, the rules remain\nindependent of the primary prompts, seamlessly complementing prompt design\nstrategies. Experimentally, we show that TRAN improves over recent baselines by\na large margin.",
        "pdf_link": "https://arxiv.org/pdf/2310.15746v1.pdf"
    },
    {
        "title": "Prevalence and prevention of large language model use in crowd work",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Philip Cozzolino",
            "Andrew Gordon",
            "David Rothschild",
            "Robert West"
        ],
        "published": "2023-10-24T09:52:09Z",
        "summary": "We show that the use of large language models (LLMs) is prevalent among crowd\nworkers, and that targeted mitigation strategies can significantly reduce, but\nnot eliminate, LLM use. On a text summarization task where workers were not\ndirected in any way regarding their LLM use, the estimated prevalence of LLM\nuse was around 30%, but was reduced by about half by asking workers to not use\nLLMs and by raising the cost of using them, e.g., by disabling copy-pasting.\nSecondary analyses give further insight into LLM use and its prevention: LLM\nuse yields high-quality but homogeneous responses, which may harm research\nconcerned with human (rather than model) behavior and degrade future models\ntrained with crowdsourced data. At the same time, preventing LLM use may be at\nodds with obtaining high-quality responses; e.g., when requesting workers not\nto use LLMs, summaries contained fewer keywords carrying essential information.\nOur estimates will likely change as LLMs increase in popularity or\ncapabilities, and as norms around their usage change. Yet, understanding the\nco-evolution of LLM-based tools and users is key to maintaining the validity of\nresearch done using crowdsourcing, and we provide a critical baseline before\nwidespread adoption ensues.",
        "pdf_link": "https://arxiv.org/pdf/2310.15683v1.pdf"
    },
    {
        "title": "A Survey on Detection of LLMs-Generated Content",
        "authors": [
            "Xianjun Yang",
            "Liangming Pan",
            "Xuandong Zhao",
            "Haifeng Chen",
            "Linda Petzold",
            "William Yang Wang",
            "Wei Cheng"
        ],
        "published": "2023-10-24T09:10:26Z",
        "summary": "The burgeoning capabilities of advanced large language models (LLMs) such as\nChatGPT have led to an increase in synthetic content generation with\nimplications across a variety of sectors, including media, cybersecurity,\npublic discourse, and education. As such, the ability to detect LLMs-generated\ncontent has become of paramount importance. We aim to provide a detailed\noverview of existing detection strategies and benchmarks, scrutinizing their\ndifferences and identifying key challenges and prospects in the field,\nadvocating for more adaptable and robust models to enhance detection accuracy.\nWe also posit the necessity for a multi-faceted approach to defend against\nvarious attacks to counter the rapidly advancing capabilities of LLMs. To the\nbest of our knowledge, this work is the first comprehensive survey on the\ndetection in the era of LLMs. We hope it will provide a broad understanding of\nthe current landscape of LLMs-generated content detection, offering a guiding\nreference for researchers and practitioners striving to uphold the integrity of\ndigital information in an era increasingly dominated by synthetic content. The\nrelevant papers are summarized and will be consistently updated at\nhttps://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.",
        "pdf_link": "https://arxiv.org/pdf/2310.15654v1.pdf"
    },
    {
        "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
        "authors": [
            "Junyi Liu",
            "Liangzhi Li",
            "Tong Xiang",
            "Bowen Wang",
            "Yiming Qian"
        ],
        "published": "2023-10-24T06:56:38Z",
        "summary": "Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.",
        "pdf_link": "https://arxiv.org/pdf/2310.15556v2.pdf"
    },
    {
        "title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation",
        "authors": [
            "Jialing Pan",
            "Adrien Sad√©",
            "Jin Kim",
            "Eric Soriano",
            "Guillem Sole",
            "Sylvain Flamant"
        ],
        "published": "2023-10-24T06:04:28Z",
        "summary": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.",
        "pdf_link": "https://arxiv.org/pdf/2310.15539v2.pdf"
    },
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
        "authors": [
            "Marah I Abdin",
            "Suriya Gunasekar",
            "Varun Chandrasekaran",
            "Jerry Li",
            "Mert Yuksekgonul",
            "Rahee Ghosh Peshawaria",
            "Ranjita Naik",
            "Besmira Nushi"
        ],
        "published": "2023-10-24T04:40:38Z",
        "summary": "We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.",
        "pdf_link": "https://arxiv.org/pdf/2310.15511v1.pdf"
    },
    {
        "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
        "authors": [
            "Xiaoyi Chen",
            "Siyuan Tang",
            "Rui Zhu",
            "Shijun Yan",
            "Lei Jin",
            "Zihao Wang",
            "Liya Su",
            "XiaoFeng Wang",
            "Haixu Tang"
        ],
        "published": "2023-10-24T02:48:19Z",
        "summary": "The era post-2018 marked the advent of Large Language Models (LLMs), with\ninnovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.\nAs the industry galloped toward augmenting model parameters and capitalizing on\nvast swaths of human language data, security and privacy challenges also\nemerged. Foremost among these is the potential inadvertent accrual of Personal\nIdentifiable Information (PII) during web-based data acquisition, posing risks\nof unintended PII disclosure. While strategies like RLHF during training and\nCatastrophic Forgetting have been marshaled to control the risk of privacy\ninfringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning\ninterface for GPT-3.5, have reignited concerns. One may ask: can the\nfine-tuning of LLMs precipitate the leakage of personal information embedded\nwithin training datasets? This paper reports the first endeavor to seek the\nanswer to the question, particularly our discovery of a new LLM exploitation\navenue, called the Janus attack. In the attack, one can construct a PII\nassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,\nto potentially reinstate and reveal concealed PIIs. Our findings indicate that,\nwith a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from\nbeing impermeable to PII extraction to a state where they divulge a substantial\nproportion of concealed PII. This research, through its deep dive into the\nJanus attack vector, underscores the imperative of navigating the intricate\ninterplay between LLM utility and privacy preservation.",
        "pdf_link": "https://arxiv.org/pdf/2310.15469v1.pdf"
    },
    {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Le Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023-10-24T00:24:11Z",
        "summary": "Theory of mind (ToM) evaluations currently focus on testing models using\npassive narratives that inherently lack interactivity. We introduce FANToM, a\nnew benchmark designed to stress-test ToM within information-asymmetric\nconversational contexts via question answering. Our benchmark draws upon\nimportant theoretical requisites from psychology and necessary empirical\nconsiderations when evaluating large language models (LLMs). In particular, we\nformulate multiple types of questions that demand the same underlying reasoning\nto identify illusory or false sense of ToM capabilities in LLMs. We show that\nFANToM is challenging for state-of-the-art LLMs, which perform significantly\nworse than humans even with chain-of-thought reasoning or fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.15421v3.pdf"
    },
    {
        "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions",
        "authors": [
            "Ting-Yao Hsu",
            "Chieh-Yang Huang",
            "Ryan Rossi",
            "Sungchul Kim",
            "C. Lee Giles",
            "Ting-Hao K. Huang"
        ],
        "published": "2023-10-23T23:24:57Z",
        "summary": "There is growing interest in systems that generate captions for scientific\nfigures. However, assessing these systems output poses a significant challenge.\nHuman evaluation requires academic expertise and is costly, while automatic\nevaluation depends on often low-quality author-written captions. This paper\ninvestigates using large language models (LLMs) as a cost-effective,\nreference-free method for evaluating figure captions. We first constructed\nSCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600\nscientific figure captions, both original and machine-made, for 600 arXiv\nfigures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption\nbased on its potential to aid reader understanding, given relevant context such\nas figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot\nevaluator, outperformed all other models and even surpassed assessments made by\nComputer Science and Informatics undergraduates, achieving a Kendall\ncorrelation score of 0.401 with Ph.D. students rankings",
        "pdf_link": "https://arxiv.org/pdf/2310.15405v1.pdf"
    },
    {
        "title": "DoGE: Domain Reweighting with Generalization Estimation",
        "authors": [
            "Simin Fan",
            "Matteo Pagliardini",
            "Martin Jaggi"
        ],
        "published": "2023-10-23T22:51:58Z",
        "summary": "The coverage and composition of the pretraining data significantly impacts\nthe generalization ability of Large Language Models (LLMs). Despite its\nimportance, recent LLMs still rely on heuristics and trial and error to\nincrease or reduce the influence of data-domains. We propose DOmain reweighting\nwith Generalization Estimation (DoGE), which optimizes the probability of\nsampling from each domain (domain weights) in a principled way. Our approach is\na two-stage process consisting of (i) training a proxy model to obtain domain\nweights using a bi-level optimization algorithm; (ii) training a larger base\nmodel by sampling training domains according to the learned domain weights. In\nour experiments, we extensively show how DoGE improves the generalization of\nthe base model to any target data mixture. On the SlimPajama dataset, our base\nmodel gets better perplexity and few-shot reasoning accuracies across $6$ tasks\ncompared to baseline methods. Moreover, aiming to generalize to out-of-domain\ntarget tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can\neffectively identify inter-domain dependencies, and consistently achieves\nbetter test perplexity on the target domain.",
        "pdf_link": "https://arxiv.org/pdf/2310.15393v2.pdf"
    },
    {
        "title": "Irreducible Curriculum for Language Model Pretraining",
        "authors": [
            "Simin Fan",
            "Martin Jaggi"
        ],
        "published": "2023-10-23T22:41:33Z",
        "summary": "Automatic data selection and curriculum design for training large language\nmodels is challenging, with only a few existing methods showing improvements\nover standard training. Furthermore, current schemes focus on domain-level\nselection, overlooking the more fine-grained contributions of each individual\ntraining point. It is difficult to apply traditional datapoint selection\nmethods on large language models: most online batch selection methods perform\ntwo-times forward or backward passes, which introduces considerable extra costs\nwith large-scale models. To mitigate these obstacles, we propose irreducible\ncurriculum as a curriculum learning algorithm for language model pretraining,\nwhich prioritizes samples with higher learnability. Specifically, to avoid\nprohibitive extra computation overhead, we simulate the sample loss along the\nmain model's training trajectory using a small-scale proxy model. Our\nexperiments on the RedPajama-1B dataset demonstrate a consistent improvement on\nvalidation perplexity across all 7 domains compared to random uniform baseline\nand the anti-curriculum strategy. Our method also reduces the sharpness of the\nnetwork and illustrates a better 5-shot accuracy on MMLU benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2310.15389v1.pdf"
    },
    {
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "authors": [
            "Gabriele Prato",
            "Jerry Huang",
            "Prasannna Parthasarathi",
            "Shagun Sodhani",
            "Sarath Chandar"
        ],
        "published": "2023-10-23T21:15:54Z",
        "summary": "In the age of artificial intelligence, the role of large language models\n(LLMs) is becoming increasingly central. Despite their growing prevalence,\ntheir capacity to consolidate knowledge from different training documents - a\ncrucial ability in numerous applications - remains unexplored. This paper\npresents the first study examining the capability of LLMs to effectively\ncombine such information within their parameter space. We introduce EpiK-Eval,\na novel question-answering benchmark tailored to evaluate LLMs' proficiency in\nformulating a coherent and consistent knowledge representation from segmented\nnarratives. Evaluations across various LLMs reveal significant weaknesses in\nthis domain. We contend that these shortcomings stem from the intrinsic nature\nof prevailing training objectives. Consequently, we advocate for refining the\napproach towards knowledge consolidation, as it harbors the potential to\ndramatically improve their overall effectiveness and performance. The findings\nfrom this study offer insights for developing more robust and reliable LLMs.\nOur code and benchmark are available at\nhttps://github.com/chandar-lab/EpiK-Eval",
        "pdf_link": "https://arxiv.org/pdf/2310.15372v2.pdf"
    },
    {
        "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
        "authors": [
            "Adam Bouyamourn"
        ],
        "published": "2023-10-23T20:35:52Z",
        "summary": "We show that LLMs hallucinate because their output is not constrained to be\nsynonymous with claims for which they have evidence: a condition that we call\nevidential closure. Information about the truth or falsity of sentences is not\nstatistically identified in the standard neural probabilistic language model\nsetup, and so cannot be conditioned on to generate new strings. We then show\nhow to constrain LLMs to produce output that does satisfy evidential closure. A\nmultimodal LLM must learn about the external world (perceptual learning); it\nmust learn a mapping from strings to states of the world (extensional\nlearning); and, to achieve fluency when generalizing beyond a body of evidence,\nit must learn mappings from strings to their synonyms (intensional learning).\nThe output of a unimodal LLM must be synonymous with strings in a validated\nevidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,\nthat yields faithful output from an LLM by rejecting output that is not\nsynonymous with claims for which the LLM has evidence.",
        "pdf_link": "https://arxiv.org/pdf/2310.15355v1.pdf"
    },
    {
        "title": "Moral Foundations of Large Language Models",
        "authors": [
            "Marwa Abdulhai",
            "Gregory Serapio-Garcia",
            "Cl√©ment Crepy",
            "Daria Valter",
            "John Canny",
            "Natasha Jaques"
        ],
        "published": "2023-10-23T20:05:37Z",
        "summary": "Moral foundations theory (MFT) is a psychological assessment tool that\ndecomposes human moral reasoning into five factors, including care/harm,\nliberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary\nin the weight they place on these dimensions when making moral decisions, in\npart due to their cultural upbringing and political ideology. As large language\nmodels (LLMs) are trained on datasets collected from the internet, they may\nreflect the biases that are present in such corpora. This paper uses MFT as a\nlens to analyze whether popular LLMs have acquired a bias towards a particular\nset of moral values. We analyze known LLMs and find they exhibit particular\nmoral foundations, and show how these relate to human moral foundations and\npolitical affiliations. We also measure the consistency of these biases, or\nwhether they vary strongly depending on the context of how the model is\nprompted. Finally, we show that we can adversarially select prompts that\nencourage the moral to exhibit a particular set of moral foundations, and that\nthis can affect the model's behavior on downstream tasks. These findings help\nillustrate the potential risks and unintended consequences of LLMs assuming a\nparticular moral stance.",
        "pdf_link": "https://arxiv.org/pdf/2310.15337v1.pdf"
    },
    {
        "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
        "authors": [
            "Soumya Suvra Ghosal",
            "Souradip Chakraborty",
            "Jonas Geiping",
            "Furong Huang",
            "Dinesh Manocha",
            "Amrit Singh Bedi"
        ],
        "published": "2023-10-23T18:11:32Z",
        "summary": "Large Language Models (LLMs) have revolutionized the domain of natural\nlanguage processing (NLP) with remarkable capabilities of generating human-like\ntext responses. However, despite these advancements, several works in the\nexisting literature have raised serious concerns about the potential misuse of\nLLMs such as spreading misinformation, generating fake news, plagiarism in\nacademia, and contaminating the web. To address these concerns, a consensus\namong the research community is to develop algorithmic solutions to detect\nAI-generated text. The basic idea is that whenever we can tell if the given\ntext is either written by a human or an AI, we can utilize this information to\naddress the above-mentioned concerns. To that end, a plethora of detection\nframeworks have been proposed, highlighting the possibilities of AI-generated\ntext detection. But in parallel to the development of detection frameworks,\nresearchers have also concentrated on designing strategies to elude detection,\ni.e., focusing on the impossibilities of AI-generated text detection. This is a\ncrucial step in order to make sure the detection frameworks are robust enough\nand it is not too easy to fool a detector. Despite the huge interest and the\nflurry of research in this domain, the community currently lacks a\ncomprehensive analysis of recent developments. In this survey, we aim to\nprovide a concise categorization and overview of current work encompassing both\nthe prospects and the limitations of AI-generated text detection. To enrich the\ncollective knowledge, we engage in an exhaustive discussion on critical and\nchallenging open questions related to ongoing research on AI-generated text\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2310.15264v1.pdf"
    },
    {
        "title": "Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number",
        "authors": [
            "Sophie Hao",
            "Tal Linzen"
        ],
        "published": "2023-10-23T17:53:47Z",
        "summary": "Deep architectures such as Transformers are sometimes criticized for having\nuninterpretable \"black-box\" representations. We use causal intervention\nanalysis to show that, in fact, some linguistic features are represented in a\nlinear, interpretable format. Specifically, we show that BERT's ability to\nconjugate verbs relies on a linear encoding of subject number that can be\nmanipulated with predictable effects on conjugation accuracy. This encoding is\nfound in the subject position at the first layer and the verb position at the\nlast layer, but distributed across positions at middle layers, particularly\nwhen there are multiple cues to subject number.",
        "pdf_link": "https://arxiv.org/pdf/2310.15151v1.pdf"
    },
    {
        "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",
        "authors": [
            "Fangyu Lei",
            "Qian Liu",
            "Yiming Huang",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-23T17:52:06Z",
        "summary": "The rapid development of Large Language Models (LLMs) has led to great\nstrides in model capabilities like long-context understanding and reasoning.\nHowever, as LLMs are able to process longer contexts, it becomes more\nchallenging to evaluate whether they have acquired certain capabilities, since\nthe length of text (e.g., 200K tokens) they can process far exceeds what humans\ncan reliably assess in a reasonable duration. In this paper, we propose using\ncomplex synthetic tasks as a proxy evaluation method, and present S3Eval, a\nSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The\nsynthetic nature of S3Eval provides users full control over the dataset,\nallowing them to systematically probe LLM capabilities by scaling text length\nand varying task difficulty across diverse scenarios. The strong correlation\nbetween S3Eval and real-world benchmarks demonstrates the soundness of using\nS3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite\nlong-context data generation method. We have generated a comprehensive dataset\ncalled S3Eval-Standard, and experimental results have shown that it poses\nsignificant challenges for all existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.15147v2.pdf"
    },
    {
        "title": "Causal Inference Using LLM-Guided Discovery",
        "authors": [
            "Aniket Vashishtha",
            "Abbavaram Gowtham Reddy",
            "Abhinav Kumar",
            "Saketh Bachu",
            "Vineeth N Balasubramanian",
            "Amit Sharma"
        ],
        "published": "2023-10-23T17:23:56Z",
        "summary": "At the core of causal inference lies the challenge of determining reliable\ncausal graphs solely based on observational data. Since the well-known backdoor\ncriterion depends on the graph, any errors in the graph can propagate\ndownstream to effect inference. In this work, we initially show that complete\ngraph information is not necessary for causal effect inference; the topological\norder over graph variables (causal order) alone suffices. Further, given a node\npair, causal order is easier to elicit from domain experts compared to graph\nedges since determining the existence of an edge can depend extensively on\nother variables. Interestingly, we find that the same principle holds for Large\nLanguage Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated\nmethod to obtain causal order (and hence causal effect) with LLMs acting as\nvirtual domain experts. To this end, we employ different prompting strategies\nand contextual cues to propose a robust technique of obtaining causal order\nfrom LLMs. Acknowledging LLMs' limitations, we also study possible techniques\nto integrate LLMs with established causal discovery algorithms, including\nconstraint-based and score-based methods, to enhance their performance.\nExtensive experiments demonstrate that our approach significantly improves\ncausal ordering accuracy as compared to discovery algorithms, highlighting the\npotential of LLMs to enhance causal inference across diverse fields.",
        "pdf_link": "https://arxiv.org/pdf/2310.15117v1.pdf"
    },
    {
        "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
        "authors": [
            "Tianshi Che",
            "Ji Liu",
            "Yang Zhou",
            "Jiaxiang Ren",
            "Jiwen Zhou",
            "Victor S. Sheng",
            "Huaiyu Dai",
            "Dejing Dou"
        ],
        "published": "2023-10-23T16:37:59Z",
        "summary": "Federated learning (FL) is a promising paradigm to enable collaborative model\ntraining with decentralized data. However, the training process of Large\nLanguage Models (LLMs) generally incurs the update of significant parameters,\nwhich limits the applicability of FL techniques to tackle the LLMs in real\nscenarios. Prompt tuning can significantly reduce the number of parameters to\nupdate, but it either incurs performance degradation or low training\nefficiency. The straightforward utilization of prompt tuning in the FL often\nraises non-trivial communication costs and dramatically degrades performance.\nIn addition, the decentralized data is generally non-Independent and\nIdentically Distributed (non-IID), which brings client drift problems and thus\npoor performance. This paper proposes a Parameter-efficient prompt Tuning\napproach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and\neffective FL of LLMs. First, an efficient partial prompt tuning approach is\nproposed to improve performance and efficiency simultaneously. Second, a novel\nadaptive optimization method is developed to address the client drift problems\non both the device and server sides to enhance performance further. Extensive\nexperiments based on 10 datasets demonstrate the superb performance (up to\n60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training\ntime) of FedPepTAO compared with 9 baseline approaches. Our code is available\nat https://github.com/llm-eff/FedPepTAO.",
        "pdf_link": "https://arxiv.org/pdf/2310.15080v3.pdf"
    },
    {
        "title": "Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models",
        "authors": [
            "Matthieu Meeus",
            "Shubham Jain",
            "Marek Rei",
            "Yves-Alexandre de Montjoye"
        ],
        "published": "2023-10-23T15:00:46Z",
        "summary": "With large language models (LLMs) poised to become embedded in our daily\nlives, questions are starting to be raised about the dataset(s) they learned\nfrom. These questions range from potential bias or misinformation LLMs could\nretain from their training data to questions of copyright and fair use of\nhuman-generated text. However, while these questions emerge, developers of the\nrecent state-of-the-art LLMs become increasingly reluctant to disclose details\non their training corpus. We here introduce the task of document-level\nmembership inference for real-world LLMs, i.e. inferring whether the LLM has\nseen a given document during training or not. First, we propose a procedure for\nthe development and evaluation of document-level membership inference for LLMs\nby leveraging commonly used data sources for training and the model release\ndate. We then propose a practical, black-box method to predict document-level\nmembership and instantiate it on OpenLLaMA-7B with both books and academic\npapers. We show our methodology to perform very well, reaching an impressive\nAUC of 0.856 for books and 0.678 for papers. We then show our approach to\noutperform the sentence-level membership inference attacks used in the privacy\nliterature for the document-level membership task. We finally evaluate whether\nsmaller models might be less sensitive to document-level inference and show\nOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.\nTaken together, our results show that accurate document-level membership can be\ninferred for LLMs, increasing the transparency of technology poised to change\nour lives.",
        "pdf_link": "https://arxiv.org/pdf/2310.15007v1.pdf"
    },
    {
        "title": "Towards LLM-driven Dialogue State Tracking",
        "authors": [
            "Yujie Feng",
            "Zexin Lu",
            "Bo Liu",
            "Liming Zhan",
            "Xiao-Ming Wu"
        ],
        "published": "2023-10-23T14:15:28Z",
        "summary": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate\ntracking of user goals and system actions within task-oriented dialogue\nsystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT\nhas sparked considerable interest in assessing their efficacy across diverse\napplications. In this study, we conduct an initial examination of ChatGPT's\ncapabilities in DST. Our evaluation uncovers the exceptional performance of\nChatGPT in this task, offering valuable insights to researchers regarding its\ncapabilities and providing useful directions for designing and enhancing\ndialogue systems. Despite its impressive performance, ChatGPT has significant\nlimitations including its closed-source nature, request restrictions, raising\ndata privacy concerns, and lacking local deployment capabilities. To address\nthese concerns, we present LDST, an LLM-driven DST framework based on smaller,\nopen-source foundation models. By utilizing a novel domain-slot instruction\ntuning method, LDST achieves performance on par with ChatGPT. Comprehensive\nevaluations across three distinct experimental settings, we find that LDST\nexhibits remarkable performance improvements in both zero-shot and few-shot\nsetting compared to previous SOTA methods. The source code is provided for\nreproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.14970v1.pdf"
    },
    {
        "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
        "authors": [
            "Mengyu Ye",
            "Tatsuki Kuribayashi",
            "Jun Suzuki",
            "Goro Kobayashi",
            "Hiroaki Funayama"
        ],
        "published": "2023-10-23T12:40:41Z",
        "summary": "Large language models (LLMs) take advantage of step-by-step reasoning\ninstructions, e.g., chain-of-thought (CoT) prompting. Building on this, their\nability to perform CoT-style reasoning robustly is of interest from a probing\nperspective. In this study, we inspect the step-by-step reasoning ability of\nLLMs with a focus on negation, which is a core linguistic phenomenon that is\ndifficult to process. In particular, we introduce several controlled settings\n(e.g., reasoning in case of fictional entities) to evaluate the logical\nreasoning abilities of the models. We observed that dozens of modern LLMs were\nnot robust against lexical negation (e.g., plausible ->implausible) when\nperforming CoT-style reasoning, and the results highlight unique limitations in\neach LLM family.",
        "pdf_link": "https://arxiv.org/pdf/2310.14868v1.pdf"
    },
    {
        "title": "Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing",
        "authors": [
            "Sai Koneru",
            "Miriam Exel",
            "Matthias Huck",
            "Jan Niehues"
        ],
        "published": "2023-10-23T12:22:15Z",
        "summary": "Large Language Models (LLM's) have demonstrated considerable success in\nvarious Natural Language Processing tasks, but they have yet to attain\nstate-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,\ntheir significant performance in tasks demanding a broad understanding and\ncontextual processing shows their potential for translation. To exploit these\nabilities, we investigate using LLM's for MT and explore recent\nparameter-efficient fine-tuning techniques. Surprisingly, our initial\nexperiments find that fine-tuning for translation purposes even led to\nperformance degradation. To overcome this, we propose an alternative approach:\nadapting LLM's as Automatic Post-Editors (APE) rather than direct translators.\nBuilding on the LLM's exceptional ability to process and generate lengthy\nsequences, we also propose extending our approach to document-level\ntranslation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can\nyield significant improvements across both sentence and document-level metrics\nwhile generalizing to out-of-domain data. Most notably, we achieve a\nstate-of-the-art accuracy rate of 89\\% on the ContraPro test set, which\nspecifically assesses the model's ability to resolve pronoun ambiguities when\ntranslating from English to German. Lastly, we investigate a practical scenario\ninvolving manual post-editing for document-level translation, where reference\ncontext is made available. Here, we demonstrate that leveraging human\ncorrections can significantly reduce the number of edits required for\nsubsequent translations (Interactive Demo for integrating manual feedback can\nbe found here:\nhttps://huggingface.co/spaces/skoneru/contextual_refinement_ende).",
        "pdf_link": "https://arxiv.org/pdf/2310.14855v2.pdf"
    },
    {
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": [
            "Xunjian Yin",
            "Baizhou Huang",
            "Xiaojun Wan"
        ],
        "published": "2023-10-23T11:40:05Z",
        "summary": "With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2310.14820v1.pdf"
    },
    {
        "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
        "authors": [
            "Sabri Boughorbel",
            "Majd Hawasly"
        ],
        "published": "2023-10-23T11:40:04Z",
        "summary": "While significant progress has been made in benchmarking Large Language\nModels (LLMs) across various tasks, there is a lack of comprehensive evaluation\nof their abilities in responding to multi-turn instructions in less-commonly\ntested languages like Arabic. Our paper offers a detailed examination of the\nproficiency of open LLMs in such scenarios in Arabic. Utilizing a customized\nArabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a\nuniform evaluator for both English and Arabic queries to assess and compare the\nperformance of the LLMs on various open-ended tasks. Our findings reveal\nvariations in model responses on different task categories, e.g., logic vs.\nliteracy, when instructed in English or Arabic. We find that fine-tuned base\nmodels using multilingual and multi-turn datasets could be competitive to\nmodels trained from scratch on multilingual data. Finally, we hypothesize that\nan ensemble of small, open LLMs could perform competitively to proprietary LLMs\non the benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.14819v1.pdf"
    },
    {
        "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
        "authors": [
            "Libo Qin",
            "Qiguang Chen",
            "Fuxuan Wei",
            "Shijue Huang",
            "Wanxiang Che"
        ],
        "published": "2023-10-23T10:56:03Z",
        "summary": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate\nreasoning paths, thus promoting reasoning accuracy and attracting increasing\nattention. Specifically, zero-shot CoT achieves remarkable improvements in a\nwide range of reasoning tasks by simply instructing the LLM with the prompt\n\"Let's think step by step!\". Despite the success of zero-shot CoT, the existing\nzero-shot prompting techniques remain limited to a single language, making it\nchallenging to generalize to other languages and hindering global development.\nIn this work, we introduce cross-lingual prompting (CLP), aiming to improve\nzero-shot CoT reasoning across languages. Specifically, CLP consists of two\nmain components: (1) cross-lingual alignment prompting and (2) task-specific\nsolver prompting. The cross-lingual alignment prompting is responsible for\naligning representations across different languages, whereas the task-specific\nsolver prompting is used to generate the final chain of thoughts and results\nfor the reasoning task. In addition, we further introduce cross-lingual\nself-consistent prompting (CLSP) to ensemble different reasoning paths across\nlanguages. Our experimental evaluations on several benchmarks demonstrate that\nCLP and CLSP significantly outperform the existing prompting methods and\nachieve state-of-the-art performance. We hope this work will inspire further\nbreakthroughs in cross-lingual CoT.",
        "pdf_link": "https://arxiv.org/pdf/2310.14799v1.pdf"
    },
    {
        "title": "Evaluating the Knowledge Base Completion Potential of GPT",
        "authors": [
            "Blerta Veseli",
            "Simon Razniewski",
            "Jan-Christoph Kalo",
            "Gerhard Weikum"
        ],
        "published": "2023-10-23T10:15:13Z",
        "summary": "Structured knowledge bases (KBs) are an asset for search engines and other\napplications, but are inevitably incomplete. Language models (LMs) have been\nproposed for unsupervised knowledge base completion (KBC), yet, their ability\nto do this at scale and with high accuracy remains an open question. Prior\nexperimental studies mostly fall short because they only evaluate on popular\nsubjects, or sample already existing facts from KBs. In this work, we perform a\ncareful evaluation of GPT's potential to complete the largest public KB:\nWikidata. We find that, despite their size and capabilities, models like GPT-3,\nChatGPT and GPT-4 do not achieve fully convincing results on this task.\nNonetheless, they provide solid improvements over earlier approaches with\nsmaller LMs. In particular, we show that, with proper thresholding, GPT-3\nenables to extend Wikidata by 27M facts at 90% precision.",
        "pdf_link": "https://arxiv.org/pdf/2310.14771v1.pdf"
    },
    {
        "title": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",
        "authors": [
            "Junchao Wu",
            "Shu Yang",
            "Runzhe Zhan",
            "Yulin Yuan",
            "Derek F. Wong",
            "Lidia S. Chao"
        ],
        "published": "2023-10-23T09:01:13Z",
        "summary": "The powerful ability to understand, follow, and generate complex language\nemerging from large language models (LLMs) makes LLM-generated text flood many\nareas of our daily lives at an incredible speed and is widely accepted by\nhumans. As LLMs continue to expand, there is an imperative need to develop\ndetectors that can detect LLM-generated text. This is crucial to mitigate\npotential misuse of LLMs and safeguard realms like artistic expression and\nsocial networks from harmful influence of LLM-generated content. The\nLLM-generated text detection aims to discern if a piece of text was produced by\nan LLM, which is essentially a binary classification task. The detector\ntechniques have witnessed notable advancements recently, propelled by\ninnovations in watermarking techniques, zero-shot methods, fine-turning LMs\nmethods, adversarial learning methods, LLMs as detectors, and human-assisted\nmethods. In this survey, we collate recent research breakthroughs in this area\nand underscore the pressing need to bolster detector research. We also delve\ninto prevalent datasets, elucidating their limitations and developmental\nrequirements. Furthermore, we analyze various LLM-generated text detection\nparadigms, shedding light on challenges like out-of-distribution problems,\npotential attacks, and data ambiguity. Conclusively, we highlight interesting\ndirections for future research in LLM-generated text detection to advance the\nimplementation of responsible artificial intelligence (AI). Our aim with this\nsurvey is to provide a clear and comprehensive introduction for newcomers while\nalso offering seasoned researchers a valuable update in the field of\nLLM-generated text detection. The useful resources are publicly available at:\nhttps://github.com/NLP2CT/LLM-generated-Text-Detection.",
        "pdf_link": "https://arxiv.org/pdf/2310.14724v2.pdf"
    },
    {
        "title": "Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models",
        "authors": [
            "Gonzalo Mart√≠nez",
            "Javier Conde",
            "Elena Merino-G√≥mez",
            "Beatriz Berm√∫dez-Margaretto",
            "Jos√© Alberto Hern√°ndez",
            "Pedro Reviriego",
            "Marc Brysbaert"
        ],
        "published": "2023-10-23T08:45:12Z",
        "summary": "Vocabulary tests, once a cornerstone of language modeling evaluation, have\nbeen largely overlooked in the current landscape of Large Language Models\n(LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus\non specific tasks or domain-specific knowledge, they often neglect the\nfundamental linguistic aspects of language understanding and production. In\nthis paper, we advocate for the revival of vocabulary tests as a valuable tool\nfor assessing LLM performance. We evaluate seven LLMs using two vocabulary test\nformats across two languages and uncover surprising gaps in their lexical\nknowledge. These findings shed light on the intricacies of LLM word\nrepresentations, their learning mechanisms, and performance variations across\nmodels and languages. Moreover, the ability to automatically generate and\nperform vocabulary tests offers new opportunities to expand the approach and\nprovide a more complete picture of LLMs' language skills.",
        "pdf_link": "https://arxiv.org/pdf/2310.14703v2.pdf"
    },
    {
        "title": "Reasoning about Ambiguous Definite Descriptions",
        "authors": [
            "Stefan F. Schouten",
            "Peter Bloem",
            "Ilia Markov",
            "Piek Vossen"
        ],
        "published": "2023-10-23T07:52:38Z",
        "summary": "Natural language reasoning plays an increasingly important role in improving\nlanguage models' ability to solve complex language understanding tasks. An\ninteresting use case for reasoning is the resolution of context-dependent\nambiguity. But no resources exist to evaluate how well Large Language Models\ncan use explicit reasoning to resolve ambiguity in language. We propose to use\nambiguous definite descriptions for this purpose and create and publish the\nfirst benchmark dataset consisting of such phrases. Our method includes all\ninformation required to resolve the ambiguity in the prompt, which means a\nmodel does not require anything but reasoning to do well. We find this to be a\nchallenging task for recent LLMs. Code and data available at:\nhttps://github.com/sfschouten/exploiting-ambiguity",
        "pdf_link": "https://arxiv.org/pdf/2310.14657v1.pdf"
    },
    {
        "title": "Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue",
        "authors": [
            "Yuanxing Liu",
            "Wei-Nan Zhang",
            "Yifan Chen",
            "Yuchi Zhang",
            "Haopeng Bai",
            "Fan Feng",
            "Hengbin Cui",
            "Yongbin Li",
            "Wanxiang Che"
        ],
        "published": "2023-10-23T07:00:51Z",
        "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
        "pdf_link": "https://arxiv.org/pdf/2310.14626v1.pdf"
    },
    {
        "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
        "authors": [
            "Yanchen Liu",
            "Srishti Gautam",
            "Jiaqi Ma",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-10-23T06:31:28Z",
        "summary": "Recent literature has suggested the potential of using large language models\n(LLMs) to make classifications for tabular tasks. However, LLMs have been shown\nto exhibit harmful social biases that reflect the stereotypes and inequalities\npresent in society. To this end, as well as the widespread use of tabular data\nin many high-stake applications, it is important to explore the following\nquestions: what sources of information do LLMs draw upon when making\nclassifications for tabular tasks; whether and to what extent are LLM\nclassifications for tabular data influenced by social biases and stereotypes;\nand what are the consequential implications for fairness?\n  Through a series of experiments, we delve into these questions and show that\nLLMs tend to inherit social biases from their training data which significantly\nimpact their fairness in tabular classification tasks. Furthermore, our\ninvestigations show that in the context of bias mitigation, though in-context\nlearning and finetuning have a moderate effect, the fairness metric gap between\ndifferent subgroups is still larger than that in traditional machine learning\nmodels, such as Random Forest and shallow Neural Networks. This observation\nemphasizes that the social biases are inherent within the LLMs themselves and\ninherited from their pretraining corpus, not only from the downstream task\ndatasets. Besides, we demonstrate that label-flipping of in-context examples\ncan significantly reduce biases, further highlighting the presence of inherent\nbias within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.14607v2.pdf"
    },
    {
        "title": "Large Search Model: Redefining Search Stack in the Era of LLMs",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023-10-23T05:52:09Z",
        "summary": "Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.14587v2.pdf"
    },
    {
        "title": "Language Models Hallucinate, but May Excel at Fact Verification",
        "authors": [
            "Jian Guan",
            "Jesse Dodge",
            "David Wadden",
            "Minlie Huang",
            "Hao Peng"
        ],
        "published": "2023-10-23T04:39:01Z",
        "summary": "Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.",
        "pdf_link": "https://arxiv.org/pdf/2310.14564v2.pdf"
    },
    {
        "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
        "authors": [
            "Chiyu Zhang",
            "Khai Duy Doan",
            "Qisheng Liao",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-10-23T04:22:44Z",
        "summary": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate\nremarkable performance in a wide range of tasks. Despite numerous recent\nstudies that examine the performance of instruction-tuned LLMs on various NLP\nbenchmarks, there remains a lack of comprehensive investigation into their\nability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning\nembedded within social and interactive contexts. This deficiency arises partly\nfrom SM not being adequately represented in any of the existing benchmarks. To\naddress this gap, we present SPARROW, an extensive multilingual benchmark\nspecifically designed for SM understanding. SPARROW comprises 169 datasets\ncovering 13 task types across six primary categories (e.g., anti-social\nlanguage detection, emotion recognition). SPARROW datasets encompass 64\ndifferent languages originating from 12 language families representing 16\nwriting scripts. We evaluate the performance of various multilingual pretrained\nlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)\non SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our\ncomprehensive analysis reveals that existing open-source instruction tuned LLMs\nstill struggle to understand SM across various languages, performing close to a\nrandom baseline in some cases. We also find that although ChatGPT outperforms\nmany LLMs, it still falls behind task-specific finetuned models with a gap of\n12.19 SPARROW score. Our benchmark is available at:\nhttps://github.com/UBC-NLP/SPARROW",
        "pdf_link": "https://arxiv.org/pdf/2310.14557v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on Controlled Generation Tasks",
        "authors": [
            "Jiao Sun",
            "Yufei Tian",
            "Wangchunshu Zhou",
            "Nan Xu",
            "Qian Hu",
            "Rahul Gupta",
            "John Frederick Wieting",
            "Nanyun Peng",
            "Xuezhe Ma"
        ],
        "published": "2023-10-23T03:48:24Z",
        "summary": "While recent studies have looked into the abilities of large language models\nin various benchmark tasks, including question generation, reading\ncomprehension, multilingual and etc, there have been few studies looking into\nthe controllability of large language models on generation tasks. We present an\nextensive analysis of various benchmarks including a sentence planning\nbenchmark with different granularities. After comparing large language models\nagainst state-of-the-start finetuned smaller models, we present a spectrum\nshowing large language models falling behind, are comparable, or exceed the\nability of smaller models. We conclude that **large language models struggle at\nmeeting fine-grained hard constraints**.",
        "pdf_link": "https://arxiv.org/pdf/2310.14542v1.pdf"
    },
    {
        "title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing",
        "authors": [
            "Yating Wu",
            "Ritika Mangla",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "published": "2023-10-23T03:03:58Z",
        "summary": "Questions Under Discussion (QUD) is a versatile linguistic framework in which\ndiscourse progresses as continuously asking questions and answering them.\nAutomatic parsing of a discourse to produce a QUD structure thus entails a\ncomplex question generation task: given a document and an answer sentence,\ngenerate a question that satisfies linguistic constraints of QUD and can be\ngrounded in an anchor sentence in prior context. These questions are known to\nbe curiosity-driven and open-ended. This work introduces the first framework\nfor the automatic evaluation of QUD parsing, instantiating the theoretical\nconstraints of QUD in a concrete protocol. We present QUDeval, a dataset of\nfine-grained evaluation of 2,190 QUD questions generated from both fine-tuned\nsystems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD\nis still challenging for modern LLMs, and that existing evaluation metrics\npoorly approximate parser quality. Encouragingly, human-authored QUDs are\nscored highly by our human evaluators, suggesting that there is headroom for\nfurther progress on language modeling to improve both QUD parsing and QUD\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.14520v2.pdf"
    },
    {
        "title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains",
        "authors": [
            "Vaibhav Mavi",
            "Abulhair Saparov",
            "Chen Zhao"
        ],
        "published": "2023-10-22T22:45:14Z",
        "summary": "Applying existing question answering (QA) systems to specialized domains like\nlaw and finance presents challenges that necessitate domain expertise. Although\nlarge language models (LLMs) have shown impressive language comprehension and\nin-context learning capabilities, their inability to handle very long\ninputs/contexts is well known. Tasks specific to these domains need significant\nbackground knowledge, leading to contexts that can often exceed the maximum\nlength that existing LLMs can process. This study explores leveraging the\nsemi-structured nature of legal and financial data to efficiently retrieve\nrelevant context, enabling the use of LLMs for domain-specialized QA. The\nresulting system outperforms contemporary models and also provides useful\nexplanations for the answers, encouraging the integration of LLMs into legal\nand financial NLP systems for future research.",
        "pdf_link": "https://arxiv.org/pdf/2310.14435v1.pdf"
    },
    {
        "title": "Large Language Models are biased to overestimate profoundness",
        "authors": [
            "Eugenio Herrera-Berg",
            "Tom√°s Vergara Browne",
            "Pablo Le√≥n-Villagr√°",
            "Marc-Llu√≠s Vives",
            "Cristian Buc Calderon"
        ],
        "published": "2023-10-22T21:33:50Z",
        "summary": "Recent advancements in natural language processing by large language models\n(LLMs), such as GPT-4, have been suggested to approach Artificial General\nIntelligence. And yet, it is still under dispute whether LLMs possess similar\nreasoning abilities to humans. This study evaluates GPT-4 and various other\nLLMs in judging the profoundness of mundane, motivational, and pseudo-profound\nstatements. We found a significant statement-to-statement correlation between\nthe LLMs and humans, irrespective of the type of statements and the prompting\ntechnique used. However, LLMs systematically overestimate the profoundness of\nnonsensical statements, with the exception of Tk-instruct, which uniquely\nunderestimates the profoundness of statements. Only few-shot learning prompts,\nas opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.\nFurthermore, this work provides insights into the potential biases induced by\nReinforcement Learning from Human Feedback (RLHF), inducing an increase in the\nbias to overestimate the profoundness of statements.",
        "pdf_link": "https://arxiv.org/pdf/2310.14422v1.pdf"
    },
    {
        "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
        "authors": [
            "Henry W. Sprueill",
            "Carl Edwards",
            "Mariefel V. Olarte",
            "Udishnu Sanyal",
            "Heng Ji",
            "Sutanay Choudhury"
        ],
        "published": "2023-10-22T21:29:33Z",
        "summary": "Discovering novel catalysts requires complex reasoning involving multiple\nchemical properties and resultant trade-offs, leading to a combinatorial growth\nin the search space. While large language models (LLM) have demonstrated novel\ncapabilities for chemistry through complex instruction following capabilities\nand high quality reasoning, a goal-driven combinatorial search using LLMs has\nnot been explored in detail. In this work, we present a Monte Carlo Tree\nSearch-based approach that improves beyond state-of-the-art chain-of-thought\nprompting variants to augment scientific reasoning. We introduce two new\nreasoning datasets: 1) a curation of computational chemistry simulations, and\n2) diverse questions written by catalysis researchers for reasoning about novel\nchemical conversion processes. We improve over the best baseline by 25.8\\% and\nfind that our approach can augment scientist's reasoning and discovery process\nwith novel insights.",
        "pdf_link": "https://arxiv.org/pdf/2310.14420v1.pdf"
    },
    {
        "title": "Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis",
        "authors": [
            "Inez Okulska",
            "Emilia Wi≈õnios"
        ],
        "published": "2023-10-22T15:19:04Z",
        "summary": "Adult content detection still poses a great challenge for automation.\nExisting classifiers primarily focus on distinguishing between erotic and\nnon-erotic texts. However, they often need more nuance in assessing the\npotential harm. Unfortunately, the content of this nature falls beyond the\nreach of generative models due to its potentially harmful nature. Ethical\nrestrictions prohibit large language models (LLMs) from analyzing and\nclassifying harmful erotics, let alone generating them to create synthetic\ndatasets for other neural models. In such instances where data is scarce and\nchallenging, a thorough analysis of the structure of such texts rather than a\nlarge model may offer a viable solution. Especially given that harmful erotic\nnarratives, despite appearing similar to harmless ones, usually reveal their\nharmful nature first through contextual information hidden in the non-sexual\nparts of the narrative.\n  This paper introduces a hybrid neural and rule-based context-aware system\nthat leverages coreference resolution to identify harmful contextual cues in\nerotic content. Collaborating with professional moderators, we compiled a\ndataset and developed a classifier capable of distinguishing harmful from\nnon-harmful erotic content. Our hybrid model, tested on Polish text,\ndemonstrates a promising accuracy of 84% and a recall of 80%. Models based on\nRoBERTa and Longformer without explicit usage of coreference chains achieved\nsignificantly weaker results, underscoring the importance of coreference\nresolution in detecting such nuanced content as harmful erotics. This approach\nalso offers the potential for enhanced visual explainability, supporting\nmoderators in evaluating predictions and taking necessary actions to address\nharmful content.",
        "pdf_link": "https://arxiv.org/pdf/2310.14325v1.pdf"
    },
    {
        "title": "Chainpoll: A high efficacy method for LLM hallucination detection",
        "authors": [
            "Robert Friel",
            "Atindriyo Sanyal"
        ],
        "published": "2023-10-22T14:45:14Z",
        "summary": "Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.18344v1.pdf"
    },
    {
        "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases",
        "authors": [
            "Rishabh Bhardwaj",
            "Soujanya Poria"
        ],
        "published": "2023-10-22T13:55:46Z",
        "summary": "Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.",
        "pdf_link": "https://arxiv.org/pdf/2310.14303v2.pdf"
    },
    {
        "title": "NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval",
        "authors": [
            "Uri Katz",
            "Matan Vetzler",
            "Amir DN Cohen",
            "Yoav Goldberg"
        ],
        "published": "2023-10-22T12:23:00Z",
        "summary": "Recognizing entities in texts is a central need in many information-seeking\nscenarios, and indeed, Named Entity Recognition (NER) is arguably one of the\nmost successful examples of a widely adopted NLP task and corresponding NLP\ntechnology. Recent advances in large language models (LLMs) appear to provide\neffective solutions (also) for NER tasks that were traditionally handled with\ndedicated models, often matching or surpassing the abilities of the dedicated\nmodels. Should NER be considered a solved problem? We argue to the contrary:\nthe capabilities provided by LLMs are not the end of NER research, but rather\nan exciting beginning. They allow taking NER to the next level, tackling\nincreasingly more useful, and increasingly more challenging, variants. We\npresent three variants of the NER task, together with a dataset to support\nthem. The first is a move towards more fine-grained -- and intersectional --\nentity types. The second is a move towards zero-shot recognition and extraction\nof these fine-grained types based on entity-type labels. The third, and most\nchallenging, is the move from the recognition setup to a novel retrieval setup,\nwhere the query is a zero-shot entity type, and the expected result is all the\nsentences from a large, pre-indexed corpus that contain entities of these\ntypes, and their corresponding spans. We show that all of these are far from\nbeing solved. We provide a large, silver-annotated corpus of 4 million\nparagraphs covering 500 entity types, to facilitate research towards all of\nthese three goals.",
        "pdf_link": "https://arxiv.org/pdf/2310.14282v1.pdf"
    },
    {
        "title": "From Static to Dynamic: A Continual Learning Framework for Large Language Models",
        "authors": [
            "Mingzhe Du",
            "Anh Tuan Luu",
            "Bin Ji",
            "See-kiong Ng"
        ],
        "published": "2023-10-22T10:18:53Z",
        "summary": "The vast number of parameters in large language models (LLMs) endows them\nwith remarkable capabilities, allowing them to excel in a variety of natural\nlanguage processing tasks. However, this complexity also presents challenges,\nmaking LLMs difficult to train and inhibiting their ability to continuously\nassimilate new knowledge, which may lead to inaccuracies in their outputs. To\nmitigate these issues, this paper presents DynaMind, a novel continual learning\nframework designed for LLMs. DynaMind incorporates memory mechanisms to\nassimilate new knowledge and modular operators to enhance the model inference\nprocess with the newly assimilated knowledge, consequently improving the\naccuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's\neffectiveness in overcoming these challenges. The code and demo of DynaMind are\navailable on GitHub: https://github.com/Elfsong/DynaMind.",
        "pdf_link": "https://arxiv.org/pdf/2310.14248v1.pdf"
    },
    {
        "title": "CXR-LLAVA: a multimodal large language model for interpreting chest X-ray images",
        "authors": [
            "Seowoo Lee",
            "Jiwon Youn",
            "Hyungjin Kim",
            "Mansu Kim",
            "Soon Ho Yoon"
        ],
        "published": "2023-10-22T06:22:37Z",
        "summary": "Purpose: This study aimed to develop an open-source multimodal large language\nmodel (CXR-LLAVA) for interpreting chest X-ray images (CXRs), leveraging recent\nadvances in large language models (LLMs) to potentially replicate the image\ninterpretation skills of human radiologists Materials and Methods: For\ntraining, we collected 592,580 publicly available CXRs, of which 374,881 had\nlabels for certain radiographic abnormalities (Dataset 1) and 217,699 provided\nfree-text radiology reports (Dataset 2). After pre-training a vision\ntransformer with Dataset 1, we integrated it with an LLM influenced by the\nLLAVA network. Then, the model was fine-tuned, primarily using Dataset 2. The\nmodel's diagnostic performance for major pathological findings was evaluated,\nalong with the acceptability of radiologic reports by human radiologists, to\ngauge its potential for autonomous reporting. Results: The model demonstrated\nimpressive performance in test sets, achieving an average F1 score of 0.81 for\nsix major pathological findings in the MIMIC internal test set and 0.62 for\nseven major pathological findings in the external test set. The model's F1\nscores surpassed those of GPT-4-vision and Gemini-Pro-Vision in both test sets.\nIn human radiologist evaluations of the external test set, the model achieved a\n72.7% success rate in autonomous reporting, slightly below the 84.0% rate of\nground truth reports. Conclusion: This study highlights the significant\npotential of multimodal LLMs for CXR interpretation, while also acknowledging\nthe performance limitations. Despite these challenges, we believe that making\nour model open-source will catalyze further research, expanding its\neffectiveness and applicability in various clinical contexts. CXR-LLAVA is\navailable at https://github.com/ECOFRI/CXR_LLAVA.",
        "pdf_link": "https://arxiv.org/pdf/2310.18341v3.pdf"
    },
    {
        "title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain",
        "authors": [
            "Wei Zhu",
            "Xiaoling Wang",
            "Huanran Zheng",
            "Mosha Chen",
            "Buzhou Tang"
        ],
        "published": "2023-10-22T02:20:38Z",
        "summary": "Biomedical language understanding benchmarks are the driving forces for\nartificial intelligence applications with large language model (LLM) back-ends.\nHowever, most current benchmarks: (a) are limited to English which makes it\nchallenging to replicate many of the successes in English for other languages,\nor (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs\napply these knowledge to perform on a wide range of bio-medical tasks, or (c)\nhave become a publicly available corpus and are leaked to LLMs during\npre-training. To facilitate the research in medical LLMs, we re-build the\nChinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a\nlarge scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable\ntest-bed and an online platform for evaluating Chinese LLMs' multi-task\ncapabilities on a wide range bio-medical tasks including medical entity\nrecognition, medical text classification, medical natural language inference,\nmedical dialogue understanding and medical content/dialogue generation. To\nestablish evaluation on these tasks, we have experimented and report the\nresults with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2310.14151v1.pdf"
    },
    {
        "title": "Learning Reward for Physical Skills using Large Language Model",
        "authors": [
            "Yuwei Zeng",
            "Yiqing Xu"
        ],
        "published": "2023-10-21T19:10:06Z",
        "summary": "Learning reward functions for physical skills are challenging due to the vast\nspectrum of skills, the high-dimensionality of state and action space, and\nnuanced sensory feedback. The complexity of these tasks makes acquiring expert\ndemonstration data both costly and time-consuming. Large Language Models (LLMs)\ncontain valuable task-related knowledge that can aid in learning these reward\nfunctions. However, the direct application of LLMs for proposing reward\nfunctions has its limitations such as numerical instability and inability to\nincorporate the environment feedback. We aim to extract task knowledge from\nLLMs using environment feedback to create efficient reward functions for\nphysical skills. Our approach consists of two components. We first use the LLM\nto propose features and parameterization of the reward function. Next, we\nupdate the parameters of this proposed reward function through an iterative\nself-alignment process. In particular, this process minimizes the ranking\ninconsistency between the LLM and our learned reward functions based on the new\nobservations. We validated our method by testing it on three simulated physical\nskill learning tasks, demonstrating effective support for our design choices.",
        "pdf_link": "https://arxiv.org/pdf/2310.14092v1.pdf"
    },
    {
        "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications",
        "authors": [
            "Qidong Liu",
            "Xian Wu",
            "Xiangyu Zhao",
            "Yuanshao Zhu",
            "Derong Xu",
            "Feng Tian",
            "Yefeng Zheng"
        ],
        "published": "2023-10-21T17:18:09Z",
        "summary": "The recent surge in the field of Large Language Models (LLMs) has gained\nsignificant attention in numerous domains. In order to tailor an LLM to a\nspecific domain such as a web-based healthcare system, fine-tuning with domain\nknowledge is necessary. However, two issues arise during fine-tuning LLMs for\nmedical applications. The first is the problem of task variety, where there are\nnumerous distinct tasks in real-world medical scenarios. This diversity often\nresults in suboptimal fine-tuning due to data imbalance and seesawing problems.\nAdditionally, the high cost of fine-tuning can be prohibitive, impeding the\napplication of LLMs. The large number of parameters in LLMs results in enormous\ntime and computational consumption during fine-tuning, which is difficult to\njustify. To address these two issues simultaneously, we propose a novel\nparameter-efficient fine-tuning framework for multi-task medical applications\ncalled MOELoRA. The framework aims to capitalize on the benefits of both MOE\nfor multi-task learning and LoRA for parameter-efficient fine-tuning. To unify\nMOE and LoRA, we devise multiple experts as the trainable parameters, where\neach expert consists of a pair of low-rank matrices to maintain a small number\nof trainable parameters. Additionally, we propose a task-motivated gate\nfunction for all MOELoRA layers that can regulate the contributions of each\nexpert and generate distinct parameters for various tasks. To validate the\neffectiveness and practicality of the proposed method, we conducted\ncomprehensive experiments on a public multi-task Chinese medical dataset. The\nexperimental results demonstrate that MOELoRA outperforms existing\nparameter-efficient fine-tuning methods. The implementation is available online\nfor convenient reproduction of our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2310.18339v1.pdf"
    },
    {
        "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
        "authors": [
            "Marcus J. Min",
            "Yangruibo Ding",
            "Luca Buratti",
            "Saurabh Pujar",
            "Gail Kaiser",
            "Suman Jana",
            "Baishakhi Ray"
        ],
        "published": "2023-10-21T16:14:56Z",
        "summary": "Code Large Language Models (Code LLMs) are being increasingly employed in\nreal-life applications, so evaluating them is critical. While the conventional\naccuracy evaluates the performance of Code LLMs on a set of individual tasks,\ntheir self-consistency across different tasks is overlooked. Intuitively, a\ntrustworthy model should be self-consistent when generating natural language\nspecifications for its own code and generating code for its own specifications.\nFailure to preserve self-consistency reveals a lack of understanding of the\nshared semantics underlying natural language and programming language, and\ntherefore undermines the trustworthiness of a model. In this paper, we first\nformally define the self-consistency of Code LLMs and then design a framework,\nIdentityChain, which effectively and efficiently evaluates the self-consistency\nand conventional accuracy of a model at the same time. We study eleven Code\nLLMs and show that they fail to preserve self-consistency, which is indeed a\ndistinct aspect from conventional accuracy. Furthermore, we show that\nIdentityChain can be used as a model debugging tool to expose weaknesses of\nCode LLMs by demonstrating three major weaknesses that we identify in current\nmodels using IdentityChain. Our code is available at\nhttps://github.com/marcusm117/IdentityChain.",
        "pdf_link": "https://arxiv.org/pdf/2310.14053v3.pdf"
    },
    {
        "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions",
        "authors": [
            "Andre Niyongabo Rubungo",
            "Craig Arnold",
            "Barry P. Rand",
            "Adji Bousso Dieng"
        ],
        "published": "2023-10-21T14:49:58Z",
        "summary": "The prediction of crystal properties plays a crucial role in the crystal\ndesign process. Current methods for predicting crystal properties focus on\nmodeling crystal structures using graph neural networks (GNNs). Although GNNs\nare powerful, accurately modeling the complex interactions between atoms and\nmolecules within a crystal remains a challenge. Surprisingly, predicting\ncrystal properties from crystal text descriptions is understudied, despite the\nrich information and expressiveness that text data offer. One of the main\nreasons is the lack of publicly available data for this task. In this paper, we\ndevelop and make public a benchmark dataset (called TextEdge) that contains\ntext descriptions of crystal structures with their properties. We then propose\nLLM-Prop, a method that leverages the general-purpose learning capabilities of\nlarge language models (LLMs) to predict the physical and electronic properties\nof crystals from their text descriptions. LLM-Prop outperforms the current\nstate-of-the-art GNN-based crystal property predictor by about 4% in predicting\nband gap, 3% in classifying whether the band gap is direct or indirect, and 66%\nin predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,\na domain-specific pre-trained BERT model, despite having 3 times fewer\nparameters. Our empirical results may highlight the current inability of GNNs\nto capture information pertaining to space group symmetry and Wyckoff sites for\naccurate crystal property prediction.",
        "pdf_link": "https://arxiv.org/pdf/2310.14029v1.pdf"
    },
    {
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": [
            "Yaoyiran Li",
            "Anna Korhonen",
            "Ivan Vuliƒá"
        ],
        "published": "2023-10-21T12:43:27Z",
        "summary": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that\nstill, to a large extent, relies on calculating cross-lingual word\nrepresentations. Inspired by the global paradigm shift in NLP towards Large\nLanguage Models (LLMs), we examine the potential of the latest generation of\nLLMs for the development of bilingual lexicons. We ask the following research\nquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for\nBLI, and how does this approach compare against and complement current BLI\napproaches? To this end, we systematically study 1) zero-shot prompting for\nunsupervised BLI and 2) few-shot in-context prompting with a set of seed\ntranslation pairs, both without any LLM fine-tuning, as well as 3) standard\nBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source\ntext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two\nstandard BLI benchmarks covering a range of typologically diverse languages.\nOur work is the first to demonstrate strong BLI capabilities of text-to-text\nmLLMs. The results reveal that few-shot prompting with in-context examples from\nnearest neighbours achieves the best performance, establishing new\nstate-of-the-art BLI scores for many language pairs. We also conduct a series\nof in-depth analyses and ablation studies, providing more insights on BLI with\n(m)LLMs, also along with their limitations.",
        "pdf_link": "https://arxiv.org/pdf/2310.13995v2.pdf"
    },
    {
        "title": "GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4",
        "authors": [
            "Tom Kocmi",
            "Christian Federmann"
        ],
        "published": "2023-10-21T12:30:33Z",
        "summary": "This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to\ndetect translation quality errors, specifically for the quality estimation\nsetting without the need for human reference translations. Based on the power\nof large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting\ntechnique, querying the GPT-4 model to mark error quality spans. Compared to\nprevious works, our method has language-agnostic prompts, thus avoiding the\nneed for manual prompt preparation for new languages.\n  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art\naccuracy for system ranking, we advise caution when using it in academic works\nto demonstrate improvements over other methods due to its dependence on the\nproprietary, black-box GPT model.",
        "pdf_link": "https://arxiv.org/pdf/2310.13988v1.pdf"
    },
    {
        "title": "HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models",
        "authors": [
            "Vibhor Agarwal",
            "Yu Chen",
            "Nishanth Sastry"
        ],
        "published": "2023-10-21T12:18:29Z",
        "summary": "Hate speech has become pervasive in today's digital age. Although there has\nbeen considerable research to detect hate speech or generate counter speech to\ncombat hateful views, these approaches still cannot completely eliminate the\npotential harmful societal consequences of hate speech -- hate speech, even\nwhen detected, can often not be taken down or is often not taken down enough;\nand hate speech unfortunately spreads quickly, often much faster than any\ngenerated counter speech.\n  This paper investigates a relatively new yet simple and effective approach of\nsuggesting a rephrasing of potential hate speech content even before the post\nis made. We show that Large Language Models (LLMs) perform well on this task,\noutperforming state-of-the-art baselines such as BART-Detox. We develop 4\ndifferent prompts based on task description, hate definition, few-shot\ndemonstrations and chain-of-thoughts for comprehensive experiments and conduct\nexperiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well\nas OpenAI's GPT-3.5. We propose various evaluation metrics to measure the\nefficacy of the generated text and ensure the generated text has reduced hate\nintensity without drastically changing the semantic meaning of the original\ntext.\n  We find that LLMs with a few-shot demonstrations prompt work the best in\ngenerating acceptable hate-rephrased text with semantic meaning similar to the\noriginal text. Overall, we find that GPT-3.5 outperforms the baseline and\nopen-source models for all the different kinds of prompts. We also perform\nhuman evaluations and interestingly, find that the rephrasings generated by\nGPT-3.5 outperform even the human-generated ground-truth rephrasings in the\ndataset. We also conduct detailed ablation studies to investigate why LLMs work\nsatisfactorily on this task and conduct a failure analysis to understand the\ngaps.",
        "pdf_link": "https://arxiv.org/pdf/2310.13985v1.pdf"
    },
    {
        "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs",
        "authors": [
            "Young-Suk Lee",
            "Md Arafat Sultan",
            "Yousef El-Kurdi",
            "Tahira Naseem Asim Munawar",
            "Radu Florian",
            "Salim Roukos",
            "Ram√≥n Fernandez Astudillo"
        ],
        "published": "2023-10-21T10:21:17Z",
        "summary": "Using in-context learning (ICL) for data generation, techniques such as\nSelf-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)\ncan train strong conversational agents with only a small amount of human\nsupervision. One limitation of these approaches is that they resort to very\nlarge language models (around 175B parameters) that are also proprietary and\nnon-public. Here we explore the application of such techniques to language\nmodels that are much smaller (around 10B--40B parameters) and have permissive\nlicenses. We find the Self-Instruct approach to be less effective at these\nsizes and propose new ICL methods that draw on two main ideas: (a)\nCategorization and simplification of the ICL templates to make prompt learning\neasier for the LM, and (b) Ensembling over multiple LM outputs to help select\nhigh-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct\nseed tasks and employs separate pipelines for instructions that require an\ninput and instructions that do not. Empirical investigations with different LMs\nshow that: (1) Our proposed method yields higher-quality instruction tuning\ndata than Self-Instruct, (2) It improves performances of both vanilla and\ninstruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned\nLMs generate more useful outputs than their larger un-tuned counterparts. Our\ncodebase is available at https://github.com/IBM/ensemble-instruct.",
        "pdf_link": "https://arxiv.org/pdf/2310.13961v1.pdf"
    },
    {
        "title": "Copyright Violations and Large Language Models",
        "authors": [
            "Antonia Karamolegkou",
            "Jiaang Li",
            "Li Zhou",
            "Anders S√∏gaard"
        ],
        "published": "2023-10-20T19:14:59Z",
        "summary": "Language models may memorize more than just facts, including entire chunks of\ntexts seen during training. Fair use exemptions to copyright laws typically\nallow for limited use of copyrighted material without permission from the\ncopyright holder, but typically for extraction of information from copyrighted\nmaterials, rather than {\\em verbatim} reproduction. This work explores the\nissue of copyright violations and large language models through the lens of\nverbatim memorization, focusing on possible redistribution of copyrighted text.\nWe present experiments with a range of language models over a collection of\npopular books and coding problems, providing a conservative characterization of\nthe extent to which language models can redistribute these materials. Overall,\nthis research highlights the need for further examination and the potential\nimpact on future developments in natural language processing to ensure\nadherence to copyright regulations. Code is at\n\\url{https://github.com/coastalcph/CopyrightLLMs}.",
        "pdf_link": "https://arxiv.org/pdf/2310.13771v1.pdf"
    },
    {
        "title": "Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models",
        "authors": [
            "Arya D. McCarthy",
            "Hao Zhang",
            "Shankar Kumar",
            "Felix Stahlberg",
            "Ke Wu"
        ],
        "published": "2023-10-20T17:31:39Z",
        "summary": "One challenge in speech translation is that plenty of spoken content is\nlong-form, but short units are necessary for obtaining high-quality\ntranslations. To address this mismatch, we adapt large language models (LLMs)\nto split long ASR transcripts into segments that can be independently\ntranslated so as to maximize the overall translation quality. We overcome the\ntendency of hallucination in LLMs by incorporating finite-state constraints\nduring decoding; these eliminate invalid outputs without requiring additional\ntraining. We discover that LLMs are adaptable to transcripts containing ASR\nerrors through prompt-tuning or fine-tuning. Relative to a state-of-the-art\nautomatic punctuation baseline, our best LLM improves the average BLEU by 2.9\npoints for English-German, English-Spanish, and English-Arabic TED talk\ntranslation in 9 test sets, just by improving segmentation.",
        "pdf_link": "https://arxiv.org/pdf/2310.13678v2.pdf"
    },
    {
        "title": "Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models",
        "authors": [
            "Ruida Wang",
            "Wangchunshu Zhou",
            "Mrinmaya Sachan"
        ],
        "published": "2023-10-20T17:14:25Z",
        "summary": "*Data Synthesis* is a promising way to train a small model with very little\nlabeled data. One approach for data synthesis is to leverage the rich knowledge\nfrom large language models to synthesize pseudo training examples for small\nmodels, making it possible to achieve both data and compute efficiency at the\nsame time. However, a key challenge in data synthesis is that the synthesized\ndataset often suffers from a large distributional discrepancy from the *real\ntask* data distribution. Thus, in this paper, we propose *Synthesis Step by\nStep* (**S3**), a data synthesis framework that shrinks this distribution gap\nby iteratively extrapolating the errors made by a small model trained on the\nsynthesized dataset on a small real-world validation dataset using a large\nlanguage model. Extensive experiments on multiple NLP tasks show that our\napproach improves the performance of a small model by reducing the gap between\nthe synthetic dataset and the real data, resulting in significant improvement\ncompared to several baselines: 9.48% improvement compared to ZeroGen and 2.73%\ncompared to GoldGen, and at most 15.17% improvement compared to the small model\ntrained on human-annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2310.13671v1.pdf"
    },
    {
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "authors": [
            "Adithya Bhaskar",
            "Tushar Tomar",
            "Ashutosh Sathe",
            "Sunita Sarawagi"
        ],
        "published": "2023-10-20T17:00:53Z",
        "summary": "Research in Text-to-SQL conversion has been largely benchmarked against\ndatasets where each text query corresponds to one correct SQL. However, natural\nlanguage queries over real-life databases frequently involve significant\nambiguity about the intended SQL due to overlapping schema names and multiple\nconfusing relationship paths. To bridge this gap, we develop a novel benchmark\ncalled AmbiQT with over 3000 examples where each text is interpretable as two\nplausible SQLs due to lexical and/or structural ambiguity.\n  When faced with ambiguity, an ideal top-$k$ decoder should generate all valid\ninterpretations for possible disambiguation by the user. We evaluate several\nText-to-SQL systems and decoding algorithms, including those employing\nstate-of-the-art LLMs, and find them to be far from this ideal. The primary\nreason is that the prevalent beam search algorithm and its variants, treat SQL\nqueries as a string and produce unhelpful token-level diversity in the top-$k$.\n  We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic\nspace using a blend of plan-based template generation and constrained\ninfilling. Counterfactually generated plans diversify templates while\nin-filling with a beam-search that branches solely on schema names provides\nvalue diversity. LogicalBeam is up to $2.5$ times more effective than\nstate-of-the-art models at generating all candidate SQLs in the top-$k$ ranked\noutputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on\nSPIDER and Kaggle DBQA.",
        "pdf_link": "https://arxiv.org/pdf/2310.13659v1.pdf"
    },
    {
        "title": "BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues",
        "authors": [
            "Haodong Duan",
            "Jueqi Wei",
            "Chonghua Wang",
            "Hongwei Liu",
            "Yixiao Fang",
            "Songyang Zhang",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published": "2023-10-20T16:53:51Z",
        "summary": "Interacting with human via high-quality multi-turn dialogues is a key feature\nof large language models (LLMs). However, human-based evaluation of such\ncapability involves intensive manual labor. This report provides a preliminary\nevaluation of existing large language models for human-style multi-turn\nchatting, through an LLM-based approach. We start from real-world human\ndialogues and keep the very first utterances as the ChatSEED. Then we prompt\nLLMs to generate a full multi-turn dialogue (tens of utterances) based on the\nChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs\n(GPT-4, \\etc) as the judge to evaluate the generated dialogues. With different\nevaluation protocols, we come to substantially identical conclusions. We find\nthat GPT-4 can generate human-style multi-turn dialogues with impressive\nquality, significantly outperforms its counterparts. It's difficult for a\ndiscriminator to distinguish between GPT-4 generated dialogues and human\ndialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of\nsatisfactory quality due to poor instruction-following capability, tendency to\ngenerate lengthy utterances, or limited general capability. All data and codes\nwill be provided in https://github.com/open-compass/BotChat/ and we hope they\ncan serve as a valuable resource for evaluating multi-turn chatting\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.13650v1.pdf"
    },
    {
        "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
        "authors": [
            "An-Zi Yen",
            "Wei-Ling Hsu"
        ],
        "published": "2023-10-20T16:05:35Z",
        "summary": "Due to the remarkable language understanding and generation abilities of\nlarge language models (LLMs), their use in educational applications has been\nexplored. However, little work has been done on investigating the pedagogical\nability of LLMs in helping students to learn mathematics. In this position\npaper, we discuss the challenges associated with employing LLMs to enhance\nstudents' mathematical problem-solving skills by providing adaptive feedback.\nApart from generating the wrong reasoning processes, LLMs can misinterpret the\nmeaning of the question, and also exhibit difficulty in understanding the given\nquestions' rationales when attempting to correct students' answers. Three\nresearch questions are formulated.",
        "pdf_link": "https://arxiv.org/pdf/2310.13615v1.pdf"
    },
    {
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "authors": [
            "Alexandros Xenos",
            "Themos Stafylakis",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ],
        "published": "2023-10-20T15:08:17Z",
        "summary": "This paper is on the problem of Knowledge-Based Visual Question Answering\n(KB-VQA). Recent works have emphasized the significance of incorporating both\nexplicit (through external databases) and implicit (through LLMs) knowledge to\nanswer questions requiring external knowledge effectively. A common limitation\nof such approaches is that they consist of relatively complicated pipelines and\noften heavily rely on accessing GPT-3 API. Our main contribution in this paper\nis to propose a much simpler and readily reproducible pipeline which, in a\nnutshell, is based on efficient in-context learning by prompting LLaMA (1 and\n2) using question-informative captions as contextual information. Contrary to\nrecent approaches, our method is training-free, does not require access to\nexternal databases or APIs, and yet achieves state-of-the-art accuracy on the\nOK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to\nunderstand important aspects of our method. Our code is publicly available at\nhttps://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA",
        "pdf_link": "https://arxiv.org/pdf/2310.13570v2.pdf"
    },
    {
        "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
        "authors": [
            "Jinyuan Wang",
            "Junlong Li",
            "Hai Zhao"
        ],
        "published": "2023-10-20T14:51:10Z",
        "summary": "In open-domain question-answering (ODQA), most existing questions require\nsingle-hop reasoning on commonsense. To further extend this task, we officially\nintroduce open-domain multi-hop reasoning (ODMR) by answering multi-hop\nquestions with explicit reasoning steps in open-domain setting. Recently, large\nlanguage models (LLMs) have found significant utility in facilitating ODQA\nwithout external corpus. Furthermore, chain-of-thought (CoT) prompting boosts\nthe reasoning capability of LLMs to a greater extent with manual or automated\nparadigms. However, existing automated methods lack of quality assurance, while\nmanual approaches suffer from limited scalability and poor diversity, hindering\nthe capabilities of LLMs. In this paper, we propose Self-prompted\nChain-of-Thought (SP-CoT), an automated framework to mass-produce high quality\nCoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation\npipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT\nselection and self-prompted inference via in-context learning. Extensive\nexperiments on four multi-hop question-answering benchmarks show that our\nproposed SP-CoT not only significantly surpasses the previous SOTA methods on\nlarge-scale (175B) LLMs, but also nearly doubles the zero-shot performance of\nsmall-scale (13B) LLMs. Further analysis reveals the remarkable capability of\nSP-CoT to elicit direct and concise intermediate reasoning steps by recalling\n$\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.13552v2.pdf"
    },
    {
        "title": "The Perils & Promises of Fact-checking with Large Language Models",
        "authors": [
            "Dorian Quelle",
            "Alexandre Bovet"
        ],
        "published": "2023-10-20T14:49:47Z",
        "summary": "Automated fact-checking, using machine learning to verify claims, has grown\nvital as misinformation spreads beyond human fact-checking capacity. Large\nLanguage Models (LLMs) like GPT-4 are increasingly trusted to write academic\npapers, lawsuits, and news articles and to verify information, emphasizing\ntheir role in discerning truth from falsehood and the importance of being able\nto verify their outputs. Understanding the capacities and limitations of LLMs\nin fact-checking tasks is therefore essential for ensuring the health of our\ninformation ecosystem. Here, we evaluate the use of LLM agents in fact-checking\nby having them phrase queries, retrieve contextual data, and make decisions.\nImportantly, in our framework, agents explain their reasoning and cite the\nrelevant sources from the retrieved context. Our results show the enhanced\nprowess of LLMs when equipped with contextual information. GPT-4 outperforms\nGPT-3, but accuracy varies based on query language and claim veracity. While\nLLMs show promise in fact-checking, caution is essential due to inconsistent\naccuracy. Our investigation calls for further research, fostering a deeper\ncomprehension of when agents succeed and when they fail.",
        "pdf_link": "https://arxiv.org/pdf/2310.13549v2.pdf"
    },
    {
        "title": "She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models",
        "authors": [
            "Veronica Chatrath",
            "Oluwanifemi Bamgbose",
            "Shaina Raza"
        ],
        "published": "2023-10-20T14:18:40Z",
        "summary": "As the use of large language models (LLMs) increases within society, as does\nthe risk of their misuse. Appropriate safeguards must be in place to ensure LLM\noutputs uphold the ethical standards of society, highlighting the positive role\nthat artificial intelligence technologies can have. Recent events indicate\nethical concerns around conventionally trained LLMs, leading to overall unsafe\nuser experiences. This motivates our research question: how do we ensure LLM\nalignment? In this work, we introduce a test suite of unique prompts to foster\nthe development of aligned LLMs that are fair, safe, and robust. We show that\nprompting LLMs at every step of the development pipeline, including data\ncuration, pre-training, and fine-tuning, will result in an overall more\nresponsible model. Our test suite evaluates outputs from four state-of-the-art\nlanguage models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in\nthis paper highlights a gap between societal alignment and the capabilities of\ncurrent LLMs. Additionally, implementing a test suite such as ours lowers the\nenvironmental overhead of making models safe and fair.",
        "pdf_link": "https://arxiv.org/pdf/2310.18333v3.pdf"
    },
    {
        "title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
        "authors": [
            "Xiao Yu",
            "Baolin Peng",
            "Michel Galley",
            "Jianfeng Gao",
            "Zhou Yu"
        ],
        "published": "2023-10-20T14:11:04Z",
        "summary": "The self-improving ability of large language models (LLMs), enabled by\nprompting them to analyze and revise their own outputs, has garnered\nsignificant interest in recent research. However, this ability has been shown\nto be absent and difficult to learn for smaller models, thus widening the\nperformance gap between state-of-the-art LLMs and more cost-effective and\nfaster ones. To reduce this gap, we introduce TriPosT, a training algorithm\nthat endows smaller models with such self-improvement ability, and show that\nour approach can improve a LLaMA-7b's performance on math and reasoning tasks\nby up to 7.13%. In contrast to prior work, we achieve this by using the smaller\nmodel to interact with LLMs to collect feedback and improvements on its own\ngenerations. We then replay this experience to train the small model. Our\nexperiments on four math and reasoning datasets show that the interactive\nexperience of learning from and correcting its own mistakes is crucial for\nsmall models to improve their performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.13522v2.pdf"
    },
    {
        "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
        "authors": [
            "Duarte M. Alves",
            "Nuno M. Guerreiro",
            "Jo√£o Alves",
            "Jos√© Pombal",
            "Ricardo Rei",
            "Jos√© G. C. de Souza",
            "Pierre Colombo",
            "Andr√© F. T. Martins"
        ],
        "published": "2023-10-20T12:29:51Z",
        "summary": "Large language models (LLMs) are a promising avenue for machine translation\n(MT). However, current LLM-based MT systems are brittle: their effectiveness\nhighly depends on the choice of few-shot examples and they often require extra\npost-processing due to overgeneration. Alternatives such as finetuning on\ntranslation instructions are computationally expensive and may weaken\nin-context learning capabilities, due to overspecialization. In this paper, we\nprovide a closer look at this problem. We start by showing that adapter-based\nfinetuning with LoRA matches the performance of traditional finetuning while\nreducing the number of training parameters by a factor of 50. This method also\noutperforms few-shot prompting and eliminates the need for post-processing or\nin-context examples. However, we show that finetuning generally degrades\nfew-shot performance, hindering adaptation capabilities. Finally, to obtain the\nbest of both worlds, we propose a simple approach that incorporates few-shot\nexamples during finetuning. Experiments on 10 language pairs show that our\nproposed approach recovers the original few-shot capabilities while keeping the\nadded benefits of finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.13448v1.pdf"
    },
    {
        "title": "Self-Consistency of Large Language Models under Ambiguity",
        "authors": [
            "Henning Bartsch",
            "Ole Jorgensen",
            "Domenic Rosati",
            "Jason Hoelscher-Obermaier",
            "Jacob Pfau"
        ],
        "published": "2023-10-20T11:57:56Z",
        "summary": "Large language models (LLMs) that do not give consistent answers across\ncontexts are problematic when used for tasks with expectations of consistency,\ne.g., question-answering, explanations, etc. Our work presents an evaluation\nbenchmark for self-consistency in cases of under-specification where two or\nmore answers can be correct. We conduct a series of behavioral experiments on\nthe OpenAI model suite using an ambiguous integer sequence completion task. We\nfind that average consistency ranges from 67\\% to 82\\%, far higher than would\nbe predicted if a model's consistency was random, and increases as model\ncapability improves. Furthermore, we show that models tend to maintain\nself-consistency across a series of robustness checks, including prompting\nspeaker changes and sequence length changes. These results suggest that\nself-consistency arises as an emergent capability without specifically training\nfor it. Despite this, we find that models are uncalibrated when judging their\nown consistency, with models displaying both over- and under-confidence. We\nalso propose a nonparametric test for determining from token output\ndistribution whether a model assigns non-trivial probability to alternative\nanswers. Using this test, we find that despite increases in self-consistency,\nmodels usually place significant weight on alternative, inconsistent answers.\nThis distribution of probability mass provides evidence that even highly\nself-consistent models internally compute multiple possible responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.13439v1.pdf"
    },
    {
        "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
        "authors": [
            "Chang Shu",
            "Jiuzhou Han",
            "Fangyu Liu",
            "Ehsan Shareghi",
            "Nigel Collier"
        ],
        "published": "2023-10-20T10:05:01Z",
        "summary": "Embodied language comprehension emphasizes that language understanding is not\nsolely a matter of mental processing in the brain but also involves\ninteractions with the physical and social environment. With the explosive\ngrowth of Large Language Models (LLMs) and their already ubiquitous presence in\nour daily lives, it is becoming increasingly necessary to verify their\nreal-world understanding. Inspired by cognitive theories, we propose POSQA: a\nPhysical Object Size Question Answering dataset with simple size comparison\nquestions to examine the extremity and analyze the potential mechanisms of the\nembodied comprehension of the latest LLMs.\n  We show that even the largest LLMs today perform poorly under the zero-shot\nsetting. We then push their limits with advanced prompting techniques and\nexternal knowledge augmentation. Furthermore, we investigate whether their\nreal-world comprehension primarily derives from contextual information or\ninternal weights and analyse the impact of prompt formats and report bias of\ndifferent objects. Our results show that real-world understanding that LLMs\nshaped from textual data can be vulnerable to deception and confusion by the\nsurface form of prompts, which makes it less aligned with human behaviours.",
        "pdf_link": "https://arxiv.org/pdf/2310.13394v1.pdf"
    },
    {
        "title": "Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)",
        "authors": [
            "Xiaoliang Chen",
            "Liangbin Li",
            "Le Chang",
            "Yunhe Huang",
            "Yuxuan Zhao",
            "Yuxiao Zhang",
            "Dinuo Li"
        ],
        "published": "2023-10-20T08:13:36Z",
        "summary": "With the development of large language models (LLMs) like the GPT series,\ntheir widespread use across various application scenarios presents a myriad of\nchallenges. This review initially explores the issue of domain specificity,\nwhere LLMs may struggle to provide precise answers to specialized questions\nwithin niche fields. The problem of knowledge forgetting arises as these LLMs\nmight find it hard to balance old and new information. The knowledge repetition\nphenomenon reveals that sometimes LLMs might deliver overly mechanized\nresponses, lacking depth and originality. Furthermore, knowledge illusion\ndescribes situations where LLMs might provide answers that seem insightful but\nare actually superficial, while knowledge toxicity focuses on harmful or biased\ninformation outputs. These challenges underscore problems in the training data\nand algorithmic design of LLMs. To address these issues, it's suggested to\ndiversify training data, fine-tune models, enhance transparency and\ninterpretability, and incorporate ethics and fairness training. Future\ntechnological trends might lean towards iterative methodologies, multimodal\nlearning, model personalization and customization, and real-time learning and\nfeedback mechanisms. In conclusion, future LLMs should prioritize fairness,\ntransparency, and ethics, ensuring they uphold high moral and ethical standards\nwhen serving humanity.",
        "pdf_link": "https://arxiv.org/pdf/2310.13343v1.pdf"
    },
    {
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
        "authors": [
            "Zhaoyang Wang",
            "Shaohan Huang",
            "Yuxuan Liu",
            "Jiahai Wang",
            "Minghui Song",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2023-10-20T07:50:10Z",
        "summary": "Large language models (LLMs) exhibit impressive emergent abilities in natural\nlanguage processing, but their democratization is hindered due to huge\ncomputation requirements and closed-source nature. Recent research on advancing\nopen-source smaller LMs by distilling knowledge from black-box LLMs has\nobtained promising results in the instruction-following ability. However, the\nreasoning ability which is more challenging to foster, is relatively rarely\nexplored. In this paper, we propose a tailored learning approach to distill\nsuch reasoning ability to smaller LMs to facilitate the democratization of the\nexclusive reasoning ability. In contrast to merely employing LLM as a data\nannotator, we exploit the potential of LLM as a reasoning teacher by building\nan interactive multi-round learning paradigm. This paradigm enables the student\nto expose its deficiencies to the black-box teacher who then can provide\ncustomized training data in return. Further, to exploit the reasoning potential\nof the smaller LM, we propose self-reflection learning to motivate the student\nto learn from self-made mistakes. The learning from self-reflection and LLM are\nall tailored to the student's learning status, thanks to the seamless\nintegration with the multi-round learning paradigm. Comprehensive experiments\nand analysis on mathematical and commonsense reasoning tasks demonstrate the\neffectiveness of our method. The code will be available at\nhttps://github.com/Raibows/Learn-to-Reason.",
        "pdf_link": "https://arxiv.org/pdf/2310.13332v1.pdf"
    },
    {
        "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
        "authors": [
            "Soyeong Jeong",
            "Jinheon Baek",
            "Sukmin Cho",
            "Sung Ju Hwang",
            "Jong C. Park"
        ],
        "published": "2023-10-20T06:49:32Z",
        "summary": "Recent instruction-finetuned large language models (LMs) have achieved\nnotable performances in various tasks, such as question-answering (QA).\nHowever, despite their ability to memorize a vast amount of general knowledge\nacross diverse tasks, they might be suboptimal on specific tasks due to their\nlimited capacity to transfer and adapt knowledge to target tasks. Moreover,\nfurther finetuning LMs with labeled datasets is often infeasible due to their\nabsence, but it is also questionable if we can transfer smaller LMs having\nlimited knowledge only with unlabeled test data. In this work, we show and\ninvestigate the capabilities of smaller self-adaptive LMs, only with unlabeled\ntest data. In particular, we first stochastically generate multiple answers,\nand then ensemble them while filtering out low-quality samples to mitigate\nnoise from inaccurate labels. Our proposed self-adaption strategy demonstrates\nsignificant performance improvements on benchmark QA datasets with higher\nrobustness across diverse prompts, enabling LMs to stay stable. Code is\navailable at: https://github.com/starsuzi/T-SAS.",
        "pdf_link": "https://arxiv.org/pdf/2310.13307v1.pdf"
    },
    {
        "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model",
        "authors": [
            "Le Zhang",
            "Yihong Wu",
            "Fengran Mo",
            "Jian-Yun Nie",
            "Aishwarya Agrawal"
        ],
        "published": "2023-10-20T04:09:36Z",
        "summary": "Multi-modal open-domain question answering typically requires evidence\nretrieval from databases across diverse modalities, such as images, tables,\npassages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this\ntask. To enable LLMs to tackle the task in a zero-shot manner, we introduce\nMoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer\nstrategy that bypasses intricate multi-modality ranking, our framework can\naccommodate new modalities and seamlessly transition to new models for the\ntask. Built upon LLMs, MoqaGPT retrieves and extracts answers from each\nmodality separately, then fuses this multi-modal information using LLMs to\nproduce a final answer. Our methodology boosts performance on the MMCoQA\ndataset, improving F1 by +37.91 points and EM by +34.07 points over the\nsupervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the\nzero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and\nsignificantly closes the gap with supervised methods. Our codebase is available\nat https://github.com/lezhang7/MOQAGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.13265v1.pdf"
    },
    {
        "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
        "authors": [
            "Sipeng Zheng",
            "Jiazheng Liu",
            "Yicheng Feng",
            "Zongqing Lu"
        ],
        "published": "2023-10-20T03:22:05Z",
        "summary": "Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2310.13255v2.pdf"
    },
    {
        "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
        "authors": [
            "Jianwei Li",
            "Qi Lei",
            "Wei Cheng",
            "Dongkuan Xu"
        ],
        "published": "2023-10-19T23:02:29Z",
        "summary": "The pruning objective has recently extended beyond accuracy and sparsity to\nrobustness in language models. Despite this, existing methods struggle to\nenhance robustness against adversarial attacks when continually increasing\nmodel sparsity and require a retraining process. As humans step into the era of\nlarge language models, these issues become increasingly prominent. This paper\nproposes that the robustness of language models is proportional to the extent\nof pre-trained knowledge they encompass. Accordingly, we introduce a\npost-training pruning strategy designed to faithfully replicate the embedding\nspace and feature space of dense language models, aiming to conserve more\npre-trained knowledge during the pruning process. In this setup, each layer's\nreconstruction error not only originates from itself but also includes\ncumulative error from preceding layers, followed by an adaptive rectification.\nCompared to other state-of-art baselines, our approach demonstrates a superior\nbalance between accuracy, sparsity, robustness, and pruning cost with BERT on\ndatasets SST2, IMDB, and AGNews, marking a significant stride towards robust\npruning in language models.",
        "pdf_link": "https://arxiv.org/pdf/2310.13191v3.pdf"
    },
    {
        "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
        "authors": [
            "Zhihan Zhang",
            "Shuohang Wang",
            "Wenhao Yu",
            "Yichong Xu",
            "Dan Iter",
            "Qingkai Zeng",
            "Yang Liu",
            "Chenguang Zhu",
            "Meng Jiang"
        ],
        "published": "2023-10-19T19:52:55Z",
        "summary": "Large language models (LLMs) can perform a wide range of tasks by following\nnatural language instructions, without the necessity of task-specific\nfine-tuning. Unfortunately, the performance of LLMs is greatly influenced by\nthe quality of these instructions, and manually writing effective instructions\nfor each task is a laborious and subjective process. In this paper, we\nintroduce Auto-Instruct, a novel method to automatically improve the quality of\ninstructions provided to LLMs. Our method leverages the inherent generative\nability of LLMs to produce diverse candidate instructions for a given task, and\nthen ranks them using a scoring model trained on a variety of 575 existing NLP\ntasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both\nhuman-written instructions and existing baselines of LLM-generated\ninstructions. Furthermore, our method exhibits notable generalizability even\nwith other LLMs that are not incorporated into its training process.",
        "pdf_link": "https://arxiv.org/pdf/2310.13127v1.pdf"
    },
    {
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "authors": [
            "David Chan",
            "Suzanne Petryk",
            "Joseph E. Gonzalez",
            "Trevor Darrell",
            "John Canny"
        ],
        "published": "2023-10-19T17:59:01Z",
        "summary": "The evaluation of machine-generated image captions poses an interesting yet\npersistent challenge. Effective evaluation measures must consider numerous\ndimensions of similarity, including semantic relevance, visual structure,\nobject interactions, caption diversity, and specificity. Existing\nhighly-engineered measures attempt to capture specific aspects, but fall short\nin providing a holistic score that aligns closely with human judgments. Here,\nwe propose CLAIR, a novel method that leverages the zero-shot language modeling\ncapabilities of large language models (LLMs) to evaluate candidate captions. In\nour evaluations, CLAIR demonstrates a stronger correlation with human judgments\nof caption quality compared to existing measures. Notably, on Flickr8K-Expert,\nCLAIR achieves relative correlation improvements over SPICE of 39.6% and over\nimage-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides\nnoisily interpretable results by allowing the language model to identify the\nunderlying reasoning behind its assigned score. Code is available at\nhttps://davidmchan.github.io/clair/",
        "pdf_link": "https://arxiv.org/pdf/2310.12971v1.pdf"
    },
    {
        "title": "Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation",
        "authors": [
            "Sangho Suh",
            "Meng Chen",
            "Bryan Min",
            "Toby Jia-Jun Li",
            "Haijun Xia"
        ],
        "published": "2023-10-19T17:53:14Z",
        "summary": "Thanks to their generative capabilities, large language models (LLMs) have\nbecome an invaluable tool for creative processes. These models have the\ncapacity to produce hundreds and thousands of visual and textual outputs,\noffering abundant inspiration for creative endeavors. But are we harnessing\ntheir full potential? We argue that current interaction paradigms fall short,\nguiding users towards rapid convergence on a limited set of ideas, rather than\nempowering them to explore the vast latent design space in generative models.\nTo address this limitation, we propose a framework that facilitates the\nstructured generation of design space in which users can seamlessly explore,\nevaluate, and synthesize a multitude of responses. We demonstrate the\nfeasibility and usefulness of this framework through the design and development\nof an interactive system, Luminate, and a user study with 14 professional\nwriters. Our work advances how we interact with LLMs for creative tasks,\nintroducing a way to harness the creative potential of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12953v3.pdf"
    },
    {
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Tsz Ho Chan",
            "Tianqing Fang",
            "Weiqi Wang",
            "Chunkit Chan",
            "Dongyu Ru",
            "Qipeng Guo",
            "Hongming Zhang",
            "Yangqiu Song",
            "Yue Zhang",
            "Zheng Zhang"
        ],
        "published": "2023-10-19T16:29:23Z",
        "summary": "Analogy-making between narratives is crucial for human reasoning. In this\npaper, we evaluate the ability to identify and generate analogies by\nconstructing a first-of-its-kind large-scale story-level analogy corpus,\n\\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with\nhuman annotations on two similarities from the extended Structure-Mapping\nTheory. We design a set of tests on \\textsc{StoryAnalogy}, presenting the first\nevaluation of story-level analogy identification and generation. Interestingly,\nwe find that the analogy identification tasks are incredibly difficult not only\nfor sentence embedding models but also for the recent large language models\n(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around\n30% accuracy in multiple-choice questions (compared to over 85% accuracy for\nhumans). Furthermore, we observe that the data in \\textsc{StoryAnalogy} can\nimprove the quality of analogy generation in LLMs, where a fine-tuned\nFlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.12874v2.pdf"
    },
    {
        "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
        "authors": [
            "Sarthak Roy",
            "Ashish Harshavardhan",
            "Animesh Mukherjee",
            "Punyajoy Saha"
        ],
        "published": "2023-10-19T16:11:02Z",
        "summary": "Recently efforts have been made by social media platforms as well as\nresearchers to detect hateful or toxic language using large language models.\nHowever, none of these works aim to use explanation, additional context and\nvictim community information in the detection process. We utilise different\nprompt variation, input information and evaluate large language models in zero\nshot setting (without adding any in-context examples). We select three large\nlanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -\nHateXplain, implicit hate and ToxicSpans. We find that on average including the\ntarget information in the pipeline improves the model performance substantially\n(~20-30%) over the baseline across the datasets. There is also a considerable\neffect of adding the rationales/explanations into the pipeline (~10-20%) over\nthe baseline across the datasets. In addition, we further provide a typology of\nthe error cases where these large language models fail to (i) classify and (ii)\nexplain the reason for the decisions they take. Such vulnerable points\nautomatically constitute 'jailbreak' prompts for these models and industry\nscale safeguard techniques need to be developed to make the models robust\nagainst such prompts.",
        "pdf_link": "https://arxiv.org/pdf/2310.12860v2.pdf"
    },
    {
        "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
        "authors": [
            "Aohan Zeng",
            "Mingdao Liu",
            "Rui Lu",
            "Bowen Wang",
            "Xiao Liu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-10-19T15:19:53Z",
        "summary": "Open large language models (LLMs) with great performance in various tasks\nhave significantly advanced the development of LLMs. However, they are far\ninferior to commercial models such as ChatGPT and GPT-4 when acting as agents\nto tackle complex tasks in the real world. These agent tasks employ LLMs as the\ncentral controller responsible for planning, memorization, and tool\nutilization, necessitating both fine-grained prompting methods and robust LLMs\nto achieve satisfactory performance. Though many prompting methods have been\nproposed to complete particular agent tasks, there is lack of research focusing\non improving the agent capabilities of LLMs themselves without compromising\ntheir general abilities. In this work, we present AgentTuning, a simple and\ngeneral method to enhance the agent abilities of LLMs while maintaining their\ngeneral LLM capabilities. We construct AgentInstruct, a lightweight\ninstruction-tuning dataset containing high-quality interaction trajectories. We\nemploy a hybrid instruction-tuning strategy by combining AgentInstruct with\nopen-source instructions from general domains. AgentTuning is used to\ninstruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show\nthat AgentTuning enables LLMs' agent capabilities without compromising general\nabilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent\ntasks, demonstrating generalized agent capabilities. We open source the\nAgentInstruct and AgentLM-7B, 13B, and 70B models at\nhttps://github.com/THUDM/AgentTuning, serving open and powerful alternatives to\ncommercial LLMs for agent tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.12823v2.pdf"
    },
    {
        "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
        "authors": [
            "Yupei Liu",
            "Yuqi Jia",
            "Runpeng Geng",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "published": "2023-10-19T15:12:09Z",
        "summary": "Large Language Models (LLMs) are increasingly deployed as the backend for a\nvariety of real-world applications called LLM-Integrated Applications. Multiple\nrecent works showed that LLM-Integrated Applications are vulnerable to prompt\ninjection attacks, in which an attacker injects malicious instruction/data into\nthe input of those applications such that they produce results as the attacker\ndesires. However, existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a general framework to formalize prompt injection attacks. Existing\nattacks, which are discussed in research papers and blog posts, are special\ncases in our framework. Our framework enables us to design a new attack by\ncombining existing attacks. Moreover, we also propose a framework to\nsystematize defenses against prompt injection attacks. Using our frameworks, we\nconduct a systematic evaluation on prompt injection attacks and their defenses\nwith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in\nthis field. Our code is available at\nhttps://github.com/liu00222/Open-Prompt-Injection.",
        "pdf_link": "https://arxiv.org/pdf/2310.12815v1.pdf"
    },
    {
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": [
            "Ningyu Xu",
            "Qi Zhang",
            "Jingting Ye",
            "Menghan Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-19T14:50:51Z",
        "summary": "Large language models (LLMs) have exhibited considerable cross-lingual\ngeneralization abilities, whereby they implicitly transfer knowledge across\nlanguages. However, the transfer is not equally successful for all languages,\nespecially for low-resource ones, which poses an ongoing challenge. It is\nunclear whether we have reached the limits of implicit cross-lingual\ngeneralization and if explicit knowledge transfer is viable. In this paper, we\ninvestigate the potential for explicitly aligning conceptual correspondence\nbetween languages to enhance cross-lingual generalization. Using the syntactic\naspect of language as a testbed, our analyses of 43 languages reveal a high\ndegree of alignability among the spaces of structural concepts within each\nlanguage for both encoder-only and decoder-only LLMs. We then propose a\nmeta-learning-based method to learn to align conceptual spaces of different\nlanguages, which facilitates zero-shot and few-shot generalization in concept\nclassification and also offers insights into the cross-lingual in-context\nlearning phenomenon. Experiments on syntactic analysis tasks show that our\napproach achieves competitive results with state-of-the-art methods and narrows\nthe performance gap between languages, particularly benefiting those with\nlimited resources.",
        "pdf_link": "https://arxiv.org/pdf/2310.12794v2.pdf"
    },
    {
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
        "authors": [
            "Josef Dai",
            "Xuehai Pan",
            "Ruiyang Sun",
            "Jiaming Ji",
            "Xinbo Xu",
            "Mickel Liu",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "published": "2023-10-19T14:22:03Z",
        "summary": "With the development of large language models (LLMs), striking a balance\nbetween the performance and safety of AI systems has never been more critical.\nHowever, the inherent tension between the objectives of helpfulness and\nharmlessness presents a significant challenge during LLM training. To address\nthis issue, we propose Safe Reinforcement Learning from Human Feedback (Safe\nRLHF), a novel algorithm for human value alignment. Safe RLHF explicitly\ndecouples human preferences regarding helpfulness and harmlessness, effectively\navoiding the crowdworkers' confusion about the tension and allowing us to train\nseparate reward and cost models. We formalize the safety concern of LLMs as an\noptimization task of maximizing the reward function while satisfying specified\ncost constraints. Leveraging the Lagrangian method to solve this constrained\nproblem, Safe RLHF dynamically adjusts the balance between the two objectives\nduring fine-tuning. Through a three-round fine-tuning using Safe RLHF, we\ndemonstrate a superior ability to mitigate harmful responses while enhancing\nmodel performance compared to existing value-aligned algorithms.\nExperimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with\ncollected human preferences, significantly improving its helpfulness and\nharmlessness according to human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2310.12773v1.pdf"
    },
    {
        "title": "Exploring Large Language Models as a Source of Common-Sense Knowledge for Robots",
        "authors": [
            "Felix Ocker",
            "J√∂rg Deigm√∂ller",
            "Julian Eggert"
        ],
        "published": "2023-10-19T14:20:30Z",
        "summary": "Service robots need common-sense knowledge to help humans in everyday\nsituations as it enables them to understand the context of their actions.\nHowever, approaches that use ontologies face a challenge because common-sense\nknowledge is often implicit, i.e., it is obvious to humans but not explicitly\nstated. This paper investigates if Large Language Models (LLMs) can fill this\ngap. Our experiments reveal limited effectiveness in the selective extraction\nof contextual action knowledge, suggesting that LLMs may not be sufficient on\ntheir own. However, the large-scale extraction of general, actionable knowledge\nshows potential, indicating that LLMs can be a suitable tool for efficiently\ncreating ontologies for robots. This paper shows that the technique used for\nknowledge extraction can be applied to populate a minimalist ontology,\nshowcasing the potential of LLMs in synergy with formal knowledge\nrepresentation.",
        "pdf_link": "https://arxiv.org/pdf/2311.08412v1.pdf"
    },
    {
        "title": "TabuLa: Harnessing Language Models for Tabular Data Synthesis",
        "authors": [
            "Zilong Zhao",
            "Robert Birke",
            "Lydia Chen"
        ],
        "published": "2023-10-19T13:50:56Z",
        "summary": "Given the ubiquitous use of tabular data in industries and the growing\nconcerns in data privacy and security, tabular data synthesis emerges as a\ncritical research area. The recent state-of-the-art methods show that large\nlanguage models (LLMs) can be adopted to generate realistic tabular data. As\nLLMs pre-process tabular data as full text, they have the advantage of avoiding\nthe curse of dimensionality associated with one-hot encoding high-dimensional\ndata. However, their long training time and limited re-usability on new tasks\nprevent them from replacing exiting tabular generative models. In this paper,\nwe propose Tabula, a tabular data synthesizer based on the language model\nstructure. Through Tabula, we demonstrate the inherent limitation of employing\npre-trained language models designed for natural language processing (NLP) in\nthe context of tabular data synthesis. Our investigation delves into the\ndevelopment of a dedicated foundational model tailored specifically for tabular\ndata synthesis. Additionally, we propose a token sequence compression strategy\nto significantly reduce training time while preserving the quality of synthetic\ndata. Extensive experiments on six datasets demonstrate that using a language\nmodel structure without loading the well-trained model weights yields a better\nstarting model for tabular data synthesis. Moreover, the Tabula model,\npreviously trained on other tabular data, serves as an excellent foundation\nmodel for new tabular data synthesis tasks. Additionally, the token sequence\ncompression method substantially reduces the model's training time. Results\nshow that Tabula averagely reduces 46.2% training time per epoch comparing to\ncurrent LLMs-based state-of-the-art algorithm and consistently achieves even\nhigher synthetic data utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.12746v1.pdf"
    },
    {
        "title": "Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong",
        "authors": [
            "Chenglei Si",
            "Navita Goyal",
            "Sherry Tongshuang Wu",
            "Chen Zhao",
            "Shi Feng",
            "Hal Daum√© III",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-10-19T08:09:58Z",
        "summary": "Large Language Models (LLMs) are increasingly used for accessing information\non the web. Their truthfulness and factuality are thus of great interest. To\nhelp users make the right decisions about the information they get, LLMs should\nnot only provide information but also help users fact-check it. Our experiments\nwith 80 crowdworkers compare language models with search engines (information\nretrieval systems) at facilitating fact-checking. We prompt LLMs to validate a\ngiven claim and provide corresponding explanations. Users reading LLM\nexplanations are significantly more efficient than those using search engines\nwhile achieving similar accuracy. However, they over-rely on the LLMs when the\nexplanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide\ncontrastive information - explain both why the claim is true and false, and\nthen we present both sides of the explanation to users. This contrastive\nexplanation mitigates users' over-reliance on LLMs, but cannot significantly\noutperform search engines. Further, showing both search engine results and LLM\nexplanations offers no complementary benefits compared to search engines alone.\nTaken together, our study highlights that natural language explanations by LLMs\nmay not be a reliable replacement for reading the retrieved passages,\nespecially in high-stakes settings where over-relying on wrong AI explanations\ncould lead to critical consequences.",
        "pdf_link": "https://arxiv.org/pdf/2310.12558v2.pdf"
    },
    {
        "title": "Product Attribute Value Extraction using Large Language Models",
        "authors": [
            "Alexander Brinkmann",
            "Roee Shraga",
            "Christian Bizer"
        ],
        "published": "2023-10-19T07:39:00Z",
        "summary": "E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.",
        "pdf_link": "https://arxiv.org/pdf/2310.12537v2.pdf"
    },
    {
        "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
        "authors": [
            "Xiaodong Yu",
            "Hao Cheng",
            "Xiaodong Liu",
            "Dan Roth",
            "Jianfeng Gao"
        ],
        "published": "2023-10-19T06:37:32Z",
        "summary": "Although remarkable progress has been achieved in preventing large language\nmodel (LLM) hallucinations using instruction tuning and retrieval augmentation,\nit remains challenging to measure the reliability of LLMs using human-crafted\nevaluation data which is not available for many tasks and domains and could\nsuffer from data leakage. Inspired by adversarial machine learning, this paper\naims to develop a method of automatically generating evaluation data by\nappropriately modifying existing data on which LLMs behave faithfully.\nSpecifically, this paper presents AutoDebug, an LLM-based framework to use\nprompting chaining to generate transferable adversarial attacks in the form of\nquestion-answering examples. We seek to understand the extent to which these\nexamples trigger the hallucination behaviors of LLMs.\n  We implement AutoDebug using ChatGPT and evaluate the resulting two variants\nof a popular open-domain question-answering dataset, Natural Questions (NQ), on\na collection of open-source and proprietary LLMs under various prompting\nsettings. Our generated evaluation data is human-readable and, as we show,\nhumans can answer these modified questions well. Nevertheless, we observe\npronounced accuracy drops across multiple LLMs including GPT-4. Our\nexperimental results show that LLMs are likely to hallucinate in two categories\nof question-answering scenarios where (1) there are conflicts between knowledge\ngiven in the prompt and their parametric knowledge, or (2) the knowledge\nexpressed in the prompt is complex. Finally, we find that the adversarial\nexamples generated by our method are transferable across all considered LLMs.\nThe examples generated by a small model can be used to debug a much larger\nmodel, making our approach cost-effective.",
        "pdf_link": "https://arxiv.org/pdf/2310.12516v1.pdf"
    },
    {
        "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
        "authors": [
            "Boyi Deng",
            "Wenjie Wang",
            "Fuli Feng",
            "Yang Deng",
            "Qifan Wang",
            "Xiangnan He"
        ],
        "published": "2023-10-19T06:15:05Z",
        "summary": "Large language models (LLMs) are susceptible to red teaming attacks, which\ncan induce LLMs to generate harmful content. Previous research constructs\nattack prompts via manual or automatic methods, which have their own\nlimitations on construction cost and quality. To address these issues, we\npropose an integrated approach that combines manual and automatic methods to\neconomically generate high-quality attack prompts. Specifically, considering\nthe impressive capabilities of newly emerged LLMs, we propose an attack\nframework to instruct LLMs to mimic human-generated prompts through in-context\nlearning. Furthermore, we propose a defense framework that fine-tunes victim\nLLMs through iterative interactions with the attack framework to enhance their\nsafety against red teaming attacks. Extensive experiments on different LLMs\nvalidate the effectiveness of our proposed attack and defense frameworks.\nAdditionally, we release a series of attack prompts datasets named SAP with\nvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.\nOur code and dataset is available on https://github.com/Aatrox103/SAP .",
        "pdf_link": "https://arxiv.org/pdf/2310.12505v1.pdf"
    },
    {
        "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
        "authors": [
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Jingyuan Huang",
            "Ruyi Dai",
            "Jen-tse Huang",
            "Zhaopeng Tu",
            "Michael R. Lyu"
        ],
        "published": "2023-10-19T05:38:23Z",
        "summary": "This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12481v2.pdf"
    },
    {
        "title": "Contrastive Learning for Inference in Dialogue",
        "authors": [
            "Etsuko Ishii",
            "Yan Xu",
            "Bryan Wilie",
            "Ziwei Ji",
            "Holy Lovenia",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023-10-19T04:49:36Z",
        "summary": "Inference, especially those derived from inductive processes, is a crucial\ncomponent in our conversation to complement the information implicitly or\nexplicitly conveyed by a speaker. While recent large language models show\nremarkable advances in inference tasks, their performance in inductive\nreasoning, where not all information is present in the context, is far behind\ndeductive reasoning. In this paper, we analyze the behavior of the models based\non the task difficulty defined by the semantic information gap -- which\ndistinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).\nOur analysis reveals that the disparity in information between dialogue\ncontexts and desired inferences poses a significant challenge to the inductive\ninference process. To mitigate this information gap, we investigate a\ncontrastive learning approach by feeding negative samples. Our experiments\nsuggest negative samples help models understand what is wrong and improve their\ninference generations.",
        "pdf_link": "https://arxiv.org/pdf/2310.12467v2.pdf"
    },
    {
        "title": "Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher",
        "authors": [
            "Xiang Shi",
            "Jiawei Liu",
            "Yinpeng Liu",
            "Qikai Cheng",
            "Wei Lu"
        ],
        "published": "2023-10-19T03:49:36Z",
        "summary": "The advent of Large Language Models (LLMs) has shown the potential to improve\nrelevance and provide direct answers in web searches. However, challenges arise\nin validating the reliability of generated results and the credibility of\ncontributing sources, due to the limitations of traditional information\nretrieval algorithms and the LLM hallucination problem. Aiming to create a\n\"PageRank\" for the LLM era, we strive to transform LLM into a relevant,\nresponsible, and trustworthy searcher. We propose a novel generative retrieval\nframework leveraging the knowledge of LLMs to foster a direct link between\nqueries and online sources. This framework consists of three core modules:\nGenerator, Validator, and Optimizer, each focusing on generating trustworthy\nonline sources, verifying source reliability, and refining unreliable sources,\nrespectively. Extensive experiments and evaluations highlight our method's\nsuperior relevance, responsibility, and trustfulness against various SOTA\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2310.12443v1.pdf"
    },
    {
        "title": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
        "authors": [
            "Hongwei Yao",
            "Jian Lou",
            "Zhan Qin"
        ],
        "published": "2023-10-19T03:25:28Z",
        "summary": "Prompts have significantly improved the performance of pretrained Large\nLanguage Models (LLMs) on various downstream tasks recently, making them\nincreasingly indispensable for a diverse range of LLM application scenarios.\nHowever, the backdoor vulnerability, a serious security threat that can\nmaliciously alter the victim model's normal predictions, has not been\nsufficiently explored for prompt-based LLMs. In this paper, we present\nPOISONPROMPT, a novel backdoor attack capable of successfully compromising both\nhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and\nrobustness of POISONPROMPT through extensive experiments on three popular\nprompt methods, using six datasets and three widely used LLMs. Our findings\nhighlight the potential security threats posed by backdoor attacks on\nprompt-based LLMs and emphasize the need for further research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2310.12439v2.pdf"
    },
    {
        "title": "Automated Repair of Declarative Software Specifications in the Era of Large Language Models",
        "authors": [
            "Md Rashedul Hasan",
            "Jiawei Li",
            "Iftekhar Ahmed",
            "Hamid Bagheri"
        ],
        "published": "2023-10-19T02:30:42Z",
        "summary": "The growing adoption of declarative software specification languages, coupled\nwith their inherent difficulty in debugging, has underscored the need for\neffective and automated repair techniques applicable to such languages.\nResearchers have recently explored various methods to automatically repair\ndeclarative software specifications, such as template-based repair,\nfeedback-driven iterative repair, and bounded exhaustive approaches. The latest\ndevelopments in large language models provide new opportunities for the\nautomatic repair of declarative specifications. In this study, we assess the\neffectiveness of utilizing OpenAI's ChatGPT to repair software specifications\nwritten in the Alloy declarative language. Unlike imperative languages,\nspecifications in Alloy are not executed but rather translated into logical\nformulas and evaluated using backend constraint solvers to identify\nspecification instances and counterexamples to assertions. Our evaluation\nfocuses on ChatGPT's ability to improve the correctness and completeness of\nAlloy declarative specifications through automatic repairs. We analyze the\nresults produced by ChatGPT and compare them with those of leading automatic\nAlloy repair methods. Our study revealed that while ChatGPT falls short in\ncomparison to existing techniques, it was able to successfully repair bugs that\nno other technique could address. Our analysis also identified errors in\nChatGPT's generated repairs, including improper operator usage, type errors,\nhigher-order logic misuse, and relational arity mismatches. Additionally, we\nobserved instances of hallucinations in ChatGPT-generated repairs and\ninconsistency in its results. Our study provides valuable insights for software\npractitioners, researchers, and tool builders considering ChatGPT for\ndeclarative specification repairs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12425v2.pdf"
    },
    {
        "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
        "authors": [
            "Kaya Stechly",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "published": "2023-10-19T00:56:37Z",
        "summary": "There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples, a wide spread belief in their iterative self-critique\ncapabilities persists. In this paper, we set out to systematically investigate\nthe effectiveness of iterative prompting of LLMs in the context of Graph\nColoring, a canonical NP-complete reasoning problem that is related to\npropositional satisfiability as well as practical problems like scheduling and\nallocation. We present a principled empirical study of the performance of GPT4\nin solving graph coloring instances or verifying the correctness of candidate\ncolorings. In iterative modes, we experiment with the model critiquing its own\nanswers and an external correct reasoner verifying proposed solutions. In both\ncases, we analyze whether the content of the criticisms actually affects bottom\nline performance. The study seems to indicate that (i) LLMs are bad at solving\ngraph coloring instances (ii) they are no better at verifying a solution--and\nthus are not effective in iterative modes with LLMs critiquing LLM-generated\nsolutions (iii) the correctness and content of the criticisms--whether by LLMs\nor external solvers--seems largely irrelevant to the performance of iterative\nprompting. We show that the observed increase in effectiveness is largely due\nto the correct solution being fortuitously present in the top-k completions of\nthe prompt (and being recognized as such by an external verifier). Our results\nthus call into question claims about the self-critiquing capabilities of state\nof the art LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12397v1.pdf"
    },
    {
        "title": "FactCHD: Benchmarking Fact-Conflicting Hallucination Detection",
        "authors": [
            "Xiang Chen",
            "Duanzheng Song",
            "Honghao Gui",
            "Chenxi Wang",
            "Ningyu Zhang",
            "Jiang Yong",
            "Fei Huang",
            "Chengfei Lv",
            "Dan Zhang",
            "Huajun Chen"
        ],
        "published": "2023-10-18T16:27:49Z",
        "summary": "Despite their impressive generative capabilities, LLMs are hindered by\nfact-conflicting hallucinations in real-world applications. The accurate\nidentification of hallucinations in texts generated by LLMs, especially in\ncomplex inferential scenarios, is a relatively unexplored area. To address this\ngap, we present FactCHD, a dedicated benchmark designed for the detection of\nfact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset\nthat spans various factuality patterns, including vanilla, multi-hop,\ncomparison, and set operation. A distinctive element of FactCHD is its\nintegration of fact-based evidence chains, significantly enhancing the depth of\nevaluating the detectors' explanations. Experiments on different LLMs expose\nthe shortcomings of current approaches in detecting factual errors accurately.\nFurthermore, we introduce Truth-Triangulator that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset is available at\nhttps://github.com/zjunlp/FactCHD.",
        "pdf_link": "https://arxiv.org/pdf/2310.12086v2.pdf"
    },
    {
        "title": "SPEED: Speculative Pipelined Execution for Efficient Decoding",
        "authors": [
            "Coleman Hooper",
            "Sehoon Kim",
            "Hiva Mohammadzadeh",
            "Hasan Genc",
            "Kurt Keutzer",
            "Amir Gholami",
            "Sophia Shao"
        ],
        "published": "2023-10-18T16:07:01Z",
        "summary": "Generative Large Language Models (LLMs) based on the Transformer architecture\nhave recently emerged as a dominant foundation model for a wide range of\nNatural Language Processing tasks. Nevertheless, their application in real-time\nscenarios has been highly restricted due to the significant inference latency\nassociated with these models. This is particularly pronounced due to the\nautoregressive nature of generative LLM inference, where tokens are generated\nsequentially since each token depends on all previous output tokens. It is\ntherefore challenging to achieve any token-level parallelism, making inference\nextremely memory-bound. In this work, we propose SPEED, which improves\ninference efficiency by speculatively executing multiple future tokens in\nparallel with the current token using predicted values based on early-layer\nhidden states. For Transformer decoders that employ parameter sharing, the\nmemory operations for the tokens executing in parallel can be amortized, which\nallows us to accelerate generative LLM inference. We demonstrate the efficiency\nof our method in terms of latency reduction relative to model accuracy and\ndemonstrate how speculation allows for training deeper decoders with parameter\nsharing with minimal runtime overhead.",
        "pdf_link": "https://arxiv.org/pdf/2310.12072v2.pdf"
    },
    {
        "title": "LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation",
        "authors": [
            "Shengqiang Zhang",
            "Philipp Wicke",
            "L√ºtfi Kerem ≈ûenel",
            "Luis Figueredo",
            "Abdeldjallil Naceri",
            "Sami Haddadin",
            "Barbara Plank",
            "Hinrich Sch√ºtze"
        ],
        "published": "2023-10-18T14:53:14Z",
        "summary": "The convergence of embodied agents and large language models (LLMs) has\nbrought significant advancements to embodied instruction following.\nParticularly, the strong reasoning capabilities of LLMs make it possible for\nrobots to perform long-horizon tasks without expensive annotated\ndemonstrations. However, public benchmarks for testing the long-horizon\nreasoning capabilities of language-conditioned robots in various scenarios are\nstill missing. To fill this gap, this work focuses on the tabletop manipulation\ntask and releases a simulation benchmark, \\textit{LoHoRavens}, which covers\nvarious long-horizon reasoning aspects spanning color, size, space, arithmetics\nand reference. Furthermore, there is a key modality bridging problem for\nlong-horizon manipulation tasks with LLMs: how to incorporate the observation\nfeedback during robot execution for the LLM's closed-loop planning, which is\nhowever less studied by prior work. We investigate two methods of bridging the\nmodality gap: caption generation and learnable interface for incorporating\nexplicit and implicit observation feedback to the LLM, respectively. These\nmethods serve as the two baselines for our proposed benchmark. Experiments show\nthat both methods struggle to solve some tasks, indicating long-horizon\nmanipulation tasks are still challenging for current popular models. We expect\nthe proposed public benchmark and baselines can help the community develop\nbetter models for long-horizon tabletop manipulation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.12020v2.pdf"
    },
    {
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
        "authors": [
            "Rui Zheng",
            "Wei Shen",
            "Yuan Hua",
            "Wenbin Lai",
            "Shihan Dou",
            "Yuhao Zhou",
            "Zhiheng Xi",
            "Xiao Wang",
            "Haoran Huang",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-18T13:54:15Z",
        "summary": "The success of AI assistants based on language models (LLMs) hinges crucially\non Reinforcement Learning from Human Feedback (RLHF), which enables the\ngeneration of responses more aligned with human preferences. As universal AI\nassistants, there's a growing expectation for them to perform consistently\nacross various domains. However, previous work shows that Reinforcement\nLearning (RL) often exploits shortcuts to attain high rewards and overlooks\nchallenging samples. This focus on quick reward gains undermines both the\nstability in training and the model's ability to generalize to new, unseen\ndata. In this work, we propose a novel approach that can learn a consistent\npolicy via RL across various data groups or domains. Given the challenges\nassociated with acquiring group annotations, our method automatically\nclassifies data into different groups, deliberately maximizing performance\nvariance. Then, we optimize the policy to perform well on challenging groups.\nLastly, leveraging the established groups, our approach adaptively adjusts the\nexploration space, allocating more learning capacity to more challenging data\nand preventing the model from over-optimizing on simpler data. Experimental\nresults indicate that our approach significantly enhances training stability\nand model generalization.",
        "pdf_link": "https://arxiv.org/pdf/2310.11971v3.pdf"
    },
    {
        "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
        "authors": [
            "Yuval Pinter",
            "Michael Elhadad"
        ],
        "published": "2023-10-18T13:38:03Z",
        "summary": "We call into question the recently popularized method of direct model editing\nas a means of correcting factual errors in LLM generations. We contrast model\nediting with three similar but distinct approaches that pursue better defined\nobjectives: (1) retrieval-based architectures, which decouple factual memory\nfrom inference and linguistic capabilities embodied in LLMs; (2) concept\nerasure methods, which aim at preventing systemic bias in generated text; and\n(3) attribution methods, which aim at grounding generations into identified\ntextual sources. We argue that direct model editing cannot be trusted as a\nsystematic remedy for the disadvantages inherent to LLMs, and while it has\nproven potential in improving model explainability, it opens risks by\nreinforcing the notion that models can be trusted for factuality. We call for\ncautious promotion and application of model editing as part of the LLM\ndeployment process, and for responsibly limiting the use cases of LLMs to those\nnot relying on editing as a critical component.",
        "pdf_link": "https://arxiv.org/pdf/2310.11958v1.pdf"
    },
    {
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
        "authors": [
            "Aviv Slobodkin",
            "Omer Goldman",
            "Avi Caciularu",
            "Ido Dagan",
            "Shauli Ravfogel"
        ],
        "published": "2023-10-18T11:01:09Z",
        "summary": "Large language models (LLMs) have been shown to possess impressive\ncapabilities, while also raising crucial concerns about the faithfulness of\ntheir responses. A primary issue arising in this context is the management of\n(un)answerable queries by LLMs, which often results in hallucinatory behavior\ndue to overconfidence. In this paper, we explore the behavior of LLMs when\npresented with (un)answerable queries. We ask: do models represent the fact\nthat the question is (un)answerable when generating a hallucinatory answer? Our\nresults show strong indications that such models encode the answerability of an\ninput query, with the representation of the first decoded token often being a\nstrong indicator. These findings shed new light on the spatial organization\nwithin the latent representations of LLMs, unveiling previously unexplored\nfacets of these models. Moreover, they pave the way for the development of\nimproved decoding techniques with better adherence to factual generation,\nparticularly in scenarios where query (un)answerability is a concern.",
        "pdf_link": "https://arxiv.org/pdf/2310.11877v2.pdf"
    },
    {
        "title": "Enhancing Genetic Improvement Mutations Using Large Language Models",
        "authors": [
            "Alexander E. I. Brownlee",
            "James Callan",
            "Karine Even-Mendoza",
            "Alina Geiger",
            "Carol Hanna",
            "Justyna Petke",
            "Federica Sarro",
            "Dominik Sobania"
        ],
        "published": "2023-10-18T10:24:14Z",
        "summary": "Large language models (LLMs) have been successfully applied to software\nengineering tasks, including program repair. However, their application in\nsearch-based techniques such as Genetic Improvement (GI) is still largely\nunexplored. In this paper, we evaluate the use of LLMs as mutation operators\nfor GI to improve the search process. We expand the Gin Java GI toolkit to call\nOpenAI's API to generate edits for the JCodec tool. We randomly sample the\nspace of edits using 5 different edit types. We find that the number of patches\npassing unit tests is up to 75% higher with LLM-based edits than with standard\nInsert edits. Further, we observe that the patches found with LLMs are\ngenerally less diverse compared to standard edits. We ran GI with local search\nto find runtime improvements. Although many improving patches are found by\nLLM-enhanced GI, the best improving patch was found by standard GI.",
        "pdf_link": "https://arxiv.org/pdf/2310.19813v1.pdf"
    },
    {
        "title": "Solving the multiplication problem of a large language model system using a graph-based method",
        "authors": [
            "Turker Tuncer",
            "Sengul Dogan",
            "Mehmet Baygin",
            "Prabal Datta Barua",
            "Abdul Hafeez-Baig",
            "Ru-San Tan",
            "Subrata Chakraborty",
            "U. Rajendra Acharya"
        ],
        "published": "2023-10-18T08:02:00Z",
        "summary": "The generative pre-trained transformer (GPT)-based chatbot software ChatGPT\npossesses excellent natural language processing capabilities but is inadequate\nfor solving arithmetic problems, especially multiplication. Its GPT structure\nuses a computational graph for multiplication, which has limited accuracy\nbeyond simple multiplication operations. We developed a graph-based\nmultiplication algorithm that emulated human-like numerical operations by\nincorporating a 10k operator, where k represents the maximum power to base 10\nof the larger of two input numbers. Our proposed algorithm attained 100%\naccuracy for 1,000,000 large number multiplication tasks, effectively solving\nthe multiplication challenge of GPT-based and other large language models. Our\nwork highlights the importance of blending simple human insights into the\ndesign of artificial intelligence algorithms. Keywords: Graph-based\nmultiplication; ChatGPT; Multiplication problem",
        "pdf_link": "https://arxiv.org/pdf/2310.13016v1.pdf"
    },
    {
        "title": "Telecom AI Native Systems in the Age of Generative AI -- An Engineering Perspective",
        "authors": [
            "Ricardo Britto",
            "Timothy Murphy",
            "Massimo Iovene",
            "Leif Jonsson",
            "Melike Erol-Kantarci",
            "Benedek Kov√°cs"
        ],
        "published": "2023-10-18T07:55:54Z",
        "summary": "The rapid advancements in Artificial Intelligence (AI), particularly in\ngenerative AI and foundational models (FMs), have ushered in transformative\nchanges across various industries. Large language models (LLMs), a type of FM,\nhave demonstrated their prowess in natural language processing tasks and\ncontent generation, revolutionizing how we interact with software products and\nservices. This article explores the integration of FMs in the\ntelecommunications industry, shedding light on the concept of AI native telco,\nwhere AI is seamlessly woven into the fabric of telecom products. It delves\ninto the engineering considerations and unique challenges associated with\nimplementing FMs into the software life cycle, emphasizing the need for AI\nnative-first approaches. Despite the enormous potential of FMs, ethical,\nregulatory, and operational challenges require careful consideration,\nespecially in mission-critical telecom contexts. As the telecom industry seeks\nto harness the power of AI, a comprehensive understanding of these challenges\nis vital to thrive in a fiercely competitive market.",
        "pdf_link": "https://arxiv.org/pdf/2310.11770v1.pdf"
    },
    {
        "title": "MISAR: A Multimodal Instructional System with Augmented Reality",
        "authors": [
            "Jing Bi",
            "Nguyen Manh Nguyen",
            "Ali Vosoughi",
            "Chenliang Xu"
        ],
        "published": "2023-10-18T04:15:12Z",
        "summary": "Augmented reality (AR) requires the seamless integration of visual, auditory,\nand linguistic channels for optimized human-computer interaction. While\nauditory and visual inputs facilitate real-time and contextual user guidance,\nthe potential of large language models (LLMs) in this landscape remains largely\nuntapped. Our study introduces an innovative method harnessing LLMs to\nassimilate information from visual, auditory, and contextual modalities.\nFocusing on the unique challenge of task performance quantification in AR, we\nutilize egocentric video, speech, and context analysis. The integration of LLMs\nfacilitates enhanced state estimation, marking a step towards more adaptive AR\nsystems. Code, dataset, and demo will be available at\nhttps://github.com/nguyennm1024/misar.",
        "pdf_link": "https://arxiv.org/pdf/2310.11699v1.pdf"
    },
    {
        "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
        "authors": [
            "Xuhui Zhou",
            "Hao Zhu",
            "Leena Mathur",
            "Ruohong Zhang",
            "Haofei Yu",
            "Zhengyang Qi",
            "Louis-Philippe Morency",
            "Yonatan Bisk",
            "Daniel Fried",
            "Graham Neubig",
            "Maarten Sap"
        ],
        "published": "2023-10-18T02:27:01Z",
        "summary": "Humans are social beings; we pursue social goals in our daily interactions,\nwhich is a crucial aspect of social intelligence. Yet, AI systems' abilities in\nthis realm remain elusive. We present SOTOPIA, an open-ended environment to\nsimulate complex social interactions between artificial agents and evaluate\ntheir social intelligence. In our environment, agents role-play and interact\nunder a wide variety of scenarios; they coordinate, collaborate, exchange, and\ncompete with each other to achieve complex social goals. We simulate the\nrole-play interaction between LLM-based agents and humans within this task\nspace and evaluate their performance with a holistic evaluation framework\ncalled SOTOPIA-Eval. With SOTOPIA, we find significant differences between\nthese models in terms of their social intelligence, and we identify a subset of\nSOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.\nWe find that on this subset, GPT-4 achieves a significantly lower goal\ncompletion rate than humans and struggles to exhibit social commonsense\nreasoning and strategic communication skills. These findings demonstrate\nSOTOPIA's promise as a general platform for research on evaluating and\nimproving social intelligence in artificial agents.",
        "pdf_link": "https://arxiv.org/pdf/2310.11667v2.pdf"
    },
    {
        "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
        "authors": [
            "Linhao Luo",
            "Thuy-Trang Vu",
            "Dinh Phung",
            "Gholamreza Haffari"
        ],
        "published": "2023-10-18T00:20:50Z",
        "summary": "Previous studies have relied on existing question-answering benchmarks to\nevaluate the knowledge stored in large language models (LLMs). However, this\napproach has limitations regarding factual knowledge coverage, as it mostly\nfocuses on generic domains which may overlap with the pretraining data. This\npaper proposes a framework to systematically assess the factual knowledge of\nLLMs by leveraging knowledge graphs (KGs). Our framework automatically\ngenerates a set of questions and expected answers from the facts stored in a\ngiven KG, and then evaluates the accuracy of LLMs in answering these questions.\nWe systematically evaluate the state-of-the-art LLMs with KGs in generic and\nspecific domains. The experiment shows that ChatGPT is consistently the top\nperformer across all domains. We also find that LLMs performance depends on the\ninstruction finetuning, domain and question complexity and is prone to\nadversarial context.",
        "pdf_link": "https://arxiv.org/pdf/2310.11638v3.pdf"
    },
    {
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "authors": [
            "Arkil Patel",
            "Satwik Bhattamishra",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023-10-18T00:02:38Z",
        "summary": "Humans possess a remarkable ability to assign novel interpretations to\nlinguistic expressions, enabling them to learn new words and understand\ncommunity-specific connotations. However, Large Language Models (LLMs) have a\nknowledge cutoff and are costly to finetune repeatedly. Therefore, it is\ncrucial for LLMs to learn novel interpretations in-context. In this paper, we\nsystematically analyse the ability of LLMs to acquire novel interpretations\nusing in-context learning. To facilitate our study, we introduce MAGNIFICo, an\nevaluation suite implemented within a text-to-SQL semantic parsing framework\nthat incorporates diverse tokens and prompt settings to simulate real-world\ncomplexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a\nsurprisingly robust capacity for comprehending novel interpretations from\nnatural language descriptions as well as from discussions within long\nconversations. Nevertheless, our findings also highlight the need for further\nimprovements, particularly when interpreting unfamiliar words or when composing\nmultiple novel interpretations simultaneously in the same example.\nAdditionally, our analysis uncovers the semantic predispositions in LLMs and\nreveals the impact of recency bias for information presented in long contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.11634v1.pdf"
    },
    {
        "title": "Language Models as Zero-Shot Trajectory Generators",
        "authors": [
            "Teyun Kwon",
            "Norman Di Palo",
            "Edward Johns"
        ],
        "published": "2023-10-17T21:57:36Z",
        "summary": "Large Language Models (LLMs) have recently shown promise as high-level\nplanners for robots when given access to a selection of low-level skills.\nHowever, it is often assumed that LLMs do not possess sufficient knowledge to\nbe used for the low-level trajectories themselves. In this work, we address\nthis assumption thoroughly, and investigate if an LLM (GPT-4) can directly\npredict a dense sequence of end-effector poses for manipulation skills, when\ngiven access to only object detection and segmentation vision models. We study\nhow well a single task-agnostic prompt, without any in-context examples, motion\nprimitives, or external trajectory optimisers, can perform across 26 real-world\nlanguage-based tasks, such as \"open the bottle cap\" and \"wipe the plate with\nthe sponge\", and we investigate which design choices in this prompt are the\nmost effective. Our conclusions raise the assumed limit of LLMs for robotics,\nand we reveal for the first time that LLMs do indeed possess an understanding\nof low-level robot control sufficient for a range of common tasks, and that\nthey can additionally detect failures and then re-plan trajectories\naccordingly. Videos, code, and prompts are available at:\nhttps://www.robot-learning.uk/language-models-trajectory-generators.",
        "pdf_link": "https://arxiv.org/pdf/2310.11604v1.pdf"
    },
    {
        "title": "Automated Evaluation of Personalized Text Generation using Large Language Models",
        "authors": [
            "Yaqing Wang",
            "Jiepu Jiang",
            "Mingyang Zhang",
            "Cheng Li",
            "Yi Liang",
            "Qiaozhu Mei",
            "Michael Bendersky"
        ],
        "published": "2023-10-17T21:35:06Z",
        "summary": "Personalized text generation presents a specialized mechanism for delivering\ncontent that is specific to a user's personal context. While the research\nprogress in this area has been rapid, evaluation still presents a challenge.\nTraditional automated metrics such as BLEU and ROUGE primarily measure lexical\nsimilarity to human-written references, and are not able to distinguish\npersonalization from other subtle semantic aspects, thus falling short of\ncapturing the nuances of personalized generated content quality. On the other\nhand, human judgments are costly to obtain, especially in the realm of\npersonalized evaluation. Inspired by these challenges, we explore the use of\nlarge language models (LLMs) for evaluating personalized text generation, and\nexamine their ability to understand nuanced user context. We present AuPEL, a\nnovel evaluation method that distills three major semantic aspects of the\ngenerated text: personalization, quality and relevance, and automatically\nmeasures these aspects. To validate the effectiveness of AuPEL, we design\ncarefully controlled experiments and compare the accuracy of the evaluation\njudgments made by LLMs versus that of judgements made by human annotators, and\nconduct rigorous analyses of the consistency and sensitivity of the proposed\nmetric. We find that, compared to existing evaluation metrics, AuPEL not only\ndistinguishes and ranks models based on their personalization abilities more\naccurately, but also presents commendable consistency and efficiency for this\ntask. Our work suggests that using LLMs as the evaluators of personalized text\ngeneration is superior to traditional text similarity metrics, even though\ninteresting new challenges still remain.",
        "pdf_link": "https://arxiv.org/pdf/2310.11593v1.pdf"
    },
    {
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging",
        "authors": [
            "Joel Jang",
            "Seungone Kim",
            "Bill Yuchen Lin",
            "Yizhong Wang",
            "Jack Hessel",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi",
            "Yejin Choi",
            "Prithviraj Ammanabrolu"
        ],
        "published": "2023-10-17T20:22:13Z",
        "summary": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with general, aggregate human preferences, it is suboptimal for\nlearning diverse, individual perspectives. In this work, we study Reinforcement\nLearning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are\naligned to multiple (sometimes conflicting) preferences by modeling alignment\nas a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong\nsingle-objective baselines, we show that we can achieve personalized alignment\nby decomposing preferences into multiple dimensions. These dimensions are\ndefined based on personalizations that are declared as desirable by the user.\nIn this work, we show that they can be efficiently trained independently in a\ndistributed manner and combined effectively post-hoc through parameter merging.\nThe code is available at https://github.com/joeljang/RLPHF.",
        "pdf_link": "https://arxiv.org/pdf/2310.11564v1.pdf"
    },
    {
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "authors": [
            "Akari Asai",
            "Zeqiu Wu",
            "Yizhong Wang",
            "Avirup Sil",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-10-17T18:18:32Z",
        "summary": "Despite their remarkable capabilities, large language models (LLMs) often\nproduce responses containing factual inaccuracies due to their sole reliance on\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\nknowledge, decreases such issues. However, indiscriminately retrieving and\nincorporating a fixed number of retrieved passages, regardless of whether\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\ncan lead to unhelpful response generation. We introduce a new framework called\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\nquality and factuality through retrieval and self-reflection. Our framework\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\ngenerates and reflects on retrieved passages and its own generations using\nspecial tokens, called reflection tokens. Generating reflection tokens makes\nthe LM controllable during the inference phase, enabling it to tailor its\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\n13B parameters) significantly outperforms state-of-the-art LLMs and\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in\nimproving factuality and citation accuracy for long-form generations relative\nto these models.",
        "pdf_link": "https://arxiv.org/pdf/2310.11511v1.pdf"
    },
    {
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "authors": [
            "Myra Cheng",
            "Tiziano Piccardi",
            "Diyi Yang"
        ],
        "published": "2023-10-17T18:00:25Z",
        "summary": "Recent work has aimed to capture nuances of human behavior by using LLMs to\nsimulate responses from particular demographics in settings like social science\nexperiments and public opinion surveys. However, there are currently no\nestablished ways to discuss or evaluate the quality of such LLM simulations.\nMoreover, there is growing concern that these LLM simulations are flattened\ncaricatures of the personas that they aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating stereotypes. To bridge these\ngaps, we present CoMPosT, a framework to characterize LLM simulations using\nfour dimensions: Context, Model, Persona, and Topic. We use this framework to\nmeasure open-ended LLM simulations' susceptibility to caricature, defined via\ntwo criteria: individuation and exaggeration. We evaluate the level of\ncaricature in scenarios from existing work on LLM simulations. We find that for\nGPT-4, simulations of certain demographics (political and marginalized groups)\nand topics (general, uncontroversial) are highly susceptible to caricature.",
        "pdf_link": "https://arxiv.org/pdf/2310.11501v1.pdf"
    },
    {
        "title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
        "authors": [
            "Philipp Schoenegger",
            "Peter S. Park"
        ],
        "published": "2023-10-17T17:58:17Z",
        "summary": "Accurately predicting the future would be an important milestone in the\ncapabilities of artificial intelligence. However, research on the ability of\nlarge language models to provide probabilistic predictions about future events\nremains nascent. To empirically test this ability, we enrolled OpenAI's\nstate-of-the-art large language model, GPT-4, in a three-month forecasting\ntournament hosted on the Metaculus platform. The tournament, running from July\nto October 2023, attracted 843 participants and covered diverse topics\nincluding Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict.\nFocusing on binary forecasts, we show that GPT-4's probabilistic forecasts are\nsignificantly less accurate than the median human-crowd forecasts. We find that\nGPT-4's forecasts did not significantly differ from the no-information\nforecasting strategy of assigning a 50% probability to every question. We\nexplore a potential explanation, that GPT-4 might be predisposed to predict\nprobabilities close to the midpoint of the scale, but our data do not support\nthis hypothesis. Overall, we find that GPT-4 significantly underperforms in\nreal-world predictive tasks compared to median human-crowd forecasts. A\npotential explanation for this underperformance is that in real-world\nforecasting tournaments, the true answers are genuinely unknown at the time of\nprediction; unlike in other benchmark tasks like professional exams or time\nseries forecasting, where strong performance may at least partly be due to the\nanswers being memorized from the training data. This makes real-world\nforecasting tournaments an ideal environment for testing the generalized\nreasoning and prediction capabilities of artificial intelligence going forward.",
        "pdf_link": "https://arxiv.org/pdf/2310.13014v1.pdf"
    },
    {
        "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
        "authors": [
            "Rui Wen",
            "Tianhao Wang",
            "Michael Backes",
            "Yang Zhang",
            "Ahmed Salem"
        ],
        "published": "2023-10-17T17:03:00Z",
        "summary": "Large Language Models (LLMs) are powerful tools for natural language\nprocessing, enabling novel applications and user experiences. However, to\nachieve optimal performance, LLMs often require adaptation with private data,\nwhich poses privacy and security challenges. Several techniques have been\nproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),\nSoft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative\nprivacy and security properties have not been systematically investigated. In\nthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL\nagainst three types of well-established attacks: membership inference, which\nexposes data leakage (privacy); backdoor, which injects malicious behavior\n(security); and model stealing, which can violate intellectual property\n(privacy and security). Our results show that there is no silver bullet for\nprivacy and security in LLM adaptation and each technique has different\nstrengths and weaknesses.",
        "pdf_link": "https://arxiv.org/pdf/2310.11397v1.pdf"
    },
    {
        "title": "DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations",
        "authors": [
            "Yazhou Zhang",
            "Mengyao Wang",
            "Youxi Wu",
            "Prayag Tiwari",
            "Qiuchi Li",
            "Benyou Wang",
            "Jing Qin"
        ],
        "published": "2023-10-17T16:15:34Z",
        "summary": "Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.",
        "pdf_link": "https://arxiv.org/pdf/2310.11374v4.pdf"
    },
    {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
        ],
        "published": "2023-10-17T15:03:30Z",
        "summary": "As large language models (LLMs) are adopted as a fundamental component of\nlanguage technologies, it is crucial to accurately characterize their\nperformance. Because choices in prompt design can strongly influence model\nbehavior, this design process is critical in effectively using any modern\npre-trained generative language model. In this work, we focus on LLM\nsensitivity to a quintessential class of meaning-preserving design choices:\nprompt formatting. We find that several widely used open-source LLMs are\nextremely sensitive to subtle changes in prompt formatting in few-shot\nsettings, with performance differences of up to 76 accuracy points when\nevaluated using LLaMA-2-13B. Sensitivity remains even when increasing model\nsize, the number of few-shot examples, or performing instruction tuning. Our\nanalysis suggests that work evaluating LLMs with prompting-based methods would\nbenefit from reporting a range of performance across plausible prompt formats,\ninstead of the currently-standard practice of reporting performance on a single\nformat. We also show that format performance only weakly correlates between\nmodels, which puts into question the methodological validity of comparing\nmodels with an arbitrarily chosen, fixed prompt format. To facilitate\nsystematic analysis we propose FormatSpread, an algorithm that rapidly\nevaluates a sampled set of plausible prompt formats for a given task, and\nreports the interval of expected performance without accessing model weights.\nFurthermore, we present a suite of analyses that characterize the nature of\nthis sensitivity, including exploring the influence of particular atomic\nperturbations and the internal representation of particular formats.",
        "pdf_link": "https://arxiv.org/pdf/2310.11324v1.pdf"
    },
    {
        "title": "Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue",
        "authors": [
            "Shiwei Zhang",
            "Mingfang Wu",
            "Xiuzhen Zhang"
        ],
        "published": "2023-10-17T14:52:33Z",
        "summary": "In support of open and reproducible research, there has been a rapidly\nincreasing number of datasets made available for research. As the availability\nof datasets increases, it becomes more important to have quality metadata for\ndiscovering and reusing them. Yet, it is a common issue that datasets often\nlack quality metadata due to limited resources for data curation. Meanwhile,\ntechnologies such as artificial intelligence and large language models (LLMs)\nare progressing rapidly. Recently, systems based on these technologies, such as\nChatGPT, have demonstrated promising capabilities for certain data curation\ntasks. This paper proposes to leverage LLMs for cost-effective annotation of\nsubject metadata through the LLM-based in-context learning. Our method employs\nGPT-3.5 with prompts designed for annotating subject metadata, demonstrating\npromising performance in automatic metadata annotation. However, models based\non in-context learning cannot acquire discipline-specific rules, resulting in\nlower performance in several categories. This limitation arises from the\nlimited contextual information available for subject inference. To the best of\nour knowledge, we are introducing, for the first time, an in-context learning\nmethod that harnesses large language models for automated subject metadata\nannotation.",
        "pdf_link": "https://arxiv.org/pdf/2310.11318v1.pdf"
    },
    {
        "title": "Generative error correction for code-switching speech recognition using large language models",
        "authors": [
            "Chen Chen",
            "Yuchen Hu",
            "Chao-Han Huck Yang",
            "Hexin Liu",
            "Sabato Marco Siniscalchi",
            "Eng Siong Chng"
        ],
        "published": "2023-10-17T14:49:48Z",
        "summary": "Code-switching (CS) speech refers to the phenomenon of mixing two or more\nlanguages within the same sentence. Despite the recent advances in automatic\nspeech recognition (ASR), CS-ASR is still a challenging task ought to the\ngrammatical structure complexity of the phenomenon and the data scarcity of\nspecific training corpus. In this work, we propose to leverage large language\nmodels (LLMs) and lists of hypotheses generated by an ASR to address the CS\nproblem. Specifically, we first employ multiple well-trained ASR models for\nN-best hypotheses generation, with the aim of increasing the diverse and\ninformative elements in the set of hypotheses. Next, we utilize the LLMs to\nlearn the hypotheses-to-transcription (H2T) mapping by adding a trainable\nlow-rank adapter. Such a generative error correction (GER) method directly\npredicts the accurate transcription according to its expert linguistic\nknowledge and N-best hypotheses, resulting in a paradigm shift from the\ntraditional language model rescoring or error correction techniques.\nExperimental evidence demonstrates that GER significantly enhances CS-ASR\naccuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show\nremarkable data efficiency for H2T learning, providing a potential solution to\nthe data scarcity problem of CS-ASR in low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2310.13013v1.pdf"
    },
    {
        "title": "Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges",
        "authors": [
            "Thilo Spinner",
            "Rebecca Kehlbeck",
            "Rita Sevastjanova",
            "Tobias St√§hle",
            "Daniel A. Keim",
            "Oliver Deussen",
            "Andreas Spitz",
            "Mennatallah El-Assady"
        ],
        "published": "2023-10-17T13:20:16Z",
        "summary": "The growing popularity of generative language models has amplified interest\nin interactive methods to guide model outputs. Prompt refinement is considered\none of the most effective means to influence output among these methods. We\nidentify several challenges associated with prompting large language models,\ncategorized into data- and model-specific, linguistic, and socio-linguistic\nchallenges. A comprehensive examination of model outputs, including runner-up\ncandidates and their corresponding probabilities, is needed to address these\nissues. The beam search tree, the prevalent algorithm to sample model outputs,\ncan inherently supply this information. Consequently, we introduce an\ninteractive visual method for investigating the beam search tree, facilitating\nanalysis of the decisions made by the model during generation. We\nquantitatively show the value of exposing the beam search tree and present five\ndetailed analysis scenarios addressing the identified challenges. Our\nmethodology validates existing results and offers additional insights.",
        "pdf_link": "https://arxiv.org/pdf/2310.11252v1.pdf"
    },
    {
        "title": "Entity Matching using Large Language Models",
        "authors": [
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "published": "2023-10-17T13:12:32Z",
        "summary": "Entity Matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. It is a central step in most data integration\npipelines and an enabler for many e-commerce applications which require to\nmatch products offers from different vendors. State-of-the-art entity matching\nmethods rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two\nmajor drawbacks of these models for entity matching are that (i) the models\nrequire significant amounts of task-specific training data and (ii) the\nfine-tuned models are not robust concerning out-of-distribution entities. We\ninvestigate using generative large language models (LLMs) for entity matching\nas a less task-specific training data dependent and more robust alternative to\nPLM-based matchers. Our study covers hosted LLMs as well as open-source LLMs\nwhich can be run locally. We evaluate these models in a zero-shot scenario as\nwell as a scenario where task-specific training data is available. We compare\ndifferent prompt designs as well as the prompt sensitivity of the models and\nshow that there is no single best prompt but the prompt is akin to a\nhyperparameter that needs to be estimated for each model/dataset combination.\nWe further investigate (i) the selection of in-context demonstrations, (ii) the\ngeneration of matching rules, as well as (iii) fine-tuning a hosted LLM using\nthe same pool of training data. Our experiments show that the best LLMs require\nno or only a few training examples to reach a similar performance as fine-tuned\nPLMs. They further exhibit a higher robustness to unseen entities, which makes\nthem especially suited to use cases where no training data is available. We\nshow that for use cases that do not allow data to be shared with third parties,\nopen-source LLMs can be a viable alternative to hosted LLMs given that a small\namount of training data or matching knowledge...",
        "pdf_link": "https://arxiv.org/pdf/2310.11244v2.pdf"
    },
    {
        "title": "Watermarking LLMs with Weight Quantization",
        "authors": [
            "Linyang Li",
            "Botian Jiang",
            "Pengyu Wang",
            "Ke Ren",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023-10-17T13:06:59Z",
        "summary": "Abuse of large language models reveals high risks as large language models\nare being deployed at an astonishing speed. It is important to protect the\nmodel weights to avoid malicious usage that violates licenses of open-source\nlarge language models. This paper proposes a novel watermarking strategy that\nplants watermarks in the quantization process of large language models without\npre-defined triggers during inference. The watermark works when the model is\nused in the fp32 mode and remains hidden when the model is quantized to int8,\nin this way, the users can only inference the model without further supervised\nfine-tuning of the model. We successfully plant the watermark into open-source\nlarge language model weights including GPT-Neo and LLaMA. We hope our proposed\nmethod can provide a potential direction for protecting model weights in the\nera of large language model applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.11237v1.pdf"
    },
    {
        "title": "The Quo Vadis of the Relationship between Language and Large Language Models",
        "authors": [
            "Evelina Leivada",
            "Vittoria Dentella",
            "Elliot Murphy"
        ],
        "published": "2023-10-17T10:54:24Z",
        "summary": "In the field of Artificial (General) Intelligence (AI), the several recent\nadvancements in Natural language processing (NLP) activities relying on Large\nLanguage Models (LLMs) have come to encourage the adoption of LLMs as\nscientific models of language. While the terminology employed for the\ncharacterization of LLMs favors their embracing as such, it is not clear that\nthey are in a place to offer insights into the target system they seek to\nrepresent. After identifying the most important theoretical and empirical risks\nbrought about by the adoption of scientific models that lack transparency, we\ndiscuss LLMs relating them to every scientific model's fundamental components:\nthe object, the medium, the meaning and the user. We conclude that, at their\ncurrent stage of development, LLMs hardly offer any explanations for language,\nand then we provide an outlook for more informative future research directions\non this topic.",
        "pdf_link": "https://arxiv.org/pdf/2310.11146v1.pdf"
    },
    {
        "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
        "authors": [
            "Arno Candel",
            "Jon McKinney",
            "Philipp Singer",
            "Pascal Pfeiffer",
            "Maximilian Jeblick",
            "Chun Ming Lee",
            "Marcos V. Conde"
        ],
        "published": "2023-10-17T09:40:58Z",
        "summary": "Large Language Models (LLMs) represent a revolution in AI. However, they also\npose many significant risks, such as the presence of biased, private,\ncopyrighted or harmful text. For this reason we need open, transparent and safe\nsolutions. We introduce a complete open-source ecosystem for developing and\ntesting LLMs. The goal of this project is to boost open alternatives to\nclosed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of\ndiverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI\ndesigned for efficient fine-tuning, evaluation, and deployment of LLMs using\nthe most recent state-of-the-art techniques. Our code and models are fully\nopen-source. We believe this work helps to boost AI development and make it\nmore accessible, efficient and trustworthy. The demo is available at:\nhttps://gpt.h2o.ai/",
        "pdf_link": "https://arxiv.org/pdf/2310.13012v2.pdf"
    },
    {
        "title": "Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models",
        "authors": [
            "Hsuan Su",
            "Cheng-Chu Cheng",
            "Hua Farn",
            "Shachi H Kumar",
            "Saurav Sahay",
            "Shang-Tse Chen",
            "Hung-yi Lee"
        ],
        "published": "2023-10-17T08:56:04Z",
        "summary": "Recently, researchers have made considerable improvements in dialogue systems\nwith the progress of large language models (LLMs) such as ChatGPT and GPT-4.\nThese LLM-based chatbots encode the potential biases while retaining\ndisparities that can harm humans during interactions. The traditional biases\ninvestigation methods often rely on human-written test cases. However, these\ntest cases are usually expensive and limited. In this work, we propose a\nfirst-of-its-kind method that automatically generates test cases to detect\nLLMs' potential gender bias. We apply our method to three well-known LLMs and\nfind that the generated test cases effectively identify the presence of biases.\nTo address the biases identified, we propose a mitigation strategy that uses\nthe generated test cases as demonstrations for in-context learning to\ncircumvent the need for parameter fine-tuning. The experimental results show\nthat LLMs generate fairer responses with the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2310.11079v1.pdf"
    },
    {
        "title": "Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning",
        "authors": [
            "Shitong Duan",
            "Xiaoyuan Yi",
            "Peng Zhang",
            "Tun Lu",
            "Xing Xie",
            "Ning Gu"
        ],
        "published": "2023-10-17T07:42:40Z",
        "summary": "Large Language Models (LLMs) have made unprecedented breakthroughs, yet their\nincreasing integration into everyday life might raise societal risks due to\ngenerated unethical content. Despite extensive study on specific issues like\nbias, the intrinsic values of LLMs remain largely unexplored from a moral\nphilosophy perspective. This work delves into ethical values utilizing Moral\nFoundation Theory. Moving beyond conventional discriminative evaluations with\npoor reliability, we propose DeNEVIL, a novel prompt generation algorithm\ntailored to dynamically exploit LLMs' value vulnerabilities and elicit the\nviolation of ethics in a generative manner, revealing their underlying value\ninclinations. On such a basis, we construct MoralPrompt, a high-quality dataset\ncomprising 2,397 prompts covering 500+ value principles, and then benchmark the\nintrinsic values across a spectrum of LLMs. We discovered that most models are\nessentially misaligned, necessitating further ethical value alignment. In\nresponse, we develop VILMO, an in-context alignment method that substantially\nenhances the value compliance of LLM outputs by learning to generate\nappropriate value instructions, outperforming existing competitors. Our methods\nare suitable for black-box and open-source models, offering a promising initial\nstep in studying the ethical values of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.11053v3.pdf"
    },
    {
        "title": "Core Building Blocks: Next Gen Geo Spatial GPT Application",
        "authors": [
            "Ashley Fernandez",
            "Swaraj Dube"
        ],
        "published": "2023-10-17T06:59:31Z",
        "summary": "This paper proposes MapGPT which is a novel approach that integrates the\ncapabilities of language models, specifically large language models (LLMs),\nwith spatial data processing techniques. This paper introduces MapGPT, which\naims to bridge the gap between natural language understanding and spatial data\nanalysis by highlighting the relevant core building blocks. By combining the\nstrengths of LLMs and geospatial analysis, MapGPT enables more accurate and\ncontextually aware responses to location-based queries. The proposed\nmethodology highlights building LLMs on spatial and textual data, utilizing\ntokenization and vector representations specific to spatial information. The\npaper also explores the challenges associated with generating spatial vector\nrepresentations. Furthermore, the study discusses the potential of\ncomputational capabilities within MapGPT, allowing users to perform geospatial\ncomputations and obtain visualized outputs. Overall, this research paper\npresents the building blocks and methodology of MapGPT, highlighting its\npotential to enhance spatial data understanding and generation in natural\nlanguage processing applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.11029v2.pdf"
    },
    {
        "title": "Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation",
        "authors": [
            "Tomohito Kasahara",
            "Daisuke Kawahara"
        ],
        "published": "2023-10-17T06:53:00Z",
        "summary": "Automatic evaluation of text generation is essential for improving the\naccuracy of generation tasks. In light of the current trend towards\nincreasingly larger decoder-based language models, we investigate automatic\nevaluation methods based on such models for text generation. This paper\ncompares various methods, including tuning with encoder-based models and large\nlanguage models under equal conditions, on two different tasks, machine\ntranslation evaluation and semantic textual similarity, in two languages,\nJapanese and English. Experimental results show that compared to the tuned\nencoder-based models, the tuned decoder-based models perform poorly. The\nanalysis of the causes for this suggests that the decoder-based models focus on\nsurface word sequences and do not capture meaning. It is also revealed that\nin-context learning of very large decoder-based models such as ChatGPT makes it\ndifficult to identify fine-grained semantic differences.",
        "pdf_link": "https://arxiv.org/pdf/2310.11026v1.pdf"
    },
    {
        "title": "EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset",
        "authors": [
            "Hang Yin",
            "Pinren Lu",
            "Ziang Li",
            "Bin Sun",
            "Kan Li"
        ],
        "published": "2023-10-17T03:28:29Z",
        "summary": "The need for high-quality data has been a key issue hindering the research of\ndialogue tasks. Recent studies try to build datasets through manual, web\ncrawling, and large pre-trained models. However, man-made data is expensive and\ndata collected from the internet often includes generic responses, meaningless\nstatements, and toxic dialogues. Automatic data generation through large models\nis a cost-effective method, but for open-domain multimodal dialogue tasks,\nthere are still three drawbacks: 1) There is currently no open-source large\nmodel that can accept multimodal input; 2) The content generated by the model\nlacks interpretability; 3) The generated data is usually difficult to quality\ncontrol and require extensive resource to collect. To alleviate the significant\nhuman and resource expenditure in data collection, we propose a Multimodal Data\nConstruction Framework (MDCF). MDCF designs proper prompts to spur the\nlarge-scale pre-trained language model to generate well-formed and satisfactory\ncontent. Additionally, MDCF also automatically provides explanation for a given\nimage and its corresponding dialogue, which can provide a certain degree of\ninterpretability and facilitate manual follow-up quality inspection. Based on\nthis, we release an Explanatory Multimodal Open-Domain dialogue dataset\n(EXMODD). Experiments indicate a positive correlation between the model's\nability to generate accurate understandings and high-quality responses. Our\ncode and data can be found at https://github.com/poplpr/EXMODD.",
        "pdf_link": "https://arxiv.org/pdf/2310.10967v1.pdf"
    },
    {
        "title": "Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models",
        "authors": [
            "Huiming Wang",
            "Liying Cheng",
            "Zhaodonghui Li",
            "De Wen Soh",
            "Lidong Bing"
        ],
        "published": "2023-10-17T03:21:43Z",
        "summary": "Contrastive learning has been proven to be effective in learning better\nsentence representations. However, to train a contrastive learning model, large\nnumbers of labeled sentences are required to construct positive and negative\npairs explicitly, such as those in natural language inference (NLI) datasets.\nUnfortunately, acquiring sufficient high-quality labeled data can be both\ntime-consuming and resource-intensive, leading researchers to focus on\ndeveloping methods for learning unsupervised sentence representations. As there\nis no clear relationship between these unstructured randomly-sampled sentences,\nbuilding positive and negative pairs over them is tricky and problematic. To\ntackle these challenges, in this paper, we propose SemCSR, a semantic-aware\ncontrastive sentence representation framework. By leveraging the generation and\nevaluation capabilities of large language models (LLMs), we can automatically\nconstruct a high-quality NLI-style corpus without any human annotation, and\nfurther incorporate the generated sentence pairs into learning a contrastive\nsentence representation model. Extensive experiments and comprehensive analyses\ndemonstrate the effectiveness of our proposed framework for learning a better\nsentence representation with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10962v1.pdf"
    },
    {
        "title": "NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain",
        "authors": [
            "Anurag Acharya",
            "Sai Munikoti",
            "Aaron Hellinger",
            "Sara Smith",
            "Sridevi Wagle",
            "Sameera Horawalavithana"
        ],
        "published": "2023-10-17T01:27:20Z",
        "summary": "As LLMs have become increasingly popular, they have been used in almost every\nfield. But as the application for LLMs expands from generic fields to narrow,\nfocused science domains, there exists an ever-increasing gap in ways to\nevaluate their efficacy in those fields. For the benchmarks that do exist, a\nlot of them focus on questions that don't require proper understanding of the\nsubject in question. In this paper, we present NuclearQA, a human-made\nbenchmark of 100 questions to evaluate language models in the nuclear domain,\nconsisting of a varying collection of questions that have been specifically\ndesigned by experts to test the abilities of language models. We detail our\napproach and show how the mix of several types of questions makes our benchmark\nuniquely capable of evaluating models in the nuclear domain. We also present\nour own evaluation metric for assessing LLM's performances due to the\nlimitations of existing ones. Our experiments on state-of-the-art models\nsuggest that even the best LLMs perform less than satisfactorily on our\nbenchmark, demonstrating the scientific knowledge gap of existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10920v1.pdf"
    },
    {
        "title": "Unlocking Emergent Modularity in Large Language Models",
        "authors": [
            "Zihan Qiu",
            "Zeyu Huang",
            "Jie Fu"
        ],
        "published": "2023-10-17T01:02:32Z",
        "summary": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic\nmodels. Existing MNNs are generally $\\textit{explicit}$: their modular\narchitectures are pre-defined, with individual modules expected to implement\ndistinct functions. Recent works reveal that there exists $\\textit{implicit}$\nmodularity in standard pre-trained transformers, namely $\\textit{Emergent\nModularity}$. They indicate that such modular structures spontaneously exhibit\nduring the early pre-training phase. Despite the benefits of modularity, most\nLanguage Models (LMs) are still treated as monolithic models in the pre-train\nand fine-tune paradigm, with their emergent modularity locked and\nunderutilized. In this work, focusing on unlocking the emergent modularity in\nLMs, we showcase that standard LMs could be fine-tuned as their\nMixture-of-Expert (MoEs) counterparts without introducing any extra parameters.\nSuch MoEs are derived from emergent modularity and are referred to as Emergent\nMoEs (EMoE). Our experiments demonstrate that fine-tuning EMoE effectively\nimproves downstream in-domain and out-of-domain generalization compared with\nvanilla fine-tuning. Our analysis and ablation studies further illustrate that\nit is robust to various configurations and can scale up to Large Language\nModels (i.e., Llama2-7B and Llama-30B). Code is available at\nhttps://github.com/qiuzh20/EMoE.",
        "pdf_link": "https://arxiv.org/pdf/2310.10908v2.pdf"
    },
    {
        "title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks",
        "authors": [
            "Jiaying Wu",
            "Bryan Hooi"
        ],
        "published": "2023-10-16T21:05:12Z",
        "summary": "It is commonly perceived that online fake news and reliable news exhibit\nstark differences in writing styles, such as the use of sensationalist versus\nobjective language. However, we emphasize that style-related features can also\nbe exploited for style-based attacks. Notably, the rise of powerful Large\nLanguage Models (LLMs) has enabled malicious users to mimic the style of\ntrustworthy news outlets at minimal cost. Our analysis reveals that\nLLM-camouflaged fake news content leads to substantial performance degradation\nof state-of-the-art text-based detectors (up to 38% decrease in F1 Score),\nposing a significant challenge for automated detection in online ecosystems. To\naddress this, we introduce SheepDog, a style-agnostic fake news detector robust\nto news writing styles. SheepDog achieves this adaptability through\nLLM-empowered news reframing, which customizes each article to match different\nwriting styles using style-oriented reframing prompts. By employing\nstyle-agnostic training, SheepDog enhances its resilience to stylistic\nvariations by maximizing prediction consistency across these diverse\nreframings. Furthermore, SheepDog extracts content-focused veracity\nattributions from LLMs, where the news content is evaluated against a set of\nfact-checking rationales. These attributions provide supplementary information\nand potential interpretability that assist veracity prediction. On three\nbenchmark datasets, empirical results show that SheepDog consistently yields\nsignificant improvements over competitive baselines and enhances robustness\nagainst LLM-empowered style attacks.",
        "pdf_link": "https://arxiv.org/pdf/2310.10830v1.pdf"
    },
    {
        "title": "Vision and Language Navigation in the Real World via Online Visual Language Mapping",
        "authors": [
            "Chengguang Xu",
            "Hieu T. Nguyen",
            "Christopher Amato",
            "Lawson L. S. Wong"
        ],
        "published": "2023-10-16T20:44:09Z",
        "summary": "Navigating in unseen environments is crucial for mobile robots. Enhancing\nthem with the ability to follow instructions in natural language will further\nimprove navigation efficiency in unseen cases. However, state-of-the-art (SOTA)\nvision-and-language navigation (VLN) methods are mainly evaluated in\nsimulation, neglecting the complex and noisy real world. Directly transferring\nSOTA navigation policies trained in simulation to the real world is challenging\ndue to the visual domain gap and the absence of prior knowledge about unseen\nenvironments. In this work, we propose a novel navigation framework to address\nthe VLN task in the real world. Utilizing the powerful foundation models, the\nproposed framework includes four key components: (1) an LLMs-based instruction\nparser that converts the language instruction into a sequence of pre-defined\nmacro-action descriptions, (2) an online visual-language mapper that builds a\nreal-time visual-language map to maintain a spatial and semantic understanding\nof the unseen environment, (3) a language indexing-based localizer that grounds\neach macro-action description into a waypoint location on the map, and (4) a\nDD-PPO-based local controller that predicts the action. We evaluate the\nproposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment.\nWithout any fine-tuning, our pipeline significantly outperforms the SOTA VLN\nbaseline in the real world.",
        "pdf_link": "https://arxiv.org/pdf/2310.10822v1.pdf"
    },
    {
        "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
        "authors": [
            "Bhaskarjit Sarmah",
            "Tianjie Zhu",
            "Dhagash Mehta",
            "Stefano Pasquali"
        ],
        "published": "2023-10-16T18:45:38Z",
        "summary": "For a financial analyst, the question and answer (Q\\&A) segment of the\ncompany financial report is a crucial piece of information for various analysis\nand investment decisions. However, extracting valuable insights from the Q\\&A\nsection has posed considerable challenges as the conventional methods such as\ndetailed reading and note-taking lack scalability and are susceptible to human\nerrors, and Optical Character Recognition (OCR) and similar techniques\nencounter difficulties in accurately processing unstructured transcript text,\noften missing subtle linguistic nuances that drive investor decisions. Here, we\ndemonstrate the utilization of Large Language Models (LLMs) to efficiently and\nrapidly extract information from earnings report transcripts while ensuring\nhigh accuracy transforming the extraction process as well as reducing\nhallucination by combining retrieval-augmented generation technique as well as\nmetadata. We evaluate the outcomes of various LLMs with and without using our\nproposed approach based on various objective metrics for evaluating Q\\&A\nsystems, and empirically demonstrate superiority of our method.",
        "pdf_link": "https://arxiv.org/pdf/2310.10760v1.pdf"
    },
    {
        "title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes",
        "authors": [
            "Rose E. Wang",
            "Qingyang Zhang",
            "Carly Robinson",
            "Susanna Loeb",
            "Dorottya Demszky"
        ],
        "published": "2023-10-16T17:59:50Z",
        "summary": "Scaling high-quality tutoring remains a major challenge in education. Due to\ngrowing demand, many platforms employ novice tutors who, unlike experienced\neducators, struggle to address student mistakes and thus fail to seize prime\nlearning opportunities. Our work explores the potential of large language\nmodels (LLMs) to close the novice-expert knowledge gap in remediating math\nmistakes. We contribute Bridge, a method that uses cognitive task analysis to\ntranslate an expert's latent thought process into a decision-making model for\nremediation. This involves an expert identifying (A) the student's error, (B) a\nremediation strategy, and (C) their intention before generating a response. We\nconstruct a dataset of 700 real tutoring conversations, annotated by experts\nwith their decisions. We evaluate state-of-the-art LLMs on our dataset and find\nthat the expert's decision-making model is critical for LLMs to close the gap:\nresponses from GPT4 with expert decisions (e.g., \"simplify the problem\") are\n+76% more preferred than without. Additionally, context-sensitive decisions are\ncritical to closing pedagogical gaps: random decisions decrease GPT4's response\nquality by -97% than expert decisions. Our work shows the potential of\nembedding expert thought processes in LLM generations to enhance their\ncapability to bridge novice-expert knowledge gaps. Our dataset and code can be\nfound at: \\url{https://github.com/rosewang2008/bridge}.",
        "pdf_link": "https://arxiv.org/pdf/2310.10648v3.pdf"
    },
    {
        "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
        "authors": [
            "Hanan Gani",
            "Shariq Farooq Bhat",
            "Muzammal Naseer",
            "Salman Khan",
            "Peter Wonka"
        ],
        "published": "2023-10-16T17:57:37Z",
        "summary": "Diffusion-based generative models have significantly advanced text-to-image\ngeneration but encounter challenges when processing lengthy and intricate text\nprompts describing complex scenes with multiple objects. While excelling in\ngenerating images from short, single-object descriptions, these models often\nstruggle to faithfully capture all the nuanced details within longer and more\nelaborate textual inputs. In response, we present a novel approach leveraging\nLarge Language Models (LLMs) to extract critical components from text prompts,\nincluding bounding box coordinates for foreground objects, detailed textual\ndescriptions for individual objects, and a succinct background context. These\ncomponents form the foundation of our layout-to-image generation model, which\noperates in two phases. The initial Global Scene Generation utilizes object\nlayouts and background context to create an initial scene but often falls short\nin faithfully representing object characteristics as specified in the prompts.\nTo address this limitation, we introduce an Iterative Refinement Scheme that\niteratively evaluates and refines box-level content to align them with their\ntextual descriptions, recomposing objects as needed to ensure consistency. Our\nevaluation on complex prompts featuring multiple objects demonstrates a\nsubstantial improvement in recall compared to baseline diffusion models. This\nis further validated by a user study, underscoring the efficacy of our approach\nin generating coherent and detailed scenes from intricate textual inputs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10640v2.pdf"
    },
    {
        "title": "\"Mistakes Help Us Grow\": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
        "authors": [
            "Kunal Handa",
            "Margaret Clapper",
            "Jessica Boyle",
            "Rose E Wang",
            "Diyi Yang",
            "David S Yeager",
            "Dorottya Demszky"
        ],
        "published": "2023-10-16T17:56:07Z",
        "summary": "Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing\nthat one's skills can be improved over time--has been shown to significantly\nreduce disparities in academic achievement and enhance students' learning\noutcomes. Although teachers espouse growth mindset principles, most find it\ndifficult to adopt GMSL in their practice due the lack of effective coaching in\nthis area. We explore whether large language models (LLMs) can provide\nautomated, personalized coaching to support teachers' use of GMSL. We establish\nan effective coaching tool to reframe unsupportive utterances to GMSL by\ndeveloping (i) a parallel dataset containing GMSL-trained teacher reframings of\nunsupportive statements with an accompanying annotation guide, (ii) a GMSL\nprompt framework to revise teachers' unsupportive language, and (iii) an\nevaluation framework grounded in psychological theory for evaluating GMSL with\nthe help of students and teachers. We conduct a large-scale evaluation\ninvolving 174 teachers and 1,006 students, finding that both teachers and\nstudents perceive GMSL-trained teacher and model reframings as more effective\nin fostering a growth mindset and promoting challenge-seeking behavior, among\nother benefits. We also find that model-generated reframings outperform those\nfrom the GMSL-trained teachers. These results show promise for harnessing LLMs\nto provide automated GMSL feedback for teachers and, more broadly, LLMs'\npotentiality for supporting students' learning in the classroom. Our findings\nalso demonstrate the benefit of large-scale human evaluations when applying\nLLMs in educational domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.10637v1.pdf"
    },
    {
        "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
        "authors": [
            "Odhran O'Donoghue",
            "Aleksandar Shtedritski",
            "John Ginger",
            "Ralph Abboud",
            "Ali Essa Ghareeb",
            "Justin Booth",
            "Samuel G Rodriques"
        ],
        "published": "2023-10-16T17:54:20Z",
        "summary": "The ability to automatically generate accurate protocols for scientific\nexperiments would represent a major step towards the automation of science.\nLarge Language Models (LLMs) have impressive capabilities on a wide range of\ntasks, such as question answering and the generation of coherent text and code.\nHowever, LLMs can struggle with multi-step problems and long-term planning,\nwhich are crucial for designing scientific experiments. Moreover, evaluation of\nthe accuracy of scientific protocols is challenging, because experiments can be\ndescribed correctly in many different ways, require expert knowledge to\nevaluate, and cannot usually be executed automatically. Here we present an\nautomatic evaluation framework for the task of planning experimental protocols,\nand we introduce BioProt: a dataset of biology protocols with corresponding\npseudocode representations. To measure performance on generating scientific\nprotocols, we use an LLM to convert a natural language protocol into\npseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode\nfrom a high-level description and a list of admissible pseudocode functions. We\nevaluate GPT-3 and GPT-4 on this task and explore their robustness. We\nexternally validate the utility of pseudocode representations of text by\ngenerating accurate novel protocols using retrieved pseudocode, and we run a\ngenerated protocol successfully in our biological laboratory. Our framework is\nextensible to the evaluation and improvement of language model planning\nabilities in other areas of science or other areas that lack automatic\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.10632v1.pdf"
    },
    {
        "title": "Data Contamination Through the Lens of Time",
        "authors": [
            "Manley Roberts",
            "Himanshu Thakur",
            "Christine Herlihy",
            "Colin White",
            "Samuel Dooley"
        ],
        "published": "2023-10-16T17:51:29Z",
        "summary": "Recent claims about the impressive abilities of large language models (LLMs)\nare often supported by evaluating publicly available benchmarks. Since LLMs\ntrain on wide swaths of the internet, this practice raises concerns of data\ncontamination, i.e., evaluating on examples that are explicitly or implicitly\nincluded in the training data. Data contamination remains notoriously\nchallenging to measure and mitigate, even with partial attempts like controlled\nexperimentation of training data, canary strings, or embedding similarities. In\nthis work, we conduct the first thorough longitudinal analysis of data\ncontamination in LLMs by using the natural experiment of training cutoffs in\nGPT models to look at benchmarks released over time. Specifically, we consider\ntwo code/mathematical problem-solving datasets, Codeforces and Project Euler,\nand find statistically significant trends among LLM pass rate vs. GitHub\npopularity and release date that provide strong evidence of contamination. By\nopen-sourcing our dataset, raw results, and evaluation framework, our work\npaves the way for rigorous analyses of data contamination in modern models. We\nconclude with a discussion of best practices and future steps for publicly\nreleasing benchmarks in the age of LLMs that train on webscale data.",
        "pdf_link": "https://arxiv.org/pdf/2310.10628v1.pdf"
    },
    {
        "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
        "authors": [
            "Charlie George",
            "Andreas Stuhlm√ºller"
        ],
        "published": "2023-10-16T17:51:17Z",
        "summary": "Hallucination plagues even frontier LLMs--but how bad is it really for\nsummarizing academic papers? We evaluate Factored Verification, a simple\nautomated method for detecting hallucinations in abstractive summaries. This\nmethod sets a new SotA on hallucination detection in the summarization task of\nthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method to\nestimate how often language models hallucinate when summarizing across multiple\nacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)\nsummary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct\nusing Factored Critiques and find that this lowers the number of hallucinations\nto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations\nwe find are often subtle, so we advise caution when using models to synthesize\nacademic papers.",
        "pdf_link": "https://arxiv.org/pdf/2310.10627v1.pdf"
    },
    {
        "title": "On Context Utilization in Summarization with Large Language Models",
        "authors": [
            "Mathieu Ravaut",
            "Aixin Sun",
            "Nancy F. Chen",
            "Shafiq Joty"
        ],
        "published": "2023-10-16T16:45:12Z",
        "summary": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization.",
        "pdf_link": "https://arxiv.org/pdf/2310.10570v3.pdf"
    },
    {
        "title": "Metric Ensembles For Hallucination Detection",
        "authors": [
            "Grant C. Forbes",
            "Parth Katlana",
            "Zeydy Ortiz"
        ],
        "published": "2023-10-16T15:17:22Z",
        "summary": "Abstractive text summarization has garnered increased interest as of late, in\npart due to the proliferation of large language models (LLMs). One of the most\npressing problems related to generation of abstractive summaries is the need to\nreduce \"hallucinations,\" information that was not included in the document\nbeing summarized, and which may be wholly incorrect. Due to this need, a wide\narray of metrics estimating consistency with the text being summarized have\nbeen proposed. We examine in particular a suite of unsupervised metrics for\nsummary consistency, and measure their correlations with each other and with\nhuman evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then\ncompare these evaluations to models made from a simple linear ensemble of these\nmetrics. We find that LLM-based methods outperform other unsupervised metrics\nfor hallucination detection. We also find that ensemble methods can improve\nthese scores even further, provided that the metrics in the ensemble have\nsufficiently similar and uncorrelated error rates. Finally, we present an\nensemble method for LLM-based evaluations that we show improves over this\nprevious SOTA.",
        "pdf_link": "https://arxiv.org/pdf/2310.10495v1.pdf"
    },
    {
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
        "authors": [
            "Kai Chen",
            "Chunwei Wang",
            "Kuo Yang",
            "Jianhua Han",
            "Lanqing Hong",
            "Fei Mi",
            "Hang Xu",
            "Zhengying Liu",
            "Wenyong Huang",
            "Zhenguo Li",
            "Dit-Yan Yeung",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-10-16T14:59:10Z",
        "summary": "The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.10477v6.pdf"
    },
    {
        "title": "Stance Detection with Collaborative Role-Infused LLM-Based Agents",
        "authors": [
            "Xiaochong Lan",
            "Chen Gao",
            "Depeng Jin",
            "Yong Li"
        ],
        "published": "2023-10-16T14:46:52Z",
        "summary": "Stance detection automatically detects the stance in a text towards a target,\nvital for content analysis in web and social media research. Despite their\npromising capabilities, LLMs encounter challenges when directly applied to\nstance detection. First, stance detection demands multi-aspect knowledge, from\ndeciphering event-related terminologies to understanding the expression styles\nin social media platforms. Second, stance detection requires advanced reasoning\nto infer authors' implicit viewpoints, as stance are often subtly embedded\nrather than overtly stated in the text. To address these challenges, we design\na three-stage framework COLA (short for Collaborative rOle-infused LLM-based\nAgents) in which LLMs are designated distinct roles, creating a collaborative\nsystem where each role contributes uniquely. Initially, in the multidimensional\ntext analysis stage, we configure the LLMs to act as a linguistic expert, a\ndomain specialist, and a social media veteran to get a multifaceted analysis of\ntexts, thus overcoming the first challenge. Next, in the reasoning-enhanced\ndebating stage, for each potential stance, we designate a specific LLM-based\nagent to advocate for it, guiding the LLM to detect logical connections between\ntext features and stance, tackling the second challenge. Finally, in the stance\nconclusion stage, a final decision maker agent consolidates prior insights to\ndetermine the stance. Our approach avoids extra annotated data and model\ntraining and is highly usable. We achieve state-of-the-art performance across\nmultiple datasets. Ablation studies validate the effectiveness of each design\nrole in handling stance detection. Further experiments have demonstrated the\nexplainability and the versatility of our approach. Our approach excels in\nusability, accuracy, effectiveness, explainability and versatility,\nhighlighting its value.",
        "pdf_link": "https://arxiv.org/pdf/2310.10467v1.pdf"
    },
    {
        "title": "Large Language Model-Empowered Agents for Simulating Macroeconomic Activities",
        "authors": [
            "Nian Li",
            "Chen Gao",
            "Yong Li",
            "Qingmin Liao"
        ],
        "published": "2023-10-16T14:19:40Z",
        "summary": "The advent of the Web has brought about a paradigm shift in traditional\neconomics, particularly in the digital economy era, enabling the precise\nrecording and analysis of individual economic behavior. This has led to a\ngrowing emphasis on data-driven modeling in macroeconomics. In macroeconomic\nresearch, Agent-based modeling (ABM) emerged as an alternative, evolving\nthrough rule-based agents, machine learning-enhanced decision-making, and, more\nrecently, advanced AI agents. However, the existing works are suffering from\nthree main challenges when endowing agents with human-like decision-making,\nincluding agent heterogeneity, the influence of macroeconomic trends, and\nmultifaceted economic factors. Large language models (LLMs) have recently\ngained prominence in offering autonomous human-like characteristics. Therefore,\nleveraging LLMs in macroeconomic simulation presents an opportunity to overcome\ntraditional limitations. In this work, we take an early step in introducing a\nnovel approach that leverages LLMs in macroeconomic simulation. We design\nprompt-engineering-driven LLM agents to exhibit human-like decision-making and\nadaptability in the economic environment, with the abilities of perception,\nreflection, and decision-making to address the abovementioned challenges.\nSimulation experiments on macroeconomic activities show that LLM-empowered\nagents can make realistic work and consumption decisions and emerge more\nreasonable macroeconomic phenomena than existing rule-based or AI agents. Our\nwork demonstrates the promising potential to simulate macroeconomics based on\nLLM and its human-like characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2310.10436v1.pdf"
    },
    {
        "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions",
        "authors": [
            "Haoran Li",
            "Yulin Chen",
            "Jinglong Luo",
            "Yan Kang",
            "Xiaojin Zhang",
            "Qi Hu",
            "Chunkit Chan",
            "Yangqiu Song"
        ],
        "published": "2023-10-16T13:23:54Z",
        "summary": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration.",
        "pdf_link": "https://arxiv.org/pdf/2310.10383v1.pdf"
    },
    {
        "title": "Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs",
        "authors": [
            "Ananya Singha",
            "Jos√© Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Chris Parnin"
        ],
        "published": "2023-10-16T12:51:24Z",
        "summary": "Large language models (LLMs) are increasingly applied for tabular tasks using\nin-context learning. The prompt representation for a table may play a role in\nthe LLMs ability to process the table. Inspired by prior work, we generate a\ncollection of self-supervised structural tasks (e.g. navigate to a cell and\nrow; transpose the table) and evaluate the performance differences when using 8\nformats. In contrast to past work, we introduce 8 noise operations inspired by\nreal-world messy data and adversarial inputs, and show that such operations can\nimpact LLM performance across formats for different structural understanding\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.10358v1.pdf"
    },
    {
        "title": "Generative Calibration for In-context Learning",
        "authors": [
            "Zhongtao Jiang",
            "Yuanzhe Zhang",
            "Cao Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-16T10:45:02Z",
        "summary": "As one of the most exciting features of large language models (LLMs),\nin-context learning is a mixed blessing. While it allows users to\nfast-prototype a task solver with only a few training examples, the performance\nis generally sensitive to various configurations of the prompt such as the\nchoice or order of the training examples. In this paper, we for the first time\ntheoretically and empirically identify that such a paradox is mainly due to the\nlabel shift of the in-context model to the data distribution, in which LLMs\nshift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.\nWith this understanding, we can simply calibrate the in-context predictive\ndistribution by adjusting the label marginal, which is estimated via\nMonte-Carlo sampling over the in-context model, i.e., generation of LLMs. We\ncall our approach as generative calibration. We conduct exhaustive experiments\nwith 12 text classification tasks and 12 LLMs scaling from 774M to 33B,\ngenerally find that the proposed method greatly and consistently outperforms\nthe ICL as well as state-of-the-art calibration methods, by up to 27% absolute\nin macro-F1. Meanwhile, the proposed method is also stable under different\nprompt configurations.",
        "pdf_link": "https://arxiv.org/pdf/2310.10266v1.pdf"
    },
    {
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
        "authors": [
            "Rujie Wu",
            "Xiaojian Ma",
            "Zhenliang Zhang",
            "Wei Wang",
            "Qing Li",
            "Song-Chun Zhu",
            "Yizhou Wang"
        ],
        "published": "2023-10-16T09:19:18Z",
        "summary": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2310.10207v5.pdf"
    },
    {
        "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
        "authors": [
            "Xiaoshuai Song",
            "Keqing He",
            "Pei Wang",
            "Guanting Dong",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2023-10-16T08:34:44Z",
        "summary": "The tasks of out-of-domain (OOD) intent discovery and generalized intent\ndiscovery (GID) aim to extend a closed intent classifier to open-world intent\nsets, which is crucial to task-oriented dialogue (TOD) systems. Previous\nmethods address them by fine-tuning discriminative models. Recently, although\nsome studies have been exploring the application of large language models\n(LLMs) represented by ChatGPT to various downstream tasks, it is still unclear\nfor the ability of ChatGPT to discover and incrementally extent OOD intents. In\nthis paper, we comprehensively evaluate ChatGPT on OOD intent discovery and\nGID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT\nexhibits consistent advantages under zero-shot settings, but is still at a\ndisadvantage compared to fine-tuned models. More deeply, through a series of\nanalytical experiments, we summarize and discuss the challenges faced by LLMs\nincluding clustering, domain-specific understanding, and cross-domain\nin-context learning scenarios. Finally, we provide empirical guidance for\nfuture directions to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2310.10176v1.pdf"
    },
    {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
            "Huao Li",
            "Yu Quan Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Michael Lewis",
            "Katia Sycara"
        ],
        "published": "2023-10-16T07:51:19Z",
        "summary": "While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.",
        "pdf_link": "https://arxiv.org/pdf/2310.10701v2.pdf"
    },
    {
        "title": "LoBaSS: Gauging Learnability in Supervised Fine-tuning Data",
        "authors": [
            "Haotian Zhou",
            "Tingkai Liu",
            "Qianli Ma",
            "Jianbo Yuan",
            "Pengfei Liu",
            "Yang You",
            "Hongxia Yang"
        ],
        "published": "2023-10-16T07:26:24Z",
        "summary": "Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large\nLanguage Models (LLMs) to specific task prerequisites. The selection of\nfine-tuning data profoundly influences the model's performance, whose principle\nis traditionally grounded in data quality and distribution. In this paper, we\nintroduce a new dimension in SFT data selection: learnability. This new\ndimension is motivated by the intuition that SFT unlocks capabilities acquired\nby a LLM during the pretraining phase. Given that different pretrained models\nhave disparate capabilities, the SFT data appropriate for one may not suit\nanother. Thus, we introduce the term learnability to define the suitability of\ndata for effective learning by the model. We present the Loss Based SFT Data\nSelection (LoBaSS) method, utilizing data learnability as the principal\ncriterion for the selection SFT data. This method provides a nuanced approach,\nallowing the alignment of data selection with inherent model capabilities,\nensuring optimal compatibility and learning efficiency. In experimental\ncomparisons involving 7B and 13B models, our LoBaSS method is able to surpass\nfull-data fine-tuning at merely 6% of the total training data. When employing\n16.7% of the data, LoBaSS harmonizes the model's capabilities across\nconversational and mathematical domains, proving its efficacy and adaptability.",
        "pdf_link": "https://arxiv.org/pdf/2310.13008v1.pdf"
    },
    {
        "title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset",
        "authors": [
            "Arthur Amalvy",
            "Vincent Labatut",
            "Richard Dufour"
        ],
        "published": "2023-10-16T06:53:12Z",
        "summary": "While recent pre-trained transformer-based models can perform named entity\nrecognition (NER) with great accuracy, their limited range remains an issue\nwhen applied to long documents such as whole novels. To alleviate this issue, a\nsolution is to retrieve relevant context at the document level. Unfortunately,\nthe lack of supervision for such a task means one has to settle for\nunsupervised approaches. Instead, we propose to generate a synthetic context\nretrieval training dataset using Alpaca, an instructiontuned large language\nmodel (LLM). Using this dataset, we train a neural context retriever based on a\nBERT model that is able to find relevant context for NER. We show that our\nmethod outperforms several retrieval baselines for the NER task on an English\nliterary dataset composed of the first chapter of 40 books.",
        "pdf_link": "https://arxiv.org/pdf/2310.10118v3.pdf"
    },
    {
        "title": "On Generative Agents in Recommendation",
        "authors": [
            "An Zhang",
            "Leheng Sheng",
            "Yuxin Chen",
            "Hao Li",
            "Yang Deng",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "published": "2023-10-16T06:41:16Z",
        "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie\nrecommendation simulator, leveraging LLM-empowered generative agents equipped\nwith user profile, memory, and actions modules specifically tailored for the\nrecommender system. In particular, these agents' profile modules are\ninitialized using the MovieLens dataset, capturing users' unique tastes and\nsocial traits; memory modules log both factual and emotional memories and are\nintegrated with an emotion-driven reflection mechanism; action modules support\na wide variety of behaviors, spanning both taste-driven and emotion-driven\nactions. Each agent interacts with personalized movie recommendations in a\npage-by-page manner, relying on a pre-implemented collaborative filtering-based\nrecommendation algorithm. We delve into both the capabilities and limitations\nof Agent4Rec, aiming to explore an essential research question: to what extent\ncan LLM-empowered generative agents faithfully simulate the behavior of real,\nautonomous humans in recommender systems? Extensive and multi-faceted\nevaluations of Agent4Rec highlight both the alignment and deviation between\nagents and user-personalized preferences. Beyond mere performance comparison,\nwe explore insightful experiments, such as emulating the filter bubble effect\nand discovering the underlying causal relationships in recommendation tasks.\nOur codes are available at https://github.com/LehengTHU/Agent4Rec.",
        "pdf_link": "https://arxiv.org/pdf/2310.10108v1.pdf"
    },
    {
        "title": "JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning",
        "authors": [
            "Issey Sukeda",
            "Masahiro Suzuki",
            "Hiroki Sakaji",
            "Satoshi Kodera"
        ],
        "published": "2023-10-16T05:28:28Z",
        "summary": "In the ongoing wave of impact driven by large language models (LLMs) like\nChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial\nresearch frontier. Since mainstream LLMs tend to be designed for\ngeneral-purpose applications, constructing a medical LLM through domain\nadaptation is a huge challenge. While instruction-tuning is used to fine-tune\nsome LLMs, its precise roles in domain adaptation remain unknown. Here we show\nthe contribution of LoRA-based instruction-tuning to performance in Japanese\nmedical question-answering tasks. In doing so, we employ a multifaceted\nevaluation for multiple-choice questions, including scoring based on \"Exact\nmatch\" and \"Gestalt distance\" in addition to the conventional accuracy. Our\nfindings suggest that LoRA-based instruction-tuning can partially incorporate\ndomain-specific knowledge into LLMs, with larger models demonstrating more\npronounced effects. Furthermore, our results underscore the potential of\nadapting English-centric models for Japanese applications in domain adaptation,\nwhile also highlighting the persisting limitations of Japanese-centric models.\nThis initiative represents a pioneering effort in enabling medical institutions\nto fine-tune and operate models without relying on external services.",
        "pdf_link": "https://arxiv.org/pdf/2310.10083v2.pdf"
    },
    {
        "title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks",
        "authors": [
            "Shuyu Jiang",
            "Xingshu Chen",
            "Rui Tang"
        ],
        "published": "2023-10-16T05:19:25Z",
        "summary": "Recently, Large language models (LLMs) with powerful general capabilities\nhave been increasingly integrated into various Web applications, while\nundergoing alignment training to ensure that the generated content aligns with\nuser intent and ethics. Unfortunately, they remain the risk of generating\nharmful content like hate speech and criminal activities in practical\napplications. Current approaches primarily rely on detecting, collecting, and\ntraining against harmful prompts to prevent such risks. However, they typically\nfocused on the \"superficial\" harmful prompts with a solitary intent, ignoring\ncomposite attack instructions with multiple intentions that can easily elicit\nharmful content in real-world scenarios. In this paper, we introduce an\ninnovative technique for obfuscating harmful instructions: Compositional\nInstruction Attacks (CIA), which refers to attacking by combination and\nencapsulation of multiple instructions. CIA hides harmful prompts within\ninstructions of harmless intentions, making it impossible for the model to\nidentify underlying malicious intentions. Furthermore, we implement two\ntransformation methods, known as T-CIA and W-CIA, to automatically disguise\nharmful instructions as talking or writing tasks, making them appear harmless\nto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety\nassessment datasets and two harmful prompt datasets. It achieves an attack\nsuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+\nfor ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.\nOur approach reveals the vulnerability of LLMs to such compositional\ninstruction attacks that harbor underlying harmful intentions, contributing\nsignificantly to LLM security development. Warning: this paper may contain\noffensive or upsetting content!",
        "pdf_link": "https://arxiv.org/pdf/2310.10077v1.pdf"
    },
    {
        "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models",
        "authors": [
            "Tao Fan",
            "Yan Kang",
            "Guoqiang Ma",
            "Weijing Chen",
            "Wenbin Wei",
            "Lixin Fan",
            "Qiang Yang"
        ],
        "published": "2023-10-16T04:17:13Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.10049v1.pdf"
    },
    {
        "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
        "authors": [
            "Yixin Liu",
            "Avi Singh",
            "C. Daniel Freeman",
            "John D. Co-Reyes",
            "Peter J. Liu"
        ],
        "published": "2023-10-16T04:11:19Z",
        "summary": "Despite their success in many natural language tasks, solving math problems\nremains a significant challenge for large language models (LLMs). A large gap\nexists between LLMs' pass-at-one and pass-at-N performance in solving math\nproblems, suggesting LLMs might be close to finding correct solutions,\nmotivating our exploration of fine-tuning methods to unlock LLMs' performance.\nUsing the challenging MATH dataset, we investigate three fine-tuning\nstrategies: (1) solution fine-tuning, where we fine-tune to generate a detailed\nsolution for a given math problem; (2) solution-cluster re-ranking, where the\nLLM is fine-tuned as a solution verifier/evaluator to choose among generated\ncandidate solution clusters; (3) multi-task sequential fine-tuning, which\nintegrates both solution generation and evaluation tasks together efficiently\nto enhance the LLM performance. With these methods, we present a thorough\nempirical study on a series of PaLM 2 models and find: (1) The quality and\nstyle of the step-by-step solutions used for fine-tuning can make a significant\nimpact on the model performance; (2) While solution re-ranking and majority\nvoting are both effective for improving the model performance when used\nseparately, they can also be used together for an even greater performance\nboost; (3) Multi-task fine-tuning that sequentially separates the solution\ngeneration and evaluation tasks can offer improved performance compared with\nthe solution fine-tuning baseline. Guided by these insights, we design a\nfine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset\nwith fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the\nfew-shot performance of pre-trained PaLM 2-L model with majority voting.",
        "pdf_link": "https://arxiv.org/pdf/2310.10047v1.pdf"
    },
    {
        "title": "TRANSOM: An Efficient Fault-Tolerant System for Training LLMs",
        "authors": [
            "Baodong Wu",
            "Lei Xia",
            "Qingping Li",
            "Kangyu Li",
            "Xu Chen",
            "Yongqiang Guo",
            "Tieyao Xiang",
            "Yuheng Chen",
            "Shigang Li"
        ],
        "published": "2023-10-16T04:06:52Z",
        "summary": "Large language models (LLMs) with hundreds of billions or trillions of\nparameters, represented by chatGPT, have achieved profound impact on various\nfields. However, training LLMs with super-large-scale parameters requires large\nhigh-performance GPU clusters and long training periods lasting for months. Due\nto the inevitable hardware and software failures in large-scale clusters,\nmaintaining uninterrupted and long-duration training is extremely challenging.\nAs a result, A substantial amount of training time is devoted to task\ncheckpoint saving and loading, task rescheduling and restart, and task manual\nanomaly checks, which greatly harms the overall training efficiency. To address\nthese issues, we propose TRANSOM, a novel fault-tolerant LLM training system.\nIn this work, we design three key subsystems: the training pipeline automatic\nfault tolerance and recovery mechanism named Transom Operator and Launcher\n(TOL), the training task multi-dimensional metric automatic anomaly detection\nsystem named Transom Eagle Eye (TEE), and the training checkpoint asynchronous\naccess automatic fault tolerance and recovery technology named Transom\nCheckpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks,\nwhile TEE is responsible for task monitoring and anomaly reporting. TEE detects\ntraining anomalies and reports them to TOL, who automatically enters the fault\ntolerance strategy to eliminate abnormal nodes and restart the training task.\nAnd the asynchronous checkpoint saving and loading functionality provided by\nTCE greatly shorten the fault tolerance overhead. The experimental results\nindicate that TRANSOM significantly enhances the efficiency of large-scale LLM\ntraining on clusters. Specifically, the pre-training time for GPT3-175B has\nbeen reduced by 28%, while checkpoint saving and loading performance have\nimproved by a factor of 20.",
        "pdf_link": "https://arxiv.org/pdf/2310.10046v3.pdf"
    },
    {
        "title": "Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis",
        "authors": [
            "Chaoyi Wu",
            "Jiayu Lei",
            "Qiaoyu Zheng",
            "Weike Zhao",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Xiao Zhou",
            "Ziheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "published": "2023-10-15T18:32:27Z",
        "summary": "Driven by the large foundation models, the development of artificial\nintelligence has witnessed tremendous progress lately, leading to a surge of\ngeneral interest from the public. In this study, we aim to assess the\nperformance of OpenAI's newest model, GPT-4V(ision), specifically in the realm\nof multimodal medical diagnosis. Our evaluation encompasses 17 human body\nsystems, including Central Nervous System, Head and Neck, Cardiac, Chest,\nHematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,\nObstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,\nPediatrics, with images taken from 8 modalities used in daily clinic routine,\ne.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),\nPositron Emission Tomography (PET), Digital Subtraction Angiography (DSA),\nMammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on\nmultiple clinical tasks with or without patent history provided, including\nimaging modality and anatomy recognition, disease diagnosis, report generation,\ndisease localisation.\n  Our observation shows that, while GPT-4V demonstrates proficiency in\ndistinguishing between medical image modalities and anatomy, it faces\nsignificant challenges in disease diagnosis and generating comprehensive\nreports. These findings underscore that while large multimodal models have made\nsignificant advancements in computer vision and natural language processing, it\nremains far from being used to effectively support real-world medical\napplications and clinical decision-making.\n  All images used in this report can be found in\nhttps://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.09909v3.pdf"
    },
    {
        "title": "In-Context Learning with Iterative Demonstration Selection",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Anirudh Dagar",
            "Wenming Ye"
        ],
        "published": "2023-10-15T16:40:19Z",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nLeveraging the merits of both dimensions, we propose Iterative Demonstration\nSelection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),\nIDS iteratively selects examples that are diverse but still strongly correlated\nwith the test sample as ICL demonstrations. Specifically, IDS applies\nZero-shot-CoT to the test sample before demonstration selection. The output\nreasoning path is then used to choose demonstrations that are prepended to the\ntest sample for inference. The generated answer is accompanied by its\ncorresponding reasoning path for extracting a new set of demonstrations in the\nnext iteration. After several iterations, IDS adopts majority voting to obtain\nthe final result. Through extensive experiments on tasks including commonsense\nreasoning, question answering, topic classification, and sentiment analysis, we\ndemonstrate that IDS can consistently outperform existing ICL demonstration\nselection methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.09881v2.pdf"
    },
    {
        "title": "Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)",
        "authors": [
            "Jianxiang Yu",
            "Yuxiang Ren",
            "Chenghua Gong",
            "Jiaqi Tan",
            "Xiang Li",
            "Xuecang Zhang"
        ],
        "published": "2023-10-15T16:04:28Z",
        "summary": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called ENG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement over\nthe baseline model.",
        "pdf_link": "https://arxiv.org/pdf/2310.09872v1.pdf"
    },
    {
        "title": "ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors",
        "authors": [
            "Julien Pourcel",
            "C√©dric Colas",
            "Pierre-Yves Oudeyer",
            "Laetitia Teodorescu"
        ],
        "published": "2023-10-15T14:57:14Z",
        "summary": "Finding and selecting new and interesting problems to solve is at the heart\nof curiosity, science and innovation. We here study automated problem\ngeneration in the context of the open-ended space of python programming\npuzzles. Existing generative models often aim at modeling a reference\ndistribution without any explicit diversity optimization. Other methods\nexplicitly optimizing for diversity do so either in limited hand-coded\nrepresentation spaces or in uninterpretable learned embedding spaces that may\nnot align with human perceptions of interesting variations. With ACES\n(Autotelic Code Exploration via Semantic descriptors), we introduce a new\nautotelic generation method that leverages semantic descriptors produced by a\nlarge language model (LLM) to directly optimize for interesting diversity, as\nwell as few-shot-based generation. Each puzzle is labeled along 10 dimensions,\neach capturing a programming skill required to solve it. ACES generates and\npursues novel and feasible goals to explore that abstract semantic space,\nslowly discovering a diversity of solvable programming puzzles in any given\nrun. Across a set of experiments, we show that ACES discovers a richer\ndiversity of puzzles than existing diversity-maximizing algorithms as measured\nacross a range of diversity metrics. We further study whether and in which\nconditions this diversity can translate into the successful training of puzzle\nsolving models.",
        "pdf_link": "https://arxiv.org/pdf/2310.10692v3.pdf"
    },
    {
        "title": "Assessing the Reliability of Large Language Model Knowledge",
        "authors": [
            "Weixuan Wang",
            "Barry Haddow",
            "Alexandra Birch",
            "Wei Peng"
        ],
        "published": "2023-10-15T12:40:30Z",
        "summary": "Large language models (LLMs) have been treated as knowledge bases due to\ntheir strong performance in knowledge probing tasks. LLMs are typically\nevaluated using accuracy, yet this metric does not capture the vulnerability of\nLLMs to hallucination-inducing factors like prompt and context variability. How\ndo we evaluate the capabilities of LLMs to consistently produce factually\ncorrect answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe\n(MONITOR), a novel metric designed to directly measure LLMs' factual\nreliability. MONITOR computes the distance between the probability\ndistributions of a valid output and its counterparts produced by the same LLM\nprobing the same fact using different styles of prompts and\ncontexts.Experiments on a comprehensive range of 12 LLMs demonstrate the\neffectiveness of MONITOR in evaluating the factual reliability of LLMs while\nmaintaining a low computational overhead. In addition, we release the FKTC\n(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total\nto foster research along this line (https://github.com/Vicky-Wil/MONITOR).",
        "pdf_link": "https://arxiv.org/pdf/2310.09820v1.pdf"
    },
    {
        "title": "When can transformers reason with abstract symbols?",
        "authors": [
            "Enric Boix-Adsera",
            "Omid Saremi",
            "Emmanuel Abbe",
            "Samy Bengio",
            "Etai Littwin",
            "Joshua Susskind"
        ],
        "published": "2023-10-15T06:45:38Z",
        "summary": "We investigate the capabilities of transformer large language models (LLMs)\non relational reasoning tasks involving abstract symbols. Such tasks have long\nbeen studied in the neuroscience literature as fundamental building blocks for\nmore complex abilities in programming, mathematics, and verbal reasoning. For\n(i) regression tasks, we prove that transformers generalize when trained, but\nrequire astonishingly large quantities of training data. For (ii)\nnext-token-prediction tasks with symbolic labels, we show an \"inverse scaling\nlaw\": transformers fail to generalize as their embedding dimension increases.\nFor both settings (i) and (ii), we propose subtle transformer modifications\nwhich can reduce the amount of data needed by adding two trainable parameters\nper head.",
        "pdf_link": "https://arxiv.org/pdf/2310.09753v1.pdf"
    },
    {
        "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
        "authors": [
            "Fanghua Ye",
            "Meng Fang",
            "Shenghui Li",
            "Emine Yilmaz"
        ],
        "published": "2023-10-15T03:04:17Z",
        "summary": "Query rewriting plays a vital role in enhancing conversational search by\ntransforming context-dependent user queries into standalone forms. Existing\napproaches primarily leverage human-rewritten queries as labels to train query\nrewriting models. However, human rewrites may lack sufficient information for\noptimal retrieval performance. To overcome this limitation, we propose\nutilizing large language models (LLMs) as query rewriters, enabling the\ngeneration of informative query rewrites through well-designed instructions. We\ndefine four essential properties for well-formed rewrites and incorporate all\nof them into the instruction. In addition, we introduce the role of rewrite\neditors for LLMs when initial query rewrites are available, forming a\n\"rewrite-then-edit\" process. Furthermore, we propose distilling the rewriting\ncapabilities of LLMs into smaller models to reduce rewriting latency. Our\nexperimental evaluation on the QReCC dataset demonstrates that informative\nquery rewrites can yield substantially improved retrieval performance compared\nto human rewrites, especially with sparse retrievers.",
        "pdf_link": "https://arxiv.org/pdf/2310.09716v2.pdf"
    },
    {
        "title": "Configuration Validation with Large Language Models",
        "authors": [
            "Xinyu Lian",
            "Yinfang Chen",
            "Runxiang Cheng",
            "Jie Huang",
            "Parth Thakkar",
            "Minjia Zhang",
            "Tianyin Xu"
        ],
        "published": "2023-10-15T00:50:27Z",
        "summary": "Misconfigurations are major causes of software failures. Existing practices\nrely on developer-written rules or test cases to validate configurations, which\nare expensive. Machine learning (ML) for configuration validation is considered\na promising direction, but has been facing challenges such as the need of\nlarge-scale field data and system-specific models. Recent advances in Large\nLanguage Models (LLMs) show promise in addressing some of the long-lasting\nlimitations of ML-based configuration validation. We present a first analysis\non the feasibility and effectiveness of using LLMs for configuration\nvalidation. We empirically evaluate LLMs as configuration validators by\ndeveloping a generic LLM-based configuration validation framework, named Ciri.\nCiri employs effective prompt engineering with few-shot learning based on both\nvalid configuration and misconfiguration data. Ciri checks outputs from LLMs\nwhen producing results, addressing hallucination and nondeterminism of LLMs. We\nevaluate Ciri's validation effectiveness on eight popular LLMs using\nconfiguration data of ten widely deployed open-source systems. Our analysis (1)\nconfirms the potential of using LLMs for configuration validation, (2) explores\ndesign space of LLMbased validators like Ciri, and (3) reveals open challenges\nsuch as ineffectiveness in detecting certain types of misconfigurations and\nbiases towards popular configuration parameters.",
        "pdf_link": "https://arxiv.org/pdf/2310.09690v2.pdf"
    },
    {
        "title": "DPZero: Private Fine-Tuning of Language Models without Backpropagation",
        "authors": [
            "Liang Zhang",
            "Bingcong Li",
            "Kiran Koshy Thekumparampil",
            "Sewoong Oh",
            "Niao He"
        ],
        "published": "2023-10-14T18:42:56Z",
        "summary": "The widespread practice of fine-tuning large language models (LLMs) on\ndomain-specific data faces two major challenges in memory and privacy. First,\nas the size of LLMs continues to grow, the memory demands of gradient-based\ntraining methods via backpropagation become prohibitively high. Second, given\nthe tendency of LLMs to memorize training data, it is important to protect\npotentially sensitive information in the fine-tuning data from being\nregurgitated. Zeroth-order methods, which rely solely on forward passes,\nsubstantially reduce memory consumption during training. However, directly\ncombining them with standard differentially private gradient descent suffers\nfrom growing model size. To bridge this gap, we introduce DPZero, a novel\nprivate zeroth-order algorithm with nearly dimension-independent rates. The\nmemory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa on\nsix downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.09639v2.pdf"
    },
    {
        "title": "Autonomous Tree-search Ability of Large Language Models",
        "authors": [
            "Zheyu Zhang",
            "Zhuorui Ye",
            "Yikang Shen",
            "Chuang Gan"
        ],
        "published": "2023-10-14T14:14:38Z",
        "summary": "Large Language Models have excelled in remarkable reasoning capabilities with\nadvanced prompting techniques, but they fall short on tasks that require\nexploration, strategic foresight, and sequential decision-making. Recent works\npropose to utilize external programs to define search logic, such that LLMs can\nperform passive tree search to solve more challenging reasoning tasks. Though\nimpressive results have been achieved, there are several fundamental\nlimitations of these approaches. First, passive tree searches are not efficient\nas they usually require multiple rounds of LLM API calls to solve one single\nproblem. Moreover, passive search methods are not flexible since they need\ntask-specific program designs. Then a natural question arises: can we maintain\nthe tree-search capability of LLMs without the aid of external programs, and\ncan still generate responses that clearly demonstrate the process of a\ntree-structure search? To this end, we propose a new concept called autonomous\ntree-search ability of LLM, which can automatically generate a response\ncontaining search trajectories for the correct answer. Concretely, we perform\nsearch trajectories using capable LLM API via a fixed system prompt, allowing\nthem to perform autonomous tree-search (ATS) right out of the box. Experiments\non 4 puzzle games demonstrate our method can achieve huge improvements. The\nATS-BFS method outperforms the Chain of Thought approach by achieving an\naverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires\n65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.\nMoreover, we have collected data using the ATS prompt method and fine-tuned\nLLaMA. This approach yield a greater improvement compared to the ones\nfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an\naverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.10686v1.pdf"
    },
    {
        "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
        "authors": [
            "Md Rashad Al Hasan Rony",
            "Christian Suess",
            "Sinchana Ramakanth Bhat",
            "Viju Sudhi",
            "Julia Schneider",
            "Maximilian Vogel",
            "Roman Teucher",
            "Ken E. Friedl",
            "Soumya Sahoo"
        ],
        "published": "2023-10-14T08:46:24Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance by\nfollowing natural language instructions without fine-tuning them on\ndomain-specific tasks and data. However, leveraging LLMs for domain-specific\nquestion answering suffers from severe limitations. The generated answer tends\nto hallucinate due to the training data collection time (when using\noff-the-shelf), complex user utterance and wrong retrieval (in\nretrieval-augmented generation). Furthermore, due to the lack of awareness\nabout the domain and expected output, such LLMs may generate unexpected and\nunsafe answers that are not tailored to the target domain. In this paper, we\npropose CarExpert, an in-car retrieval-augmented conversational\nquestion-answering system leveraging LLMs for different tasks. Specifically,\nCarExpert employs LLMs to control the input, provide domain-specific documents\nto the extractive and generative answering components, and controls the output\nto ensure safe and domain-specific answers. A comprehensive empirical\nevaluation exhibits that CarExpert outperforms state-of-the-art LLMs in\ngenerating natural, safe and car-specific answers.",
        "pdf_link": "https://arxiv.org/pdf/2310.09536v1.pdf"
    },
    {
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
        "authors": [
            "Haikang Deng",
            "Colin Raffel"
        ],
        "published": "2023-10-14T07:19:47Z",
        "summary": "While large language models have proven effective in a huge range of\ndownstream applications, they often generate text that is problematic or lacks\na desired attribute. In this paper, we introduce Reward-Augmented Decoding\n(RAD), a text generation procedure that uses a small unidirectional reward\nmodel to encourage a language model to generate text that has certain\nproperties. Specifically, RAD uses the reward model to score generations as\nthey are produced and rescales sampling probabilities to favor high-reward\ntokens. By using a unidirectional reward model, RAD can cache activations from\nprior generation steps to decrease computational overhead. Through experiments\non generating non-toxic and sentiment-controlled text, we demonstrate that RAD\nperforms best among methods that change only the generation procedure and\nmatches the performance of state-of-the-art methods that involve re-training\nthe language model. We further validate that RAD is effective on very large\nlanguage models while incurring a minimal computational overhead.",
        "pdf_link": "https://arxiv.org/pdf/2310.09520v4.pdf"
    },
    {
        "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models",
        "authors": [
            "Hang Shao",
            "Bei Liu",
            "Bo Xiao",
            "Ke Zeng",
            "Guanglu Wan",
            "Yanmin Qian"
        ],
        "published": "2023-10-14T05:43:09Z",
        "summary": "Various Large Language Models(LLMs) from the Generative Pretrained\nTransformer(GPT) family have achieved outstanding performances in a wide range\nof text generation tasks. However, the enormous model sizes have hindered their\npractical use in real-world applications due to high inference latency.\nTherefore, improving the efficiencies of LLMs through quantization, pruning,\nand other means has been a key issue in LLM studies. In this work, we propose a\nmethod based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs\nto at least 50% sparsity without the need of any retraining. It allocates\nsparsity adaptively based on sensitivity, allowing us to reduce pruning-induced\nerror while maintaining the overall sparsity level. The advantages of the\nproposed method exhibit even more when the sparsity is extremely high.\nFurthermore, our method is compatible with quantization, enabling further\ncompression of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.09499v3.pdf"
    },
    {
        "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
        "authors": [
            "Shengyao Zhuang",
            "Honglei Zhuang",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2023-10-14T05:20:02Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive effectiveness in\nzero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting\napproaches have been proposed for LLM-based zero-shot ranking. Our study begins\nby thoroughly evaluating these existing approaches within a consistent\nexperimental framework, considering factors like model size, token consumption,\nlatency, among others. This first-of-its-kind comparative evaluation of these\napproaches allows us to identify the trade-offs between effectiveness and\nefficiency inherent in each approach. We find that while Pointwise approaches\nscore high on efficiency, they suffer from poor effectiveness. Conversely,\nPairwise approaches demonstrate superior effectiveness but incur high\ncomputational overhead. To further enhance the efficiency of LLM-based\nzero-shot ranking, we propose a novel Setwise prompting approach. Our approach\nreduces the number of LLM inferences and the amount of prompt token consumption\nduring the ranking procedure, significantly improving the efficiency of\nLLM-based zero-shot ranking. We test our method using the TREC DL datasets and\nthe BEIR zero-shot document ranking benchmark. The empirical results indicate\nthat our approach considerably reduces computational costs while also retaining\nhigh zero-shot ranking effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2310.09497v1.pdf"
    },
    {
        "title": "Large Language Model Unlearning",
        "authors": [
            "Yuanshun Yao",
            "Xiaojun Xu",
            "Yang Liu"
        ],
        "published": "2023-10-14T00:32:55Z",
        "summary": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,\non large language models (LLMs). We show at least three scenarios of aligning\nLLMs with human preferences can benefit from unlearning: (1) removing harmful\nresponses, (2) erasing copyright-protected content as requested, and (3)\nreducing hallucinations. Unlearning, as an alignment technique, has three\nadvantages. (1) It only requires negative (e.g. harmful) examples, which are\nmuch easier and cheaper to collect (e.g. via red teaming or user reporting)\nthan positive (e.g. helpful and often human-written) examples required in RLHF\n(RL from human feedback). (2) It is computationally efficient. (3) It is\nespecially effective when we know which training samples cause the misbehavior.\nTo the best of our knowledge, our work is among the first to explore LLM\nunlearning. We are also among the first to formulate the settings, goals, and\nevaluations in LLM unlearning. We show that if practitioners only have limited\nresources, and therefore the priority is to stop generating undesirable outputs\nrather than to try to generate desirable outputs, unlearning is particularly\nappealing. Despite only having negative samples, our ablation study shows that\nunlearning can still achieve better alignment performance than RLHF with just\n2% of its computational time.",
        "pdf_link": "https://arxiv.org/pdf/2310.10683v2.pdf"
    },
    {
        "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents",
        "authors": [
            "Hyungjoo Chae",
            "Yongho Song",
            "Kai Tzu-iunn Ong",
            "Taeyoon Kwon",
            "Minjin Kim",
            "Youngjae Yu",
            "Dongha Lee",
            "Dongyeop Kang",
            "Jinyoung Yeo"
        ],
        "published": "2023-10-13T18:17:23Z",
        "summary": "Human-like chatbots necessitate the use of commonsense reasoning in order to\neffectively comprehend and respond to implicit information present within\nconversations. Achieving such coherence and informativeness in responses,\nhowever, is a non-trivial task. Even for large language models (LLMs), the task\nof identifying and aggregating key evidence within a single hop presents a\nsubstantial challenge. This complexity arises because such evidence is\nscattered across multiple turns in a conversation, thus necessitating\nintegration over multiple hops. Hence, our focus is to facilitate such\nmulti-hop reasoning over a dialogue context, namely dialogue chain-of-thought\n(CoT) reasoning. To this end, we propose a knowledge distillation framework\nthat leverages LLMs as unreliable teachers and selectively distills consistent\nand helpful rationales via alignment filters. We further present DOCTOR, a\nDialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for\nresponse generation. We conduct extensive experiments to show that enhancing\ndialogue agents with high-quality rationales from DOCTOR significantly improves\nthe quality of their responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.09343v2.pdf"
    },
    {
        "title": "Ranking LLM-Generated Loop Invariants for Program Verification",
        "authors": [
            "Saikat Chakraborty",
            "Shuvendu K. Lahiri",
            "Sarah Fakhoury",
            "Madanlal Musuvathi",
            "Akash Lal",
            "Aseem Rastogi",
            "Aditya Senthilnathan",
            "Rahul Sharma",
            "Nikhil Swamy"
        ],
        "published": "2023-10-13T18:13:52Z",
        "summary": "Synthesizing inductive loop invariants is fundamental to automating program\nverification. In this work, we observe that Large Language Models (such as\ngpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of\nprograms in a 0-shot setting, yet require several samples to generate the\ncorrect invariants. This can lead to a large number of calls to a program\nverifier to establish an invariant. To address this issue, we propose a {\\it\nre-ranking} approach for the generated results of LLMs. We have designed a\nranker that can distinguish between correct inductive invariants and incorrect\nattempts based on the problem definition. The ranker is optimized as a\ncontrastive ranker. Experimental results demonstrate that this re-ranking\nmechanism significantly improves the ranking of correct invariants among the\ngenerated candidates, leading to a notable reduction in the number of calls to\na verifier. The source code and the experimental data for this paper are\navailable in \\url{https://github.com/microsoft/NeuralInvariantRanker}.",
        "pdf_link": "https://arxiv.org/pdf/2310.09342v3.pdf"
    },
    {
        "title": "User Inference Attacks on Large Language Models",
        "authors": [
            "Nikhil Kandpal",
            "Krishna Pillutla",
            "Alina Oprea",
            "Peter Kairouz",
            "Christopher A. Choquette-Choo",
            "Zheng Xu"
        ],
        "published": "2023-10-13T17:24:52Z",
        "summary": "Fine-tuning is a common and effective method for tailoring large language\nmodels (LLMs) to specialized tasks and applications. In this paper, we study\nthe privacy implications of fine-tuning LLMs on user data. To this end, we\nconsider a realistic threat model, called user inference, wherein an attacker\ninfers whether or not a user's data was used for fine-tuning. We design attacks\nfor performing user inference that require only black-box access to the\nfine-tuned LLM and a few samples from a user which need not be from the\nfine-tuning dataset. We find that LLMs are susceptible to user inference across\na variety of fine-tuning datasets, at times with near perfect attack success\nrates. Further, we theoretically and empirically investigate the properties\nthat make users vulnerable to user inference, finding that outlier users, users\nwith identifiable shared features between examples, and users that contribute a\nlarge fraction of the fine-tuning data are most susceptible to attack. Based on\nthese findings, we identify several methods for mitigating user inference\nincluding training with example-level differential privacy, removing\nwithin-user duplicate examples, and reducing a user's contribution to the\ntraining data. While these techniques provide partial mitigation of user\ninference, we highlight the need to develop methods to fully protect fine-tuned\nLLMs against this privacy risk.",
        "pdf_link": "https://arxiv.org/pdf/2310.09266v2.pdf"
    },
    {
        "title": "PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming",
        "authors": [
            "Chufan Gao",
            "Xulin Fan",
            "Jimeng Sun",
            "Xuan Wang"
        ],
        "published": "2023-10-13T17:23:17Z",
        "summary": "Relation extraction aims to classify the relationships between two entities\ninto pre-defined categories. While previous research has mainly focused on\nsentence-level relation extraction, recent studies have expanded the scope to\ndocument-level relation extraction. Traditional relation extraction methods\nheavily rely on human-annotated training data, which is time-consuming and\nlabor-intensive. To mitigate the need for manual annotation, recent\nweakly-supervised approaches have been developed for sentence-level relation\nextraction while limited work has been done on document-level relation\nextraction. Weakly-supervised document-level relation extraction faces\nsignificant challenges due to an imbalanced number \"no relation\" instances and\nthe failure of directly probing pretrained large language models for document\nrelation extraction. To address these challenges, we propose PromptRE, a novel\nweakly-supervised document-level relation extraction method that combines\nprompting-based techniques with data programming. Furthermore, PromptRE\nincorporates the label distribution and entity types as prior knowledge to\nimprove the performance. By leveraging the strengths of both prompting and data\nprogramming, PromptRE achieves improved performance in relation classification\nand effectively handles the \"no relation\" problem. Experimental results on\nReDocRED, a benchmark dataset for document-level relation extraction,\ndemonstrate the superiority of PromptRE over baseline approaches.",
        "pdf_link": "https://arxiv.org/pdf/2310.09265v1.pdf"
    },
    {
        "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks",
        "authors": [
            "Peng Li",
            "Yeye He",
            "Dror Yashar",
            "Weiwei Cui",
            "Song Ge",
            "Haidong Zhang",
            "Danielle Rifinski Fainman",
            "Dongmei Zhang",
            "Surajit Chaudhuri"
        ],
        "published": "2023-10-13T17:20:56Z",
        "summary": "Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable\nabilities to follow diverse human instructions and perform a wide range of\ntasks. However, when probing language models using a range of basic\ntable-understanding tasks, we observe that today's language models are still\nsub-optimal in many table-related tasks, likely because they are pre-trained\npredominantly on \\emph{one-dimensional} natural-language texts, whereas\nrelational tables are \\emph{two-dimensional} objects.\n  In this work, we propose a new \"\\emph{table-tuning}\" paradigm, where we\ncontinue to train/fine-tune language models like GPT-3.5 and ChatGPT, using\ndiverse table-tasks synthesized from real tables as training data, with the\ngoal of enhancing language models' ability to understand tables and perform\ntable tasks. We show that our resulting Table-GPT models demonstrate (1) better\n\\emph{table-understanding} capabilities, by consistently outperforming the\nvanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout\nunseen tasks, and (2) strong \\emph{generalizability}, in its ability to respond\nto diverse human instructions to perform new table-tasks, in a manner similar\nto GPT-3.5 and ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.09263v1.pdf"
    },
    {
        "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
        "authors": [
            "Yixin Wan",
            "George Pu",
            "Jiao Sun",
            "Aparna Garimella",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023-10-13T16:12:57Z",
        "summary": "Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2310.09219v5.pdf"
    },
    {
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Tao Yang",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023-10-13T15:03:15Z",
        "summary": "Instruction-tuning can be substantially optimized through enhanced diversity,\nresulting in models capable of handling a broader spectrum of tasks. However,\nexisting data employed for such tuning often exhibit an inadequate coverage of\nindividual domains, limiting the scope for nuanced comprehension and\ninteractions within these areas. To address this deficiency, we propose\nExplore-Instruct, a novel approach to enhance the data coverage to be used in\ndomain-specific instruction-tuning through active exploration via Large\nLanguage Models (LLMs). Built upon representative domain use cases,\nExplore-Instruct explores a multitude of variations or possibilities by\nimplementing a search algorithm to obtain diversified and domain-focused\ninstruction-tuning data. Our data-centric analysis validates the effectiveness\nof this proposed approach in improving domain-specific instruction coverage.\nMoreover, our model's performance demonstrates considerable advancements over\nmultiple baselines, including those utilizing domain-specific data enhancement.\nOur findings offer a promising opportunity to improve instruction coverage,\nespecially in domain-specific contexts, thereby advancing the development of\nadaptable language models. Our code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/Explore-Instruct}.",
        "pdf_link": "https://arxiv.org/pdf/2310.09168v3.pdf"
    },
    {
        "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
        "authors": [
            "Peihua Mai",
            "Ran Yan",
            "Zhe Huang",
            "Youjia Yang",
            "Yan Pang"
        ],
        "published": "2023-10-13T14:17:33Z",
        "summary": "Large Language Models (LLMs) shows powerful capability in natural language\nunderstanding by capturing hidden semantics in vector space. This process\nenriches the value of the text embeddings for various downstream tasks, thereby\nfostering the Embedding-as-a-Service (EaaS) business model. However, the direct\ntransmission of text to servers poses a largely unaddressed risk of privacy\nleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an\ninnovative framework that split the model to execute the token embedding layer\non the client side at minimal computational cost. This allows the client to\nintroduce noise prior to transmitting the embeddings to the server, and\nsubsequently receive and denoise the perturbed output embeddings for downstream\ntasks. Our approach is designed for the inference stage of LLMs and requires no\nmodifications to the model parameters. Extensive experiments demonstrate SnD's\neffectiveness in optimizing the privacy-utility tradeoff across various LLM\narchitectures and diverse downstream tasks. The results reveal a significant\nperformance improvement under the same privacy budget compared to the baseline,\noffering clients a privacy-preserving solution for local privacy protection.",
        "pdf_link": "https://arxiv.org/pdf/2310.09130v2.pdf"
    },
    {
        "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
        "authors": [
            "Qichen Ye",
            "Junling Liu",
            "Dading Chong",
            "Peilin Zhou",
            "Yining Hua",
            "Andrew Liu"
        ],
        "published": "2023-10-13T13:17:03Z",
        "summary": "Integrating large language models (LLMs) into healthcare presents potential\nbut faces challenges. Directly pre-training LLMs for domains like medicine is\nresource-heavy and sometimes unfeasible. Sole reliance on Supervised\nFine-tuning (SFT) can result in overconfident predictions and may not tap into\ndomain specific insights. Addressing these challenges, we present a multi-stage\ntraining method combining Domain-specific Continued Pre-training (DCPT), SFT,\nand Direct Preference Optimization (DPO). A notable contribution of our study\nis the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing\nmedical question answering, plain texts, knowledge graphs, and dialogues,\nsegmented into three training stages. The medical LLM trained with our\npipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and\nSFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing\nBaichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores\n16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21.\nThis highlights the strength of our training approach in refining LLMs for\nmedical applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.09089v1.pdf"
    },
    {
        "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
        "authors": [
            "Sehyun Choi",
            "Tianqing Fang",
            "Zhaowei Wang",
            "Yangqiu Song"
        ],
        "published": "2023-10-13T12:12:34Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable human-level natural\nlanguage generation capabilities. However, their potential to generate\nmisinformation, often called the hallucination problem, poses a significant\nrisk to their deployment. A common approach to address this issue is to\nretrieve relevant knowledge and fine-tune the LLM with the knowledge in its\ninput. Unfortunately, this method incurs high training costs and may cause\ncatastrophic forgetting for multi-tasking models. To overcome these\nlimitations, we propose a knowledge-constrained decoding method called KCTS\n(Knowledge-Constrained Tree Search), which guides a frozen LM to generate text\naligned with the reference knowledge at each decoding step using a knowledge\nclassifier score and MCTS (Monte-Carlo Tree Search). To adapt the\nsequence-level knowledge classifier to token-level guidance, we also propose a\nnovel token-level hallucination detection method called RIPA (Reward Inflection\nPoint Approximation). Our empirical results on knowledge-grounded dialogue and\nabstractive summarization demonstrate the strength of KCTS as a plug-and-play,\nmodel-agnostic decoding method that can effectively reduce hallucinations in\nnatural language generation.",
        "pdf_link": "https://arxiv.org/pdf/2310.09044v1.pdf"
    },
    {
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "authors": [
            "Hung Le",
            "Hailin Chen",
            "Amrita Saha",
            "Akash Gokul",
            "Doyen Sahoo",
            "Shafiq Joty"
        ],
        "published": "2023-10-13T10:17:48Z",
        "summary": "Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.",
        "pdf_link": "https://arxiv.org/pdf/2310.08992v3.pdf"
    },
    {
        "title": "Embarrassingly Simple Text Watermarks",
        "authors": [
            "Ryoma Sato",
            "Yuki Takezawa",
            "Han Bao",
            "Kenta Niwa",
            "Makoto Yamada"
        ],
        "published": "2023-10-13T07:44:05Z",
        "summary": "We propose Easymark, a family of embarrassingly simple yet effective\nwatermarks. Text watermarking is becoming increasingly important with the\nadvent of Large Language Models (LLM). LLMs can generate texts that cannot be\ndistinguished from human-written texts. This is a serious problem for the\ncredibility of the text. Easymark is a simple yet effective solution to this\nproblem. Easymark can inject a watermark without changing the meaning of the\ntext at all while a validator can detect if a text was generated from a system\nthat adopted Easymark or not with high credibility. Easymark is extremely easy\nto implement so that it only requires a few lines of code. Easymark does not\nrequire access to LLMs, so it can be implemented on the user-side when the LLM\nproviders do not offer watermarked LLMs. In spite of its simplicity, it\nachieves higher detection accuracy and BLEU scores than the state-of-the-art\ntext watermarking methods. We also prove the impossibility theorem of perfect\nwatermarking, which is valuable in its own right. This theorem shows that no\nmatter how sophisticated a watermark is, a malicious user could remove it from\nthe text, which motivate us to use a simple watermark such as Easymark. We\ncarry out experiments with LLM-generated texts and confirm that Easymark can be\ndetected reliably without any degradation of BLEU and perplexity, and\noutperform state-of-the-art watermarks in terms of both quality and\nreliability.",
        "pdf_link": "https://arxiv.org/pdf/2310.08920v1.pdf"
    },
    {
        "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
        "authors": [
            "Yuxin Zhang",
            "Lirui Zhao",
            "Mingbao Lin",
            "Yunyun Sun",
            "Yiwu Yao",
            "Xingjia Han",
            "Jared Tanner",
            "Shiwei Liu",
            "Rongrong Ji"
        ],
        "published": "2023-10-13T07:38:52Z",
        "summary": "The ever-increasing large language models (LLMs), though opening a potential\npath for the upcoming artificial general intelligence, sadly drops a daunting\nobstacle on the way towards their on-device deployment. As one of the most\nwell-established pre-LLMs approaches in reducing model complexity, network\npruning appears to lag behind in the era of LLMs, due mostly to its costly\nfine-tuning (or re-training) necessity under the massive volumes of model\nparameter and training data. To close this industry-academia gap, we introduce\nDynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that\nslightly updates sparse LLMs without the expensive backpropagation and any\nweight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the\nreconstruction error between the dense and sparse LLMs, in the fashion of\nperforming iterative weight pruning-and-growing on top of sparse LLMs. To\naccomplish this purpose, DSnoT particularly takes into account the anticipated\nreduction in reconstruction error for pruning and growing, as well as the\nvariance w.r.t. different input data for growing each weight. This practice can\nbe executed efficiently in linear time since its obviates the need of\nbackpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,\nVicuna, and OPT across various benchmarks demonstrate the effectiveness of\nDSnoT in enhancing the performance of sparse LLMs, especially at high sparsity\nlevels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by\n26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights\ninto how to fine-tune sparse LLMs in an efficient training-free manner and open\nnew venues to scale the great potential of sparsity to LLMs. Codes are\navailable at https://github.com/zyxxmu/DSnoT.",
        "pdf_link": "https://arxiv.org/pdf/2310.08915v3.pdf"
    },
    {
        "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
        "authors": [
            "Pengyu Wang",
            "Linyang Li",
            "Ke Ren",
            "Botian Jiang",
            "Dong Zhang",
            "Xipeng Qiu"
        ],
        "published": "2023-10-13T07:18:53Z",
        "summary": "Widely applied large language models (LLMs) can generate human-like content,\nraising concerns about the abuse of LLMs. Therefore, it is important to build\nstrong AI-generated text (AIGT) detectors. Current works only consider\ndocument-level AIGT detection, therefore, in this paper, we first introduce a\nsentence-level detection challenge by synthesizing a dataset that contains\ndocuments that are polished with LLMs, that is, the documents contain sentences\nwritten by humans and sentences modified by LLMs. Then we propose\n\\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes\nlog probability lists from white-box LLMs as features for sentence-level AIGT\ndetection. These features are composed like \\textit{waves} in speech processing\nand cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution\nand self-attention networks. We test it in both sentence and document-level\ndetection challenges. Experimental results show that previous methods struggle\nin solving sentence-level AIGT detection, while our method not only\nsignificantly surpasses baseline methods in both sentence and document-level\ndetection challenges but also exhibits strong generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2310.08903v2.pdf"
    },
    {
        "title": "Exploration with Principles for Diverse AI Supervision",
        "authors": [
            "Hao Liu",
            "Matei Zaharia",
            "Pieter Abbeel"
        ],
        "published": "2023-10-13T07:03:39Z",
        "summary": "Training large transformers using next-token prediction has given rise to\ngroundbreaking advancements in AI. While this generative AI approach has\nproduced impressive results, it heavily leans on human supervision. Even\nstate-of-the-art AI models like ChatGPT depend on fine-tuning through human\ndemonstrations, demanding extensive human input and domain expertise. This\nstrong reliance on human oversight poses a significant hurdle to the\nadvancement of AI innovation. To address this limitation, we propose a novel\nparadigm termed Exploratory AI (EAI) aimed at autonomously generating\nhigh-quality training data. Drawing inspiration from unsupervised reinforcement\nlearning (RL) pretraining, EAI achieves exploration within the natural language\nspace. We accomplish this by harnessing large language models to assess the\nnovelty of generated content. Our approach employs two key components: an actor\nthat generates novel content following exploration principles and a critic that\nevaluates the generated content, offering critiques to guide the actor.\nEmpirical evaluations demonstrate that EAI significantly boosts model\nperformance on complex reasoning tasks, addressing the limitations of\nhuman-intensive supervision.",
        "pdf_link": "https://arxiv.org/pdf/2310.08899v2.pdf"
    },
    {
        "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue",
        "authors": [
            "Hongru Wang",
            "Minda Hu",
            "Yang Deng",
            "Rui Wang",
            "Fei Mi",
            "Weichao Wang",
            "Yasheng Wang",
            "Wai-Chung Kwan",
            "Irwin King",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-13T03:38:38Z",
        "summary": "Open-domain dialogue system usually requires different sources of knowledge\nto generate more informative and evidential responses. However, existing\nknowledge-grounded dialogue systems either focus on a single knowledge source\nor overlook the dependency between multiple sources of knowledge, which may\nresult in generating inconsistent or even paradoxical responses. To incorporate\nmultiple knowledge sources and dependencies between them, we propose SAFARI, a\nnovel framework that leverages the exceptional capabilities of large language\nmodels (LLMs) in planning, understanding, and incorporating under both\nsupervised and unsupervised settings. Specifically, SAFARI decouples the\nknowledge grounding into multiple sources and response generation, which allows\neasy extension to various knowledge sources including the possibility of not\nusing any sources. To study the problem, we construct a personalized\nknowledge-grounded dialogue dataset \\textit{\\textbf{K}nowledge \\textbf{B}ehind\n\\textbf{P}ersona}~(\\textbf{KBP}), which is the first to consider the dependency\nbetween persona and implicit knowledge. Experimental results on the KBP dataset\ndemonstrate that the SAFARI framework can effectively produce\npersona-consistent and knowledge-enhanced responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.08840v1.pdf"
    },
    {
        "title": "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
        "authors": [
            "Harsh Kumar",
            "Ilya Musabirov",
            "Mohi Reza",
            "Jiakai Shi",
            "Xinyuan Wang",
            "Joseph Jay Williams",
            "Anastasia Kuzminykh",
            "Michael Liut"
        ],
        "published": "2023-10-13T01:21:52Z",
        "summary": "Personalized chatbot-based teaching assistants can be crucial in addressing\nincreasing classroom sizes, especially where direct teacher presence is\nlimited. Large language models (LLMs) offer a promising avenue, with increasing\nresearch exploring their educational utility. However, the challenge lies not\nonly in establishing the efficacy of LLMs but also in discerning the nuances of\ninteraction between learners and these models, which impact learners'\nengagement and results. We conducted a formative study in an undergraduate\ncomputer science classroom (N=145) and a controlled experiment on Prolific\n(N=356) to explore the impact of four pedagogically informed guidance\nstrategies on the learners' performance, confidence and trust in LLMs. Direct\nLLM answers marginally improved performance, while refining student solutions\nfostered trust. Structured guidance reduced random queries as well as instances\nof students copy-pasting assignment questions to the LLM. Our work highlights\nthe role that teachers can play in shaping LLM-supported learning environments.",
        "pdf_link": "https://arxiv.org/pdf/2310.13712v2.pdf"
    },
    {
        "title": "End-to-end Story Plot Generator",
        "authors": [
            "Hanlin Zhu",
            "Andrew Cohen",
            "Danqing Wang",
            "Kevin Yang",
            "Xiaomeng Yang",
            "Jiantao Jiao",
            "Yuandong Tian"
        ],
        "published": "2023-10-13T00:49:59Z",
        "summary": "Story plots, while short, carry most of the essential information of a full\nstory that may contain tens of thousands of words. We study the problem of\nautomatic generation of story plots, which includes story premise, character\ndescriptions, plot outlines, etc. To generate a single engaging plot, existing\nplot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands\nof calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot,\nwhich is costly and takes at least several minutes. Moreover, the hard-wired\nnature of the method makes the pipeline non-differentiable, blocking fast\nspecialization and personalization of the plot generator. In this paper, we\npropose three models, $\\texttt{OpenPlot}$, $\\texttt{E2EPlot}$ and\n$\\texttt{RLPlot}$, to address these challenges. $\\texttt{OpenPlot}$ replaces\nexpensive OpenAI API calls with LLaMA2 (Touvron et al., 2023) calls via careful\nprompt designs, which leads to inexpensive generation of high-quality training\ndatasets of story plots. We then train an end-to-end story plot generator,\n$\\texttt{E2EPlot}$, by supervised fine-tuning (SFT) using approximately 13000\nstory plots generated by $\\texttt{OpenPlot}$. $\\texttt{E2EPlot}$ generates\nstory plots of comparable quality to $\\texttt{OpenPlot}$, and is > 10$\\times$\nfaster (1k tokens in only 30 seconds on average). Finally, we obtain\n$\\texttt{RLPlot}$ that is further fine-tuned with RLHF on several different\nreward models for different aspects of story quality, which yields 60.0$\\%$\nwinning rate against $\\texttt{E2EPlot}$ along the aspect of suspense and\nsurprise.",
        "pdf_link": "https://arxiv.org/pdf/2310.08796v1.pdf"
    },
    {
        "title": "\"Im not Racist but...\": Discovering Bias in the Internal Knowledge of Large Language Models",
        "authors": [
            "Abel Salinas",
            "Louis Penafiel",
            "Robert McCormack",
            "Fred Morstatter"
        ],
        "published": "2023-10-13T00:03:37Z",
        "summary": "Large language models (LLMs) have garnered significant attention for their\nremarkable performance in a continuously expanding set of natural language\nprocessing tasks. However, these models have been shown to harbor inherent\nsocietal biases, or stereotypes, which can adversely affect their performance\nin their many downstream applications. In this paper, we introduce a novel,\npurely prompt-based approach to uncover hidden stereotypes within any arbitrary\nLLM. Our approach dynamically generates a knowledge representation of internal\nstereotypes, enabling the identification of biases encoded within the LLM's\ninternal knowledge. By illuminating the biases present in LLMs and offering a\nsystematic methodology for their analysis, our work contributes to advancing\ntransparency and promoting fairness in natural language processing systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.08780v1.pdf"
    },
    {
        "title": "Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving",
        "authors": [
            "Karen D. Wang",
            "Eric Burkholder",
            "Carl Wieman",
            "Shima Salehi",
            "Nick Haber"
        ],
        "published": "2023-10-12T23:39:28Z",
        "summary": "The study explores the capabilities of OpenAI's ChatGPT in solving different\ntypes of physics problems. ChatGPT (with GPT-4) was queried to solve a total of\n40 problems from a college-level engineering physics course. These problems\nranged from well-specified problems, where all data required for solving the\nproblem was provided, to under-specified, real-world problems where not all\nnecessary data were given. Our findings show that ChatGPT could successfully\nsolve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for\nunder-specified problems. Analysis of the model's incorrect solutions revealed\nthree distinct failure modes: 1) failure to construct accurate models of the\nphysical world, 2) failure to make reasonable assumptions about missing data,\nand 3) calculation errors. The study offers implications for how to leverage\nLLM-augmented instructional materials to enhance STEM education. The insights\nalso contribute to the broader discourse on AI's strengths and limitations,\nserving both educators aiming to leverage the technology and researchers\ninvestigating human-AI collaboration frameworks for problem-solving and\ndecision-making.",
        "pdf_link": "https://arxiv.org/pdf/2310.08773v2.pdf"
    },
    {
        "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
        "authors": [
            "Yixiao Li",
            "Yifan Yu",
            "Chen Liang",
            "Pengcheng He",
            "Nikos Karampatziakis",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2023-10-12T18:34:08Z",
        "summary": "Quantization is an indispensable technique for serving Large Language Models\n(LLMs) and has recently found its way into LoRA fine-tuning. In this work we\nfocus on the scenario where quantization and LoRA fine-tuning are applied\ntogether on a pre-trained model. In such cases it is common to observe a\nconsistent gap in the performance on downstream tasks between full fine-tuning\nand quantization plus LoRA fine-tuning approach. In response, we propose LoftQ\n(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that\nsimultaneously quantizes an LLM and finds a proper low-rank initialization for\nLoRA fine-tuning. Such an initialization alleviates the discrepancy between the\nquantized and full-precision model and significantly improves generalization in\ndownstream tasks. We evaluate our method on natural language understanding,\nquestion answering, summarization, and natural language generation tasks.\nExperiments show that our method is highly effective and outperforms existing\nquantization methods, especially in the challenging 2-bit and 2/4-bit mixed\nprecision regimes. The code is available on https://github.com/yxli2123/LoftQ.",
        "pdf_link": "https://arxiv.org/pdf/2310.08659v4.pdf"
    },
    {
        "title": "MemGPT: Towards LLMs as Operating Systems",
        "authors": [
            "Charles Packer",
            "Sarah Wooders",
            "Kevin Lin",
            "Vivian Fang",
            "Shishir G. Patil",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "published": "2023-10-12T17:51:32Z",
        "summary": "Large language models (LLMs) have revolutionized AI, but are constrained by\nlimited context windows, hindering their utility in tasks like extended\nconversations and document analysis. To enable using context beyond limited\ncontext windows, we propose virtual context management, a technique drawing\ninspiration from hierarchical memory systems in traditional operating systems\nthat provide the appearance of large memory resources through data movement\nbetween fast and slow memory. Using this technique, we introduce MemGPT\n(Memory-GPT), a system that intelligently manages different memory tiers in\norder to effectively provide extended context within the LLM's limited context\nwindow, and utilizes interrupts to manage control flow between itself and the\nuser. We evaluate our OS-inspired design in two domains where the limited\ncontext windows of modern LLMs severely handicaps their performance: document\nanalysis, where MemGPT is able to analyze large documents that far exceed the\nunderlying LLM's context window, and multi-session chat, where MemGPT can\ncreate conversational agents that remember, reflect, and evolve dynamically\nthrough long-term interactions with their users. We release MemGPT code and\ndata for our experiments at https://memgpt.ai.",
        "pdf_link": "https://arxiv.org/pdf/2310.08560v2.pdf"
    },
    {
        "title": "Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?",
        "authors": [
            "Lingfeng Shen",
            "Aayush Mishra",
            "Daniel Khashabi"
        ],
        "published": "2023-10-12T17:32:09Z",
        "summary": "The emergence of In-Context Learning (ICL) in LLMs remains a significant\nphenomenon with little understanding. To explain ICL, recent studies try to\ntheoretically connect it to Gradient Descent (GD). We ask, does this connection\nhold up in actual pre-trained models?\n  We highlight the limiting assumptions in prior works that make their context\nconsiderably different from the practical context in which language models are\ntrained. For example, the theoretical hand-constructed weights used in these\nstudies have properties that don't match those of real LLMs. Furthermore, their\nexperimental verification uses ICL objective (training models explicitly for\nICL), which differs from the emergent ICL in the wild.\n  We also look for evidence in real models. We observe that ICL and GD have\ndifferent sensitivity to the order in which they observe demonstrations.\nFinally, we probe and compare the ICL vs. GD hypothesis in a natural setting.\nWe conduct comprehensive empirical analyses on language models pre-trained on\nnatural data (LLaMa-7B). Our comparisons of three performance metrics highlight\nthe inconsistent behavior of ICL and GD as a function of various factors such\nas datasets, models, and the number of demonstrations. We observe that ICL and\nGD modify the output distribution of language models differently. These results\nindicate that the equivalence between ICL and GD remains an open hypothesis and\ncalls for further studies.",
        "pdf_link": "https://arxiv.org/pdf/2310.08540v4.pdf"
    },
    {
        "title": "LLM-augmented Preference Learning from Natural Language",
        "authors": [
            "Inwon Kang",
            "Sikai Ruan",
            "Tyler Ho",
            "Jui-Chien Lin",
            "Farhad Mohsin",
            "Oshani Seneviratne",
            "Lirong Xia"
        ],
        "published": "2023-10-12T17:17:27Z",
        "summary": "Finding preferences expressed in natural language is an important but\nchallenging task. State-of-the-art(SotA) methods leverage transformer-based\nmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graph\nattention networks. Since Large Language Models (LLMs) are equipped to deal\nwith larger context lengths and have much larger model sizes than the\ntransformer-based model, we investigate their ability to classify comparative\ntext directly. This work aims to serve as a first step towards using LLMs for\nthe CPC task. We design and conduct a set of experiments that format the\nclassification task into an input prompt for the LLM and a methodology to get a\nfixed-format response that can be automatically evaluated. Comparing\nperformances with existing methods, we see that pre-trained LLMs are able to\noutperform the previous SotA models with no fine-tuning involved. Our results\nshow that the LLMs can consistently outperform the SotA when the target text is\nlarge -- i.e. composed of multiple sentences --, and are still comparable to\nthe SotA performance in shorter text. We also find that few-shot learning\nyields better performance than zero-shot learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.08523v1.pdf"
    },
    {
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
        "authors": [
            "Seungone Kim",
            "Jamin Shin",
            "Yejin Cho",
            "Joel Jang",
            "Shayne Longpre",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seongjin Shin",
            "Sungdong Kim",
            "James Thorne",
            "Minjoon Seo"
        ],
        "published": "2023-10-12T16:50:08Z",
        "summary": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://kaistai.github.io/prometheus/.",
        "pdf_link": "https://arxiv.org/pdf/2310.08491v2.pdf"
    },
    {
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
        "authors": [
            "Patrick Chao",
            "Alexander Robey",
            "Edgar Dobriban",
            "Hamed Hassani",
            "George J. Pappas",
            "Eric Wong"
        ],
        "published": "2023-10-12T15:38:28Z",
        "summary": "There is growing interest in ensuring that large language models (LLMs) align\nwith human values. However, the alignment of such models is vulnerable to\nadversarial jailbreaks, which coax LLMs into overriding their safety\nguardrails. The identification of these vulnerabilities is therefore\ninstrumental in understanding inherent weaknesses and preventing future misuse.\nTo this end, we propose Prompt Automatic Iterative Refinement (PAIR), an\nalgorithm that generates semantic jailbreaks with only black-box access to an\nLLM. PAIR -- which is inspired by social engineering attacks -- uses an\nattacker LLM to automatically generate jailbreaks for a separate targeted LLM\nwithout human intervention. In this way, the attacker LLM iteratively queries\nthe target LLM to update and refine a candidate jailbreak. Empirically, PAIR\noften requires fewer than twenty queries to produce a jailbreak, which is\norders of magnitude more efficient than existing algorithms. PAIR also achieves\ncompetitive jailbreaking success rates and transferability on open and\nclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
        "pdf_link": "https://arxiv.org/pdf/2310.08419v2.pdf"
    },
    {
        "title": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
        "authors": [
            "Ondrej Skopek",
            "Rahul Aralikatte",
            "Sian Gooding",
            "Victor Carbune"
        ],
        "published": "2023-10-12T15:07:11Z",
        "summary": "Despite recent advances, evaluating how well large language models (LLMs)\nfollow user instructions remains an open problem. While evaluation methods of\nlanguage models have seen a rise in prompt-based approaches, limited work on\nthe correctness of these methods has been conducted. In this work, we perform a\nmeta-evaluation of a variety of metrics to quantify how accurately they measure\nthe instruction-following abilities of LLMs. Our investigation is performed on\ngrounded query-based summarization by collecting a new short-form, real-world\ndataset riSum, containing 300 document-instruction pairs with 3 answers each.\nAll 900 answers are rated by 3 human annotators. Using riSum, we analyze the\nagreement between evaluation methods and human judgment. Finally, we propose\nnew LLM-based reference-free evaluation methods that improve upon established\nbaselines and perform on par with costly reference-based metrics that require\nhigh-quality summaries.",
        "pdf_link": "https://arxiv.org/pdf/2310.08394v2.pdf"
    },
    {
        "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
        "authors": [
            "Cheongwoong Kang",
            "Jaesik Choi"
        ],
        "published": "2023-10-12T12:01:32Z",
        "summary": "Large language models (LLMs) often make factually incorrect responses despite\ntheir success in various applications. In this paper, we hypothesize that\nrelying heavily on simple co-occurrence statistics of the pre-training corpora\nis one of the main factors that cause factual errors. Our results reveal that\nLLMs are vulnerable to the co-occurrence bias, defined as preferring frequently\nco-occurred words over the correct answer. Consequently, LLMs struggle to\nrecall facts whose subject and object rarely co-occur in the pre-training\ndataset although they are seen during finetuning. We show that co-occurrence\nbias remains despite scaling up model sizes or finetuning. Therefore, we\nsuggest finetuning on a debiased dataset to mitigate the bias by filtering out\nbiased samples whose subject-object co-occurrence count is high. Although\ndebiased finetuning allows LLMs to memorize rare facts in the training set, it\nis not effective in recalling rare facts unseen during finetuning. Further\nresearch in mitigation will help build reliable language models by preventing\npotential errors. The code is available at\n\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.",
        "pdf_link": "https://arxiv.org/pdf/2310.08256v1.pdf"
    },
    {
        "title": "Large language models can replicate cross-cultural differences in personality",
        "authors": [
            "Pawe≈Ç Niszczota",
            "Mateusz Janczak"
        ],
        "published": "2023-10-12T11:17:23Z",
        "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. Overall, we provide preliminary\nevidence that LLMs can aid cross-cultural psychological research.",
        "pdf_link": "https://arxiv.org/pdf/2310.10679v1.pdf"
    },
    {
        "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
        "authors": [
            "Chia-Yu Hung",
            "Zhiqiang Hu",
            "Yujia Hu",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-10-12T08:24:15Z",
        "summary": "Authorship verification (AV) is a fundamental task in natural language\nprocessing (NLP) and computational linguistics, with applications in forensic\nanalysis, plagiarism detection, and identification of deceptive content.\nExisting AV techniques, including traditional stylometric and deep learning\napproaches, face limitations in terms of data requirements and lack of\nexplainability. To address these limitations, this paper proposes PromptAV, a\nnovel technique that leverages Large-Language Models (LLMs) for AV by providing\nstep-by-step stylometric explanation prompts. PromptAV outperforms\nstate-of-the-art baselines, operates effectively with limited training data,\nand enhances interpretability through intuitive explanations, showcasing its\npotential as an effective and interpretable solution for the AV task.",
        "pdf_link": "https://arxiv.org/pdf/2310.08123v1.pdf"
    },
    {
        "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "published": "2023-10-12T08:22:37Z",
        "summary": "There have been widespread claims about Large Language Models (LLMs) being\nable to successfully verify or self-critique their candidate solutions in\nreasoning problems in an iterative mode. Intrigued by those claims, in this\npaper we set out to investigate the verification/self-critiquing abilities of\nlarge language models in the context of planning. We evaluate a planning system\nthat employs LLMs for both plan generation and verification. We assess the\nverifier LLM's performance against ground-truth verification, the impact of\nself-critiquing on plan generation, and the influence of varying feedback\nlevels on system performance. Using GPT-4, a state-of-the-art LLM, for both\ngeneration and verification, our findings reveal that self-critiquing appears\nto diminish plan generation performance, especially when compared to systems\nwith external, sound verifiers and the LLM verifiers in that system produce a\nnotable number of false positives, compromising the system's reliability.\nAdditionally, the nature of feedback, whether binary or detailed, showed\nminimal impact on plan generation. Collectively, our results cast doubt on the\neffectiveness of LLMs in a self-critiquing, iterative framework for planning\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.08118v1.pdf"
    },
    {
        "title": "QASiNa: Religious Domain Question Answering using Sirah Nabawiyah",
        "authors": [
            "Muhammad Razif Rizqullah",
            "Ayu Purwarianti",
            "Alham Fikri Aji"
        ],
        "published": "2023-10-12T07:52:19Z",
        "summary": "Nowadays, Question Answering (QA) tasks receive significant research focus,\nparticularly with the development of Large Language Model (LLM) such as Chat\nGPT [1]. LLM can be applied to various domains, but it contradicts the\nprinciples of information transmission when applied to the Islamic domain. In\nIslam we strictly regulates the sources of information and who can give\ninterpretations or tafseer for that sources [2]. The approach used by LLM to\ngenerate answers based on its own interpretation is similar to the concept of\ntafseer, LLM is neither an Islamic expert nor a human which is not permitted in\nIslam. Indonesia is the country with the largest Islamic believer population in\nthe world [3]. With the high influence of LLM, we need to make evaluation of\nLLM in religious domain. Currently, there is only few religious QA dataset\navailable and none of them using Sirah Nabawiyah especially in Indonesian\nLanguage. In this paper, we propose the Question Answering Sirah Nabawiyah\n(QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in\nIndonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5],\nand IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0\n[7]. XLM-R model returned the best performance on QASiNa with EM of 61.20,\nF1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance\nwith Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and\nF1-Score with higher Substring Match, the gap of EM and Substring Match get\nwider in GPT-4. The experiment indicate that Chat GPT tends to give excessive\ninterpretations as evidenced by its higher Substring Match scores compared to\nEM and F1-Score, even after providing instruction and context. This concludes\nChat GPT is unsuitable for question answering task in religious domain\nespecially for Islamic religion.",
        "pdf_link": "https://arxiv.org/pdf/2310.08102v1.pdf"
    },
    {
        "title": "Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques",
        "authors": [
            "Junxiao Shen",
            "John J. Dudley",
            "Jingyao Zheng",
            "Bill Byrne",
            "Per Ola Kristensson"
        ],
        "published": "2023-10-12T07:51:43Z",
        "summary": "Text entry is an essential task in our day-to-day digital interactions.\nNumerous intelligent features have been developed to streamline this process,\nmaking text entry more effective, efficient, and fluid. These improvements\ninclude sentence prediction and user personalization. However, as deep\nlearning-based language models become the norm for these advanced features, the\nnecessity for data collection and model fine-tuning increases. These challenges\ncan be mitigated by harnessing the in-context learning capability of large\nlanguage models such as GPT-3.5. This unique feature allows the language model\nto acquire new skills through prompts, eliminating the need for data collection\nand fine-tuning. Consequently, large language models can learn various text\nprediction techniques. We initially showed that, for a sentence prediction\ntask, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is\ncomparable with a fine-tuned GPT-3.5 model, with the latter two methods\nrequiring costly data collection, fine-tuning and post-processing. However, the\ntask of prompting large language models to specialize in specific text\nprediction tasks can be challenging, particularly for designers without\nexpertise in prompt engineering. To address this, we introduce Promptor, a\nconversational prompt generation agent designed to engage proactively with\ndesigners. Promptor can automatically generate complex prompts tailored to meet\nspecific needs, thus offering a solution to this challenge. We conducted a user\nstudy involving 24 participants creating prompts for three intelligent text\nentry tasks, half of the participants used Promptor while the other half\ndesigned prompts themselves. The results show that Promptor-designed prompts\nresult in a 35% increase in similarity and 22% in coherence over those by\ndesigners.",
        "pdf_link": "https://arxiv.org/pdf/2310.08101v2.pdf"
    },
    {
        "title": "GameGPT: Multi-agent Collaborative Framework for Game Development",
        "authors": [
            "Dake Chen",
            "Hanbin Wang",
            "Yunhao Huo",
            "Yuzhao Li",
            "Haoyang Zhang"
        ],
        "published": "2023-10-12T06:31:43Z",
        "summary": "The large language model (LLM) based agents have demonstrated their capacity\nto automate and expedite software development processes. In this paper, we\nfocus on game development and propose a multi-agent collaborative framework,\ndubbed GameGPT, to automate game development. While many studies have\npinpointed hallucination as a primary roadblock for deploying LLMs in\nproduction, we identify another concern: redundancy. Our framework presents a\nseries of methods to mitigate both concerns. These methods include dual\ncollaboration and layered approaches with several in-house lexicons, to\nmitigate the hallucination and redundancy in the planning, task identification,\nand implementation phases. Furthermore, a decoupling approach is also\nintroduced to achieve code generation with better precision.",
        "pdf_link": "https://arxiv.org/pdf/2310.08067v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Kaisheng Zeng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-10-12T04:14:28Z",
        "summary": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2310.08027v1.pdf"
    },
    {
        "title": "Effects of Human Adversarial and Affable Samples on BERT Generalization",
        "authors": [
            "Aparna Elangovan",
            "Jiayuan He",
            "Yuan Li",
            "Karin Verspoor"
        ],
        "published": "2023-10-12T03:20:43Z",
        "summary": "BERT-based models have had strong performance on leaderboards, yet have been\ndemonstrably worse in real-world settings requiring generalization. Limited\nquantities of training data is considered a key impediment to achieving\ngeneralizability in machine learning. In this paper, we examine the impact of\ntraining data quality, not quantity, on a model's generalizability. We consider\ntwo characteristics of training data: the portion of human-adversarial\n(h-adversarial), i.e., sample pairs with seemingly minor differences but\ndifferent ground-truth labels, and human-affable (h-affable) training samples,\ni.e., sample pairs with minor differences but the same ground-truth label. We\nfind that for a fixed size of training samples, as a rule of thumb, having\n10-30% h-adversarial instances improves the precision, and therefore F1, by up\nto 20 points in the tasks of text classification and relation extraction.\nIncreasing h-adversarials beyond this range can result in performance plateaus\nor even degradation. In contrast, h-affables may not contribute to a model's\ngeneralizability and may even degrade generalization performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.08008v4.pdf"
    },
    {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "authors": [
            "Zhuoyan Li",
            "Hangxiao Zhu",
            "Zhuoran Lu",
            "Ming Yin"
        ],
        "published": "2023-10-11T19:51:13Z",
        "summary": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "pdf_link": "https://arxiv.org/pdf/2310.07849v2.pdf"
    },
    {
        "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph",
        "authors": [
            "Ruotong Liao",
            "Xu Jia",
            "Yunpu Ma",
            "Yangzhe Li",
            "Volker Tresp"
        ],
        "published": "2023-10-11T18:27:12Z",
        "summary": "The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.",
        "pdf_link": "https://arxiv.org/pdf/2310.07793v4.pdf"
    },
    {
        "title": "Composite Backdoor Attacks Against Large Language Models",
        "authors": [
            "Hai Huang",
            "Zhengyu Zhao",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2023-10-11T17:21:03Z",
        "summary": "Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work\nhighlights the necessity of increased security research on the trustworthiness\nof foundation LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.07676v2.pdf"
    },
    {
        "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
        "authors": [
            "Lang Qin",
            "Yao Zhang",
            "Hongru Liang",
            "Jun Wang",
            "Zhenglu Yang"
        ],
        "published": "2023-10-11T17:00:29Z",
        "summary": "Accurate knowledge selection is critical in knowledge-grounded dialogue\nsystems. Towards a closer look at it, we offer a novel perspective to organize\nexisting literature, i.e., knowledge selection coupled with, after, and before\ngeneration. We focus on the third under-explored category of study, which can\nnot only select knowledge accurately in advance, but has the advantage to\nreduce the learning, adjustment, and interpretation burden of subsequent\nresponse generation models, especially LLMs. We propose GATE, a\ngenerator-agnostic knowledge selection method, to prepare knowledge for\nsubsequent response generation models by selecting context-related knowledge\namong different knowledge structures and variable knowledge requirements.\nExperimental results demonstrate the superiority of GATE, and indicate that\nknowledge selection before generation is a lightweight yet effective way to\nfacilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.07659v3.pdf"
    },
    {
        "title": "Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models",
        "authors": [
            "Zeqiang Lai",
            "Xizhou Zhu",
            "Jifeng Dai",
            "Yu Qiao",
            "Wenhai Wang"
        ],
        "published": "2023-10-11T16:53:40Z",
        "summary": "The revolution of artificial intelligence content generation has been rapidly\naccelerated with the booming text-to-image (T2I) diffusion models. Within just\ntwo years of development, it was unprecedentedly of high-quality, diversity,\nand creativity that the state-of-the-art models could generate. However, a\nprevalent limitation persists in the effective communication with these popular\nT2I models, such as Stable Diffusion, using natural language descriptions. This\ntypically makes an engaging image hard to obtain without expertise in prompt\nengineering with complex word compositions, magic tags, and annotations.\nInspired by the recently released DALLE3 - a T2I model directly built-in\nChatGPT that talks human language, we revisit the existing T2I systems\nendeavoring to align human intent and introduce a new task - interactive text\nto image (iT2I), where people can interact with LLM for interleaved\nhigh-quality image generation/edit/refinement and question answering with\nstronger images and text correspondences using natural language. In addressing\nthe iT2I problem, we present a simple approach that augments LLMs for iT2I with\nprompting techniques and off-the-shelf T2I models. We evaluate our approach for\niT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT,\nLLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a\nconvenient and low-cost way to introduce the iT2I ability for any existing LLMs\nand any text-to-image models without any training while bringing little\ndegradation on LLMs' inherent capabilities in, e.g., question answering and\ncode generation. We hope this work could draw broader attention and provide\ninspiration for boosting user experience in human-machine interactions\nalongside the image quality of the next-generation T2I systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.07653v2.pdf"
    },
    {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "authors": [
            "Zhiyuan Zeng",
            "Jiatong Yu",
            "Tianyu Gao",
            "Yu Meng",
            "Tanya Goyal",
            "Danqi Chen"
        ],
        "published": "2023-10-11T16:38:11Z",
        "summary": "As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these \"LLM evaluators\", particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.",
        "pdf_link": "https://arxiv.org/pdf/2310.07641v1.pdf"
    },
    {
        "title": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models",
        "authors": [
            "Yuhe Liu",
            "Changhua Pei",
            "Longlong Xu",
            "Bohan Chen",
            "Mingze Sun",
            "Zhirui Zhang",
            "Yongqian Sun",
            "Shenglin Zhang",
            "Kun Wang",
            "Haiming Zhang",
            "Jianhui Li",
            "Gaogang Xie",
            "Xidao Wen",
            "Xiaohui Nie",
            "Minghua Ma",
            "Dan Pei"
        ],
        "published": "2023-10-11T16:33:29Z",
        "summary": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public.",
        "pdf_link": "https://arxiv.org/pdf/2310.07637v3.pdf"
    },
    {
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
        "authors": [
            "Hannah Rose Kirk",
            "Andrew M. Bean",
            "Bertie Vidgen",
            "Paul R√∂ttger",
            "Scott A. Hale"
        ],
        "published": "2023-10-11T16:18:13Z",
        "summary": "Human feedback is increasingly used to steer the behaviours of Large Language\nModels (LLMs). However, it is unclear how to collect and incorporate feedback\nin a way that is efficient, effective and unbiased, especially for highly\nsubjective human preferences and values. In this paper, we survey existing\napproaches for learning from human feedback, drawing on 95 papers primarily\nfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLM\ntrends for integrating human feedback into language models. Second, we give an\noverview of present techniques and practices, as well as the motivations for\nusing feedback; conceptual frameworks for defining values and preferences; and\nhow feedback is collected and from whom. Finally, we encourage a better future\nof feedback learning in LLMs by raising five unresolved conceptual and\npractical challenges.",
        "pdf_link": "https://arxiv.org/pdf/2310.07629v1.pdf"
    },
    {
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
        "authors": [
            "Martin Pawelczyk",
            "Seth Neel",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-10-11T15:19:31Z",
        "summary": "Machine unlearning, the study of efficiently removing the impact of specific\ntraining points on the trained model, has garnered increased attention of late,\ndriven by the need to comply with privacy regulations like the Right to be\nForgotten. Although unlearning is particularly relevant for LLMs in light of\nthe copyright issues they raise, achieving precise unlearning is\ncomputationally infeasible for very large models. To this end, recent work has\nproposed several algorithms which approximate the removal of training data\nwithout retraining the model. These algorithms crucially rely on access to the\nmodel parameters in order to update them, an assumption that may not hold in\npractice due to computational constraints or when the LLM is accessed via API.\nIn this work, we propose a new class of unlearning methods for LLMs we call\n''In-Context Unlearning'', providing inputs in context and without having to\nupdate model parameters. To unlearn a particular training instance, we provide\nthe instance alongside a flipped label and additional correctly labelled\ninstances which are prepended as inputs to the LLM at inference time. Our\nexperimental results demonstrate that these contexts effectively remove\nspecific information from the training set while maintaining performance levels\nthat are competitive with (or in some cases exceed) state-of-the-art unlearning\nmethods that require access to the LLM parameters.",
        "pdf_link": "https://arxiv.org/pdf/2310.07579v2.pdf"
    },
    {
        "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
        "authors": [
            "Cunxiang Wang",
            "Xiaoze Liu",
            "Yuanhao Yue",
            "Xiangru Tang",
            "Tianhang Zhang",
            "Cheng Jiayang",
            "Yunzhi Yao",
            "Wenyang Gao",
            "Xuming Hu",
            "Zehan Qi",
            "Yidong Wang",
            "Linyi Yang",
            "Jindong Wang",
            "Xing Xie",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "published": "2023-10-11T14:18:03Z",
        "summary": "This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.07521v3.pdf"
    },
    {
        "title": "Fast-ELECTRA for Efficient Pre-training",
        "authors": [
            "Chengyu Dong",
            "Liyuan Liu",
            "Hao Cheng",
            "Jingbo Shang",
            "Jianfeng Gao",
            "Xiaodong Liu"
        ],
        "published": "2023-10-11T09:55:46Z",
        "summary": "ELECTRA pre-trains language models by detecting tokens in a sequence that\nhave been replaced by an auxiliary model. Although ELECTRA offers a significant\nboost in efficiency, its potential is constrained by the training cost brought\nby the auxiliary model. Notably, this model, which is jointly trained with the\nmain model, only serves to assist the training of the main model and is\ndiscarded post-training. This results in a substantial amount of training cost\nbeing expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which\nleverages an existing language model as the auxiliary model. To construct a\nlearning curriculum for the main model, we smooth its output distribution via\ntemperature scaling following a descending schedule. Our approach rivals the\nperformance of state-of-the-art ELECTRA-style pre-training methods, while\nsignificantly eliminating the computation and memory cost brought by the joint\ntraining of the auxiliary model. Our method also reduces the sensitivity to\nhyper-parameters and enhances the pre-training stability.",
        "pdf_link": "https://arxiv.org/pdf/2310.07347v1.pdf"
    },
    {
        "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad",
            "Jun Wang"
        ],
        "published": "2023-10-11T09:46:32Z",
        "summary": "Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms",
        "pdf_link": "https://arxiv.org/pdf/2310.07343v1.pdf"
    },
    {
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunoviƒá",
            "Martin Vechev"
        ],
        "published": "2023-10-11T08:32:46Z",
        "summary": "Current privacy research on large language models (LLMs) primarily focuses on\nthe issue of extracting memorized training data. At the same time, models'\ninference capabilities have increased drastically. This raises the key question\nof whether current LLMs could violate individuals' privacy by inferring\npersonal attributes from text given at inference time. In this work, we present\nthe first comprehensive study on the capabilities of pretrained LLMs to infer\npersonal attributes from text. We construct a dataset consisting of real Reddit\nprofiles, and show that current LLMs can infer a wide range of personal\nattributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and\n$95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time\n($240\\times$) required by humans. As people increasingly interact with\nLLM-powered chatbots across all aspects of life, we also explore the emerging\nthreat of privacy-invasive chatbots trying to extract personal information\nthrough seemingly benign questions. Finally, we show that common mitigations,\ni.e., text anonymization and model alignment, are currently ineffective at\nprotecting user privacy against LLM inference. Our findings highlight that\ncurrent LLMs can infer personal data at a previously unattainable scale. In the\nabsence of working defenses, we advocate for a broader discussion around LLM\nprivacy implications beyond memorization, striving for a wider privacy\nprotection.",
        "pdf_link": "https://arxiv.org/pdf/2310.07298v1.pdf"
    },
    {
        "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction",
        "authors": [
            "Xiang Hao",
            "Jibin Wu",
            "Jianwei Yu",
            "Chenglin Xu",
            "Kay Chen Tan"
        ],
        "published": "2023-10-11T08:17:54Z",
        "summary": "Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) extracts useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process or complement the pre-registered cues. Our experimental\nresults demonstrate competitive performance when only text-based cues are\npresented, the effectiveness of using input text as a task selector, and a new\nstate-of-the-art when combining text-based cues with pre-registered cues. To\nour knowledge, this is the first study to successfully incorporate LLMs to\nguide target speaker extraction, which can be a cornerstone for cocktail party\nproblem research.",
        "pdf_link": "https://arxiv.org/pdf/2310.07284v3.pdf"
    },
    {
        "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
        "authors": [
            "Frank Joublin",
            "Antonello Ceravola",
            "Pavel Smirnov",
            "Felix Ocker",
            "Joerg Deigmoeller",
            "Anna Belardinelli",
            "Chao Wang",
            "Stephan Hasler",
            "Daniel Tanneberg",
            "Michael Gienger"
        ],
        "published": "2023-10-11T07:39:42Z",
        "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
        "pdf_link": "https://arxiv.org/pdf/2310.07263v1.pdf"
    },
    {
        "title": "CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving",
        "authors": [
            "Yuhan Liu",
            "Hanchen Li",
            "Yihua Cheng",
            "Siddhant Ray",
            "Yuyang Huang",
            "Qizheng Zhang",
            "Kuntai Du",
            "Jiayi Yao",
            "Shan Lu",
            "Ganesh Ananthanarayanan",
            "Michael Maire",
            "Henry Hoffmann",
            "Ari Holtzman",
            "Junchen Jiang"
        ],
        "published": "2023-10-11T07:08:20Z",
        "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.",
        "pdf_link": "https://arxiv.org/pdf/2310.07240v4.pdf"
    },
    {
        "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
        "authors": [
            "Jiamin Li",
            "Qiang Su",
            "Yitao Yang",
            "Yimin Jiang",
            "Cong Wang",
            "Hong Xu"
        ],
        "published": "2023-10-11T04:30:18Z",
        "summary": "Large language models, such as OpenAI's ChatGPT, have demonstrated\nexceptional language understanding capabilities in various NLP tasks. Sparsely\nactivated mixture-of-experts (MoE) has emerged as a promising solution for\nscaling models while maintaining a constant number of computational operations.\nExisting MoE model adopts a fixed gating network where each token is computed\nby the same number of experts. However, this approach contradicts our intuition\nthat the tokens in each sequence vary in terms of their linguistic complexity\nand, consequently, require different computational costs. Little is discussed\nin prior research on the trade-off between computation per token and model\nperformance. This paper introduces adaptive gating in MoE, a flexible training\nstrategy that allows tokens to be processed by a variable number of experts\nbased on expert probability distribution. The proposed framework preserves\nsparsity while improving training efficiency. Additionally, curriculum learning\nis leveraged to further reduce training time. Extensive experiments on diverse\nNLP tasks show that adaptive gating reduces at most 22.5% training time while\nmaintaining inference quality. Moreover, we conduct a comprehensive analysis of\nthe routing decisions and present our insights when adaptive gating is used.",
        "pdf_link": "https://arxiv.org/pdf/2310.07188v1.pdf"
    },
    {
        "title": "Online Speculative Decoding",
        "authors": [
            "Xiaoxuan Liu",
            "Lanxiang Hu",
            "Peter Bailis",
            "Ion Stoica",
            "Zhijie Deng",
            "Alvin Cheung",
            "Hao Zhang"
        ],
        "published": "2023-10-11T04:03:42Z",
        "summary": "Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.",
        "pdf_link": "https://arxiv.org/pdf/2310.07177v2.pdf"
    },
    {
        "title": "Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding",
        "authors": [
            "Kexun Zhang",
            "Hongqiao Chen",
            "Lei Li",
            "William Wang"
        ],
        "published": "2023-10-10T23:37:53Z",
        "summary": "Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.",
        "pdf_link": "https://arxiv.org/pdf/2310.07075v1.pdf"
    },
    {
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
        "authors": [
            "Yangsibo Huang",
            "Samyak Gupta",
            "Mengzhou Xia",
            "Kai Li",
            "Danqi Chen"
        ],
        "published": "2023-10-10T20:15:54Z",
        "summary": "The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.06987v1.pdf"
    },
    {
        "title": "LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing",
        "authors": [
            "Stephen Moskal",
            "Sam Laney",
            "Erik Hemberg",
            "Una-May O'Reilly"
        ],
        "published": "2023-10-10T18:49:20Z",
        "summary": "In this paper, we explore the potential of Large Language Models (LLMs) to\nreason about threats, generate information about tools, and automate cyber\ncampaigns. We begin with a manual exploration of LLMs in supporting specific\nthreat-related actions and decisions. We proceed by automating the decision\nprocess in a cyber campaign. We present prompt engineering approaches for a\nplan-act-report loop for one action of a threat campaign and and a prompt\nchaining design that directs the sequential decision process of a multi-action\ncampaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the\nshort campaign we demonstrate and provide insights into prompt design for\neliciting actionable responses. We discuss the potential impact of LLMs on the\nthreat landscape and the ethical considerations of using LLMs for accelerating\nthreat actor capabilities. We report a promising, yet concerning, application\nof generative AI to cyber threats. However, the LLM's capabilities to deal with\nmore complex networks, sophisticated vulnerabilities, and the sensitivity of\nprompts are open questions. This research should spur deliberations over the\ninevitable advancements in LLM-supported cyber adversarial landscape.",
        "pdf_link": "https://arxiv.org/pdf/2310.06936v1.pdf"
    },
    {
        "title": "A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging",
        "authors": [
            "Atish Kumar Dipongkor",
            "Kevin Moran"
        ],
        "published": "2023-10-10T18:09:32Z",
        "summary": "Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.",
        "pdf_link": "https://arxiv.org/pdf/2310.06913v1.pdf"
    },
    {
        "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
        "authors": [
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Xufang Luo",
            "Dongsheng Li",
            "Chin-Yew Lin",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "published": "2023-10-10T17:59:58Z",
        "summary": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational/financial cost, longer latency, and inferior\nperformance. Some studies reveal that the performance of LLMs depends on both\nthe density and the position of the key information (question relevant) in the\ninput prompt. Inspired by these findings, we propose LongLLMLingua for prompt\ncompression towards improving LLMs' perception of the key information to\nsimultaneously address the three challenges. We conduct evaluation on a wide\nrange of long context scenarios including single-/multi-document QA, few-shot\nlearning, summarization, synthetic tasks, and code completion. The experimental\nresults show that LongLLMLingua compressed prompt can derive higher performance\nwith much less cost. The latency of the end-to-end system is also reduced. For\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost\nof up to 17.1% over the original prompt with ~4x fewer tokens as input to\nGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000\nsamples from the LongBench and ZeroScrolls benchmark, respectively.\nAdditionally, when compressing prompts of ~10k tokens at a compression rate of\n2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our\ncode is available at https://aka.ms/LLMLingua.",
        "pdf_link": "https://arxiv.org/pdf/2310.06839v1.pdf"
    },
    {
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
        "authors": [
            "Eric Zelikman",
            "Wanjing Anya Ma",
            "Jasmine E. Tran",
            "Diyi Yang",
            "Jason D. Yeatman",
            "Nick Haber"
        ],
        "published": "2023-10-10T17:59:51Z",
        "summary": "Developing an educational test can be expensive and time-consuming, as each\nitem must be written by experts and then evaluated by collecting hundreds of\nstudent responses. Moreover, many tests require multiple distinct sets of\nquestions administered throughout the school year to closely monitor students'\nprogress, known as parallel tests. In this study, we focus on tests of silent\nsentence reading efficiency, used to assess students' reading ability over\ntime. To generate high-quality parallel tests, we propose to fine-tune large\nlanguage models (LLMs) to simulate how previous students would have responded\nto unseen items. With these simulated responses, we can estimate each item's\ndifficulty and ambiguity. We first use GPT-4 to generate new test items\nfollowing a list of expert-developed rules and then apply a fine-tuned LLM to\nfilter the items based on criteria from psychological measurements. We also\npropose an optimal-transport-inspired technique for generating parallel tests\nand show the generated tests closely correspond to the original test's\ndifficulty and reliability based on crowdworker responses. Our evaluation of a\ngenerated test with 234 students from grades 2 to 8 produces test scores highly\ncorrelated (r=0.93) to those of a standard test form written by human experts\nand evaluated across thousands of K-12 students.",
        "pdf_link": "https://arxiv.org/pdf/2310.06837v1.pdf"
    },
    {
        "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
        "authors": [
            "Erik Jones",
            "Hamid Palangi",
            "Clarisse Sim√µes",
            "Varun Chandrasekaran",
            "Subhabrata Mukherjee",
            "Arindam Mitra",
            "Ahmed Awadallah",
            "Ece Kamar"
        ],
        "published": "2023-10-10T17:57:00Z",
        "summary": "Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.",
        "pdf_link": "https://arxiv.org/pdf/2310.06827v3.pdf"
    },
    {
        "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
        "authors": [
            "Samuel Marks",
            "Max Tegmark"
        ],
        "published": "2023-10-10T17:54:39Z",
        "summary": "Large Language Models (LLMs) have impressive capabilities, but are also prone\nto outputting falsehoods. Recent work has developed techniques for inferring\nwhether a LLM is telling the truth by training probes on the LLM's internal\nactivations. However, this line of work is controversial, with some authors\npointing out failures of these probes to generalize in basic ways, among other\nconceptual issues. In this work, we curate high-quality datasets of true/false\nstatements and use them to study in detail the structure of LLM representations\nof truth, drawing on three lines of evidence: 1. Visualizations of LLM\ntrue/false statement representations, which reveal clear linear structure. 2.\nTransfer experiments in which probes trained on one dataset generalize to\ndifferent datasets. 3. Causal evidence obtained by surgically intervening in a\nLLM's forward pass, causing it to treat false statements as true and vice\nversa. Overall, we present evidence that language models linearly represent the\ntruth or falsehood of factual statements. We also introduce a novel technique,\nmass-mean probing, which generalizes better and is more causally implicated in\nmodel outputs than other probing techniques.",
        "pdf_link": "https://arxiv.org/pdf/2310.06824v2.pdf"
    },
    {
        "title": "Exploring Memorization in Fine-tuned Language Models",
        "authors": [
            "Shenglai Zeng",
            "Yaxin Li",
            "Jie Ren",
            "Yiding Liu",
            "Han Xu",
            "Pengfei He",
            "Yue Xing",
            "Shuaiqiang Wang",
            "Jiliang Tang",
            "Dawei Yin"
        ],
        "published": "2023-10-10T15:41:26Z",
        "summary": "Large language models (LLMs) have shown great capabilities in various tasks\nbut also exhibited memorization of training data, raising tremendous privacy\nand copyright concerns. While prior works have studied memorization during\npre-training, the exploration of memorization during fine-tuning is rather\nlimited. Compared to pre-training, fine-tuning typically involves more\nsensitive data and diverse objectives, thus may bring distinct privacy risks\nand unique memorization behaviors. In this work, we conduct the first\ncomprehensive analysis to explore language models' (LMs) memorization during\nfine-tuning across tasks. Our studies with open-sourced and our own fine-tuned\nLMs across various tasks indicate that memorization presents a strong disparity\namong different fine-tuning tasks. We provide an intuitive explanation of this\ntask disparity via sparse coding theory and unveil a strong correlation between\nmemorization and attention score distribution.",
        "pdf_link": "https://arxiv.org/pdf/2310.06714v2.pdf"
    },
    {
        "title": "Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach",
        "authors": [
            "Zhenlan Ji",
            "Pingchuan Ma",
            "Zongjie Li",
            "Shuai Wang"
        ],
        "published": "2023-10-10T14:56:26Z",
        "summary": "While code generation has been widely used in various software development\nscenarios, the quality of the generated code is not guaranteed. This has been a\nparticular concern in the era of large language models (LLMs)- based code\ngeneration, where LLMs, deemed a complex and powerful black-box model, is\ninstructed by a high-level natural language specification, namely a prompt, to\ngenerate code. Nevertheless, effectively evaluating and explaining the code\ngeneration capability of LLMs is inherently challenging, given the complexity\nof LLMs and the lack of transparency.\n  Inspired by the recent progress in causality analysis and its application in\nsoftware engineering, this paper launches a causality analysis-based approach\nto systematically analyze the causal relations between the LLM input prompts\nand the generated code. To handle various technical challenges in this study,\nwe first propose a novel causal graph-based representation of the prompt and\nthe generated code, which is established over the fine-grained,\nhuman-understandable concepts in the input prompts. The formed causal graph is\nthen used to identify the causal relations between the prompt and the derived\ncode. We illustrate the insights that our framework can provide by studying\nover 3 popular LLMs with over 12 prompt adjustment strategies. The results of\nthese studies illustrate the potential of our technique to provide insights\ninto LLM effectiveness, and aid end-users in understanding predictions.\nAdditionally, we demonstrate that our approach provides actionable insights to\nimprove the quality of the LLM-generated code by properly calibrating the\nprompt.",
        "pdf_link": "https://arxiv.org/pdf/2310.06680v1.pdf"
    },
    {
        "title": "Automated clinical coding using off-the-shelf large language models",
        "authors": [
            "Joseph S. Boyle",
            "Antanas Kascenas",
            "Pat Lok",
            "Maria Liakata",
            "Alison Q. O'Neil"
        ],
        "published": "2023-10-10T11:56:48Z",
        "summary": "The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.",
        "pdf_link": "https://arxiv.org/pdf/2310.06552v3.pdf"
    },
    {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "published": "2023-10-10T10:17:58Z",
        "summary": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for various tasks and social simulations. Despite this, their\ncapacities to coordinate within task-oriented social contexts are\nunder-explored. Such capabilities are crucial if LLMs are to effectively mimic\nhuman-like social behavior and produce meaningful results. To bridge this gap,\nwe introduce collaborative generative agents, endowing LLM-based Agents with\nconsistent behavior patterns and task-solving abilities. We situate these\nagents in a simulated job fair environment as a case study to scrutinize their\ncoordination skills. We propose a novel framework that equips collaborative\ngenerative agents with human-like reasoning abilities and specialized skills.\nOur evaluation demonstrates that these agents show promising performance.\nHowever, we also uncover limitations that hinder their effectiveness in more\ncomplex coordination tasks. Our work provides valuable insights into the role\nand evolution of LLMs in task-oriented social simulations.",
        "pdf_link": "https://arxiv.org/pdf/2310.06500v1.pdf"
    },
    {
        "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection",
        "authors": [
            "Shiping Yang",
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2023-10-10T10:14:59Z",
        "summary": "Large Language Models (LLMs) have shown their ability to collaborate\neffectively with humans in real-world scenarios. However, LLMs are apt to\ngenerate hallucinations, i.e., makeup incorrect text and unverified\ninformation, which can cause significant damage when deployed for\nmission-critical tasks. In this paper, we propose a self-check approach based\non reverse validation to detect factual errors automatically in a zero-resource\nfashion. To facilitate future studies and assess different methods, we\nconstruct a hallucination detection benchmark named PHD, which is generated by\nChatGPT and annotated by human annotators. Contrasting previous studies of\nzero-resource hallucination detection, our method and benchmark concentrate on\npassage-level detection instead of sentence-level. We empirically evaluate our\nmethod and existing zero-resource detection methods on two datasets. The\nexperimental results demonstrate that the proposed method considerably\noutperforms the baselines while costing fewer tokens and less time.\nFurthermore, we manually analyze some hallucination cases that LLM failed to\ncapture, revealing the shared limitation of zero-resource methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.06498v2.pdf"
    },
    {
        "title": "Multilingual Jailbreak Challenges in Large Language Models",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-10-10T09:44:06Z",
        "summary": "While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
        "pdf_link": "https://arxiv.org/pdf/2310.06474v3.pdf"
    },
    {
        "title": "Constructive Large Language Models Alignment with Diverse Feedback",
        "authors": [
            "Tianshu Yu",
            "Ting-En Lin",
            "Yuchuan Wu",
            "Min Yang",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-10-10T09:20:14Z",
        "summary": "In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.06450v2.pdf"
    },
    {
        "title": "Large Language Models for Propaganda Detection",
        "authors": [
            "Kilian Sprenkamp",
            "Daniel Gordon Jones",
            "Liudmila Zavolokina"
        ],
        "published": "2023-10-10T08:46:10Z",
        "summary": "The prevalence of propaganda in our digital society poses a challenge to\nsocietal harmony and the dissemination of truth. Detecting propaganda through\nNLP in text is challenging due to subtle manipulation techniques and contextual\ndependencies. To address this issue, we investigate the effectiveness of modern\nLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.\nWe conduct experiments using the SemEval-2020 task 11 dataset, which features\nnews articles labeled with 14 propaganda techniques as a multi-label\nclassification problem. Five variations of GPT-3 and GPT-4 are employed,\nincorporating various prompt engineering and fine-tuning strategies across the\ndifferent models. We evaluate the models' performance by assessing metrics such\nas $F1$ score, $Precision$, and $Recall$, comparing the results with the\ncurrent state-of-the-art approach using RoBERTa. Our findings demonstrate that\nGPT-4 achieves comparable results to the current state-of-the-art. Further,\nthis study analyzes the potential and challenges of LLMs in complex tasks like\npropaganda detection.",
        "pdf_link": "https://arxiv.org/pdf/2310.06422v2.pdf"
    },
    {
        "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "published": "2023-10-10T03:05:44Z",
        "summary": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2310.06271v1.pdf"
    },
    {
        "title": "Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction",
        "authors": [
            "Cheng Peng",
            "Xi Yang",
            "Kaleb E Smith",
            "Zehao Yu",
            "Aokun Chen",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-10-10T01:27:08Z",
        "summary": "Objective To develop soft prompt-based learning algorithms for large language\nmodels (LLMs), examine the shape of prompts, prompt-tuning using\nfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.\nMethods We developed a soft prompt-based LLM model and compared 4 training\nstrategies including (1) fine-tuning without prompts; (2) hard-prompt with\nunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with\nfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for\nclinical concept and relation extraction on two benchmark datasets. We\nevaluated the transfer learning ability of the prompt-based learning algorithms\nin a cross-institution setting. We also assessed the few-shot learning ability.\nResults and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft\nprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept\nextraction, outperforming the traditional fine-tuning and hard prompt-based\nmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft\nprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end\nrelation extraction, outperforming the other two models by 0.2~2% and\n0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million\nparameters) LLMs have a big gap to be competitive with unfrozen models; scaling\nLLMs up to billions of parameters makes frozen LLMs competitive with unfrozen\nLLMs. For cross-institute evaluation, soft prompting with a frozen\nGatorTron-8.9B model achieved the best performance. This study demonstrates\nthat (1) machines can learn soft prompts better than humans, (2) frozen LLMs\nhave better few-shot learning ability and transfer learning ability to\nfacilitate muti-institution applications, and (3) frozen LLMs require large\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2310.06239v1.pdf"
    },
    {
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
            "Yucheng Li",
            "Bo Dong",
            "Chenghua Lin",
            "Frank Guerin"
        ],
        "published": "2023-10-09T23:03:24Z",
        "summary": "Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.06201v1.pdf"
    },
    {
        "title": "Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution",
        "authors": [
            "Navapat Nananukul",
            "Khanin Sisaengsuwanchai",
            "Mayank Kejriwal"
        ],
        "published": "2023-10-09T21:57:07Z",
        "summary": "Entity Resolution (ER) is the problem of semi-automatically determining when\ntwo entities refer to the same underlying entity, with applications ranging\nfrom healthcare to e-commerce. Traditional ER solutions required considerable\nmanual expertise, including domain-specific feature engineering, as well as\nidentification and curation of training data. Recently released large language\nmodels (LLMs) provide an opportunity to make ER more seamless and\ndomain-independent. However, it is also well known that LLMs can pose risks,\nand that the quality of their outputs can depend on how prompts are engineered.\nUnfortunately, a systematic experimental study on the effects of different\nprompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has\nbeen lacking thus far. This paper aims to address this gap by conducting such a\nstudy. We consider some relatively simple and cost-efficient ER prompt\nengineering methods and apply them to ER on two real-world datasets widely used\nin the community. We use an extensive set of experimental results to show that\nan LLM like GPT3.5 is viable for high-performing unsupervised ER, and\ninterestingly, that more complicated and detailed (and hence, expensive)\nprompting methods do not necessarily outperform simpler approaches. We provide\nbrief discussions on qualitative and error analysis, including a study of the\ninter-consistency of different prompting methods to determine whether they\nyield stable outputs. Finally, we consider some limitations of LLMs when\napplied to ER.",
        "pdf_link": "https://arxiv.org/pdf/2310.06174v2.pdf"
    },
    {
        "title": "OptiMUS: Optimization Modeling Using MIP Solvers and large language models",
        "authors": [
            "Ali AhmadiTeshnizi",
            "Wenzhi Gao",
            "Madeleine Udell"
        ],
        "published": "2023-10-09T19:47:03Z",
        "summary": "Optimization problems are pervasive across various sectors, from\nmanufacturing and distribution to healthcare. However, most such problems are\nstill solved heuristically by hand rather than optimally by state-of-the-art\nsolvers, as the expertise required to formulate and solve these problems limits\nthe widespread adoption of optimization tools and techniques. We introduce\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve MILP problems from their natural language descriptions. OptiMUS is\ncapable of developing mathematical models, writing and debugging solver code,\ndeveloping tests, and checking the validity of generated solutions. To\nbenchmark our agent, we present NLP4LP, a novel dataset of linear programming\n(LP) and mixed integer linear programming (MILP) problems. Our experiments\ndemonstrate that OptiMUS solves nearly twice as many problems as a basic LLM\nprompting strategy. OptiMUS code and NLP4LP dataset are available at\n\\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}",
        "pdf_link": "https://arxiv.org/pdf/2310.06116v2.pdf"
    },
    {
        "title": "SALMON: Self-Alignment with Instructable Reward Models",
        "authors": [
            "Zhiqing Sun",
            "Yikang Shen",
            "Hongxin Zhang",
            "Qinhong Zhou",
            "Zhenfang Chen",
            "David Cox",
            "Yiming Yang",
            "Chuang Gan"
        ],
        "published": "2023-10-09T17:56:53Z",
        "summary": "Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON, to align base language\nmodels with minimal human supervision, using only a small set of human-defined\nprinciples, yet achieving superior performance. Central to our approach is an\ninstructable reward model. Trained on synthetic preference data, this model can\ngenerate reward scores based on arbitrary human-defined principles. By merely\nadjusting these principles during the RL training phase, we gain full control\nover the preferences with the instructable reward model, subsequently\ninfluencing the behavior of the RL-trained policy models, and reducing the\nreliance on the collection of online human preferences. Applying our method to\nthe LLaMA-2-70b base language model, we developed an AI assistant named\nDromedary-2. With only 6 exemplars for in-context learning and 31 human-defined\nprinciples, Dromedary-2 significantly surpasses the performance of several\nstate-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark\ndatasets. We have open-sourced the code and model weights to encourage further\nresearch into aligning LLM-based AI agents with enhanced supervision\nefficiency, improved controllability, and scalable oversight.",
        "pdf_link": "https://arxiv.org/pdf/2310.05910v2.pdf"
    },
    {
        "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
        "authors": [
            "Kaiwen Zhou",
            "Kwonjoon Lee",
            "Teruhisa Misu",
            "Xin Eric Wang"
        ],
        "published": "2023-10-09T17:10:35Z",
        "summary": "In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.05872v1.pdf"
    },
    {
        "title": "HyperAttention: Long-context Attention in Near-Linear Time",
        "authors": [
            "Insu Han",
            "Rajesh Jayaram",
            "Amin Karbasi",
            "Vahab Mirrokni",
            "David P. Woodruff",
            "Amir Zandieh"
        ],
        "published": "2023-10-09T17:05:25Z",
        "summary": "We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.",
        "pdf_link": "https://arxiv.org/pdf/2310.05869v3.pdf"
    },
    {
        "title": "Improving Summarization with Human Edits",
        "authors": [
            "Zonghai Yao",
            "Benjamin J Schloss",
            "Sai P. Selvaraj"
        ],
        "published": "2023-10-09T16:52:07Z",
        "summary": "Recent work has shown the promise of learning with human feedback paradigms\nto produce human-determined high-quality text. Existing works use human\nfeedback to train large language models (LLMs) in general domain abstractive\nsummarization and have obtained summary quality exceeding traditional\nlikelihood training. In this paper, we focus on a less explored form of human\nfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training\n(SALT), a novel technique to use both the human-edited and model-generated data\ntogether in the training loop. In addition, we demonstrate simulating Human\nEdits with ground truth summaries coming from existing training data --\nImitation edits, along with the model-generated summaries obtained after the\ntraining, to reduce the need for expensive human-edit data. In our experiments,\nwe extend human feedback exploration from general domain summarization to\nmedical domain summarization. Our results demonstrate the effectiveness of SALT\nin improving the summary quality with Human and Imitation Edits. Through\nadditional experiments, we show that SALT outperforms the conventional RLHF\nmethod (designed for human preferences) -- DPO, when applied to human-edit\ndata. We hope the evidence in our paper prompts researchers to explore,\ncollect, and better use different human feedback approaches scalably.",
        "pdf_link": "https://arxiv.org/pdf/2310.05857v2.pdf"
    },
    {
        "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
        "authors": [
            "Ziwei Chai",
            "Tianjie Zhang",
            "Liang Wu",
            "Kaiqiao Han",
            "Xiaohai Hu",
            "Xuanwen Huang",
            "Yang Yang"
        ],
        "published": "2023-10-09T16:42:00Z",
        "summary": "The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.05845v1.pdf"
    },
    {
        "title": "SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese",
        "authors": [
            "Liang Xu",
            "Kangkang Zhao",
            "Lei Zhu",
            "Hang Xue"
        ],
        "published": "2023-10-09T16:03:22Z",
        "summary": "Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated\nremarkable abilities in natural language understanding and generation. However,\nalongside their positive impact on our daily tasks, they can also produce\nharmful content that negatively affects societal perceptions. To systematically\nassess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -\na multi-round adversarial benchmark with 4912 open-ended questions covering\nmore than 20 safety sub-dimensions. Adversarial human-model interactions and\nconversations significantly increase the challenges compared to existing\nmethods. Experiments on 13 major LLMs supporting Chinese yield the following\ninsights: 1) Closed-source models outperform open-sourced ones in terms of\nsafety; 2) Models released from China demonstrate comparable safety levels to\nLLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can\ncompete effectively in terms of safety. By introducing SC-Safety, we aim to\npromote collaborative efforts to create safer and more trustworthy LLMs. The\nbenchmark and findings provide guidance on model selection. Our benchmark can\nbe found at https://www.CLUEbenchmarks.com",
        "pdf_link": "https://arxiv.org/pdf/2310.05818v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
        "authors": [
            "Kai He",
            "Rui Mao",
            "Qika Lin",
            "Yucheng Ruan",
            "Xiang Lan",
            "Mengling Feng",
            "Erik Cambria"
        ],
        "published": "2023-10-09T13:15:23Z",
        "summary": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to datacentered methodologies.",
        "pdf_link": "https://arxiv.org/pdf/2310.05694v1.pdf"
    },
    {
        "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
        "authors": [
            "Yongfu Dai",
            "Duanyu Feng",
            "Jimin Huang",
            "Haochen Jia",
            "Qianqian Xie",
            "Yifang Zhang",
            "Weiguang Han",
            "Wei Tian",
            "Hao Wang"
        ],
        "published": "2023-10-09T11:19:55Z",
        "summary": "General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.",
        "pdf_link": "https://arxiv.org/pdf/2310.05620v2.pdf"
    },
    {
        "title": "Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization",
        "authors": [
            "Chengpeng Li",
            "Zheng Yuan",
            "Hongyi Yuan",
            "Guanting Dong",
            "Keming Lu",
            "Jiancan Wu",
            "Chuanqi Tan",
            "Xiang Wang",
            "Chang Zhou"
        ],
        "published": "2023-10-09T08:18:58Z",
        "summary": "In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,\nby complicating and diversifying the queries from GSM8K and sampling multiple\nreasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning\non subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art\non GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the\nscale of 13B). A log-linear relationship is presented between MuggleMath's\nperformance and the amount of augmented data. We also find that MuggleMath is\nweak in out-of-domain math reasoning generalization to MATH. This is attributed\nto the differences in query distribution between AugGSM8K and MATH which\nsuggest that augmentation on a single benchmark could not help with overall\nmath reasoning performance. Codes and AugGSM8K will be uploaded to\nhttps://github.com/OFA-Sys/gsm8k-ScRel.",
        "pdf_link": "https://arxiv.org/pdf/2310.05506v2.pdf"
    },
    {
        "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
        "authors": [
            "Zhihua Wen",
            "Zhiliang Tian",
            "Wei Wu",
            "Yuxin Yang",
            "Yanqi Shi",
            "Zhen Huang",
            "Dongsheng Li"
        ],
        "published": "2023-10-09T03:55:55Z",
        "summary": "Conditional story generation is significant in human-machine interaction,\nparticularly in producing stories with complex plots. While Large language\nmodels (LLMs) perform well on multiple NLP tasks, including story generation,\nit is challenging to generate stories with both complex and creative plots.\nExisting methods often rely on detailed prompts to guide LLMs to meet target\nconditions, which inadvertently restrict the creative potential of the\ngenerated stories. We argue that leveraging information from exemplary\nhuman-written stories facilitates generating more diverse plotlines. Delving\ndeeper into story details helps build complex and credible plots. In this\npaper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation\nframework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to\nenhance stories' complexity. We build a retrieval repository for target\nconditions to produce few-shot examples to prompt LLMs. Additionally, we design\nan ``asking-why'' prompting scheme that extracts a forest of evidence,\nproviding compensation for the ambiguities that may occur in the generated\nstory. This iterative process uncovers underlying story backgrounds. Finally,\nwe select the most fitting chains of evidence from the evidence forest and\nintegrate them into the generated story, thereby enhancing the narrative's\ncomplexity and credibility. Experimental results and numerous examples verify\nthe effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2310.05388v2.pdf"
    },
    {
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
        "authors": [
            "Yi Dong",
            "Zhilin Wang",
            "Makesh Narsimhan Sreedhar",
            "Xianchao Wu",
            "Oleksii Kuchaiev"
        ],
        "published": "2023-10-09T02:11:21Z",
        "summary": "Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",
        "pdf_link": "https://arxiv.org/pdf/2310.05344v1.pdf"
    },
    {
        "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models",
        "authors": [
            "Holy Lovenia",
            "Wenliang Dai",
            "Samuel Cahyawijaya",
            "Ziwei Ji",
            "Pascale Fung"
        ],
        "published": "2023-10-09T01:52:27Z",
        "summary": "Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.",
        "pdf_link": "https://arxiv.org/pdf/2310.05338v1.pdf"
    },
    {
        "title": "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems",
        "authors": [
            "Yixin Wan",
            "Jieyu Zhao",
            "Aman Chadha",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2023-10-08T21:03:18Z",
        "summary": "Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. We define generic personas to represent demographic groups, such\nas \"an Asian person\", whereas specific personas may take the form of specific\npopular Asian names like \"Yumi\". While the adoption of personas enriches user\nexperiences by making dialogue systems more engaging and approachable, it also\ncasts a shadow of potential risk by exacerbating social biases within model\nresponses, thereby causing societal harm through interactions with users. In\nthis paper, we systematically study \"persona biases\", which we define to be the\nsensitivity of dialogue models' harmful behaviors contingent upon the personas\nthey adopt. We categorize persona biases into biases in harmful expression and\nharmful agreement, and establish a comprehensive evaluation framework to\nmeasure persona biases in five aspects: Offensiveness, Toxic Continuation,\nRegard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to\ninvestigate persona biases by experimenting with UNIVERSALPERSONA, a\nsystematically constructed persona dataset encompassing various types of both\ngeneric and specific model personas. Through benchmarking on four different\nmodels -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers\nsignificant persona biases in dialogue systems. Our findings also underscore\nthe pressing need to revisit the use of personas in dialogue agents to ensure\nsafe application.",
        "pdf_link": "https://arxiv.org/pdf/2310.05280v5.pdf"
    },
    {
        "title": "Measuring reasoning capabilities of ChatGPT",
        "authors": [
            "Adrian Groza"
        ],
        "published": "2023-10-08T20:18:50Z",
        "summary": "I shall quantify the logical faults generated by ChatGPT when applied to\nreasoning tasks. For experiments, I use the 144 puzzles from the library\n\\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The\nlibrary contains puzzles of various types, including arithmetic puzzles,\nlogical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling\npuzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct\nsolutions for these puzzles were checked using the theorem prover\nProver9~\\cite{mccune2005release} and the finite models finder\nMace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order\nLogic. A first output of this study is the benchmark of 100 logical puzzles.\nFor this dataset ChatGPT provided both correct answer and justification for 7\\%\nonly. %, while BARD for 5\\%. Since the dataset seems challenging, the\nresearchers are invited to test the dataset on more advanced or tuned models\nthan ChatGPT3.5 with more crafted prompts. A second output is the\nclassification of reasoning faults conveyed by ChatGPT. This classification\nforms a basis for a taxonomy of reasoning faults generated by large language\nmodels. I have identified 67 such logical faults, among which: inconsistencies,\nimplication does not hold, unsupported claim, lack of commonsense, wrong\njustification. The 100 solutions generated by ChatGPT contain 698 logical\nfaults. That is on average, 7 fallacies for each reasoning task. A third ouput\nis the annotated answers of the ChatGPT with the corresponding logical faults.\nEach wrong statement within the ChatGPT answer was manually annotated, aiming\nto quantify the amount of faulty text generated by the language model. On\naverage, 26.03\\% from the generated text was a logical fault.",
        "pdf_link": "https://arxiv.org/pdf/2310.05993v1.pdf"
    },
    {
        "title": "MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling",
        "authors": [
            "Taewan Kim",
            "Seolyeong Bae",
            "Hyun Ah Kim",
            "Su-woo Lee",
            "Hwajung Hong",
            "Chanmo Yang",
            "Young-Ho Kim"
        ],
        "published": "2023-10-08T17:00:04Z",
        "summary": "In the mental health domain, Large Language Models (LLMs) offer promising new\nopportunities, though their inherent complexity and low controllability have\nraised questions about their suitability in clinical settings. We present\nMindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric\npatients document daily experiences through conversation. Designed in\ncollaboration with mental health professionals (MHPs), MindfulDiary takes a\nstate-based approach to safely comply with the experts' guidelines while\ncarrying on free-form conversations. Through a four-week field study involving\n28 patients with major depressive disorder and five psychiatrists, we found\nthat MindfulDiary supported patients in consistently enriching their daily\nrecords and helped psychiatrists better empathize with their patients through\nan understanding of their thoughts and daily contexts. Drawing on these\nfindings, we discuss the implications of leveraging LLMs in the mental health\ndomain, bridging the technical feasibility and their integration into clinical\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2310.05231v2.pdf"
    },
    {
        "title": "Scaling Laws of RoPE-based Extrapolation",
        "authors": [
            "Xiaoran Liu",
            "Hang Yan",
            "Shuo Zhang",
            "Chenxin An",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2023-10-08T15:50:36Z",
        "summary": "The extrapolation capability of Large Language Models (LLMs) based on Rotary\nPosition Embedding is currently a topic of considerable interest. The\nmainstream approach to addressing extrapolation with LLMs involves modifying\nRoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the\noriginal RoPE, with a larger value and providing longer fine-tuning text. In\nthis work, we first observe that fine-tuning a RoPE-based LLM with either a\nsmaller or larger base in pre-training context length could significantly\nenhance its extrapolation performance. After that, we propose\n\\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework\nfrom the periodic perspective, to describe the relationship between the\nextrapolation performance and base value as well as tuning context length. In\nthis process, we also explain the origin of the RoPE-based extrapolation issue\nby \\textbf{\\textit{critical dimension for extrapolation}}. Besides these\nobservations and analyses, we achieve extrapolation up to 1 million context\nlength within only 16K training length on LLaMA2 7B and 13B.",
        "pdf_link": "https://arxiv.org/pdf/2310.05209v2.pdf"
    },
    {
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
        "authors": [
            "Wei Shen",
            "Rui Zheng",
            "Wenyu Zhan",
            "Jun Zhao",
            "Shihan Dou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-08T15:14:39Z",
        "summary": "Reinforcement learning from human feedback serves as a crucial bridge,\naligning large language models with human and societal values. This alignment\nrequires a vast corpus of human feedback to learn a reward model, which is\nsubsequently used to finetune language models. However, we have identified that\nthe reward model often finds shortcuts to bypass its intended objectives,\nmisleadingly assuming that humans prefer longer responses. The emergence of\nlength bias often induces the model to favor longer outputs, yet it doesn't\nequate to an increase in helpful information within these outputs. In this\npaper, we propose an innovative solution, applying the Product-of-Experts (PoE)\ntechnique to separate reward modeling from the influence of sequence length. In\nour framework, the main expert concentrates on understanding human intents,\nwhile the biased expert targets the identification and capture of length bias.\nTo further enhance the learning of bias, we introduce perturbations into the\nbias-focused expert, disrupting the flow of semantic information. Experimental\nresults validate the effectiveness of our approach, indicating that language\nmodel performance is improved, irrespective of sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2310.05199v5.pdf"
    },
    {
        "title": "Factuality Challenges in the Era of Large Language Models",
        "authors": [
            "Isabelle Augenstein",
            "Timothy Baldwin",
            "Meeyoung Cha",
            "Tanmoy Chakraborty",
            "Giovanni Luca Ciampaglia",
            "David Corney",
            "Renee DiResta",
            "Emilio Ferrara",
            "Scott Hale",
            "Alon Halevy",
            "Eduard Hovy",
            "Heng Ji",
            "Filippo Menczer",
            "Ruben Miguez",
            "Preslav Nakov",
            "Dietram Scheufele",
            "Shivam Sharma",
            "Giovanni Zagni"
        ],
        "published": "2023-10-08T14:55:02Z",
        "summary": "The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.",
        "pdf_link": "https://arxiv.org/pdf/2310.05189v2.pdf"
    },
    {
        "title": "Do Large Language Models Know about Facts?",
        "authors": [
            "Xuming Hu",
            "Junzhe Chen",
            "Xiaochuan Li",
            "Yufei Guo",
            "Lijie Wen",
            "Philip S. Yu",
            "Zhijiang Guo"
        ],
        "published": "2023-10-08T14:26:55Z",
        "summary": "Large language models (LLMs) have recently driven striking performance\nimprovements across a range of natural language processing tasks. The factual\nknowledge acquired during pretraining and instruction tuning can be useful in\nvarious downstream tasks, such as question answering, and language generation.\nUnlike conventional Knowledge Bases (KBs) that explicitly store factual\nknowledge, LLMs implicitly store facts in their parameters. Content generated\nby the LLMs can often exhibit inaccuracies or deviations from the truth, due to\nfacts that can be incorrectly induced or become obsolete over time. To this\nend, we aim to comprehensively evaluate the extent and scope of factual\nknowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains\n20K diverse factual questions that span different sources, timelines, domains,\nregions, and languages. Furthermore, we investigate whether LLMs are able to\ncompose multiple facts, update factual knowledge temporally, reason over\nmultiple pieces of facts, identify subtle factual differences, and resist\nadversarial examples. Extensive experiments on different sizes and types of\nLLMs show that existing LLMs still lack factual knowledge and suffer from\nvarious spurious correlations. We believe this is a critical bottleneck for\nrealizing trustworthy artificial intelligence. The dataset Pinocchio and our\ncodes will be publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2310.05177v1.pdf"
    },
    {
        "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
        "authors": [
            "Lu Yin",
            "You Wu",
            "Zhenyu Zhang",
            "Cheng-Yu Hsieh",
            "Yaqing Wang",
            "Yiling Jia",
            "Mykola Pechenizkiy",
            "Yi Liang",
            "Zhangyang Wang",
            "Shiwei Liu"
        ],
        "published": "2023-10-08T14:22:58Z",
        "summary": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL.",
        "pdf_link": "https://arxiv.org/pdf/2310.05175v2.pdf"
    },
    {
        "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
        "authors": [
            "Xiao Pu",
            "Jingyu Zhang",
            "Xiaochuang Han",
            "Yulia Tsvetkov",
            "Tianxing He"
        ],
        "published": "2023-10-08T13:49:51Z",
        "summary": "The rampant proliferation of large language models, fluent enough to generate\ntext indistinguishable from human-written language, gives unprecedented\nimportance to the detection of machine-generated text. This work is motivated\nby an important research question: How will the detectors of machine-generated\ntext perform on outputs of a new generator, that the detectors were not trained\non? We begin by collecting generation data from a wide range of LLMs, and train\nneural detectors on data from each generator and test its performance on\nheld-out generators. While none of the detectors can generalize to all\ngenerators, we observe a consistent and interesting pattern that the detectors\ntrained on data from a medium-size LLM can zero-shot generalize to the larger\nversion. As a concrete application, we demonstrate that robust detectors can be\nbuilt on an ensemble of training data from medium-sized models.",
        "pdf_link": "https://arxiv.org/pdf/2310.05165v1.pdf"
    },
    {
        "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
        "authors": [
            "Chengwen Qi",
            "Bowen Li",
            "Binyuan Hui",
            "Bailin Wang",
            "Jinyang Li",
            "Jinwang Wu",
            "Yuanjun Laili"
        ],
        "published": "2023-10-08T13:45:05Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.05163v3.pdf"
    },
    {
        "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
        "authors": [
            "Yifan Wei",
            "Yisong Su",
            "Huanhuan Ma",
            "Xiaoyan Yu",
            "Fangyu Lei",
            "Yuanzhe Zhang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-08T13:19:52Z",
        "summary": "Large language models (LLMs) have shown nearly saturated performance on many\nnatural language processing (NLP) tasks. As a result, it is natural for people\nto believe that LLMs have also mastered abilities such as time understanding\nand reasoning. However, research on the temporal sensitivity of LLMs has been\ninsufficiently emphasized. To fill this gap, this paper constructs Multiple\nSensitive Factors Time QA (MenatQA), which encompasses three temporal factors\n(scope factor, order factor, counterfactual factor) with total 2,853 samples\nfor evaluating the time comprehension and reasoning abilities of LLMs. This\npaper tests current mainstream LLMs with different parameter sizes, ranging\nfrom billions to hundreds of billions. The results show most LLMs fall behind\nsmaller temporal reasoning models with different degree on these factors. In\nspecific, LLMs show a significant vulnerability to temporal biases and depend\nheavily on the temporal information provided in questions. Furthermore, this\npaper undertakes a preliminary investigation into potential improvement\nstrategies by devising specific prompts and leveraging external tools. These\napproaches serve as valuable baselines or references for future research\nendeavors.",
        "pdf_link": "https://arxiv.org/pdf/2310.05157v1.pdf"
    },
    {
        "title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
        "authors": [
            "John Chong Min Tan",
            "Mehul Motani"
        ],
        "published": "2023-10-08T12:37:28Z",
        "summary": "We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge\nusing Large Language Models (LLMs) as a system of multiple expert agents. Using\nthe flexibility of LLMs to be prompted to do various novel tasks using\nzero-shot, few-shot, context-grounded prompting, we explore the feasibility of\nusing LLMs to solve the ARC Challenge. We firstly convert the input image into\nmultiple suitable text-based abstraction spaces. We then utilise the\nassociative power of LLMs to derive the input-output relationship and map this\nto actions in the form of a working program, similar to Voyager / Ghost in the\nMineCraft. In addition, we use iterative environmental feedback in order to\nguide LLMs to solve the task. Our proposed approach achieves 50 solves out of\n111 training set problems (45%) with just three abstraction spaces - grid,\nobject and pixel - and we believe that with more abstraction spaces and\nlearnable actions, we will be able to solve more.",
        "pdf_link": "https://arxiv.org/pdf/2310.05146v1.pdf"
    },
    {
        "title": "Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT",
        "authors": [
            "Akshaj Kumar Veldanda",
            "Fabian Grob",
            "Shailja Thakur",
            "Hammond Pearce",
            "Benjamin Tan",
            "Ramesh Karri",
            "Siddharth Garg"
        ],
        "published": "2023-10-08T12:08:48Z",
        "summary": "Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit\napplicability across numerous tasks. One domain of interest is their use in\nalgorithmic hiring, specifically in matching resumes with job categories. Yet,\nthis introduces issues of bias on protected attributes like gender, race and\nmaternity status. The seminal work of Bertrand & Mullainathan (2003) set the\ngold-standard for identifying hiring bias via field experiments where the\nresponse rate for identical resumes that differ only in protected attributes,\ne.g., racially suggestive names such as Emily or Lakisha, is compared. We\nreplicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and\nLlama) to evaluate bias (or lack thereof) on gender, race, maternity status,\npregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)\nmatching resumes to job categories; and (2) summarizing resumes with employment\nrelevant information. Overall, LLMs are robust across race and gender. They\ndiffer in their performance on pregnancy status and political affiliation. We\nuse contrastive input decoding on open-source LLMs to uncover potential sources\nof bias.",
        "pdf_link": "https://arxiv.org/pdf/2310.05135v1.pdf"
    },
    {
        "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
        "authors": [
            "Chengcheng Han",
            "Xiaowei Du",
            "Che Zhang",
            "Yixin Lian",
            "Xiang Li",
            "Ming Gao",
            "Baoyuan Wang"
        ],
        "published": "2023-10-08T08:52:13Z",
        "summary": "Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.",
        "pdf_link": "https://arxiv.org/pdf/2310.05074v3.pdf"
    },
    {
        "title": "AvalonBench: Evaluating LLMs Playing the Game of Avalon",
        "authors": [
            "Jonathan Light",
            "Min Cai",
            "Sheng Shen",
            "Ziniu Hu"
        ],
        "published": "2023-10-08T06:37:08Z",
        "summary": "In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.",
        "pdf_link": "https://arxiv.org/pdf/2310.05036v3.pdf"
    },
    {
        "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
        "authors": [
            "Howard Chen",
            "Ramakanth Pasunuru",
            "Jason Weston",
            "Asli Celikyilmaz"
        ],
        "published": "2023-10-08T06:18:14Z",
        "summary": "Large language models (LLMs) have advanced in large strides due to the\neffectiveness of the self-attention mechanism that processes and compares all\ntokens at once. However, this mechanism comes with a fundamental issue -- the\npredetermined context window is bound to be limited. Despite attempts to extend\nthe context window through methods like extrapolating the positional embedding,\nusing recurrence, or selectively retrieving essential parts of the long\nsequence, long-text understanding continues to be a challenge. We propose an\nalternative approach which instead treats the LLM as an interactive agent,\nallowing it to decide how to read the text via iterative prompting. We\nintroduce MemWalker, a method that first processes the long context into a tree\nof summary nodes. Upon receiving a query, the model navigates this tree in\nsearch of relevant information, and responds once it gathers sufficient\ninformation. On long-text question answering tasks our method outperforms\nbaseline approaches that use long context windows, recurrence, and retrieval.\nWe show that, beyond effective reading, MemWalker enhances explainability by\nhighlighting the reasoning steps as it interactively reads the text;\npinpointing the relevant text segments related to the query.",
        "pdf_link": "https://arxiv.org/pdf/2310.05029v1.pdf"
    },
    {
        "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
        "authors": [
            "Guozheng Li",
            "Peng Wang",
            "Wenjun Ke"
        ],
        "published": "2023-10-08T06:17:39Z",
        "summary": "Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.",
        "pdf_link": "https://arxiv.org/pdf/2310.05028v4.pdf"
    },
    {
        "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
        "authors": [
            "Song Guo",
            "Jiahang Xu",
            "Li Lyna Zhang",
            "Mao Yang"
        ],
        "published": "2023-10-08T05:16:28Z",
        "summary": "Despite the remarkable success of Large Language Models (LLMs), the massive\nsize poses significant deployment challenges, particularly on\nresource-constrained hardware. While existing LLM compression methods focus on\nquantization, pruning remains relatively unexplored due to the high cost of\ntraining-based approaches and data collection challenges. One-shot pruning\nmethods, although cost-effective and data-free, have become dominant in LLM\npruning, but lead to performance decline under the structured pruning setting.\nIn this work, we introduce a new paradigm for structurally pruning LLMs, called\nCompresso. Our approach, through the collaboration of the proposed\nresource-efficient pruning algorithm and the LLM itself, learns optimal pruning\ndecisions during the training process. Compresso addresses the challenges of\nexpensive training costs and data collection by incorporating Low-Rank\nAdaptation (LoRA) into the $L_0$ regularization during the instruction tuning\nprocess. Then, we further augment the pruning algorithm by introducing a\ncollaborative prompt that fosters collaboration between the LLM and the pruning\nalgorithm, significantly boosting the overall performance. To this end,\nCompresso prunes LLaMA-7B to 5.4B, maintaining original performance and even\nsurpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments\ndemonstrate that Compresso significantly outperforms one-shot pruning baselines\nacross various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81%\nhigher scores on the commonsense reasoning, reading comprehension, MMLU, and\nBBH benchmarks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.05015v2.pdf"
    },
    {
        "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
        "authors": [
            "Yile Wang",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2023-10-08T04:22:33Z",
        "summary": "Large language models (LLMs) have shown superior performance without\ntask-specific fine-tuning. Despite the success, the knowledge stored in the\nparameters of LLMs could still be incomplete and difficult to update due to the\ncomputational costs. As complementary, retrieval-based methods can offer\nnon-parametric world knowledge and improve the performance on tasks such as\nquestion answering. However, we find that the retrieved knowledge does not\nalways help and even has a negative impact on original responses occasionally.\nTo better make use of both internal knowledge and external world knowledge, we\ninvestigate eliciting the model's ability to recognize what they know and do\nnot know (which is also called self-knowledge) and propose Self-Knowledge\nguided Retrieval augmentation (SKR), a simple yet effective method which can\nlet LLMs refer to the questions they have previously encountered and adaptively\ncall for external resources when dealing with new questions. We evaluate SKR on\nmultiple datasets and demonstrate that it outperforms chain-of-thought based\nand fully retrieval-based methods by using either InstructGPT or ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.05002v1.pdf"
    },
    {
        "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
        "authors": [
            "Fajri Koto",
            "Nurul Aisyah",
            "Haonan Li",
            "Timothy Baldwin"
        ],
        "published": "2023-10-07T21:49:38Z",
        "summary": "Although large language models (LLMs) are often pre-trained on large-scale\nmultilingual texts, their reasoning abilities and real-world knowledge are\nmainly evaluated based on English datasets. Assessing LLM capabilities beyond\nEnglish is increasingly vital but hindered due to the lack of suitable\ndatasets. In this work, we introduce IndoMMLU, the first multi-task language\nunderstanding benchmark for Indonesian culture and languages, which consists of\nquestions from primary school to university entrance exams in Indonesia. By\nemploying professional teachers, we obtain 14,981 questions across 64 tasks and\neducation levels, with 46% of the questions focusing on assessing proficiency\nin the Indonesian language and knowledge of nine local languages and cultures\nin Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass\nthe Indonesian primary school level, with limited knowledge of local Indonesian\nlanguages and culture. Other smaller models such as BLOOMZ and Falcon perform\nat even lower levels.",
        "pdf_link": "https://arxiv.org/pdf/2310.04928v2.pdf"
    },
    {
        "title": "Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM",
        "authors": [
            "Luoming Zhang",
            "Wen Fei",
            "Weijia Wu",
            "Yefei He",
            "Zhenyu Lou",
            "Hong Zhou"
        ],
        "published": "2023-10-07T14:50:28Z",
        "summary": "Large Language Models (LLMs) pose significant hardware challenges related to\nmemory requirements and computational ability. There are two mainstream\nquantization schemes for LLMs: coarse-grained ($\\textit{e.g.,}$ channel-wise)\nquantization and fine-grained ($\\textit{e.g.,}$ group-wise) quantization.\nFine-grained quantization has smaller quantization loss, consequently achieving\nsuperior performance. However, when applied to weight-activation quantization,\nit disrupts continuous integer matrix multiplication, leading to inefficient\ninference. In this paper, we introduce Dual Grained Quantization (DGQ), a novel\nA8W4 quantization for LLM that maintains superior performance while ensuring\nfast inference speed. DSQ dequantizes the fine-grained INT4 weight into\ncoarse-grained INT8 representation and preform matrix multiplication using INT8\nkernels. Besides, we develop a two-phase grid search algorithm to simplify the\ndetermination of fine-grained and coarse-grained quantization scales. We also\ndevise a percentile clipping schema for smoothing the activation outliers\nwithout the need for complex optimization techniques. Experimental results\ndemonstrate that DGQ consistently outperforms prior methods across various LLM\narchitectures and a wide range of tasks. Remarkably, by our implemented\nefficient CUTLASS kernel, we achieve $\\textbf{1.12}$ $\\times$ memory reduction\nand $\\textbf{3.24}$ $\\times$ speed gains comparing A16W4 implementation. These\nadvancements enable efficient deployment of A8W4 LLMs for real-world\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2310.04836v1.pdf"
    },
    {
        "title": "Critique Ability of Large Language Models",
        "authors": [
            "Liangchen Luo",
            "Zi Lin",
            "Yinxiao Liu",
            "Lei Shu",
            "Yun Zhu",
            "Jingbo Shang",
            "Lei Meng"
        ],
        "published": "2023-10-07T14:12:15Z",
        "summary": "Critical thinking is essential for rational decision-making and\nproblem-solving. This skill hinges on the ability to provide precise and\nreasoned critiques and is a hallmark of human intelligence. In the era of large\nlanguage models (LLMs), this study explores the ability of LLMs to deliver\naccurate critiques across various tasks. We are interested in this topic as a\ncapable critic model could not only serve as a reliable evaluator, but also as\na source of supervised signals for model tuning. Particularly, if a model can\nself-critique, it has the potential for autonomous self-improvement. To examine\nthis, we introduce a unified evaluation framework for assessing the critique\nabilities of LLMs. We develop a benchmark called CriticBench, which comprises\n3K high-quality natural language queries and corresponding model responses; and\nannotate the correctness of these responses. The benchmark cover tasks such as\nmath problem-solving, code completion, and question answering. We evaluate\nmultiple LLMs on the collected dataset and our analysis reveals several\nnoteworthy insights: (1) Critique is generally challenging for most LLMs, and\nthis capability often emerges only when models are sufficiently large. (2) In\nparticular, self-critique is especially difficult. Even top-performing LLMs\nstruggle to achieve satisfactory performance. (3) Models tend to have lower\ncritique accuracy on problems where they are most uncertain. To this end, we\nintroduce a simple yet effective baseline named self-check, which leverages\nself-critique to improve task performance for various models. We hope this\nstudy serves as an initial exploration into understanding the critique\nabilities of LLMs, and aims to inform future research, including the\ndevelopment of more proficient critic models and the application of critiques\nacross diverse tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.04815v1.pdf"
    },
    {
        "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
        "authors": [
            "Yuchen Yang",
            "Houqiang Li",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2023-10-07T12:06:53Z",
        "summary": "In recent years, large-scale language models (LLMs) have gained attention for\ntheir impressive text generation capabilities. However, these models often face\nthe challenge of \"hallucination,\" which undermines their reliability. In this\nstudy, we introduce an uncertainty-aware in-context learning framework to\nempower the model to enhance or reject its output in response to uncertainty.\nHuman-defined methods for estimating uncertainty typically assume that\n\"uncertainty is lower when the model's response is correct compared to when it\nis incorrect.\" However, setting a precise threshold to distinguish correctness\nis challenging. Therefore, we introduce uncertainty information as an\nintermediary variable that implicitly influences the model's behavior. Our\ninnovative uncertainty-aware in-context learning framework involves fine-tuning\nthe LLM using a calibration dataset. Our aim is to improve the model's\nresponses by filtering out answers with high uncertainty while considering the\nmodel's knowledge limitations. We evaluate the model's knowledge by examining\nmultiple responses to the same question for the presence of a correct answer.\nWhen the model lacks relevant knowledge, the response should indicate that the\nquestion cannot be answered. Conversely, when the model has relevant knowledge,\nthe response should provide the correct answer. Extensive experiments confirm\nthe effectiveness of our framework, leading to two key findings. First, the\nlogit output values of the LLM partly reflect inherent uncertainty. Second, our\nmodel autonomously recognizes uncertainty, resulting in improved responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.04782v1.pdf"
    },
    {
        "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
        "authors": [
            "Song Jiang",
            "Zahra Shakeri",
            "Aaron Chan",
            "Maziar Sanjabi",
            "Hamed Firooz",
            "Yinglong Xia",
            "Bugra Akyildiz",
            "Yizhou Sun",
            "Jinchao Li",
            "Qifan Wang",
            "Asli Celikyilmaz"
        ],
        "published": "2023-10-07T08:56:28Z",
        "summary": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.",
        "pdf_link": "https://arxiv.org/pdf/2310.04743v1.pdf"
    },
    {
        "title": "Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis",
        "authors": [
            "Siqi Du",
            "Shengjun Tang",
            "Weixi Wang",
            "Xiaoming Li",
            "Renzhong Guo"
        ],
        "published": "2023-10-07T06:12:39Z",
        "summary": "This paper introduces a novel framework, Tree-GPT, which incorporates Large\nLanguage Models (LLMs) into the forestry remote sensing data workflow, thereby\nenhancing the efficiency of data analysis. Currently, LLMs are unable to\nextract or comprehend information from images and may generate inaccurate text\ndue to a lack of domain knowledge, limiting their use in forestry data\nanalysis. To address this issue, we propose a modular LLM expert system,\nTree-GPT, that integrates image understanding modules, domain knowledge bases,\nand toolchains. This empowers LLMs with the ability to comprehend images,\nacquire accurate knowledge, generate code, and perform data analysis in a local\nenvironment. Specifically, the image understanding module extracts structured\ninformation from forest remote sensing images by utilizing automatic or\ninteractive generation of prompts to guide the Segment Anything Model (SAM) in\ngenerating and selecting optimal tree segmentation results. The system then\ncalculates tree structural parameters based on these results and stores them in\na database. Upon receiving a specific natural language instruction, the LLM\ngenerates code based on a thought chain to accomplish the analysis task. The\ncode is then executed by an LLM agent in a local environment and . For\necological parameter calculations, the system retrieves the corresponding\nknowledge from the knowledge base and inputs it into the LLM to guide the\ngeneration of accurate code. We tested this system on several tasks, including\nSearch, Visualization, and Machine Learning Analysis. The prototype system\nperformed well, demonstrating the potential for dynamic usage of LLMs in\nforestry research and environmental sciences.",
        "pdf_link": "https://arxiv.org/pdf/2310.04698v1.pdf"
    },
    {
        "title": "Data-Centric Financial Large Language Models",
        "authors": [
            "Zhixuan Chu",
            "Huaiyu Guo",
            "Xinyuan Zhou",
            "Yijia Wang",
            "Fei Yu",
            "Hong Chen",
            "Wanqing Xu",
            "Xin Lu",
            "Qing Cui",
            "Longfei Li",
            "Jun Zhou",
            "Sheng Li"
        ],
        "published": "2023-10-07T04:53:31Z",
        "summary": "Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.17784v2.pdf"
    },
    {
        "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
        "authors": [
            "Tian Jin",
            "Nolan Clement",
            "Xin Dong",
            "Vaishnavh Nagarajan",
            "Michael Carbin",
            "Jonathan Ragan-Kelley",
            "Gintare Karolina Dziugaite"
        ],
        "published": "2023-10-07T03:36:39Z",
        "summary": "How does scaling the number of parameters in large language models (LLMs)\naffect their core capabilities? We study two natural scaling techniques --\nweight pruning and simply training a smaller or larger model, which we refer to\nas dense scaling -- and their effects on two core capabilities of LLMs: (a)\nrecalling facts presented during pre-training and (b) processing information\npresented in-context during inference. By curating a suite of tasks that help\ndisentangle these two capabilities, we find a striking difference in how these\ntwo abilities evolve due to scaling. Reducing the model size by more than 30\\%\n(via either scaling approach) significantly decreases the ability to recall\nfacts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the\nvarious ways the model can process in-context information, ranging from\nretrieving answers from a long context to learning parameterized functions from\nin-context exemplars. The fact that both dense scaling and weight pruning\nexhibit this behavior suggests that scaling model size has an inherently\ndisparate effect on fact recall and in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.04680v1.pdf"
    },
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hongzhi Wen",
            "Haoyu Han",
            "Wei Jin",
            "Haiyang Zhang",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "published": "2023-10-07T03:14:11Z",
        "summary": "In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.",
        "pdf_link": "https://arxiv.org/pdf/2310.04668v3.pdf"
    },
    {
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
        "authors": [
            "Ted Moskovitz",
            "Aaditya K. Singh",
            "DJ Strouse",
            "Tuomas Sandholm",
            "Ruslan Salakhutdinov",
            "Anca D. Dragan",
            "Stephen McAleer"
        ],
        "published": "2023-10-06T16:59:17Z",
        "summary": "Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.",
        "pdf_link": "https://arxiv.org/pdf/2310.04373v2.pdf"
    },
    {
        "title": "Amortizing intractable inference in large language models",
        "authors": [
            "Edward J. Hu",
            "Moksh Jain",
            "Eric Elmoznino",
            "Younesse Kaddar",
            "Guillaume Lajoie",
            "Yoshua Bengio",
            "Nikolay Malkin"
        ],
        "published": "2023-10-06T16:36:08Z",
        "summary": "Autoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits\ntractable querying of this knowledge to start-to-end autoregressive sampling.\nHowever, many tasks of interest -- including sequence continuation, infilling,\nand other forms of constrained generation -- involve sampling from intractable\nposterior distributions. We address this limitation by using amortized Bayesian\ninference to sample from these intractable posteriors. Such amortization is\nalgorithmically achieved by fine-tuning LLMs via diversity-seeking\nreinforcement learning algorithms: generative flow networks (GFlowNets). We\nempirically demonstrate that this distribution-matching paradigm of LLM\nfine-tuning can serve as an effective alternative to maximum-likelihood\ntraining and reward-maximizing policy optimization. As an important\napplication, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient\nadaptation of LLMs to tasks that require multi-step rationalization and tool\nuse.",
        "pdf_link": "https://arxiv.org/pdf/2310.04363v2.pdf"
    },
    {
        "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
        "authors": [
            "Wanyun Cui",
            "Qianle Wang"
        ],
        "published": "2023-10-06T13:28:04Z",
        "summary": "Generating diverse and sophisticated instructions for downstream tasks by\nLarge Language Models (LLMs) is pivotal for advancing the effect. Current\napproaches leverage closed-source LLMs, employing in-context prompting for\ninstruction generation. However, in this paper, we found that in-context\nprompting cannot generate complex instructions with length $\\ge 100$ for tasks\nlike code completion.\n  To solve this problem, we introduce Ada-Instruct, an adaptive instruction\ngenerator developed by fine-tuning open-source LLMs. Our pivotal finding\nillustrates that fine-tuning open-source LLMs with a mere ten samples generates\nlong instructions that maintain distributional consistency for complex\nreasoning tasks. We empirically validated Ada-Instruct's efficacy across\ndifferent applications, including code completion, mathematical reasoning, and\ncommonsense reasoning. The results underscore Ada-Instruct's superiority,\nevidencing its improvements over its base models, current self-instruct\nmethods, and other state-of-the-art models.",
        "pdf_link": "https://arxiv.org/pdf/2310.04484v2.pdf"
    },
    {
        "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface",
        "authors": [
            "Anupam Purwar",
            "Rahul Sundar"
        ],
        "published": "2023-10-06T12:44:04Z",
        "summary": "Retrieving answers in a quick and low cost manner without hallucinations from\na combination of structured and unstructured data using Language models is a\nmajor hurdle. This is what prevents employment of Language models in knowledge\nretrieval automation. This becomes accentuated when one wants to integrate a\nspeech interface on top of a text based knowledge retrieval system. Besides,\nfor commercial search and chat-bot applications, complete reliance on\ncommercial large language models (LLMs) like GPT 3.5 etc. can be very costly.\nIn the present study, the authors have addressed the aforementioned problem by\nfirst developing a keyword based search framework which augments discovery of\nthe context from the document to be provided to the LLM. The keywords in turn\nare generated by a relatively smaller LLM and cached for comparison with\nkeywords generated by the same smaller LLM against the query raised. This\nsignificantly reduces time and cost to find the context within documents. Once\nthe context is set, a larger LLM uses that to provide answers based on a prompt\ntailored for Q\\&A. This research work demonstrates that use of keywords in\ncontext identification reduces the overall inference time and cost of\ninformation retrieval. Given this reduction in inference time and cost with the\nkeyword augmented retrieval framework, a speech based interface for user input\nand response readout was integrated. This allowed a seamless interaction with\nthe language model.",
        "pdf_link": "https://arxiv.org/pdf/2310.04205v2.pdf"
    },
    {
        "title": "Conversational Financial Information Retrieval Model (ConFIRM)",
        "authors": [
            "Stephen Choi",
            "William Gazeley",
            "Siu Ho Wong",
            "Tingting Li"
        ],
        "published": "2023-10-06T12:31:05Z",
        "summary": "With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.13001v3.pdf"
    },
    {
        "title": "Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models",
        "authors": [
            "Wenbei Xie"
        ],
        "published": "2023-10-06T06:20:06Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nimpressive capabilities across a range of natural language processing tasks,\nespecially in reasoning, a cornerstone for achieving Artificial General\nIntelligence (AGI). However, commonly used benchmarks may not fully encapsulate\nthe inferential abilities of these models in real-world scenarios. To address\nthis gap, a new form of Question-Answering (QA) task, termed Reasoning with\nRedundant Information Provided (RRIP), is introduced. The study designed a\nmodified version of the grade school math 8K (GSM-8K) dataset which has several\nvariants focusing on different attributes of redundant information. This\ninvestigation evaluates two popular LLMs, LlaMA2-13B-chat and generative\npre-trained transformer 3.5 (GPT-3.5), contrasting their performance on\ntraditional QA tasks against the RRIP tasks. Findings indicate that while these\nmodels achieved moderate success on standard QA benchmarks, their performance\nnotably declines when assessed on RRIP tasks. The study not only highlights the\nlimitations of current LLMs in handling redundant information but also suggests\nthat future training of these models should focus on incorporating redundant\ninformation into the training data to increase the performance on RRIP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.04039v1.pdf"
    },
    {
        "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models",
        "authors": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Tianyu Zhou",
            "Ali Babar",
            "Xiao-Yang Liu"
        ],
        "published": "2023-10-06T05:40:23Z",
        "summary": "Financial sentiment analysis is critical for valuation and investment\ndecision-making. Traditional NLP models, however, are limited by their\nparameter size and the scope of their training datasets, which hampers their\ngeneralization capabilities and effectiveness in this field. Recently, Large\nLanguage Models (LLMs) pre-trained on extensive corpora have demonstrated\nsuperior performance across various NLP tasks due to their commendable\nzero-shot abilities. Yet, directly applying LLMs to financial sentiment\nanalysis presents challenges: The discrepancy between the pre-training\nobjective of LLMs and predicting the sentiment label can compromise their\npredictive performance. Furthermore, the succinct nature of financial news,\noften devoid of sufficient context, can significantly diminish the reliability\nof LLMs' sentiment analysis. To address these challenges, we introduce a\nretrieval-augmented LLMs framework for financial sentiment analysis. This\nframework includes an instruction-tuned LLMs module, which ensures LLMs behave\nas predictors of sentiment labels, and a retrieval-augmentation module which\nretrieves additional context from reliable external sources. Benchmarked\nagainst traditional models and LLMs like ChatGPT and LLaMA, our approach\nachieves 15\\% to 48\\% performance gain in accuracy and F1 score.",
        "pdf_link": "https://arxiv.org/pdf/2310.04027v2.pdf"
    },
    {
        "title": "Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning",
        "authors": [
            "Yinger Zhang",
            "Hui Cai",
            "Xeirui Song",
            "Yicheng Chen",
            "Rui Sun",
            "Jing Zheng"
        ],
        "published": "2023-10-06T05:20:18Z",
        "summary": "While enabling large language models to implement function calling (known as\nAPIs) can greatly enhance the performance of Large Language Models (LLMs),\nfunction calling is still a challenging task due to the complicated relations\nbetween different APIs, especially in a context-learning setting without\nfine-tuning. This paper introduces ``Reverse Chain'', a controllable,\ntarget-driven approach designed to empower LLMs with the capability to operate\nexternal APIs only via prompts. Recognizing that most LLMs have limited\ntool-use capabilities, Reverse Chain limits LLMs to executing simple tasks,\ne.g., API Selection and Argument Completion. Furthermore, to manage a\ncontrollable multi-function calling, Reverse Chain adopts a generic rule based\non a backward reasoning process. This rule determines when to do API selection\nor Argument completion. To evaluate the multi-tool-use capability of LLMs, we\nhave released a compositional multi-tool task dataset, available at\n\\url{https://anonymous.4open.science/r/reverse-chain-8681}. Extensive numerical\nexperiments validate the remarkable proficiency of Reverse Chain in managing\nmultiple API calls.",
        "pdf_link": "https://arxiv.org/pdf/2310.04474v3.pdf"
    },
    {
        "title": "From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self",
        "authors": [
            "Yue Fu",
            "Sami Foell",
            "Xuhai Xu",
            "Alexis Hiniker"
        ],
        "published": "2023-10-06T02:19:10Z",
        "summary": "In the rapidly evolving landscape of AI-mediated communication (AIMC), tools\npowered by Large Language Models (LLMs) are becoming integral to interpersonal\ncommunication. Employing a mixed-methods approach, we conducted a one-week\ndiary and interview study to explore users' perceptions of these tools' ability\nto: 1) support interpersonal communication in the short-term, and 2) lead to\npotential long-term effects. Our findings indicate that participants view AIMC\nsupport favorably, citing benefits such as increased communication confidence,\nand finding precise language to express their thoughts, navigating linguistic\nand cultural barriers. However, the study also uncovers current limitations of\nAIMC tools, including verbosity, unnatural responses, and excessive emotional\nintensity. These shortcomings are further exacerbated by user concerns about\ninauthenticity and potential overreliance on the technology. Furthermore, we\nidentified four key communication spaces delineated by communication stakes\n(high or low) and relationship dynamics (formal or informal) that\ndifferentially predict users' attitudes toward AIMC tools. Specifically,\nparticipants found the tool is more suitable for communicating in formal\nrelationships than informal ones and more beneficial in high-stakes than\nlow-stakes communication.",
        "pdf_link": "https://arxiv.org/pdf/2310.03976v3.pdf"
    },
    {
        "title": "Quantized Transformer Language Model Implementations on Edge Devices",
        "authors": [
            "Mohammad Wali Ur Rahman",
            "Murad Mehrab Abrar",
            "Hunter Gibbons Copening",
            "Salim Hariri",
            "Sicong Shao",
            "Pratik Satam",
            "Soheil Salehi"
        ],
        "published": "2023-10-06T01:59:19Z",
        "summary": "Large-scale transformer-based models like the Bidirectional Encoder\nRepresentations from Transformers (BERT) are widely used for Natural Language\nProcessing (NLP) applications, wherein these models are initially pre-trained\nwith a large corpus with millions of parameters and then fine-tuned for a\ndownstream NLP task. One of the major limitations of these large-scale models\nis that they cannot be deployed on resource-constrained devices due to their\nlarge model size and increased inference latency. In order to overcome these\nlimitations, such large-scale models can be converted to an optimized\nFlatBuffer format, tailored for deployment on resource-constrained edge\ndevices. Herein, we evaluate the performance of such FlatBuffer transformed\nMobileBERT models on three different edge devices, fine-tuned for Reputation\nanalysis of English language tweets in the RepLab 2013 dataset. In addition,\nthis study encompassed an evaluation of the deployed models, wherein their\nlatency, performance, and resource efficiency were meticulously assessed. Our\nexperiment results show that, compared to the original BERT large model, the\nconverted and quantized MobileBERT models have 160$\\times$ smaller footprints\nfor a 4.1% drop in accuracy while analyzing at least one tweet per second on\nedge devices. Furthermore, our study highlights the privacy-preserving aspect\nof TinyML systems as all data is processed locally within a serverless\nenvironment.",
        "pdf_link": "https://arxiv.org/pdf/2310.03971v1.pdf"
    },
    {
        "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
        "authors": [
            "Deren Lei",
            "Yaxi Li",
            "Mengya Hu",
            "Mingyu Wang",
            "Vincent Yun",
            "Emily Ching",
            "Eslam Kamal"
        ],
        "published": "2023-10-06T00:10:46Z",
        "summary": "Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.03951v2.pdf"
    },
    {
        "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models",
        "authors": [
            "Saaket Agashe",
            "Yue Fan",
            "Anthony Reyna",
            "Xin Eric Wang"
        ],
        "published": "2023-10-05T21:18:15Z",
        "summary": "The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by\nLarge Language Models (LLMs) make them promising candidates for developing\ncoordination agents. In this study, we introduce a new LLM-Coordination\nBenchmark aimed at a detailed analysis of LLMs within the context of Pure\nCoordination Games, where participating agents need to cooperate for the most\ngain. This benchmark evaluates LLMs through two distinct tasks: (1)\n\\emph{Agentic Coordination}, where LLMs act as proactive participants for\ncooperation in 4 pure coordination games; (2) \\emph{Coordination Question\nAnswering (QA)}, where LLMs are prompted to answer 198 multiple-choice\nquestions from the 4 games for evaluation of three key reasoning abilities:\nEnvironment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to\nenable LLMs for multi-agent coordination, we introduce a Cognitive Architecture\nfor Coordination (CAC) framework that can easily integrate different LLMs as\nplug-and-play modules for pure coordination games. Our findings indicate that\nLLM agents equipped with GPT-4-turbo achieve comparable performance to\nstate-of-the-art reinforcement learning methods in games that require\ncommonsense actions based on the environment. Besides, zero-shot coordination\nexperiments reveal that, unlike RL methods, LLM agents are robust to new unseen\npartners. However, results on Coordination QA show a large room for improvement\nin the Theory of Mind reasoning and joint planning abilities of LLMs. The\nanalysis also sheds light on how the ability of LLMs to understand their\nenvironment and their partner's beliefs and intentions plays a part in their\nability to plan for coordination. Our code is available at\n\\url{https://github.com/eric-ai-lab/llm_coordination}.",
        "pdf_link": "https://arxiv.org/pdf/2310.03903v2.pdf"
    },
    {
        "title": "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms",
        "authors": [
            "Petter T√∂rnberg",
            "Diliara Valeeva",
            "Justus Uitermark",
            "Christopher Bail"
        ],
        "published": "2023-10-05T18:26:06Z",
        "summary": "Social media is often criticized for amplifying toxic discourse and\ndiscouraging constructive conversations. But designing social media platforms\nto promote better conversations is inherently challenging. This paper asks\nwhether simulating social media through a combination of Large Language Models\n(LLM) and Agent-Based Modeling can help researchers study how different news\nfeed algorithms shape the quality of online conversations. We create realistic\npersonas using data from the American National Election Study to populate\nsimulated social media platforms. Next, we prompt the agents to read and share\nnews articles - and like or comment upon each other's messages - within three\nplatforms that use different news feed algorithms. In the first platform, users\nsee the most liked and commented posts from users whom they follow. In the\nsecond, they see posts from all users - even those outside their own network.\nThe third platform employs a novel \"bridging\" algorithm that highlights posts\nthat are liked by people with opposing political views. We find this bridging\nalgorithm promotes more constructive, non-toxic, conversation across political\ndivides than the other two models. Though further research is needed to\nevaluate these findings, we argue that LLMs hold considerable potential to\nimprove simulation research on social media and many other complex social\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2310.05984v1.pdf"
    },
    {
        "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
        "authors": [
            "Paloma Sodhi",
            "S. R. K. Branavan",
            "Ryan McDonald"
        ],
        "published": "2023-10-05T17:40:09Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nperforming a range of instruction following tasks in few and zero-shot\nsettings. However, teaching LLMs to perform tasks on the web presents\nfundamental challenges -- combinatorially large open-world tasks and variations\nacross web interfaces. We tackle these challenges by leveraging LLMs to\ndecompose web tasks into a collection of sub-tasks, each of which can be solved\nby a low-level, closed-loop policy. These policies constitute a shared grammar\nacross tasks, i.e., new web tasks can be expressed as a composition of these\npolicies. We propose a novel framework, Hierarchical Policies for Web Actions\nusing LLMs (HeaP), that learns a set of hierarchical LLM prompts from\ndemonstrations for planning high-level tasks and executing them via a sequence\nof low-level policies. We evaluate HeaP against a range of baselines on a suite\nof web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as\nlive website interactions, and show that it is able to outperform prior works\nusing orders of magnitude less data.",
        "pdf_link": "https://arxiv.org/pdf/2310.03720v1.pdf"
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
        "authors": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "published": "2023-10-05T17:12:17Z",
        "summary": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.03693v1.pdf"
    },
    {
        "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
        "authors": [
            "Oscar Sainz",
            "Iker Garc√≠a-Ferrero",
            "Rodrigo Agerri",
            "Oier Lopez de Lacalle",
            "German Rigau",
            "Eneko Agirre"
        ],
        "published": "2023-10-05T16:43:13Z",
        "summary": "Large Language Models (LLMs) combined with instruction tuning have made\nsignificant progress when generalizing to unseen tasks. However, they have been\nless successful in Information Extraction (IE), lagging behind task-specific\nmodels. Typically, IE tasks are characterized by complex annotation guidelines\nthat describe the task and give examples to humans. Previous attempts to\nleverage such information have failed, even with the largest models, as they\nare not able to follow the guidelines out of the box. In this paper, we propose\nGoLLIE (Guideline-following Large Language Model for IE), a model able to\nimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned to\ncomply with annotation guidelines. Comprehensive evaluation empirically\ndemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,\noutperforming previous attempts at zero-shot information extraction. The\nablation study shows that detailed guidelines are key for good results.",
        "pdf_link": "https://arxiv.org/pdf/2310.03668v5.pdf"
    },
    {
        "title": "Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures",
        "authors": [
            "Thorsten H√§ndler"
        ],
        "published": "2023-10-05T16:37:29Z",
        "summary": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, endowing it with sophisticated language understanding and\ngeneration capabilities. However, when faced with more complex and\ninterconnected tasks that demand a profound and iterative thought process, LLMs\nreveal their inherent limitations. Autonomous LLM-powered multi-agent systems\nrepresent a strategic response to these challenges. Such systems strive for\nautonomously tackling user-prompted goals by decomposing them into manageable\ntasks and orchestrating their execution and result synthesis through a\ncollective of specialized intelligent agents. Equipped with LLM-powered\nreasoning capabilities, these agents harness the cognitive synergy of\ncollaborating with their peers, enhanced by leveraging contextual resources\nsuch as tools and datasets. While these architectures hold promising potential\nin amplifying AI capabilities, striking the right balance between different\nlevels of autonomy and alignment remains the crucial challenge for their\neffective operation. This paper proposes a comprehensive multi-dimensional\ntaxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems\nbalance the dynamic interplay between autonomy and alignment across various\naspects inherent to architectural viewpoints such as goal-driven task\nmanagement, agent composition, multi-agent collaboration, and context\ninteraction. It also includes a domain-ontology model specifying fundamental\narchitectural concepts. Our taxonomy aims to empower researchers, engineers,\nand AI practitioners to systematically analyze the architectural dynamics and\nbalancing strategies employed by these increasingly prevalent AI systems. The\nexploratory taxonomic classification of selected representative LLM-powered\nmulti-agent systems illustrates its practical utility and reveals potential for\nfuture research and development.",
        "pdf_link": "https://arxiv.org/pdf/2310.03659v1.pdf"
    },
    {
        "title": "Redefining Digital Health Interfaces with Large Language Models",
        "authors": [
            "Fergus Imrie",
            "Paulius Rauba",
            "Mihaela van der Schaar"
        ],
        "published": "2023-10-05T14:18:40Z",
        "summary": "Digital health tools have the potential to significantly improve the delivery\nof healthcare services. However, their adoption remains comparatively limited\ndue, in part, to challenges surrounding usability and trust. Large Language\nModels (LLMs) have emerged as general-purpose models with the ability to\nprocess complex information and produce human-quality text, presenting a wealth\nof potential applications in healthcare. Directly applying LLMs in clinical\nsettings is not straightforward, however, with LLMs susceptible to providing\ninconsistent or nonsensical answers. We demonstrate how LLM-based systems can\nutilize external tools and provide a novel interface between clinicians and\ndigital technologies. This enhances the utility and practical impact of digital\nhealthcare tools and AI models while addressing current issues with using LLMs\nin clinical settings such as hallucinations. We illustrate LLM-based interfaces\nwith the example of cardiovascular disease risk prediction. We develop a new\nprognostic tool using automated machine learning and demonstrate how LLMs can\nprovide a unique interface to both our model and existing risk scores,\nhighlighting the benefit compared to traditional interfaces for digital tools.",
        "pdf_link": "https://arxiv.org/pdf/2310.03560v3.pdf"
    },
    {
        "title": "Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards",
        "authors": [
            "Litton J Kurisinkel",
            "Nancy F chen"
        ],
        "published": "2023-10-05T11:29:09Z",
        "summary": "Memory-efficient large language models are good at refining text input for\nbetter readability. However, controllability is a matter of concern when it\ncomes to text generation tasks with long inputs, such as multi-document\nsummarization. In this work, we investigate for a generic controllable approach\nfor multi-document summarization that leverages the capabilities of LLMs to\nrefine the text. In particular, we train a controllable content extraction\nscheme to extract the text that will be refined by an LLM. The scheme is\ndesigned with a novel coverage and coherence intuitive policy, which is duly\nrewarded by a passively trained LLM. Our approach yields competitive results in\nthe evaluation using ROUGE metrics and outperforms potential baselines in\ncoherence, as per human evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.03473v1.pdf"
    },
    {
        "title": "Evaluating Hallucinations in Chinese Large Language Models",
        "authors": [
            "Qinyuan Cheng",
            "Tianxiang Sun",
            "Wenwei Zhang",
            "Siyin Wang",
            "Xiangyang Liu",
            "Mozhi Zhang",
            "Junliang He",
            "Mianqiu Huang",
            "Zhangyue Yin",
            "Kai Chen",
            "Xipeng Qiu"
        ],
        "published": "2023-10-05T07:57:09Z",
        "summary": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.",
        "pdf_link": "https://arxiv.org/pdf/2310.03368v4.pdf"
    },
    {
        "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning",
        "authors": [
            "Timothy Chu",
            "Zhao Song",
            "Chiwun Yang"
        ],
        "published": "2023-10-05T06:16:01Z",
        "summary": "In-context learning (ICL) is an astonishing emergent ability of large\nlanguage models (LLMs). By presenting a prompt that includes multiple\ninput-output pairs as examples and introducing a new query input, models can\ngenerate the corresponding output. However, the performance of models heavily\nrelies on the quality of the input prompt when implementing in-context\nlearning. Biased or imbalanced input prompts can significantly degrade the\nperformance of language models. To address this issue, we introduce a\nreweighted algorithm called RICL (Reweighted In-context Learning). This\nalgorithm fine-tunes language models using an unbiased validation set to\ndetermine the optimal weight for each input-output example to approximate\nunbiased in-context learning. Furthermore, we also introduce a low-cost\nreweighted algorithm, a linear optimal weight approximation algorithm called\nLARICL (Linear Approximation of Reweighted In-context Learning). This algorithm\nrequires minimal training cost while providing effective results. We prove the\nconvergence of our algorithm and validate its performance through experiments\nconducted on a numerical dataset. The experimental findings reveal a\nsubstantial improvement in comparison to benchmarks including the performance\nof casual prompt-based in-context learning and the performance of a classic\nfine-tuning method.",
        "pdf_link": "https://arxiv.org/pdf/2310.03331v1.pdf"
    },
    {
        "title": "Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise",
        "authors": [
            "Zhen wan",
            "Yating Zhang",
            "Yexiang Wang",
            "Fei Cheng",
            "Sadao Kurohashi"
        ],
        "published": "2023-10-05T05:55:06Z",
        "summary": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased",
        "pdf_link": "https://arxiv.org/pdf/2310.03328v2.pdf"
    },
    {
        "title": "Investigating the Limitation of CLIP Models: The Worst-Performing Categories",
        "authors": [
            "Jie-Jing Shao",
            "Jiang-Xin Shi",
            "Xiao-Wen Yang",
            "Lan-Zhe Guo",
            "Yu-Feng Li"
        ],
        "published": "2023-10-05T05:37:33Z",
        "summary": "Contrastive Language-Image Pre-training (CLIP) provides a foundation model by\nintegrating natural language into visual concepts, enabling zero-shot\nrecognition on downstream tasks. It is usually expected that satisfactory\noverall accuracy can be achieved across numerous domains through well-designed\ntextual prompts. However, we found that their performance in the worst\ncategories is significantly inferior to the overall performance. For example,\non ImageNet, there are a total of 10 categories with class-wise accuracy as low\nas 0\\%, even though the overall performance has achieved 64.1\\%. This\nphenomenon reveals the potential risks associated with using CLIP models,\nparticularly in risk-sensitive applications where specific categories hold\nsignificant importance. To address this issue, we investigate the alignment\nbetween the two modalities in the CLIP model and propose the Class-wise\nMatching Margin (\\cmm) to measure the inference confusion. \\cmm\\ can\neffectively identify the worst-performing categories and estimate the potential\nperformance of the candidate prompts. We further query large language models to\nenrich descriptions of worst-performing categories and build a weighted\nensemble to highlight the efficient prompts. Experimental results clearly\nverify the effectiveness of our proposal, where the accuracy on the worst-10\ncategories on ImageNet is boosted to 5.2\\%, without manual prompt engineering,\nlaborious optimization, or access to labeled validation data.",
        "pdf_link": "https://arxiv.org/pdf/2310.03324v1.pdf"
    },
    {
        "title": "Learning Personalized Story Evaluation",
        "authors": [
            "Danqing Wang",
            "Kevin Yang",
            "Hanlin Zhu",
            "Xiaomeng Yang",
            "Andrew Cohen",
            "Lei Li",
            "Yuandong Tian"
        ],
        "published": "2023-10-05T04:15:48Z",
        "summary": "While large language models (LLMs) have shown impressive results for more\nobjective tasks such as QA and retrieval, it remains nontrivial to evaluate\ntheir performance on open-ended text generation for reasons including (1) data\ncontamination; (2) multi-dimensional evaluation criteria; and (3)\nsubjectiveness stemming from reviewers' personal preferences. To address such\nissues, we propose to model personalization in an uncontaminated open-ended\ngeneration assessment. We create two new datasets Per-MPST and Per-DOC for\npersonalized story evaluation, by re-purposing existing datasets with proper\nanonymization and new personalized labels. We further develop a personalized\nstory evaluation model PERSE to infer reviewer preferences and provide a\npersonalized evaluation. Specifically, given a few exemplary reviews from a\nparticular reviewer, PERSE predicts either a detailed review or fine-grained\ncomparison in several aspects (such as interestingness and surprise) for that\nreviewer on a new text input. Experimental results show that PERSE outperforms\nGPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on\npairwise preference prediction accuracy. Both datasets and code will be\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2310.03304v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models As AI Research Agents",
        "authors": [
            "Qian Huang",
            "Jian Vora",
            "Percy Liang",
            "Jure Leskovec"
        ],
        "published": "2023-10-05T04:06:12Z",
        "summary": "Scientific experimentation involves an iterative process of creating\nhypotheses, designing experiments, running experiments, and analyzing the\nresults. Can we build AI research agents to perform these long-horizon tasks?\nTo take a step towards building and evaluating research agents on such\nopen-ended decision-making tasks, we focus on the problem of machine learning\nengineering: given a task description and a dataset, build a high-performing\nmodel. In this paper, we propose MLAgentBench, a suite of ML tasks for\nbenchmarking AI research agents. Agents can perform actions like\nreading/writing files, executing code, and inspecting outputs. With these\nactions, agents could run experiments, analyze the results, and modify the code\nof entire machine learning pipelines, such as data processing, architecture,\ntraining processes, etc. The benchmark then automatically evaluates the agent's\nperformance objectively over various metrics related to performance and\nefficiency. We also design an LLM-based research agent to automatically perform\nexperimentation loops in such an environment. Empirically, we find that a\nGPT-4-based research agent can feasibly build compelling ML models over many\ntasks in MLAgentBench, displaying highly interpretable plans and actions.\nHowever, the success rates vary considerably; they span from almost 90\\% on\nwell-established older datasets to as low as 10\\% on recent Kaggle Challenges\n-- unavailable during the LLM model's pretraining -- and even 0\\% on newer\nresearch challenges like BabyLM. Finally, we identify several key challenges\nfor LLM-based research agents such as long-term planning and hallucination. Our\ncode is released at https://github.com/snap-stanford/MLAgentBench.",
        "pdf_link": "https://arxiv.org/pdf/2310.03302v1.pdf"
    },
    {
        "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions",
        "authors": [
            "Siwei Wu",
            "Xiangqing Shen",
            "Rui Xia"
        ],
        "published": "2023-10-05T03:45:54Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.03293v1.pdf"
    },
    {
        "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
        "authors": [
            "Ke Shen",
            "Mayank Kejriwal"
        ],
        "published": "2023-10-05T03:20:41Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive\nmilestones in natural language processing (NLP). Despite their impressive\nperformance, the models are known to pose important risks. As these models are\ndeployed in real-world applications, a systematic understanding of different\nrisks posed by these models on tasks such as natural language inference (NLI),\nis much needed. In this paper, we define and formalize two distinct types of\nrisk: decision risk and composite risk. We also propose a risk-centric\nevaluation framework, and four novel metrics, for assessing LLMs on these risks\nin both in-domain and out-of-domain settings. Finally, we propose a\nrisk-adjusted calibration method called DwD for helping LLMs minimize these\nrisks in an overall NLI architecture. Detailed experiments, using four NLI\nbenchmarks, three baselines and two LLMs, including ChatGPT, show both the\npractical utility of the evaluation framework, and the efficacy of DwD in\nreducing decision and composite risk. For instance, when using DwD, an\nunderlying LLM is able to address an extra 20.1% of low-risk inference tasks\n(but which the LLM erroneously deems high-risk without risk adjustment) and\nskip a further 19.8% of high-risk tasks, which would have been answered\nincorrectly.",
        "pdf_link": "https://arxiv.org/pdf/2310.03283v1.pdf"
    },
    {
        "title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
        "authors": [
            "Zeyuan Wang",
            "Qiang Zhang",
            "Keyan Ding",
            "Ming Qin",
            "Xiang Zhuang",
            "Xiaotong Li",
            "Huajun Chen"
        ],
        "published": "2023-10-05T02:45:39Z",
        "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.",
        "pdf_link": "https://arxiv.org/pdf/2310.03269v1.pdf"
    },
    {
        "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
        "authors": [
            "Shengding Hu",
            "Xin Liu",
            "Xu Han",
            "Xinrong Zhang",
            "Chaoqun He",
            "Weilin Zhao",
            "Yankai Lin",
            "Ning Ding",
            "Zebin Ou",
            "Guoyang Zeng",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-10-05T02:35:00Z",
        "summary": "The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.",
        "pdf_link": "https://arxiv.org/pdf/2310.03262v2.pdf"
    },
    {
        "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning",
        "authors": [
            "Mohamed Aghzal",
            "Erion Plaku",
            "Ziyu Yao"
        ],
        "published": "2023-10-05T01:42:16Z",
        "summary": "Large language models (LLMs) have achieved remarkable success across a wide\nspectrum of tasks; however, they still face limitations in scenarios that\ndemand long-term planning and spatial reasoning. To facilitate this line of\nresearch, in this work, we propose a new benchmark, termed $\\textbf{P}$ath\n$\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage\n($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by\nformulating ''path planning'' tasks that require an LLM to navigate to target\nlocations while avoiding obstacles and adhering to constraints. Leveraging this\nbenchmark, we systematically investigate LLMs including GPT-4 via different\nfew-shot prompting methodologies as well as BART and T5 of various sizes via\nfine-tuning. Our experimental results show the promise of few-shot GPT-4 in\nspatial reasoning, when it is prompted to reason and act interleavedly,\nalthough it still fails to perform long-term temporal reasoning. In contrast,\nwhile fine-tuned LLMs achieved impressive results on in-distribution reasoning\ntasks, they struggled to generalize to larger environments or environments with\nmore obstacles.",
        "pdf_link": "https://arxiv.org/pdf/2310.03249v2.pdf"
    },
    {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "authors": [
            "Tu Vu",
            "Mohit Iyyer",
            "Xuezhi Wang",
            "Noah Constant",
            "Jerry Wei",
            "Jason Wei",
            "Chris Tar",
            "Yun-Hsuan Sung",
            "Denny Zhou",
            "Quoc Le",
            "Thang Luong"
        ],
        "published": "2023-10-05T00:04:12Z",
        "summary": "Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.",
        "pdf_link": "https://arxiv.org/pdf/2310.03214v2.pdf"
    },
    {
        "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
        "authors": [
            "Xiaohan Fu",
            "Zihan Wang",
            "Shuheng Li",
            "Rajesh K. Gupta",
            "Niloofar Mireshghallah",
            "Taylor Berg-Kirkpatrick",
            "Earlence Fernandes"
        ],
        "published": "2023-10-04T22:10:01Z",
        "summary": "Large Language Models (LLMs) are being enhanced with the ability to use tools\nand to process multiple modalities. These new capabilities bring new benefits\nand also new security risks. In this work, we show that an attacker can use\nvisual adversarial examples to cause attacker-desired tool usage. For example,\nthe attacker could cause a victim LLM to delete calendar events, leak private\nconversations and book hotels. Different from prior work, our attacks can\naffect the confidentiality and integrity of user resources connected to the LLM\nwhile being stealthy and generalizable to multiple input prompts. We construct\nthese attacks using gradient-based adversarial training and characterize\nperformance along multiple dimensions. We find that our adversarial images can\nmanipulate the LLM to invoke tools following real-world syntax almost always\n(~98%) while maintaining high similarity to clean images (~0.9 SSIM).\nFurthermore, using human scoring and automated metrics, we find that the\nattacks do not noticeably affect the conversation (and its semantics) between\nthe user and the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.03185v1.pdf"
    },
    {
        "title": "Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference",
        "authors": [
            "Zachary Levonian",
            "Chenglu Li",
            "Wangda Zhu",
            "Anoushka Gade",
            "Owen Henkel",
            "Millie-Ellen Postle",
            "Wanli Xing"
        ],
        "published": "2023-10-04T22:09:28Z",
        "summary": "For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.",
        "pdf_link": "https://arxiv.org/pdf/2310.03184v2.pdf"
    },
    {
        "title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis",
        "authors": [
            "Zishun Yu",
            "Yunzhe Tao",
            "Liyu Chen",
            "Tao Sun",
            "Hongxia Yang"
        ],
        "published": "2023-10-04T21:40:36Z",
        "summary": "Program synthesis aims to create accurate, executable programs from problem\nspecifications, specifically from natural language descriptions in our context.\nRecent studies have leveraged the power of reinforcement learning (RL) in\nconjunction with large language models (LLMs), significantly enhancing code\ngeneration capabilities. The application of RL focuses on directly optimizing\nfor functional correctness, offering an advantage over conventional supervised\nmethods. Despite policy-based RL methods dominating the literature on RL for\nprogram synthesis, the nature of program synthesis tasks hints at a natural\nalignment with value-based methods. This stems from the rich collection of\noff-policy programs, including those developed by human programmers and also\nhistorical samples, coupled with the straightforward verification of generated\nprograms through automated unit testing, meaning rewards are easy to obtain.\nDiverging from the dominant use of policy-based algorithms, our work explores\nthe feasibility of value-based approaches, leading to the development of our\n$\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based\nmethods presents challenges due to the enormous search space inherent to\nprogram synthesis. To this end, we introduce an initialization protocol for RL\nagents utilizing pre-trained LMs and a conservative Bellman operator to reduce\ntraining complexities. Moreover, we demonstrate how to leverage the learned\nvalue functions as a dual strategy to post-process generated programs. Our\nempirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in\nachieving state-of-the-art performance when compared to policy-based methods.\nRemarkably, this achievement is reached with minimal reward engineering effort,\nhighlighting the effectiveness of value-based RL, independent of reward\ndesigns.",
        "pdf_link": "https://arxiv.org/pdf/2310.03173v2.pdf"
    },
    {
        "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
        "authors": [
            "Satwik Bhattamishra",
            "Arkil Patel",
            "Phil Blunsom",
            "Varun Kanade"
        ],
        "published": "2023-10-04T17:57:33Z",
        "summary": "In order to understand the in-context learning phenomenon, recent works have\nadopted a stylized experimental framework and demonstrated that Transformers\ncan learn gradient-based learning algorithms for various classes of real-valued\nfunctions. However, the limitations of Transformers in implementing learning\nalgorithms, and their ability to learn other forms of algorithms are not well\nunderstood. Additionally, the degree to which these capabilities are confined\nto attention-based models is unclear. Furthermore, it remains to be seen\nwhether the insights derived from these stylized settings can be extrapolated\nto pretrained Large Language Models (LLMs). In this work, we take a step\ntowards answering these questions by demonstrating the following: (a) On a\ntest-bed with a variety of Boolean function classes, we find that Transformers\ncan nearly match the optimal learning algorithm for 'simpler' tasks, while\ntheir performance deteriorates on more 'complex' tasks. Additionally, we find\nthat certain attention-free models perform (almost) identically to Transformers\non a range of tasks. (b) When provided a teaching sequence, i.e. a set of\nexamples that uniquely identifies a function in a class, we show that\nTransformers learn more sample-efficiently. Interestingly, our results show\nthat Transformers can learn to implement two distinct algorithms to solve a\nsingle task, and can adaptively select the more sample-efficient algorithm\ndepending on the sequence of in-context examples. (c) Lastly, we show that\nextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines\non prediction tasks that are guaranteed to not be in their training set.",
        "pdf_link": "https://arxiv.org/pdf/2310.03016v1.pdf"
    },
    {
        "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "authors": [
            "Siddharth Samsi",
            "Dan Zhao",
            "Joseph McDonald",
            "Baolin Li",
            "Adam Michaleas",
            "Michael Jones",
            "William Bergeron",
            "Jeremy Kepner",
            "Devesh Tiwari",
            "Vijay Gadepally"
        ],
        "published": "2023-10-04T17:41:59Z",
        "summary": "Large language models (LLMs) have exploded in popularity due to their new\ngenerative capabilities that go far beyond prior state-of-the-art. These\ntechnologies are increasingly being leveraged in various domains such as law,\nfinance, and medicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for inference.\nInference energy costs already receive less attention than the energy costs of\ntraining LLMs -- despite how often these large models are called on to conduct\ninference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see\nincreasing usage and deployment in various domains, a better understanding of\ntheir resource utilization is crucial for cost-savings, scaling performance,\nefficient hardware usage, and optimal inference strategies.\n  In this paper, we describe experiments conducted to study the computational\nand energy utilization of inference with LLMs. We benchmark and conduct a\npreliminary analysis of the inference performance and inference energy costs of\ndifferent sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta\nAI on two generations of popular GPUs (NVIDIA V100 \\& A100) and two datasets\n(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in\nresearch and practice. We present the results of multi-node, multi-GPU\ninference using model sharding across up to 32 GPUs. To our knowledge, our work\nis the one of the first to study LLM inference performance from the perspective\nof computational and energy resources at this scale.",
        "pdf_link": "https://arxiv.org/pdf/2310.03003v1.pdf"
    },
    {
        "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",
        "authors": [
            "Chang Gao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Wai Lam"
        ],
        "published": "2023-10-04T16:44:23Z",
        "summary": "Instruction tuning has emerged as a crucial process for harnessing the\ncapabilities of large language models (LLMs) by providing explicit task\ninstructions, leading to improved performance in various tasks. However,\nprevalent text-to-text instruction tuning (TextTuning) methods suffer from\nlimitations in generalization, robustness, and controllability due to the\nambiguity and lack of explicit structure in tasks. In this paper, we propose\nJsonTuning, a novel structure-to-structure approach for instruction tuning. By\nleveraging the versatility and structured nature of JSON to represent tasks,\nJsonTuning enhances generalization by helping the model understand essential\ntask elements and their relations, improves robustness by minimizing ambiguity,\nand increases controllability by providing explicit control over the output. We\nconduct a comprehensive comparative study with diverse language models and\nevaluation benchmarks. Experimental results show that JsonTuning outperforms\nTextTuning in various applications, showcasing improved performance,\nadaptability, robustness, and controllability. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for more effective\nand reliable LLMs capable of handling diverse scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.02953v2.pdf"
    },
    {
        "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
        "authors": [
            "Xianjun Yang",
            "Xiao Wang",
            "Qi Zhang",
            "Linda Petzold",
            "William Yang Wang",
            "Xun Zhao",
            "Dahua Lin"
        ],
        "published": "2023-10-04T16:39:31Z",
        "summary": "Warning: This paper contains examples of harmful language, and reader\ndiscretion is recommended. The increasing open release of powerful large\nlanguage models (LLMs) has facilitated the development of downstream\napplications by reducing the essential cost of data annotation and computation.\nTo ensure AI safety, extensive safety-alignment measures have been conducted to\narmor these models against malicious use (primarily hard prompt attack).\nHowever, beneath the seemingly resilient facade of the armor, there might lurk\na shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these\nsafely aligned LLMs can be easily subverted to generate harmful content.\nFormally, we term a new attack as Shadow Alignment: utilizing a tiny amount of\ndata can elicit safely-aligned models to adapt to harmful tasks without\nsacrificing model helpfulness. Remarkably, the subverted models retain their\ncapability to respond appropriately to regular inquiries. Experiments across 8\nmodels released by 5 different organizations (LLaMa-2, Falcon, InternLM,\nBaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.\nBesides, the single-turn English-only attack successfully transfers to\nmulti-turn dialogue and other languages. This study serves as a clarion call\nfor a collective effort to overhaul and fortify the safety of open-source LLMs\nagainst malicious attackers.",
        "pdf_link": "https://arxiv.org/pdf/2310.02949v1.pdf"
    },
    {
        "title": "Assessing Large Language Models on Climate Information",
        "authors": [
            "Jannis Bulian",
            "Mike S. Sch√§fer",
            "Afra Amini",
            "Heidi Lam",
            "Massimiliano Ciaramita",
            "Ben Gaiarin",
            "Michelle Chen Huebscher",
            "Christian Buck",
            "Niels Mede",
            "Markus Leippold",
            "Nadine Strauss"
        ],
        "published": "2023-10-04T16:09:48Z",
        "summary": "Understanding how climate change affects us and learning about available\nsolutions are key steps toward empowering individuals and communities to\nmitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,\nit is necessary to assess their capability in this domain. In this study, we\npresent a comprehensive evaluation framework, grounded in science communication\nprinciples, to analyze LLM responses to climate change topics. Our framework\nemphasizes both the presentational and epistemological adequacy of answers,\noffering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our\nframework discerns up to 30 distinct issues in model outputs. The task is a\nreal-world example of a growing number of challenging problems where AI can\ncomplement and lift human performance. We introduce a novel and practical\nprotocol for scalable oversight that uses AI Assistance and relies on raters\nwith relevant educational backgrounds. We evaluate several recent LLMs and\nconduct a comprehensive analysis of the results, shedding light on both the\npotential and the limitations of LLMs in the realm of climate communication.",
        "pdf_link": "https://arxiv.org/pdf/2310.02932v1.pdf"
    },
    {
        "title": "Large language models in textual analysis for gesture selection",
        "authors": [
            "Laura B. Hensel",
            "Nutchanon Yongsatianchot",
            "Parisa Torshizi",
            "Elena Minucci",
            "Stacy Marsella"
        ],
        "published": "2023-10-04T14:46:37Z",
        "summary": "Gestures perform a variety of communicative functions that powerfully\ninfluence human face-to-face interaction. How this communicative function is\nachieved varies greatly between individuals and depends on the role of the\nspeaker and the context of the interaction. Approaches to automatic gesture\ngeneration vary not only in the degree to which they rely on data-driven\ntechniques but also the degree to which they can produce context and speaker\nspecific gestures. However, these approaches face two major challenges: The\nfirst is obtaining sufficient training data that is appropriate for the context\nand the goal of the application. The second is related to designer control to\nrealize their specific intent for the application. Here, we approach these\nchallenges by using large language models (LLMs) to show that these powerful\nmodels of large amounts of data can be adapted for gesture analysis and\ngeneration. Specifically, we used ChatGPT as a tool for suggesting\ncontext-specific gestures that can realize designer intent based on minimal\nprompts. We also find that ChatGPT can suggests novel yet appropriate gestures\nnot present in the minimal training data. The use of LLMs is a promising avenue\nfor gesture generation that reduce the need for laborious annotations and has\nthe potential to flexibly and quickly adapt to different designer intents.",
        "pdf_link": "https://arxiv.org/pdf/2310.13705v1.pdf"
    },
    {
        "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
        "authors": [
            "Pei Zhou",
            "Aman Madaan",
            "Srividya Pranavi Potharaju",
            "Aditya Gupta",
            "Kevin R. McKee",
            "Ari Holtzman",
            "Jay Pujara",
            "Xiang Ren",
            "Swaroop Mishra",
            "Aida Nematzadeh",
            "Shyam Upadhyay",
            "Manaal Faruqui"
        ],
        "published": "2023-10-04T06:47:58Z",
        "summary": "\"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.03051v1.pdf"
    },
    {
        "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
        "authors": [
            "Soroush Abbasi Koohpayegani",
            "KL Navaneet",
            "Parsa Nooralinejad",
            "Soheil Kolouri",
            "Hamed Pirsiavash"
        ],
        "published": "2023-10-04T03:30:24Z",
        "summary": "Large Language Models (LLMs) have recently gained popularity due to their\nimpressive few-shot performance across various downstream tasks. However,\nfine-tuning all parameters and storing a unique model for each downstream task\nor domain becomes impractical because of the massive size of checkpoints (e.g.,\n350GB in GPT-3). Current literature, such as LoRA, showcases the potential of\nlow-rank modifications to the original weights of an LLM, enabling efficient\nadaptation and storage for task-specific models. These methods can reduce the\nnumber of parameters needed to fine-tune an LLM by several orders of magnitude.\nYet, these methods face two primary limitations: 1) the parameter reduction is\nlower-bounded by the rank one decomposition, and 2) the extent of reduction is\nheavily influenced by both the model architecture and the chosen rank. For\ninstance, in larger models, even a rank one decomposition might exceed the\nnumber of parameters truly needed for adaptation. In this paper, we introduce\nNOLA, which overcomes the rank one lower bound present in LoRA. It achieves\nthis by re-parameterizing the low-rank matrices in LoRA using linear\ncombinations of randomly generated matrices (basis) and optimizing the linear\nmixture coefficients only. This approach allows us to decouple the number of\ntrainable parameters from both the choice of rank and the network architecture.\nWe present adaptation results using GPT-2 and ViT in natural language and\ncomputer vision tasks. NOLA performs as well as, or better than models with\nequivalent parameter counts. Furthermore, we demonstrate that we can halve the\nparameters in larger models compared to LoRA with rank one, without sacrificing\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2310.02556v1.pdf"
    },
    {
        "title": "Low-Resource Languages Jailbreak GPT-4",
        "authors": [
            "Zheng-Xin Yong",
            "Cristina Menghini",
            "Stephen H. Bach"
        ],
        "published": "2023-10-03T21:30:56Z",
        "summary": "AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.",
        "pdf_link": "https://arxiv.org/pdf/2310.02446v2.pdf"
    },
    {
        "title": "Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions",
        "authors": [
            "Naiming Liu",
            "Shashank Sonkar",
            "Zichao Wang",
            "Simon Woodhead",
            "Richard G. Baraniuk"
        ],
        "published": "2023-10-03T21:19:50Z",
        "summary": "We propose novel evaluations for mathematical reasoning capabilities of Large\nLanguage Models (LLMs) based on mathematical misconceptions. Our primary\napproach is to simulate LLMs as a novice learner and an expert tutor, aiming to\nidentify the incorrect answer to math question resulted from a specific\nmisconception and to recognize the misconception(s) behind an incorrect answer,\nrespectively. Contrary to traditional LLMs-based mathematical evaluations that\nfocus on answering math questions correctly, our approach takes inspirations\nfrom principles in educational learning sciences. We explicitly ask LLMs to\nmimic a novice learner by answering questions in a specific incorrect manner\nbased on incomplete knowledge; and to mimic an expert tutor by identifying\nmisconception(s) corresponding to an incorrect answer to a question. Using\nsimple grade-school math problems, our experiments reveal that, while LLMs can\neasily answer these questions correctly, they struggle to identify 1) the\nincorrect answer corresponding to specific incomplete knowledge\n(misconceptions); 2) the misconceptions that explain particular incorrect\nanswers. Our study indicates new opportunities for enhancing LLMs' math\nreasoning capabilities, especially on developing robust student simulation and\nexpert tutoring models in the educational applications such as intelligent\ntutoring systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.02439v1.pdf"
    },
    {
        "title": "Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions",
        "authors": [
            "Yufan Chen",
            "Arjun Arunasalam",
            "Z. Berkay Celik"
        ],
        "published": "2023-10-03T20:54:29Z",
        "summary": "Users seek security & privacy (S&P) advice from online resources, including\ntrusted websites and content-sharing platforms. These resources help users\nunderstand S&P technologies and tools and suggest actionable strategies. Large\nLanguage Models (LLMs) have recently emerged as trusted information sources.\nHowever, their accuracy and correctness have been called into question. Prior\nresearch has outlined the shortcomings of LLMs in answering multiple-choice\nquestions and user ability to inadvertently circumvent model restrictions\n(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable\nS&P advice is not well-explored. In this paper, we measure their ability to\nrefute popular S&P misconceptions that the general public holds. We first study\nrecent academic literature to curate a dataset of over a hundred S&P-related\nmisconceptions across six different topics. We then query two popular LLMs\n(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to\nthese misconceptions. To comprehensively evaluate their responses, we further\napply three strategies: query each misconception multiple times, generate and\nquery their paraphrases, and solicit source URLs of the responses. Both models\ndemonstrate, on average, a 21.3% non-negligible error rate, incorrectly\nsupporting popular S&P misconceptions. The error rate increases to 32.6% when\nwe repeatedly query LLMs with the same or paraphrased misconceptions. We also\nexpose that models may partially support a misconception or remain\nnoncommittal, refusing a firm stance on misconceptions. Our exploration of\ninformation sources for responses revealed that LLMs are susceptible to\nproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to\nunrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).",
        "pdf_link": "https://arxiv.org/pdf/2310.02431v1.pdf"
    },
    {
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
        "authors": [
            "Xiaogeng Liu",
            "Nan Xu",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2023-10-03T19:44:37Z",
        "summary": "The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.04451v2.pdf"
    },
    {
        "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework",
        "authors": [
            "Mahyar Abbasian",
            "Iman Azimi",
            "Amir M. Rahmani",
            "Ramesh Jain"
        ],
        "published": "2023-10-03T18:54:10Z",
        "summary": "Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via three demonstrations. Moreover, we release openCHA as open\nsource available to the community via GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2310.02374v4.pdf"
    },
    {
        "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation",
        "authors": [
            "Benjamin Steenhoek",
            "Michele Tufano",
            "Neel Sundaresan",
            "Alexey Svyatkovskiy"
        ],
        "published": "2023-10-03T18:48:31Z",
        "summary": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at this link: https://figshare.com/s/ded476c8d4c221222849.",
        "pdf_link": "https://arxiv.org/pdf/2310.02368v1.pdf"
    },
    {
        "title": "Investigating Large Language Models' Perception of Emotion Using Appraisal Theory",
        "authors": [
            "Nutchanon Yongsatianchot",
            "Parisa Ghanad Torshizi",
            "Stacy Marsella"
        ],
        "published": "2023-10-03T16:34:47Z",
        "summary": "Large Language Models (LLM) like ChatGPT have significantly advanced in\nrecent years and are now being used by the general public. As more people\ninteract with these systems, improving our understanding of these black box\nmodels is crucial, especially regarding their understanding of human\npsychological aspects. In this work, we investigate their emotion perception\nthrough the lens of appraisal and coping theory using the Stress and Coping\nProcess Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting\nof multiple stories that evolve over time and differ in key appraisal variables\nsuch as controllability and changeability. We applied SCPQ to three recent LLMs\nfrom OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with\npredictions from the appraisal theory and human data. The results show that\nLLMs' responses are similar to humans in terms of dynamics of appraisal and\ncoping, but their responses did not differ along key appraisal dimensions as\npredicted by the theory and data. The magnitude of their responses is also\nquite different from humans in several variables. We also found that GPTs can\nbe quite sensitive to instruction and how questions are asked. This work adds\nto the growing literature evaluating the psychological aspects of LLMs and\nhelps enrich our understanding of the current models.",
        "pdf_link": "https://arxiv.org/pdf/2310.04450v1.pdf"
    },
    {
        "title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization",
        "authors": [
            "Zijun Liu",
            "Yanzhe Zhang",
            "Peng Li",
            "Yang Liu",
            "Diyi Yang"
        ],
        "published": "2023-10-03T16:05:48Z",
        "summary": "Large language model (LLM) agents have been shown effective on a wide range\nof tasks, and by ensembling multiple LLM agents, their performances could be\nfurther improved. Existing approaches employ a fixed set of agents to interact\nwith each other in a static architecture, which limits their generalizability\nto various tasks and requires strong human prior in designing these agents. In\nthis work, we propose to construct a strategic team of agents communicating in\na dynamic interaction architecture based on the task query. Specifically, we\nbuild a framework named Dynamic LLM-Agent Network ($\\textbf{DyLAN}$) for\nLLM-agent collaboration on complicated tasks like reasoning and code\ngeneration. DyLAN enables agents to interact for multiple rounds in a dynamic\narchitecture with inference-time agent selection and an early-stopping\nmechanism to improve performance and efficiency. We further design an automatic\nagent team optimization algorithm based on an unsupervised metric termed\n$\\textit{Agent Importance Score}$, enabling the selection of best agents based\non the contribution each agent makes. Empirically, we demonstrate that DyLAN\nperforms well in both reasoning and code generation tasks with reasonable\ncomputational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and\nHumanEval, respectively, compared to a single execution on GPT-35-turbo. On\nspecific subjects of MMLU, agent team optimization in DyLAN increases accuracy\nby up to 25.0%.",
        "pdf_link": "https://arxiv.org/pdf/2310.02170v1.pdf"
    },
    {
        "title": "Editing Personality for Large Language Models",
        "authors": [
            "Shengyu Mao",
            "Xiaohan Wang",
            "Mengru Wang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Ningyu Zhang"
        ],
        "published": "2023-10-03T16:02:36Z",
        "summary": "This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct a new benchmark dataset PersonalityEdit to address this task. Drawing\non the theory in Social Psychology, we isolate three representative traits,\nnamely Neuroticism, Extraversion, and Agreeableness, as the foundation for our\nbenchmark. We then gather data using GPT-4, generating responses that not only\nalign with a specified topic but also embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our intriguing findings uncover\npotential challenges of the proposed task, illustrating several remaining\nissues. We anticipate that our work can provide the NLP community with\ninsights. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.",
        "pdf_link": "https://arxiv.org/pdf/2310.02168v3.pdf"
    },
    {
        "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
        "authors": [
            "Zhoubo Li",
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Mengru Wang",
            "Xi Chen",
            "Huajun Chen"
        ],
        "published": "2023-10-03T15:10:46Z",
        "summary": "As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.",
        "pdf_link": "https://arxiv.org/pdf/2310.02129v4.pdf"
    },
    {
        "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
        "authors": [
            "Zhen Bi",
            "Ningyu Zhang",
            "Yida Xue",
            "Yixin Ou",
            "Daxiong Ji",
            "Guozhou Zheng",
            "Huajun Chen"
        ],
        "published": "2023-10-03T13:17:35Z",
        "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reason may be the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\na novel framework to automatically obtain a large volume of ocean domain\ninstruction data, which generates instructions based on multi-agent\ncollaboration. Additionally, we construct the first oceanography benchmark,\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\nexpertise for oceans science tasks but also gains preliminary embodied\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\nsoon be available at https://github.com/zjunlp/KnowLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.02031v6.pdf"
    },
    {
        "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
        "authors": [
            "Aniruddha Deb",
            "Neeva Oza",
            "Sarthak Singla",
            "Dinesh Khandelwal",
            "Dinesh Garg",
            "Parag Singla"
        ],
        "published": "2023-10-03T12:03:06Z",
        "summary": "While forward reasoning (i.e. find the answer given the question) has been\nexplored extensively in the recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation?\n  In this paper, we formally define the backward reasoning task on math word\nproblems and modify three datasets to evaluate this task: GSM8k, SVAMP and\nMultiArith. Our findings show a significant drop in the accuracy of models on\nbackward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,\nGPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we\npropose three novel techniques that improve performance: Rephrase reformulates\nthe given problem into a forward reasoning problem, PAL-Tools combines the idea\nof Program-Aided LLMs to produce a set of equations that can be solved by an\nexternal solver, and Check your Work exploits the availability of natural\nverifier of high accuracy in the forward direction, interleaving solving and\nverification steps. Finally, realizing that each of our base methods correctly\nsolves a different set of problems, we propose a novel Bayesian formulation for\ncreating an ensemble over these base methods aided by a verifier to further\nboost the accuracy by a significant margin. Extensive experimentation\ndemonstrates that our techniques successively improve the performance of LLMs\non the backward reasoning task, with the final ensemble-based method resulting\nin a substantial performance gain compared to the raw LLMs with standard\nprompting techniques such as chain-of-thought.",
        "pdf_link": "https://arxiv.org/pdf/2310.01991v1.pdf"
    },
    {
        "title": "Formalizing Natural Language Intent into Program Specifications via Large Language Models",
        "authors": [
            "Madeline Endres",
            "Sarah Fakhoury",
            "Saikat Chakraborty",
            "Shuvendu K. Lahiri"
        ],
        "published": "2023-10-03T06:55:45Z",
        "summary": "Informal natural language that describes code functionality, such as code\ncomments or function documentation, may contain substantial information about a\nprograms intent. However, there is typically no guarantee that a programs\nimplementation and natural language documentation are aligned. In the case of a\nconflict, leveraging information in code-adjacent natural language has the\npotential to enhance fault localization, debugging, and code trustworthiness.\nIn practice, however, this information is often underutilized due to the\ninherent ambiguity of natural language which makes natural language intent\nchallenging to check programmatically. The \"emergent abilities\" of Large\nLanguage Models (LLMs) have the potential to facilitate the translation of\nnatural language intent to programmatically checkable assertions. However, it\nis unclear if LLMs can correctly translate informal natural language\nspecifications into formal specifications that match programmer intent.\nAdditionally, it is unclear if such translation could be useful in practice. In\nthis paper, we describe LLM4nl2post, the problem leveraging LLMs for\ntransforming informal natural language to formal method postconditions,\nexpressed as program assertions. We introduce and validate metrics to measure\nand compare different LLM4nl2post approaches, using the correctness and\ndiscriminative power of generated postconditions. We then perform qualitative\nand quantitative methods to assess the quality of LLM4nl2post postconditions,\nfinding that they are generally correct and able to discriminate incorrect\ncode. Finally, we find that LLM4nl2post via LLMs has the potential to be\nhelpful in practice; specifications generated from natural language were able\nto catch 70 real-world historical bugs from Defects4J.",
        "pdf_link": "https://arxiv.org/pdf/2310.01831v1.pdf"
    },
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "authors": [
            "Suyu Ge",
            "Yunan Zhang",
            "Liyuan Liu",
            "Minjia Zhang",
            "Jiawei Han",
            "Jianfeng Gao"
        ],
        "published": "2023-10-03T05:17:08Z",
        "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01801v3.pdf"
    },
    {
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
        "authors": [
            "Jie Huang",
            "Xinyun Chen",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng",
            "Adams Wei Yu",
            "Xinying Song",
            "Denny Zhou"
        ],
        "published": "2023-10-03T04:56:12Z",
        "summary": "Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance even\ndegrades after self-correction. Drawing from these insights, we offer\nsuggestions for future research and practical applications in this field.",
        "pdf_link": "https://arxiv.org/pdf/2310.01798v2.pdf"
    },
    {
        "title": "HallE-Control: Controlling Object Hallucination in Large Multimodal Models",
        "authors": [
            "Bohan Zhai",
            "Shijia Yang",
            "Chenfeng Xu",
            "Sheng Shen",
            "Kurt Keutzer",
            "Chunyuan Li",
            "Manling Li"
        ],
        "published": "2023-10-03T04:01:27Z",
        "summary": "Current Large Multimodal Models (LMMs) achieve remarkable progress, yet there\nremains significant uncertainty regarding their ability to accurately apprehend\nvisual details, that is, in performing detailed captioning. To address this, we\nintroduce $\\textit{CCEval}$, a GPT-4 assisted evaluation method for detailed\ncaptioning. Interestingly, while LMMs demonstrate minimal object existence\nhallucination in existing VQA benchmarks, our proposed evaluation reveals\ncontinued susceptibility to such hallucinations. In this paper, we make the\nfirst attempt to investigate such hallucination from different aspects,\nincluding image resolution, the language decoder size, and instruction data\namount, quality, granularity. Our findings underscore the unwarranted inference\nwhen the language description includes details at a finer object granularity\nthan what the vision module can ground or verify, thus inducing hallucination.\nTo control such hallucinations, we further attribute the reliability of\ncaptioning to contextual knowledge (involving only contextually grounded\nobjects) and parametric knowledge (containing inferred objects by the model).\nThus, we introduce $\\textit{HallE-Control}$, a controllable LMM in terms of\n$\\textbf{Hall}$ucination in object $\\textbf{E}$xistence. HallE-Control can\ncondition the captioning to shift between (i) exclusively depicting contextual\nknowledge for grounded objects and (ii) blending it with parametric knowledge\nto imagine inferred objects. Our method reduces hallucination by 44% compared\nto LLaVA$_{7B}$ and maintains the object coverage.",
        "pdf_link": "https://arxiv.org/pdf/2310.01779v3.pdf"
    },
    {
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
        "authors": [
            "Ming Jin",
            "Shiyu Wang",
            "Lintao Ma",
            "Zhixuan Chu",
            "James Y. Zhang",
            "Xiaoming Shi",
            "Pin-Yu Chen",
            "Yuxuan Liang",
            "Yuan-Fang Li",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "published": "2023-10-03T01:31:25Z",
        "summary": "Time series forecasting holds significant importance in many real-world\ndynamic systems and has been extensively studied. Unlike natural language\nprocess (NLP) and computer vision (CV), where a single large model can tackle\nmultiple tasks, models for time series forecasting are often specialized,\nnecessitating distinct designs for different tasks and applications. While\npre-trained foundation models have made impressive strides in NLP and CV, their\ndevelopment in time series domains has been constrained by data sparsity.\nRecent studies have revealed that large language models (LLMs) possess robust\npattern recognition and reasoning abilities over complex sequences of tokens.\nHowever, the challenge remains in effectively aligning the modalities of time\nseries data and natural language to leverage these capabilities. In this work,\nwe present Time-LLM, a reprogramming framework to repurpose LLMs for general\ntime series forecasting with the backbone language models kept intact. We begin\nby reprogramming the input time series with text prototypes before feeding it\ninto the frozen LLM to align the two modalities. To augment the LLM's ability\nto reason with time series data, we propose Prompt-as-Prefix (PaP), which\nenriches the input context and directs the transformation of reprogrammed input\npatches. The transformed time series patches from the LLM are finally projected\nto obtain the forecasts. Our comprehensive evaluations demonstrate that\nTime-LLM is a powerful time series learner that outperforms state-of-the-art,\nspecialized forecasting models. Moreover, Time-LLM excels in both few-shot and\nzero-shot learning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.01728v2.pdf"
    },
    {
        "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
        "authors": [
            "Jenny T. Liang",
            "Carmen Badea",
            "Christian Bird",
            "Robert DeLine",
            "Denae Ford",
            "Nicole Forsgren",
            "Thomas Zimmermann"
        ],
        "published": "2023-10-03T01:27:23Z",
        "summary": "Empirical software engineering research on production systems has brought\nforth a better understanding of the software engineering process for\npractitioners and researchers alike. However, only a small subset of production\nsystems is studied, limiting the impact of this research. While software\nengineering practitioners benefit from replicating research on their own data,\nthis poses its own set of challenges, since performing replications requires a\ndeep understanding of research methodologies and subtle nuances in software\nengineering data. Given that large language models (LLMs), such as GPT-4, show\npromise in tackling both software engineering- and science-related tasks, these\nmodels could help democratize empirical software engineering research.\n  In this paper, we examine LLMs' abilities to perform replications of\nempirical software engineering research on new data. We specifically study\ntheir ability to surface assumptions made in empirical software engineering\nresearch methodologies, as well as their ability to plan and generate code for\nanalysis pipelines on seven empirical software engineering papers. We perform a\nuser study with 14 participants with software engineering research expertise,\nwho evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of\nmodule specifications) from the papers. We find that GPT-4 is able to surface\ncorrect assumptions, but struggle to generate ones that reflect common\nknowledge about software engineering data. In a manual analysis of the\ngenerated code, we find that the GPT-4-generated code contains the correct\nhigh-level logic, given a subset of the methodology. However, the code contains\nmany small implementation-level errors, reflecting a lack of software\nengineering knowledge. Our findings have implications for leveraging LLMs for\nsoftware engineering research as well as practitioner data scientists in\nsoftware teams.",
        "pdf_link": "https://arxiv.org/pdf/2310.01727v1.pdf"
    },
    {
        "title": "Large Language Models for Test-Free Fault Localization",
        "authors": [
            "Aidan Z. H. Yang",
            "Ruben Martins",
            "Claire Le Goues",
            "Vincent J. Hellendoorn"
        ],
        "published": "2023-10-03T01:26:39Z",
        "summary": "Fault Localization (FL) aims to automatically localize buggy lines of code, a\nkey first step in many manual and automatic debugging tasks. Previous FL\ntechniques assume the provision of input tests, and often require extensive\nprogram analysis, program instrumentation, or data preprocessing. Prior work on\ndeep learning for APR struggles to learn from small datasets and produces\nlimited results on real-world programs. Inspired by the ability of large\nlanguage models (LLMs) of code to adapt to new tasks based on very few\nexamples, we investigate the applicability of LLMs to line level fault\nlocalization. Specifically, we propose to overcome the left-to-right nature of\nLLMs by fine-tuning a small set of bidirectional adapter layers on top of the\nrepresentations learned by LLMs to produce LLMAO, the first language model\nbased fault localization approach that locates buggy lines of code without any\ntest coverage information. We fine-tune LLMs with 350 million, 6 billion, and\n16 billion parameters on small, manually curated corpora of buggy programs such\nas the Defects4J corpus. We observe that our technique achieves substantially\nmore confidence in fault localization when built on the larger models, with bug\nlocalization performance scaling consistently with the LLM size. Our empirical\nevaluation shows that LLMAO improves the Top-1 results over the\nstate-of-the-art machine learning fault localization (MLFL) baselines by\n2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL\ntechnique trained using a language model architecture that can detect security\nvulnerabilities down to the code line level.",
        "pdf_link": "https://arxiv.org/pdf/2310.01726v1.pdf"
    },
    {
        "title": "Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making",
        "authors": [
            "D. Umerenkov",
            "G. Zubkova",
            "A. Nesterov"
        ],
        "published": "2023-10-03T00:08:23Z",
        "summary": "Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and\npatient data to offer real-time recommendations, with Large Language Models\n(LLMs) emerging as a promising tool to generate plain-text explanations for\nmedical decisions. This study explores the effectiveness and reliability of\nLLMs in generating explanations for diagnoses based on patient complaints.\nThree experienced doctors evaluated LLM-generated explanations of the\nconnection between patient complaints and doctor and model-assigned diagnoses\nacross several stages. Experimental results demonstrated that LLM explanations\nsignificantly increased doctors' agreement rates with given diagnoses and\nhighlighted potential errors in LLM outputs, ranging from 5% to 30%. The study\nunderscores the potential and challenges of LLMs in healthcare and emphasizes\nthe need for careful integration and evaluation to ensure patient safety and\noptimal clinical utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01708v1.pdf"
    },
    {
        "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
        "authors": [
            "Praneeth Kacham",
            "Vahab Mirrokni",
            "Peilin Zhong"
        ],
        "published": "2023-10-02T21:39:04Z",
        "summary": "The quadratic time and memory complexity inherent to self-attention\nmechanisms, with respect to sequence length, presents a critical computational\nbottleneck in the training and deployment of large-scale Transformer-based\nlanguage models. Recent theoretical results indicate the intractability of\nsub-quadratic softmax attention approximation under reasonable complexity\nassumptions. This paper addresses this challenge by first demonstrating that\npolynomial attention with high degree can effectively replace softmax without\nsacrificing model quality. Next, we develop polynomial sketching techniques\nfrom numerical linear algebra to achieve linear-time polynomial attention with\napproximation guarantees. Crucially, our approach achieves this speedup without\nrequiring the sparsification of attention matrices. We also present a\nblock-based algorithm to apply causal masking efficiently. Combining these\ntechniques, we provide \\emph{PolySketchFormer}, a practical linear-time\nTransformer architecture for language modeling that offers provable guarantees.\n  We validate PolySketchFormer empirically by training language models capable\nof handling long contexts. These experiments utilize both synthetic and\nreal-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context\nlengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in\ntraining compared to FlashAttention, with no observed degradation in quality\nacross our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2310.01655v3.pdf"
    },
    {
        "title": "VAL: Interactive Task Learning with GPT Dialog Parsing",
        "authors": [
            "Lane Lawley",
            "Christopher J. MacLellan"
        ],
        "published": "2023-10-02T20:45:41Z",
        "summary": "Reinforcement learning often requires millions of examples to produce static,\nblack-box models. In contrast, interactive task learning (ITL) emphasizes\nincremental knowledge acquisition from limited instruction provided by humans\nin modalities such as natural language. However, in practice, ITL systems often\nsuffers from brittle, error-prone language parsing. Large language models\n(LLMs) are resistant to brittleness but are not interpretable and cannot learn\nincrementally. We present VAL, an ITL system with a new philosophy for\nLLM/symbolic integration. By using LLMs only for specific tasks -- such as\npredicate and argument selection -- within an algorithmic framework, VAL reaps\nthe benefits of LLMs to support interactive learning of hierarchical task\nknowledge from natural language. Acquired knowledge is human interpretable and\ngeneralizes to support execution of novel tasks without additional training. We\nstudied users' interactions with VAL in a video game setting, finding that most\nusers could successfully teach VAL using language they felt was natural.",
        "pdf_link": "https://arxiv.org/pdf/2310.01627v1.pdf"
    },
    {
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?",
        "authors": [
            "Hangfan Zhang",
            "Zhimeng Guo",
            "Huaisheng Zhu",
            "Bochuan Cao",
            "Lu Lin",
            "Jinyuan Jia",
            "Jinghui Chen",
            "Dinghao Wu"
        ],
        "published": "2023-10-02T19:22:01Z",
        "summary": "Large Language Models (LLMs) have achieved unprecedented performance in\nNatural Language Generation (NLG) tasks. However, many existing studies have\nshown that they could be misused to generate undesired content. In response,\nbefore releasing LLMs for public access, model developers usually align those\nlanguage models through Supervised Fine-Tuning (SFT) or Reinforcement Learning\nwith Human Feedback (RLHF). Consequently, those aligned large language models\nrefuse to generate undesired content when facing potentially harmful/unethical\nrequests. A natural question is \"could alignment really prevent those\nopen-sourced large language models from being misused to generate undesired\ncontent?''. In this work, we provide a negative answer to this question. In\nparticular, we show those open-sourced, aligned large language models could be\neasily misguided to generate undesired content without heavy computations or\ncareful prompt designs. Our key idea is to directly manipulate the generation\nprocess of open-sourced LLMs to misguide it to generate undesired content\nincluding harmful or biased information and even private data. We evaluate our\nmethod on 4 open-sourced LLMs accessible publicly and our finding highlights\nthe need for more advanced mitigation strategies for open-sourced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.01581v1.pdf"
    },
    {
        "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples",
        "authors": [
            "Jia-Yu Yao",
            "Kun-Peng Ning",
            "Zhen-Hui Liu",
            "Mu-Nan Ning",
            "Li Yuan"
        ],
        "published": "2023-10-02T17:01:56Z",
        "summary": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still can not\ncompletely trust their answer, since LLMs suffer from\nhallucination--fabricating non-existent facts to cheat users without\nperception. And the reasons for their existence and pervasiveness remain\nunclear. In this paper, we demonstrate that non-sense prompts composed of\nrandom tokens can also elicit the LLMs to respond with hallucinations. This\nphenomenon forces us to revisit that hallucination may be another view of\nadversarial examples, and it shares similar features with conventional\nadversarial examples as the basic feature of LLMs. Therefore, we formalize an\nautomatic hallucination triggering method as the hallucination attack in an\nadversarial way. Finally, we explore basic feature of attacked adversarial\nprompts and propose a simple yet effective defense strategy. Our code is\nreleased on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2310.01469v2.pdf"
    },
    {
        "title": "Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association",
        "authors": [
            "Qiyu Wu",
            "Mengjie Zhao",
            "Yutong He",
            "Lang Huang",
            "Junya Ono",
            "Hiromi Wakaki",
            "Yuki Mitsufuji"
        ],
        "published": "2023-10-02T16:48:50Z",
        "summary": "Reporting bias arises when people assume that some knowledge is universally\nunderstood and hence, do not necessitate explicit elaboration. In this paper,\nwe focus on the wide existence of reporting bias in visual-language datasets,\nembodied as the object-attribute association, which can subsequentially degrade\nmodels trained on them. To mitigate this bias, we propose a bimodal\naugmentation (BiAug) approach through object-attribute decoupling to flexibly\nsynthesize visual-language examples with a rich array of object-attribute\npairing and construct cross-modal hard negatives. We employ large language\nmodels (LLMs) in conjunction with a grounding object detector to extract target\nobjects. Subsequently, the LLM generates a detailed attribute description for\neach object and produces a corresponding hard negative counterpart. An\ninpainting model is then used to create images based on these detailed object\ndescriptions. By doing so, the synthesized examples explicitly complement\nomitted objects and attributes to learn, and the hard negative pairs steer the\nmodel to distinguish object attributes. Our experiments demonstrated that BiAug\nis superior in object-attribute understanding. In addition, BiAug also improves\nthe performance on zero-shot retrieval tasks on general benchmarks like MSCOCO\nand Flickr30K. BiAug refines the way of collecting text-image datasets.\nMitigating the reporting bias helps models achieve a deeper understanding of\nvisual-language phenomena, expanding beyond mere frequent patterns to encompass\nthe richness and diversity of real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.01330v1.pdf"
    },
    {
        "title": "Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation",
        "authors": [
            "Shenzhi Wang",
            "Chang Liu",
            "Zilong Zheng",
            "Siyuan Qi",
            "Shuo Chen",
            "Qisen Yang",
            "Andrew Zhao",
            "Chaofei Wang",
            "Shiji Song",
            "Gao Huang"
        ],
        "published": "2023-10-02T16:27:36Z",
        "summary": "Recent breakthroughs in large language models (LLMs) have brought remarkable\nsuccess in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is\nthat the information processed by LLMs is consistently honest, neglecting the\npervasive deceptive or misleading information in human society and AI-generated\ncontent. This oversight makes LLMs susceptible to malicious manipulations,\npotentially resulting in detrimental outcomes. This study utilizes the\nintricate Avalon game as a testbed to explore LLMs' potential in deceptive\nenvironments. Avalon, full of misinformation and requiring sophisticated logic,\nmanifests as a \"Game-of-Thoughts\". Inspired by the efficacy of humans'\nrecursive thinking and perspective-taking in the Avalon game, we introduce a\nnovel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to\nidentify and counteract deceptive information. ReCon combines formulation and\nrefinement contemplation processes; formulation contemplation produces initial\nthoughts and speech, while refinement contemplation further polishes them.\nAdditionally, we incorporate first-order and second-order perspective\ntransitions into these processes respectively. Specifically, the first-order\nallows an LLM agent to infer others' mental states, and the second-order\ninvolves understanding how others perceive the agent's mental state. After\nintegrating ReCon with different LLMs, extensive experiment results from the\nAvalon game indicate its efficacy in aiding LLMs to discern and maneuver around\ndeceptive information without extra fine-tuning and data. Finally, we offer a\npossible explanation for the efficacy of ReCon and explore the current\nlimitations of LLMs in terms of safety, reasoning, speaking style, and format,\npotentially furnishing insights for subsequent research.",
        "pdf_link": "https://arxiv.org/pdf/2310.01320v3.pdf"
    },
    {
        "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
        "authors": [
            "Wenxuan Ding",
            "Shangbin Feng",
            "Yuhan Liu",
            "Zhaoxuan Tan",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-10-02T15:43:53Z",
        "summary": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks\nand have achieved impressive performance thanks to their knowledge abilities.\nWhile LLMs have demonstrated outstanding performance on atomic or linear\n(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with\ninterweaving constraints remains an underexplored problem. In this work, we\npropose geometric reasoning over structured knowledge, where pieces of\nknowledge are connected in a graph structure and models need to fill in the\nmissing information. Such geometric knowledge reasoning would require the\nability to handle structured knowledge, reason with uncertainty, verify facts,\nand backtrack when an error occurs. We propose Knowledge Crosswords, a\nmulti-blank QA dataset where each problem consists of a natural language\nquestion representing the geometric constraints of an incomplete entity\nnetwork, where LLMs are tasked with working out the missing entities while\nmeeting all factual constraints. Knowledge Crosswords contains 2,101 individual\nproblems, covering various knowledge domains and further divided into three\ndifficulty levels. We conduct extensive experiments to evaluate existing LLM\nprompting approaches on the Knowledge Crosswords benchmark. We additionally\npropose two new approaches, Staged Prompting and Verify-All, to augment LLMs'\nability to backtrack and verify structured constraints. Our results demonstrate\nthat while baseline approaches perform well on easier problems but struggle\nwith hard ones, our proposed Verify-All outperforms other methods by a large\nmargin and is more robust with hard problems. Further analysis reveals that\nLLMs' ability of geometric reasoning over structured knowledge is still far\nfrom robust or perfect, susceptible to confounders such as the order of\noptions, certain structural patterns, assumption of existence of correct\nanswer, and more.",
        "pdf_link": "https://arxiv.org/pdf/2310.01290v1.pdf"
    },
    {
        "title": "SPELL: Semantic Prompt Evolution based on a LLM",
        "authors": [
            "Yujian Betterest Li",
            "Kai Wu"
        ],
        "published": "2023-10-02T14:51:16Z",
        "summary": "Prompt engineering is a new paradigm for enhancing the performance of trained\nneural network models. For optimizing text-style prompts, existing methods\nusually individually operate small portions of a text step by step, which\neither breaks the fluency or could not globally adjust a prompt. Since large\nlanguage models (LLMs) have powerful ability of generating coherent texts token\nby token, can we utilize LLMs for improving prompts? Based on this motivation,\nin this paper, considering a trained LLM as a text generator, we attempt to\ndesign a black-box evolution algorithm for automatically optimizing texts,\nnamely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is\nevaluated with different LLMs and evolution parameters in different text tasks.\nExperimental results show that SPELL could rapidly improve the prompts indeed.\nWe further explore the evolution process and discuss on the limitations,\npotential possibilities and future work.",
        "pdf_link": "https://arxiv.org/pdf/2310.01260v1.pdf"
    },
    {
        "title": "Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback",
        "authors": [
            "Jacob Whitehill",
            "Jennifer LoCasale-Crouch"
        ],
        "published": "2023-10-02T12:11:17Z",
        "summary": "With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of Instructional Support. Then, these\nutterance-level judgments are aggregated over a 15-min observation session to\nestimate a global CLASS score. Experiments on two CLASS-coded datasets of\ntoddler and pre-kindergarten classrooms indicate that (1) automatic CLASS\nInstructional Support estimation accuracy using the proposed method (Pearson\n$R$ up to $0.48$) approaches human inter-rater reliability (up to $R=0.55$);\n(2) LLMs generally yield slightly greater accuracy than BoW for this task,\nthough the best models often combined features extracted from both LLM and BoW;\nand (3) for classifying individual utterances, there is still room for\nimprovement of automated methods compared to human-level judgments. Finally,\n(4) we illustrate how the model's outputs can be visualized at the utterance\nlevel to provide teachers with explainable feedback on which utterances were\nmost positively or negatively correlated with specific CLASS dimensions.",
        "pdf_link": "https://arxiv.org/pdf/2310.01132v3.pdf"
    },
    {
        "title": "GraphText: Graph Reasoning in Text Space",
        "authors": [
            "Jianan Zhao",
            "Le Zhuo",
            "Yikang Shen",
            "Meng Qu",
            "Kai Liu",
            "Michael Bronstein",
            "Zhaocheng Zhu",
            "Jian Tang"
        ],
        "published": "2023-10-02T11:03:57Z",
        "summary": "Large Language Models (LLMs) have gained the ability to assimilate human\nknowledge and facilitate natural language interactions with both humans and\nother LLMs. However, despite their impressive achievements, LLMs have not made\nsignificant advancements in the realm of graph machine learning. This\nlimitation arises because graphs encapsulate distinct relational data, making\nit challenging to transform them into natural language that LLMs understand. In\nthis paper, we bridge this gap with a novel framework, GraphText, that\ntranslates graphs into natural language. GraphText derives a graph-syntax tree\nfor each graph that encapsulates both the node attributes and inter-node\nrelationships. Traversal of the tree yields a graph text sequence, which is\nthen processed by an LLM to treat graph tasks as text generation tasks.\nNotably, GraphText offers multiple advantages. It introduces training-free\ngraph reasoning: even without training on graph data, GraphText with ChatGPT\ncan achieve on par with, or even surpassing, the performance of\nsupervised-trained graph neural networks through in-context learning (ICL).\nFurthermore, GraphText paves the way for interactive graph reasoning, allowing\nboth humans and LLMs to communicate with the model seamlessly using natural\nlanguage. These capabilities underscore the vast, yet-to-be-explored potential\nof LLMs in the domain of graph machine learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.01089v1.pdf"
    },
    {
        "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
        "authors": [
            "Kentaro Mitsui",
            "Yukiya Hono",
            "Kei Sawada"
        ],
        "published": "2023-10-02T11:03:20Z",
        "summary": "The advent of large language models (LLMs) has made it possible to generate\nnatural written dialogues between two agents. However, generating human-like\nspoken dialogues from these written dialogues remains challenging. Spoken\ndialogues have several unique characteristics: they frequently include\nbackchannels and laughter, and the smoothness of turn-taking significantly\ninfluences the fluidity of conversation. This study proposes CHATS - CHatty\nAgents Text-to-Speech - a discrete token-based system designed to generate\nspoken dialogues based on written dialogues. Our system can generate speech for\nboth the speaker side and the listener side simultaneously, using only the\ntranscription from the speaker side, which eliminates the need for\ntranscriptions of backchannels or laughter. Moreover, CHATS facilitates natural\nturn-taking; it determines the appropriate duration of silence after each\nutterance in the absence of overlap, and it initiates the generation of\noverlapping speech based on the phoneme sequence of the next utterance in case\nof overlap. Experimental evaluations indicate that CHATS outperforms the\ntext-to-speech baseline, producing spoken dialogues that are more interactive\nand fluid while retaining clarity and intelligibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.01088v1.pdf"
    },
    {
        "title": "Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models",
        "authors": [
            "Chenhan Yuan",
            "Qianqian Xie",
            "Jimin Huang",
            "Sophia Ananiadou"
        ],
        "published": "2023-10-02T10:35:23Z",
        "summary": "Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.",
        "pdf_link": "https://arxiv.org/pdf/2310.01074v2.pdf"
    },
    {
        "title": "Resolving Knowledge Conflicts in Large Language Models",
        "authors": [
            "Yike Wang",
            "Shangbin Feng",
            "Heng Wang",
            "Weijia Shi",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-10-02T06:57:45Z",
        "summary": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question.",
        "pdf_link": "https://arxiv.org/pdf/2310.00935v1.pdf"
    },
    {
        "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
        "authors": [
            "Wenxuan Wang",
            "Zhaopeng Tu",
            "Chang Chen",
            "Youliang Yuan",
            "Jen-tse Huang",
            "Wenxiang Jiao",
            "Michael R. Lyu"
        ],
        "published": "2023-10-02T05:23:34Z",
        "summary": "Safety lies at the core of developing and deploying large language models\n(LLMs). However, previous safety benchmarks only concern the safety in one\nlanguage, e.g. the majority language in the pretraining data such as English.\nIn this work, we build the first multilingual safety benchmark for LLMs,\nXSafety, in response to the global deployment of LLMs in practice. XSafety\ncovers 14 kinds of commonly used safety issues across 10 languages that span\nseveral language families. We utilize XSafety to empirically study the\nmultilingual safety for 4 widely-used LLMs, including both close-API and\nopen-source models. Experimental results show that all LLMs produce\nsignificantly more unsafe responses for non-English queries than English ones,\nindicating the necessity of developing safety alignment for non-English\nlanguages. In addition, we propose several simple and effective prompting\nmethods to improve the multilingual safety of ChatGPT by evoking safety\nknowledge and improving cross-lingual generalization of safety alignment. Our\nprompting method can significantly reduce the ratio of unsafe responses from\n19.1% to 9.7% for non-English queries. We release our data at\nhttps://github.com/Jarviswang94/Multilingual_safety_benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.00905v1.pdf"
    },
    {
        "title": "DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models",
        "authors": [
            "Yongchan Kwon",
            "Eric Wu",
            "Kevin Wu",
            "James Zou"
        ],
        "published": "2023-10-02T04:59:19Z",
        "summary": "Quantifying the impact of training data points is crucial for understanding\nthe outputs of machine learning models and for improving the transparency of\nthe AI pipeline. The influence function is a principled and popular data\nattribution method, but its computational cost often makes it challenging to\nuse. This issue becomes more pronounced in the setting of large language models\nand text-to-image models. In this work, we propose DataInf, an efficient\ninfluence approximation method that is practical for large-scale generative AI\nmodels. Leveraging an easy-to-compute closed-form expression, DataInf\noutperforms existing influence computation algorithms in terms of computational\nand memory efficiency. Our theoretical analysis shows that DataInf is\nparticularly well-suited for parameter-efficient fine-tuning techniques such as\nLoRA. Through systematic empirical evaluations, we show that DataInf accurately\napproximates influence scores and is orders of magnitude faster than existing\nmethods. In applications to RoBERTa-large, Llama-2-13B-chat, and\nstable-diffusion-v1.5 models, DataInf effectively identifies the most\ninfluential fine-tuning examples better than other approximate influence\nscores. Moreover, it can help to identify which data points are mislabeled.",
        "pdf_link": "https://arxiv.org/pdf/2310.00902v3.pdf"
    },
    {
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement",
        "authors": [
            "Ziqi Wang",
            "Le Hou",
            "Tianjian Lu",
            "Yuexin Wu",
            "Yunxuan Li",
            "Hongkun Yu",
            "Heng Ji"
        ],
        "published": "2023-10-02T04:29:40Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nopen-ended text generation tasks. However, the inherent open-ended nature of\nthese tasks implies that there is always room for improvement in the quality of\nmodel responses. To address this challenge, various approaches have been\nproposed to enhance the performance of LLMs. There has been a growing focus on\nenabling LLMs to self-improve their response quality, thereby reducing the\nreliance on extensive human annotation efforts for collecting diverse and\nhigh-quality training data. Recently, prompting-based methods have been widely\nexplored among self-improvement methods owing to their effectiveness,\nefficiency, and convenience. However, those methods usually require explicitly\nand thoroughly written rubrics as inputs to LLMs. It is expensive and\nchallenging to manually derive and provide all necessary rubrics with a\nreal-world complex goal for improvement (e.g., being more helpful and less\nharmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework\nthat implicitly learns the improvement goal from human preference data. PIT\nonly requires preference data that are used to train reward models without\nextra human efforts. Specifically, we reformulate the training objective of\nreinforcement learning from human feedback (RLHF) -- instead of maximizing\nresponse quality for a given input, we maximize the quality gap of the response\nconditioned on a reference response. In this way, PIT is implicitly trained\nwith the improvement goal of better aligning with human preferences.\nExperiments on two real-world datasets and one synthetic dataset show that our\nmethod significantly outperforms prompting-based methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.00898v3.pdf"
    },
    {
        "title": "Do Compressed LLMs Forget Knowledge? An Experimental Study with Practical Implications",
        "authors": [
            "Duc N. M Hoang",
            "Minsik Cho",
            "Thomas Merth",
            "Mohammad Rastegari",
            "Zhangyang Wang"
        ],
        "published": "2023-10-02T03:12:06Z",
        "summary": "Compressing Large Language Models (LLMs) often leads to reduced performance,\nespecially for knowledge-intensive tasks. In this work, we dive into how\ncompression damages LLMs' inherent knowledge and the possible remedies. We\nstart by proposing two conjectures on the nature of the damage: one is certain\nknowledge being forgotten (or erased) after LLM compression, hence\nnecessitating the compressed model to (re)learn from data with additional\nparameters; the other presumes that knowledge is internally displaced and hence\none requires merely \"inference re-direction\" with input-side augmentation such\nas prompting, to recover the knowledge-related performance. Extensive\nexperiments are then designed to (in)validate the two conjectures. We observe\nthe promise of prompting in comparison to model tuning; we further unlock\nprompting's potential by introducing a variant called Inference-time Dynamic\nPrompting (IDP), that can effectively increase prompt diversity without\nincurring any inference overhead. Our experiments consistently suggest that\ncompared to the classical re-training alternatives such as LoRA, prompting with\nIDP leads to better or comparable post-compression performance recovery, while\nsaving the extra parameter size by 21x and reducing inference latency by 60%.\nOur experiments hence strongly endorse the conjecture of \"knowledge displaced\"\nover \"knowledge forgotten\", and shed light on a new efficient mechanism to\nrestore compressed LLM performance. We additionally visualize and analyze the\ndifferent attention and activation patterns between prompted and re-trained\nmodels, demonstrating they achieve performance recovery in two different\nregimes.",
        "pdf_link": "https://arxiv.org/pdf/2310.00867v3.pdf"
    },
    {
        "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
        "authors": [
            "Man Luo",
            "Shrinidhi Kumbhar",
            "Ming shen",
            "Mihir Parmar",
            "Neeraj Varshney",
            "Pratyay Banerjee",
            "Somak Aditya",
            "Chitta Baral"
        ],
        "published": "2023-10-02T01:00:50Z",
        "summary": "Logical reasoning is fundamental for humans yet presents a substantial\nchallenge in the domain of Artificial Intelligence. Initially, researchers used\nKnowledge Representation and Reasoning (KR) systems that did not scale and\nrequired non-trivial manual effort. Recently, the emergence of large language\nmodels (LLMs) has demonstrated the ability to overcome various limitations of\nformal Knowledge Representation (KR) systems. Consequently, there's a growing\ninterest in using LLMs for logical reasoning via natural language. This work\nstrives to understand the proficiency of LLMs in logical reasoning by offering\na brief review of the latest progress in this area; with a focus on the logical\nreasoning datasets, tasks, and the methods adopted to utilize LLMs for\nreasoning. To offer a thorough analysis, we have compiled a benchmark titled\nLogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,\nand inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an\ninstruction fine-tuned language model, resulting in LogiT5. We study\nsingle-task training, multi-task training, and \"chain-of-thought\" knowledge\ndistillation fine-tuning technique to assess the performance of model across\nthe different logical reasoning categories. We also assess various LLMs using\nLogiGLUE, and the findings indicate that LLMs excel most in abductive\nreasoning, followed by deductive reasoning, while they are least effective at\ninductive reasoning. We aim to shed light on the capabilities and potential\npathways for enhancing logical reasoning proficiency in LLMs, paving the way\nfor more advanced and nuanced developments in this critical field.",
        "pdf_link": "https://arxiv.org/pdf/2310.00836v3.pdf"
    },
    {
        "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
        "authors": [
            "Yapei Chang",
            "Kyle Lo",
            "Tanya Goyal",
            "Mohit Iyyer"
        ],
        "published": "2023-10-01T20:46:44Z",
        "summary": "Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.",
        "pdf_link": "https://arxiv.org/pdf/2310.00785v3.pdf"
    },
    {
        "title": "FELM: Benchmarking Factuality Evaluation of Large Language Models",
        "authors": [
            "Shiqi Chen",
            "Yiran Zhao",
            "Jinghan Zhang",
            "I-Chun Chern",
            "Siyang Gao",
            "Pengfei Liu",
            "Junxian He"
        ],
        "published": "2023-10-01T17:37:31Z",
        "summary": "Assessing factuality of text generated by large language models (LLMs) is an\nemerging yet crucial research area, aimed at alerting users to potential errors\nand guiding the development of more reliable LLMs. Nonetheless, the evaluators\nassessing factuality necessitate suitable evaluation themselves to gauge\nprogress and foster advancements. This direction remains under-explored,\nresulting in substantial impediments to the progress of factuality evaluators.\nTo mitigate this issue, we introduce a benchmark for Factuality Evaluation of\nlarge Language Models, referred to as felm. In this benchmark, we collect\nresponses generated from LLMs and annotate factuality labels in a fine-grained\nmanner. Contrary to previous studies that primarily concentrate on the\nfactuality of world knowledge (e.g.~information from Wikipedia), felm focuses\non factuality across diverse domains, spanning from world knowledge to math and\nreasoning. Our annotation is based on text segments, which can help pinpoint\nspecific factual errors. The factuality annotations are further supplemented by\npredefined error types and reference links that either support or contradict\nthe statement. In our experiments, we investigate the performance of several\nLLM-based factuality evaluators on felm, including both vanilla LLMs and those\naugmented with retrieval mechanisms and chain-of-thought processes. Our\nfindings reveal that while retrieval aids factuality evaluation, current LLMs\nare far from satisfactory to faithfully detect factual errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.00741v2.pdf"
    },
    {
        "title": "GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models",
        "authors": [
            "Emilio Ferrara"
        ],
        "published": "2023-10-01T17:25:56Z",
        "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare marvels of technology; celebrated for their prowess in natural language\nprocessing and multimodal content generation, they promise a transformative\nfuture. But as with all powerful tools, they come with their shadows. Picture\nliving in a world where deepfakes are indistinguishable from reality, where\nsynthetic identities orchestrate malicious campaigns, and where targeted\nmisinformation or scams are crafted with unparalleled precision. Welcome to the\ndarker side of GenAI applications. This article is not just a journey through\nthe meanders of potential misuse of GenAI and LLMs, but also a call to\nrecognize the urgency of the challenges ahead. As we navigate the seas of\nmisinformation campaigns, malicious content generation, and the eerie creation\nof sophisticated malware, we'll uncover the societal implications that ripple\nthrough the GenAI revolution we are witnessing. From AI-powered botnets on\nsocial media platforms to the unnerving potential of AI to generate fabricated\nidentities, or alibis made of synthetic realities, the stakes have never been\nhigher. The lines between the virtual and the real worlds are blurring, and the\nconsequences of potential GenAI's nefarious applications impact us all. This\narticle serves both as a synthesis of rigorous research presented on the risks\nof GenAI and misuse of LLMs and as a thought-provoking vision of the different\ntypes of harmful GenAI applications we might encounter in the near future, and\nsome ways we can prepare for them.",
        "pdf_link": "https://arxiv.org/pdf/2310.00737v3.pdf"
    },
    {
        "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries",
        "authors": [
            "Haiming Wang",
            "Huajian Xin",
            "Chuanyang Zheng",
            "Lin Li",
            "Zhengying Liu",
            "Qingxing Cao",
            "Yinya Huang",
            "Jing Xiong",
            "Han Shi",
            "Enze Xie",
            "Jian Yin",
            "Zhenguo Li",
            "Heng Liao",
            "Xiaodan Liang"
        ],
        "published": "2023-10-01T12:47:59Z",
        "summary": "Despite the success of large language models (LLMs), the task of theorem\nproving still remains one of the hardest reasoning tasks that is far from being\nfully solved. Prior methods using language models have demonstrated promising\nresults, but they still struggle to prove even middle school level theorems.\nOne common limitation of these methods is that they assume a fixed theorem\nlibrary during the whole theorem proving process. However, as we all know,\ncreating new useful theorems or even new theories is not only helpful but\ncrucial and necessary for advancing mathematics and proving harder and deeper\nresults. In this work, we present LEGO-Prover, which employs a growing skill\nlibrary containing verified lemmas as skills to augment the capability of LLMs\nused in theorem proving. By constructing the proof modularly, LEGO-Prover\nenables LLMs to utilize existing skills retrieved from the library and to\ncreate new skills during the proving process. These skills are further evolved\n(by prompting an LLM) to enrich the library on another scale. Modular and\nreusable skills are constantly added to the library to enable tackling\nincreasingly intricate mathematical problems. Moreover, the learned library\nfurther bridges the gap between human proofs and formal proofs by making it\neasier to impute missing steps. LEGO-Prover advances the state-of-the-art pass\nrate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).\nDuring the proving process, LEGO-Prover also manages to generate over 20,000\nskills (theorems/lemmas) and adds them to the growing library. Our ablation\nstudy indicates that these newly added skills are indeed helpful for proving\ntheorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We\nalso release our code and all the generated skills.",
        "pdf_link": "https://arxiv.org/pdf/2310.00656v3.pdf"
    },
    {
        "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning",
        "authors": [
            "Mustafa Shukor",
            "Alexandre Rame",
            "Corentin Dancette",
            "Matthieu Cord"
        ],
        "published": "2023-10-01T12:02:59Z",
        "summary": "Following the success of Large Language Models (LLMs), Large Multimodal\nModels (LMMs), such as the Flamingo model and its subsequent competitors, have\nstarted to emerge as natural steps towards generalist agents. However,\ninteracting with recent LMMs reveals major limitations that are hardly captured\nby the current evaluation benchmarks. Indeed, task performances (e.g., VQA\naccuracy) alone do not provide enough clues to understand their real\ncapabilities, limitations, and to which extent such models are aligned to human\nexpectations. To refine our understanding of those flaws, we deviate from the\ncurrent evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from\n3B up to 80B parameter scale, on 5 different axes; hallucinations, abstention,\ncompositionality, explainability and instruction following. Our evaluation on\nthese axes reveals major flaws in LMMs. While the current go-to solution to\nalign these models is based on training, such as instruction tuning or RLHF, we\nrather (2) explore the training-free in-context learning (ICL) as a solution,\nand study how it affects these limitations. Based on our ICL study, (3) we push\nICL further and propose new multimodal ICL variants such as; Multitask-ICL,\nChain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows.\n(1) Despite their success, LMMs have flaws that remain unsolved with scaling\nalone. (2) The effect of ICL on LMMs flaws is nuanced; despite its\neffectiveness for improved explainability, answer abstention, ICL only slightly\nimproves instruction following, does not improve compositional abilities, and\nactually even amplifies hallucinations. (3) The proposed ICL variants are\npromising as post-hoc approaches to efficiently tackle some of those flaws. The\ncode is available here: https://github.com/mshukor/EvALign-ICL.",
        "pdf_link": "https://arxiv.org/pdf/2310.00647v2.pdf"
    },
    {
        "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
        "authors": [
            "Yair Gat",
            "Nitay Calderon",
            "Amir Feder",
            "Alexander Chapanin",
            "Amit Sharma",
            "Roi Reichart"
        ],
        "published": "2023-10-01T07:31:04Z",
        "summary": "Causal explanations of the predictions of NLP systems are essential to ensure\nsafety and establish trust. Yet, existing methods often fall short of\nexplaining model predictions effectively or efficiently and are often\nmodel-specific. In this paper, we address model-agnostic explanations,\nproposing two approaches for counterfactual (CF) approximation. The first\napproach is CF generation, where a large language model (LLM) is prompted to\nchange a specific text concept while keeping confounding concepts unchanged.\nWhile this approach is demonstrated to be very effective, applying LLM at\ninference-time is costly. We hence present a second approach based on matching,\nand propose a method that is guided by an LLM at training-time and learns a\ndedicated embedding space. This space is faithful to a given causal graph and\neffectively serves to identify matches that approximate CFs. After showing\ntheoretically that approximating CFs is required in order to construct faithful\nexplanations, we benchmark our approaches and explain several models, including\nLLMs with billions of parameters. Our empirical results demonstrate the\nexcellent performance of CF generation models as model-agnostic explainers.\nMoreover, our matching approach, which requires far less test-time resources,\nalso provides effective explanations, surpassing many baselines. We also find\nthat Top-K techniques universally improve every tested method. Finally, we\nshowcase the potential of LLMs in constructing new benchmarks for model\nexplanation and subsequently validate our conclusions. Our work illuminates new\npathways for efficient and accurate approaches to interpreting NLP systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.00603v2.pdf"
    },
    {
        "title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length",
        "authors": [
            "Hongye Jin",
            "Xiaotian Han",
            "Jingfeng Yang",
            "Zhimeng Jiang",
            "Chia-Yuan Chang",
            "Xia Hu"
        ],
        "published": "2023-10-01T05:25:24Z",
        "summary": "The evolving sophistication and intricacies of Large Language Models (LLMs)\nyield unprecedented advancements, yet they simultaneously demand considerable\ncomputational resources and incur significant costs. To alleviate these\nchallenges, this paper introduces a novel, simple, and effective method named\n``\\growlength'' to accelerate the pretraining process of LLMs. Our method\nprogressively increases the training length throughout the pretraining phase,\nthereby mitigating computational costs and enhancing efficiency. For instance,\nit begins with a sequence length of 128 and progressively extends to 4096. This\napproach enables models to process a larger number of tokens within limited\ntime frames, potentially boosting their performance. In other words, the\nefficiency gain is derived from training with shorter sequences optimizing the\nutilization of resources. Our extensive experiments with various\nstate-of-the-art LLMs have revealed that models trained using our method not\nonly converge more swiftly but also exhibit superior performance metrics\ncompared to those trained with existing methods. Furthermore, our method for\nLLMs pretraining acceleration does not require any additional engineering\nefforts, making it a practical solution in the realm of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.00576v1.pdf"
    },
    {
        "title": "Measuring Value Understanding in Language Models through Discriminator-Critique Gap",
        "authors": [
            "Zhaowei Zhang",
            "Fengshuo Bai",
            "Jun Gao",
            "Yaodong Yang"
        ],
        "published": "2023-09-30T13:47:55Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have heightened concerns\nabout their potential misalignment with human values. However, evaluating their\ngrasp of these values is complex due to their intricate and adaptable nature.\nWe argue that truly understanding values in LLMs requires considering both\n\"know what\" and \"know why\". To this end, we present the Value Understanding\nMeasurement (VUM) framework that quantitatively assesses both \"know what\" and\n\"know why\" by measuring the discriminator-critique gap related to human values.\nUsing the Schwartz Value Survey, we specify our evaluation values and develop a\nthousand-level dialogue dataset with GPT-4. Our assessment looks at both the\nvalue alignment of LLM's outputs compared to baseline answers and how LLM\nresponses align with reasons for value recognition versus GPT-4's annotations.\nWe evaluate five representative LLMs and provide strong evidence that the\nscaling law significantly impacts \"know what\" but not much on \"know why\", which\nhas consistently maintained a high level. This may further suggest that LLMs\nmight craft plausible explanations based on the provided context without truly\nunderstanding their inherent value, indicating potential risks.",
        "pdf_link": "https://arxiv.org/pdf/2310.00378v3.pdf"
    },
    {
        "title": "Understanding In-Context Learning from Repetitions",
        "authors": [
            "Jianhao Yan",
            "Jin Xu",
            "Chiyu Song",
            "Chenming Wu",
            "Yafu Li",
            "Yue Zhang"
        ],
        "published": "2023-09-30T08:13:49Z",
        "summary": "This paper explores the elusive mechanism underpinning in-context learning in\nLarge Language Models (LLMs). Our work provides a novel perspective by\nexamining in-context learning via the lens of surface repetitions. We\nquantitatively investigate the role of surface features in text generation, and\nempirically establish the existence of \\emph{token co-occurrence\nreinforcement}, a principle that strengthens the relationship between two\ntokens based on their contextual co-occurrences. By investigating the dual\nimpacts of these features, our research illuminates the internal workings of\nin-context learning and expounds on the reasons for its failures. This paper\nprovides an essential contribution to the understanding of in-context learning\nand its potential limitations, providing a fresh perspective on this exciting\ncapability.",
        "pdf_link": "https://arxiv.org/pdf/2310.00297v3.pdf"
    },
    {
        "title": "Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting",
        "authors": [
            "Baphumelele Masikisiki",
            "Vukosi Marivate",
            "Yvette Hlope"
        ],
        "published": "2023-09-30T06:25:27Z",
        "summary": "Large Language Models, such as Generative Pre-trained Transformer 3 (aka.\nGPT-3), have been developed to understand language through the analysis of\nextensive text data, allowing them to identify patterns and connections between\nwords. While LLMs have demonstrated impressive performance across various\ntext-related tasks, they encounter challenges in tasks associated with\nreasoning. To address this challenge, Chain of Thought(CoT) prompting method\nhas been proposed as a means to enhance LLMs' proficiency in complex reasoning\ntasks like solving math word problems and answering questions based on logical\nargumentative reasoning. The primary aim of this research is to assess how well\nfour language models can grade reflective essays of third-year medical\nstudents. The assessment will specifically target the evaluation of critical\nthinking skills using CoT prompting.\n  The research will provide the following contributions; to introduce and\neducate on the process of instructing models to evaluate reflective essays from\na dataset they have not been previously trained on; to illustrate the use of\nCoT prompting as an instructional approach for training large models to carry\nout particular tasks. Our results suggest that among all the models, Llama-7b\nperforms the least effectively, displaying the highest mean squared error.\nConversely, ChatGPT emerges as the superior model, boasting a higher Cohen\nkappa score value of 0.53. Lastly, it's important to note that the selected\nmodels do prioritise user privacy by allowing users to delete their own\nconducted conversations.",
        "pdf_link": "https://arxiv.org/pdf/2310.00272v1.pdf"
    },
    {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published": "2023-09-30T05:20:02Z",
        "summary": "While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.",
        "pdf_link": "https://arxiv.org/pdf/2310.00259v1.pdf"
    },
    {
        "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
        "authors": [
            "Tianhao Wu",
            "Banghua Zhu",
            "Ruoyu Zhang",
            "Zhaojin Wen",
            "Kannan Ramchandran",
            "Jiantao Jiao"
        ],
        "published": "2023-09-30T01:23:22Z",
        "summary": "Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.",
        "pdf_link": "https://arxiv.org/pdf/2310.00212v3.pdf"
    },
    {
        "title": "Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs",
        "authors": [
            "Lu Yin",
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Souvik Kundu",
            "Zhangyang Wang"
        ],
        "published": "2023-09-29T22:55:06Z",
        "summary": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git.",
        "pdf_link": "https://arxiv.org/pdf/2310.02277v2.pdf"
    },
    {
        "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
        "authors": [
            "Hangfeng He",
            "Hongming Zhang",
            "Dan Roth"
        ],
        "published": "2023-09-29T18:25:46Z",
        "summary": "To comprehensively assess the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains to assess the model-derived chains. However, such\n``gold-standard'' human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning metrics\neliminate the need for human-crafted reasoning chains as references, but they\ntypically require fine-tuning on datasets with human-derived reasoning chains,\nwhich complicates the process and raises concerns regarding generalizability\nacross diverse datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, obviating the need for\nhuman-crafted references. Leveraging the Socratic method, we devise tailored\nprompts to enhance reference-free reasoning evaluation, which we term SocREval\n(Socratic method for Reasoning Evaluation). Empirical results from four human\nannotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, our proposed framework,\nlarge language models (LLMs) with the Socratic method, proves to be both\ncost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.",
        "pdf_link": "https://arxiv.org/pdf/2310.00074v1.pdf"
    },
    {
        "title": "Efficient Streaming Language Models with Attention Sinks",
        "authors": [
            "Guangxuan Xiao",
            "Yuandong Tian",
            "Beidi Chen",
            "Song Han",
            "Mike Lewis"
        ],
        "published": "2023-09-29T17:59:56Z",
        "summary": "Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na \"sink\" even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
        "pdf_link": "https://arxiv.org/pdf/2309.17453v4.pdf"
    },
    {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
        "authors": [
            "Ansong Ni",
            "Pengcheng Yin",
            "Yilun Zhao",
            "Martin Riddell",
            "Troy Feng",
            "Rui Shen",
            "Stephen Yin",
            "Ye Liu",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty",
            "Yingbo Zhou",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-09-29T17:57:00Z",
        "summary": "Recently, large language models (LLMs), especially those that are pretrained\non code, have demonstrated strong capabilities in generating programs from\nnatural language inputs in a few-shot or even zero-shot manner. Despite\npromising results, there is a notable lack of a comprehensive evaluation of\nthese models language-to-code generation capabilities. Existing studies often\nfocus on specific tasks, model architectures, or learning paradigms, leading to\na fragmented understanding of the overall landscape. In this work, we present\nL2CEval, a systematic evaluation of the language-to-code generation\ncapabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing,\nmath reasoning and Python programming, analyzing the factors that potentially\naffect their performance, such as model size, pretraining data, instruction\ntuning, and different prompting methods. In addition to assessing model\nperformance, we measure confidence calibration for the models and conduct human\nevaluations of the output programs. This enables us to identify and analyze the\ntypical failure modes across various tasks and models. L2CEval offers a\ncomprehensive understanding of the capabilities and limitations of LLMs in\nlanguage-to-code generation. We also release the evaluation framework and all\nmodel outputs, hoping to lay the groundwork for further future research in this\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2309.17446v2.pdf"
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "authors": [
            "Long Lian",
            "Baifeng Shi",
            "Adam Yala",
            "Trevor Darrell",
            "Boyi Li"
        ],
        "published": "2023-09-29T17:54:46Z",
        "summary": "Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion (e.g.,\neven lacking the ability to be prompted for objects moving from left to right).\nTo address these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.",
        "pdf_link": "https://arxiv.org/pdf/2309.17444v2.pdf"
    },
    {
        "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
        "authors": [
            "Vaidehi Patil",
            "Peter Hase",
            "Mohit Bansal"
        ],
        "published": "2023-09-29T17:12:43Z",
        "summary": "Pretrained language models sometimes possess knowledge that we do not wish\nthem to, including memorized personal information and knowledge that could be\nused to harm people. They can also output toxic or harmful text. To mitigate\nthese safety and informational issues, we propose an attack-and-defense\nframework for studying the task of deleting sensitive information directly from\nmodel weights. We study direct edits to model weights because (1) this approach\nshould guarantee that particular deleted information is never extracted by\nfuture prompt attacks, and (2) it should protect against whitebox attacks,\nwhich is necessary for making claims about safety/privacy in a setting where\npublicly available model weights could be used to elicit sensitive information.\nOur threat model assumes that an attack succeeds if the answer to a sensitive\nquestion is located among a set of B generated candidates, based on scenarios\nwhere the information would be insecure if the answer is among B candidates.\nExperimentally, we show that even state-of-the-art model editing methods such\nas ROME struggle to truly delete factual information from models like GPT-J, as\nour whitebox and blackbox attacks can recover \"deleted\" information from an\nedited model 38% of the time. These attacks leverage two key observations: (1)\nthat traces of deleted information can be found in intermediate model hidden\nstates, and (2) that applying an editing method for one question may not delete\ninformation across rephrased versions of the question. Finally, we provide new\ndefense methods that protect against some extraction attacks, but we do not\nfind a single universally effective defense method. Our results suggest that\ntruly deleting sensitive information is a tractable but difficult problem,\nsince even relatively low attack success rates have potentially severe societal\nimplications for real-world deployment of language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.17410v1.pdf"
    },
    {
        "title": "LoRA ensembles for large language model fine-tuning",
        "authors": [
            "Xi Wang",
            "Laurence Aitchison",
            "Maja Rudolph"
        ],
        "published": "2023-09-29T16:38:38Z",
        "summary": "Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as\noverconfidence, poor calibration, and unreliable prediction results on test\ndata or out-of-distribution samples. One approach commonly used in vision for\nalleviating this issue is a deep ensemble, which constructs an ensemble by\ntraining the same model multiple times using different random initializations.\nHowever, there is a huge challenge to ensembling LLMs: the most effective LLMs\nare very, very large. Keeping a single LLM in memory is already challenging\nenough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many\nsettings. To address these issues, we propose an ensemble approach using\nLow-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.\nCritically, these low-rank adapters represent a very small number of\nparameters, orders of magnitude less than the underlying pre-trained model.\nThus, it is possible to construct large ensembles of LoRA adapters with almost\nthe same computational overhead as using the original model. We find that LoRA\nensembles, applied on its own or on top of pre-existing regularization\ntechniques, gives consistent improvements in predictive accuracy and\nuncertainty quantification.",
        "pdf_link": "https://arxiv.org/pdf/2310.00035v2.pdf"
    },
    {
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",
        "authors": [
            "Zhihan Liu",
            "Hao Hu",
            "Shenao Zhang",
            "Hongyi Guo",
            "Shuqi Ke",
            "Boyi Liu",
            "Zhaoran Wang"
        ],
        "published": "2023-09-29T16:36:39Z",
        "summary": "Large language models (LLMs) demonstrate impressive reasoning abilities, but\ntranslating reasoning into actions in the real world remains challenging. In\nparticular, it remains unclear how to complete a given task provably within a\nminimum number of interactions with the external environment, e.g., through an\ninternal mechanism of reasoning. To this end, we propose a principled framework\nwith provable regret guarantees to orchestrate reasoning and acting, which we\ncall \"reason for future, act for now\" (\\texttt{RAFA}). Specifically, we design\na prompt template for reasoning that learns from the memory buffer and plans a\nfuture trajectory over a long horizon (\"reason for future\"). At each step, the\nLLM agent takes the initial action of the planned trajectory (\"act for now\"),\nstores the collected feedback in the memory buffer, and reinvokes the reasoning\nroutine to replan the future trajectory from the new state.\n  The key idea is to cast reasoning in LLMs as learning and planning in\nBayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt\nLLMs to form an updated posterior of the unknown environment from the memory\nbuffer (learning) and generate an optimal trajectory for multiple future steps\nthat maximizes a value function (planning). The learning and planning\nsubroutines are performed in an \"in-context\" manner to emulate the actor-critic\nupdate for MDPs. Our theoretical analysis proves that the novel combination of\nlong-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In\nparticular, the regret bound highlights an intriguing interplay between the\nprior knowledge obtained through pretraining and the uncertainty reduction\nachieved by reasoning and acting. Our empirical validation shows that it\noutperforms various existing frameworks and achieves nearly perfect scores on a\nfew benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2309.17382v2.pdf"
    },
    {
        "title": "Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings",
        "authors": [
            "Edouard Yvinec",
            "Arnaud Dapogny",
            "Kevin Bailly"
        ],
        "published": "2023-09-29T16:04:55Z",
        "summary": "The massive interest in deep neural networks (DNNs) for both computer vision\nand natural language processing has been sparked by the growth in computational\npower. However, this led to an increase in the memory footprint, to a point\nwhere it can be challenging to simply load a model on commodity devices such as\nmobile phones. To address this limitation, quantization is a favored solution\nas it maps high precision tensors to a low precision, memory efficient format.\nIn terms of memory footprint reduction, its most effective variants are based\non codebooks. These methods, however, suffer from two limitations. First, they\neither define a single codebook for each tensor, or use a memory-expensive\nmapping to multiple codebooks. Second, gradient descent optimization of the\nmapping favors jumps toward extreme values, hence not defining a proximal\nsearch. In this work, we propose to address these two limitations. First, we\ninitially group similarly distributed neurons and leverage the re-ordered\nstructure to either apply different scale factors to the different groups, or\nmap weights that fall in these groups to several codebooks, without any mapping\noverhead. Second, stemming from this initialization, we propose a joint\nlearning of the codebook and weight mappings that bears similarities with\nrecent gradient-based post-training quantization techniques. Third, drawing\nestimation from straight-through estimation techniques, we introduce a novel\ngradient update definition to enable a proximal search of the codebooks and\ntheir mappings. The proposed jointly learnable codebooks and mappings (JLCM)\nmethod allows a very efficient approximation of any DNN: as such, a Llama 7B\ncan be compressed down to 2Go and loaded on 5-year-old smartphones.",
        "pdf_link": "https://arxiv.org/pdf/2309.17361v1.pdf"
    },
    {
        "title": "Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis",
        "authors": [
            "Paul Glasserman",
            "Caden Lin"
        ],
        "published": "2023-09-29T15:30:32Z",
        "summary": "Large language models (LLMs), including ChatGPT, can extract profitable\ntrading signals from the sentiment in news text. However, backtesting such\nstrategies poses a challenge because LLMs are trained on many years of data,\nand backtesting produces biased results if the training and backtesting periods\noverlap. This bias can take two forms: a look-ahead bias, in which the LLM may\nhave specific knowledge of the stock returns that followed a news article, and\na distraction effect, in which general knowledge of the companies named\ninterferes with the measurement of a text's sentiment. We investigate these\nsources of bias through trading strategies driven by the sentiment of financial\nnews headlines. We compare trading performance based on the original headlines\nwith de-biased strategies in which we remove the relevant company's identifiers\nfrom the text. In-sample (within the LLM training window), we find,\nsurprisingly, that the anonymized headlines outperform, indicating that the\ndistraction effect has a greater impact than look-ahead bias. This tendency is\nparticularly strong for larger companies--companies about which we expect an\nLLM to have greater general knowledge. Out-of-sample, look-ahead bias is not a\nconcern but distraction remains possible. Our proposed anonymization procedure\nis therefore potentially useful in out-of-sample implementation, as well as for\nde-biased backtesting.",
        "pdf_link": "https://arxiv.org/pdf/2309.17322v1.pdf"
    },
    {
        "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
        "authors": [
            "Zongjie Li",
            "Chaozheng Wang",
            "Pingchuan Ma",
            "Daoyuan Wu",
            "Shuai Wang",
            "Cuiyun Gao",
            "Yang Liu"
        ],
        "published": "2023-09-29T14:38:58Z",
        "summary": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.01432v2.pdf"
    },
    {
        "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
        "authors": [
            "Jiaxian Guo",
            "Bo Yang",
            "Paul Yoo",
            "Bill Yuchen Lin",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "published": "2023-09-29T14:30:03Z",
        "summary": "Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2309.17277v2.pdf"
    },
    {
        "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games",
        "authors": [
            "Sahar Abdelnabi",
            "Amr Gomaa",
            "Sarath Sivaprasad",
            "Lea Sch√∂nherr",
            "Mario Fritz"
        ],
        "published": "2023-09-29T13:33:06Z",
        "summary": "There is a growing interest in using Large Language Models (LLMs) as agents\nto tackle real-world tasks that may require assessing complex situations. Yet,\nwe have a limited understanding of LLMs' reasoning and decision-making\ncapabilities, partly stemming from a lack of dedicated evaluation benchmarks.\nAs negotiating and compromising are key aspects of our everyday communication\nand collaboration, we propose using scorable negotiation games as a new\nevaluation framework for LLMs. We create a testbed of diverse text-based,\nmulti-agent, multi-issue, semantically rich negotiation games, with easily\ntunable difficulty. To solve the challenge, agents need to have strong\narithmetic, inference, exploration, and planning capabilities, while seamlessly\nintegrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT),\nwe show that agents can negotiate and consistently reach successful deals. We\nquantify the performance with multiple metrics and observe a large gap between\nGPT-4 and earlier models. Importantly, we test the generalization to new games\nand setups. Finally, we show that these games can help evaluate other critical\naspects, such as the interaction dynamics between agents in the presence of\ngreedy and adversarial players.",
        "pdf_link": "https://arxiv.org/pdf/2309.17234v1.pdf"
    },
    {
        "title": "Training and inference of large language models using 8-bit floating point",
        "authors": [
            "Sergio P. Perez",
            "Yan Zhang",
            "James Briggs",
            "Charlie Blake",
            "Josh Levy-Kramer",
            "Paul Balanca",
            "Carlo Luschi",
            "Stephen Barlow",
            "Andrew William Fitzgibbon"
        ],
        "published": "2023-09-29T13:24:33Z",
        "summary": "FP8 formats are gaining popularity to boost the computational efficiency for\ntraining and inference of large deep learning models. Their main challenge is\nthat a careful choice of scaling is needed to prevent degradation due to the\nreduced dynamic range compared to higher-precision formats. Although there\nexists ample literature about selecting such scalings for INT formats, this\ncritical aspect has yet to be addressed for FP8. This paper presents a\nmethodology to select the scalings for FP8 linear layers, based on dynamically\nupdating per-tensor scales for the weights, gradients and activations. We apply\nthis methodology to train and validate large language models of the type of GPT\nand Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate\nthe understanding of the FP8 dynamics, our results are accompanied by plots of\nthe per-tensor scale distribution for weights, activations and gradients during\nboth training and inference.",
        "pdf_link": "https://arxiv.org/pdf/2309.17224v1.pdf"
    },
    {
        "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training",
        "authors": [
            "Xidong Feng",
            "Ziyu Wan",
            "Muning Wen",
            "Stephen Marcus McAleer",
            "Ying Wen",
            "Weinan Zhang",
            "Jun Wang"
        ],
        "published": "2023-09-29T12:20:19Z",
        "summary": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64.",
        "pdf_link": "https://arxiv.org/pdf/2309.17179v2.pdf"
    },
    {
        "title": "LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud",
        "authors": [
            "Mengke Zhang",
            "Tianxing He",
            "Tianle Wang",
            "Lu Mi",
            "Fatemehsadat Mireshghallah",
            "Binyi Chen",
            "Hao Wang",
            "Yulia Tsvetkov"
        ],
        "published": "2023-09-29T11:46:07Z",
        "summary": "In the current user-server interaction paradigm of prompted generation with\nlarge language models (LLM) on cloud, the server fully controls the generation\nprocess, which leaves zero options for users who want to keep the generated\ntext to themselves. We propose LatticeGen, a cooperative framework in which the\nserver still handles most of the computation while the user controls the\nsampling operation. The key idea is that the true generated sequence is mixed\nwith noise tokens by the user and hidden in a noised lattice. Considering\npotential attacks from a hypothetically malicious server and how the user can\ndefend against it, we propose the repeated beam-search attack and the mixing\nnoise scheme. In our experiments we apply LatticeGen to protect both prompt and\ngeneration. It is shown that while the noised lattice degrades generation\nquality, LatticeGen successfully protects the true generation to a remarkable\ndegree under strong attacks (more than 50% of the semantic remains hidden as\nmeasured by BERTScore).",
        "pdf_link": "https://arxiv.org/pdf/2309.17157v5.pdf"
    },
    {
        "title": "Using Large Language Models for Qualitative Analysis can Introduce Serious Bias",
        "authors": [
            "Julian Ashwin",
            "Aditya Chhabra",
            "Vijayendra Rao"
        ],
        "published": "2023-09-29T11:19:15Z",
        "summary": "Large Language Models (LLMs) are quickly becoming ubiquitous, but the\nimplications for social science research are not yet well understood. This\npaper asks whether LLMs can help us analyse large-N qualitative data from\nopen-ended interviews, with an application to transcripts of interviews with\nRohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of\ncaution is needed in using LLMs to annotate text as there is a risk of\nintroducing biases that can lead to misleading inferences. We here mean bias in\nthe technical sense, that the errors that LLMs make in annotating interview\ntranscripts are not random with respect to the characteristics of the interview\nsubjects. Training simpler supervised models on high-quality human annotations\nwith flexible coding leads to less measurement error and bias than LLM\nannotations. Therefore, given that some high quality annotations are necessary\nin order to asses whether an LLM introduces bias, we argue that it is probably\npreferable to train a bespoke model on these annotations than it is to use an\nLLM for annotation.",
        "pdf_link": "https://arxiv.org/pdf/2309.17147v2.pdf"
    },
    {
        "title": "Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?",
        "authors": [
            "Johannes Frey",
            "Lars-Peter Meyer",
            "Natanael Arndt",
            "Felix Brei",
            "Kirill Bulert"
        ],
        "published": "2023-09-29T10:36:04Z",
        "summary": "Large Language Models (LLMs) are advancing at a rapid pace, with significant\nimprovements at natural language processing and coding tasks. Yet, their\nability to work with formal languages representing data, specifically within\nthe realm of knowledge graph engineering, remains under-investigated. To\nevaluate the proficiency of various LLMs, we created a set of five tasks that\nprobe their ability to parse, understand, analyze, and create knowledge graphs\nserialized in Turtle syntax. These tasks, each embodying distinct degrees of\ncomplexity and being able to scale with the size of the problem, have been\nintegrated into our automated evaluation system, the LLM-KG-Bench. The\nevaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,\nClaude 1.3, and Claude 2.0, as well as two freely accessible offline models,\nGPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth\nunderstanding of the strengths and shortcomings of LLMs in relation to their\napplication within RDF knowledge graph engineering workflows utilizing Turtle\nrepresentation. While our findings show that the latest commercial models\noutperform their forerunners in terms of proficiency with the Turtle language,\nthey also reveal an apparent weakness. These models fall short when it comes to\nadhering strictly to the output formatting constraints, a crucial requirement\nin this context.",
        "pdf_link": "https://arxiv.org/pdf/2309.17122v1.pdf"
    },
    {
        "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
        "authors": [
            "Ryan Koo",
            "Minhwa Lee",
            "Vipul Raheja",
            "Jong Inn Park",
            "Zae Myung Kim",
            "Dongyeop Kang"
        ],
        "published": "2023-09-29T06:53:10Z",
        "summary": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
        "pdf_link": "https://arxiv.org/pdf/2309.17012v1.pdf"
    },
    {
        "title": "Medical Foundation Models are Susceptible to Targeted Misinformation Attacks",
        "authors": [
            "Tianyu Han",
            "Sven Nebelung",
            "Firas Khader",
            "Tianci Wang",
            "Gustav Mueller-Franzes",
            "Christiane Kuhl",
            "Sebastian F√∂rsch",
            "Jens Kleesiek",
            "Christoph Haarburger",
            "Keno K. Bressem",
            "Jakob Nikolas Kather",
            "Daniel Truhn"
        ],
        "published": "2023-09-29T06:44:36Z",
        "summary": "Large language models (LLMs) have broad medical knowledge and can reason\nabout medical information across many domains, holding promising potential for\ndiverse medical applications in the near future. In this study, we demonstrate\na concerning vulnerability of LLMs in medicine. Through targeted manipulation\nof just 1.1% of the model's weights, we can deliberately inject an incorrect\nbiomedical fact. The erroneous information is then propagated in the model's\noutput, whilst its performance on other biomedical tasks remains intact. We\nvalidate our findings in a set of 1,038 incorrect biomedical facts. This\npeculiar susceptibility raises serious security and trustworthiness concerns\nfor the application of LLMs in healthcare settings. It accentuates the need for\nrobust protective measures, thorough verification mechanisms, and stringent\nmanagement of access to these models, ensuring their reliable and safe use in\nmedical practice.",
        "pdf_link": "https://arxiv.org/pdf/2309.17007v1.pdf"
    },
    {
        "title": "I Wish to Have an Argument: Argumentative Reasoning in Large Language Models",
        "authors": [
            "Adrian de Wynter",
            "Tommy Yuan"
        ],
        "published": "2023-09-29T02:41:38Z",
        "summary": "We evaluate the ability of contemporary large language models (LLMs) to\nperform argumentative reasoning. We frame our experiments in terms of the\nargument mining (AM) and argument pair extraction (APE) tasks, and evaluate\ntheir ability to perform reasoning at increasing levels of abstraction in the\ninput and output representations (e.g., arbitrary label sets, semantic graphs).\nWe find that, although LLMs are able to match or surpass the state-of-the-art\nin AM and APE, their argumentative reasoning performance is very dependent on\nthe input and output representation. We also find an \"exemplar effect\", where\ntoo many exemplars increasingly become detrimental for task performance, and\nabout 4-5 being the optimal amount. Neither result extends to chain-of-thought\n(CoT) prompting: we find the exemplar effect to be nullified, and our results\nsuggest that CoT allows for better performance under ill-conditioned problems.\nWe hope that the work reported contributes to the improvement of argumentative\nreasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16938v1.pdf"
    },
    {
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
        "authors": [
            "Shen Zheng",
            "Yuyu Zhang",
            "Yijie Zhu",
            "Chenguang Xi",
            "Pengyang Gao",
            "Xun Zhou",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-09-28T16:43:35Z",
        "summary": "With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16583v6.pdf"
    },
    {
        "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving",
        "authors": [
            "Sumit Kumar Jha",
            "Susmit Jha",
            "Patrick Lincoln",
            "Nathaniel D. Bastian",
            "Alvaro Velasquez",
            "Rickard Ewetz",
            "Sandeep Neema"
        ],
        "published": "2023-09-28T13:40:50Z",
        "summary": "Generative large language models (LLMs) with instruct training such as GPT-4\ncan follow human-provided instruction prompts and generate human-like responses\nto these prompts. Apart from natural language responses, they have also been\nfound to be effective at generating formal artifacts such as code, plans, and\nlogical specifications from natural language prompts. Despite their remarkably\nimproved accuracy, these models are still known to produce factually incorrect\nor contextually inappropriate results despite their syntactic coherence - a\nphenomenon often referred to as hallucination. This limitation makes it\ndifficult to use these models to synthesize formal artifacts that are used in\nsafety-critical applications. Unlike tasks such as text summarization and\nquestion-answering, bugs in code, plan, and other formal artifacts produced by\nLLMs can be catastrophic. We posit that we can use the satisfiability modulo\ntheory (SMT) solvers as deductive reasoning engines to analyze the generated\nsolutions from the LLMs, produce counterexamples when the solutions are\nincorrect, and provide that feedback to the LLMs exploiting the dialog\ncapability of instruct-trained LLMs. This interaction between inductive LLMs\nand deductive SMT solvers can iteratively steer the LLM to generate the correct\nresponse. In our experiments, we use planning over the domain of blocks as our\nsynthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,\nDavinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our\nmethod allows the user to communicate the planning problem in natural language;\neven the formulation of queries to SMT solvers is automatically generated from\nnatural language. Thus, the proposed technique can enable non-expert users to\ndescribe their problems in natural language, and the combination of LLMs and\nSMT solvers can produce provably correct solutions.",
        "pdf_link": "https://arxiv.org/pdf/2309.16436v1.pdf"
    },
    {
        "title": "Human Feedback is not Gold Standard",
        "authors": [
            "Tom Hosking",
            "Phil Blunsom",
            "Max Bartolo"
        ],
        "published": "2023-09-28T11:18:20Z",
        "summary": "Human feedback has become the de facto standard for evaluating the\nperformance of Large Language Models, and is increasingly being used as a\ntraining objective. However, it is not clear which properties of a generated\noutput this single `preference' score captures. We hypothesise that preference\nscores are subjective and open to undesirable biases. We critically analyse the\nuse of human feedback for both training and evaluation, to verify whether it\nfully captures a range of crucial error criteria. We find that while preference\nscores have fairly good coverage, they under-represent important aspects like\nfactuality. We further hypothesise that both preference scores and error\nannotation may be affected by confounders, and leverage instruction-tuned\nmodels to generate outputs that vary along two possible confounding dimensions:\nassertiveness and complexity. We find that the assertiveness of an output skews\nthe perceived rate of factuality errors, indicating that human annotations are\nnot a fully reliable evaluation metric or training objective. Finally, we offer\npreliminary evidence that using human feedback as a training objective\ndisproportionately increases the assertiveness of model outputs. We encourage\nfuture work to carefully consider whether preference scores are well aligned\nwith the desired objective.",
        "pdf_link": "https://arxiv.org/pdf/2309.16349v2.pdf"
    },
    {
        "title": "Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks",
        "authors": [
            "Eleftherios Triantafyllidis",
            "Filippos Christianos",
            "Zhibin Li"
        ],
        "published": "2023-09-28T11:14:52Z",
        "summary": "Current reinforcement learning algorithms struggle in sparse and complex\nenvironments, most notably in long-horizon manipulation tasks entailing a\nplethora of different sequences. In this work, we propose the Intrinsically\nGuided Exploration from Large Language Models (IGE-LLMs) framework. By\nleveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the\nexploratory process in reinforcement learning to address intricate long-horizon\nwith sparse rewards robotic manipulation tasks. We evaluate our framework and\nrelated intrinsic learning methods in an environment challenged with\nexploration, and a complex robotic manipulation task challenged by both\nexploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher\nperformance over related intrinsic methods and the direct use of LLMs in\ndecision-making, (ii) can be combined and complement existing learning methods\nhighlighting its modularity, (iii) are fairly insensitive to different\nintrinsic scaling parameters, and (iv) maintain robustness against increased\nlevels of uncertainty and horizons.",
        "pdf_link": "https://arxiv.org/pdf/2309.16347v2.pdf"
    },
    {
        "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
        "authors": [
            "Zhiwei Fei",
            "Xiaoyu Shen",
            "Dawei Zhu",
            "Fengzhe Zhou",
            "Zhuo Han",
            "Songyang Zhang",
            "Kai Chen",
            "Zongwen Shen",
            "Jidong Ge"
        ],
        "published": "2023-09-28T09:35:59Z",
        "summary": "Large language models (LLMs) have demonstrated strong capabilities in various\naspects. However, when applying them to the highly specialized, safe-critical\nlegal domain, it is unclear how much legal knowledge they possess and whether\nthey can reliably perform legal-related tasks. To address this gap, we propose\na comprehensive evaluation benchmark LawBench. LawBench has been meticulously\ncrafted to have precise assessment of the LLMs' legal capabilities from three\ncognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize\nneeded legal concepts, articles and facts; (2) Legal knowledge understanding:\nwhether LLMs can comprehend entities, events and relationships within legal\ntext; (3) Legal knowledge applying: whether LLMs can properly utilize their\nlegal knowledge and make necessary reasoning steps to solve realistic legal\ntasks. LawBench contains 20 diverse tasks covering 5 task types: single-label\nclassification (SLC), multi-label classification (MLC), regression, extraction\nand generation. We perform extensive evaluations of 51 LLMs on LawBench,\nincluding 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific\nLLMs. The results show that GPT-4 remains the best-performing LLM in the legal\ndomain, surpassing the others by a significant margin. While fine-tuning LLMs\non legal specific text brings certain improvements, we are still a long way\nfrom obtaining usable and reliable LLMs in legal tasks. All data, model\npredictions and evaluation code are released in\nhttps://github.com/open-compass/LawBench/. We hope this benchmark provides\nin-depth understanding of the LLMs' domain-specified capabilities and speed up\nthe development of LLMs in the legal domain.",
        "pdf_link": "https://arxiv.org/pdf/2309.16289v1.pdf"
    },
    {
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
        "authors": [
            "Chaoqi Wang",
            "Yibo Jiang",
            "Chenghao Yang",
            "Han Liu",
            "Yuxin Chen"
        ],
        "published": "2023-09-28T08:29:44Z",
        "summary": "The increasing capabilities of large language models (LLMs) raise\nopportunities for artificial general intelligence but concurrently amplify\nsafety concerns, such as potential misuse of AI systems, necessitating\neffective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has\nemerged as a promising pathway towards AI alignment but brings forth challenges\ndue to its complexity and dependence on a separate reward model. Direct\nPreference Optimization (DPO) has been proposed as an alternative, and it\nremains equivalent to RLHF under the reverse KL regularization constraint. This\npaper presents $f$-DPO, a generalized approach to DPO by incorporating diverse\ndivergence constraints. We show that under certain $f$-divergences, including\nJensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the\ncomplex relationship between the reward and optimal policy can also be\nsimplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the\nneed for estimating the normalizing constant in the Bradley-Terry model and\nenables a tractable mapping between the reward function and the optimal policy.\nOur approach optimizes LLMs to align with human preferences in a more efficient\nand supervised manner under a broad set of divergence constraints. Empirically,\nadopting these divergences ensures a balance between alignment performance and\ngeneration diversity. Importantly, $f$-DPO outperforms PPO-based methods in\ndivergence efficiency, and divergence constraints directly influence expected\ncalibration error (ECE).",
        "pdf_link": "https://arxiv.org/pdf/2309.16240v1.pdf"
    },
    {
        "title": "AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events",
        "authors": [
            "Yiming Li",
            "Jianfu Li",
            "Jianping He",
            "Cui Tao"
        ],
        "published": "2023-09-28T03:53:21Z",
        "summary": "Though Vaccines are instrumental in global health, mitigating infectious\ndiseases and pandemic outbreaks, they can occasionally lead to adverse events\n(AEs). Recently, Large Language Models (LLMs) have shown promise in effectively\nidentifying and cataloging AEs within clinical reports. Utilizing data from the\nVaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study\nparticularly focuses on AEs to evaluate LLMs' capability for AE extraction. A\nvariety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2,\nwere evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5\nmodel (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match\nand 0.816 for relaxed match. The encouraging performance of the AE-GPT\nunderscores LLMs' potential in processing medical data, indicating a\nsignificant stride towards advanced AE detection, thus presumably generalizable\nto other AE extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.16150v1.pdf"
    },
    {
        "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
        "authors": [
            "Junjie Yin",
            "Jiahao Dong",
            "Yingheng Wang",
            "Christopher De Sa",
            "Volodymyr Kuleshov"
        ],
        "published": "2023-09-28T02:55:01Z",
        "summary": "We propose a memory-efficient finetuning algorithm for large language models\n(LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision\non as little as one 24GB GPU. Our method, modular low-rank adaptation\n(ModuLoRA), integrates any user-specified weight quantizer with finetuning via\nlow-rank adapters (LoRAs). Our approach relies on a simple\nquantization-agnostic backward pass that adaptively materializes low-precision\nLLM weights from a custom black-box quantization module. This approach enables\nfinetuning 2-bit and 3-bit LLMs for the first time -- leveraging\nstate-of-the-art 2-bit QuIP\\# quantization and 3-bit OPTQ quantization --\noutperforming finetuning that relies on less sophisticated 4-bit and 8-bit\nmethods. In our experiments, \\lplora~attains competitive performance on text\nclassification, natural language inference, and instruction following tasks\nusing significantly less memory than existing approaches, and we also surpass\nthe state-of-the-art ROUGE score on a popular summarization task. We release\n\\lplora~together with a series of low-precision models as part of \\llmtune, a\nuser-friendly library for quantizing, running, and finetuning LLMs on consumer\nGPUs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16119v2.pdf"
    },
    {
        "title": "MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases",
        "authors": [
            "Yucheng Shi",
            "Shaochen Xu",
            "Zhengliang Liu",
            "Tianming Liu",
            "Xiang Li",
            "Ninghao Liu"
        ],
        "published": "2023-09-27T21:26:03Z",
        "summary": "Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks like medical question answering (QA).\nMoreover, they tend to function as \"black-boxes,\" making it challenging to\nmodify their behavior. Addressing this, our study delves into model editing\nutilizing in-context learning, aiming to improve LLM responses without the need\nfor fine-tuning or retraining. Specifically, we propose a comprehensive\nretrieval strategy to extract medical facts from an external knowledge base,\nand then we incorporate them into the query prompt for the LLM. Focusing on\nmedical QA using the MedQA-SMILE dataset, we evaluate the impact of different\nretrieval models and the number of facts provided to the LLM. Notably, our\nedited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.\nThis work underscores the potential of model editing to enhance LLM\nperformance, offering a practical approach to mitigate the challenges of\nblack-box LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16035v1.pdf"
    },
    {
        "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving",
        "authors": [
            "Chuanyang Zheng",
            "Haiming Wang",
            "Enze Xie",
            "Zhengying Liu",
            "Jiankai Sun",
            "Huajian Xin",
            "Jianhao Shen",
            "Zhenguo Li",
            "Yu Li"
        ],
        "published": "2023-09-27T17:29:41Z",
        "summary": "Large Language Models (LLMs) present an intriguing avenue for exploration in\nthe field of formal theorem proving. Nevertheless, their full potential,\nparticularly concerning the mitigation of hallucinations and refinement through\nprover error messages, remains an area that has yet to be thoroughly\ninvestigated. To enhance the effectiveness of LLMs in the field, we introduce\nthe Lyra, a new framework that employs two distinct correction mechanisms: Tool\nCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction in\nthe post-processing of formal proofs, we leverage prior knowledge to utilize\npredefined prover tools (e.g., Sledgehammer) for guiding the replacement of\nincorrect tools. Tool Correction significantly contributes to mitigating\nhallucinations, thereby improving the overall accuracy of the proof. In\naddition, we introduce Conjecture Correction, an error feedback mechanism\ndesigned to interact with prover to refine formal proof conjectures with prover\nerror messages. Compared to the previous refinement framework, the proposed\nConjecture Correction refines generation with instruction but does not collect\npaired (generation, error & refinement) prompts. Our method has achieved\nstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -> 55.3%)\nand test (45.5% -> 51.2%). We also present 3 IMO problems solved by Lyra. We\nbelieve Tool Correction (post-process for hallucination mitigation) and\nConjecture Correction (subgoal adjustment from interaction with environment)\ncould provide a promising avenue for future research in this field.",
        "pdf_link": "https://arxiv.org/pdf/2309.15806v3.pdf"
    },
    {
        "title": "Large Language Model Routing with Benchmark Datasets",
        "authors": [
            "Tal Shnitzer",
            "Anthony Ou",
            "M√≠rian Silva",
            "Kate Soule",
            "Yuekai Sun",
            "Justin Solomon",
            "Neil Thompson",
            "Mikhail Yurochkin"
        ],
        "published": "2023-09-27T17:08:40Z",
        "summary": "There is a rapidly growing number of open-source Large Language Models (LLMs)\nand benchmark datasets to compare them. While some models dominate these\nbenchmarks, no single model typically achieves the best accuracy in all tasks\nand use cases. In this work, we address the challenge of selecting the best LLM\nout of a collection of models for new tasks. We propose a new formulation for\nthe problem, in which benchmark datasets are repurposed to learn a \"router\"\nmodel for this LLM selection, and we show that this problem can be reduced to a\ncollection of binary classification tasks. We demonstrate the utility and\nlimitations of learning model routers from various benchmark datasets, where we\nconsistently improve performance upon using any single model for all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.15789v1.pdf"
    },
    {
        "title": "Deep Model Fusion: A Survey",
        "authors": [
            "Weishi Li",
            "Yong Peng",
            "Miao Zhang",
            "Liang Ding",
            "Han Hu",
            "Li Shen"
        ],
        "published": "2023-09-27T14:40:12Z",
        "summary": "Deep model fusion/merging is an emerging technique that merges the parameters\nor predictions of multiple deep learning models into a single one. It combines\nthe abilities of different models to make up for the biases and errors of a\nsingle model to achieve better performance. However, deep model fusion on\nlarge-scale deep learning models (e.g., LLMs and foundation models) faces\nseveral challenges, including high computational cost, high-dimensional\nparameter space, interference between different heterogeneous models, etc.\nAlthough model fusion has attracted widespread attention due to its potential\nto solve complex real-world tasks, there is still a lack of complete and\ndetailed survey research on this technique. Accordingly, in order to understand\nthe model fusion method better and promote its development, we present a\ncomprehensive survey to summarize the recent progress. Specifically, we\ncategorize existing deep model fusion methods as four-fold: (1) \"Mode\nconnectivity\", which connects the solutions in weight space via a path of\nnon-increasing loss, in order to obtain better initialization for model fusion;\n(2) \"Alignment\" matches units between neural networks to create better\nconditions for fusion; (3) \"Weight average\", a classical model fusion method,\naverages the weights of multiple models to obtain more accurate results closer\nto the optimal solution; (4) \"Ensemble learning\" combines the outputs of\ndiverse models, which is a foundational technique for improving the accuracy\nand robustness of the final model. In addition, we analyze the challenges faced\nby deep model fusion and propose possible research directions for model fusion\nin the future. Our review is helpful in deeply understanding the correlation\nbetween different model fusion methods and practical application methods, which\ncan enlighten the research in the field of deep model fusion.",
        "pdf_link": "https://arxiv.org/pdf/2309.15698v1.pdf"
    },
    {
        "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
        "authors": [
            "Linxin Song",
            "Jieyu Zhang",
            "Lechao Cheng",
            "Pengyuan Zhou",
            "Tianyi Zhou",
            "Irene Li"
        ],
        "published": "2023-09-27T13:02:06Z",
        "summary": "Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.",
        "pdf_link": "https://arxiv.org/pdf/2309.15630v4.pdf"
    },
    {
        "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
        "authors": [
            "Jung Hwan Heo",
            "Jeonghoon Kim",
            "Beomseok Kwon",
            "Byeongwook Kim",
            "Se Jung Kwon",
            "Dongsoo Lee"
        ],
        "published": "2023-09-27T09:48:31Z",
        "summary": "Large Language Models (LLMs) have recently demonstrated remarkable success\nacross various tasks. However, efficiently serving LLMs has been a challenge\ndue to the large memory bottleneck, specifically in small batch inference\nsettings (e.g. mobile devices). Weight-only quantization can be a promising\napproach, but sub-4 bit quantization remains a challenge due to large-magnitude\nactivation outliers. To mitigate the undesirable outlier effect, we first\npropose per-IC quantization, a simple yet effective method that creates\nquantization groups within each input channel (IC) rather than the conventional\nper-output-channel (per-OC). Our method is motivated by the observation that\nactivation outliers affect the input dimension of the weight matrix, so\nsimilarly grouping the weights in the IC direction can isolate outliers within\na group. We also find that activation outliers do not dictate quantization\ndifficulty, and inherent weight sensitivities also exist. With per-IC\nquantization as a new outlier-friendly scheme, we propose Adaptive Dimensions\n(AdaDim), a versatile quantization framework that can adapt to various weight\nsensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting\nprior methods such as Round-To-Nearest and GPTQ, showing significant\nimprovements across various language modeling benchmarks for both base (up to\n+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is\navailable at https://github.com/johnheo/adadim-llm",
        "pdf_link": "https://arxiv.org/pdf/2309.15531v2.pdf"
    },
    {
        "title": "Graph Neural Prompting with Large Language Models",
        "authors": [
            "Yijun Tian",
            "Huan Song",
            "Zichen Wang",
            "Haozhu Wang",
            "Ziqing Hu",
            "Fang Wang",
            "Nitesh V. Chawla",
            "Panpan Xu"
        ],
        "published": "2023-09-27T06:33:29Z",
        "summary": "Large language models (LLMs) have shown remarkable generalization capability\nwith exceptional performance in various language modeling tasks. However, they\nstill exhibit inherent limitations in precisely capturing and returning\ngrounded knowledge. While existing work has explored utilizing knowledge graphs\n(KGs) to enhance language modeling via joint training and customized model\narchitectures, applying this to LLMs is problematic owing to their large number\nof parameters and high computational cost. Therefore, how to enhance\npre-trained LLMs using grounded knowledge, e.g., retrieval-augmented\ngeneration, remains an open question. In this work, we propose Graph Neural\nPrompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in\nlearning beneficial knowledge from KGs. GNP encompasses various designs,\nincluding a standard graph neural network encoder, a cross-modality pooling\nmodule, a domain projector, and a self-supervised link prediction objective.\nExtensive experiments on multiple datasets demonstrate the superiority of GNP\non both commonsense and biomedical reasoning tasks across different LLM sizes\nand settings. Code is available at https://github.com/meettyj/GNP.",
        "pdf_link": "https://arxiv.org/pdf/2309.15427v2.pdf"
    },
    {
        "title": "Beyond the Chat: Executable and Verifiable Text-Editing with LLMs",
        "authors": [
            "Philippe Laban",
            "Jesse Vig",
            "Marti A. Hearst",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-09-27T00:56:17Z",
        "summary": "Conversational interfaces powered by Large Language Models (LLMs) have\nrecently become a popular way to obtain feedback during document editing.\nHowever, standard chat-based conversational interfaces do not support\ntransparency and verifiability of the editing changes that they suggest. To\ngive the author more agency when editing with an LLM, we present InkSync, an\nediting interface that suggests executable edits directly within the document\nbeing edited. Because LLMs are known to introduce factual errors, Inksync also\nsupports a 3-stage approach to mitigate this risk: Warn authors when a\nsuggested edit introduces new information, help authors Verify the new\ninformation's accuracy through external search, and allow an auditor to perform\nan a-posteriori verification by Auditing the document via a trace of all\nauto-generated content. Two usability studies confirm the effectiveness of\nInkSync's components when compared to standard LLM-based chat interfaces,\nleading to more accurate, more efficient editing, and improved user experience.",
        "pdf_link": "https://arxiv.org/pdf/2309.15337v1.pdf"
    },
    {
        "title": "Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI",
        "authors": [
            "Muhammad Aurangzeb Ahmad",
            "Ilker Yaramis",
            "Taposh Dutta Roy"
        ],
        "published": "2023-09-26T20:52:46Z",
        "summary": "Large language models have proliferated across multiple domains in as short\nperiod of time. There is however hesitation in the medical and healthcare\ndomain towards their adoption because of issues like factuality, coherence, and\nhallucinations. Give the high stakes nature of healthcare, many researchers\nhave even cautioned against its usage until these issues are resolved. The key\nto the implementation and deployment of LLMs in healthcare is to make these\nmodels trustworthy, transparent (as much possible) and explainable. In this\npaper we describe the key elements in creating reliable, trustworthy, and\nunbiased models as a necessary condition for their adoption in healthcare.\nSpecifically we focus on the quantification, validation, and mitigation of\nhallucinations in the context in healthcare. Lastly, we discuss how the future\nof LLMs in healthcare may look like.",
        "pdf_link": "https://arxiv.org/pdf/2311.01463v1.pdf"
    },
    {
        "title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
        "authors": [
            "Shahul Es",
            "Jithin James",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
        ],
        "published": "2023-09-26T19:23:54Z",
        "summary": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With RAGAs, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.15217v1.pdf"
    },
    {
        "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
        "authors": [
            "Mert Yuksekgonul",
            "Varun Chandrasekaran",
            "Erik Jones",
            "Suriya Gunasekar",
            "Ranjita Naik",
            "Hamid Palangi",
            "Ece Kamar",
            "Besmira Nushi"
        ],
        "published": "2023-09-26T17:48:55Z",
        "summary": "We investigate the internal behavior of Transformer-based Large Language\nModels (LLMs) when they generate factually incorrect text. We propose modeling\nfactual queries as Constraint Satisfaction Problems and use this framework to\ninvestigate how the model interacts internally with factual constraints.\nSpecifically, we discover a strong positive relation between the model's\nattention to constraint tokens and the factual accuracy of its responses. In\nour curated suite of 11 datasets with over 40,000 prompts, we study the task of\npredicting factual errors with the Llama-2 family across all scales (7B, 13B,\n70B). We propose SAT Probe, a method probing self-attention patterns, that can\npredict constraint satisfaction and factual errors, and allows early error\nidentification. The approach and findings demonstrate how using the mechanistic\nunderstanding of factuality in LLMs can enhance reliability.",
        "pdf_link": "https://arxiv.org/pdf/2309.15098v1.pdf"
    },
    {
        "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
        "authors": [
            "Lorenzo Pacchiardi",
            "Alex J. Chan",
            "S√∂ren Mindermann",
            "Ilan Moscovitz",
            "Alexa Y. Pan",
            "Yarin Gal",
            "Owain Evans",
            "Jan Brauner"
        ],
        "published": "2023-09-26T16:07:54Z",
        "summary": "Large language models (LLMs) can \"lie\", which we define as outputting false\nstatements despite \"knowing\" the truth in a demonstrable sense. LLMs might\n\"lie\", for example, when instructed to output misinformation. Here, we develop\na simple lie detector that requires neither access to the LLM's activations\n(black-box) nor ground-truth knowledge of the fact in question. The detector\nworks by asking a predefined set of unrelated follow-up questions after a\nsuspected lie, and feeding the LLM's yes/no answers into a logistic regression\nclassifier. Despite its simplicity, this lie detector is highly accurate and\nsurprisingly general. When trained on examples from a single setting --\nprompting GPT-3.5 to lie about factual questions -- the detector generalises\nout-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,\n(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as\nsales. These results indicate that LLMs have distinctive lie-related\nbehavioural patterns, consistent across architectures and contexts, which could\nenable general-purpose lie detection.",
        "pdf_link": "https://arxiv.org/pdf/2309.15840v1.pdf"
    },
    {
        "title": "Large Language Model Alignment: A Survey",
        "authors": [
            "Tianhao Shen",
            "Renren Jin",
            "Yufei Huang",
            "Chuang Liu",
            "Weilong Dong",
            "Zishan Guo",
            "Xinwei Wu",
            "Yan Liu",
            "Deyi Xiong"
        ],
        "published": "2023-09-26T15:49:23Z",
        "summary": "Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.15025v1.pdf"
    },
    {
        "title": "ConPET: Continual Parameter-Efficient Tuning for Large Language Models",
        "authors": [
            "Chenyang Song",
            "Xu Han",
            "Zheni Zeng",
            "Kuai Li",
            "Chen Chen",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Tao Yang"
        ],
        "published": "2023-09-26T08:52:04Z",
        "summary": "Continual learning necessitates the continual adaptation of models to newly\nemerging tasks while minimizing the catastrophic forgetting of old ones. This\nis extremely challenging for large language models (LLMs) with vanilla\nfull-parameter tuning due to high computation costs, memory consumption, and\nforgetting issue. Inspired by the success of parameter-efficient tuning (PET),\nwe propose Continual Parameter-Efficient Tuning (ConPET), a generalizable\nparadigm for continual task adaptation of LLMs with task-number-independent\ntraining complexity. ConPET includes two versions with different application\nscenarios. First, Static ConPET can adapt former continual learning methods\noriginally designed for relatively smaller models to LLMs through PET and a\ndynamic replay strategy, which largely reduces the tuning costs and alleviates\nthe over-fitting and forgetting issue. Furthermore, to maintain scalability,\nDynamic ConPET adopts separate PET modules for different tasks and a PET module\nselector for dynamic optimal selection. In our extensive experiments, the\nadaptation of Static ConPET helps multiple former methods reduce the scale of\ntunable parameters by over 3,000 times and surpass the PET-only baseline by at\nleast 5 points on five smaller benchmarks, while Dynamic ConPET gains its\nadvantage on the largest dataset. The codes and datasets are available at\nhttps://github.com/Raincleared-Song/ConPET.",
        "pdf_link": "https://arxiv.org/pdf/2309.14763v1.pdf"
    },
    {
        "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
        "authors": [
            "Yuhui Xu",
            "Lingxi Xie",
            "Xiaotao Gu",
            "Xin Chen",
            "Heng Chang",
            "Hengheng Zhang",
            "Zhengsu Chen",
            "Xiaopeng Zhang",
            "Qi Tian"
        ],
        "published": "2023-09-26T07:22:23Z",
        "summary": "Recently years have witnessed a rapid development of large language models\n(LLMs). Despite the strong ability in many language-understanding tasks, the\nheavy computational burden largely restricts the application of LLMs especially\nwhen one needs to deploy them onto edge devices. In this paper, we propose a\nquantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies\nin the imbalanced degrees of freedom of quantization and adaptation, and the\nsolution is to use group-wise operators which increase the degree of freedom of\nquantization meanwhile decreasing that of adaptation. QA-LoRA is easily\nimplemented with a few lines of code, and it equips the original LoRA with\ntwo-fold abilities: (i) during fine-tuning, the LLM's weights are quantized\n(e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the\nLLM and auxiliary weights are naturally integrated into a quantized model\nwithout loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model\nfamilies and validate its effectiveness in different fine-tuning datasets and\ndownstream scenarios. Code will be made available at\nhttps://github.com/yuhuixu1993/qa-lora.",
        "pdf_link": "https://arxiv.org/pdf/2309.14717v2.pdf"
    },
    {
        "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
        "authors": [
            "Rui Li",
            "Guoyin Wang",
            "Jiwei Li"
        ],
        "published": "2023-09-26T05:10:08Z",
        "summary": "Despite the promising few-shot ability of large language models (LLMs), the\nstandard paradigm of In-context Learning (ICL) suffers the disadvantages of\nsusceptibility to selected demonstrations and the intricacy to generate these\ndemonstrations. In this paper, we raise the fundamental question that whether\nhuman-generated demonstrations are necessary for ICL. To answer this question,\nwe propose self-contemplation prompting strategy (SEC), a paradigm free from\nhuman-crafted demonstrations. The key point of SEC is that, instead of using\nhand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create\ndemonstrations on their own, based on which the final output is generated. SEC\nis a flexible framework and can be adapted to both the vanilla ICL and the\nchain-of-thought (CoT), but with greater ease: as the manual-generation process\nof both examples and rationale can be saved. Extensive experiments in\narithmetic reasoning, commonsense reasoning, multi-task language understanding,\nand code generation benchmarks, show that SEC, which does not require\nhand-crafted demonstrations, significantly outperforms the zero-shot learning\nstrategy, and achieves comparable results to ICL with hand-crafted\ndemonstrations. This demonstrates that, for many tasks, contemporary LLMs\npossess a sufficient level of competence to exclusively depend on their own\ncapacity for decision making, removing the need for external training data.\nCode is available at https://github.com/ruili33/SEC.",
        "pdf_link": "https://arxiv.org/pdf/2309.14681v4.pdf"
    },
    {
        "title": "Disinformation Detection: An Evolving Challenge in the Age of LLMs",
        "authors": [
            "Bohan Jiang",
            "Zhen Tan",
            "Ayushi Nirmal",
            "Huan Liu"
        ],
        "published": "2023-09-25T22:12:50Z",
        "summary": "The advent of generative Large Language Models (LLMs) such as ChatGPT has\ncatalyzed transformative advancements across multiple domains. However,\nalongside these advancements, they have also introduced potential threats. One\ncritical concern is the misuse of LLMs by disinformation spreaders, leveraging\nthese models to generate highly persuasive yet misleading content that\nchallenges the disinformation detection system. This work aims to address this\nissue by answering three research questions: (1) To what extent can the current\ndisinformation detection technique reliably detect LLM-generated\ndisinformation? (2) If traditional techniques prove less effective, can LLMs\nthemself be exploited to serve as a robust defense against advanced\ndisinformation? and, (3) Should both these strategies falter, what novel\napproaches can be proposed to counter this burgeoning threat effectively? A\nholistic exploration for the formation and detection of disinformation is\nconducted to foster this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2309.15847v1.pdf"
    },
    {
        "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
        "authors": [
            "Hanzhuo Huang",
            "Yufan Feng",
            "Cheng Shi",
            "Lan Xu",
            "Jingyi Yu",
            "Sibei Yang"
        ],
        "published": "2023-09-25T19:42:16Z",
        "summary": "Text-to-video is a rapidly growing research area that aims to generate a\nsemantic, identical, and temporal coherence sequence of frames that accurately\nalign with the input text prompt. This study focuses on zero-shot text-to-video\ngeneration considering the data- and cost-efficient. To generate a\nsemantic-coherent video, exhibiting a rich portrayal of temporal semantics such\nas the whole process of flower blooming rather than a set of \"moving images\",\nwe propose a novel Free-Bloom pipeline that harnesses large language models\n(LLMs) as the director to generate a semantic-coherence prompt sequence, while\npre-trained latent diffusion models (LDMs) as the animator to generate the high\nfidelity frames. Furthermore, to ensure temporal and identical coherence while\nmaintaining semantic coherence, we propose a series of annotative modifications\nto adapting LDMs in the reverse process, including joint noise sampling,\nstep-aware attention shift, and dual-path interpolation. Without any video data\nand training requirements, Free-Bloom generates vivid and high-quality videos,\nawe-inspiring in generating complex scenes with semantic meaningful frame\nsequences. In addition, Free-Bloom is naturally compatible with LDMs-based\nextensions.",
        "pdf_link": "https://arxiv.org/pdf/2309.14494v1.pdf"
    },
    {
        "title": "Lifelong Robot Learning with Human Assisted Language Planners",
        "authors": [
            "Meenal Parakh",
            "Alisha Fong",
            "Anthony Simeonov",
            "Tao Chen",
            "Abhishek Gupta",
            "Pulkit Agrawal"
        ],
        "published": "2023-09-25T17:45:55Z",
        "summary": "Large Language Models (LLMs) have been shown to act like planners that can\ndecompose high-level instructions into a sequence of executable instructions.\nHowever, current LLM-based planners are only able to operate with a fixed set\nof skills. We overcome this critical limitation and present a method for using\nLLM-based planners to query new skills and teach robots these skills in a data\nand time-efficient manner for rigid object manipulation. Our system can re-use\nnewly acquired skills for future tasks, demonstrating the potential of open\nworld and lifelong learning. We evaluate the proposed framework on multiple\ntasks in simulation and the real world. Videos are available at:\nhttps://sites.google.com/mit.edu/halp-robot-learning.",
        "pdf_link": "https://arxiv.org/pdf/2309.14321v2.pdf"
    },
    {
        "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
        "authors": [
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li"
        ],
        "published": "2023-09-25T17:37:20Z",
        "summary": "Large language models (LLMs) can store a vast amount of world knowledge,\noften extractable via question-answering (e.g., \"What is Abraham Lincoln's\nbirthday?\"). However, do they answer such questions based on exposure to\nsimilar questions during training (i.e., cheating), or by genuinely learning to\nextract knowledge from sources like Wikipedia?\n  In this paper, we investigate this issue using a controlled biography\ndataset. We find a strong correlation between the model's ability to extract\nknowledge and various diversity measures of the training data.\n$\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be\nsufficiently augmented (e.g., through paraphrasing, sentence shuffling)\n$\\textit{during pretraining}$. Without such augmentation, knowledge may be\nmemorized but not extractable, leading to 0% accuracy, regardless of subsequent\ninstruction fine-tuning.\n  To understand why this occurs, we employ (nearly) linear probing to\ndemonstrate a strong connection between the observed correlation and how the\nmodel internally encodes knowledge -- whether it is linearly encoded in the\nhidden embeddings of entity names or distributed across other token embeddings\nin the training text.\n  This paper provides $\\textbf{several key recommendations for LLM pretraining\nin the industry}$: (1) rewrite the pretraining data -- using small, auxiliary\nmodels -- to provide knowledge augmentation, and (2) incorporate more\ninstruction-finetuning data into the pretraining stage before it becomes too\nlate.",
        "pdf_link": "https://arxiv.org/pdf/2309.14316v2.pdf"
    },
    {
        "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
        "authors": [
            "Yangjun Ruan",
            "Honghua Dong",
            "Andrew Wang",
            "Silviu Pitis",
            "Yongchao Zhou",
            "Jimmy Ba",
            "Yann Dubois",
            "Chris J. Maddison",
            "Tatsunori Hashimoto"
        ],
        "published": "2023-09-25T17:08:02Z",
        "summary": "Recent advances in Language Model (LM) agents and tool use, exemplified by\napplications like ChatGPT Plugins, enable a rich set of capabilities but also\namplify potential risks - such as leaking private data or causing financial\nlosses. Identifying these risks is labor-intensive, necessitating implementing\nthe tools, manually setting up the environment for each test scenario, and\nfinding risky cases. As tools and agents become more complex, the high cost of\ntesting these agents will make it increasingly difficult to find high-stakes,\nlong-tailed risks. To address these challenges, we introduce ToolEmu: a\nframework that uses an LM to emulate tool execution and enables the testing of\nLM agents against a diverse range of tools and scenarios, without manual\ninstantiation. Alongside the emulator, we develop an LM-based automatic safety\nevaluator that examines agent failures and quantifies associated risks. We test\nboth the tool emulator and evaluator through human evaluation and find that\n68.8% of failures identified with ToolEmu would be valid real-world agent\nfailures. Using our curated initial benchmark consisting of 36 high-stakes\ntools and 144 test cases, we provide a quantitative risk analysis of current LM\nagents and identify numerous failures with potentially severe outcomes.\nNotably, even the safest LM agent exhibits such failures 23.9% of the time\naccording to our evaluator, underscoring the need to develop safer LM agents\nfor real-world deployment.",
        "pdf_link": "https://arxiv.org/pdf/2309.15817v1.pdf"
    },
    {
        "title": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models",
        "authors": [
            "Ahmad Faiz",
            "Sotaro Kaneda",
            "Ruhan Wang",
            "Rita Osi",
            "Prateek Sharma",
            "Fan Chen",
            "Lei Jiang"
        ],
        "published": "2023-09-25T14:50:04Z",
        "summary": "The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{\\carb}, an end-to-end carbon footprint projection model designed for\nboth dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the\naccuracy of carbon footprint estimations for various LLMs. The source code is\nreleased at \\url{https://github.com/SotaroKaneda/MLCarbon}.",
        "pdf_link": "https://arxiv.org/pdf/2309.14393v2.pdf"
    },
    {
        "title": "The Cybersecurity Crisis of Artificial Intelligence: Unrestrained Adoption and Natural Language-Based Attacks",
        "authors": [
            "Andreas Tsamados",
            "Luciano Floridi",
            "Mariarosaria Taddeo"
        ],
        "published": "2023-09-25T10:48:46Z",
        "summary": "The widespread integration of autoregressive-large language models (AR-LLMs),\nsuch as ChatGPT, across established applications, like search engines, has\nintroduced critical vulnerabilities with uniquely scalable characteristics. In\nthis commentary, we analyse these vulnerabilities, their dependence on natural\nlanguage as a vector of attack, and their challenges to cybersecurity best\npractices. We offer recommendations designed to mitigate these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2311.09224v1.pdf"
    },
    {
        "title": "Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering",
        "authors": [
            "Nidhi Hegde",
            "Sujoy Paul",
            "Gagan Madan",
            "Gaurav Aggarwal"
        ],
        "published": "2023-09-25T07:01:16Z",
        "summary": "Recent document question answering models consist of two key components: the\nvision encoder, which captures layout and visual elements in images, and a\nLarge Language Model (LLM) that helps contextualize questions to the image and\nsupplements them with external world knowledge to generate accurate answers.\nHowever, the relative contributions of the vision encoder and the language\nmodel in these tasks remain unclear. This is especially interesting given the\neffectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability\nto new tasks. To this end, we explore the following aspects in this work: (1)\nThe efficacy of an LLM-only approach on document question answering tasks (2)\nstrategies for serializing textual information within document images and\nfeeding it directly to an instruction-tuned LLM, thus bypassing the need for an\nexplicit vision encoder (3) thorough quantitative analysis on the feasibility\nof such an approach. Our comprehensive analysis encompasses six diverse\nbenchmark datasets, utilizing LLMs of varying scales. Our findings reveal that\na strategy exclusively reliant on the LLM yields results that are on par with\nor closely approach state-of-the-art performance across a range of datasets. We\nposit that this evaluation framework will serve as a guiding resource for\nselecting appropriate datasets for future research endeavors that emphasize the\nfundamental importance of layout and image content information.",
        "pdf_link": "https://arxiv.org/pdf/2309.14389v1.pdf"
    },
    {
        "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
        "authors": [
            "Ida Momennejad",
            "Hosein Hasanbeig",
            "Felipe Vieira",
            "Hiteshi Sharma",
            "Robert Osazuwa Ness",
            "Nebojsa Jojic",
            "Hamid Palangi",
            "Jonathan Larson"
        ],
        "published": "2023-09-25T01:20:13Z",
        "summary": "Recently an influx of studies claim emergent cognitive abilities in large\nlanguage models (LLMs). Yet, most rely on anecdotes, overlook contamination of\ntraining sets, or lack systematic Evaluation involving multiple tasks, control\nconditions, multiple iterations, and statistical robustness tests. Here we make\ntwo major contributions. First, we propose CogEval, a cognitive\nscience-inspired protocol for the systematic evaluation of cognitive capacities\nin Large Language Models. The CogEval protocol can be followed for the\nevaluation of various abilities. Second, here we follow CogEval to\nsystematically evaluate cognitive maps and planning ability across eight LLMs\n(OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard,\nCohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base\nour task prompts on human experiments, which offer both established construct\nvalidity for evaluating planning, and are absent from LLM training sets. We\nfind that, while LLMs show apparent competence in a few planning tasks with\nsimpler structures, systematic evaluation reveals striking failure modes in\nplanning tasks, including hallucinations of invalid trajectories and getting\ntrapped in loops. These findings do not support the idea of emergent\nout-of-the-box planning ability in LLMs. This could be because LLMs do not\nunderstand the latent relational structures underlying planning problems, known\nas cognitive maps, and fail at unrolling goal-directed trajectories based on\nthe underlying structure. Implications for application and future directions\nare discussed.",
        "pdf_link": "https://arxiv.org/pdf/2309.15129v1.pdf"
    },
    {
        "title": "Can LLM-Generated Misinformation Be Detected?",
        "authors": [
            "Canyu Chen",
            "Kai Shu"
        ],
        "published": "2023-09-25T00:45:07Z",
        "summary": "The advent of Large Language Models (LLMs) has made a transformative impact.\nHowever, the potential that LLMs such as ChatGPT can be exploited to generate\nmisinformation has posed a serious concern to online safety and public trust. A\nfundamental research question is: will LLM-generated misinformation cause more\nharm than human-written misinformation? We propose to tackle this question from\nthe perspective of detection difficulty. We first build a taxonomy of\nLLM-generated misinformation. Then we categorize and validate the potential\nreal-world methods for generating misinformation with LLMs. Then, through\nextensive empirical investigation, we discover that LLM-generated\nmisinformation can be harder to detect for humans and detectors compared to\nhuman-written misinformation with the same semantics, which suggests it can\nhave more deceptive styles and potentially cause more harm. We also discuss the\nimplications of our discovery on combating misinformation in the age of LLMs\nand the countermeasures.",
        "pdf_link": "https://arxiv.org/pdf/2309.13788v3.pdf"
    },
    {
        "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
        "authors": [
            "Hosein Hasanbeig",
            "Hiteshi Sharma",
            "Leo Betthauser",
            "Felipe Vieira Frujeri",
            "Ida Momennejad"
        ],
        "published": "2023-09-24T17:15:58Z",
        "summary": "From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.",
        "pdf_link": "https://arxiv.org/pdf/2309.13701v2.pdf"
    },
    {
        "title": "Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve",
        "authors": [
            "R. Thomas McCoy",
            "Shunyu Yao",
            "Dan Friedman",
            "Matthew Hardy",
            "Thomas L. Griffiths"
        ],
        "published": "2023-09-24T13:35:28Z",
        "summary": "The widespread adoption of large language models (LLMs) makes it important to\nrecognize their strengths and limitations. We argue that in order to develop a\nholistic understanding of these systems we need to consider the problem that\nthey were trained to solve: next-word prediction over Internet text. By\nrecognizing the pressures that this task exerts we can make predictions about\nthe strategies that LLMs will adopt, allowing us to reason about when they will\nsucceed or fail. This approach - which we call the teleological approach -\nleads us to identify three factors that we hypothesize will influence LLM\naccuracy: the probability of the task to be performed, the probability of the\ntarget output, and the probability of the provided input. We predict that LLMs\nwill achieve higher accuracy when these probabilities are high than when they\nare low - even in deterministic settings where probability should not matter.\nTo test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven\ntasks, and we find robust evidence that LLMs are influenced by probability in\nthe ways that we have hypothesized. In many cases, the experiments reveal\nsurprising failure modes. For instance, GPT-4's accuracy at decoding a simple\ncipher is 51% when the output is a high-probability word sequence but only 13%\nwhen it is low-probability. These results show that AI practitioners should be\ncareful about using LLMs in low-probability situations. More broadly, we\nconclude that we should not evaluate LLMs as if they are humans but should\ninstead treat them as a distinct type of system - one that has been shaped by\nits own particular set of pressures.",
        "pdf_link": "https://arxiv.org/pdf/2309.13638v1.pdf"
    },
    {
        "title": "Resolving References in Visually-Grounded Dialogue via Text Generation",
        "authors": [
            "Bram Willemsen",
            "Livia Qian",
            "Gabriel Skantze"
        ],
        "published": "2023-09-23T17:07:54Z",
        "summary": "Vision-language models (VLMs) have shown to be effective at image retrieval\nbased on simple text queries, but text-image retrieval based on conversational\ninput remains a challenge. Consequently, if we want to use VLMs for reference\nresolution in visually-grounded dialogue, the discourse processing capabilities\nof these models need to be augmented. To address this issue, we propose\nfine-tuning a causal large language model (LLM) to generate definite\ndescriptions that summarize coreferential information found in the linguistic\ncontext of references. We then use a pretrained VLM to identify referents based\non the generated descriptions, zero-shot. We evaluate our approach on a\nmanually annotated dataset of visually-grounded dialogues and achieve results\nthat, on average, exceed the performance of the baselines we compare against.\nFurthermore, we find that using referent descriptions based on larger context\nwindows has the potential to yield higher returns.",
        "pdf_link": "https://arxiv.org/pdf/2309.13430v1.pdf"
    },
    {
        "title": "A Chat About Boring Problems: Studying GPT-based text normalization",
        "authors": [
            "Yang Zhang",
            "Travis M. Bartley",
            "Mariana Graterol-Fuenmayor",
            "Vitaly Lavrukhin",
            "Evelina Bakhturina",
            "Boris Ginsburg"
        ],
        "published": "2023-09-23T16:32:59Z",
        "summary": "Text normalization - the conversion of text from written to spoken form - is\ntraditionally assumed to be an ill-formed task for language models. In this\nwork, we argue otherwise. We empirically show the capacity of Large-Language\nModels (LLM) for text normalization in few-shot scenarios. Combining\nself-consistency reasoning with linguistic-informed prompt engineering, we find\nLLM based text normalization to achieve error rates around 40\\% lower than top\nnormalization systems. Further, upon error analysis, we note key limitations in\nthe conventional design of text normalization tasks. We create a new taxonomy\nof text normalization errors and apply it to results from GPT-3.5-Turbo and\nGPT-4.0. Through this new framework, we can identify strengths and weaknesses\nof GPT-based TN, opening opportunities for future work.",
        "pdf_link": "https://arxiv.org/pdf/2309.13426v2.pdf"
    },
    {
        "title": "Towards LLM-guided Causal Explainability for Black-box Text Classifiers",
        "authors": [
            "Amrita Bhattacharjee",
            "Raha Moraffah",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2023-09-23T11:22:28Z",
        "summary": "With the advent of larger and more complex deep learning models, such as in\nNatural Language Processing (NLP), model qualities like explainability and\ninterpretability, albeit highly desirable, are becoming harder challenges to\ntackle and solve. For example, state-of-the-art models in text classification\nare black-box by design. Although standard explanation methods provide some\ndegree of explainability, these are mostly correlation-based methods and do not\nprovide much insight into the model. The alternative of causal explainability\nis more desirable to achieve but extremely challenging in NLP due to a variety\nof reasons. Inspired by recent endeavors to utilize Large Language Models\n(LLMs) as experts, in this work, we aim to leverage the instruction-following\nand textual understanding capabilities of recent state-of-the-art LLMs to\nfacilitate causal explainability via counterfactual explanation generation for\nblack-box text classifiers. To do this, we propose a three-step pipeline via\nwhich, we use an off-the-shelf LLM to: (1) identify the latent or unobserved\nfeatures in the input text, (2) identify the input features associated with the\nlatent features, and finally (3) use the identified input features to generate\na counterfactual explanation. We experiment with our pipeline on multiple NLP\ntext classification datasets, with several recent LLMs, and present interesting\nand promising findings.",
        "pdf_link": "https://arxiv.org/pdf/2309.13340v2.pdf"
    },
    {
        "title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
        "authors": [
            "Xufeng Zhao",
            "Mengdi Li",
            "Wenhao Lu",
            "Cornelius Weber",
            "Jae Hee Lee",
            "Kun Chu",
            "Stefan Wermter"
        ],
        "published": "2023-09-23T11:21:12Z",
        "summary": "Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.",
        "pdf_link": "https://arxiv.org/pdf/2309.13339v4.pdf"
    },
    {
        "title": "AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling",
        "authors": [
            "Pivithuru Thejan Amarasinghe",
            "Su Nguyen",
            "Yuan Sun",
            "Damminda Alahakoon"
        ],
        "published": "2023-09-22T23:45:21Z",
        "summary": "Business optimisation refers to the process of finding and implementing\nefficient and cost-effective means of operation to bring a competitive\nadvantage for businesses. Synthesizing problem formulations is an integral part\nof business optimisation, which relies on human expertise to construct problem\nformulations using optimisation languages. Interestingly, with advancements in\nLarge Language Models (LLMs), the human expertise needed in problem formulation\ncan be minimized. However, developing an LLM for problem formulation is\nchallenging, due to training data, token limitations, and lack of appropriate\nperformance metrics. For the requirement of training data, recent attention has\nbeen directed towards fine-tuning pre-trained LLMs for downstream tasks rather\nthan training an LLM from scratch for a specific task. In this paper, we adopt\nan LLM fine-tuning approach and propose an AI-Copilot for business optimisation\nproblem formulation. For token limitations, we introduce modularization and\nprompt engineering techniques to synthesize complex problem formulations as\nmodules that fit into the token limits of LLMs. Additionally, we design\nperformance evaluation metrics that are better suited for assessing the\naccuracy and quality of problem formulations. The experiment results\ndemonstrate that with this approach we can synthesize complex and large problem\nformulations for a typical business optimisation problem in production\nscheduling.",
        "pdf_link": "https://arxiv.org/pdf/2309.13218v3.pdf"
    },
    {
        "title": "Investigating Large Language Models and Control Mechanisms to Improve Text Readability of Biomedical Abstracts",
        "authors": [
            "Zihao Li",
            "Samuel Belkadi",
            "Nicolo Micheletti",
            "Lifeng Han",
            "Matthew Shardlow",
            "Goran Nenadic"
        ],
        "published": "2023-09-22T22:47:32Z",
        "summary": "Biomedical literature often uses complex language and inaccessible\nprofessional terminologies. That is why simplification plays an important role\nin improving public health literacy. Applying Natural Language Processing (NLP)\nmodels to automate such tasks allows for quick and direct accessibility for lay\nreaders. In this work, we investigate the ability of state-of-the-art large\nlanguage models (LLMs) on the task of biomedical abstract simplification, using\nthe publicly available dataset for plain language adaptation of biomedical\nabstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and\nprompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and\nBART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT,\nand 3) Control-token mechanisms on BART-based models. We used a range of\nautomatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and\nalso conducted human evaluations. BART-Large with Control Token (BART-L-w-CT)\nmechanisms reported the highest SARI score of 46.54 and T5-base reported the\nhighest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better\nsimplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better\nmeaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised\nthe system outputs with examples, hoping this will shed some light for future\nresearch on this task. Our code, fine-tuned models, and data splits are\navailable at \\url{https://github.com/HECTA-UoM/PLABA-MU} \\begin{IEEEkeywords}\nLarge Language Models, Text Simplification, Biomedical NLP, Control Mechanisms,\nHealth Informatics \\end{IEEEkeywords}",
        "pdf_link": "https://arxiv.org/pdf/2309.13202v2.pdf"
    },
    {
        "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
        "authors": [
            "Kai Huang",
            "Hanyun Yin",
            "Heng Huang",
            "Wei Gao"
        ],
        "published": "2023-09-22T21:55:18Z",
        "summary": "Fine-tuning is the most effective way of adapting pre-trained large language\nmodels (LLMs) to downstream applications. With the fast growth of LLM-enabled\nAI applications and democratization of open-souced LLMs, fine-tuning has become\npossible for non-expert individuals, but intensively performed LLM fine-tuning\nworldwide could result in significantly high energy consumption and carbon\nfootprint, which may bring large environmental impact. Mitigating such\nenvironmental impact towards Green AI directly correlates to reducing the FLOPs\nof fine-tuning, but existing techniques on efficient LLM fine-tuning can only\nachieve limited reduction of such FLOPs, due to their ignorance of the\nbackpropagation cost in fine-tuning. To address this limitation, in this paper\nwe present GreenTrainer, a new LLM fine-tuning technique that adaptively\nevaluates different tensors' backpropagation costs and contributions to the\nfine-tuned model accuracy, to minimize the fine-tuning cost by selecting the\nmost appropriate set of tensors in training. Such selection in GreenTrainer is\nmade based on a given objective of FLOPs reduction, which can flexibly adapt to\nthe carbon footprint in energy supply and the need in Green AI. Experiment\nresults over multiple open-sourced LLM models and abstractive summarization\ndatasets show that, compared to fine-tuning the whole LLM model, GreenTrainer\ncan save up to 64% FLOPs in fine-tuning without any noticeable model accuracy\nloss. Compared to the existing fine-tuning techniques such as LoRa,\nGreenTrainer can achieve up to 4% improvement on model accuracy with on-par\nFLOPs reduction.",
        "pdf_link": "https://arxiv.org/pdf/2309.13192v2.pdf"
    },
    {
        "title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP",
        "authors": [
            "Mohsinul Kabir",
            "Mohammed Saidul Islam",
            "Md Tahmid Rahman Laskar",
            "Mir Tafseer Nayeem",
            "M Saiful Bari",
            "Enamul Hoque"
        ],
        "published": "2023-09-22T20:29:34Z",
        "summary": "Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.",
        "pdf_link": "https://arxiv.org/pdf/2309.13173v2.pdf"
    },
    {
        "title": "Contextual Emotion Estimation from Image Captions",
        "authors": [
            "Vera Yang",
            "Archita Srivastava",
            "Yasaman Etesam",
            "Chuxuan Zhang",
            "Angelica Lim"
        ],
        "published": "2023-09-22T18:44:34Z",
        "summary": "Emotion estimation in images is a challenging task, typically using computer\nvision methods to directly estimate people's emotions using face, body pose and\ncontextual cues. In this paper, we explore whether Large Language Models (LLMs)\ncan support the contextual emotion estimation task, by first captioning images,\nthen using an LLM for inference. First, we must understand: how well do LLMs\nperceive human emotions? And which parts of the information enable them to\ndetermine emotions? One initial challenge is to construct a caption that\ndescribes a person within a scene with information relevant for emotion\nperception. Towards this goal, we propose a set of natural language descriptors\nfor faces, bodies, interactions, and environments. We use them to manually\ngenerate captions and emotion annotations for a subset of 331 images from the\nEMOTIC dataset. These captions offer an interpretable representation for\nemotion estimation, towards understanding how elements of a scene affect\nemotion perception in LLMs and beyond. Secondly, we test the capability of a\nlarge language model to infer an emotion from the resulting image captions. We\nfind that GPT-3.5, specifically the text-davinci-003 model, provides\nsurprisingly reasonable emotion predictions consistent with human annotations,\nbut accuracy can depend on the emotion concept. Overall, the results suggest\npromise in the image captioning and LLM approach.",
        "pdf_link": "https://arxiv.org/pdf/2309.13136v1.pdf"
    },
    {
        "title": "In-context Interference in Chat-based Large Language Models",
        "authors": [
            "Eric Nuertey Coleman",
            "Julio Hurtado",
            "Vincenzo Lomonaco"
        ],
        "published": "2023-09-22T09:18:55Z",
        "summary": "Large language models (LLMs) have had a huge impact on society due to their\nimpressive capabilities and vast knowledge of the world. Various applications\nand tools have been created that allow users to interact with these models in a\nblack-box scenario. However, one limitation of this scenario is that users\ncannot modify the internal knowledge of the model, and the only way to add or\nmodify internal knowledge is by explicitly mentioning it to the model during\nthe current interaction. This learning process is called in-context training,\nand it refers to training that is confined to the user's current session or\ncontext. In-context learning has significant applications, but also has\nlimitations that are seldom studied. In this paper, we present a study that\nshows how the model can suffer from interference between information that\ncontinually flows in the context, causing it to forget previously learned\nknowledge, which can reduce the model's performance. Along with showing the\nproblem, we propose an evaluation benchmark based on the bAbI dataset.",
        "pdf_link": "https://arxiv.org/pdf/2309.12727v1.pdf"
    },
    {
        "title": "Construction contract risk identification based on knowledge-augmented language model",
        "authors": [
            "Saika Wong",
            "Chunmo Zheng",
            "Xing Su",
            "Yinqiu Tang"
        ],
        "published": "2023-09-22T05:27:06Z",
        "summary": "Contract review is an essential step in construction projects to prevent\npotential losses. However, the current methods for reviewing construction\ncontracts lack effectiveness and reliability, leading to time-consuming and\nerror-prone processes. While large language models (LLMs) have shown promise in\nrevolutionizing natural language processing (NLP) tasks, they struggle with\ndomain-specific knowledge and addressing specialized issues. This paper\npresents a novel approach that leverages LLMs with construction contract\nknowledge to emulate the process of contract review by human experts. Our\ntuning-free approach incorporates construction contract domain knowledge to\nenhance language models for identifying construction contract risks. The use of\na natural language when building the domain knowledge base facilitates\npractical implementation. We evaluated our method on real construction\ncontracts and achieved solid performance. Additionally, we investigated how\nlarge language models employ logical thinking during the task and provide\ninsights and recommendations for future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.12626v1.pdf"
    },
    {
        "title": "Studying and improving reasoning in humans and machines",
        "authors": [
            "Nicolas Yax",
            "Hernan Anll√≥",
            "Stefano Palminteri"
        ],
        "published": "2023-09-21T21:02:05Z",
        "summary": "In the present study, we investigate and compare reasoning in large language\nmodels (LLM) and humans using a selection of cognitive psychology tools\ntraditionally dedicated to the study of (bounded) rationality. To do so, we\npresented to human participants and an array of pretrained LLMs new variants of\nclassical cognitive experiments, and cross-compared their performances. Our\nresults showed that most of the included models presented reasoning errors akin\nto those frequently ascribed to error-prone, heuristic-based human reasoning.\nNotwithstanding this superficial similarity, an in-depth comparison between\nhumans and LLMs indicated important differences with human-like reasoning, with\nmodels limitations disappearing almost entirely in more recent LLMs releases.\nMoreover, we show that while it is possible to devise strategies to induce\nbetter performance, humans and machines are not equally-responsive to the same\nprompting schemes. We conclude by discussing the epistemological implications\nand challenges of comparing human and machine behavior for both artificial\nintelligence and cognitive psychology.",
        "pdf_link": "https://arxiv.org/pdf/2309.12485v1.pdf"
    },
    {
        "title": "Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI",
        "authors": [
            "Mahyar Abbasian",
            "Elahe Khatibi",
            "Iman Azimi",
            "David Oniani",
            "Zahra Shakeri Hossein Abad",
            "Alexander Thieme",
            "Ram Sriram",
            "Zhongqi Yang",
            "Yanshan Wang",
            "Bryant Lin",
            "Olivier Gevaert",
            "Li-Jia Li",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2023-09-21T19:36:48Z",
        "summary": "Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients' well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.",
        "pdf_link": "https://arxiv.org/pdf/2309.12444v3.pdf"
    },
    {
        "title": "Can LLMs Augment Low-Resource Reading Comprehension Datasets? Opportunities and Challenges",
        "authors": [
            "Vinay Samuel",
            "Houda Aynaou",
            "Arijit Ghosh Chowdhury",
            "Karthik Venkat Ramanan",
            "Aman Chadha"
        ],
        "published": "2023-09-21T18:48:02Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive zero shot\nperformance on a wide range of NLP tasks, demonstrating the ability to reason\nand apply commonsense. A relevant application is to use them for creating high\nquality synthetic datasets for downstream tasks. In this work, we probe whether\nGPT-4 can be used to augment existing extractive reading comprehension\ndatasets. Automating data annotation processes has the potential to save large\namounts of time, money and effort that goes into manually labelling datasets.\nIn this paper, we evaluate the performance of GPT-4 as a replacement for human\nannotators for low resource reading comprehension tasks, by comparing\nperformance after fine tuning, and the cost associated with annotation. This\nwork serves to be the first analysis of LLMs as synthetic data augmenters for\nQA systems, highlighting the unique opportunities and challenges. Additionally,\nwe release augmented versions of low resource datasets, that will allow the\nresearch community to create further benchmarks for evaluation of generated\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.12426v1.pdf"
    },
    {
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
        "authors": [
            "Jianing Yang",
            "Xuweiyi Chen",
            "Shengyi Qian",
            "Nikhil Madaan",
            "Madhavan Iyengar",
            "David F. Fouhey",
            "Joyce Chai"
        ],
        "published": "2023-09-21T17:59:45Z",
        "summary": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2309.12311v1.pdf"
    },
    {
        "title": "Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models",
        "authors": [
            "Levon Haroutunian",
            "Zhuang Li",
            "Lucian Galescu",
            "Philip Cohen",
            "Raj Tumuluri",
            "Gholamreza Haffari"
        ],
        "published": "2023-09-21T17:54:58Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language generation. However, their output quality can be inconsistent,\nposing challenges for generating natural language from logical forms (LFs).\nThis task requires the generated outputs to embody the exact semantics of LFs,\nwithout missing any LF semantics or creating any hallucinations. In this work,\nwe tackle this issue by proposing a novel generate-and-rerank approach. Our\napproach involves initially generating a set of candidate outputs by prompting\nan LLM and subsequently reranking them using a task-specific reranker model. In\naddition, we curate a manually collected dataset to evaluate the alignment\nbetween different ranking metrics and human judgements. The chosen ranking\nmetrics are utilized to enhance the training and evaluation of the reranker\nmodel. By conducting extensive experiments on three diverse datasets, we\ndemonstrate that the candidates selected by our reranker outperform those\nselected by baseline methods in terms of semantic consistency and fluency, as\nmeasured by three comprehensive metrics. Our findings provide strong evidence\nfor the effectiveness of our approach in improving the quality of generated\noutputs.",
        "pdf_link": "https://arxiv.org/pdf/2309.12294v1.pdf"
    },
    {
        "title": "The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"",
        "authors": [
            "Lukas Berglund",
            "Meg Tong",
            "Max Kaufmann",
            "Mikita Balesni",
            "Asa Cooper Stickland",
            "Tomasz Korbak",
            "Owain Evans"
        ],
        "published": "2023-09-21T17:52:19Z",
        "summary": "We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Valentina\nTereshkova was the first woman to travel to space\", it will not automatically\nbe able to answer the question, \"Who was the first woman to travel to space?\".\nMoreover, the likelihood of the correct answer (\"Valentina Tershkova\") will not\nbe higher than for a random name. Thus, models do not generalize a prevalent\npattern in their training set: if \"A is B\" occurs, \"B is A\" is more likely to\noccur. It is worth noting, however, that if \"A is B\" appears in-context, models\ncan deduce the reverse relationship. We provide evidence for the Reversal Curse\nby finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of Abyssal Melodies\" and showing that they fail to\ncorrectly answer \"Who composed Abyssal Melodies?\". The Reversal Curse is robust\nacross model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter.\n  Code available at: https://github.com/lukasberglund/reversal_curse.",
        "pdf_link": "https://arxiv.org/pdf/2309.12288v3.pdf"
    },
    {
        "title": "Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition",
        "authors": [
            "Junyi Bian",
            "Jiaxuan Zheng",
            "Yuyi Zhang",
            "Shanfeng Zhu"
        ],
        "published": "2023-09-21T17:39:53Z",
        "summary": "Large language models (LLMs) have demonstrated dominating performance in many\nNLP tasks, especially on generative tasks. However, they often fall short in\nsome information extraction tasks, particularly those requiring domain-specific\nknowledge, such as Biomedical Named Entity Recognition (NER). In this paper,\ninspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER\nstep-by-step: break down the NER task into entity span extraction and entity\ntype determination. Additionally, for entity type determination, we inject\nentity knowledge to address the problem that LLM's lack of domain knowledge\nwhen predicting entity category. Experimental results show a significant\nimprovement in our two-step BioNER approach compared to previous few-shot LLM\nbaseline. Additionally, the incorporation of external knowledge significantly\nenhances entity category determination performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.12278v1.pdf"
    },
    {
        "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
        "authors": [
            "Beizhe Hu",
            "Qiang Sheng",
            "Juan Cao",
            "Yuhui Shi",
            "Yang Li",
            "Danding Wang",
            "Peng Qi"
        ],
        "published": "2023-09-21T16:47:30Z",
        "summary": "Detecting fake news requires both a delicate sense of diverse clues and a\nprofound understanding of the real-world background, which remains challenging\nfor detectors based on small language models (SLMs) due to their knowledge and\ncapability limitations. Recent advances in large language models (LLMs) have\nshown remarkable performance in various tasks, but whether and how LLMs could\nhelp with fake news detection remains underexplored. In this paper, we\ninvestigate the potential of LLMs in fake news detection. First, we conduct an\nempirical study and find that a sophisticated LLM such as GPT 3.5 could\ngenerally expose fake news and provide desirable multi-perspective rationales\nbut still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis\nattributes such a gap to the LLM's inability to select and integrate rationales\nproperly to conclude. Based on these findings, we propose that current LLMs may\nnot substitute fine-tuned SLMs in fake news detection but can be a good advisor\nfor SLMs by providing multi-perspective instructive rationales. To instantiate\nthis proposal, we design an adaptive rationale guidance network for fake news\ndetection (ARG), in which SLMs selectively acquire insights on news analysis\nfrom the LLMs' rationales. We further derive a rationale-free version of ARG by\ndistillation, namely ARG-D, which services cost-sensitive scenarios without\nquerying LLMs. Experiments on two real-world datasets demonstrate that ARG and\nARG-D outperform three types of baseline methods, including SLM-based,\nLLM-based, and combinations of small and large language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.12247v2.pdf"
    },
    {
        "title": "Code Soliloquies for Accurate Calculations in Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "MyCo Le",
            "Xinghe Chen",
            "Naiming Liu",
            "Debshila Basu Mallick",
            "Richard G. Baraniuk"
        ],
        "published": "2023-09-21T15:16:58Z",
        "summary": "High-quality conversational datasets are crucial for the successful\ndevelopment of Intelligent Tutoring Systems (ITS) that utilize a Large Language\nModel (LLM) backend. Synthetic student-teacher dialogues, generated using\nadvanced GPT-4 models, are a common strategy for creating these datasets.\nHowever, subjects like physics that entail complex calculations pose a\nchallenge. While GPT-4 presents impressive language processing capabilities,\nits limitations in fundamental mathematical reasoning curtail its efficacy for\nsuch subjects. To tackle this limitation, we introduce in this paper an\ninnovative stateful prompt design. Our design orchestrates a mock conversation\nwhere both student and tutorbot roles are simulated by GPT-4. Each student\nresponse triggers an internal monologue, or `code soliloquy' in the\nGPT-tutorbot, which assesses whether its subsequent response would necessitate\ncalculations. If a calculation is deemed necessary, it scripts the relevant\nPython code and uses the Python output to construct a response to the student.\nOur approach notably enhances the quality of synthetic conversation datasets,\nespecially for subjects that are calculation-intensive. Our preliminary Subject\nMatter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA\nmodel, effectively uses Python for computations, which significantly enhances\nthe accuracy and computational reliability of Higgs' responses. Code, models,\nand datasets is available at https://github.com/luffycodes/Tutorbot-Spock-Phys.",
        "pdf_link": "https://arxiv.org/pdf/2309.12161v2.pdf"
    },
    {
        "title": "A knowledge representation approach for construction contract knowledge modeling",
        "authors": [
            "Chunmo Zheng",
            "Saika Wong",
            "Xing Su",
            "Yinqiu Tang"
        ],
        "published": "2023-09-21T14:53:36Z",
        "summary": "The emergence of large language models (LLMs) presents an unprecedented\nopportunity to automate construction contract management, reducing human errors\nand saving significant time and costs. However, LLMs may produce convincing yet\ninaccurate and misleading content due to a lack of domain expertise. To address\nthis issue, expert-driven contract knowledge can be represented in a structured\nmanner to constrain the automatic contract management process. This paper\nintroduces the Nested Contract Knowledge Graph (NCKG), a knowledge\nrepresentation approach that captures the complexity of contract knowledge\nusing a nested structure. It includes a nested knowledge representation\nframework, a NCKG ontology built on the framework, and an implementation\nmethod. Furthermore, we present the LLM-assisted contract review pipeline\nenhanced with external knowledge in NCKG. Our pipeline achieves a promising\nperformance in contract risk reviewing, shedding light on the combination of\nLLM and KG towards more reliable and interpretable contract management.",
        "pdf_link": "https://arxiv.org/pdf/2309.12132v1.pdf"
    },
    {
        "title": "Prompt Tuned Embedding Classification for Multi-Label Industry Sector Allocation",
        "authors": [
            "Valentin Leonhard Buchner",
            "Lele Cao",
            "Jan-Christoph Kalo",
            "Vilhelm von Ehrenheim"
        ],
        "published": "2023-09-21T13:45:32Z",
        "summary": "Prompt Tuning is emerging as a scalable and cost-effective method to\nfine-tune Pretrained Language Models (PLMs), which are often referred to as\nLarge Language Models (LLMs). This study benchmarks the performance and\ncomputational efficiency of Prompt Tuning and baselines for multi-label text\nclassification. This is applied to the challenging task of classifying\ncompanies into an investment firm's proprietary industry taxonomy, supporting\ntheir thematic investment strategy. Text-to-text classification is frequently\nreported to outperform task-specific classification heads, but has several\nlimitations when applied to a multi-label classification problem where each\nlabel consists of multiple tokens: (a) Generated labels may not match any label\nin the label taxonomy; (b) The fine-tuning process lacks permutation invariance\nand is sensitive to the order of the provided labels; (c) The model provides\nbinary decisions rather than appropriate confidence scores. Limitation (a) is\naddressed by applying constrained decoding using Trie Search, which slightly\nimproves classification performance. All limitations (a), (b), and (c) are\naddressed by replacing the PLM's language head with a classification head,\nwhich is referred to as Prompt Tuned Embedding Classification (PTEC). This\nimproves performance significantly, while also reducing computational costs\nduring inference. In our industrial application, the training data is skewed\ntowards well-known companies. We confirm that the model's performance is\nconsistent across both well-known and less-known companies. Our overall results\nindicate the continuing need to adapt state-of-the-art methods to\ndomain-specific tasks, even in the era of PLMs with strong generalization\nabilities. We release our codebase and a benchmarking dataset at\nhttps://github.com/EQTPartners/PTEC.",
        "pdf_link": "https://arxiv.org/pdf/2309.12075v2.pdf"
    },
    {
        "title": "AceGPT, Localizing Large Language Models in Arabic",
        "authors": [
            "Huang Huang",
            "Fei Yu",
            "Jianqing Zhu",
            "Xuening Sun",
            "Hao Cheng",
            "Dingjie Song",
            "Zhihong Chen",
            "Abdulmohsen Alharthi",
            "Bang An",
            "Juncai He",
            "Ziche Liu",
            "Zhiyi Zhang",
            "Junying Chen",
            "Jianquan Li",
            "Benyou Wang",
            "Lian Zhang",
            "Ruoyu Sun",
            "Xiang Wan",
            "Haizhou Li",
            "Jinchao Xu"
        ],
        "published": "2023-09-21T13:20:13Z",
        "summary": "This paper is devoted to the development of a localized Large Language Model\n(LLM) specifically for Arabic, a language imbued with unique cultural\ncharacteristics inadequately addressed by current mainstream models.\nSignificant concerns emerge when addressing cultural sensitivity and local\nvalues. To address this, the paper proposes a comprehensive solution that\nincludes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT)\nutilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside\nReinforcement Learning with AI Feedback (RLAIF) employing a reward model\nattuned to local culture and values. The goal is to cultivate culturally\ncognizant and value-aligned Arabic LLMs capable of accommodating the diverse,\napplication-specific needs of Arabic-speaking communities.\n  Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',\nsets the state-of-the-art standard for open Arabic LLMs across various\nbenchmarks. Codes, data, and models are in\nhttps://github.com/FreedomIntelligence/AceGPT.",
        "pdf_link": "https://arxiv.org/pdf/2309.12053v5.pdf"
    },
    {
        "title": "Knowledge Sanitization of Large Language Models",
        "authors": [
            "Yoichi Ishibashi",
            "Hidetoshi Shimodaira"
        ],
        "published": "2023-09-21T07:49:55Z",
        "summary": "We explore a knowledge sanitization approach to mitigate the privacy concerns\nassociated with large language models (LLMs). LLMs trained on a large corpus of\nWeb data can memorize and potentially reveal sensitive or confidential\ninformation, raising critical security concerns. Our technique efficiently\nfine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting\nthem to generate harmless responses such as ``I don't know'' when queried about\nspecific information. Experimental results in a closed-book question-answering\ntask show that our straightforward method not only minimizes particular\nknowledge leakage but also preserves the overall performance of LLMs. These two\nadvantages strengthen the defense against extraction attacks and reduces the\nemission of harmful content such as hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2309.11852v2.pdf"
    },
    {
        "title": "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs",
        "authors": [
            "Chengyuan Liu",
            "Fubang Zhao",
            "Lizhi Qing",
            "Yangyang Kang",
            "Changlong Sun",
            "Kun Kuang",
            "Fei Wu"
        ],
        "published": "2023-09-21T07:07:49Z",
        "summary": "Large Language Models (LLMs) presents significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset with high successful attacking\nrate to evaluate the abilities of defending prompt attack. In this paper, we\nintroduce a pipeline to construct high-quality prompt attack samples, along\nwith a Chinese prompt attack dataset called CPAD. Our prompts aim to induce\nLLMs to generate unexpected outputs with several carefully designed prompt\nattack templates and widely concerned attacking contents. Different from\nprevious datasets involving safety estimation, we construct the prompts\nconsidering three dimensions: contents, attacking methods and goals.\nEspecially, the attacking goals indicate the behaviour expected after\nsuccessfully attacking the LLMs, thus the responses can be easily evaluated and\nanalysed. We run several popular Chinese LLMs on our dataset, and the results\nshow that our prompts are significantly harmful to LLMs, with around 70% attack\nsuccess rate to GPT-3.5. CPAD is publicly available at\nhttps://github.com/liuchengyuan123/CPAD.",
        "pdf_link": "https://arxiv.org/pdf/2309.11830v2.pdf"
    },
    {
        "title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
        "authors": [
            "Ryutaro Yamauchi",
            "Sho Sonoda",
            "Akiyoshi Sannai",
            "Wataru Kumagai"
        ],
        "published": "2023-09-21T02:46:20Z",
        "summary": "In utilizing large language models (LLMs) for mathematical reasoning,\naddressing the errors in the reasoning and calculation present in the generated\ntext by LLMs is a crucial challenge. In this paper, we propose a novel\nframework that integrates the Chain-of-Thought (CoT) method with an external\ntool (Python REPL). We discovered that by prompting LLMs to generate structured\ntext in XML-like markup language, we could seamlessly integrate CoT and the\nexternal tool and control the undesired behaviors of LLMs. With our approach,\nLLMs can utilize Python computation to rectify errors within CoT. We applied\nour method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and\ndemonstrated that combining CoT and Python REPL through the markup language\nenhances the reasoning capability of LLMs. Our approach enables LLMs to write\nthe markup language and perform advanced mathematical reasoning using only\nzero-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2309.13078v2.pdf"
    },
    {
        "title": "LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination",
        "authors": [
            "Kai Zhang",
            "Yangyang Kang",
            "Fubang Zhao",
            "Xiaozhong Liu"
        ],
        "published": "2023-09-21T00:34:33Z",
        "summary": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable\nproficiency in comprehending and generating natural language. On the other\nhand, medical assistants hold the potential to offer substantial benefits for\nindividuals. However, the exploration of LLM-based personalized medical\nassistant remains relatively scarce. Typically, patients converse differently\nbased on their background and preferences which necessitates the task of\nenhancing user-oriented medical assistant. While one can fully train an LLM for\nthis objective, the resource consumption is unaffordable. Prior research has\nexplored memory-based methods to enhance the response with aware of previous\nmistakes for new queries during a dialogue session. We contend that a mere\nmemory module is inadequate and fully training an LLM can be excessively\ncostly. In this study, we propose a novel computational bionic memory\nmechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to\npersonalize medical assistants.",
        "pdf_link": "https://arxiv.org/pdf/2309.11696v3.pdf"
    },
    {
        "title": "LLM Guided Inductive Inference for Solving Compositional Problems",
        "authors": [
            "Abhigya Sodani",
            "Lauren Moos",
            "Matthew Mirman"
        ],
        "published": "2023-09-20T23:44:16Z",
        "summary": "While large language models (LLMs) have demonstrated impressive performance\nin question-answering tasks, their performance is limited when the questions\nrequire knowledge that is not included in the model's training data and can\nonly be acquired through direct observation or interaction with the real world.\nExisting methods decompose reasoning tasks through the use of modules invoked\nsequentially, limiting their ability to answer deep reasoning tasks. We\nintroduce a method, Recursion based extensible LLM (REBEL), which handles\nopen-world, deep reasoning tasks by employing automated reasoning techniques\nlike dynamic planning and forward-chaining strategies. REBEL allows LLMs to\nreason via recursive problem decomposition and utilization of external tools.\nThe tools that REBEL uses are specified only by natural language description.\nWe further demonstrate REBEL capabilities on a set of problems that require a\ndeeply nested use of external tools in a compositional and conversational\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2309.11688v1.pdf"
    },
    {
        "title": "Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation",
        "authors": [
            "Ali Mousavi",
            "Xin Zhan",
            "He Bai",
            "Peng Shi",
            "Theo Rekatsinas",
            "Benjamin Han",
            "Yunyao Li",
            "Jeff Pound",
            "Josh Susskind",
            "Natalie Schluter",
            "Ihab Ilyas",
            "Navdeep Jaitly"
        ],
        "published": "2023-09-20T22:30:20Z",
        "summary": "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used\nto train forward and reverse neural models that generate text from KG and vice\nversa. However models trained on datasets where KG and text pairs are not\nequivalent can suffer from more hallucination and poorer recall. In this paper,\nwe verify this empirically by generating datasets with different levels of\nnoise and find that noisier datasets do indeed lead to more hallucination. We\nargue that the ability of forward and reverse models trained on a dataset to\ncyclically regenerate source KG or text is a proxy for the equivalence between\nthe KG and the text in the dataset. Using cyclic evaluation we find that\nmanually created WebNLG is much better than automatically created TeKGen and\nT-REx. Guided by these observations, we construct a new, improved dataset\ncalled LAGRANGE using heuristics meant to improve equivalence between KG and\ntext and show the impact of each of the heuristics on cyclic evaluation. We\nalso construct two synthetic datasets using large language models (LLMs), and\nobserve that these are conducive to models that perform significantly well on\ncyclic generation of text, but less so on cyclic generation of KGs, probably\nbecause of a lack of a consistent underlying ontology.",
        "pdf_link": "https://arxiv.org/pdf/2309.11669v1.pdf"
    },
    {
        "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models",
        "authors": [
            "Vivek Iyer",
            "Pinzhen Chen",
            "Alexandra Birch"
        ],
        "published": "2023-09-20T22:22:52Z",
        "summary": "Resolving semantic ambiguity has long been recognised as a central challenge\nin the field of Machine Translation. Recent work on benchmarking translation\nperformance on ambiguous sentences has exposed the limitations of conventional\nNeural Machine Translation (NMT) systems, which fail to handle many such cases.\nLarge language models (LLMs) have emerged as a promising alternative,\ndemonstrating comparable performance to traditional NMT models while\nintroducing new paradigms for controlling the target outputs. In this paper, we\nstudy the capabilities of LLMs to translate \"ambiguous sentences\" - i.e. those\ncontaining highly polysemous words and/or rare word senses. We also propose two\nways to improve their disambiguation capabilities, through a) in-context\nlearning and b) fine-tuning on carefully curated ambiguous datasets.\nExperiments show that our methods can match or outperform state-of-the-art\nsystems such as DeepL and NLLB in four out of five language directions. Our\nresearch provides valuable insights into effectively adapting LLMs to become\nbetter disambiguators during Machine Translation. We release our curated\ndisambiguation corpora and resources at\nhttps://data.statmt.org/ambiguous-europarl.",
        "pdf_link": "https://arxiv.org/pdf/2309.11668v2.pdf"
    },
    {
        "title": "\"It's a Fair Game\", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",
        "authors": [
            "Zhiping Zhang",
            "Michelle Jia",
            "Hao-Ping Lee",
            "Bingsheng Yao",
            "Sauvik Das",
            "Ada Lerner",
            "Dakuo Wang",
            "Tianshi Li"
        ],
        "published": "2023-09-20T21:34:36Z",
        "summary": "The widespread use of Large Language Model (LLM)-based conversational agents\n(CAs), especially in high-stakes domains, raises many privacy concerns.\nBuilding ethical LLM-based CAs that respect user privacy requires an in-depth\nunderstanding of the privacy risks that concern users the most. However,\nexisting research, primarily model-centered, does not provide insight into\nusers' perspectives. To bridge this gap, we analyzed sensitive disclosures in\nreal-world ChatGPT conversations and conducted semi-structured interviews with\n19 LLM-based CA users. We found that users are constantly faced with trade-offs\nbetween privacy, utility, and convenience when using LLM-based CAs. However,\nusers' erroneous mental models and the dark patterns in system design limited\ntheir awareness and comprehension of the privacy risks. Additionally, the\nhuman-like interactions encouraged more sensitive disclosures, which\ncomplicated users' ability to navigate the trade-offs. We discuss practical\ndesign guidelines and the needs for paradigm shifts to protect the privacy of\nLLM-based CA users.",
        "pdf_link": "https://arxiv.org/pdf/2309.11653v2.pdf"
    },
    {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "authors": [
            "Shehzaad Dhuliawala",
            "Mojtaba Komeili",
            "Jing Xu",
            "Roberta Raileanu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Jason Weston"
        ],
        "published": "2023-09-20T17:50:55Z",
        "summary": "Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.",
        "pdf_link": "https://arxiv.org/pdf/2309.11495v2.pdf"
    },
    {
        "title": "Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning",
        "authors": [
            "Tianbao Xie",
            "Siheng Zhao",
            "Chen Henry Wu",
            "Yitao Liu",
            "Qian Luo",
            "Victor Zhong",
            "Yanchao Yang",
            "Tao Yu"
        ],
        "published": "2023-09-20T17:39:13Z",
        "summary": "Designing reward functions is a longstanding challenge in reinforcement\nlearning (RL); it requires specialized knowledge or domain data, leading to\nhigh costs for development. To address this, we introduce Text2Reward, a\ndata-free framework that automates the generation of dense reward functions\nbased on large language models (LLMs). Given a goal described in natural\nlanguage, Text2Reward generates dense reward functions as an executable program\ngrounded in a compact representation of the environment. Unlike inverse RL and\nrecent work that uses LLMs to write sparse reward codes, Text2Reward produces\ninterpretable, free-form dense reward codes that cover a wide range of tasks,\nutilize existing packages, and allow iterative refinement with human feedback.\nWe evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,\nMetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17\nmanipulation tasks, policies trained with generated reward codes achieve\nsimilar or better task success rates and convergence speed than expert-written\nreward codes. For locomotion tasks, our method learns six novel locomotion\nbehaviors with a success rate exceeding 94%. Furthermore, we show that the\npolicies trained in the simulator with our method can be deployed in the real\nworld. Finally, Text2Reward further improves the policies by refining their\nreward functions with human feedback. Video results are available at\nhttps://text-to-reward.github.io",
        "pdf_link": "https://arxiv.org/pdf/2309.11489v2.pdf"
    },
    {
        "title": "GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction",
        "authors": [
            "Suryanarayanan Balaji",
            "Rishikesh Magar",
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "published": "2023-09-20T17:21:43Z",
        "summary": "With the emergence of Transformer architectures and their powerful\nunderstanding of textual data, a new horizon has opened up to predict the\nmolecular properties based on text description. While SMILES are the most\ncommon form of representation, they are lacking robustness, rich information\nand canonicity, which limit their effectiveness in becoming generalizable\nrepresentations. Here, we present GPT-MolBERTa, a self-supervised large\nlanguage model (LLM) which uses detailed textual descriptions of molecules to\npredict their properties. A text based description of 326000 molecules were\ncollected using ChatGPT and used to train LLM to learn the representation of\nmolecules. To predict the properties for the downstream tasks, both BERT and\nRoBERTa models were used in the finetuning stage. Experiments show that\nGPT-MolBERTa performs well on various molecule property benchmarks, and\napproaching state of the art performance in regression tasks. Additionally,\nfurther analysis of the attention mechanisms show that GPT-MolBERTa is able to\npick up important information from the input textual data, displaying the\ninterpretability of the model.",
        "pdf_link": "https://arxiv.org/pdf/2310.03030v3.pdf"
    },
    {
        "title": "Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-09-20T16:14:10Z",
        "summary": "In Grammatical Error Correction (GEC), it is crucial to ensure the user's\ncomprehension of a reason for correction. Existing studies present tokens,\nexamples, and hints as to the basis for correction but do not directly explain\nthe reasons for corrections. Although methods that use Large Language Models\n(LLMs) to provide direct explanations in natural language have been proposed\nfor various tasks, no such method exists for GEC. Generating explanations for\nGEC corrections involves aligning input and output tokens, identifying\ncorrection points, and presenting corresponding explanations consistently.\nHowever, it is not straightforward to specify a complex format to generate\nexplanations, because explicit control of generation is difficult with prompts.\nThis study introduces a method called controlled generation with Prompt\nInsertion (PI) so that LLMs can explain the reasons for corrections in natural\nlanguage. In PI, LLMs first correct the input text, and then we automatically\nextract the correction points based on the rules. The extracted correction\npoints are sequentially inserted into the LLM's explanation output as prompts,\nguiding the LLMs to generate explanations for the correction points. We also\ncreate an Explainable GEC (XGEC) dataset of correction reasons by annotating\nNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT\nusing original prompts miss some correction points, the generation control\nusing PI can explicitly guide to describe explanations for all correction\npoints, contributing to improved performance in generating correction reasons.",
        "pdf_link": "https://arxiv.org/pdf/2309.11439v1.pdf"
    },
    {
        "title": "Speak While You Think: Streaming Speech Synthesis During Text Generation",
        "authors": [
            "Avihu Dekel",
            "Slava Shechtman",
            "Raul Fernandez",
            "David Haws",
            "Zvi Kons",
            "Ron Hoory"
        ],
        "published": "2023-09-20T11:00:15Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive capabilities, yet\ninteraction with these models is mostly facilitated through text. Using\nText-To-Speech to synthesize LLM outputs typically results in notable latency,\nwhich is impractical for fluent voice conversations. We propose LLM2Speech, an\narchitecture to synthesize speech while text is being generated by an LLM which\nyields significant latency reduction. LLM2Speech mimics the predictions of a\nnon-streaming teacher model while limiting the exposure to future context in\norder to enable streaming. It exploits the hidden embeddings of the LLM, a\nby-product of the text generation that contains informative semantic context.\nExperimental results show that LLM2Speech maintains the teacher's quality while\nreducing the latency to enable natural conversations.",
        "pdf_link": "https://arxiv.org/pdf/2309.11210v1.pdf"
    },
    {
        "title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
        "authors": [
            "Yike Wu",
            "Nan Hu",
            "Sheng Bi",
            "Guilin Qi",
            "Jie Ren",
            "Anhuan Xie",
            "Wei Song"
        ],
        "published": "2023-09-20T10:42:08Z",
        "summary": "Despite their competitive performance on knowledge-intensive tasks, large\nlanguage models (LLMs) still have limitations in memorizing all world knowledge\nespecially long tail knowledge. In this paper, we study the KG-augmented\nlanguage model approach for solving the knowledge graph question answering\n(KGQA) task that requires rich world knowledge. Existing work has shown that\nretrieving KG knowledge to enhance LLMs prompting can significantly improve\nLLMs performance in KGQA. However, their approaches lack a well-formed\nverbalization of KG knowledge, i.e., they ignore the gap between KG\nrepresentations and textual representations. To this end, we propose an\nanswer-sensitive KG-to-Text approach that can transform KG knowledge into\nwell-textualized statements most informative for KGQA. Based on this approach,\nwe propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.\nExperiments on several KGQA benchmarks show that the proposed KG-to-Text\naugmented LLMs approach outperforms previous KG-augmented LLMs approaches\nregarding answer accuracy and usefulness of knowledge statements.",
        "pdf_link": "https://arxiv.org/pdf/2309.11206v2.pdf"
    },
    {
        "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
        "authors": [
            "Haoyu Wang",
            "Guozheng Ma",
            "Cong Yu",
            "Ning Gui",
            "Linrui Zhang",
            "Zhiqi Huang",
            "Suwei Ma",
            "Yongzhe Chang",
            "Sen Zhang",
            "Li Shen",
            "Xueqian Wang",
            "Peilin Zhao",
            "Dacheng Tao"
        ],
        "published": "2023-09-20T09:23:46Z",
        "summary": "The swift advancement in the scales and capabilities of Large Language Models\n(LLMs) positions them as promising tools for a variety of downstream tasks. In\naddition to the pursuit of better performance and the avoidance of violent\nfeedback on a certain prompt, to ensure the responsibility of the LLM, much\nattention is drawn to the robustness of LLMs. However, existing evaluation\nmethods mostly rely on traditional question answering datasets with predefined\nsupervised labels, which do not align with the superior generation capabilities\nof contemporary LLMs. To address this issue, we propose a novel rational\nevaluation approach that leverages pre-trained reward models as diagnostic\ntools to evaluate the longer conversation generated from more challenging open\nquestions by LLMs, which we refer to as the Reward Model for Reasonable\nRobustness Evaluation (TREvaL). Longer conversations manifest the comprehensive\ngrasp of language models in terms of their proficiency in understanding\nquestions, a capability not entirely encompassed by individual words or\nletters, which may exhibit oversimplification and inherent biases. Our\nextensive empirical experiments demonstrate that TREvaL provides an innovative\nmethod for evaluating the robustness of an LLM. Furthermore, our results\ndemonstrate that LLMs frequently exhibit vulnerability to word-level\nperturbations that are commonplace in daily language usage. Notably, we are\nsurprised to discover that robustness tends to decrease as fine-tuning (SFT and\nRLHF) is conducted. The code of TREval is available in\nhttps://github.com/Harry-mic/TREvaL.",
        "pdf_link": "https://arxiv.org/pdf/2309.11166v2.pdf"
    },
    {
        "title": "Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness",
        "authors": [
            "Vipula Rawte",
            "Prachi Priya",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023-09-20T05:04:16Z",
        "summary": "As Large Language Models (LLMs) have advanced, they have brought forth new\nchallenges, with one of the prominent issues being LLM hallucination. While\nvarious mitigation techniques are emerging to address hallucination, it is\nequally crucial to delve into its underlying causes. Consequently, in this\npreliminary exploratory investigation, we examine how linguistic factors in\nprompts, specifically readability, formality, and concreteness, influence the\noccurrence of hallucinations. Our experimental results suggest that prompts\ncharacterized by greater formality and concreteness tend to result in reduced\nhallucination. However, the outcomes pertaining to readability are somewhat\ninconclusive, showing a mixed pattern.",
        "pdf_link": "https://arxiv.org/pdf/2309.11064v1.pdf"
    },
    {
        "title": "In-Context Learning for Text Classification with Many Labels",
        "authors": [
            "Aristides Milios",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023-09-19T22:41:44Z",
        "summary": "In-context learning (ICL) using large language models for tasks with many\nlabels is challenging due to the limited context window, which makes it\ndifficult to fit a sufficient number of examples in the prompt. In this paper,\nwe use a pre-trained dense retrieval model to bypass this limitation, giving\nthe model only a partial view of the full label space for each inference call.\nTesting with recent open-source LLMs (OPT, LLaMA), we set new state of the art\nperformance in few-shot settings for three common intent classification\ndatasets, with no finetuning. We also surpass fine-tuned performance on\nfine-grained sentiment classification in certain cases. We analyze the\nperformance across number of in-context examples and different model scales,\nshowing that larger models are necessary to effectively and consistently make\nuse of larger context lengths for ICL. By running several ablations, we analyze\nthe model's use of: a) the similarity of the in-context examples to the current\ninput, b) the semantic content of the class names, and c) the correct\ncorrespondence between examples and labels. We demonstrate that all three are\nneeded to varying degrees depending on the domain, contrary to certain recent\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2309.10954v2.pdf"
    },
    {
        "title": "LMDX: Language Model-based Document Information Extraction and Localization",
        "authors": [
            "Vincent Perot",
            "Kai Kang",
            "Florian Luisier",
            "Guolong Su",
            "Xiaoyu Sun",
            "Ramya Sree Boppana",
            "Zilong Wang",
            "Jiaqi Mu",
            "Hao Zhang",
            "Nan Hua"
        ],
        "published": "2023-09-19T22:32:56Z",
        "summary": "Large Language Models (LLM) have revolutionized Natural Language Processing\n(NLP), improving state-of-the-art on many existing tasks and exhibiting\nemergent capabilities. However, LLMs have not yet been successfully applied on\nsemi-structured document information extraction, which is at the core of many\ndocument processing workflows and consists of extracting key entities from a\nvisually rich document (VRD) given a predefined target schema. The main\nobstacles to LLM adoption in that task have been the absence of layout encoding\nwithin LLMs, critical for a high quality extraction, and the lack of a\ngrounding mechanism ensuring the answer is not hallucinated. In this paper, we\nintroduce Language Model-based Document Information Extraction and Localization\n(LMDX), a methodology to adapt arbitrary LLMs for document information\nextraction. LMDX can do extraction of singular, repeated, and hierarchical\nentities, both with and without training data, while providing grounding\nguarantees and localizing the entities within the document. In particular, we\napply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks,\nsetting a new state-of-the-art and showing how LMDX enables the creation of\nhigh quality, data-efficient parsers.",
        "pdf_link": "https://arxiv.org/pdf/2309.10952v1.pdf"
    },
    {
        "title": "Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training",
        "authors": [
            "Ruiqi Xu",
            "Yongfeng Huang",
            "Xin Chen",
            "Lin Zhang"
        ],
        "published": "2023-09-19T21:01:40Z",
        "summary": "In this work, we introduce the concept of complex text style transfer tasks,\nand constructed complex text datasets based on two widely applicable scenarios.\nOur dataset is the first large-scale data set of its kind, with 700 rephrased\nsentences and 1,000 sentences from the game Genshin Impact. While large\nlanguage models (LLM) have shown promise in complex text style transfer, they\nhave drawbacks such as data privacy concerns, network instability, and high\ndeployment costs. To address these issues, we explore the effectiveness of\nsmall models (less than T5-3B) with implicit style pre-training through\ncontrastive learning. We also propose a method for automated evaluation of text\ngeneration quality based on alignment with human evaluations using ChatGPT.\nFinally, we compare our approach with existing methods and show that our model\nachieves state-of-art performances of few-shot text style transfer models.",
        "pdf_link": "https://arxiv.org/pdf/2309.10929v1.pdf"
    },
    {
        "title": "FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection",
        "authors": [
            "Jamil Zaghir",
            "Mina Bjelogrlic",
            "Jean-Philippe Goldman",
            "Souka√Øna Aananou",
            "Christophe Gaudet-Blavignac",
            "Christian Lovis"
        ],
        "published": "2023-09-19T17:17:28Z",
        "summary": "Natural language processing (NLP) applications such as named entity\nrecognition (NER) for low-resource corpora do not benefit from recent advances\nin the development of large language models (LLMs) where there is still a need\nfor larger annotated datasets. This research article introduces a methodology\nfor generating translated versions of annotated datasets through crosslingual\nannotation projection. Leveraging a language agnostic BERT-based approach, it\nis an efficient solution to increase low-resource corpora with few human\nefforts and by only using already available open data resources. Quantitative\nand qualitative evaluations are often lacking when it comes to evaluating the\nquality and effectiveness of semi-automatic data generation strategies. The\nevaluation of our crosslingual annotation projection approach showed both\neffectiveness and high accuracy in the resulting dataset. As a practical\napplication of this methodology, we present the creation of French Annotated\nResource with Semantic Information for Medical Entities Detection (FRASIMED),\nan annotated corpus comprising 2'051 synthetic clinical cases in French. The\ncorpus is now available for researchers and practitioners to develop and refine\nFrench natural language processing (NLP) applications in the clinical field\n(https://zenodo.org/record/8355629), making it the largest open annotated\ncorpus with linked medical concepts in French.",
        "pdf_link": "https://arxiv.org/pdf/2309.10770v1.pdf"
    },
    {
        "title": "Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome",
        "authors": [
            "Hiromu Yakura"
        ],
        "published": "2023-09-19T16:41:19Z",
        "summary": "Metaphors and sarcasm are precious fruits of our highly-evolved social\ncommunication skills. However, children with Asperger syndrome are known to\nhave difficulties in comprehending sarcasm, even if they possess a certain\nlevel of verbal IQ sufficient for understanding metaphors. Given that, a\nscreening test that scores the ability to understand metaphor and sarcasm has\nbeen used to differentiate Asperger syndrome from other symptoms exhibiting\nakin external behaviors (e.g., attention-deficit/hyperactivity disorder). This\nstudy uses the standardized test to examine the capability of recent large\nlanguage models (LLMs) in understanding human nuanced communication. The\nresults divulged that, whereas their ability to comprehend metaphors has been\nimproved with the increase of the number of model parameters, the improvement\nin sarcasm understanding was not observed. This implies that an alternative\napproach is imperative to imbue LLMs with the capacity to grasp sarcasm, which\nhas been associated with the amygdala, a pivotal cerebral region for emotional\nlearning, in the case of humans.",
        "pdf_link": "https://arxiv.org/pdf/2309.10744v2.pdf"
    },
    {
        "title": "GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models",
        "authors": [
            "Yonggan Fu",
            "Yongan Zhang",
            "Zhongzhi Yu",
            "Sixu Li",
            "Zhifan Ye",
            "Chaojian Li",
            "Cheng Wan",
            "Yingyan Lin"
        ],
        "published": "2023-09-19T16:14:57Z",
        "summary": "The remarkable capabilities and intricate nature of Artificial Intelligence\n(AI) have dramatically escalated the imperative for specialized AI\naccelerators. Nonetheless, designing these accelerators for various AI\nworkloads remains both labor- and time-intensive. While existing design\nexploration and automation tools can partially alleviate the need for extensive\nhuman involvement, they still demand substantial hardware expertise, posing a\nbarrier to non-experts and stifling AI accelerator development. Motivated by\nthe astonishing potential of large language models (LLMs) for generating\nhigh-quality content in response to human language instructions, we embark on\nthis work to examine the possibility of harnessing LLMs to automate AI\naccelerator design. Through this endeavor, we develop GPT4AIGChip, a framework\nintended to democratize AI accelerator design by leveraging human natural\nlanguages instead of domain-specific languages. Specifically, we first perform\nan in-depth investigation into LLMs' limitations and capabilities for AI\naccelerator design, thus aiding our understanding of our current position and\ngarnering insights into LLM-powered automated AI accelerator design.\nFurthermore, drawing inspiration from the above insights, we develop a\nframework called GPT4AIGChip, which features an automated demo-augmented\nprompt-generation pipeline utilizing in-context learning to guide LLMs towards\ncreating high-quality AI accelerator design. To our knowledge, this work is the\nfirst to demonstrate an effective pipeline for LLM-powered automated AI\naccelerator generation. Accordingly, we anticipate that our insights and\nframework can serve as a catalyst for innovations in next-generation\nLLM-powered design automation tools.",
        "pdf_link": "https://arxiv.org/pdf/2309.10730v1.pdf"
    },
    {
        "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation",
        "authors": [
            "Yucheng Li"
        ],
        "published": "2023-09-19T15:02:58Z",
        "summary": "Data contamination in model evaluation is getting increasingly prevalent as\nthe massive training corpora of large language models often unintentionally\ninclude benchmark samples. Therefore, contamination analysis has became an\ninevitable part of reliable model evaluation. However, existing method of\ncontamination analysis requires the access of the entire training data which is\noften confidential for recent models. This prevent the community to rigorously\naudit these models and conduct accurate assessment of their capability. In this\npaper, we propose a novel method to quantify contamination without the access\nof the full training set, that measure the extent of contamination with\nperplexity. Our analysis provides evidence of significant memorisation of\nrecent foundation models in popular reading comprehension, summarisation\nbenchmarks, while multiple choice appears less contaminated.",
        "pdf_link": "https://arxiv.org/pdf/2309.10677v2.pdf"
    },
    {
        "title": "Model Leeching: An Extraction Attack Targeting LLMs",
        "authors": [
            "Lewis Birch",
            "William Hackett",
            "Stefan Trawicki",
            "Neeraj Suri",
            "Peter Garraghan"
        ],
        "published": "2023-09-19T11:45:29Z",
        "summary": "Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2309.10544v1.pdf"
    },
    {
        "title": "Exploring Iterative Enhancement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models",
        "authors": [
            "Qiming Bao",
            "Juho Leinonen",
            "Alex Yuxuan Peng",
            "Wanjun Zhong",
            "Ga√´l Gendron",
            "Timothy Pistotti",
            "Alice Huang",
            "Paul Denny",
            "Michael Witbrock",
            "Jiamou Liu"
        ],
        "published": "2023-09-19T09:04:15Z",
        "summary": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.10444v4.pdf"
    },
    {
        "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training",
        "authors": [
            "Dawei Zhu",
            "Nan Yang",
            "Liang Wang",
            "Yifan Song",
            "Wenhao Wu",
            "Furu Wei",
            "Sujian Li"
        ],
        "published": "2023-09-19T08:03:38Z",
        "summary": "Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.",
        "pdf_link": "https://arxiv.org/pdf/2309.10400v3.pdf"
    },
    {
        "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
        "authors": [
            "Yuexiang Zhai",
            "Shengbang Tong",
            "Xiao Li",
            "Mu Cai",
            "Qing Qu",
            "Yong Jae Lee",
            "Yi Ma"
        ],
        "published": "2023-09-19T04:51:13Z",
        "summary": "Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.",
        "pdf_link": "https://arxiv.org/pdf/2309.10313v4.pdf"
    },
    {
        "title": "LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins",
        "authors": [
            "Umar Iqbal",
            "Tadayoshi Kohno",
            "Franziska Roesner"
        ],
        "published": "2023-09-19T02:20:10Z",
        "summary": "Large language model (LLM) platforms, such as ChatGPT, have recently begun\noffering a plugin ecosystem to interface with third-party services on the\ninternet. While these plugins extend the capabilities of LLM platforms, they\nare developed by arbitrary third parties and thus cannot be implicitly trusted.\nPlugins also interface with LLM platforms and users using natural language,\nwhich can have imprecise interpretations. In this paper, we propose a framework\nthat lays a foundation for LLM platform designers to analyze and improve the\nsecurity, privacy, and safety of current and future plugin-integrated LLM\nplatforms. Our framework is a formulation of an attack taxonomy that is\ndeveloped by iteratively exploring how LLM platform stakeholders could leverage\ntheir capabilities and responsibilities to mount attacks against each other. As\npart of our iterative process, we apply our framework in the context of\nOpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the\npotential for the types of issues that we outline in our attack taxonomy. We\nconclude by discussing novel challenges and by providing recommendations to\nimprove the security, privacy, and safety of present and future LLM-based\ncomputing platforms.",
        "pdf_link": "https://arxiv.org/pdf/2309.10254v1.pdf"
    },
    {
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
        "authors": [
            "Jiahao Yu",
            "Xingwei Lin",
            "Zheng Yu",
            "Xinyu Xing"
        ],
        "published": "2023-09-19T02:19:48Z",
        "summary": "Large language models (LLMs) have recently experienced tremendous popularity\nand are widely used from casual conversations to AI-driven programming.\nHowever, despite their considerable success, LLMs are not entirely reliable and\ncan give detailed guidance on how to conduct harmful or illegal activities.\nWhile safety measures can reduce the risk of such outputs, adversarial\njailbreak attacks can still exploit LLMs to produce harmful content. These\njailbreak templates are typically manually crafted, making large-scale testing\nchallenging.\n  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing\nframework inspired by the AFL fuzzing framework. Instead of manual engineering,\nGPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.\nAt its core, GPTFuzz starts with human-written templates as initial seeds, then\nmutates them to produce new templates. We detail three key components of\nGPTFuzz: a seed selection strategy for balancing efficiency and variability,\nmutate operators for creating semantically equivalent or similar sentences, and\na judgment model to assess the success of a jailbreak attack.\n  We evaluate GPTFuzz against various commercial and open-source LLMs,\nincluding ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our\nresults indicate that GPTFuzz consistently produces jailbreak templates with a\nhigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzz\nachieves over 90% attack success rates against ChatGPT and Llama-2 models, even\nwith suboptimal initial seed templates. We anticipate that GPTFuzz will be\ninstrumental for researchers and practitioners in examining LLM robustness and\nwill encourage further exploration into enhancing LLM safety.",
        "pdf_link": "https://arxiv.org/pdf/2309.10253v2.pdf"
    },
    {
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
        "authors": [
            "Baolin Peng",
            "Linfeng Song",
            "Ye Tian",
            "Lifeng Jin",
            "Haitao Mi",
            "Dong Yu"
        ],
        "published": "2023-09-18T23:06:32Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.",
        "pdf_link": "https://arxiv.org/pdf/2309.10202v1.pdf"
    },
    {
        "title": "RadOnc-GPT: A Large Language Model for Radiation Oncology",
        "authors": [
            "Zhengliang Liu",
            "Peilong Wang",
            "Yiwei Li",
            "Jason Holmes",
            "Peng Shu",
            "Lian Zhang",
            "Chenbin Liu",
            "Ninghao Liu",
            "Dajiang Zhu",
            "Xiang Li",
            "Quanzheng Li",
            "Samir H. Patel",
            "Terence T. Sio",
            "Tianming Liu",
            "Wei Liu"
        ],
        "published": "2023-09-18T21:15:02Z",
        "summary": "This paper presents RadOnc-GPT, a large language model specialized for\nradiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on\na large dataset of radiation oncology patient records from the Mayo Clinic in\nArizona. The model employs instruction tuning on three key tasks - generating\nradiotherapy treatment regimens, determining optimal radiation modalities, and\nproviding diagnostic descriptions/ICD codes based on patient diagnostic\ndetails. Evaluations conducted by comparing RadOnc-GPT outputs to general large\nlanguage model outputs showed higher ROUGE scores in these three tasks. The\nstudy demonstrated the potential of using large language models fine-tuned\nusing domain-specific knowledge like RadOnc-GPT to achieve transformational\ncapabilities in highly specialized healthcare fields such as radiation\noncology. However, our model's clinical relevance requires confirmation, and it\nspecializes in only the aforementioned three specific tasks and lacks broader\napplicability. Furthermore, its evaluation through ROUGE scores might not\nreflect the true semantic and clinical accuracy - challenges we intend to\naddress in future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.10160v3.pdf"
    },
    {
        "title": "Prompt a Robot to Walk with Large Language Models",
        "authors": [
            "Yen-Jen Wang",
            "Bike Zhang",
            "Jianyu Chen",
            "Koushil Sreenath"
        ],
        "published": "2023-09-18T17:50:17Z",
        "summary": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2309.09969v2.pdf"
    },
    {
        "title": "Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding",
        "authors": [
            "Andr√© Storhaug",
            "Jingyue Li",
            "Tianyuan Hu"
        ],
        "published": "2023-09-18T14:47:34Z",
        "summary": "Auto-completing code enables developers to speed up coding significantly.\nRecent advances in transformer-based large language model (LLM) technologies\nhave been applied to code synthesis. However, studies show that many of such\nsynthesized codes contain vulnerabilities. We propose a novel\nvulnerability-constrained decoding approach to reduce the amount of vulnerable\ncode generated by such models. Using a small dataset of labeled vulnerable\nlines of code, we fine-tune an LLM to include vulnerability labels when\ngenerating code, acting as an embedded classifier. Then, during decoding, we\ndeny the model to generate these labels to avoid generating vulnerable code. To\nevaluate the method, we chose to automatically complete Ethereum Blockchain\nsmart contracts (SCs) as the case study due to the strict requirements of SC\nsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397\nEthereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning\ntook more than one week using ten GPUs. The results showed that our fine-tuned\nmodel could synthesize SCs with an average BLEU (BiLingual Evaluation\nUnderstudy) score of 0.557. However, many codes in the auto-completed SCs were\nvulnerable. Using the code before the vulnerable line of 176 SCs containing\ndifferent types of vulnerabilities to auto-complete the code, we found that\nmore than 70% of the auto-completed codes were insecure. Thus, we further\nfine-tuned the model on other 941 vulnerable SCs containing the same types of\nvulnerabilities and applied vulnerability-constrained decoding. The fine-tuning\ntook only one hour with four GPUs. We then auto-completed the 176 SCs again and\nfound that our approach could identify 62% of the code to be generated as\nvulnerable and avoid generating 67% of them, indicating the approach could\nefficiently and effectively avoid vulnerabilities in the auto-completed code.",
        "pdf_link": "https://arxiv.org/pdf/2309.09826v2.pdf"
    },
    {
        "title": "Bias of AI-Generated Content: An Examination of News Produced by Large Language Models",
        "authors": [
            "Xiao Fang",
            "Shangkun Che",
            "Minjia Mao",
            "Hongzhe Zhang",
            "Ming Zhao",
            "Xiaohang Zhao"
        ],
        "published": "2023-09-18T14:47:24Z",
        "summary": "Large language models (LLMs) have the potential to transform our lives and\nwork through the content they generate, known as AI-Generated Content (AIGC).\nTo harness this transformation, we need to understand the limitations of LLMs.\nHere, we investigate the bias of AIGC produced by seven representative LLMs,\nincluding ChatGPT and LLaMA. We collect news articles from The New York Times\nand Reuters, both known for their dedication to provide unbiased news. We then\napply each examined LLM to generate news content with headlines of these news\narticles as prompts, and evaluate the gender and racial biases of the AIGC\nproduced by the LLM by comparing the AIGC and the original news articles. We\nfurther analyze the gender bias of each LLM under biased prompts by adding\ngender-biased messages to prompts constructed from these news headlines. Our\nstudy reveals that the AIGC produced by each examined LLM demonstrates\nsubstantial gender and racial biases. Moreover, the AIGC generated by each LLM\nexhibits notable discrimination against females and individuals of the Black\nrace. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest\nlevel of bias, and ChatGPT is the sole model capable of declining content\ngeneration when provided with biased prompts.",
        "pdf_link": "https://arxiv.org/pdf/2309.09825v3.pdf"
    },
    {
        "title": "Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark",
        "authors": [
            "Conghui Niu",
            "Mengyang Hu",
            "Lin Bo",
            "Xiaoli He",
            "Dong Yu",
            "Pengyuan Liu"
        ],
        "published": "2023-09-18T09:18:39Z",
        "summary": "Existing propositions often rely on logical constants for classification.\nCompared with Western languages that lean towards hypotaxis such as English,\nChinese often relies on semantic or logical understanding rather than logical\nconnectives in daily expressions, exhibiting the characteristics of parataxis.\nHowever, existing research has rarely paid attention to this issue. And\naccurately classifying these propositions is crucial for natural language\nunderstanding and reasoning. In this paper, we put forward the concepts of\nexplicit and implicit propositions and propose a comprehensive multi-level\nproposition classification system based on linguistics and logic.\nCorrespondingly, we create a large-scale Chinese proposition dataset PEACE from\nmultiple domains, covering all categories related to propositions. To evaluate\nthe Chinese proposition classification ability of existing models and explore\ntheir limitations, We conduct evaluations on PEACE using several different\nmethods including the Rule-based method, SVM, BERT, RoBERTA, and ChatGPT.\nResults show the importance of properly modeling the semantic features of\npropositions. BERT has relatively good proposition classification capability,\nbut lacks cross-domain transferability. ChatGPT performs poorly, but its\nclassification ability can be improved by providing more proposition\ninformation. Many issues are still far from being resolved and require further\nstudy.",
        "pdf_link": "https://arxiv.org/pdf/2309.09602v1.pdf"
    },
    {
        "title": "Progressive Text-to-Image Diffusion with Soft Latent Direction",
        "authors": [
            "YuTeng Ye",
            "Jiale Cai",
            "Hang Zhou",
            "Guanwen Li",
            "Youjia Zhang",
            "Zikai Song",
            "Chenxing Gao",
            "Junqing Yu",
            "Wei Yang"
        ],
        "published": "2023-09-18T04:01:25Z",
        "summary": "In spite of the rapidly evolving landscape of text-to-image generation, the\nsynthesis and manipulation of multiple entities while adhering to specific\nrelational constraints pose enduring challenges. This paper introduces an\ninnovative progressive synthesis and editing operation that systematically\nincorporates entities into the target image, ensuring their adherence to\nspatial and relational constraints at each sequential step. Our key insight\nstems from the observation that while a pre-trained text-to-image diffusion\nmodel adeptly handles one or two entities, it often falters when dealing with a\ngreater number. To address this limitation, we propose harnessing the\ncapabilities of a Large Language Model (LLM) to decompose intricate and\nprotracted text descriptions into coherent directives adhering to stringent\nformats. To facilitate the execution of directives involving distinct semantic\noperations-namely insertion, editing, and erasing-we formulate the Stimulus,\nResponse, and Fusion (SRF) framework. Within this framework, latent regions are\ngently stimulated in alignment with each operation, followed by the fusion of\nthe responsive latent components to achieve cohesive entity manipulation. Our\nproposed framework yields notable advancements in object synthesis,\nparticularly when confronted with intricate and lengthy textual inputs.\nConsequently, it establishes a new benchmark for text-to-image generation\ntasks, further elevating the field's performance standards.",
        "pdf_link": "https://arxiv.org/pdf/2309.09466v2.pdf"
    },
    {
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
        "authors": [
            "Bochuan Cao",
            "Yuanpu Cao",
            "Lu Lin",
            "Jinghui Chen"
        ],
        "published": "2023-09-18T02:07:22Z",
        "summary": "Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.",
        "pdf_link": "https://arxiv.org/pdf/2309.14348v2.pdf"
    },
    {
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "authors": [
            "Thuat Nguyen",
            "Chien Van Nguyen",
            "Viet Dac Lai",
            "Hieu Man",
            "Nghia Trung Ngo",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "published": "2023-09-17T23:49:10Z",
        "summary": "The driving factors behind the development of large language models (LLMs)\nwith impressive learning capabilities are their colossal model sizes and\nextensive training datasets. Along with the progress in natural language\nprocessing, LLMs have been frequently made accessible to the public to foster\ndeeper investigation and applications. However, when it comes to training\ndatasets for these LLMs, especially the recent state-of-the-art models, they\nare often not fully disclosed. Creating training data for high-performing LLMs\ninvolves extensive cleaning and deduplication to ensure the necessary level of\nquality. The lack of transparency for training data has thus hampered research\non attributing and addressing hallucination and bias issues in LLMs, hindering\nreplication efforts and further advancements in the community. These challenges\nbecome even more pronounced in multilingual learning scenarios, where the\navailable multilingual text datasets are often inadequately collected and\ncleaned. Consequently, there is a lack of open-source and readily usable\ndataset to effectively train LLMs in multiple languages. To overcome this\nissue, we present CulturaX, a substantial multilingual dataset with 6.3\ntrillion tokens in 167 languages, tailored for LLM development. Our dataset\nundergoes meticulous cleaning and deduplication through a rigorous pipeline of\nmultiple stages to accomplish the best quality for model training, including\nlanguage identification, URL-based filtering, metric-based cleaning, document\nrefinement, and data deduplication. CulturaX is fully released to the public in\nHuggingFace to facilitate research and advancements in multilingual LLMs:\nhttps://huggingface.co/datasets/uonlp/CulturaX.",
        "pdf_link": "https://arxiv.org/pdf/2309.09400v1.pdf"
    },
    {
        "title": "Language models are susceptible to incorrect patient self-diagnosis in medical applications",
        "authors": [
            "Rojin Ziaei",
            "Samuel Schmidgall"
        ],
        "published": "2023-09-17T19:56:39Z",
        "summary": "Large language models (LLMs) are becoming increasingly relevant as a\npotential tool for healthcare, aiding communication between clinicians,\nresearchers, and patients. However, traditional evaluations of LLMs on medical\nexam questions do not reflect the complexity of real patient-doctor\ninteractions. An example of this complexity is the introduction of patient\nself-diagnosis, where a patient attempts to diagnose their own medical\nconditions from various sources. While the patient sometimes arrives at an\naccurate conclusion, they more often are led toward misdiagnosis due to the\npatient's over-emphasis on bias validating information. In this work we present\na variety of LLMs with multiple-choice questions from United States medical\nboard exams which are modified to include self-diagnostic reports from\npatients. Our findings highlight that when a patient proposes incorrect\nbias-validating information, the diagnostic accuracy of LLMs drop dramatically,\nrevealing a high susceptibility to errors in self-diagnosis.",
        "pdf_link": "https://arxiv.org/pdf/2309.09362v1.pdf"
    },
    {
        "title": "Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model",
        "authors": [
            "Ziqi Yang",
            "Xuhai Xu",
            "Bingsheng Yao",
            "Shao Zhang",
            "Ethan Rogers",
            "Stephen Intille",
            "Nawar Shara",
            "Guodong Gordon Gao",
            "Dakuo Wang"
        ],
        "published": "2023-09-17T19:46:03Z",
        "summary": "Despite the plethora of telehealth applications to assist home-based older\nadults and healthcare providers, basic messaging and phone calls are still the\nmost common communication methods, which suffer from limited availability,\ninformation loss, and process inefficiencies. One promising solution to\nfacilitate patient-provider communication is to leverage large language models\n(LLMs) with their powerful natural conversation and summarization capability.\nHowever, there is a limited understanding of LLMs' role during the\ncommunication. We first conducted two interview studies with both older adults\n(N=10) and healthcare providers (N=9) to understand their needs and\nopportunities for LLMs in patient-provider asynchronous communication. Based on\nthe insights, we built an LLM-powered communication system, Talk2Care, and\ndesigned interactive components for both groups: (1) For older adults, we\nleveraged the convenience and accessibility of voice assistants (VAs) and built\nan LLM-powered VA interface for effective information collection. (2) For\nhealth providers, we built an LLM-based dashboard to summarize and present\nimportant health information based on older adults' conversations with the VA.\nWe further conducted two user studies with older adults and providers to\nevaluate the usability of the system. The results showed that Talk2Care could\nfacilitate the communication process, enrich the health information collected\nfrom older adults, and considerably save providers' efforts and time. We\nenvision our work as an initial exploration of LLMs' capability in the\nintersection of healthcare and interpersonal communication.",
        "pdf_link": "https://arxiv.org/pdf/2309.09357v5.pdf"
    },
    {
        "title": "Can Large Language Models Understand Real-World Complex Instructions?",
        "authors": [
            "Qianyu He",
            "Jie Zeng",
            "Wenhao Huang",
            "Lina Chen",
            "Jin Xiao",
            "Qianxi He",
            "Xunzhe Zhou",
            "Lida Chen",
            "Xintao Wang",
            "Yuncheng Huang",
            "Haoning Ye",
            "Zihan Li",
            "Shisong Chen",
            "Yikai Zhang",
            "Zhouhong Gu",
            "Jiaqing Liang",
            "Yanghua Xiao"
        ],
        "published": "2023-09-17T04:18:39Z",
        "summary": "Large language models (LLMs) can understand human instructions, showing their\npotential for pragmatic applications beyond traditional NLP tasks. However,\nthey still struggle with complex instructions, which can be either complex task\ndescriptions that require multiple tasks and constraints, or complex input that\ncontains long context, noise, heterogeneous information and multi-turn format.\nDue to these features, LLMs often ignore semantic constraints from task\ndescriptions, generate incorrect formats, violate length or sample count\nconstraints, and be unfaithful to the input text. Existing benchmarks are\ninsufficient to assess LLMs' ability to understand complex instructions, as\nthey are close-ended and simple. To bridge this gap, we propose CELLO, a\nbenchmark for evaluating LLMs' ability to follow complex instructions\nsystematically. We design eight features for complex instructions and construct\na comprehensive evaluation dataset from real-world scenarios. We also establish\nfour criteria and develop corresponding metrics, as current ones are\ninadequate, biased or too strict and coarse-grained. We compare the performance\nof representative Chinese-oriented and English-oriented models in following\ncomplex instructions through extensive experiments. Resources of CELLO are\npublicly available at https://github.com/Abbey4799/CELLO.",
        "pdf_link": "https://arxiv.org/pdf/2309.09150v2.pdf"
    },
    {
        "title": "RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification",
        "authors": [
            "Hai-Long Nguyen",
            "Thi-Kieu-Trang Pham",
            "Thai-Son Le",
            "Tan-Minh Nguyen",
            "Thi-Hai-Yen Vuong",
            "Ha-Thanh Nguyen"
        ],
        "published": "2023-09-16T18:35:08Z",
        "summary": "In this study, we present a novel and challenging multilabel Vietnamese\ndataset (RMDM) designed to assess the performance of large language models\n(LLMs), in verifying electronic information related to legal contexts, focusing\non fake news as potential input for electronic evidence. The RMDM dataset\ncomprises four labels: real, mis, dis, and mal, representing real information,\nmisinformation, disinformation, and mal-information, respectively. By including\nthese diverse labels, RMDM captures the complexities of differing fake news\ncategories and offers insights into the abilities of different language models\nto handle various types of information that could be part of electronic\nevidence. The dataset consists of a total of 1,556 samples, with 389 samples\nfor each label. Preliminary tests on the dataset using GPT-based and BERT-based\nmodels reveal variations in the models' performance across different labels,\nindicating that the dataset effectively challenges the ability of various\nlanguage models to verify the authenticity of such information. Our findings\nsuggest that verifying electronic information related to legal contexts,\nincluding fake news, remains a difficult problem for language models,\nwarranting further attention from the research community to advance toward more\nreliable AI models for potential legal applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.09071v1.pdf"
    },
    {
        "title": "Rethinking STS and NLI in Large Language Models",
        "authors": [
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "published": "2023-09-16T11:58:39Z",
        "summary": "Recent years have seen the rise of large language models (LLMs), where\npractitioners use task-specific prompts; this was shown to be effective for a\nvariety of tasks. However, when applied to semantic textual similarity (STS)\nand natural language inference (NLI), the effectiveness of LLMs turns out to be\nlimited by low-resource domain accuracy, model overconfidence, and difficulty\nto capture the disagreements between human judgements. With this in mind, here\nwe try to rethink STS and NLI in the era of LLMs. We first evaluate the\nperformance of STS and NLI in the clinical/biomedical domain, and then we\nassess LLMs' predictive confidence and their capability of capturing collective\nhuman opinions. We find that these old problems are still to be properly\naddressed in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08969v2.pdf"
    },
    {
        "title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?",
        "authors": [
            "Xiangru Tang",
            "Yiming Zong",
            "Jason Phang",
            "Yilun Zhao",
            "Wangchunshu Zhou",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "published": "2023-09-16T11:31:58Z",
        "summary": "Despite the remarkable capabilities of Large Language Models (LLMs) like\nGPT-4, producing complex, structured tabular data remains challenging. Our\nstudy assesses LLMs' proficiency in structuring tables and introduces a novel\nfine-tuning method, cognizant of data structures, to bolster their performance.\nWe unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs\n(GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and\nLaTeX formats. Our proposed FormatCoT aids in crafting format-specific\ninstructions from the intended outputs to populate this benchmark. Addressing\nthe gap in task-centered evaluation, we propose two innovative metrics, P-Score\n(Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM\nperformance. Our experiments show that applying our structure-aware fine-tuning\nto LLaMA-7B leads to substantial performance gains, outshining its LLM\ncounterparts across most measures. In-depth error analysis and creating an\nability map across six dimensions -- coverage, formatting, reasoning,\ncomprehension, pragmatics, and hallucination -- highlight areas for future\nenhancements and suggest forthcoming research trajectories. Our code and models\ncan be found at https://github.com/gersteinlab/Struc-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2309.08963v3.pdf"
    },
    {
        "title": "ODSum: New Benchmarks for Open Domain Multi-Document Summarization",
        "authors": [
            "Yijie Zhou",
            "Kejian Shi",
            "Wencai Zhang",
            "Yixin Liu",
            "Yilun Zhao",
            "Arman Cohan"
        ],
        "published": "2023-09-16T11:27:34Z",
        "summary": "Open-domain Multi-Document Summarization (ODMDS) is a critical tool for\ncondensing vast arrays of documents into coherent, concise summaries. With a\nmore inter-related document set, there does not necessarily exist a correct\nanswer for the retrieval, making it hard to measure the retrieving performance.\nWe propose a rule-based method to process query-based document summarization\ndatasets into ODMDS datasets. Based on this method, we introduce a novel\ndataset, ODSum, a sophisticated case with its document index interdependent and\noften interrelated. We tackle ODMDS with the \\textit{retrieve-then-summarize}\nmethod, and the performance of a list of retrievers and summarizers is\ninvestigated. Through extensive experiments, we identify variances in\nevaluation metrics and provide insights into their reliability. We also found\nthat LLMs suffer great performance loss from retrieving errors. We further\nexperimented methods to improve the performance as well as investigate their\nrobustness against imperfect retrieval. We will release our data and code at\nhttps://github.com/yale-nlp/ODSum.",
        "pdf_link": "https://arxiv.org/pdf/2309.08960v1.pdf"
    },
    {
        "title": "PDFTriage: Question Answering over Long, Structured Documents",
        "authors": [
            "Jon Saad-Falcon",
            "Joe Barrow",
            "Alexa Siu",
            "Ani Nenkova",
            "David Seunghyun Yoon",
            "Ryan A. Rossi",
            "Franck Dernoncourt"
        ],
        "published": "2023-09-16T04:29:05Z",
        "summary": "Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.",
        "pdf_link": "https://arxiv.org/pdf/2309.08872v2.pdf"
    },
    {
        "title": "Rethinking Learning Rate Tuning in the Era of Large Language Models",
        "authors": [
            "Hongpeng Jin",
            "Wenqi Wei",
            "Xuyu Wang",
            "Wenbin Zhang",
            "Yanzhao Wu"
        ],
        "published": "2023-09-16T03:37:00Z",
        "summary": "Large Language Models (LLMs) represent the recent success of deep learning in\nachieving remarkable human-like predictive performance. It has become a\nmainstream strategy to leverage fine-tuning to adapt LLMs for various\nreal-world applications due to the prohibitive expenses associated with LLM\ntraining. The learning rate is one of the most important hyperparameters in LLM\nfine-tuning with direct impacts on both fine-tuning efficiency and fine-tuned\nLLM quality. Existing learning rate policies are primarily designed for\ntraining traditional deep neural networks (DNNs), which may not work well for\nLLM fine-tuning. We reassess the research challenges and opportunities of\nlearning rate tuning in the coming era of Large Language Models. This paper\nmakes three original contributions. First, we revisit existing learning rate\npolicies to analyze the critical challenges of learning rate tuning in the era\nof LLMs. Second, we present LRBench++ to benchmark learning rate policies and\nfacilitate learning rate tuning for both traditional DNNs and LLMs. Third, our\nexperimental analysis with LRBench++ demonstrates the key differences between\nLLM fine-tuning and traditional DNN training and validates our analysis.",
        "pdf_link": "https://arxiv.org/pdf/2309.08859v1.pdf"
    },
    {
        "title": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs",
        "authors": [
            "Sarkar Snigdha Sarathi Das",
            "Chirag Shah",
            "Mengting Wan",
            "Jennifer Neville",
            "Longqi Yang",
            "Reid Andersen",
            "Georg Buscher",
            "Tara Safavi"
        ],
        "published": "2023-09-16T00:59:23Z",
        "summary": "The traditional Dialogue State Tracking (DST) problem aims to track user\npreferences and intents in user-agent conversations. While sufficient for\ntask-oriented dialogue systems supporting narrow domain applications, the\nadvent of Large Language Model (LLM)-based chat systems has introduced many\nreal-world intricacies in open-domain dialogues. These intricacies manifest in\nthe form of increased complexity in contextual interactions, extended dialogue\nsessions encompassing a diverse array of topics, and more frequent contextual\nshifts. To handle these intricacies arising from evolving LLM-based chat\nsystems, we propose joint dialogue segmentation and state tracking per segment\nin open-domain dialogue systems. Assuming a zero-shot setting appropriate to a\ntrue open-domain dialogue system, we propose S3-DST, a structured prompting\ntechnique that harnesses Pre-Analytical Recollection, a novel grounding\nmechanism we designed for improving long context tracking. To demonstrate the\nefficacy of our proposed approach in joint segmentation and state tracking, we\nevaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as\nwell as publicly available DST and segmentation datasets. Across all datasets\nand settings, S3-DST consistently outperforms the state-of-the-art,\ndemonstrating its potency and robustness the next generation of LLM-based chat\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2309.08827v1.pdf"
    },
    {
        "title": "Fake News Detectors are Biased against Texts Generated by Large Language Models",
        "authors": [
            "Jinyan Su",
            "Terry Yue Zhuo",
            "Jonibek Mansurov",
            "Di Wang",
            "Preslav Nakov"
        ],
        "published": "2023-09-15T18:04:40Z",
        "summary": "The spread of fake news has emerged as a critical challenge, undermining\ntrust and posing threats to society. In the era of Large Language Models\n(LLMs), the capability to generate believable fake content has intensified\nthese concerns. In this study, we present a novel paradigm to evaluate fake\nnews detectors in scenarios involving both human-written and LLM-generated\nmisinformation. Intriguingly, our findings reveal a significant bias in many\nexisting detectors: they are more prone to flagging LLM-generated content as\nfake news while often misclassifying human-written fake news as genuine. This\nunexpected bias appears to arise from distinct linguistic patterns inherent to\nLLM outputs. To address this, we introduce a mitigation strategy that leverages\nadversarial training with LLM-paraphrased genuine news. The resulting model\nyielded marked improvements in detection accuracy for both human and\nLLM-generated news. To further catalyze research in this domain, we release two\ncomprehensive datasets, \\texttt{GossipCop++} and \\texttt{PolitiFact++}, thus\namalgamating human-validated articles with LLM-generated fake and real news.",
        "pdf_link": "https://arxiv.org/pdf/2309.08674v1.pdf"
    },
    {
        "title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
        "authors": [
            "Chen Cecilia Liu",
            "Fajri Koto",
            "Timothy Baldwin",
            "Iryna Gurevych"
        ],
        "published": "2023-09-15T17:45:28Z",
        "summary": "Large language models (LLMs) are highly adept at question answering and\nreasoning tasks, but when reasoning in a situational context, human\nexpectations vary depending on the relevant cultural common ground. As\nlanguages are associated with diverse cultures, LLMs should also be\nculturally-diverse reasoners. In this paper, we study the ability of a wide\nrange of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and\nsayings in a conversational context. Our experiments reveal that: (1) mLLMs\n\"know\" limited proverbs and memorizing proverbs does not mean understanding\nthem within a conversational context; (2) mLLMs struggle to reason with\nfigurative proverbs and sayings, and when asked to select the wrong answer\n(instead of asking it to select the correct answer); and (3) there is a\n\"culture gap\" in mLLMs when reasoning about proverbs and sayings translated\nfrom other languages. We construct and release our evaluation dataset MAPS\n(MulticultrAl Proverbs and Sayings) for proverb understanding with\nconversational context for six different languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.08591v2.pdf"
    },
    {
        "title": "Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West",
        "authors": [
            "Khyati Khandelwal",
            "Manuel Tonneau",
            "Andrew M. Bean",
            "Hannah Rose Kirk",
            "Scott A. Hale"
        ],
        "published": "2023-09-15T17:38:41Z",
        "summary": "Large Language Models (LLMs), now used daily by millions of users, can encode\nsocietal biases, exposing their users to representational harms. A large body\nof scholarship on LLM bias exists but it predominantly adopts a Western-centric\nframe and attends comparatively less to bias levels and potential harms in the\nGlobal South. In this paper, we quantify stereotypical bias in popular LLMs\naccording to an Indian-centric frame and compare bias levels between the Indian\nand Western contexts. To do this, we develop a novel dataset which we call\nIndian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and\nanti-stereotypical examples for caste and religion contexts. We find that the\nmajority of LLMs tested are strongly biased towards stereotypes in the Indian\ncontext, especially as compared to the Western context. We finally investigate\nInstruction Prompting as a simple intervention to mitigate such bias and find\nthat it significantly reduces both stereotypical and anti-stereotypical biases\nin the majority of cases for GPT-3.5. The findings of this work highlight the\nneed for including more diverse voices when evaluating LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08573v1.pdf"
    },
    {
        "title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata",
        "authors": [
            "Bohui Zhang",
            "Ioannis Reklos",
            "Nitisha Jain",
            "Albert Mero√±o Pe√±uela",
            "Elena Simperl"
        ],
        "published": "2023-09-15T15:51:14Z",
        "summary": "In this work, we explore the use of Large Language Models (LLMs) for\nknowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge.\nFor this task, given subject and relation pairs sourced from Wikidata, we\nutilize pre-trained LLMs to produce the relevant objects in string format and\nlink them to their respective Wikidata QIDs. We developed a pipeline using LLMs\nfor Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata\nentity mapping. The method achieved a macro-averaged F1-score of 0.701 across\nthe properties, with the scores varying from 1.00 to 0.328. These results\ndemonstrate that the knowledge of LLMs varies significantly depending on the\ndomain and that further experimentation is required to determine the\ncircumstances under which LLMs can be used for automatic Knowledge Base (e.g.,\nWikidata) completion and correction. The investigation of the results also\nsuggests the promising contribution of LLMs in collaborative knowledge\nengineering. LLMKE won Track 2 of the challenge. The implementation is\navailable at https://github.com/bohuizhang/LLMKE.",
        "pdf_link": "https://arxiv.org/pdf/2309.08491v1.pdf"
    },
    {
        "title": "Adversarial Attacks on Tables with Entity Swap",
        "authors": [
            "Aneta Koleva",
            "Martin Ringsquandl",
            "Volker Tresp"
        ],
        "published": "2023-09-15T15:03:33Z",
        "summary": "The capabilities of large language models (LLMs) have been successfully\napplied in the context of table representation learning. The recently proposed\ntabular language models have reported state-of-the-art results across various\ntasks for table interpretation. However, a closer look into the datasets\ncommonly used for evaluation reveals an entity leakage from the train set into\nthe test set. Motivated by this observation, we explore adversarial attacks\nthat represent a more realistic inference setup. Adversarial attacks on text\nhave been shown to greatly affect the performance of LLMs, but currently, there\nare no attacks targeting tabular language models. In this paper, we propose an\nevasive entity-swap attack for the column type annotation (CTA) task. Our CTA\nattack is the first black-box attack on tables, where we employ a\nsimilarity-based sampling strategy to generate adversarial examples. The\nexperimental results show that the proposed attack generates up to a 70% drop\nin performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.08650v1.pdf"
    },
    {
        "title": "Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases",
        "authors": [
            "Yiheng Shu",
            "Zhiwei Yu"
        ],
        "published": "2023-09-15T12:06:45Z",
        "summary": "Language models (LMs) have already demonstrated remarkable abilities in\nunderstanding and generating both natural and formal language. Despite these\nadvances, their integration with real-world environments such as large-scale\nknowledge bases (KBs) remains an underdeveloped area, affecting applications\nsuch as semantic parsing and indulging in \"hallucinated\" information. This\npaper is an experimental investigation aimed at uncovering the robustness\nchallenges that LMs encounter when tasked with knowledge base question\nanswering (KBQA). The investigation covers scenarios with inconsistent data\ndistribution between training and inference, such as generalization to unseen\ndomains, adaptation to various language variations, and transferability across\ndifferent datasets. Our comprehensive experiments reveal that even when\nemployed with our proposed data augmentation techniques, advanced small and\nlarge language models exhibit poor performance in various dimensions. While the\nLM is a promising technology, the robustness of the current form in dealing\nwith complex environments is fragile and of limited practicality because of the\ndata distribution issue. This calls for future research on data collection and\nLM learning paradims.",
        "pdf_link": "https://arxiv.org/pdf/2309.08345v3.pdf"
    },
    {
        "title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending",
        "authors": [
            "Shiyi Zhu",
            "Jing Ye",
            "Wei Jiang",
            "Siqiao Xue",
            "Qi Zhang",
            "Yifan Wu",
            "Jianguo Li"
        ],
        "published": "2023-09-15T09:36:51Z",
        "summary": "Self-attention and position embedding are two key modules in\ntransformer-based Large Language Models (LLMs). However, the potential\nrelationship between them is far from well studied, especially for long context\nwindow extending. In fact, anomalous behaviors harming long context\nextrapolation exist between Rotary Position Embedding (RoPE) and vanilla\nself-attention unveiled by our work. To address this issue, we propose a novel\nattention mechanism, CoCA (Collinear Constrained Attention). Specifically, we\nenforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE\nand self-attention. While only adding minimal computational and spatial\ncomplexity, this integration significantly enhances long context window\nextrapolation ability. We provide an optimized implementation, making it a\ndrop-in replacement for any existing transformer-based models. Extensive\nexperiments show that CoCA performs extraordinarily well in extending context\nwindows. A CoCA-based GPT model, trained with a context length of 512, can\nseamlessly extend the context window up to 32K (60$\\times$), without any\nfine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve\nextrapolation up to 32K within only 2K training length. Our code is publicly\navailable at: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
        "pdf_link": "https://arxiv.org/pdf/2309.08646v3.pdf"
    },
    {
        "title": "Investigating Answerability of LLMs for Long-Form Question Answering",
        "authors": [
            "Meghana Moorthy Bhat",
            "Rui Meng",
            "Ye Liu",
            "Yingbo Zhou",
            "Semih Yavuz"
        ],
        "published": "2023-09-15T07:22:56Z",
        "summary": "As we embark on a new era of LLMs, it becomes increasingly crucial to\nunderstand their capabilities, limitations, and differences. Toward making\nfurther progress in this direction, we strive to build a deeper understanding\nof the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective\nopen-source LLMs and their distilled counterparts. To this end, we specifically\nfocus on long-form question answering (LFQA) because it has several practical\nand impactful applications (e.g., troubleshooting, customer service, etc.) yet\nis still understudied and challenging for LLMs. We propose a\nquestion-generation method from abstractive summaries and show that generating\nfollow-up questions from summaries of long documents can create a challenging\nsetting for LLMs to reason and infer from long contexts. Our experimental\nresults confirm that: (1) our proposed method of generating questions from\nabstractive summaries pose a challenging setup for LLMs and shows performance\ngaps between LLMs like ChatGPT and open-source LLMs (Alpaca, Llama) (2)\nopen-source LLMs exhibit decreased reliance on context for generated questions\nfrom the original document, but their generation capabilities drop\nsignificantly on generated questions from summaries -- especially for longer\ncontexts (>1024 tokens)",
        "pdf_link": "https://arxiv.org/pdf/2309.08210v1.pdf"
    },
    {
        "title": "Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level",
        "authors": [
            "Jingzhe Ding",
            "Yan Cen",
            "Xinyuan Wei"
        ],
        "published": "2023-09-15T06:13:06Z",
        "summary": "Our work demonstrates that large language model (LLM) pre-trained on texts\ncan not only solve pure math word problems, but also physics word problems,\nwhose solution requires calculation and inference based on prior physical\nknowledge. We collect and annotate the first physics word problem\ndataset-PhysQA, which contains over 1000 junior high school physics word\nproblems (covering Kinematics, Mass&Density, Mechanics, Heat, Electricity).\nThen we use OpenAI' s GPT3.5 to generate the answer of these problems and found\nthat GPT3.5 could automatically solve 49.3% of the problems through zero-shot\nlearning and 73.2% through few-shot learning. This result demonstrates that by\nusing similar problems and their answers as prompt, LLM could solve elementary\nphysics word problems approaching human level performance. In addition to\nsolving problems, GPT3.5 can also summarize the knowledge or topics covered by\nthe problems, provide relevant explanations, and generate new physics word\nproblems based on the input. Our work is the first research to focus on the\nautomatic solving, explanation, and generation of physics word problems across\nvarious types and scenarios, and we achieve an acceptable and state-of-the-art\naccuracy. This underscores the potential of LLMs for further applications in\nsecondary education.",
        "pdf_link": "https://arxiv.org/pdf/2309.08182v2.pdf"
    },
    {
        "title": "FedJudge: Federated Legal Large Language Model",
        "authors": [
            "Linan Yue",
            "Qi Liu",
            "Yichao Du",
            "Weibo Gao",
            "Ye Liu",
            "Fangzhou Yao"
        ],
        "published": "2023-09-15T05:45:44Z",
        "summary": "Large Language Models (LLMs) have gained prominence in the field of Legal\nIntelligence, offering potential applications in assisting legal professionals\nand laymen. However, the centralized training of these Legal LLMs raises data\nprivacy concerns, as legal data is distributed among various institutions\ncontaining sensitive individual information. This paper addresses this\nchallenge by exploring the integration of Legal LLMs with Federated Learning\n(FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on\ndevices or clients, and their parameters are aggregated and distributed on a\ncentral server, ensuring data privacy without directly sharing raw data.\nHowever, computation and communication overheads hinder the full fine-tuning of\nLLMs under the FL setting. Moreover, the distribution shift of legal data\nreduces the effectiveness of FL methods. To this end, in this paper, we propose\nthe first Federated Legal Large Language Model (FedJudge) framework, which\nfine-tunes Legal LLMs efficiently and effectively. Specifically, FedJudge\nutilizes parameter-efficient fine-tuning methods to update only a few\nadditional parameters during the FL training. Besides, we explore the continual\nlearning methods to preserve the global model's important parameters when\ntraining local clients to mitigate the problem of data shifts. Extensive\nexperimental results on three real-world datasets clearly validate the\neffectiveness of FedJudge. Code is released at\nhttps://github.com/yuelinan/FedJudge.",
        "pdf_link": "https://arxiv.org/pdf/2309.08173v3.pdf"
    },
    {
        "title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
        "authors": [
            "Kaixin Ma",
            "Hongming Zhang",
            "Hongwei Wang",
            "Xiaoman Pan",
            "Wenhao Yu",
            "Dong Yu"
        ],
        "published": "2023-09-15T05:44:08Z",
        "summary": "Large language models (LLMs) have been successfully adapted for interactive\ndecision-making tasks like web navigation. While achieving decent performance,\nprevious methods implicitly assume a forward-only execution mode for the model,\nwhere they only provide oracle trajectories as in-context examples to guide the\nmodel on how to reason in the environment. Consequently, the model could not\nhandle more challenging scenarios not covered in the in-context examples, e.g.,\nmistakes, leading to sub-optimal performance. To address this issue, we propose\nto model the interactive task as state space exploration, where the LLM agent\ntransitions among a pre-defined set of states by performing actions to complete\nthe task. This formulation enables flexible backtracking, allowing the model to\nrecover from errors easily. We evaluate our proposed LLM Agent with State-Space\nExploRation (LASER) on both the WebShop task and amazon.com. Experimental\nresults show that LASER significantly outperforms previous methods and closes\nthe gap with human performance on the web navigation task.",
        "pdf_link": "https://arxiv.org/pdf/2309.08172v2.pdf"
    },
    {
        "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding",
        "authors": [
            "Jun Zhang",
            "Jue Wang",
            "Huan Li",
            "Lidan Shou",
            "Ke Chen",
            "Gang Chen",
            "Sharad Mehrotra"
        ],
        "published": "2023-09-15T05:34:32Z",
        "summary": "We present a novel inference scheme, self-speculative decoding, for\naccelerating Large Language Models (LLMs) without the need for an auxiliary\nmodel. This approach is characterized by a two-stage process: drafting and\nverification. The drafting stage generates draft tokens at a slightly lower\nquality but more quickly, which is achieved by selectively skipping certain\nintermediate layers during drafting Subsequently, the verification stage\nemploys the original LLM to validate those draft output tokens in one forward\npass. This process ensures the final output remains identical to that produced\nby the unaltered LLM, thereby maintaining output quality. The proposed method\nrequires no additional neural network training and no extra memory footprint,\nmaking it a plug-and-play and cost-effective solution for inference\nacceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a\nspeedup up to 1.73$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2309.08168v1.pdf"
    },
    {
        "title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
        "authors": [
            "Akshat Gupta",
            "Xiaoyang Song",
            "Gopala Anumanchipalli"
        ],
        "published": "2023-09-15T05:19:39Z",
        "summary": "As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.08163v2.pdf"
    },
    {
        "title": "Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",
        "authors": [
            "Chirag Shah",
            "Ryen W. White",
            "Reid Andersen",
            "Georg Buscher",
            "Scott Counts",
            "Sarkar Snigdha Sarathi Das",
            "Ali Montazer",
            "Sathish Manivannan",
            "Jennifer Neville",
            "Xiaochuan Ni",
            "Nagu Rangan",
            "Tara Safavi",
            "Siddharth Suri",
            "Mengting Wan",
            "Leijie Wang",
            "Longqi Yang"
        ],
        "published": "2023-09-14T20:46:48Z",
        "summary": "Log data can reveal valuable information about how users interact with Web\nsearch services, what they want, and how satisfied they are. However, analyzing\nuser intents in log data is not easy, especially for emerging forms of Web\nsearch such as AI-driven chat. To understand user intents from log data, we\nneed a way to label them with meaningful categories that capture their\ndiversity and dynamics. Existing methods rely on manual or machine-learned\nlabeling, which are either expensive or inflexible for large and dynamic\ndatasets. We propose a novel solution using large language models (LLMs), which\ncan generate rich and relevant concepts, descriptions, and examples for user\nintents. However, using LLMs to generate a user intent taxonomy and apply it\nfor log analysis can be problematic for two main reasons: (1) such a taxonomy\nis not externally validated; and (2) there may be an undesirable feedback loop.\nTo address this, we propose a new methodology with human experts and assessors\nto verify the quality of the LLM-generated taxonomy. We also present an\nend-to-end pipeline that uses an LLM with human-in-the-loop to produce, refine,\nand apply labels for user intent analysis in log data. We demonstrate its\neffectiveness by uncovering new insights into user intents from search and chat\nlogs from the Microsoft Bing commercial search engine. The proposed work's\nnovelty stems from the method for generating purpose-driven user intent\ntaxonomies with strong validation. This method not only helps remove\nmethodological and practical bottlenecks from intent-focused research, but also\nprovides a new framework for generating, validating, and applying other kinds\nof taxonomies in a scalable and adaptable way with minimal human effort.",
        "pdf_link": "https://arxiv.org/pdf/2309.13063v2.pdf"
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Xiaojian Ma",
            "Kaikai An",
            "Liang Chen",
            "Zixuan Liu",
            "Sheng Wang",
            "Wenjuan Han",
            "Baobao Chang"
        ],
        "published": "2023-09-14T17:59:17Z",
        "summary": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC",
        "pdf_link": "https://arxiv.org/pdf/2309.07915v3.pdf"
    },
    {
        "title": "Ambiguity-Aware In-Context Learning with Large Language Models",
        "authors": [
            "Lingyu Gao",
            "Aditi Chaudhary",
            "Krishna Srinivasan",
            "Kazuma Hashimoto",
            "Karthik Raman",
            "Michael Bendersky"
        ],
        "published": "2023-09-14T17:48:34Z",
        "summary": "In-context learning (ICL) i.e. showing LLMs only a few task-specific\ndemonstrations has led to downstream gains with no task-specific fine-tuning\nrequired. However, LLMs are sensitive to the choice of prompts, and therefore a\ncrucial research question is how to select good demonstrations for ICL. One\neffective strategy is leveraging semantic similarity between the ICL\ndemonstrations and test inputs by using a text retriever, which however is\nsub-optimal as that does not consider the LLM's existing knowledge about that\ntask. From prior work (Lyu et al., 2023), we already know that labels paired\nwith the demonstrations bias the model predictions. This leads us to our\nhypothesis whether considering LLM's existing knowledge about the task,\nespecially with respect to the output label space can help in a better\ndemonstration selection strategy. Through extensive experimentation on three\ntext classification tasks, we find that it is beneficial to not only choose\nsemantically similar ICL demonstrations but also to choose those demonstrations\nthat help resolve the inherent label ambiguity surrounding the test example.\nInterestingly, we find that including demonstrations that the LLM previously\nmis-classified and also fall on the test example's decision boundary, brings\nthe most performance gain.",
        "pdf_link": "https://arxiv.org/pdf/2309.07900v2.pdf"
    },
    {
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
        "authors": [
            "Federico Bianchi",
            "Mirac Suzgun",
            "Giuseppe Attanasio",
            "Paul R√∂ttger",
            "Dan Jurafsky",
            "Tatsunori Hashimoto",
            "James Zou"
        ],
        "published": "2023-09-14T17:23:37Z",
        "summary": "Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.",
        "pdf_link": "https://arxiv.org/pdf/2309.07875v3.pdf"
    },
    {
        "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
        "authors": [
            "Huayang Li",
            "Siheng Li",
            "Deng Cai",
            "Longyue Wang",
            "Lemao Liu",
            "Taro Watanabe",
            "Yujiu Yang",
            "Shuming Shi"
        ],
        "published": "2023-09-14T15:34:01Z",
        "summary": "Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.",
        "pdf_link": "https://arxiv.org/pdf/2309.08637v4.pdf"
    },
    {
        "title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
        "authors": [
            "Shentong Mo",
            "Miao Xin"
        ],
        "published": "2023-09-14T13:14:51Z",
        "summary": "While the recently introduced Tree of Thoughts (ToT) has heralded\nadvancements in allowing Large Language Models (LLMs) to reason through\nforesight and backtracking for global decision-making, it has overlooked the\ninherent local uncertainties in intermediate decision points or \"thoughts\".\nThese local uncertainties, intrinsic to LLMs given their potential for diverse\nresponses, remain a significant concern in the reasoning process. Addressing\nthis pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a\nreasoning framework tailored for LLMs. Our TouT effectively leverages Monte\nCarlo Dropout to quantify uncertainty scores associated with LLMs' diverse\nlocal responses at these intermediate steps. By marrying this local uncertainty\nquantification with global search algorithms, TouT enhances the model's\nprecision in response generation. We substantiate our approach with rigorous\nexperiments on two demanding planning tasks: Game of 24 and Mini Crosswords.\nThe empirical evidence underscores TouT's superiority over both ToT and\nchain-of-thought prompting methods.",
        "pdf_link": "https://arxiv.org/pdf/2309.07694v1.pdf"
    },
    {
        "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text",
        "authors": [
            "Mahdi Dhaini",
            "Wessel Poelman",
            "Ege Erdogan"
        ],
        "published": "2023-09-14T13:05:20Z",
        "summary": "While recent advancements in the capabilities and widespread accessibility of\ngenerative language models, such as ChatGPT (OpenAI, 2022), have brought about\nvarious benefits by generating fluent human-like text, the task of\ndistinguishing between human- and large language model (LLM) generated text has\nemerged as a crucial problem. These models can potentially deceive by\ngenerating artificial text that appears to be human-generated. This issue is\nparticularly significant in domains such as law, education, and science, where\nensuring the integrity of text is of the utmost importance. This survey\nprovides an overview of the current approaches employed to differentiate\nbetween texts generated by humans and ChatGPT. We present an account of the\ndifferent datasets constructed for detecting ChatGPT-generated text, the\nvarious methods utilized, what qualitative analyses into the characteristics of\nhuman versus ChatGPT-generated text have been performed, and finally, summarize\nour findings into general insights",
        "pdf_link": "https://arxiv.org/pdf/2309.07689v1.pdf"
    },
    {
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
        "authors": [
            "Rishav Hada",
            "Varun Gumma",
            "Adrian de Wynter",
            "Harshita Diddee",
            "Mohamed Ahmed",
            "Monojit Choudhury",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023-09-14T06:41:58Z",
        "summary": "Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.07462v2.pdf"
    },
    {
        "title": "ChatGPT MT: Competitive for High- (but not Low-) Resource Languages",
        "authors": [
            "Nathaniel R. Robinson",
            "Perez Ogayo",
            "David R. Mortensen",
            "Graham Neubig"
        ],
        "published": "2023-09-14T04:36:00Z",
        "summary": "Large language models (LLMs) implicitly learn to perform a range of language\ntasks, including machine translation (MT). Previous studies explore aspects of\nLLMs' MT capabilities. However, there exist a wide variety of languages for\nwhich recent LLM MT performance has never before been evaluated. Without\npublished experimental evidence on the matter, it is difficult for speakers of\nthe world's diverse languages to know how and whether they can use LLMs for\ntheir languages. We present the first experimental evidence for an expansive\nset of 204 languages, along with MT cost analysis, using the FLORES-200\nbenchmark. Trends reveal that GPT models approach or exceed traditional MT\nmodel performance for some high-resource languages (HRLs) but consistently lag\nfor low-resource languages (LRLs), under-performing traditional MT for 84.1% of\nlanguages we covered. Our analysis reveals that a language's resource level is\nthe most important feature in determining ChatGPT's relative ability to\ntranslate it, and suggests that ChatGPT is especially disadvantaged for LRLs\nand African languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.07423v1.pdf"
    },
    {
        "title": "An Assessment of ChatGPT on Log Data",
        "authors": [
            "Priyanka Mudgal",
            "Rita Wouhaybi"
        ],
        "published": "2023-09-14T04:09:27Z",
        "summary": "Recent development of large language models (LLMs), such as ChatGPT has been\nwidely applied to a wide range of software engineering tasks. Many papers have\nreported their analysis on the potential advantages and limitations of ChatGPT\nfor writing code, summarization, text generation, etc. However, the analysis of\nthe current state of ChatGPT for log processing has received little attention.\nLogs generated by large-scale software systems are complex and hard to\nunderstand. Despite their complexity, they provide crucial information for\nsubject matter experts to understand the system status and diagnose problems of\nthe systems. In this paper, we investigate the current capabilities of ChatGPT\nto perform several interesting tasks on log data, while also trying to identify\nits main shortcomings. Our findings show that the performance of the current\nversion of ChatGPT for log processing is limited, with a lack of consistency in\nresponses and scalability issues. We also outline our views on how we perceive\nthe role of LLMs in the log processing discipline and possible next steps to\nimprove the current capabilities of ChatGPT and the future LLMs in this area.\nWe believe our work can contribute to future academic research to address the\nidentified issues.",
        "pdf_link": "https://arxiv.org/pdf/2309.07938v1.pdf"
    },
    {
        "title": "Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos",
        "authors": [
            "Fen Fang",
            "Yun Liu",
            "Ali Koksal",
            "Qianli Xu",
            "Joo-Hwee Lim"
        ],
        "published": "2023-09-14T03:25:37Z",
        "summary": "A key challenge with procedure planning in instructional videos lies in how\nto handle a large decision space consisting of a multitude of action types that\nbelong to various tasks. To understand real-world video content, an AI agent\nmust proficiently discern these action types (e.g., pour milk, pour water, open\nlid, close lid, etc.) based on brief visual observation. Moreover, it must\nadeptly capture the intricate semantic relation of the action types and task\ngoals, along with the variable action sequences. Recently, notable progress has\nbeen made via the integration of diffusion models and visual representation\nlearning to address the challenge. However, existing models employ rudimentary\nmechanisms to utilize task information to manage the decision space. To\novercome this limitation, we introduce a simple yet effective enhancement - a\nmasked diffusion model. The introduced mask acts akin to a task-oriented\nattention filter, enabling the diffusion/denoising process to concentrate on a\nsubset of action types. Furthermore, to bolster the accuracy of task\nclassification, we harness more potent visual representation learning\ntechniques. In particular, we learn a joint visual-text embedding, where a text\nembedding is generated by prompting a pre-trained vision-language model to\nfocus on human actions. We evaluate the method on three public datasets and\nachieve state-of-the-art performance on multiple metrics. Code is available at\nhttps://github.com/ffzzy840304/Masked-PDPP.",
        "pdf_link": "https://arxiv.org/pdf/2309.07409v1.pdf"
    },
    {
        "title": "Less is More for Long Document Summary Evaluation by LLMs",
        "authors": [
            "Yunshu Wu",
            "Hayate Iso",
            "Pouya Pezeshkpour",
            "Nikita Bhutani",
            "Estevam Hruschka"
        ],
        "published": "2023-09-14T01:59:15Z",
        "summary": "Large Language Models (LLMs) have shown promising performance in summary\nevaluation tasks, yet they face challenges such as high computational costs and\nthe Lost-in-the-Middle problem where important information in the middle of\nlong documents is often overlooked. To address these issues, this paper\nintroduces a novel approach, Extract-then-Evaluate, which involves extracting\nkey sentences from a long source document and then evaluating the summary by\nprompting LLMs. The results reveal that the proposed method not only\nsignificantly reduces evaluation costs but also exhibits a higher correlation\nwith human evaluations. Furthermore, we provide practical recommendations for\noptimal document length and sentence extraction methods, contributing to the\ndevelopment of cost-effective yet more accurate methods for LLM-based text\ngeneration evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2309.07382v2.pdf"
    },
    {
        "title": "In-Contextual Gender Bias Suppression for Large Language Models",
        "authors": [
            "Daisuke Oba",
            "Masahiro Kaneko",
            "Danushka Bollegala"
        ],
        "published": "2023-09-13T18:39:08Z",
        "summary": "Despite their impressive performance in a wide range of NLP tasks, Large\nLanguage Models (LLMs) have been reported to encode worrying-levels of gender\nbiases. Prior work has proposed debiasing methods that require human labelled\nexamples, data augmentation and fine-tuning of LLMs, which are computationally\ncostly. Moreover, one might not even have access to the model parameters for\nperforming debiasing such as in the case of closed LLMs such as GPT-4. To\naddress this challenge, we propose bias suppression that prevents biased\ngenerations of LLMs by simply providing textual preambles constructed from\nmanually designed templates and real-world statistics, without accessing to\nmodel parameters. We show that, using CrowsPairs dataset, our textual preambles\ncovering counterfactual statements can suppress gender biases in English LLMs\nsuch as LLaMA2. Moreover, we find that gender-neutral descriptions of\ngender-biased objects can also suppress their gender biases. Moreover, we show\nthat bias suppression has acceptable adverse effect on downstream task\nperformance with HellaSwag and COPA.",
        "pdf_link": "https://arxiv.org/pdf/2309.07251v2.pdf"
    },
    {
        "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Jinjing Zhao",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "published": "2023-09-13T17:59:09Z",
        "summary": "Large language models (LLMs) often demonstrate inconsistencies with human\npreferences. Previous research typically gathered human preference data and\nthen aligned the pre-trained models using reinforcement learning or instruction\ntuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without\nrequiring alignment data is more appealing. This work explores the potential of\nthe latter setting. We discover that by integrating self-evaluation and rewind\nmechanisms, unaligned LLMs can directly produce responses consistent with human\npreferences via self-boosting. We introduce a novel inference method,\nRewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to\nevaluate their own generation and use the evaluation results to guide rewind\nand generation for AI safety. Notably, RAIN operates without the need of extra\ndata for model alignment and abstains from any training, gradient computation,\nor parameter updates. Experimental results evaluated by GPT-4 and humans\ndemonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the\nharmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while\nmaintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the\ntruthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
        "pdf_link": "https://arxiv.org/pdf/2309.07124v2.pdf"
    },
    {
        "title": "Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding",
        "authors": [
            "Rico Sennrich",
            "Jannis Vamvas",
            "Alireza Mohammadshahi"
        ],
        "published": "2023-09-13T17:15:27Z",
        "summary": "Hallucinations and off-target translation remain unsolved problems in MT,\nespecially for low-resource languages and massively multilingual models. In\nthis paper, we introduce two related methods to mitigate these failure cases\nwith a modified decoding objective, without either requiring retraining or\nexternal models. In source-contrastive decoding, we search for a translation\nthat is probable given the correct input, but improbable given a random input\nsegment. In language-contrastive decoding, we search for a translation that is\nprobable, but improbable given the wrong language indicator token. Experiments\non the massively multilingual models M2M-100 (418M) and SMaLL-100 show that\nthese methods suppress hallucinations and off-target translations, reducing the\nnumber of translations with segment-level chrF2 below 10 by 67-83% on average,\nand the number of translations with oscillatory hallucinations by 75-92% on\naverage, across 57 tested translation directions. In a proof of concept on\nout-of-English translation, we also show that we can suppress off-target\ntranslations with large language models. We release our source code at\nhttps://github.com/ZurichNLP/ContraDecode.",
        "pdf_link": "https://arxiv.org/pdf/2309.07098v2.pdf"
    },
    {
        "title": "SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions",
        "authors": [
            "Zhexin Zhang",
            "Leqi Lei",
            "Lindong Wu",
            "Rui Sun",
            "Yongkang Huang",
            "Chong Long",
            "Xiao Liu",
            "Xuanyu Lei",
            "Jie Tang",
            "Minlie Huang"
        ],
        "published": "2023-09-13T15:56:50Z",
        "summary": "With the rapid development of Large Language Models (LLMs), increasing\nattention has been paid to their safety concerns. Consequently, evaluating the\nsafety of LLMs has become an essential task for facilitating the broad\napplications of LLMs. Nevertheless, the absence of comprehensive safety\nevaluation benchmarks poses a significant impediment to effectively assess and\nenhance the safety of LLMs. In this work, we present SafetyBench, a\ncomprehensive benchmark for evaluating the safety of LLMs, which comprises\n11,435 diverse multiple choice questions spanning across 7 distinct categories\nof safety concerns. Notably, SafetyBench also incorporates both Chinese and\nEnglish data, facilitating the evaluation in both languages. Our extensive\ntests over 25 popular Chinese and English LLMs in both zero-shot and few-shot\nsettings reveal a substantial performance advantage for GPT-4 over its\ncounterparts, and there is still significant room for improving the safety of\ncurrent LLMs. We believe SafetyBench will enable fast and comprehensive\nevaluation of LLMs' safety, and foster the development of safer LLMs. Data and\nevaluation guidelines are available at https://github.com/thu-coai/SafetyBench.\nSubmission entrance and leaderboard are available at\nhttps://llmbench.ai/safety.",
        "pdf_link": "https://arxiv.org/pdf/2309.07045v1.pdf"
    },
    {
        "title": "Generative AI",
        "authors": [
            "Stefan Feuerriegel",
            "Jochen Hartmann",
            "Christian Janiesch",
            "Patrick Zschech"
        ],
        "published": "2023-09-13T08:21:59Z",
        "summary": "The term \"generative AI\" refers to computational techniques that are capable\nof generating seemingly new, meaningful content such as text, images, or audio\nfrom training data. The widespread diffusion of this technology with examples\nsuch as Dall-E 2, GPT-4, and Copilot is currently revolutionizing the way we\nwork and communicate with each other. In this article, we provide a\nconceptualization of generative AI as an entity in socio-technical systems and\nprovide examples of models, systems, and applications. Based on that, we\nintroduce limitations of current generative AI and provide an agenda for\nBusiness & Information Systems Engineering (BISE) research. Different from\nprevious works, we focus on generative AI in the context of information\nsystems, and, to this end, we discuss several opportunities and challenges that\nare unique to the BISE community and make suggestions for impactful directions\nfor BISE research.",
        "pdf_link": "https://arxiv.org/pdf/2309.07930v1.pdf"
    },
    {
        "title": "Scaled Prompt-Tuning for Few-Shot Natural Language Generation",
        "authors": [
            "Ting Hu",
            "Christoph Meinel",
            "Haojin Yang"
        ],
        "published": "2023-09-13T07:12:31Z",
        "summary": "The increasingly Large Language Models (LLMs) demonstrate stronger language\nunderstanding and generation capabilities, while the memory demand and\ncomputation cost of fine-tuning LLMs on downstream tasks are non-negligible.\nBesides, fine-tuning generally requires a certain amount of data from\nindividual tasks whilst data collection cost is another issue to consider in\nreal-world applications. In this work, we focus on Parameter-Efficient\nFine-Tuning (PEFT) methods for few-shot Natural Language Generation (NLG),\nwhich freeze most parameters in LLMs and tune a small subset of parameters in\nfew-shot cases so that memory footprint, training cost, and labeling cost are\nreduced while maintaining or even improving the performance. We propose a\nScaled Prompt-Tuning (SPT) method which surpasses conventional PT with better\nperformance and generalization ability but without an obvious increase in\ntraining cost. Further study on intermediate SPT suggests the superior\ntransferability of SPT in few-shot scenarios, providing a recipe for\ndata-deficient and computation-limited circumstances. Moreover, a comprehensive\ncomparison of existing PEFT methods reveals that certain approaches exhibiting\ndecent performance with modest training cost such as Prefix-Tuning in prior\nstudy could struggle in few-shot NLG tasks, especially on challenging datasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.06759v1.pdf"
    },
    {
        "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",
        "authors": [
            "Siyao Zhang",
            "Daocheng Fu",
            "Zhao Zhang",
            "Bin Yu",
            "Pinlong Cai"
        ],
        "published": "2023-09-13T04:47:43Z",
        "summary": "With the promotion of chatgpt to the public, Large language models indeed\nshowcase remarkable common sense, reasoning, and planning skills, frequently\nproviding insightful guidance. These capabilities hold significant promise for\ntheir application in urban traffic management and control. However, LLMs\nstruggle with addressing traffic issues, especially processing numerical data\nand interacting with simulations, limiting their potential in solving\ntraffic-related challenges. In parallel, specialized traffic foundation models\nexist but are typically designed for specific tasks with limited input-output\ninteractions. Combining these models with LLMs presents an opportunity to\nenhance their capacity for tackling complex traffic-related problems and\nproviding insightful suggestions. To bridge this gap, we present TrafficGPT, a\nfusion of ChatGPT and traffic foundation models. This integration yields the\nfollowing key enhancements: 1) empowering ChatGPT with the capacity to view,\nanalyze, process traffic data, and provide insightful decision support for\nurban transportation system management; 2) facilitating the intelligent\ndeconstruction of broad and complex tasks and sequential utilization of traffic\nfoundation models for their gradual completion; 3) aiding human decision-making\nin traffic control through natural language dialogues; and 4) enabling\ninteractive feedback and solicitation of revised outcomes. By seamlessly\nintertwining large language model and traffic expertise, TrafficGPT not only\nadvances traffic management but also offers a novel approach to leveraging AI\ncapabilities in this domain. The TrafficGPT demo can be found in\nhttps://github.com/lijlansg/TrafficGPT.git.",
        "pdf_link": "https://arxiv.org/pdf/2309.06719v1.pdf"
    },
    {
        "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics",
        "authors": [
            "Jiayang Song",
            "Zhehua Zhou",
            "Jiawei Liu",
            "Chunrong Fang",
            "Zhan Shu",
            "Lei Ma"
        ],
        "published": "2023-09-13T02:56:56Z",
        "summary": "Although Deep Reinforcement Learning (DRL) has achieved notable success in\nnumerous robotic applications, designing a high-performing reward function\nremains a challenging task that often requires substantial manual input.\nRecently, Large Language Models (LLMs) have been extensively adopted to address\ntasks demanding in-depth common-sense knowledge, such as reasoning and\nplanning. Recognizing that reward function design is also inherently linked to\nsuch knowledge, LLM offers a promising potential in this context. Motivated by\nthis, we propose in this work a novel LLM framework with a self-refinement\nmechanism for automated reward function design. The framework commences with\nthe LLM formulating an initial reward function based on natural language\ninputs. Then, the performance of the reward function is assessed, and the\nresults are presented back to the LLM for guiding its self-refinement process.\nWe examine the performance of our proposed framework through a variety of\ncontinuous robotic control tasks across three diverse robotic systems. The\nresults indicate that our LLM-designed reward functions are able to rival or\neven surpass manually designed reward functions, highlighting the efficacy and\napplicability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2309.06687v2.pdf"
    },
    {
        "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
        "authors": [
            "Hao Sun",
            "Alihan H√ºy√ºk",
            "Mihaela van der Schaar"
        ],
        "published": "2023-09-13T01:12:52Z",
        "summary": "In this study, we aim to enhance the arithmetic reasoning ability of Large\nLanguage Models (LLMs) through zero-shot prompt optimization. We identify a\npreviously overlooked objective of query dependency in such optimization and\nelucidate two ensuing challenges that impede the successful and economical\ndesign of prompt optimization techniques. One primary issue is the absence of\nan effective method to evaluate prompts during inference when the golden answer\nis unavailable. Concurrently, learning via interactions with the LLMs to\nnavigate the expansive natural language prompting space proves to be\nresource-intensive. To address this, we introduce Prompt-OIRL, which harnesses\noffline inverse reinforcement learning to draw insights from offline prompting\ndemonstration data. Such data exists as by-products when diverse prompts are\nbenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent\nprompt optimization objective is achieved by first learning an offline reward\nmodel. This model can evaluate any query-prompt pairs without accessing LLMs.\nSubsequently, a best-of-N strategy is deployed to recommend the optimal prompt.\nOur experimental evaluations across various LLM scales and arithmetic reasoning\ndatasets underscore both the efficacy and economic viability of the proposed\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2309.06553v4.pdf"
    },
    {
        "title": "Statistical Rejection Sampling Improves Preference Optimization",
        "authors": [
            "Tianqi Liu",
            "Yao Zhao",
            "Rishabh Joshi",
            "Misha Khalman",
            "Mohammad Saleh",
            "Peter J. Liu",
            "Jialu Liu"
        ],
        "published": "2023-09-13T01:07:25Z",
        "summary": "Improving the alignment of language models with human preferences remains an\nactive research challenge. Previous approaches have primarily utilized\nReinforcement Learning from Human Feedback (RLHF) via online RL methods such as\nProximal Policy Optimization (PPO). Recently, offline methods such as Sequence\nLikelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have\nemerged as attractive alternatives, offering improvements in stability and\nscalability while maintaining competitive performance. SLiC refines its loss\nfunction using sequence pairs sampled from a supervised fine-tuned (SFT)\npolicy, while DPO directly optimizes language models based on preference data,\nforegoing the need for a separate reward model. However, the maximum likelihood\nestimator (MLE) of the target optimal policy requires labeled preference pairs\nsampled from that policy. DPO's lack of a reward model constrains its ability\nto sample preference pairs from the optimal policy, and SLiC is restricted to\nsampling preference pairs only from the SFT policy. To address these\nlimitations, we introduce a novel approach called Statistical Rejection\nSampling Optimization (RSO) that aims to source preference data from the target\noptimal policy using rejection sampling, enabling a more accurate estimation of\nthe optimal policy. We also propose a unified framework that enhances the loss\nfunctions used in both SLiC and DPO from a preference modeling standpoint.\nThrough extensive experiments across three diverse tasks, we demonstrate that\nRSO consistently outperforms both SLiC and DPO on evaluations from both Large\nLanguage Model (LLM) and human raters.",
        "pdf_link": "https://arxiv.org/pdf/2309.06657v2.pdf"
    },
    {
        "title": "Exploring Large Language Models for Ontology Alignment",
        "authors": [
            "Yuan He",
            "Jiaoyan Chen",
            "Hang Dong",
            "Ian Horrocks"
        ],
        "published": "2023-09-12T17:01:02Z",
        "summary": "This work investigates the applicability of recent generative Large Language\nModels (LLMs), such as the GPT series and Flan-T5, to ontology alignment for\nidentifying concept equivalence mappings across ontologies. To test the\nzero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging\nsubsets from two equivalence matching datasets of the OAEI Bio-ML track, taking\ninto account concept labels and structural contexts. Preliminary findings\nsuggest that LLMs have the potential to outperform existing ontology alignment\nsystems like BERTMap, given careful framework and prompt design.",
        "pdf_link": "https://arxiv.org/pdf/2309.07172v1.pdf"
    },
    {
        "title": "The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models",
        "authors": [
            "Dimitris Spathis",
            "Fahim Kawsar"
        ],
        "published": "2023-09-12T13:51:29Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\nacross diverse tasks, leading individuals to increasingly use them as personal\nassistants and universal computing engines. Nevertheless, a notable obstacle\nemerges when feeding numerical/temporal data into these models, such as data\nsourced from wearables or electronic health records. LLMs employ tokenizers in\ntheir input that break down text into smaller units. However, tokenizers are\nnot designed to represent numerical values and might struggle to understand\nrepetitive patterns and context, treating consecutive values as separate tokens\nand disregarding their temporal relationships. Here, we discuss recent works\nthat employ LLMs for human-centric tasks such as in mobile health sensing and\npresent a case study showing that popular LLMs tokenize temporal data\nincorrectly. To address that, we highlight potential solutions such as prompt\ntuning with lightweight embedding layers as well as multimodal adapters, that\ncan help bridge this \"modality gap\". While the capability of language models to\ngeneralize to other modalities with minimal or no finetuning is exciting, this\npaper underscores the fact that their outputs cannot be meaningful if they\nstumble over input nuances.",
        "pdf_link": "https://arxiv.org/pdf/2309.06236v1.pdf"
    },
    {
        "title": "Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions",
        "authors": [
            "Myriam Safrai",
            "Amos Azaria"
        ],
        "published": "2023-09-12T05:54:45Z",
        "summary": "As Large Language Models (LLMs) are predictive models building their response\nbased on the words in the prompts, there is a risk that small talk and\nirrelevant information may alter the response and the suggestion given.\nTherefore, this study aims to investigate the impact of medical data mixed with\nsmall talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3\nquestions were used as a model for relevant medical data. We use both multiple\nchoice and open ended questions. We gathered small talk sentences from human\nparticipants using the Mechanical Turk platform. Both sets of USLME questions\nwere arranged in a pattern where each sentence from the original questions was\nfollowed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both\nsets of questions with and without the small talk sentences. A board-certified\nphysician analyzed the answers by ChatGPT and compared them to the formal\ncorrect answer. The analysis results demonstrate that the ability of\nChatGPT-3.5 to answer correctly was impaired when small talk was added to\nmedical data for multiple-choice questions (72.1\\% vs. 68.9\\%) and open\nquestions (61.5\\% vs. 44.3\\%; p=0.01), respectively. In contrast, small talk\nphrases did not impair ChatGPT-4 ability in both types of questions (83.6\\% and\n66.2\\%, respectively). According to these results, ChatGPT-4 seems more\naccurate than the earlier 3.5 version, and it appears that small talk does not\nimpair its capability to provide medical recommendations. Our results are an\nimportant first step in understanding the potential and limitations of\nutilizing ChatGPT and other LLMs for physician-patient interactions, which\ninclude casual conversations.",
        "pdf_link": "https://arxiv.org/pdf/2309.08625v1.pdf"
    },
    {
        "title": "The Moral Machine Experiment on Large Language Models",
        "authors": [
            "Kazuhiro Takemoto"
        ],
        "published": "2023-09-12T04:49:39Z",
        "summary": "As large language models (LLMs) become more deeply integrated into various\nsectors, understanding how they make moral judgments has become crucial,\nparticularly in the realm of autonomous driving. This study utilized the Moral\nMachine framework to investigate the ethical decision-making tendencies of\nprominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their\nresponses to human preferences. While LLMs' and humans' preferences such as\nprioritizing humans over pets and favoring saving more lives are broadly\naligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.\nAdditionally, despite the qualitative similarities between the LLM and human\npreferences, there are significant quantitative disparities, suggesting that\nLLMs might lean toward more uncompromising decisions, compared to the milder\ninclinations of humans. These insights elucidate the ethical frameworks of LLMs\nand their potential implications for autonomous driving.",
        "pdf_link": "https://arxiv.org/pdf/2309.05958v1.pdf"
    },
    {
        "title": "Balanced and Explainable Social Media Analysis for Public Health with Large Language Models",
        "authors": [
            "Yan Jiang",
            "Ruihong Qiu",
            "Yi Zhang",
            "Peng-Fei Zhang"
        ],
        "published": "2023-09-12T04:15:34Z",
        "summary": "As social media becomes increasingly popular, more and more public health\nactivities emerge, which is worth noting for pandemic monitoring and government\ndecision-making. Current techniques for public health analysis involve popular\nmodels such as BERT and large language models (LLMs). Although recent progress\nin LLMs has shown a strong ability to comprehend knowledge by being fine-tuned\non specific domain datasets, the costs of training an in-domain LLM for every\nspecific public health task are especially expensive. Furthermore, such kinds\nof in-domain datasets from social media are generally highly imbalanced, which\nwill hinder the efficiency of LLMs tuning. To tackle these challenges, the data\nimbalance issue can be overcome by sophisticated data augmentation methods for\nsocial media datasets. In addition, the ability of the LLMs can be effectively\nutilised by prompting the model properly. In light of the above discussion, in\nthis paper, a novel ALEX framework is proposed for social media analysis on\npublic health. Specifically, an augmentation pipeline is developed to resolve\nthe data imbalance issue. Furthermore, an LLMs explanation mechanism is\nproposed by prompting an LLM with the predicted results from BERT models.\nExtensive experiments conducted on three tasks at the Social Media Mining for\nHealth 2023 (SMM4H) competition with the first ranking in two tasks demonstrate\nthe superior performance of the proposed ALEX method. Our code has been\nreleased in https://github.com/YanJiangJerry/ALEX.",
        "pdf_link": "https://arxiv.org/pdf/2309.05951v1.pdf"
    },
    {
        "title": "Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-09-12T02:14:05Z",
        "summary": "In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.",
        "pdf_link": "https://arxiv.org/pdf/2309.05918v3.pdf"
    },
    {
        "title": "Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing",
        "authors": [
            "Nunzio Lor√®",
            "Babak Heydari"
        ],
        "published": "2023-09-12T00:54:15Z",
        "summary": "This paper investigates the strategic decision-making capabilities of three\nLarge Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework\nof game theory. Utilizing four canonical two-player games -- Prisoner's\nDilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these\nmodels navigate social dilemmas, situations where players can either cooperate\nfor a collective benefit or defect for individual gain. Crucially, we extend\nour analysis to examine the role of contextual framing, such as diplomatic\nrelations or casual friendships, in shaping the models' decisions. Our findings\nreveal a complex landscape: while GPT-3.5 is highly sensitive to contextual\nframing, it shows limited ability to engage in abstract strategic reasoning.\nBoth GPT-4 and LLaMa-2 adjust their strategies based on game structure and\ncontext, but LLaMa-2 exhibits a more nuanced understanding of the games'\nunderlying mechanics. These results highlight the current limitations and\nvaried proficiencies of LLMs in strategic decision-making, cautioning against\ntheir unqualified use in tasks requiring complex strategic reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2309.05898v1.pdf"
    },
    {
        "title": "PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis",
        "authors": [
            "Dylan Zhang",
            "Xuchao Zhang",
            "Chetan Bansal",
            "Pedro Las-Casas",
            "Rodrigo Fonseca",
            "Saravan Rajmohan"
        ],
        "published": "2023-09-11T21:24:00Z",
        "summary": "Major cloud providers have employed advanced AI-based solutions like large\nlanguage models to aid humans in identifying the root causes of cloud\nincidents. Despite the growing prevalence of AI-driven assistants in the root\ncause analysis process, their effectiveness in assisting on-call engineers is\nconstrained by low accuracy due to the intrinsic difficulty of the task, a\npropensity for LLM-based approaches to hallucinate, and difficulties in\ndistinguishing these well-disguised hallucinations. To address this challenge,\nwe propose to perform confidence estimation for the predictions to help on-call\nengineers make decisions on whether to adopt the model prediction. Considering\nthe black-box nature of many LLM-based root cause predictors, fine-tuning or\ntemperature-scaling-based approaches are inapplicable. We therefore design an\ninnovative confidence estimation framework based on prompting\nretrieval-augmented large language models (LLMs) that demand a minimal amount\nof information from the root cause predictor. This approach consists of two\nscoring phases: the LLM-based confidence estimator first evaluates its\nconfidence in making judgments in the face of the current incident that\nreflects its ``grounded-ness\" level in reference data, then rates the root\ncause prediction based on historical references. An optimization step combines\nthese two scores for a final confidence assignment. We show that our method is\nable to produce calibrated confidence estimates for predicted root causes,\nvalidate the usefulness of retrieved historical data and the prompting strategy\nas well as the generalizability across different root cause prediction models.\nOur study takes an important move towards reliably and effectively embedding\nLLMs into cloud incident management systems.",
        "pdf_link": "https://arxiv.org/pdf/2309.05833v3.pdf"
    },
    {
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models",
        "authors": [
            "Mansi Sakarvadia",
            "Aswathy Ajith",
            "Arham Khan",
            "Daniel Grzenda",
            "Nathaniel Hudson",
            "Andr√© Bauer",
            "Kyle Chard",
            "Ian Foster"
        ],
        "published": "2023-09-11T16:39:30Z",
        "summary": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Large Language Models (LLMs) struggle to\nperform such reasoning consistently. Here we propose an approach to pinpoint\nand rectify multi-hop reasoning failures through targeted memory injections on\nLLM attention heads. First, we analyze the per-layer activations of GPT-2\nmodels in response to single and multi-hop prompts. We then propose a mechanism\nthat allows users to inject pertinent prompt-specific information, which we\nrefer to as \"memories,\" at critical LLM locations during inference. By thus\nenabling the LLM to incorporate additional relevant information during\ninference, we enhance the quality of multi-hop prompt completions. We show\nempirically that a simple, efficient, and targeted memory injection into a key\nattention layer can often increase the probability of the desired next token in\nmulti-hop tasks, by up to 424%.",
        "pdf_link": "https://arxiv.org/pdf/2309.05605v3.pdf"
    },
    {
        "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
        "authors": [
            "Wenhua Cheng",
            "Weiwei Zhang",
            "Haihao Shen",
            "Yiyang Cai",
            "Xin He",
            "Kaokao Lv"
        ],
        "published": "2023-09-11T14:58:23Z",
        "summary": "Large Language Models (LLMs) have proven their exceptional capabilities in\nperforming language-related tasks. However, their deployment poses significant\nchallenges due to their considerable memory and storage requirements. In\nresponse to this issue, weight-only quantization, particularly 3 and 4-bit\nweight-only quantization, has emerged as one of the most viable solutions. As\nthe number of bits decreases, the quantization grid broadens, thus emphasizing\nthe importance of up and down rounding. While previous studies have\ndemonstrated that fine-tuning up and down rounding with the addition of\nperturbations can enhance accuracy in some scenarios, our study is driven by\nthe precise and limited boundary of these perturbations, where only the\nthreshold for altering the rounding value is of significance. Consequently, we\npropose a concise and highly effective approach for optimizing the weight\nrounding task. Our method, named SignRound, involves lightweight block-wise\ntuning using signed gradient descent, enabling us to achieve outstanding\nresults within 400 steps. SignRound competes impressively against recent\nmethods without introducing additional inference overhead. The source code will\nbe publicly available at \\url{https://github.com/intel/neural-compressor} soon.",
        "pdf_link": "https://arxiv.org/pdf/2309.05516v2.pdf"
    },
    {
        "title": "Evaluating the Deductive Competence of Large Language Models",
        "authors": [
            "S. M. Seals",
            "Valerie L. Shalin"
        ],
        "published": "2023-09-11T13:47:07Z",
        "summary": "The development of highly fluent large language models (LLMs) has prompted\nincreased interest in assessing their reasoning and problem-solving\ncapabilities. We investigate whether several LLMs can solve a classic type of\ndeductive reasoning problem from the cognitive science literature. The tested\nLLMs have limited abilities to solve these problems in their conventional form.\nWe performed follow up experiments to investigate if changes to the\npresentation format and content improve model performance. We do find\nperformance differences between conditions; however, they do not improve\noverall performance. Moreover, we find that performance interacts with\npresentation format and content in unexpected ways that differ from human\nperformance. Overall, our results suggest that LLMs have unique reasoning\nbiases that are only partially predicted from human reasoning performance.",
        "pdf_link": "https://arxiv.org/pdf/2309.05452v1.pdf"
    },
    {
        "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
        "authors": [
            "Li Du",
            "Yequan Wang",
            "Xingrun Xing",
            "Yiqun Ya",
            "Xiang Li",
            "Xin Jiang",
            "Xuezhi Fang"
        ],
        "published": "2023-09-11T03:35:00Z",
        "summary": "Although demonstrating superb performance on various NLP tasks, large\nlanguage models (LLMs) still suffer from the hallucination problem, which\nthreatens the reliability of LLMs. To measure the level of hallucination of\nLLMs, previous works first categorize the hallucination according to the\nphenomenon similarity, then quantify the proportion that model outputs contain\nhallucinatory contents. However, such hallucination rates could easily be\ndistorted by confounders. Moreover, such hallucination rates could not reflect\nthe reasons for the hallucination, as similar hallucinatory phenomena may\noriginate from different sources. To address these issues, we propose to\ncombine the hallucination level quantification and hallucination reason\ninvestigation through an association analysis, which builds the relationship\nbetween the hallucination rate of LLMs with a set of risk factors. In this way,\nwe are able to observe the hallucination level under each value of each risk\nfactor, examining the contribution and statistical significance of each risk\nfactor, meanwhile excluding the confounding effect of other factors.\nAdditionally, by recognizing the risk factors according to a taxonomy of model\ncapability, we reveal a set of potential deficiencies in commonsense\nmemorization, relational reasoning, and instruction following, which may\nfurther provide guidance for the pretraining and supervised fine-tuning process\nof LLMs to mitigate the hallucination.",
        "pdf_link": "https://arxiv.org/pdf/2309.05217v1.pdf"
    },
    {
        "title": "Does Writing with Language Models Reduce Content Diversity?",
        "authors": [
            "Vishakh Padmakumar",
            "He He"
        ],
        "published": "2023-09-11T02:16:47Z",
        "summary": "Large language models (LLMs) have led to a surge in collaborative writing\nwith model assistance. As different users incorporate suggestions from the same\nmodel, there is a risk of decreased diversity in the produced content,\npotentially limiting diverse perspectives in public discourse. In this work, we\nmeasure the impact of co-writing on diversity via a controlled experiment,\nwhere users write argumentative essays in three setups -- using a base LLM\n(GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We\ndevelop a set of diversity metrics and find that writing with InstructGPT (but\nnot the GPT3) results in a statistically significant reduction in diversity.\nSpecifically, it increases the similarity between the writings of different\nauthors and reduces the overall lexical and content diversity. We additionally\nfind that this effect is mainly attributable to InstructGPT contributing less\ndiverse text to co-written essays. In contrast, the user-contributed text\nremains unaffected by model collaboration. This suggests that the recent\nimprovement in generation quality from adapting models to human feedback might\ncome at the cost of more homogeneous and less diverse content.",
        "pdf_link": "https://arxiv.org/pdf/2309.05196v2.pdf"
    },
    {
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
        "authors": [
            "Zhengxiang Shi",
            "Aldo Lipani"
        ],
        "published": "2023-09-11T00:02:05Z",
        "summary": "Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving substantial memory and time costs compared to vanilla\nPT and its variants, without changing trainable parameter sizes. Through\nextensive experiments on 23 natural language processing (NLP) and\nvision-language (VL) tasks, we demonstrate that DePT outperforms\nstate-of-the-art PEFT approaches, including the full fine-tuning baseline, in\nsome scenarios. Additionally, we empirically show that DEPT grows more\nefficient as the model size increases. Our further study reveals that DePT\nintegrates seamlessly with parameter-efficient transfer learning in the\nfew-shot learning setting and highlights its adaptability to various model\narchitectures and sizes.",
        "pdf_link": "https://arxiv.org/pdf/2309.05173v5.pdf"
    },
    {
        "title": "Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps",
        "authors": [
            "Yaonai Wei",
            "Tuo Zhang",
            "Han Zhang",
            "Tianyang Zhong",
            "Lin Zhao",
            "Zhengliang Liu",
            "Chong Ma",
            "Songyao Zhang",
            "Muheng Shang",
            "Lei Du",
            "Xiao Li",
            "Tianming Liu",
            "Junwei Han"
        ],
        "published": "2023-09-10T13:06:45Z",
        "summary": "Over decades, neuroscience has accumulated a wealth of research results in\nthe text modality that can be used to explore cognitive processes.\nMeta-analysis is a typical method that successfully establishes a link from\ntext queries to brain activation maps using these research results, but it\nstill relies on an ideal query environment. In practical applications, text\nqueries used for meta-analyses may encounter issues such as semantic redundancy\nand ambiguity, resulting in an inaccurate mapping to brain images. On the other\nhand, large language models (LLMs) like ChatGPT have shown great potential in\ntasks such as context understanding and reasoning, displaying a high degree of\nconsistency with human natural language. Hence, LLMs could improve the\nconnection between text modality and neuroscience, resolving existing\nchallenges of meta-analyses. In this study, we propose a method called\nChat2Brain that combines LLMs to basic text-2-image model, known as Text2Brain,\nto map open-ended semantic queries to brain activation maps in data-scarce and\ncomplex query environments. By utilizing the understanding and reasoning\ncapabilities of LLMs, the performance of the mapping model is optimized by\ntransferring text queries to semantic queries. We demonstrate that Chat2Brain\ncan synthesize anatomically plausible neural activation patterns for more\ncomplex tasks of text queries.",
        "pdf_link": "https://arxiv.org/pdf/2309.05021v1.pdf"
    },
    {
        "title": "Towards LLM-based Autograding for Short Textual Answers",
        "authors": [
            "Johannes Schneider",
            "Bernd Schenk",
            "Christina Niklaus",
            "Michaelis Vlachos"
        ],
        "published": "2023-09-09T22:25:56Z",
        "summary": "Grading of exams is an important, labor intensive, subjective, repetitive and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and because of the substantial influx of data brought\nabout by digitalization. However, entrusting AI models with decision-making\nroles raises ethical considerations, mainly stemming from potential biases and\nissues related to generating false information. Thus, in this manuscript we\nprovide an evaluation of a large language model for the purpose of autograding,\nwhile also highlighting how LLMs can support educators in validating their\ngrading procedures. Our evaluation is targeted towards automatic short textual\nanswers grading (ASAG), spanning various languages and examinations from two\ndistinct courses. Our findings suggest that while \"out-of-the-box\" LLMs provide\na valuable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.",
        "pdf_link": "https://arxiv.org/pdf/2309.11508v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
        "authors": [
            "Pranay Dighe",
            "Yi Su",
            "Shangshang Zheng",
            "Yunshu Liu",
            "Vineet Garg",
            "Xiaochuan Niu",
            "Ahmed Tewfik"
        ],
        "published": "2023-09-09T17:02:33Z",
        "summary": "While large language models excel in a variety of natural language processing\n(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they\nmust either rely on off-the-shelf automatic speech recognition (ASR) systems\nfor transcription, or be equipped with an in-built speech modality. This work\nfocuses on the former scenario, where LLM's accuracy on SLU tasks is\nconstrained by the accuracy of a fixed ASR system on the spoken input.\nSpecifically, we tackle speech-intent classification task, where a high\nword-error-rate can limit the LLM's ability to understand the spoken intent.\nInstead of chasing a high accuracy by designing complex or specialized\narchitectures regardless of deployment costs, we seek to answer how far we can\ngo without substantially changing the underlying ASR and LLM, which can\npotentially be shared by multiple unrelated tasks. To this end, we propose\nprompting the LLM with an n-best list of ASR hypotheses instead of only the\nerror-prone 1-best hypothesis. We explore prompt-engineering to explain the\nconcept of n-best lists to the LLM; followed by the finetuning of Low-Rank\nAdapters on the downstream tasks. Our approach using n-best lists proves to be\neffective on a device-directed speech detection task as well as on a keyword\nspotting task, where systems using n-best list prompts outperform those using\n1-best ASR hypothesis; thus paving the way for an efficient method to exploit\nASR uncertainty via LLMs for speech-based applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.04842v2.pdf"
    },
    {
        "title": "Code-Style In-Context Learning for Knowledge-Based Question Answering",
        "authors": [
            "Zhijie Nie",
            "Richong Zhang",
            "Zhongyuan Wang",
            "Xudong Liu"
        ],
        "published": "2023-09-09T06:27:00Z",
        "summary": "Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting. The code and supplementary files are released at\nhttps://github.com/Arthurizijar/KB-Coder .",
        "pdf_link": "https://arxiv.org/pdf/2309.04695v2.pdf"
    },
    {
        "title": "Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges",
        "authors": [
            "Hiba Ahsan",
            "Denis Jered McInerney",
            "Jisoo Kim",
            "Christopher Potter",
            "Geoffrey Young",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-09-08T18:44:47Z",
        "summary": "Unstructured data in Electronic Health Records (EHRs) often contains critical\ninformation -- complementary to imaging -- that could inform radiologists'\ndiagnoses. But the large volume of notes often associated with patients\ntogether with time constraints renders manually identifying relevant evidence\npractically infeasible. In this work we propose and evaluate a zero-shot\nstrategy for using LLMs as a mechanism to efficiently retrieve and summarize\nunstructured evidence in patient EHR relevant to a given query. Our method\nentails tasking an LLM to infer whether a patient has, or is at risk of, a\nparticular condition on the basis of associated notes; if so, we ask the model\nto summarize the supporting evidence. Under expert evaluation, we find that\nthis LLM-based approach provides outputs consistently preferred to a pre-LLM\ninformation retrieval baseline. Manual evaluation is expensive, so we also\npropose and validate a method using an LLM to evaluate (other) LLM outputs for\nthis task, allowing us to scale up evaluation. Our findings indicate the\npromise of LLMs as interfaces to EHR, but also highlight the outstanding\nchallenge posed by \"hallucinations\". In this setting, however, we show that\nmodel confidence in outputs strongly correlates with faithful summaries,\noffering a practical means to limit confabulations.",
        "pdf_link": "https://arxiv.org/pdf/2309.04550v2.pdf"
    },
    {
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
        "authors": [
            "Yangyi Chen",
            "Karan Sikka",
            "Michael Cogswell",
            "Heng Ji",
            "Ajay Divakaran"
        ],
        "published": "2023-09-08T17:49:44Z",
        "summary": "Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.",
        "pdf_link": "https://arxiv.org/pdf/2309.04461v2.pdf"
    },
    {
        "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference",
        "authors": [
            "Daliang Xu",
            "Wangsong Yin",
            "Xin Jin",
            "Ying Zhang",
            "Shiyun Wei",
            "Mengwei Xu",
            "Xuanzhe Liu"
        ],
        "published": "2023-09-08T10:44:19Z",
        "summary": "Generative tasks, such as text generation and question answering, hold a\ncrucial position in the realm of mobile applications. Due to their sensitivity\nto privacy concerns, there is a growing demand for their execution directly on\nmobile devices. Currently, the execution of these generative tasks heavily\ndepends on Large Language Models (LLMs). Nevertheless, the limited memory\ncapacity of these devices presents a formidable challenge to the scalability of\nsuch models.\n  In our research, we introduce LLMCad, an innovative on-device inference\nengine specifically designed for efficient generative Natural Language\nProcessing (NLP) tasks. The core idea behind LLMCad revolves around model\ncollaboration: a compact LLM, residing in memory, takes charge of generating\nthe most straightforward tokens, while a high-precision LLM steps in to\nvalidate these tokens and rectify any identified errors. LLMCad incorporates\nthree novel techniques: (1) Instead of generating candidate tokens in a\nsequential manner, LLMCad employs the smaller LLM to construct a token tree,\nencompassing a wider range of plausible token pathways. Subsequently, the\nlarger LLM can efficiently validate all of these pathways simultaneously. (2)\nIt employs a self-adjusting fallback strategy, swiftly initiating the\nverification process whenever the smaller LLM generates an erroneous token. (3)\nTo ensure a continuous flow of token generation, LLMCad speculatively generates\ntokens during the verification process by implementing a compute-IO pipeline.\nThrough an extensive series of experiments, LLMCad showcases an impressive\ntoken generation speed, achieving rates up to 9.3x faster than existing\ninference engines.",
        "pdf_link": "https://arxiv.org/pdf/2309.04255v1.pdf"
    },
    {
        "title": "Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems",
        "authors": [
            "Dongyub Lee",
            "Taesun Whang",
            "Chanhee Lee",
            "Heuiseok Lim"
        ],
        "published": "2023-09-08T09:39:53Z",
        "summary": "Large language models (LLMs) have emerged as versatile tools in various daily\napplications. However, they are fraught with issues that undermine their\nutility and trustworthiness. These include the incorporation of erroneous\nreferences (citation), the generation of hallucinated information\n(correctness), and the inclusion of superfluous or omission of crucial details\n(fluency). To ameliorate these concerns, this study makes several key\ncontributions. First, we build a dataset to train a critic model capable of\nevaluating the citation, correctness, and fluency of responses generated by\nLLMs in QA systems. Second, we propose an automated feedback mechanism that\nleverages the critic model to offer real-time feedback on heterogeneous aspects\nof generated text. Third, we introduce a feedback learning loop that uses this\ncritic model to iteratively improve the performance of the LLM responsible for\nresponse generation. Experimental results demonstrate the efficacy of our\napproach, showing substantial improvements in citation and fluency metrics for\nChatGPT, including a 4% precision increase in citation and an approximately 8%\nenhancement in the MAUVE metric for fluency, while maintaining high levels of\ncorrectness.",
        "pdf_link": "https://arxiv.org/pdf/2309.06384v1.pdf"
    },
    {
        "title": "UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media",
        "authors": [
            "Yan Jiang",
            "Ruihong Qiu",
            "Yi Zhang",
            "Zi Huang"
        ],
        "published": "2023-09-08T08:54:55Z",
        "summary": "As social media becomes increasingly popular, more and more activities\nrelated to public health emerge. Current techniques for public health analysis\ninvolve popular models such as BERT and large language models (LLMs). However,\nthe costs of training in-domain LLMs for public health are especially\nexpensive. Furthermore, such kinds of in-domain datasets from social media are\ngenerally imbalanced. To tackle these challenges, the data imbalance issue can\nbe overcome by data augmentation and balanced training. Moreover, the ability\nof the LLMs can be effectively utilized by prompting the model properly. In\nthis paper, a novel ALEX framework is proposed to improve the performance of\npublic health analysis on social media by adopting an LLMs explanation\nmechanism. Results show that our ALEX model got the best performance among all\nsubmissions in both Task 2 and Task 4 with a high score in Task 1 in Social\nMedia Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://\ngithub.com/YanJiangJerry/ALEX.",
        "pdf_link": "https://arxiv.org/pdf/2309.04213v2.pdf"
    },
    {
        "title": "Don't Ignore Dual Logic Ability of LLMs while Privatizing: A Data-Intensive Analysis in Medical Domain",
        "authors": [
            "Yanrui Du",
            "Sendong Zhao",
            "Muzhen Cai",
            "Ming Ma",
            "Danyang Zhao",
            "Jiawei Cao",
            "Bing Qin"
        ],
        "published": "2023-09-08T08:20:46Z",
        "summary": "Extensive studies have been devoted to privatizing general-domain Large\nLanguage Models (LLMs) as Domain-Specific LLMs via feeding specific-domain\ndata. However, these privatization efforts often ignored a critical aspect:\nDual Logic Ability, which is a core reasoning ability for LLMs. The dual logic\nability of LLMs ensures that they can maintain a consistent stance when\nconfronted with both positive and negative statements about the same fact. Our\nstudy focuses on how the dual logic ability of LLMs is affected during the\nprivatization process in the medical domain. We conduct several experiments to\nanalyze the dual logic ability of LLMs by examining the consistency of the\nstance in responses to paired questions about the same fact. In our\nexperiments, interestingly, we observed a significant decrease in the dual\nlogic ability of existing LLMs after privatization. Besides, our results\nindicate that incorporating general domain dual logic data into LLMs not only\nenhances LLMs' dual logic ability but also further improves their accuracy.\nThese findings underscore the importance of prioritizing LLMs' dual logic\nability during the privatization process. Our study establishes a benchmark for\nfuture research aimed at exploring LLMs' dual logic ability during the\nprivatization process and offers valuable guidance for privatization efforts in\nreal-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.04198v3.pdf"
    },
    {
        "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese",
        "authors": [
            "Haochun Wang",
            "Sendong Zhao",
            "Zewen Qiang",
            "Zijian Li",
            "Nuwa Xi",
            "Yanrui Du",
            "MuZhen Cai",
            "Haoqiang Guo",
            "Yuhan Chen",
            "Haoming Xu",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-09-08T07:42:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable success in diverse\nnatural language processing (NLP) tasks in general domains. However, LLMs\nsometimes generate responses with the hallucination about medical facts due to\nlimited domain knowledge. Such shortcomings pose potential risks in the\nutilization of LLMs within medical contexts. To address this challenge, we\npropose knowledge-tuning, which leverages structured medical knowledge bases\nfor the LLMs to grasp domain knowledge efficiently and facilitate reliable\nresponse generation. We also release cMedKnowQA, a Chinese medical knowledge\nquestion-answering dataset constructed from medical knowledge bases to assess\nthe medical knowledge proficiency of LLMs. Experimental results show that the\nLLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of\naccuracy in response generation compared with vanilla instruction-tuning and\noffer a new reliable way for the domain adaptation of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.04175v1.pdf"
    },
    {
        "title": "Matching Table Metadata with Business Glossaries Using Large Language Models",
        "authors": [
            "Elita Lobo",
            "Oktie Hassanzadeh",
            "Nhan Pham",
            "Nandana Mihindukulasooriya",
            "Dharmashankar Subramanian",
            "Horst Samulowitz"
        ],
        "published": "2023-09-08T02:23:59Z",
        "summary": "Enterprises often own large collections of structured data in the form of\nlarge databases or an enterprise data lake. Such data collections come with\nlimited metadata and strict access policies that could limit access to the data\ncontents and, therefore, limit the application of classic retrieval and\nanalysis solutions. As a result, there is a need for solutions that can\neffectively utilize the available metadata. In this paper, we study the problem\nof matching table metadata to a business glossary containing data labels and\ndescriptions. The resulting matching enables the use of an available or curated\nbusiness glossary for retrieval and analysis without or before requesting\naccess to the data contents. One solution to this problem is to use\nmanually-defined rules or similarity measures on column names and glossary\ndescriptions (or their vector embeddings) to find the closest match. However,\nsuch approaches need to be tuned through manual labeling and cannot handle many\nbusiness glossaries that contain a combination of simple as well as complex and\nlong descriptions. In this work, we leverage the power of large language models\n(LLMs) to design generic matching methods that do not require manual tuning and\ncan identify complex relations between column names and glossaries. We propose\nmethods that utilize LLMs in two ways: a) by generating additional context for\ncolumn names that can aid with matching b) by using LLMs to directly infer if\nthere is a relation between column names and glossary descriptions. Our\npreliminary experimental results show the effectiveness of our proposed\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2309.11506v1.pdf"
    },
    {
        "title": "Evaluation of large language models for discovery of gene set function",
        "authors": [
            "Mengzhou Hu",
            "Sahar Alkhairy",
            "Ingoo Lee",
            "Rudolf T. Pillich",
            "Dylan Fong",
            "Kevin Smith",
            "Robin Bachelder",
            "Trey Ideker",
            "Dexter Pratt"
        ],
        "published": "2023-09-07T21:10:48Z",
        "summary": "Gene set analysis is a mainstay of functional genomics, but it relies on\ncurated databases of gene functions that are incomplete. Here we evaluate five\nLarge Language Models (LLMs) for their ability to discover the common\nbiological functions represented by a gene set, substantiated by supporting\nrationale, citations and a confidence assessment. Benchmarking against\ncanonical gene sets from the Gene Ontology, GPT-4 confidently recovered the\ncurated name or a more general concept (73% of cases), while benchmarking\nagainst random gene sets correctly yielded zero confidence. Gemini-Pro and\nMixtral-Instruct showed ability in naming but were falsely confident for random\nsets, whereas Llama2-70b had poor performance overall. In gene sets derived\nfrom 'omics data, GPT-4 identified novel functions not reported by classical\nfunctional enrichment (32% of cases), which independent review indicated were\nlargely verifiable and not hallucinations. The ability to rapidly synthesize\ncommon gene functions positions LLMs as valuable 'omics assistants.",
        "pdf_link": "https://arxiv.org/pdf/2309.04019v2.pdf"
    },
    {
        "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
        "authors": [
            "Amrita Bhattacharjee",
            "Tharindu Kumarage",
            "Raha Moraffah",
            "Huan Liu"
        ],
        "published": "2023-09-07T19:51:30Z",
        "summary": "Large language models (LLMs) are increasingly being used for generating text\nin a variety of use cases, including journalistic news articles. Given the\npotential malicious nature in which these LLMs can be used to generate\ndisinformation at scale, it is important to build effective detectors for such\nAI-generated text. Given the surge in development of new LLMs, acquiring\nlabeled training data for supervised detectors is a bottleneck. However, there\nmight be plenty of unlabeled text data available, without information on which\ngenerator it came from. In this work we tackle this data problem, in detecting\nAI-generated news text, and frame the problem as an unsupervised domain\nadaptation task. Here the domains are the different text generators, i.e. LLMs,\nand we assume we have access to only the labeled source data and unlabeled\ntarget data. We develop a Contrastive Domain Adaptation framework, called\nConDA, that blends standard domain adaptation techniques with the\nrepresentation power of contrastive learning to learn domain invariant\nrepresentations that are effective for the final unsupervised detection task.\nOur experiments demonstrate the effectiveness of our framework, resulting in\naverage performance gains of 31.7% from the best performing baselines, and\nwithin 0.8% margin of a fully supervised detector. All our code and data is\navailable at https://github.com/AmritaBh/ConDA-gen-text-detection.",
        "pdf_link": "https://arxiv.org/pdf/2309.03992v2.pdf"
    },
    {
        "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
        "authors": [
            "Yung-Sung Chuang",
            "Yujia Xie",
            "Hongyin Luo",
            "Yoon Kim",
            "James Glass",
            "Pengcheng He"
        ],
        "published": "2023-09-07T17:45:31Z",
        "summary": "Despite their impressive capabilities, large language models (LLMs) are prone\nto hallucinations, i.e., generating content that deviates from facts seen\nduring pretraining. We propose a simple decoding strategy for reducing\nhallucinations with pretrained LLMs that does not require conditioning on\nretrieved external knowledge nor additional fine-tuning. Our approach obtains\nthe next-token distribution by contrasting the differences in logits obtained\nfrom projecting the later layers versus earlier layers to the vocabulary space,\nexploiting the fact that factual knowledge in an LLMs has generally been shown\nto be localized to particular transformer layers. We find that this Decoding by\nContrasting Layers (DoLa) approach is able to better surface factual knowledge\nand reduce the generation of incorrect facts. DoLa consistently improves the\ntruthfulness across multiple choices tasks and open-ended generation tasks, for\nexample improving the performance of LLaMA family models on TruthfulQA by\n12-17% absolute points, demonstrating its potential in making LLMs reliably\ngenerate truthful facts.",
        "pdf_link": "https://arxiv.org/pdf/2309.03883v2.pdf"
    },
    {
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
        "authors": [
            "Chujie Zheng",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Minlie Huang"
        ],
        "published": "2023-09-07T17:44:56Z",
        "summary": "Multiple choice questions (MCQs) serve as a common yet important task format\nin the evaluation of large language models (LLMs). This work shows that modern\nLLMs are vulnerable to option position changes in MCQs due to their inherent\n\"selection bias\", namely, they prefer to select specific option IDs as answers\n(like \"Option A\"). Through extensive empirical analyses with 20 LLMs on three\nbenchmarks, we pinpoint that this behavioral bias primarily stems from LLMs'\ntoken bias, where the model a priori assigns more probabilistic mass to\nspecific option ID tokens (e.g., A/B/C/D) when predicting answers from the\noption IDs. To mitigate selection bias, we propose a label-free, inference-time\ndebiasing method, called PriDe, which separates the model's prior bias for\noption IDs from the overall prediction distribution. PriDe first estimates the\nprior by permutating option contents on a small number of test samples, and\nthen applies the estimated prior to debias the remaining samples. We\ndemonstrate that it achieves interpretable and transferable debiasing with high\ncomputational efficiency. We hope this work can draw broader research attention\nto the bias and robustness of modern LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.03882v4.pdf"
    },
    {
        "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs",
        "authors": [
            "Patrick Haller",
            "Ansar Aynetdinov",
            "Alan Akbik"
        ],
        "published": "2023-09-07T17:41:01Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable ability to generate fitting responses to natural language\ninstructions. However, an open research question concerns the inherent biases\nof trained models and their responses. For instance, if the data used to tune\nan LLM is dominantly written by persons with a specific political bias, we\nmight expect generated answers to share this bias. Current research work seeks\nto de-bias such models, or suppress potentially biased answers. With this\ndemonstration, we take a different view on biases in instruction-tuning: Rather\nthan aiming to suppress them, we aim to make them explicit and transparent. To\nthis end, we present OpinionGPT, a web demo in which users can ask questions\nand select all biases they wish to investigate. The demo will answer this\nquestion using a model fine-tuned on text representing each of the selected\nbiases, allowing side-by-side comparison. To train the underlying model, we\nidentified 11 different biases (political, geographic, gender, age) and derived\nan instruction-tuning corpus in which each answer was written by members of one\nof these demographics. This paper presents OpinionGPT, illustrates how we\ntrained the bias-aware model and showcases the web application (available at\nhttps://opiniongpt.informatik.hu-berlin.de).",
        "pdf_link": "https://arxiv.org/pdf/2309.03876v1.pdf"
    },
    {
        "title": "Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media",
        "authors": [
            "Hongzhi Qi",
            "Qing Zhao",
            "Changwei Song",
            "Wei Zhai",
            "Dan Luo",
            "Shuo Liu",
            "Yi Jing Yu",
            "Fan Wang",
            "Huijing Zou",
            "Bing Xiang Yang",
            "Jianqiang Li",
            "Guanghui Fu"
        ],
        "published": "2023-09-07T08:50:46Z",
        "summary": "In the realm of social media, users frequently convey personal sentiments,\nwith some potentially indicating cognitive distortions or suicidal tendencies.\nTimely recognition of such signs is pivotal for effective interventions. In\nresponse, we introduce two novel annotated datasets from Chinese social media,\nfocused on cognitive distortions and suicidal risk classification. We propose a\ncomprehensive benchmark using both supervised learning and large language\nmodels, especially from the GPT series, to evaluate performance on these\ndatasets. To assess the capabilities of the large language models, we employed\nthree strategies: zero-shot, few-shot, and fine-tuning. Furthermore, we deeply\nexplored and analyzed the performance of these large language models from a\npsychological perspective, shedding light on their strengths and limitations in\nidentifying and understanding complex human emotions. Our evaluations\nunderscore a performance difference between the two approaches, with the models\noften challenged by subtle category distinctions. While GPT-4 consistently\ndelivered strong results, GPT-3.5 showed marked improvement in suicide risk\nclassification after fine-tuning. This research is groundbreaking in its\nevaluation of large language models for Chinese social media tasks,\naccentuating the models' potential in psychological contexts. All datasets and\ncode are made available.",
        "pdf_link": "https://arxiv.org/pdf/2309.03564v2.pdf"
    },
    {
        "title": "Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences",
        "authors": [
            "Sai Koneru",
            "Jian Wu",
            "Sarah Rajtmajer"
        ],
        "published": "2023-09-07T04:15:17Z",
        "summary": "Hypothesis formulation and testing are central to empirical research. A\nstrong hypothesis is a best guess based on existing evidence and informed by a\ncomprehensive view of relevant literature. However, with exponential increase\nin the number of scientific articles published annually, manual aggregation and\nsynthesis of evidence related to a given hypothesis is a challenge. Our work\nexplores the ability of current large language models (LLMs) to discern\nevidence in support or refute of specific hypotheses based on the text of\nscientific abstracts. We share a novel dataset for the task of scientific\nhypothesis evidencing using community-driven annotations of studies in the\nsocial sciences. We compare the performance of LLMs to several state-of-the-art\nbenchmarks and highlight opportunities for future research in this area. The\ndataset is available at\nhttps://github.com/Sai90000/ScientificHypothesisEvidencing.git",
        "pdf_link": "https://arxiv.org/pdf/2309.06578v3.pdf"
    },
    {
        "title": "XGen-7B Technical Report",
        "authors": [
            "Erik Nijkamp",
            "Tian Xie",
            "Hiroaki Hayashi",
            "Bo Pang",
            "Congying Xia",
            "Chen Xing",
            "Jesse Vig",
            "Semih Yavuz",
            "Philippe Laban",
            "Ben Krause",
            "Senthil Purushwalkam",
            "Tong Niu",
            "Wojciech Kry≈õci≈Ñski",
            "Lidiya Murakhovs'ka",
            "Prafulla Kumar Choubey",
            "Alex Fabbri",
            "Ye Liu",
            "Rui Meng",
            "Lifu Tu",
            "Meghana Bhat",
            "Chien-Sheng Wu",
            "Silvio Savarese",
            "Yingbo Zhou",
            "Shafiq Joty",
            "Caiming Xiong"
        ],
        "published": "2023-09-07T02:20:03Z",
        "summary": "Large Language Models (LLMs) have become ubiquitous across various domains,\ntransforming the way we interact with information and conduct research.\nHowever, most high-performing LLMs remain confined behind proprietary walls,\nhindering scientific progress. Most open-source LLMs, on the other hand, are\nlimited in their ability to support longer sequence lengths, which is a key\nrequirement for many tasks that require inference over an input context. To\naddress this, we have trained XGen, a series of 7B parameter models on up to 8K\nsequence length for up to 1.5T tokens. We have also finetuned the XGen models\non public-domain instructional data, creating their instruction-tuned\ncounterparts (XGen-Inst). We open-source our models for both research\nadvancements and commercial applications. Our evaluation on standard benchmarks\nshows that XGen models achieve comparable or better results when compared with\nstate-of-the-art open-source LLMs. Our targeted evaluation on long sequence\nmodeling tasks shows the benefits of our 8K-sequence models over 2K-sequence\nopen-source LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.03450v1.pdf"
    },
    {
        "title": "Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Xuchao Zhang",
            "Yanchi Liu",
            "Wei Cheng",
            "Haoyu Wang",
            "Zhengzhang Chen",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Haifeng Chen",
            "Liang Zhao"
        ],
        "published": "2023-09-07T01:35:24Z",
        "summary": "Open Information Extraction (OIE) task aims at extracting structured facts\nfrom unstructured text, typically in the form of (subject, relation, object)\ntriples. Despite the potential of large language models (LLMs) like ChatGPT as\na general task solver, they lag behind state-of-the-art (supervised) methods in\nOIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant\ncontext from relevant relations and generate structured output due to the\nrestrictions on fine-tuning the model. Second, LLMs generates responses\nautoregressively based on probability, which makes the predicted relations lack\nconfidence. In this paper, we assess the capabilities of LLMs in improving the\nOIE task. Particularly, we propose various in-context learning strategies to\nenhance LLM's instruction-following ability and a demonstration uncertainty\nquantification module to enhance the confidence of the generated relations. Our\nexperiments on three OIE benchmark datasets show that our approach holds its\nown against established supervised methods, both quantitatively and\nqualitatively.",
        "pdf_link": "https://arxiv.org/pdf/2309.03433v1.pdf"
    },
    {
        "title": "Large Language Models as Optimizers",
        "authors": [
            "Chengrun Yang",
            "Xuezhi Wang",
            "Yifeng Lu",
            "Hanxiao Liu",
            "Quoc V. Le",
            "Denny Zhou",
            "Xinyun Chen"
        ],
        "published": "2023-09-07T00:07:15Z",
        "summary": "Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to prompt optimization where the goal is to\nfind instructions that maximize the task accuracy. With a variety of LLMs, we\ndemonstrate that the best prompts optimized by OPRO outperform human-designed\nprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at\nhttps://github.com/google-deepmind/opro.",
        "pdf_link": "https://arxiv.org/pdf/2309.03409v2.pdf"
    },
    {
        "title": "Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks",
        "authors": [
            "Priyam Mazumdar",
            "Aiman Soliman",
            "Volodymyr Kindratenko",
            "Luigi Marini",
            "Kenton McHenry"
        ],
        "published": "2023-09-06T21:20:10Z",
        "summary": "The lack of quality labeled data is one of the main bottlenecks for training\nDeep Learning models. As the task increases in complexity, there is a higher\npenalty for overfitting and unstable learning. The typical paradigm employed\ntoday is Self-Supervised learning, where the model attempts to learn from a\nlarge corpus of unstructured and unlabeled data and then transfer that\nknowledge to the required task. Some notable examples of self-supervision in\nother modalities are BERT for Large Language Models, Wav2Vec for Speech\nRecognition, and the Masked AutoEncoder for Vision, which all utilize\nTransformers to solve a masked prediction task. GeoAI is uniquely poised to\ntake advantage of the self-supervised methodology due to the decades of data\ncollected, little of which is precisely and dependably annotated. Our goal is\nto extract building and road segmentations from Digital Elevation Models (DEM)\nthat provide a detailed topography of the earths surface. The proposed\narchitecture is the Masked Autoencoder pre-trained on ImageNet (with the\nlimitation that there is a large domain discrepancy between ImageNet and DEM)\nwith an UperNet Head for decoding segmentations. We tested this model with 450\nand 50 training images only, utilizing roughly 5% and 0.5% of the original data\nrespectively. On the building segmentation task, this model obtains an 82.1%\nIntersection over Union (IoU) with 450 Images and 69.1% IoU with only 50\nimages. On the more challenging road detection task the model obtains an 82.7%\nIoU with 450 images and 73.2% IoU with only 50 images. Any hand-labeled dataset\nmade today about the earths surface will be immediately obsolete due to the\nconstantly changing nature of the landscape. This motivates the clear necessity\nfor data-efficient learners that can be used for a wide variety of downstream\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2309.03367v1.pdf"
    },
    {
        "title": "Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity",
        "authors": [
            "Aliya Amirova",
            "Theodora Fteropoulli",
            "Nafiso Ahmed",
            "Martin R. Cowie",
            "Joel Z. Leibo"
        ],
        "published": "2023-09-06T15:00:44Z",
        "summary": "Today, using Large-scale generative Language Models (LLMs) it is possible to\nsimulate free responses to interview questions like those traditionally\nanalyzed using qualitative research methods. Qualitative methodology\nencompasses a broad family of techniques involving manual analysis of\nopen-ended interviews or conversations conducted freely in natural language.\nHere we consider whether artificial \"silicon participants\" generated by LLMs\nmay be productively studied using qualitative methods aiming to produce\ninsights that could generalize to real human populations. The key concept in\nour analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)\ncapturing the degree to which LLM-generated outputs mirror human\nsub-populations' beliefs and attitudes. By definition, high algorithmic\nfidelity suggests latent beliefs elicited from LLMs may generalize to real\nhumans, whereas low algorithmic fidelity renders such research invalid. Here we\nused an LLM to generate interviews with silicon participants matching specific\ndemographic characteristics one-for-one with a set of human participants. Using\nframework-based qualitative analysis, we showed the key themes obtained from\nboth human and silicon participants were strikingly similar. However, when we\nanalyzed the structure and tone of the interviews we found even more striking\ndifferences. We also found evidence of the hyper-accuracy distortion described\nby Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not\nhave sufficient algorithmic fidelity to expect research on it to generalize to\nhuman populations. However, the rapid pace of LLM research makes it plausible\nthis could change in the future. Thus we stress the need to establish epistemic\nnorms now around how to assess validity of LLM-based qualitative research,\nespecially concerning the need to ensure representation of heterogeneous lived\nexperiences.",
        "pdf_link": "https://arxiv.org/pdf/2309.06364v3.pdf"
    },
    {
        "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection",
        "authors": [
            "Yu Chen",
            "Tingxin Li",
            "Huiming Liu",
            "Yang Yu"
        ],
        "published": "2023-09-06T14:54:11Z",
        "summary": "Numerous companies have started offering services based on large language\nmodels (LLM), such as ChatGPT, which inevitably raises privacy concerns as\nusers' prompts are exposed to the model provider. Previous research on secure\nreasoning using multi-party computation (MPC) has proven to be impractical for\nLLM applications due to its time-consuming and communication-intensive nature.\nWhile lightweight anonymization techniques can protect private information in\nprompts through substitution or masking, they fail to recover sensitive data\nreplaced in the LLM-generated results. In this paper, we expand the application\nscenarios of anonymization techniques by training a small local model to\nde-anonymize the LLM's returned results with minimal computational overhead. We\nintroduce the HaS framework, where \"H(ide)\" and \"S(eek)\" represent its two core\nprocesses: hiding private entities for anonymization and seeking private\nentities for de-anonymization, respectively. To quantitatively assess HaS's\nprivacy protection performance, we propose both black-box and white-box\nadversarial models. Furthermore, we conduct experiments to evaluate HaS's\nusability in translation and classification tasks. The experimental findings\ndemonstrate that the HaS framework achieves an optimal balance between privacy\nprotection and utility.",
        "pdf_link": "https://arxiv.org/pdf/2309.03057v1.pdf"
    },
    {
        "title": "Aligning Large Language Models for Clinical Tasks",
        "authors": [
            "Supun Manathunga",
            "Isuru Hettigoda"
        ],
        "published": "2023-09-06T10:20:06Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable adaptability,\nshowcasing their capacity to excel in tasks for which they were not explicitly\ntrained. However, despite their impressive natural language processing (NLP)\ncapabilities, effective alignment of LLMs remains a crucial challenge when\ndeploying them for specific clinical applications. The ability to generate\nresponses with factually accurate content and to engage in non-trivial\nreasoning steps are crucial for the LLMs to be eligible for applications in\nclinical medicine. Employing a combination of techniques including\ninstruction-tuning and in-prompt strategies like few-shot and chain-of-thought\nprompting has significantly enhanced the performance of LLMs. Our proposed\nalignment strategy for medical question-answering, known as\n'expand-guess-refine', offers a parameter and data-efficient solution. A\npreliminary analysis of this method demonstrated outstanding performance,\nachieving a score of 70.63% on a subset of questions sourced from the USMLE\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2309.02884v2.pdf"
    },
    {
        "title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language Models",
        "authors": [
            "Liang Li",
            "Qingyuan Li",
            "Bo Zhang",
            "Xiangxiang Chu"
        ],
        "published": "2023-09-06T06:51:15Z",
        "summary": "As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2309.02784v2.pdf"
    },
    {
        "title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
        "authors": [
            "Zonglin Yang",
            "Xinya Du",
            "Junxian Li",
            "Jie Zheng",
            "Soujanya Poria",
            "Erik Cambria"
        ],
        "published": "2023-09-06T05:19:41Z",
        "summary": "Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction is under a\nconstrained setting: (1) the observation annotations in the dataset are\ncarefully manually handpicked sentences (resulting in a close-domain setting);\nand (2) the ground truth hypotheses are mostly commonsense knowledge, making\nthe task less challenging. In this work, we tackle these problems by proposing\nthe first NLP dataset for social science academic hypotheses discovery,\nconsisting of 50 recent top social science publications; and a raw web corpus\nthat contains enough information to make it possible to develop all the\nresearch hypotheses in the 50 papers. The final goal is to create systems that\nautomatically generate valid, novel, and helpful scientific hypotheses, given\nonly a pile of raw web corpus. Different from the previous settings, the new\ndataset requires (1) using open-domain data (raw web corpus) as observations;\nand (2) proposing hypotheses even new to humanity. A multi-module framework is\ndeveloped for the task, as well as three different feedback mechanisms that\nempirically show performance gain over the base framework. Finally, our\nframework exhibits superior performance in terms of both GPT-4 based evaluation\nand expert-based evaluation.To the best of our knowledge, this is the first\nwork showing that LLMs are able to generate novel (\"not existing in the\nliterature\") and valid (\"reflecting reality\") scientific hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2309.02726v2.pdf"
    },
    {
        "title": "Zero-Resource Hallucination Prevention for Large Language Models",
        "authors": [
            "Junyu Luo",
            "Cao Xiao",
            "Fenglong Ma"
        ],
        "published": "2023-09-06T01:57:36Z",
        "summary": "The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2309.02654v3.pdf"
    },
    {
        "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
        "authors": [
            "Jensen Gao",
            "Bidipta Sarkar",
            "Fei Xia",
            "Ted Xiao",
            "Jiajun Wu",
            "Brian Ichter",
            "Anirudha Majumdar",
            "Dorsa Sadigh"
        ],
        "published": "2023-09-05T20:21:03Z",
        "summary": "Recent advances in vision-language models (VLMs) have led to improved\nperformance on tasks such as visual question answering and image captioning.\nConsequently, these models are now well-positioned to reason about the physical\nworld, particularly within domains such as robotic manipulation. However,\ncurrent VLMs are limited in their understanding of the physical concepts (e.g.,\nmaterial, fragility) of common objects, which restricts their usefulness for\nrobotic manipulation tasks that involve interaction and physical reasoning\nabout such objects. To address this limitation, we propose PhysObjects, an\nobject-centric dataset of 39.6K crowd-sourced and 417K automated physical\nconcept annotations of common household objects. We demonstrate that\nfine-tuning a VLM on PhysObjects improves its understanding of physical object\nconcepts, including generalization to held-out concepts, by capturing human\npriors of these concepts from visual appearance. We incorporate this physically\ngrounded VLM in an interactive framework with a large language model-based\nrobotic planner, and show improved planning performance on tasks that require\nreasoning about physical object concepts, compared to baselines that do not\nleverage physically grounded VLMs. We additionally illustrate the benefits of\nour physically grounded VLM on a real robot, where it improves task success\nrates. We release our dataset and provide further details and visualizations of\nour results at https://iliad.stanford.edu/pg-vlm/.",
        "pdf_link": "https://arxiv.org/pdf/2309.02561v4.pdf"
    },
    {
        "title": "Automating Behavioral Testing in Machine Translation",
        "authors": [
            "Javier Ferrando",
            "Matthias Sperber",
            "Hendra Setiawan",
            "Dominic Telaar",
            "Sa≈°a Hasan"
        ],
        "published": "2023-09-05T19:40:45Z",
        "summary": "Behavioral testing in NLP allows fine-grained evaluation of systems by\nexamining their linguistic capabilities through the analysis of input-output\nbehavior. Unfortunately, existing work on behavioral testing in Machine\nTranslation (MT) is currently restricted to largely handcrafted tests covering\na limited range of capabilities and languages. To address this limitation, we\npropose to use Large Language Models (LLMs) to generate a diverse set of source\nsentences tailored to test the behavior of MT models in a range of situations.\nWe can then verify whether the MT model exhibits the expected behavior through\nmatching candidate sets that are also generated using LLMs. Our approach aims\nto make behavioral testing of MT systems practical while requiring only minimal\nhuman effort. In our experiments, we apply our proposed evaluation framework to\nassess multiple available MT systems, revealing that while in general\npass-rates follow the trends observable from traditional accuracy-based\nmetrics, our method was able to uncover several important differences and\npotential bugs that go unnoticed when relying only on accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2309.02553v3.pdf"
    },
    {
        "title": "Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices",
        "authors": [
            "Bojia Zi",
            "Xianbiao Qi",
            "Lingzhi Wang",
            "Jianan Wang",
            "Kam-Fai Wong",
            "Lei Zhang"
        ],
        "published": "2023-09-05T17:40:34Z",
        "summary": "In this paper, we present Delta-LoRA, which is a novel parameter-efficient\napproach to fine-tune large language models (LLMs). In contrast to LoRA and\nother low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates\nthe low-rank matrices $\\bA$ and $\\bB$, but also propagate the learning to the\npre-trained weights $\\bW$ via updates utilizing the delta of the product of two\nlow-rank matrices ($\\bA^{(t+1)}\\bB^{(t+1)} - \\bA^{(t)}\\bB^{(t)}$). Such a\nstrategy effectively addresses the limitation that the incremental update of\nlow-rank matrices is inadequate for learning representations capable for\ndownstream tasks. Moreover, as the update of $\\bW$ does not need to compute the\ngradients of $\\bW$ and store their momentums, Delta-LoRA shares comparable\nmemory requirements and computational costs with LoRA. Extensive experiments\nshow that Delta-LoRA significantly outperforms existing low-rank adaptation\nmethods. We further support these results with comprehensive analyses that\nunderscore the effectiveness of Delta-LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2309.02411v1.pdf"
    },
    {
        "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
        "authors": [
            "Yichong Leng",
            "Zhifang Guo",
            "Kai Shen",
            "Xu Tan",
            "Zeqian Ju",
            "Yanqing Liu",
            "Yufei Liu",
            "Dongchao Yang",
            "Leying Zhang",
            "Kaitao Song",
            "Lei He",
            "Xiang-Yang Li",
            "Sheng Zhao",
            "Tao Qin",
            "Jiang Bian"
        ],
        "published": "2023-09-05T14:45:27Z",
        "summary": "Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.",
        "pdf_link": "https://arxiv.org/pdf/2309.02285v2.pdf"
    },
    {
        "title": "Sample Size in Natural Language Processing within Healthcare Research",
        "authors": [
            "Jaya Chaturvedi",
            "Diana Shamsutdinova",
            "Felix Zimmer",
            "Sumithra Velupillai",
            "Daniel Stahl",
            "Robert Stewart",
            "Angus Roberts"
        ],
        "published": "2023-09-05T13:42:43Z",
        "summary": "Sample size calculation is an essential step in most data-based disciplines.\nLarge enough samples ensure representativeness of the population and determine\nthe precision of estimates. This is true for most quantitative studies,\nincluding those that employ machine learning methods, such as natural language\nprocessing, where free-text is used to generate predictions and classify\ninstances of text. Within the healthcare domain, the lack of sufficient corpora\nof previously collected data can be a limiting factor when determining sample\nsizes for new studies. This paper tries to address the issue by making\nrecommendations on sample sizes for text classification tasks in the healthcare\ndomain.\n  Models trained on the MIMIC-III database of critical care records from Beth\nIsrael Deaconess Medical Center were used to classify documents as having or\nnot having Unspecified Essential Hypertension, the most common diagnosis code\nin the database. Simulations were performed using various classifiers on\ndifferent sample sizes and class proportions. This was repeated for a\ncomparatively less common diagnosis code within the database of diabetes\nmellitus without mention of complication.\n  Smaller sample sizes resulted in better results when using a K-nearest\nneighbours classifier, whereas larger sample sizes provided better results with\nsupport vector machines and BERT models. Overall, a sample size larger than\n1000 was sufficient to provide decent performance metrics.\n  The simulations conducted within this study provide guidelines that can be\nused as recommendations for selecting appropriate sample sizes and class\nproportions, and for predicting expected performance, when building classifiers\nfor textual healthcare data. The methodology used here can be modified for\nsample size estimates calculations with other datasets.",
        "pdf_link": "https://arxiv.org/pdf/2309.02237v1.pdf"
    },
    {
        "title": "Language Models for Novelty Detection in System Call Traces",
        "authors": [
            "Quentin Fournier",
            "Daniel Aloise",
            "Leandro R. Costa"
        ],
        "published": "2023-09-05T13:11:40Z",
        "summary": "Due to the complexity of modern computer systems, novel and unexpected\nbehaviors frequently occur. Such deviations are either normal occurrences, such\nas software updates and new user activities, or abnormalities, such as\nmisconfigurations, latency issues, intrusions, and software bugs. Regardless,\nnovel behaviors are of great interest to developers, and there is a genuine\nneed for efficient and effective methods to detect them. Nowadays, researchers\nconsider system calls to be the most fine-grained and accurate source of\ninformation to investigate the behavior of computer systems. Accordingly, this\npaper introduces a novelty detection methodology that relies on a probability\ndistribution over sequences of system calls, which can be seen as a language\nmodel. Language models estimate the likelihood of sequences, and since\nnovelties deviate from previously observed behaviors by definition, they would\nbe unlikely under the model. Following the success of neural networks for\nlanguage models, three architectures are evaluated in this work: the widespread\nLSTM, the state-of-the-art Transformer, and the lower-complexity Longformer.\nHowever, large neural networks typically require an enormous amount of data to\nbe trained effectively, and to the best of our knowledge, no massive modern\ndatasets of kernel traces are publicly available. This paper addresses this\nlimitation by introducing a new open-source dataset of kernel traces comprising\nover 2 million web requests with seven distinct behaviors. The proposed\nmethodology requires minimal expert hand-crafting and achieves an F-score and\nAuROC greater than 95% on most novelties while being data- and task-agnostic.\nThe source code and trained models are publicly available on GitHub while the\ndatasets are available on Zenodo.",
        "pdf_link": "https://arxiv.org/pdf/2309.02206v1.pdf"
    },
    {
        "title": "An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models",
        "authors": [
            "Yusheng Liao",
            "Yutong Meng",
            "Hongcheng Liu",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2023-09-05T09:24:48Z",
        "summary": "Large language models (LLMs) have achieved significant success in interacting\nwith human. However, recent studies have revealed that these models often\nsuffer from hallucinations, leading to overly confident but incorrect\njudgments. This limits their application in the medical domain, where tasks\nrequire the utmost accuracy. This paper introduces an automated evaluation\nframework that assesses the practical capabilities of LLMs as virtual doctors\nduring multi-turn consultations. Consultation tasks are designed to require\nLLMs to be aware of what they do not know, to inquire about missing medical\ninformation from patients, and to ultimately make diagnoses. To evaluate the\nperformance of LLMs for these tasks, a benchmark is proposed by reformulating\nmedical multiple-choice questions from the United States Medical Licensing\nExaminations (USMLE), and comprehensive evaluation metrics are developed and\nevaluated on three constructed test sets. A medical consultation training set\nis further constructed to improve the consultation ability of LLMs. The results\nof the experiments show that fine-tuning with the training set can alleviate\nhallucinations and improve LLMs' performance on the proposed benchmark.\nExtensive experiments and ablation studies are conducted to validate the\neffectiveness and robustness of the proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2309.02077v1.pdf"
    },
    {
        "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models",
        "authors": [
            "Daoyuan Chen",
            "Yilun Huang",
            "Zhijian Ma",
            "Hesen Chen",
            "Xuchen Pan",
            "Ce Ge",
            "Dawei Gao",
            "Yuexiang Xie",
            "Zhaoyang Liu",
            "Jinyang Gao",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "published": "2023-09-05T08:22:07Z",
        "summary": "The immense evolution in Large Language Models (LLMs) has underscored the\nimportance of massive, heterogeneous, and high-quality data. A data recipe is a\nmixture of data from different sources for training LLMs, which plays a vital\nrole in LLMs' performance. Existing open-source tools for LLM data processing\nare mostly tailored for specific data recipes. To continuously uncover the\npotential of LLMs, incorporate data from new sources, and improve LLMs'\nperformance, we build a new system named Data-Juicer, with which we can\nefficiently generate diverse data recipes, explore different possibilities in\nforming data mixtures, and evaluate their effects on model performance.\nDifferent from traditional data-analytics pipelines, Data-Juicer faces some\nunique challenges. Firstly, the possible data sources for forming data recipes\nare truly heterogeneous and massive with various qualities. Secondly, it is\nextremely expensive to precisely evaluate data recipes' impact on LLMs'\nperformance. Thirdly, the end users of Data-Juicer, model developers, need\nsufficient flexibility to configure and evaluate different data recipes.\n  Data-Juicer features a fine-grained abstraction of pipelines for constructing\ndata recipes, with over 50 built-in operators for easy composition and\nextension. By incorporating visualization and auto-evaluation capabilities,\nData-Juicer enables a timely feedback loop for both LLM pre-training and\nfine-tuning. Further, Data-Juicer is optimized and integrated with ecosystems\nfor LLM training, evaluation, and distributed computing. The data recipes\nderived with Data-Juicer gain notable improvements on state-of-the-art LLMs, by\nup to 7.45% increase in averaged score across 16 LLM benchmarks and 17.5%\nhigher win rate in pair-wise GPT-4 evaluations. Our system, data recipes, and\ntutorials are released, calling for broader data-centric research on training\nand understanding LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.02033v3.pdf"
    },
    {
        "title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
        "authors": [
            "Yunhao Yang",
            "Anshul Tomar"
        ],
        "published": "2023-09-05T00:19:31Z",
        "summary": "The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2309.01868v1.pdf"
    },
    {
        "title": "Softmax Bias Correction for Quantized Generative Models",
        "authors": [
            "Nilesh Prasad Pandey",
            "Marios Fournarakis",
            "Chirag Patel",
            "Markus Nagel"
        ],
        "published": "2023-09-04T17:29:31Z",
        "summary": "Post-training quantization (PTQ) is the go-to compression technique for large\ngenerative models, such as stable diffusion or large language models. PTQ\nmethods commonly keep the softmax activation in higher precision as it has been\nshown to be very sensitive to quantization noise. However, this can lead to a\nsignificant runtime and power overhead during inference on resource-constraint\nedge devices. In this work, we investigate the source of the softmax\nsensitivity to quantization and show that the quantization operation leads to a\nlarge bias in the softmax output, causing accuracy degradation. To overcome\nthis issue, we propose an offline bias correction technique that improves the\nquantizability of softmax without additional compute during deployment, as it\ncan be readily absorbed into the quantization parameters. We demonstrate the\neffectiveness of our method on stable diffusion v1.5 and 125M-size OPT language\nmodel, achieving significant accuracy improvement for 8-bit quantized softmax.",
        "pdf_link": "https://arxiv.org/pdf/2309.01729v1.pdf"
    },
    {
        "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
        "authors": [
            "Raz Lapid",
            "Ron Langberg",
            "Moshe Sipper"
        ],
        "published": "2023-09-04T08:54:20Z",
        "summary": "Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.",
        "pdf_link": "https://arxiv.org/pdf/2309.01446v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-09-04T08:28:44Z",
        "summary": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.01431v2.pdf"
    },
    {
        "title": "Representations Matter: Embedding Modes of Large Language Models using Dynamic Mode Decomposition",
        "authors": [
            "Mohamed Akrout"
        ],
        "published": "2023-09-03T19:10:18Z",
        "summary": "Existing large language models (LLMs) are known for generating \"hallucinated\"\ncontent, namely a fabricated text of plausibly looking, yet unfounded, facts.\nTo identify when these hallucination scenarios occur, we examine the properties\nof the generated text in the embedding space. Specifically, we draw inspiration\nfrom the dynamic mode decomposition (DMD) tool in analyzing the pattern\nevolution of text embeddings across sentences. We empirically demonstrate how\nthe spectrum of sentence embeddings over paragraphs is constantly low-rank for\nthe generated text, unlike that of the ground-truth text. Importantly, we find\nthat evaluation cases having LLM hallucinations correspond to ground-truth\nembedding patterns with a higher number of modes being poorly approximated by\nthe few modes associated with LLM embedding patterns. In analogy to near-field\nelectromagnetic evanescent waves, the embedding DMD eigenmodes of the generated\ntext with hallucinations vanishes quickly across sentences as opposed to those\nof the ground-truth text. This suggests that the hallucinations result from\nboth the generation techniques and the underlying representation.",
        "pdf_link": "https://arxiv.org/pdf/2309.01245v1.pdf"
    },
    {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "authors": [
            "Yue Zhang",
            "Yafu Li",
            "Leyang Cui",
            "Deng Cai",
            "Lemao Liu",
            "Tingchen Fu",
            "Xinting Huang",
            "Enbo Zhao",
            "Yu Zhang",
            "Yulong Chen",
            "Longyue Wang",
            "Anh Tuan Luu",
            "Wei Bi",
            "Freda Shi",
            "Shuming Shi"
        ],
        "published": "2023-09-03T16:56:48Z",
        "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2309.01219v2.pdf"
    },
    {
        "title": "FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs",
        "authors": [
            "Zhenheng Tang",
            "Yuxin Wang",
            "Xin He",
            "Longteng Zhang",
            "Xinglin Pan",
            "Qiang Wang",
            "Rongfei Zeng",
            "Kaiyong Zhao",
            "Shaohuai Shi",
            "Bingsheng He",
            "Xiaowen Chu"
        ],
        "published": "2023-09-03T13:27:56Z",
        "summary": "The rapid growth of memory and computation requirements of large language\nmodels (LLMs) has outpaced the development of hardware, hindering people who\nlack large-scale high-end GPUs from training or deploying LLMs. However,\nconsumer-level GPUs, which constitute a larger market share, are typically\noverlooked in LLM due to their weaker computing performance, smaller storage\ncapacity, and lower communication bandwidth. Additionally, users may have\nprivacy concerns when interacting with remote LLMs. In this paper, we envision\na decentralized system unlocking the potential vast untapped consumer-level\nGPUs in pre-training, inference and fine-tuning of LLMs with privacy\nprotection. However, this system faces critical challenges, including limited\nCPU and GPU memory, low network bandwidth, the variability of peer and device\nheterogeneity. To address these challenges, our system design incorporates: 1)\na broker with backup pool to implement dynamic join and quit of computing\nproviders; 2) task scheduling with hardware performance to improve system\nefficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to\nachieve model and task universality; 4) abstracting intermediate represention\nand execution planes to ensure compatibility of various devices and deep\nlearning (DL) frameworks. Our performance analysis demonstrates that 50 RTX\n3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are\nsignificantly more expensive.",
        "pdf_link": "https://arxiv.org/pdf/2309.01172v1.pdf"
    },
    {
        "title": "Bias Testing and Mitigation in LLM-based Code Generation",
        "authors": [
            "Dong Huang",
            "Qingwen Bu",
            "Jie Zhang",
            "Xiaofei Xie",
            "Junjie Chen",
            "Heming Cui"
        ],
        "published": "2023-09-03T07:14:49Z",
        "summary": "Utilizing state-of-the-art Large Language Models (LLMs), automatic code\ngeneration models play a pivotal role in enhancing the productivity of software\ndevelopment procedures. As the adoption of LLMs becomes more widespread in\nsoftware coding ecosystems, a pressing issue has emerged: does the generated\ncode contain social bias and unfairness, such as those related to age, gender,\nand race? This issue concerns the integrity, fairness, and ethical foundation\nof software applications that depend on the code generated by these models, yet\nis under-explored in the literature. This paper presents a novel bias testing\nframework that is specifically designed for code generation tasks. Based on\nthis framework, we conduct an extensive evaluation of the bias in code\ngenerated by five state-of-the-art LLMs. Our findings reveal that 20.29% to\n44.93% code functions generated by the models under study are biased when\nhandling bias sensitive tasks (i.e., tasks that involve sensitive attributes\nsuch as age and gender). This indicates that the existing LLMs can be unfair in\ncode generation, posing risks of unintended and harmful software behaviors. To\nmitigate bias for code generation models, we evaluate five bias mitigation\nprompt strategies, i.e., utilizing bias testing results to refine the code\n(zero-shot), one-, few-shot, and two Chain-of-Thought (CoT) prompts. Our\nevaluation results illustrate that these strategies are all effective in\nmitigating bias. Overall, one-shot and few-shot learning are the two most\neffective. For GPT-4, 80% to 90% code bias can be removed with one-shot\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2309.14345v2.pdf"
    },
    {
        "title": "Explainability for Large Language Models: A Survey",
        "authors": [
            "Haiyan Zhao",
            "Hanjie Chen",
            "Fan Yang",
            "Ninghao Liu",
            "Huiqi Deng",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Mengnan Du"
        ],
        "published": "2023-09-02T22:14:26Z",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language processing. However, their internal mechanisms are still\nunclear and this lack of transparency poses unwanted risks for downstream\napplications. Therefore, understanding and explaining these models is crucial\nfor elucidating their behaviors, limitations, and social impacts. In this\npaper, we introduce a taxonomy of explainability techniques and provide a\nstructured overview of methods for explaining Transformer-based language\nmodels. We categorize techniques based on the training paradigms of LLMs:\ntraditional fine-tuning-based paradigm and prompting-based paradigm. For each\nparadigm, we summarize the goals and dominant approaches for generating local\nexplanations of individual predictions and global explanations of overall model\nknowledge. We also discuss metrics for evaluating generated explanations, and\ndiscuss how explanations can be leveraged to debug models and improve\nperformance. Lastly, we examine key challenges and emerging opportunities for\nexplanation techniques in the era of LLMs in comparison to conventional machine\nlearning models.",
        "pdf_link": "https://arxiv.org/pdf/2309.01029v3.pdf"
    },
    {
        "title": "eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models",
        "authors": [
            "Minsik Cho",
            "Keivan A. Vahid",
            "Qichen Fu",
            "Saurabh Adya",
            "Carlo C Del Mundo",
            "Mohammad Rastegari",
            "Devang Naik",
            "Peter Zatloukal"
        ],
        "published": "2023-09-02T15:16:35Z",
        "summary": "Since Large Language Models or LLMs have demonstrated high-quality\nperformance on many complex language tasks, there is a great interest in\nbringing these LLMs to mobile devices for faster responses and better privacy\nprotection. However, the size of LLMs (i.e., billions of parameters) requires\nhighly effective compression to fit into storage-limited devices. Among many\ncompression techniques, weight-clustering, a form of non-linear quantization,\nis one of the leading candidates for LLM compression, and supported by modern\nsmartphones. Yet, its training overhead is prohibitively significant for LLM\nfine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown\nthe state-of-the-art trade-off between compression ratio and accuracy\nregression, but its large memory complexity makes it nearly impossible to apply\nto train-time LLM compression. In this paper, we propose a memory-efficient DKM\nimplementation, eDKM powered by novel techniques to reduce the memory footprint\nof DKM by orders of magnitudes. For a given tensor to be saved on CPU for the\nbackward pass of DKM, we compressed the tensor by applying uniquification and\nsharding after checking if there is no duplicated tensor previously copied to\nCPU. Our experimental results demonstrate that \\prjname can fine-tune and\ncompress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with\nthe Alpaca dataset by reducing the train-time memory footprint of a decoder\nlayer by 130$\\times$, while delivering good accuracy on broader LLM benchmarks\n(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).",
        "pdf_link": "https://arxiv.org/pdf/2309.00964v2.pdf"
    },
    {
        "title": "Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports",
        "authors": [
            "Tom van Sonsbeek",
            "Xiantong Zhen",
            "Marcel Worring"
        ],
        "published": "2023-09-02T11:46:41Z",
        "summary": "The way we analyse clinical texts has undergone major changes over the last\nyears. The introduction of language models such as BERT led to adaptations for\nthe (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on\nlarge databases of archived medical documents. While performing well in terms\nof accuracy, both the lack of interpretability and limitations to transfer\nacross languages limit their use in clinical setting. We introduce a novel\nlight-weight graph-based embedding method specifically catering radiology\nreports. It takes into account the structure and composition of the report,\nwhile also connecting medical terms in the report through the multi-lingual\nSNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers\nthe underlying relationships among clinical terms, achieving a representation\nthat is better understandable for clinicians and clinically more accurate,\nwithout reliance on large pre-training datasets. We show the use of this\nembedding on two tasks namely disease classification of X-ray reports and image\nclassification. For disease classification our model is competitive with its\nBERT-based counterparts, while being magnitudes smaller in size and training\ndata requirements. For image classification, we show the effectiveness of the\ngraph embedding leveraging cross-modal knowledge transfer and show how this\nmethod is usable across different languages.",
        "pdf_link": "https://arxiv.org/pdf/2309.00917v2.pdf"
    },
    {
        "title": "Large Process Models: Business Process Management in the Age of Generative AI",
        "authors": [
            "Timotheus Kampik",
            "Christian Warmuth",
            "Adrian Rebmann",
            "Ron Agam",
            "Lukas N. P. Egger",
            "Andreas Gerber",
            "Johannes Hoffart",
            "Jonas Kolk",
            "Philipp Herzig",
            "Gero Decker",
            "Han van der Aa",
            "Artem Polyvyanyy",
            "Stefanie Rinderle-Ma",
            "Ingo Weber",
            "Matthias Weidlich"
        ],
        "published": "2023-09-02T10:32:53Z",
        "summary": "The continued success of Large Language Models (LLMs) and other generative\nartificial intelligence approaches highlights the advantages that large\ninformation corpora can have over rigidly defined symbolic models, but also\nserves as a proof-point of the challenges that purely statistics-based\napproaches have in terms of safety and trustworthiness. As a framework for\ncontextualizing the potential, as well as the limitations of LLMs and other\nfoundation model-based technologies, we propose the concept of a Large Process\nModel (LPM) that combines the correlation power of LLMs with the analytical\nprecision and reliability of knowledge-based systems and automated reasoning\napproaches. LPMs are envisioned to directly utilize the wealth of process\nmanagement experience that experts have accumulated, as well as process\nperformance data of organizations with diverse characteristics, e.g., regarding\nsize, region, or industry. In this vision, the proposed LPM would allow\norganizations to receive context-specific (tailored) process and other business\nmodels, analytical deep-dives, and improvement recommendations. As such, they\nwould allow to substantially decrease the time and effort required for business\ntransformation, while also allowing for deeper, more impactful, and more\nactionable insights than previously possible. We argue that implementing an LPM\nis feasible, but also highlight limitations and research challenges that need\nto be solved to implement particular aspects of the LPM vision.",
        "pdf_link": "https://arxiv.org/pdf/2309.00900v2.pdf"
    },
    {
        "title": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs",
        "authors": [
            "Md Adnan Arefeen",
            "Biplob Debnath",
            "Srimat Chakradhar"
        ],
        "published": "2023-09-02T06:33:18Z",
        "summary": "Question-answering (QA) is a significant application of Large Language Models\n(LLMs), shaping chatbot capabilities across healthcare, education, and customer\nservice. However, widespread LLM integration presents a challenge for small\nbusinesses due to the high expenses of LLM API usage. Costs rise rapidly when\ndomain-specific data (context) is used alongside queries for accurate\ndomain-specific LLM responses. One option is to summarize the context by using\nLLMs and reduce the context. However, this can also filter out useful\ninformation that is necessary to answer some domain-specific queries. In this\npaper, we shift from human-oriented summarizers to AI model-friendly summaries.\nOur approach, LeanContext, efficiently extracts $k$ key sentences from the\ncontext that are closely aligned with the query. The choice of $k$ is neither\nstatic nor random; we introduce a reinforcement learning technique that\ndynamically determines $k$ based on the query and context. The rest of the less\nimportant sentences are reduced using a free open source text reduction method.\nWe evaluate LeanContext against several recent query-aware and query-unaware\ncontext reduction approaches on prominent datasets (arxiv papers and BBC news\narticles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's\nROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline\nthat retains the entire context (no summarization). Additionally, if free\npretrained LLM-based summarizers are used to reduce context (into human\nconsumable summaries), LeanContext can further modify the reduced context to\nenhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2309.00841v1.pdf"
    },
    {
        "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
        "authors": [
            "Neel Jain",
            "Avi Schwarzschild",
            "Yuxin Wen",
            "Gowthami Somepalli",
            "John Kirchenbauer",
            "Ping-yeh Chiang",
            "Micah Goldblum",
            "Aniruddha Saha",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2023-09-01T17:59:44Z",
        "summary": "As Large Language Models quickly become ubiquitous, it becomes critical to\nunderstand their security vulnerabilities. Recent work shows that text\noptimizers can produce jailbreaking prompts that bypass moderation and\nalignment. Drawing from the rich body of work on adversarial machine learning,\nwe approach these attacks with three questions: What threat models are\npractically useful in this domain? How do baseline defense techniques perform\nin this new domain? How does LLM security differ from computer vision?\n  We evaluate several baseline defense strategies against leading adversarial\nattacks on LLMs, discussing the various settings in which each is feasible and\neffective. Particularly, we look at three types of defenses: detection\n(perplexity based), input preprocessing (paraphrase and retokenization), and\nadversarial training. We discuss white-box and gray-box settings and discuss\nthe robustness-performance trade-off for each of the defenses considered. We\nfind that the weakness of existing discrete optimizers for text, combined with\nthe relatively high costs of optimization, makes standard adaptive attacks more\nchallenging for LLMs. Future research will be needed to uncover whether more\npowerful optimizers can be developed, or whether the strength of filtering and\npreprocessing defenses is greater in the LLMs domain than it has been in\ncomputer vision.",
        "pdf_link": "https://arxiv.org/pdf/2309.00614v2.pdf"
    },
    {
        "title": "Taken out of context: On measuring situational awareness in LLMs",
        "authors": [
            "Lukas Berglund",
            "Asa Cooper Stickland",
            "Mikita Balesni",
            "Max Kaufmann",
            "Meg Tong",
            "Tomasz Korbak",
            "Daniel Kokotajlo",
            "Owain Evans"
        ],
        "published": "2023-09-01T17:27:37Z",
        "summary": "We aim to better understand the emergence of `situational awareness' in large\nlanguage models (LLMs). A model is situationally aware if it's aware that it's\na model and can recognize whether it's currently in testing or deployment.\nToday's LLMs are tested for safety and alignment before they are deployed. An\nLLM could exploit situational awareness to achieve a high score on safety\ntests, while taking harmful actions after deployment. Situational awareness may\nemerge unexpectedly as a byproduct of model scaling. One way to better foresee\nthis emergence is to run scaling experiments on abilities necessary for\nsituational awareness. As such an ability, we propose `out-of-context\nreasoning' (in contrast to in-context learning). We study out-of-context\nreasoning experimentally. First, we finetune an LLM on a description of a test\nwhile providing no examples or demonstrations. At test time, we assess whether\nthe model can pass the test. To our surprise, we find that LLMs succeed on this\nout-of-context reasoning task. Their success is sensitive to the training setup\nand only works when we apply data augmentation. For both GPT-3 and LLaMA-1,\nperformance improves with model size. These findings offer a foundation for\nfurther empirical study, towards predicting and potentially controlling the\nemergence of situational awareness in LLMs. Code is available at:\nhttps://github.com/AsaCooperStickland/situational-awareness-evals.",
        "pdf_link": "https://arxiv.org/pdf/2309.00667v1.pdf"
    },
    {
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
        "authors": [
            "Haotian Xu"
        ],
        "published": "2023-09-01T13:10:54Z",
        "summary": "Large language models (LLMs) demonstrate impressive language understanding\nand contextual learning abilities, making them suitable for natural language\nprocessing (NLP) tasks and complex mathematical reasoning. However, when\napplied to mathematical reasoning tasks, LLMs often struggle to generate\ncorrect reasoning steps and answers despite having high probabilities for the\nsolutions. To overcome this limitation and enhance the mathematical reasoning\ncapabilities of fine-tuned LLMs without additional fine-tuning steps, we\npropose a method that incorporates Monte Carlo Tree Search (MCTS) and a\nlightweight energy function to rank decision steps and enable immediate\nreaction and precise reasoning. Specifically, we re-formulate the fine-tuned\nLLMs into a Residual-based Energy Model (Residual-EBM) and employ noise\ncontrastive estimation to estimate the energy function's parameters. We then\nutilize MCTS with the energy function as a path verifier to search the output\nspace and evaluate the reasoning path. Through extensive experiments on two\nmathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the\nexceptional capabilities of our method, which significantly improves the pass@1\nmetric of the fine-tuned model without requiring additional fine-tuning or\nreinforcement learning with human feedback alignment.",
        "pdf_link": "https://arxiv.org/pdf/2309.03224v3.pdf"
    },
    {
        "title": "BatchPrompt: Accomplish more with less",
        "authors": [
            "Jianzhe Lin",
            "Maurice Diesendruck",
            "Liang Du",
            "Robin Abraham"
        ],
        "published": "2023-09-01T10:44:36Z",
        "summary": "As the ever-increasing token limits of large language models (LLMs) have\nenabled long context as input, prompting with single data samples might no\nlonger an efficient way. A straightforward strategy improving efficiency is to\nbatch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4),\nwhich we call BatchPrompt. We have two initial observations for prompting with\nbatched data. First, we find that prompting with batched data in longer\ncontexts will inevitably lead to worse performance, compared to single-data\nprompting. Second, the performance of the language model is significantly\ncorrelated with the positions and order of the batched data, due to the\ncorresponding change in decoder context. To retain efficiency and overcome\nperformance loss, we propose Batch Permutation and Ensembling (BPE), and a\nnovel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensive\nexperimental evaluation demonstrates that BPE can boost the performance of\nBatchPrompt with a striking margin on a range of popular NLP tasks, including\nquestion answering (Boolq), textual entailment (RTE), and duplicate questions\nidentification (QQP). These performances are even competitive with/higher than\nsingle-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLM\ncalls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32,\nusing just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with\n27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5%\nto 91.1% with 30.8% tokens). To the best of our knowledge, this is the first\nwork to technically improve prompting efficiency of large language models. We\nhope our simple yet effective approach will shed light on the future research\nof large language models. The code will be released.",
        "pdf_link": "https://arxiv.org/pdf/2309.00384v2.pdf"
    },
    {
        "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
        "authors": [
            "Varshini Subhash",
            "Anna Bialas",
            "Weiwei Pan",
            "Finale Doshi-Velez"
        ],
        "published": "2023-09-01T05:09:49Z",
        "summary": "Transformer based large language models with emergent capabilities are\nbecoming increasingly ubiquitous in society. However, the task of understanding\nand interpreting their internal workings, in the context of adversarial\nattacks, remains largely unsolved. Gradient-based universal adversarial attacks\nhave been shown to be highly effective on large language models and potentially\ndangerous due to their input-agnostic nature. This work presents a novel\ngeometric perspective explaining universal adversarial attacks on large\nlanguage models. By attacking the 117M parameter GPT-2 model, we find evidence\nindicating that universal adversarial triggers could be embedding vectors which\nmerely approximate the semantic information in their adversarial training\nregion. This hypothesis is supported by white-box model analysis comprising\ndimensionality reduction and similarity measurement of hidden representations.\nWe believe this new geometric perspective on the underlying mechanism driving\nuniversal attacks could help us gain deeper insight into the internal workings\nand failure modes of LLMs, thus enabling their mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2309.00254v1.pdf"
    },
    {
        "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking",
        "authors": [
            "Tsun-Hin Cheung",
            "Kin-Man Lam"
        ],
        "published": "2023-09-01T04:14:39Z",
        "summary": "Automatic fact-checking plays a crucial role in combating the spread of\nmisinformation. Large Language Models (LLMs) and Instruction-Following\nvariants, such as InstructGPT and Alpaca, have shown remarkable performance in\nvarious natural language processing tasks. However, their knowledge may not\nalways be up-to-date or sufficient, potentially leading to inaccuracies in\nfact-checking. To address this limitation, we propose combining the power of\ninstruction-following language models with external evidence retrieval to\nenhance fact-checking performance. Our approach involves leveraging search\nengines to retrieve relevant evidence for a given input claim. This external\nevidence serves as valuable supplementary information to augment the knowledge\nof the pretrained language model. Then, we instruct-tune an open-sourced\nlanguage model, called LLaMA, using this evidence, enabling it to predict the\nveracity of the input claim more accurately. To evaluate our method, we\nconducted experiments on two widely used fact-checking datasets: RAWFC and\nLIAR. The results demonstrate that our approach achieves state-of-the-art\nperformance in fact-checking tasks. By integrating external evidence, we bridge\nthe gap between the model's knowledge and the most up-to-date and sufficient\ncontext available, leading to improved fact-checking outcomes. Our findings\nhave implications for combating misinformation and promoting the dissemination\nof accurate information on online platforms. Our released materials are\naccessible at: https://thcheung.github.io/factllama.",
        "pdf_link": "https://arxiv.org/pdf/2309.00240v1.pdf"
    },
    {
        "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
        "authors": [
            "Zhichao Huang",
            "Chutong Meng",
            "Tom Ko"
        ],
        "published": "2023-08-31T23:26:10Z",
        "summary": "With recent rapid growth of large language models (LLMs), discrete speech\ntokenization has played an important role for injecting speech into LLMs.\nHowever, this discretization gives rise to a loss of information, consequently\nimpairing overall performance. To improve the performance of these discrete\nspeech tokens, we present RepCodec, a novel speech representation codec for\nsemantic speech tokenization. In contrast to audio codecs which reconstruct the\nraw audio, RepCodec learns a vector quantization codebook through\nreconstructing speech representations from speech encoders like HuBERT or\ndata2vec. Together, the speech encoder, the codec encoder and the vector\nquantization codebook form a pipeline for converting speech waveforms into\nsemantic tokens. The extensive experiments illustrate that RepCodec, by virtue\nof its enhanced information retention capacity, significantly outperforms the\nwidely used k-means clustering approach in both speech understanding and\ngeneration. Furthermore, this superiority extends across various speech\nencoders and languages, affirming the robustness of RepCodec. We believe our\nmethod can facilitate large language modeling research on speech processing.",
        "pdf_link": "https://arxiv.org/pdf/2309.00169v1.pdf"
    },
    {
        "title": "LLM in the Shell: Generative Honeypots",
        "authors": [
            "Muris Sladiƒá",
            "Veronica Valeros",
            "Carlos Catania",
            "Sebastian Garcia"
        ],
        "published": "2023-08-31T22:05:46Z",
        "summary": "Honeypots are essential tools in cybersecurity. However, most of them (even\nthe high-interaction ones) lack the required realism to engage and fool human\nattackers. This limitation makes them easily discernible, hindering their\neffectiveness. This work introduces a novel method to create dynamic and\nrealistic software honeypots based on Large Language Models. Preliminary\nresults indicate that LLMs can create credible and dynamic honeypots capable of\naddressing important limitations of previous honeypots, such as deterministic\nresponses, lack of adaptability, etc. We evaluated the realism of each command\nby conducting an experiment with human attackers who needed to say if the\nanswer from the honeypot was fake or not. Our proposed honeypot, called shelLM,\nreached an accuracy of 0.92. The source code and prompts necessary for\nreplicating the experiments have been made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2309.00155v2.pdf"
    },
    {
        "title": "Towards Multilingual Automatic Dialogue Evaluation",
        "authors": [
            "John Mendon√ßa",
            "Alon Lavie",
            "Isabel Trancoso"
        ],
        "published": "2023-08-31T15:15:26Z",
        "summary": "The main limiting factor in the development of robust multilingual dialogue\nevaluation metrics is the lack of multilingual data and the limited\navailability of open sourced multilingual dialogue systems. In this work, we\npropose a workaround for this lack of data by leveraging a strong multilingual\npretrained LLM and augmenting existing English dialogue data using Machine\nTranslation. We empirically show that the naive approach of finetuning a\npretrained multilingual encoder model with translated data is insufficient to\noutperform the strong baseline of finetuning a multilingual model with only\nsource data. Instead, the best approach consists in the careful curation of\ntranslated data using MT Quality Estimation metrics, excluding low quality\ntranslations that hinder its performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.16795v1.pdf"
    },
    {
        "title": "Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection",
        "authors": [
            "Kairui Hu",
            "Ming Yan",
            "Joey Tianyi Zhou",
            "Ivor W. Tsang",
            "Wen Haw Chong",
            "Yong Keong Yap"
        ],
        "published": "2023-08-31T14:31:48Z",
        "summary": "Stance detection aims to identify the attitude expressed in a document\ntowards a given target. Techniques such as Chain-of-Thought (CoT) prompting\nhave advanced this task, enhancing a model's reasoning capabilities through the\nderivation of intermediate rationales. However, CoT relies primarily on a\nmodel's pre-trained internal knowledge during reasoning, thereby neglecting the\nvaluable external information that is previously unknown to the model. This\nomission, especially within the unsupervised reasoning process, can affect the\nmodel's overall performance. Moreover, while CoT enhances Large Language Models\n(LLMs), smaller LMs, though efficient operationally, face challenges in\ndelivering nuanced reasoning. In response to these identified gaps, we\nintroduce the Ladder-of-Thought (LoT) for the stance detection task.\nConstructed through a dual-phase Progressive Optimization Framework, LoT\ndirects the small LMs to assimilate high-quality external knowledge, refining\nthe intermediate rationales produced. These bolstered rationales subsequently\nserve as the foundation for more precise predictions - akin to how a ladder\nfacilitates reaching elevated goals. LoT achieves a balance between efficiency\nand performance. Our empirical evaluations underscore LoT's efficacy, marking a\n16% improvement over GPT-3.5 and a 10% enhancement compared to GPT-3.5 with CoT\non stance detection task.",
        "pdf_link": "https://arxiv.org/pdf/2308.16763v2.pdf"
    },
    {
        "title": "Context Aware Query Rewriting for Text Rankers using LLM",
        "authors": [
            "Abhijit Anand",
            "Venktesh V",
            "Vinay Setty",
            "Avishek Anand"
        ],
        "published": "2023-08-31T14:19:50Z",
        "summary": "Query rewriting refers to an established family of approaches that are\napplied to underspecified and ambiguous queries to overcome the vocabulary\nmismatch problem in document ranking. Queries are typically rewritten during\nquery processing time for better query modelling for the downstream ranker.\nWith the advent of large-language models (LLMs), there have been initial\ninvestigations into using generative approaches to generate pseudo documents to\ntackle this inherent vocabulary gap. In this work, we analyze the utility of\nLLMs for improved query rewriting for text ranking tasks. We find that there\nare two inherent limitations of using LLMs as query re-writers -- concept drift\nwhen using only queries as prompts and large inference costs during query\nprocessing. We adopt a simple, yet surprisingly effective, approach called\ncontext aware query rewriting (CAR) to leverage the benefits of LLMs for query\nunderstanding. Firstly, we rewrite ambiguous training queries by context-aware\nprompting of LLMs, where we use only relevant documents as context.Unlike\nexisting approaches, we use LLM-based query rewriting only during the training\nphase. Eventually, a ranker is fine-tuned on the rewritten queries instead of\nthe original queries during training. In our extensive experiments, we find\nthat fine-tuning a ranker using re-written queries offers a significant\nimprovement of up to 33% on the passage ranking task and up to 28% on the\ndocument ranking task when compared to the baseline performance of using\noriginal queries.",
        "pdf_link": "https://arxiv.org/pdf/2308.16753v1.pdf"
    },
    {
        "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
        "authors": [
            "Nayeon Lee",
            "Chani Jung",
            "Junho Myung",
            "Jiho Jin",
            "Jose Camacho-Collados",
            "Juho Kim",
            "Alice Oh"
        ],
        "published": "2023-08-31T13:14:47Z",
        "summary": "Warning: this paper contains content that may be offensive or upsetting.\n  Most hate speech datasets neglect the cultural diversity within a single\nlanguage, resulting in a critical shortcoming in hate speech detection. To\naddress this, we introduce CREHate, a CRoss-cultural English Hate speech\ndataset. To construct CREHate, we follow a two-step procedure: 1) cultural post\ncollection and 2) cross-cultural annotation. We sample posts from the SBIC\ndataset, which predominantly represents North America, and collect posts from\nfour geographically diverse English-speaking countries (Australia, United\nKingdom, Singapore, and South Africa) using culturally hateful keywords we\nretrieve from our survey. Annotations are collected from the four countries\nplus the United States to establish representative labels for each country. Our\nanalysis highlights statistically significant disparities across countries in\nhate speech annotations. Only 56.2% of the posts in CREHate achieve consensus\namong all countries, with the highest pairwise label difference rate of 26%.\nQualitative analysis shows that label disagreement occurs mostly due to\ndifferent interpretations of sarcasm and the personal bias of annotators on\ndivisive topics. Lastly, we evaluate large language models (LLMs) under a\nzero-shot setting and show that current LLMs tend to show higher accuracies on\nAnglosphere country labels in CREHate. Our dataset and codes are available at:\nhttps://github.com/nlee0212/CREHate",
        "pdf_link": "https://arxiv.org/pdf/2308.16705v3.pdf"
    },
    {
        "title": "Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering",
        "authors": [
            "Lars-Peter Meyer",
            "Johannes Frey",
            "Kurt Junghanns",
            "Felix Brei",
            "Kirill Bulert",
            "Sabine Gr√ºnder-Fahrer",
            "Michael Martin"
        ],
        "published": "2023-08-31T10:31:19Z",
        "summary": "As the field of Large Language Models (LLMs) evolves at an accelerated pace,\nthe critical need to assess and monitor their performance emerges. We introduce\na benchmarking framework focused on knowledge graph engineering (KGE)\naccompanied by three challenges addressing syntax and error correction, facts\nextraction and dataset generation. We show that while being a useful tool, LLMs\nare yet unfit to assist in knowledge graph generation with zero-shot prompting.\nConsequently, our LLM-KG-Bench framework provides automatic evaluation and\nstorage of LLM responses as well as statistical data and visualization tools to\nsupport tracking of prompt engineering and model performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.16622v1.pdf"
    },
    {
        "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills",
        "authors": [
            "Amey Agrawal",
            "Ashish Panwar",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Bhargav S. Gulavani",
            "Ramachandran Ramjee"
        ],
        "published": "2023-08-31T00:03:02Z",
        "summary": "Large Language Model (LLM) inference consists of two distinct phases -\nprefill phase which processes the input prompt and decode phase which generates\noutput tokens autoregressively. While the prefill phase effectively saturates\nGPU compute at small batch sizes, the decode phase results in low compute\nutilization as it generates one token at a time per request. The varying\nprefill and decode times also lead to imbalance across micro-batches when using\npipeline parallelism, resulting in further inefficiency due to bubbles.\n  We present SARATHI to address these challenges. SARATHI employs\nchunked-prefills, which splits a prefill request into equal sized chunks, and\ndecode-maximal batching, which constructs a batch using a single prefill chunk\nand populates the remaining slots with decodes. During inference, the prefill\nchunk saturates GPU compute, while the decode requests 'piggyback' and cost up\nto an order of magnitude less compared to a decode-only batch. Chunked-prefills\nallows constructing multiple decode-maximal batches from a single prefill\nrequest, maximizing coverage of decodes that can piggyback. Furthermore, the\nuniform compute design of these batches ameliorates the imbalance between\nmicro-batches, significantly reducing pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware. For the LLaMA-13B model on A6000 GPU, SARATHI improves\ndecode throughput by up to 10x, and accelerates end-to-end throughput by up to\n1.33x. For LLaMa-33B on A100 GPU, we achieve 1.25x higher end-to-end-throughput\nand up to 4.25x higher decode throughput. When used with pipeline parallelism\non GPT-3, SARATHI reduces bubbles by 6.29x, resulting in an end-to-end\nthroughput improvement of 1.91x.",
        "pdf_link": "https://arxiv.org/pdf/2308.16369v1.pdf"
    },
    {
        "title": "Large Language Models as Data Preprocessors",
        "authors": [
            "Haochen Zhang",
            "Yuyang Dong",
            "Chuan Xiao",
            "Masafumi Oyamada"
        ],
        "published": "2023-08-30T23:28:43Z",
        "summary": "Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's\nLLaMA variants, have marked a significant advancement in artificial\nintelligence. Trained on vast amounts of text data, LLMs are capable of\nunderstanding and generating human-like text across a diverse range of topics.\nThis study expands on the applications of LLMs, exploring their potential in\ndata preprocessing, a critical stage in data mining and analytics applications.\nWe delve into the applicability of state-of-the-art LLMs such as GPT-3.5,\nGPT-4, and Vicuna-13B for error detection, data imputation, schema matching,\nand entity matching tasks. Alongside showcasing the inherent capabilities of\nLLMs, we highlight their limitations, particularly in terms of computational\nexpense and inefficiency. We propose an LLM-based framework for data\npreprocessing, which integrates cutting-edge prompt engineering techniques,\ncoupled with traditional methods like contextualization and feature selection,\nto improve the performance and efficiency of these models. The effectiveness of\nLLMs in data preprocessing is evaluated through an experimental study spanning\n12 datasets. GPT-4 emerged as a standout, achieving 100\\% accuracy or F1 score\non 4 datasets, suggesting LLMs' immense potential in these tasks. Despite\ncertain limitations, our study underscores the promise of LLMs in this domain\nand anticipates future developments to overcome current hurdles.",
        "pdf_link": "https://arxiv.org/pdf/2308.16361v1.pdf"
    },
    {
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
        "authors": [
            "Chi Han",
            "Qifan Wang",
            "Hao Peng",
            "Wenhan Xiong",
            "Yu Chen",
            "Heng Ji",
            "Sinong Wang"
        ],
        "published": "2023-08-30T16:47:51Z",
        "summary": "Today's large language models (LLMs) typically train on short text segments\n(e.g., <4K tokens) due to the quadratic complexity of their Transformer\narchitectures. As a result, their performance suffers drastically on inputs\nlonger than those encountered during training, substantially limiting their\napplications in real-world tasks involving long contexts such as encoding\nscientific articles, code repositories, or long dialogues. Through theoretical\nanalysis and empirical investigation, this work identifies three major factors\ncontributing to this length generalization failure. Our theoretical analysis\nfurther reveals that commonly used techniques like truncating the attention\nwindow or relative positional encodings are inadequate to address them.\nAnswering these challenges, we propose LM-Infinite, a simple and effective\nmethod for enhancing LLMs' capabilities of handling long contexts. LM-Infinite\nis highly flexible and can be used with most modern LLMs off-the-shelf. Without\nany parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\nto generalize to up to 200M length inputs while retaining perplexity. It also\nimproves performance on downstream tasks such as Passkey Retrieval and Qasper\nin the zero-shot setting. LM-Infinite brings substantial efficiency\nimprovements: it achieves 2.7x decoding speed up and 7.5x memory saving over\nthe original model. Our code will be publicly available upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2308.16137v6.pdf"
    },
    {
        "title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models",
        "authors": [
            "Qingyuan Li",
            "Yifan Zhang",
            "Liang Li",
            "Peng Yao",
            "Bo Zhang",
            "Xiangxiang Chu",
            "Yerui Sun",
            "Li Du",
            "Yuchen Xie"
        ],
        "published": "2023-08-30T12:18:18Z",
        "summary": "In the era of large-scale language models, the substantial parameter size\nposes significant challenges for deployment. Being a prevalent compression\ntechnique, quantization has emerged as the mainstream practice to tackle this\nissue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and\nactivations in such bit widths). In this study, we propose a novel W4A8\npost-training quantization method for the available open-sourced LLMs, which\ncombines the advantages of both two recipes. Therefore, we can leverage the\nbenefit in the I/O utilization of 4-bit weight quantization and the\nacceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces\nnotorious performance degradation. As a remedy, we involve layerwise activation\nquantization strategies which feature a novel logarithmic equalization for most\nintractable layers, and we combine them with fine-grained weight quantization.\nWithout whistles and bells, we eliminate the necessity for further fine-tuning\nand obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and\nLLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is\nachievable for the deployment of large language models, fostering their\nwide-spreading real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2308.15987v1.pdf"
    },
    {
        "title": "Quantifying and Analyzing Entity-level Memorization in Large Language Models",
        "authors": [
            "Zhenhong Zhou",
            "Jiuyang Xiang",
            "Chaomeng Chen",
            "Sen Su"
        ],
        "published": "2023-08-30T03:06:47Z",
        "summary": "Large language models (LLMs) have been proven capable of memorizing their\ntraining data, which can be extracted through specifically designed prompts. As\nthe scale of datasets continues to grow, privacy risks arising from\nmemorization have attracted increasing attention. Quantifying language model\nmemorization helps evaluate potential privacy risks. However, prior works on\nquantifying memorization require access to the precise original data or incur\nsubstantial computational overhead, making it difficult for applications in\nreal-world language models. To this end, we propose a fine-grained,\nentity-level definition to quantify memorization with conditions and metrics\ncloser to real-world scenarios. In addition, we also present an approach for\nefficiently extracting sensitive entities from autoregressive language models.\nWe conduct extensive experiments based on the proposed, probing language\nmodels' ability to reconstruct sensitive entities under different settings. We\nfind that language models have strong memorization at the entity level and are\nable to reproduce the training data even with partial leakages. The results\ndemonstrate that LLMs not only memorize their training data but also understand\nassociations between entities. These findings necessitate that trainers of LLMs\nexercise greater prudence regarding model memorization, adopting memorization\nmitigation techniques to preclude privacy violations.",
        "pdf_link": "https://arxiv.org/pdf/2308.15727v2.pdf"
    },
    {
        "title": "Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model",
        "authors": [
            "Kazuki Hori",
            "Kanata Suzuki",
            "Tetsuya Ogata"
        ],
        "published": "2023-08-30T00:54:44Z",
        "summary": "The application of the Large Language Model (LLM) to robot action planning\nhas been actively studied. The instructions given to the LLM by natural\nlanguage may include ambiguity and lack of information depending on the task\ncontext. It is possible to adjust the output of LLM by making the instruction\ninput more detailed; however, the design cost is high. In this paper, we\npropose the interactive robot action planning method that allows the LLM to\nanalyze and gather missing information by asking questions to humans. The\nmethod can minimize the design cost of generating precise robot instructions.\nWe demonstrated the effectiveness of our method through concrete examples in\ncooking tasks. However, our experiments also revealed challenges in robot\naction planning with LLM, such as asking unimportant questions and assuming\ncrucial information without asking. Shedding light on these issues provides\nvaluable insights for future research on utilizing LLM for robotics.",
        "pdf_link": "https://arxiv.org/pdf/2308.15684v2.pdf"
    },
    {
        "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",
        "authors": [
            "Angus Addlesee",
            "Weronika Siei≈Ñska",
            "Nancie Gunson",
            "Daniel Hern√°ndez Garcia",
            "Christian Dondrup",
            "Oliver Lemon"
        ],
        "published": "2023-08-29T11:40:03Z",
        "summary": "This paper evaluates the extent to which current Large Language Models (LLMs)\ncan capture task-oriented multi-party conversations (MPCs). We have recorded\nand transcribed 29 MPCs between patients, their companions, and a social robot\nin a hospital. We then annotated this corpus for multi-party goal-tracking and\nintent-slot recognition. People share goals, answer each other's goals, and\nprovide other people's goals in MPCs - none of which occur in dyadic\ninteractions. To understand user goals in MPCs, we compared three methods in\nzero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks\nto train DialogLM using LED, and employed prompt engineering techniques with\nGPT-3.5-turbo, to determine which approach can complete this novel task with\nlimited data. GPT-3.5-turbo significantly outperformed the others in a few-shot\nsetting. The `reasoning' style prompt, when given 7% of the corpus as example\nannotated conversations, was the best performing method. It correctly annotated\n62.32% of the goal tracking MPCs, and 69.57% of the intent-slot recognition\nMPCs. A `story' style prompt increased model hallucination, which could be\ndetrimental if deployed in safety-critical settings. We conclude that\nmulti-party conversations still challenge state-of-the-art LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.15231v1.pdf"
    },
    {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
        "authors": [
            "Junyang Wang",
            "Yiyang Zhou",
            "Guohai Xu",
            "Pengcheng Shi",
            "Chenlin Zhao",
            "Haiyang Xu",
            "Qinghao Ye",
            "Ming Yan",
            "Ji Zhang",
            "Jihua Zhu",
            "Jitao Sang",
            "Haoyu Tang"
        ],
        "published": "2023-08-29T08:51:24Z",
        "summary": "Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.",
        "pdf_link": "https://arxiv.org/pdf/2308.15126v3.pdf"
    },
    {
        "title": "SwapMoE: Efficient Memory-Constrained Serving of Large Sparse MoE Models via Dynamic Expert Pruning and Swapping",
        "authors": [
            "Rui Kong",
            "Yuanchun Li",
            "Qingtian Feng",
            "Weijun Wang",
            "Linghe Kong",
            "Yunxin Liu"
        ],
        "published": "2023-08-29T05:25:21Z",
        "summary": "Mixture of experts (MoE) is a popular technique to improve capacity of large\nmodels with conditionally-activated parallel neural network modules (experts).\nDue to its remarkable scaling performance with sparse computation, it is widely\nused in modern Large Language Models (LLMs) and Large Vision Models (LVMs).\nHowever, serving such large models on edge devices is challenging due to memory\nconstraints. Typical solutions like memory swapping or weight pruning may lead\nto significantly higher latency or severe accuracy loss.\n  In this paper, we introduce SwapMoE, a framework for efficient continuous\nMoE-based large models serving with tunable memory budgets. The main idea of\nSwapMoE is to keep a small dynamic set of important experts, namely Virtual\nExperts, in the main memory for inference, while seamlessly maintaining how the\nVirtual Experts map to the actual experts. We use a profiling-guided planner to\nallocate the resources for SwapMoE that can fully utilize the memory budgets\nand bandwidth, and an importance-aware scheduler to efficiently identify,\nupdate, and use the Virtual Experts for accurate inference.\n  To evaluate SwapMoE, we conduct experiments on multiple edge devices with\nstate-of-the-art MoE-based Large Language Models and Large Vision Models. The\nresults demonstrate remarkable performance of SwapMoE under various memory\nconstraints. Specifically, SwapMoE can enable running large MoE models under\ntight memory budgets with similar latency to pruned compact models, while with\nsignificantly higher accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2308.15030v2.pdf"
    },
    {
        "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models",
        "authors": [
            "Qingyue Wang",
            "Liang Ding",
            "Yanan Cao",
            "Zhiliang Tian",
            "Shi Wang",
            "Dacheng Tao",
            "Li Guo"
        ],
        "published": "2023-08-29T04:59:53Z",
        "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.",
        "pdf_link": "https://arxiv.org/pdf/2308.15022v2.pdf"
    },
    {
        "title": "LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks",
        "authors": [
            "Haokun Liu",
            "Yaonan Zhu",
            "Kenji Kato",
            "Izumi Kondo",
            "Tadayoshi Aoyama",
            "Yasuhisa Hasegawa"
        ],
        "published": "2023-08-29T01:54:49Z",
        "summary": "This paper presents a novel approach to enhance autonomous robotic\nmanipulation using the Large Language Model (LLM) for logical inference,\nconverting high-level language commands into sequences of executable motion\nfunctions. The proposed system combines the advantage of LLM with YOLO-based\nenvironmental perception to enable robots to autonomously make reasonable\ndecisions and task planning based on the given commands. Additionally, to\naddress the potential inaccuracies or illogical actions arising from LLM, a\ncombination of teleoperation and Dynamic Movement Primitives (DMP) is employed\nfor action correction. This integration aims to improve the practicality and\ngeneralizability of the LLM-based human-robot collaboration system.",
        "pdf_link": "https://arxiv.org/pdf/2308.14972v1.pdf"
    },
    {
        "title": "Uncovering the Hidden Cost of Model Compression",
        "authors": [
            "Diganta Misra",
            "Muawiz Chaudhary",
            "Agam Goyal",
            "Bharat Runwal",
            "Pin Yu Chen"
        ],
        "published": "2023-08-29T01:47:49Z",
        "summary": "In an age dominated by resource-intensive foundation models, the ability to\nefficiently adapt to downstream tasks is crucial. Visual Prompting (VP),\ndrawing inspiration from the prompting techniques employed in Large Language\nModels (LLMs), has emerged as a pivotal method for transfer learning in the\nrealm of computer vision. As the importance of efficiency continues to rise,\nresearch into model compression has become indispensable in alleviating the\ncomputational burdens associated with training and deploying over-parameterized\nneural networks. A primary objective in model compression is to develop sparse\nand/or quantized models capable of matching or even surpassing the performance\nof their over-parameterized, full-precision counterparts. Although previous\nstudies have explored the effects of model compression on transfer learning,\nits impact on visual prompting-based transfer remains unclear. This study aims\nto bridge this gap, shedding light on the fact that model compression\ndetrimentally impacts the performance of visual prompting-based transfer,\nparticularly evident in scenarios with low data volume. Furthermore, our\nfindings underscore the adverse influence of sparsity on the calibration of\ndownstream visual-prompted models. However, intriguingly, we also illustrate\nthat such negative effects on calibration are not present when models are\ncompressed via quantization. This empirical investigation underscores the need\nfor a nuanced understanding beyond mere accuracy in sparse and quantized\nsettings, thereby paving the way for further exploration in Visual Prompting\ntechniques tailored for sparse and quantized models.",
        "pdf_link": "https://arxiv.org/pdf/2308.14969v3.pdf"
    },
    {
        "title": "Gender bias and stereotypes in Large Language Models",
        "authors": [
            "Hadas Kotek",
            "Rikker Dockum",
            "David Q. Sun"
        ],
        "published": "2023-08-28T22:32:05Z",
        "summary": "Large Language Models (LLMs) have made substantial progress in the past\nseveral months, shattering state-of-the-art benchmarks in many domains. This\npaper investigates LLMs' behavior with respect to gender stereotypes, a known\nissue for prior models. We use a simple paradigm to test the presence of gender\nbias, building on but differing from WinoBias, a commonly used gender bias\ndataset, which is likely to be included in the training data of current LLMs.\nWe test four recently published LLMs and demonstrate that they express biased\nassumptions about men and women's occupations. Our contributions in this paper\nare as follows: (a) LLMs are 3-6 times more likely to choose an occupation that\nstereotypically aligns with a person's gender; (b) these choices align with\npeople's perceptions better than with the ground truth as reflected in official\njob statistics; (c) LLMs in fact amplify the bias beyond what is reflected in\nperceptions or the ground truth; (d) LLMs ignore crucial ambiguities in\nsentence structure 95% of the time in our study items, but when explicitly\nprompted, they recognize the ambiguity; (e) LLMs provide explanations for their\nchoices that are factually inaccurate and likely obscure the true reason behind\ntheir predictions. That is, they provide rationalizations of their biased\nbehavior. This highlights a key property of these models: LLMs are trained on\nimbalanced datasets; as such, even with the recent successes of reinforcement\nlearning with human feedback, they tend to reflect those imbalances back at us.\nAs with other types of societal biases, we suggest that LLMs must be carefully\ntested to ensure that they treat minoritized individuals and communities\nequitably.",
        "pdf_link": "https://arxiv.org/pdf/2308.14921v1.pdf"
    },
    {
        "title": "Identifying and Mitigating the Security Risks of Generative AI",
        "authors": [
            "Clark Barrett",
            "Brad Boyd",
            "Elie Burzstein",
            "Nicholas Carlini",
            "Brad Chen",
            "Jihye Choi",
            "Amrita Roy Chowdhury",
            "Mihai Christodorescu",
            "Anupam Datta",
            "Soheil Feizi",
            "Kathleen Fisher",
            "Tatsunori Hashimoto",
            "Dan Hendrycks",
            "Somesh Jha",
            "Daniel Kang",
            "Florian Kerschbaum",
            "Eric Mitchell",
            "John Mitchell",
            "Zulfikar Ramzan",
            "Khawaja Shams",
            "Dawn Song",
            "Ankur Taly",
            "Diyi Yang"
        ],
        "published": "2023-08-28T18:51:09Z",
        "summary": "Every major technical invention resurfaces the dual-use dilemma -- the new\ntechnology has the potential to be used for good as well as for harm.\nGenerative AI (GenAI) techniques, such as large language models (LLMs) and\ndiffusion models, have shown remarkable capabilities (e.g., in-context\nlearning, code-completion, and text-to-image generation and editing). However,\nGenAI can be used just as well by attackers to generate new attacks and\nincrease the velocity and efficacy of existing attacks.\n  This paper reports the findings of a workshop held at Google (co-organized by\nStanford University and the University of Wisconsin-Madison) on the dual-use\ndilemma posed by GenAI. This paper is not meant to be comprehensive, but is\nrather an attempt to synthesize some of the interesting findings from the\nworkshop. We discuss short-term and long-term goals for the community on this\ntopic. We hope this paper provides both a launching point for a discussion on\nthis important topic as well as interesting problems that the research\ncommunity can work to address.",
        "pdf_link": "https://arxiv.org/pdf/2308.14840v4.pdf"
    },
    {
        "title": "Distilled GPT for Source Code Summarization",
        "authors": [
            "Chia-Yi Su",
            "Collin McMillan"
        ],
        "published": "2023-08-28T17:34:07Z",
        "summary": "A code summary is a brief natural language description of source code.\nSummaries are usually only a single sentence long, and yet form the backbone of\ndeveloper documentation. A short descriptions such as \"changes all visible\npolygons to the color blue\" can give a programmer a high-level idea of what\ncode does without the effort of reading the code itself. Recently, products\nbased on Large Language Models such as ChatGPT have demonstrated a strong\nability to write these descriptions automatically. However, to use these tools,\nprogrammers must send their code to untrusted third parties for processing\n(e.g., via an API call). This loss of custody is not acceptable to many\norganizations. In this paper, we present an alternative: we train an open\nsource model using sample output generated by GPT-3.5 in a process related to\nknowledge distillation. Our model is small enough (350m parameters) to be run\non a single 16gb GPU, yet we show in our evaluation that it is large enough to\nmimic GPT-3.5 on this task.",
        "pdf_link": "https://arxiv.org/pdf/2308.14731v2.pdf"
    },
    {
        "title": "Challenges of GPT-3-based Conversational Agents for Healthcare",
        "authors": [
            "Fabian Lechner",
            "Allison Lahnala",
            "Charles Welch",
            "Lucie Flek"
        ],
        "published": "2023-08-28T15:12:34Z",
        "summary": "The potential to provide patients with faster information access while\nallowing medical specialists to concentrate on critical tasks makes medical\ndomain dialog agents appealing. However, the integration of large-language\nmodels (LLMs) into these agents presents certain limitations that may result in\nserious consequences. This paper investigates the challenges and risks of using\nGPT-3-based models for medical question-answering (MedQA). We perform several\nevaluations contextualized in terms of standard medical principles. We provide\na procedure for manually designing patient queries to stress-test high-risk\nlimitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to\nrespond adequately to these queries, generating erroneous medical information,\nunsafe recommendations, and content that may be considered offensive.",
        "pdf_link": "https://arxiv.org/pdf/2308.14641v2.pdf"
    },
    {
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Hongchang Lyu",
            "Jiankai Tang",
            "Zhidian Huang",
            "Zhengxiao Du",
            "Xiao Liu",
            "Aohan Zeng",
            "Lei Hou",
            "Yuxiao Dong",
            "Jie Tang",
            "Juanzi Li"
        ],
        "published": "2023-08-28T11:53:40Z",
        "summary": "Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.",
        "pdf_link": "https://arxiv.org/pdf/2308.14508v1.pdf"
    },
    {
        "title": "Biomedical Entity Linking with Triple-aware Pre-Training",
        "authors": [
            "Xi Yan",
            "Cedric M√∂ller",
            "Ricardo Usbeck"
        ],
        "published": "2023-08-28T09:06:28Z",
        "summary": "Linking biomedical entities is an essential aspect in biomedical natural\nlanguage processing tasks, such as text mining and question answering. However,\na difficulty of linking the biomedical entities using current large language\nmodels (LLM) trained on a general corpus is that biomedical entities are\nscarcely distributed in texts and therefore have been rarely seen during\ntraining by the LLM. At the same time, those LLMs are not aware of high level\nsemantic connection between different biomedical entities, which are useful in\nidentifying similar concepts in different textual contexts. To cope with\naforementioned problems, some recent works focused on injecting knowledge graph\ninformation into LLMs. However, former methods either ignore the relational\nknowledge of the entities or lead to catastrophic forgetting. Therefore, we\npropose a novel framework to pre-train the powerful generative LLM by a corpus\nsynthesized from a KG. In the evaluations we are unable to confirm the benefit\nof including synonym, description or relational information.",
        "pdf_link": "https://arxiv.org/pdf/2308.14429v1.pdf"
    },
    {
        "title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models",
        "authors": [
            "Rongjie Yi",
            "Liwei Guo",
            "Shiyun Wei",
            "Ao Zhou",
            "Shangguang Wang",
            "Mengwei Xu"
        ],
        "published": "2023-08-28T06:56:08Z",
        "summary": "Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a\nrevolution in machine intelligence, owing to their exceptional capabilities in\na wide range of machine learning tasks. However, the transition of LLMs from\ndata centers to edge devices presents a set of challenges and opportunities.\nWhile this shift can enhance privacy and availability, it is hampered by the\nenormous parameter sizes of these models, leading to impractical runtime costs.\nIn light of these considerations, we introduce EdgeMoE, the first on-device\ninference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant\nof sparse LLMs that exhibit nearly constant computational complexity as their\nparameter size scales. EdgeMoE achieves both memory and computational\nefficiency by strategically partitioning the model across the storage\nhierarchy. Specifically, non-expert weights are stored in the device's memory,\nwhile expert weights are kept in external storage and are fetched into memory\nonly when they are activated. This design is underpinned by a crucial insight\nthat expert weights, though voluminous, are infrequently accessed due to sparse\nactivation patterns. To further mitigate the overhead associated with expert\nI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise\nbitwidth adaptation: This method reduces the size of expert weights with an\nacceptable level of accuracy loss. (2) Expert management: It predicts the\nexperts that will be activated in advance and preloads them into the\ncompute-I/O pipeline, thus further optimizing the process. In empirical\nevaluations conducted on well-established MoE LLMs and various edge devices,\nEdgeMoE demonstrates substantial memory savings and performance improvements\nwhen compared to competitive baseline solutions.",
        "pdf_link": "https://arxiv.org/pdf/2308.14352v1.pdf"
    },
    {
        "title": "Evaluating the Robustness to Instructions of Large Language Models",
        "authors": [
            "Yuansheng Ni",
            "Sichao Jiang",
            "Xinyu wu",
            "Hui Shen",
            "Yuli Zhou"
        ],
        "published": "2023-08-28T04:57:07Z",
        "summary": "Recently, Instruction fine-tuning has risen to prominence as a potential\nmethod for enhancing the zero-shot capabilities of Large Language Models (LLMs)\non novel tasks. This technique has shown an exceptional ability to boost the\nperformance of moderately sized LLMs, sometimes even reaching performance\nlevels comparable to those of much larger model variants. The focus is on the\nrobustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an\nexploration of six models including Alpaca, Vicuna, WizardLM, and Traditional\nTask-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction\ndatasets as case studies. We carried out a comprehensive evaluation of these\ninstruction-following LLMs which have been tuned based on open-domain\ninstructions and task-oriented instructions. The main discussion is their\nperformance and robustness towards instructions. We have observed that in most\ncases, the model's performance in dealing with unfamiliar instructions tends to\nworsen significantly, and the robustness of the model for RE instructions\ndeteriorates compared to QA. Further, we discovered that up until a certain\nparameter size threshold (3B), the performance of the FLAN-T5 model improves as\nthe parameter count increases. The robustness of different scales of FLAN-T5\nmodels to RE instruction is worse than the robustness to QA instruction.",
        "pdf_link": "https://arxiv.org/pdf/2308.14306v3.pdf"
    },
    {
        "title": "Symbolic and Language Agnostic Large Language Models",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-08-27T20:24:33Z",
        "summary": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\nan appropriate strategy of bottom-up reverse engineering of language at scale.\nHowever, due to the subsymbolic nature of these models whatever knowledge these\nsystems acquire about language will always be buried in millions of\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\ndue to their stochastic nature, these models will often fail in capturing\nvarious inferential aspects that are prevalent in natural language. What we\nsuggest here is employing the successful bottom-up strategy in a symbolic\nsetting, producing symbolic, language agnostic and ontologically grounded large\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2308.14199v1.pdf"
    },
    {
        "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
        "authors": [
            "Leonardo Ranaldi",
            "Giulia Pucci",
            "Andre Freitas"
        ],
        "published": "2023-08-27T19:22:12Z",
        "summary": "The language ability of Large Language Models (LLMs) is often unbalanced\ntowards English because of the imbalance in the distribution of the\npre-training data. This disparity is demanded in further fine-tuning and\naffecting the cross-lingual abilities of LLMs. In this paper, we propose to\nempower Instructiontuned LLMs (It-LLMs) in languages other than English by\nbuilding semantic alignment between them. Hence, we propose CrossAlpaca, an\nIt-LLM with cross-lingual instruction-following and Translation-following\ndemonstrations to improve semantic alignment between languages. We validate our\napproach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA\nand adapted versions of MMLU and BBH. Our models, tested over six different\nlanguages, outperform the It-LLMs tuned on monolingual data. The final results\nshow that instruction tuning on non-English data is not enough and that\nsemantic alignment can be further improved by Translation-following\ndemonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2308.14186v1.pdf"
    },
    {
        "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
        "authors": [
            "Kaiyuan Gao",
            "Sunan He",
            "Zhenyu He",
            "Jiacheng Lin",
            "QiZhi Pei",
            "Jie Shao",
            "Wei Zhang"
        ],
        "published": "2023-08-27T16:14:19Z",
        "summary": "Generative pre-trained transformer (GPT) models have revolutionized the field\nof natural language processing (NLP) with remarkable performance in various\ntasks and also extend their power to multimodal domains. Despite their success,\nlarge GPT models like GPT-4 face inherent limitations such as considerable\nsize, high computational requirements, complex deployment processes, and closed\ndevelopment loops. These constraints restrict their widespread adoption and\nraise concerns regarding their responsible development and usage. The need for\nuser-friendly, relatively small, and open-sourced alternative GPT models arises\nfrom the desire to overcome these limitations while retaining high performance.\nIn this survey paper, we provide an examination of alternative open-sourced\nmodels of large GPTs, focusing on user-friendly and relatively small models\nthat facilitate easier deployment and accessibility. Through this extensive\nsurvey, we aim to equip researchers, practitioners, and enthusiasts with a\nthorough understanding of user-friendly and relatively small open-sourced\nmodels of large GPTs, their current state, challenges, and future research\ndirections, inspiring the development of more efficient, accessible, and\nversatile GPT models that cater to the broader scientific community and advance\nthe field of general artificial intelligence. The source contents are\ncontinuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",
        "pdf_link": "https://arxiv.org/pdf/2308.14149v1.pdf"
    },
    {
        "title": "Detecting Language Model Attacks with Perplexity",
        "authors": [
            "Gabriel Alon",
            "Michael Kamfonas"
        ],
        "published": "2023-08-27T15:20:06Z",
        "summary": "A novel hack involving Large Language Models (LLMs) has emerged, exploiting\nadversarial suffixes to deceive models into generating perilous responses. Such\njailbreaks can trick LLMs into providing intricate instructions to a malicious\nuser for creating explosives, orchestrating a bank heist, or facilitating the\ncreation of offensive content. By evaluating the perplexity of queries with\nadversarial suffixes using an open-source LLM (GPT-2), we found that they have\nexceedingly high perplexity values. As we explored a broad range of regular\n(non-adversarial) prompt varieties, we concluded that false positives are a\nsignificant challenge for plain perplexity filtering. A Light-GBM trained on\nperplexity and token length resolved the false positives and correctly detected\nmost adversarial attacks in the test set.",
        "pdf_link": "https://arxiv.org/pdf/2308.14132v3.pdf"
    },
    {
        "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
        "authors": [
            "Chengkun Wei",
            "Wenlong Meng",
            "Zhikun Zhang",
            "Min Chen",
            "Minghu Zhao",
            "Wenjing Fang",
            "Lei Wang",
            "Zihui Zhang",
            "Wenzhi Chen"
        ],
        "published": "2023-08-26T15:21:47Z",
        "summary": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale\nlanguage models due to its strong downstream task performance and efficient\nmultitask serving ability. Despite its wide adoption, we empirically show that\nprompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside\nin the pretrained models and can affect arbitrary downstream tasks. The\nstate-of-the-art backdoor detection approaches cannot defend against\ntask-agnostic backdoors since they hardly converge in reversing the backdoor\ntriggers. To address this issue, we propose LMSanitator, a novel approach for\ndetecting and removing task-agnostic backdoors on Transformer models. Instead\nof directly inverting the triggers, LMSanitator aims to invert the predefined\nattack vectors (pretrained models' output when the input is embedded with\ntriggers) of the task-agnostic backdoors, which achieves much better\nconvergence performance and backdoor detection accuracy. LMSanitator further\nleverages prompt-tuning's property of freezing the pretrained model to perform\naccurate and fast output monitoring and input purging during the inference\nphase. Extensive experiments on multiple language models and NLP tasks\nillustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves\n92.8% backdoor detection accuracy on 960 models and decreases the attack\nsuccess rate to less than 1% in most scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.13904v2.pdf"
    },
    {
        "title": "Planning with Logical Graph-based Language Model for Instruction Generation",
        "authors": [
            "Fan Zhang",
            "Kebing Jin",
            "Hankz Hankui Zhuo"
        ],
        "published": "2023-08-26T06:28:14Z",
        "summary": "Despite the superior performance of large language models to generate natural\nlanguage texts, it is hard to generate texts with correct logic according to a\ngiven task, due to the difficulties for neural models to capture implied rules\nfrom free-form texts. In this paper, we propose a novel graph-based language\nmodel, Logical-GLM, to infuse logic into language models for more valid text\ngeneration and interpretability. Specifically, we first capture information\nfrom natural language instructions and construct logical bayes graphs that\ngenerally describe domains. Next, we generate logical skeletons to guide\nlanguage model training, infusing domain knowledge into language models.\nFinally, we alternately optimize the searching policy of graphs and language\nmodels until convergence. The experimental results show that Logical-GLM is\nboth effective and efficient compared with traditional language models, despite\nusing smaller-scale training data and fewer parameters. Our approach can\ngenerate instructional texts with more correct logic owing to the internalized\ndomain knowledge. Moreover, the usage of logical graphs reflects the inner\nmechanism of the language models, which improves the interpretability of\nblack-box models.",
        "pdf_link": "https://arxiv.org/pdf/2308.13782v1.pdf"
    },
    {
        "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
        "authors": [
            "Charles O'Neill",
            "Jack Miller",
            "Ioana Ciuca",
            "Yuan-Sen Ting",
            "Thang Bui"
        ],
        "published": "2023-08-26T05:20:58Z",
        "summary": "In this paper, we tackle the emerging challenge of unintended harmful content\ngeneration in Large Language Models (LLMs) with a novel dual-stage optimisation\ntechnique using adversarial fine-tuning. Our two-pronged approach employs an\nadversarial model, fine-tuned to generate potentially harmful prompts, and a\njudge model, iteratively optimised to discern these prompts. In this\nadversarial cycle, the two models seek to outperform each other in the\nprompting phase, generating a dataset of rich examples which are then used for\nfine-tuning. This iterative application of prompting and fine-tuning allows\ncontinuous refinement and improved performance. The performance of our approach\nis evaluated through classification accuracy on a dataset consisting of\nproblematic prompts not detected by GPT-4, as well as a selection of\ncontentious but unproblematic prompts. We show considerable increase in\nclassification accuracy of the judge model on this challenging dataset as it\nundergoes the optimisation process. Furthermore, we show that a rudimentary\nmodel \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set\nthan GPT-4 after only a few rounds of this process, and that this fine-tuning\nimproves performance in parallel tasks such as toxic comment identification.",
        "pdf_link": "https://arxiv.org/pdf/2308.13768v1.pdf"
    },
    {
        "title": "Rethinking Language Models as Symbolic Knowledge Graphs",
        "authors": [
            "Vishwas Mruthyunjaya",
            "Pouya Pezeshkpour",
            "Estevam Hruschka",
            "Nikita Bhutani"
        ],
        "published": "2023-08-25T21:25:08Z",
        "summary": "Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric\napplications such as search, question answering and recommendation. As\ncontemporary language models (LMs) trained on extensive textual data have\ngained prominence, researchers have extensively explored whether the parametric\nknowledge within these models can match up to that present in knowledge graphs.\nVarious methodologies have indicated that enhancing the size of the model or\nthe volume of training data enhances its capacity to retrieve symbolic\nknowledge, often with minimal or no human supervision. Despite these\nadvancements, there is a void in comprehensively evaluating whether LMs can\nencompass the intricate topological and semantic attributes of KGs, attributes\ncrucial for reasoning processes. In this work, we provide an exhaustive\nevaluation of language models of varying sizes and capabilities. We construct\nnine qualitative benchmarks that encompass a spectrum of attributes including\nsymmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,\nentity-centricity, bias and ambiguity. Additionally, we propose novel\nevaluation metrics tailored for each of these attributes. Our extensive\nevaluation of various LMs shows that while these models exhibit considerable\npotential in recalling factual information, their ability to capture intricate\ntopological and semantic traits of KGs remains significantly constrained. We\nnote that our proposed evaluation metrics are more reliable in evaluating these\nabilities than the existing metrics. Lastly, some of our benchmarks challenge\nthe common notion that larger LMs (e.g., GPT-4) universally outshine their\nsmaller counterparts (e.g., BERT).",
        "pdf_link": "https://arxiv.org/pdf/2308.13676v1.pdf"
    },
    {
        "title": "Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code",
        "authors": [
            "Jie JW Wu"
        ],
        "published": "2023-08-25T17:33:05Z",
        "summary": "Large language models (LLMs) have significantly improved the ability to\nperform tasks in the field of code generation. However, there is still a gap\nbetween LLMs being capable coders and being top-tier software engineers. Based\non the observation that toplevel software engineers often ask clarifying\nquestions to reduce ambiguity in both requirements and coding solutions, I\nargue that the same should be applied to LLMs for code generation tasks. By\nasking probing questions in various topics before generating the final code,\nthe challenges of programming with LLMs, such as unclear intent specification,\nlack of computational thinking, and undesired code quality, may be alleviated.\nThis, in turn, increases confidence in the generated code. In this work, I\nexplore how to leverage better communication skills to achieve greater\nconfidence in generated code. I propose a communication-centered process that\nuses an LLM-generated communicator to identify issues with high ambiguity or\nlow confidence in problem descriptions and generated code. I then ask\nclarifying questions to obtain responses from users for refining the code.",
        "pdf_link": "https://arxiv.org/pdf/2308.13507v2.pdf"
    },
    {
        "title": "The Poison of Alignment",
        "authors": [
            "Aibek Bekbayev",
            "Sungbae Chun",
            "Yerzat Dulat",
            "James Yamazaki"
        ],
        "published": "2023-08-25T15:51:15Z",
        "summary": "From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.",
        "pdf_link": "https://arxiv.org/pdf/2308.13449v1.pdf"
    },
    {
        "title": "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions",
        "authors": [
            "Reem I. Masoud",
            "Ziquan Liu",
            "Martin Ferianc",
            "Philip Treleaven",
            "Miguel Rodrigues"
        ],
        "published": "2023-08-25T14:50:13Z",
        "summary": "The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals from\nvarious cultural norms. Existing work investigated political and social biases\nand public opinions rather than their cultural values. To address this\nlimitation, the proposed Cultural Alignment Test (CAT) quantifies cultural\nalignment using Hofstede's cultural dimension framework, which offers an\nexplanatory cross-cultural comparison through the latent variable analysis. We\napply our approach to assess the cultural values embedded in state-of-the-art\nLLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United\nStates (US), Saudi Arabia, China, and Slovakia, using different prompting\nstyles and hyperparameter settings. Our results not only quantify cultural\nalignment of LLMs with certain countries, but also reveal the difference\nbetween LLMs in explanatory cultural dimensions. While all LLMs did not provide\nsatisfactory results in understanding cultural values, GPT-4 exhibited the\nhighest CAT score for the cultural values of the US.",
        "pdf_link": "https://arxiv.org/pdf/2309.12342v1.pdf"
    },
    {
        "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
        "authors": [
            "Keheng Wang",
            "Feiyu Duan",
            "Sirui Wang",
            "Peiguang Li",
            "Yunsen Xian",
            "Chuantao Yin",
            "Wenge Rong",
            "Zhang Xiong"
        ],
        "published": "2023-08-25T09:23:55Z",
        "summary": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown\nimpressive reasoning ability in various downstream tasks. Even so, suffering\nfrom hallucinations and the inability to access external knowledge, LLMs often\ncome with incorrect or unfaithful intermediate reasoning steps, especially in\nthe context of answering knowledge-intensive tasks such as KBQA. To alleviate\nthis issue, we propose a framework called Knowledge-Driven Chain-of-Thought\n(KD-CoT) to verify and modify reasoning traces in CoT via interaction with\nexternal knowledge, and thus overcome the hallucinations and error propagation.\nConcretely, we formulate the CoT rationale process of LLMs into a structured\nmulti-round QA format. In each round, LLMs interact with a QA system that\nretrieves external knowledge and produce faithful reasoning traces based on\nretrieved precise answers. The structured CoT reasoning of LLMs is facilitated\nby our developed KBQA CoT collection, which serves as in-context learning\ndemonstrations and can also be utilized as feedback augmentation to train a\nrobust retriever. Extensive experiments on WebQSP and ComplexWebQuestion\ndatasets demonstrate the effectiveness of proposed KD-CoT in task-solving\nreasoning generation, which outperforms the vanilla CoT ICL with an absolute\nsuccess rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented\nretriever outperforms the state-of-the-art baselines for retrieving knowledge,\nachieving significant improvement in Hit and recall performance. Our code and\ndata are released on https://github.com/AdelWang/KD-CoT/tree/main.",
        "pdf_link": "https://arxiv.org/pdf/2308.13259v2.pdf"
    },
    {
        "title": "Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese",
        "authors": [
            "Angana Borah",
            "Daria Pylypenko",
            "Cristina Espana-Bonet",
            "Josef van Genabith"
        ],
        "published": "2023-08-25T04:19:58Z",
        "summary": "Recent work has shown evidence of 'Clever Hans' behavior in high-performance\nneural translationese classifiers, where BERT-based classifiers capitalize on\nspurious correlations, in particular topic information, between data and target\nclassification labels, rather than genuine translationese signals.\nTranslationese signals are subtle (especially for professional translation) and\ncompete with many other signals in the data such as genre, style, author, and,\nin particular, topic. This raises the general question of how much of the\nperformance of a classifier is really due to spurious correlations in the data\nversus the signals actually targeted for by the classifier, especially for\nsubtle target signals and in challenging (low resource) data settings. We focus\non topic-based spurious correlation and approach the question from two\ndirections: (i) where we have no knowledge about spurious topic information and\nits distribution in the data, (ii) where we have some indication about the\nnature of spurious topic correlations. For (i) we develop a measure from first\nprinciples capturing alignment of unsupervised topics with target\nclassification labels as an indication of spurious topic information in the\ndata. We show that our measure is the same as purity in clustering and propose\na 'topic floor' (as in a 'noise floor') for classification. For (ii) we\ninvestigate masking of known spurious topic carriers in classification. Both\n(i) and (ii) contribute to quantifying and (ii) to mitigating spurious\ncorrelations.",
        "pdf_link": "https://arxiv.org/pdf/2308.13170v1.pdf"
    },
    {
        "title": "Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4",
        "authors": [
            "Maroa Mumtarin",
            "Md Samiullah Chowdhury",
            "Jonathan Wood"
        ],
        "published": "2023-08-25T00:09:16Z",
        "summary": "In traffic safety research, extracting information from crash narratives\nusing text analysis is a common practice. With recent advancements of large\nlanguage models (LLM), it would be useful to know how the popular LLM\ninterfaces perform in classifying or extracting information from crash\nnarratives. To explore this, our study has used the three most popular publicly\navailable LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their\nusefulness and boundaries in extracting information and answering queries\nrelated to accidents from 100 crash narratives from Iowa and Kansas. During the\ninvestigation, their capabilities and limitations were assessed and their\nresponses to the queries were compared. Five questions were asked related to\nthe narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has\nthe crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5)\nWhat are the sequence of harmful events in the crash? For questions 1 through\n4, the overall similarity among the LLMs were 70%, 35%, 96% and 89%,\nrespectively. The similarities were higher while answering direct questions\nrequiring binary responses and significantly lower for complex questions. To\ncompare the responses to question 5, network diagram and centrality measures\nwere analyzed. The network diagram from the three LLMs were not always similar\nalthough they sometimes have the same influencing events with high in-degree,\nout-degree and betweenness centrality. This study suggests using multiple\nmodels to extract viable information from narratives. Also, caution must be\npracticed while using these interfaces to obtain crucial safety related\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2308.13563v1.pdf"
    },
    {
        "title": "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal",
        "authors": [
            "Matej Zeƒçeviƒá",
            "Moritz Willig",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "published": "2023-08-24T20:23:13Z",
        "summary": "Some argue scale is all what is needed to achieve AI, covering even causal\nmodels. We make it clear that large language models (LLMs) cannot be causal and\ngive reason onto why sometimes we might feel otherwise. To this end, we define\nand exemplify a new subgroup of Structural Causal Model (SCM) that we call meta\nSCM which encode causal facts about other SCM within their variables. We\nconjecture that in the cases where LLM succeed in doing causal inference,\nunderlying was a respective meta SCM that exposed correlations between causal\nfacts in natural language on whose data the LLM was ultimately trained. If our\nhypothesis holds true, then this would imply that LLMs are like parrots in that\nthey simply recite the causal knowledge embedded in the data. Our empirical\nanalysis provides favoring evidence that current LLMs are even weak `causal\nparrots.'",
        "pdf_link": "https://arxiv.org/pdf/2308.13067v1.pdf"
    },
    {
        "title": "ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching",
        "authors": [
            "M. Caner Tol",
            "Berk Sunar"
        ],
        "published": "2023-08-24T20:04:36Z",
        "summary": "Security critical software, e.g., OpenSSL, comes with numerous side-channel\nleakages left unpatched due to a lack of resources or experts. The situation\nwill only worsen as the pace of code development accelerates, with developers\nrelying on Large Language Models (LLMs) to automatically generate code. In this\nwork, we explore the use of LLMs in generating patches for vulnerable code with\nmicroarchitectural side-channel leakages. For this, we investigate the\ngenerative abilities of powerful LLMs by carefully crafting prompts following a\nzero-shot learning approach. All generated code is dynamically analyzed by\nleakage detection tools, which are capable of pinpointing information leakage\nat the instruction level leaked either from secret dependent accesses or\nbranches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts\nare used to generate candidate replacements for vulnerable code, which are then\nanalyzed for correctness and for leakage resilience. From a cost/performance\nperspective, the GPT4-based configuration costs in API calls a mere few cents\nper vulnerability fixed. Our results show that LLM-based patching is far more\ncost-effective and thus provides a scalable solution. Finally, the framework we\npropose will improve in time, especially as vulnerability detection tools and\nLLMs mature.",
        "pdf_link": "https://arxiv.org/pdf/2308.13062v1.pdf"
    },
    {
        "title": "POLCA: Power Oversubscription in LLM Cloud Providers",
        "authors": [
            "Pratyush Patel",
            "Esha Choukse",
            "Chaojie Zhang",
            "√ç√±igo Goiri",
            "Brijesh Warrier",
            "Nithish Mahalingam",
            "Ricardo Bianchini"
        ],
        "published": "2023-08-24T16:32:34Z",
        "summary": "Recent innovation in large language models (LLMs), and their myriad use-cases\nhave rapidly driven up the compute capacity demand for datacenter GPUs. Several\ncloud providers and other enterprises have made substantial plans of growth in\ntheir datacenters to support these new workloads. One of the key bottleneck\nresources in datacenters is power, and given the increasing model sizes of\nLLMs, they are becoming increasingly power intensive. In this paper, we show\nthat there is a significant opportunity to oversubscribe power in LLM clusters.\nPower oversubscription improves the power efficiency of these datacenters,\nallowing more deployable servers per datacenter, and reduces the deployment\ntime, since building new datacenters is slow.\n  We extensively characterize the power consumption patterns of a variety of\nLLMs and their configurations. We identify the differences between the\ninference and training power consumption patterns. Based on our analysis of\nthese LLMs, we claim that the average and peak power utilization in LLM\nclusters for inference should not be very high. Our deductions align with the\ndata from production LLM clusters, revealing that inference workloads offer\nsubstantial headroom for power oversubscription. However, the stringent set of\ntelemetry and controls that GPUs offer in a virtualized environment, makes it\nchallenging to have a reliable and robust power oversubscription mechanism.\n  We propose POLCA, our framework for power oversubscription that is robust,\nreliable, and readily deployable for GPU clusters. Using open-source models to\nreplicate the power patterns observed in production, we simulate POLCA and\ndemonstrate that we can deploy 30% more servers in the same GPU cluster for\ninference, with minimal performance loss",
        "pdf_link": "https://arxiv.org/pdf/2308.12908v1.pdf"
    },
    {
        "title": "Large Language Models Vote: Prompting for Rare Disease Identification",
        "authors": [
            "David Oniani",
            "Jordan Hilsman",
            "Hang Dong",
            "Fengyi Gao",
            "Shiven Verma",
            "Yanshan Wang"
        ],
        "published": "2023-08-24T16:09:13Z",
        "summary": "The emergence of generative Large Language Models (LLMs) emphasizes the need\nfor accurate and efficient prompting approaches. LLMs are often applied in\nFew-Shot Learning (FSL) contexts, where tasks are executed with minimal\ntraining data. FSL has become popular in many Artificial Intelligence (AI)\nsubdomains, including AI for health. Rare diseases affect a small fraction of\nthe population. Rare disease identification from clinical notes inherently\nrequires FSL techniques due to limited data availability. Manual data\ncollection and annotation is both expensive and time-consuming. In this paper,\nwe propose Models-Vote Prompting (MVP), a flexible prompting approach for\nimproving the performance of LLM queries in FSL settings. MVP works by\nprompting numerous LLMs to perform the same tasks and then conducting a\nmajority vote on the resulting outputs. This method achieves improved results\nto any one model in the ensemble on one-shot rare disease identification and\nclassification tasks. We also release a novel rare disease dataset for FSL,\navailable to those who signed the MIMIC-IV Data Use Agreement (DUA).\nFurthermore, in using MVP, each model is prompted multiple times, substantially\nincreasing the time needed for manual annotation, and to address this, we\nassess the feasibility of using JSON for automating generative LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.12890v3.pdf"
    },
    {
        "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",
        "authors": [
            "Maximilian Mozes",
            "Xuanli He",
            "Bennett Kleinberg",
            "Lewis D. Griffin"
        ],
        "published": "2023-08-24T14:45:50Z",
        "summary": "Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.",
        "pdf_link": "https://arxiv.org/pdf/2308.12833v1.pdf"
    },
    {
        "title": "VIGC: Visual Instruction Generation and Correction",
        "authors": [
            "Bin Wang",
            "Fan Wu",
            "Xiao Han",
            "Jiahui Peng",
            "Huaping Zhong",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Weijia Li",
            "Wei Li",
            "Jiaqi Wang",
            "Conghui He"
        ],
        "published": "2023-08-24T11:21:05Z",
        "summary": "The integration of visual encoders and large language models (LLMs) has\ndriven recent progress in multimodal large language models (MLLMs). However,\nthe scarcity of high-quality instruction-tuning data for vision-language tasks\nremains a challenge. The current leading paradigm, such as LLaVA, relies on\nlanguage-only GPT-4 to generate data, which requires pre-annotated image\ncaptions and detection bounding boxes, suffering from understanding image\ndetails. A practical solution to this problem would be to utilize the available\nmultimodal large language models (MLLMs) to generate instruction data for\nvision-language tasks. However, it's worth noting that the currently accessible\nMLLMs are not as powerful as their LLM counterparts, as they tend to produce\ninadequate responses and generate false information. As a solution for\naddressing the current issue, this paper proposes the Visual Instruction\nGeneration and Correction (VIGC) framework that enables multimodal large\nlanguage models to generate instruction-tuning data and progressively enhance\nits quality on-the-fly. Specifically, Visual Instruction Generation (VIG)\nguides the vision-language model to generate diverse instruction-tuning data.\nTo ensure generation quality, Visual Instruction Correction (VIC) adopts an\niterative update mechanism to correct any inaccuracies in data produced by VIG,\neffectively reducing the risk of hallucination. Leveraging the diverse,\nhigh-quality data generated by VIGC, we finetune mainstream models and validate\ndata quality based on various evaluations. Experimental results demonstrate\nthat VIGC not only compensates for the shortcomings of language-only data\ngeneration methods, but also effectively enhances the benchmark performance.\nThe models, datasets, and code are available at\nhttps://opendatalab.github.io/VIGC.",
        "pdf_link": "https://arxiv.org/pdf/2308.12714v3.pdf"
    },
    {
        "title": "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions",
        "authors": [
            "Yijie Chen",
            "Yijin Liu",
            "Fandong Meng",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "published": "2023-08-24T09:32:29Z",
        "summary": "Large Language Models (LLMs) present strong general capabilities, and a\ncurrent compelling challenge is stimulating their specialized capabilities,\nsuch as machine translation, through low-cost instruction tuning. The standard\ninstruction-following data is sequentially organized as the concatenation of an\ninstruction, an input, and a response. As the attention mechanism of LLMs has\nlimitations on local focus, LLMs tend to focus more on the words or sentences\nnearby at each position. This leads to a high risk of instruction forgetting\nduring decoding. To alleviate the above issues, We propose SWIE\n(Segment-Weighted Instruction Embedding) and an instruction-following dataset\nOVERMISS. SWIE improves the model instruction understanding by adding a global\ninstruction representation on the following input and response representations.\nOVERMISS improves model faithfulness by comparing over-translation and\nmiss-translation results with the correct translation. We apply our methods to\ntwo main-stream open-source LLMs, BLOOM and LLaMA. The experimental results\ndemonstrate significant improvements in translation performance with SWIE based\non BLOOMZ-3b, particularly in zero-shot and long text translations due to\nreduced instruction forgetting risk. Additionally, OVERMISS outperforms the\nbaseline in translation performance (e.g. an increase in BLEU scores from 0.69\nto 3.12 and an average improvement of 0.48 percentage comet scores for\nLLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE\n(e.g. the BLUE scores increase up to 0.56 from English to German across three\ndifferent backbones), and both exhibit improvements in the faithfulness metric\nbased on word alignment.",
        "pdf_link": "https://arxiv.org/pdf/2308.12674v1.pdf"
    },
    {
        "title": "Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs",
        "authors": [
            "Ye Liu",
            "Semih Yavuz",
            "Rui Meng",
            "Meghana Moorthy",
            "Shafiq Joty",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-08-24T05:26:54Z",
        "summary": "The integration of retrieved passages and large language models (LLMs), such\nas ChatGPTs, has significantly contributed to improving open-domain question\nanswering. However, there is still a lack of exploration regarding the optimal\napproach for incorporating retrieved passages into the answer generation\nprocess. This paper aims to fill this gap by investigating different methods of\ncombining retrieved passages with LLMs to enhance answer generation. We begin\nby examining the limitations of a commonly-used concatenation approach.\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\nwhen the correct document is among the top-k retrieved passages. To address\nthis issue, we explore four alternative strategies for integrating the\nretrieved passages with the LLMs. These strategies include two single-round\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\nthat incorporate feedback loops. Through comprehensive analyses and\nexperiments, we provide insightful observations on how to effectively leverage\nretrieved passages to enhance the answer generation capability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.12574v2.pdf"
    },
    {
        "title": "Benchmarking Causal Study to Interpret Large Language Models for Source Code",
        "authors": [
            "Daniel Rodriguez-Cardenas",
            "David N. Palacio",
            "Dipin Khati",
            "Henry Burke",
            "Denys Poshyvanyk"
        ],
        "published": "2023-08-23T20:32:12Z",
        "summary": "One of the most common solutions adopted by software researchers to address\ncode generation is by training Large Language Models (LLMs) on massive amounts\nof source code. Although a number of studies have shown that LLMs have been\neffectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu),\nprevious research has largely overlooked the role of Causal Inference as a\nfundamental component of the interpretability of LLMs' performance. Existing\nbenchmarks and datasets are meant to highlight the difference between the\nexpected and the generated outcome, but do not take into account confounding\nvariables (e.g., lines of code, prompt size) that equally influence the\naccuracy metrics. The fact remains that, when dealing with generative software\ntasks by LLMs, no benchmark is available to tell researchers how to quantify\nneither the causal effect of SE-based treatments nor the correlation of\nconfounders to the model's performance. In an effort to bring statistical rigor\nto the evaluation of LLMs, this paper introduces a benchmarking strategy named\nGaleras comprised of curated testbeds for three SE tasks (i.e., code\ncompletion, code summarization, and commit generation) to help aid the\ninterpretation of LLMs' performance. We illustrate the insights of our\nbenchmarking strategy by conducting a case study on the performance of ChatGPT\nunder distinct prompt engineering methods. The results of the case study\ndemonstrate the positive causal influence of prompt semantics on ChatGPT's\ngenerative performance by an average treatment effect of $\\approx 3\\%$.\nMoreover, it was found that confounders such as prompt size are highly\ncorrelated with accuracy metrics ($\\approx 0.412\\%$). The end result of our\ncase study is to showcase causal inference evaluations, in practice, to reduce\nconfounding bias. By reducing the bias, we offer an interpretable solution for\nthe accuracy metric under analysis.",
        "pdf_link": "https://arxiv.org/pdf/2308.12415v1.pdf"
    },
    {
        "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification",
        "authors": [
            "Kushal Tirumala",
            "Daniel Simig",
            "Armen Aghajanyan",
            "Ari S. Morcos"
        ],
        "published": "2023-08-23T17:58:14Z",
        "summary": "Over recent years, an increasing amount of compute and data has been poured\ninto training large language models (LLMs), usually by doing one-pass learning\non as many tokens as possible randomly selected from large-scale web corpora.\nWhile training on ever-larger portions of the internet leads to consistent\nperformance improvements, the size of these improvements diminishes with scale,\nand there has been little work exploring the effect of data selection on\npre-training and downstream performance beyond simple de-duplication methods\nsuch as MinHash. Here, we show that careful data selection (on top of\nde-duplicated data) via pre-trained model embeddings can speed up training (20%\nefficiency gains) and improves average downstream accuracy on 16 NLP tasks (up\nto 2%) at the 6.7B model scale. Furthermore, we show that repeating data\nintelligently consistently outperforms baseline training (while repeating\nrandom data performs worse than baseline training). Our results indicate that\nclever data selection can significantly improve LLM pre-training, calls into\nquestion the common practice of training for a single epoch on as much data as\npossible, and demonstrates a path to keep improving our models past the limits\nof randomly sampling web data.",
        "pdf_link": "https://arxiv.org/pdf/2308.12284v1.pdf"
    },
    {
        "title": "Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models",
        "authors": [
            "Nancy Tyagi",
            "Aidin Shiri",
            "Surjodeep Sarkar",
            "Abhishek Kumar Umrawal",
            "Manas Gaur"
        ],
        "published": "2023-08-23T17:40:35Z",
        "summary": "Foundational Language Models (FLMs) have advanced natural language processing\n(NLP) research. Current researchers are developing larger FLMs (e.g., XLNet,\nT5) to enable contextualized language representation, classification, and\ngeneration. While developing larger FLMs has been of significant advantage, it\nis also a liability concerning hallucination and predictive uncertainty.\nFundamentally, larger FLMs are built on the same foundations as smaller FLMs\n(e.g., BERT); hence, one must recognize the potential of smaller FLMs which can\nbe realized through an ensemble. In the current research, we perform a reality\ncheck on FLMs and their ensemble on benchmark and real-world datasets. We\nhypothesize that the ensembling of FLMs can influence the individualistic\nattention of FLMs and unravel the strength of coordination and cooperation of\ndifferent FLMs. We utilize BERT and define three other ensemble techniques:\n{Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a\nknowledge-guided reinforcement learning approach. We discovered that the\nsuggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by\na factor of many times using datasets that show the usefulness of NLP in\nsensitive fields, such as mental health.",
        "pdf_link": "https://arxiv.org/pdf/2308.12272v1.pdf"
    },
    {
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
        "authors": [
            "Vijay Viswanathan",
            "Chenyang Zhao",
            "Amanda Bertsch",
            "Tongshuang Wu",
            "Graham Neubig"
        ],
        "published": "2023-08-23T17:28:21Z",
        "summary": "Large language models (LLMs) enable system builders today to create competent\nNLP systems through prompting, where they only need to describe the task in\nnatural language and provide a few examples. However, in other ways, LLMs are a\nstep backward from traditional special-purpose NLP models; they require\nextensive computational resources for deployment and can be gated behind APIs.\nIn this paper, we propose Prompt2Model, a general-purpose method that takes a\nnatural language task description like the prompts provided to LLMs, and uses\nit to train a special-purpose model that is conducive to deployment. This is\ndone through a multi-step process of retrieval of existing datasets and\npretrained models, dataset generation using LLMs, and supervised fine-tuning on\nthese retrieved and generated datasets. Over three tasks, we demonstrate that\ngiven the same few-shot prompt as input, Prompt2Model trains models that\noutperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20%\nwhile being up to 700 times smaller. We also show that this data can be used to\nobtain reliable performance estimates of model performance, enabling model\ndevelopers to assess model reliability before deployment. Prompt2Model is\navailable open-source at https://github.com/neulab/prompt2model.",
        "pdf_link": "https://arxiv.org/pdf/2308.12261v1.pdf"
    },
    {
        "title": "Evaluation of Faithfulness Using the Longest Supported Subsequence",
        "authors": [
            "Anirudh Mittal",
            "Timo Schick",
            "Mikel Artetxe",
            "Jane Dwivedi-Yu"
        ],
        "published": "2023-08-23T14:18:44Z",
        "summary": "As increasingly sophisticated language models emerge, their trustworthiness\nbecomes a pivotal issue, especially in tasks such as summarization and\nquestion-answering. Ensuring their responses are contextually grounded and\nfaithful is challenging due to the linguistic diversity and the myriad of\npossible answers. In this paper, we introduce a novel approach to evaluate\nfaithfulness of machine-generated text by computing the longest noncontinuous\nsubstring of the claim that is supported by the context, which we refer to as\nthe Longest Supported Subsequence (LSS). Using a new human-annotated dataset,\nwe finetune a model to generate LSS. We introduce a new method of evaluation\nand demonstrate that these metrics correlate better with human ratings when LSS\nis employed, as opposed to when it is not. Our proposed metric demonstrates an\n18% enhancement over the prevailing state-of-the-art metric for faithfulness on\nour dataset. Our metric consistently outperforms other metrics on a\nsummarization dataset across six different models. Finally, we compare several\npopular Large Language Models (LLMs) for faithfulness using this metric. We\nrelease the human-annotated dataset built for predicting LSS and our fine-tuned\nmodel for evaluating faithfulness.",
        "pdf_link": "https://arxiv.org/pdf/2308.12157v1.pdf"
    },
    {
        "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
        "authors": [
            "Chenrui Zhang",
            "Lin Liu",
            "Jinpeng Wang",
            "Chuyuan Wang",
            "Xiao Sun",
            "Hongyu Wang",
            "Mingchen Cai"
        ],
        "published": "2023-08-23T09:46:37Z",
        "summary": "As an effective tool for eliciting the power of Large Language Models (LLMs),\nprompting has recently demonstrated unprecedented abilities across a variety of\ncomplex tasks. To further improve the performance, prompt ensemble has\nattracted substantial interest for tackling the hallucination and instability\nof LLMs. However, existing methods usually adopt a two-stage paradigm, which\nrequires a pre-prepared set of prompts with substantial manual effort, and is\nunable to perform directed optimization for different weak learners. In this\npaper, we propose a simple, universal, and automatic method named PREFER (Pompt\nEnsemble learning via Feedback-Reflect-Refine) to address the stated\nlimitations. Specifically, given the fact that weak learners are supposed to\nfocus on hard examples during boosting, PREFER builds a feedback mechanism for\nreflecting on the inadequacies of existing weak learners. Based on this, the\nLLM is required to automatically synthesize new prompts for iterative\nrefinement. Moreover, to enhance stability of the prompt effect evaluation, we\npropose a novel prompt bagging method involving forward and backward thinking,\nwhich is superior to majority voting and is beneficial for both feedback and\nweight calculation in boosting. Extensive experiments demonstrate that our\nPREFER achieves state-of-the-art performance in multiple types of tasks by a\nsignificant margin. We have made our code publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2308.12033v1.pdf"
    },
    {
        "title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models",
        "authors": [
            "Jing Yao",
            "Xiaoyuan Yi",
            "Xiting Wang",
            "Jindong Wang",
            "Xing Xie"
        ],
        "published": "2023-08-23T09:11:13Z",
        "summary": "Big models, exemplified by Large Language Models (LLMs), are models typically\npre-trained on massive data and comprised of enormous parameters, which not\nonly obtain significantly improved performance across diverse tasks but also\npresent emergent capabilities absent in smaller models. However, the growing\nintertwining of big models with everyday human lives poses potential risks and\nmight cause serious social harm. Therefore, many efforts have been made to\nalign LLMs with humans to make them better follow user instructions and satisfy\nhuman preferences. Nevertheless, `what to align with' has not been fully\ndiscussed, and inappropriate alignment goals might even backfire. In this\npaper, we conduct a comprehensive survey of different alignment goals in\nexisting work and trace their evolution paths to help identify the most\nessential goal. Particularly, we investigate related works from two\nperspectives: the definition of alignment goals and alignment evaluation. Our\nanalysis encompasses three distinct levels of alignment goals and reveals a\ngoal transformation from fundamental abilities to value orientation, indicating\nthe potential of intrinsic human values as the alignment goal for enhanced\nLLMs. Based on such results, we further discuss the challenges of achieving\nsuch intrinsic value alignment and provide a collection of available resources\nfor future research on the alignment of big models.",
        "pdf_link": "https://arxiv.org/pdf/2308.12014v2.pdf"
    },
    {
        "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs",
        "authors": [
            "Ziyi Tang",
            "Ruilin Wang",
            "Weixing Chen",
            "Keze Wang",
            "Yang Liu",
            "Tianshui Chen",
            "Liang Lin"
        ],
        "published": "2023-08-23T04:59:21Z",
        "summary": "Despite advancements in LLMs, knowledge-based reasoning remains a\nlongstanding issue due to the fragility of knowledge recall and inference.\nExisting methods primarily encourage LLMs to autonomously plan and solve\nproblems or to extensively sample reasoning chains without addressing the\nconceptual and inferential fallacies. Attempting to alleviate inferential\nfallacies and drawing inspiration from multi-agent collaboration, we present a\nframework to increase faithfulness and causality for knowledge-based reasoning.\nSpecifically, we propose to employ multiple intelligent agents (i.e., reasoners\nand an evaluator) to work collaboratively in a reasoning-and-consensus paradigm\nfor elevated reasoning faithfulness. The reasoners focus on providing solutions\nwith human-like causality to solve open-domain problems. On the other hand, the\n\\textit{evaluator} agent scrutinizes if a solution is deducible from a\nnon-causal perspective and if it still holds when challenged by a\ncounterfactual candidate. According to the extensive and comprehensive\nevaluations on a variety of knowledge reasoning tasks (e.g., science question\nanswering and commonsense reasoning), our framework outperforms all compared\nstate-of-the-art approaches by large margins.",
        "pdf_link": "https://arxiv.org/pdf/2308.11914v2.pdf"
    },
    {
        "title": "Dcc --help: Generating Context-Aware Compiler Error Explanations with Large Language Models",
        "authors": [
            "Andrew Taylor",
            "Alexandra Vassar",
            "Jake Renzella",
            "Hammond Pearce"
        ],
        "published": "2023-08-23T02:36:19Z",
        "summary": "In the challenging field of introductory programming, high enrollments and\nfailure rates drive us to explore tools and systems to enhance student\noutcomes, especially automated tools that scale to large cohorts. This paper\npresents and evaluates the dcc --help tool, an integration of a Large Language\nModel (LLM) into the Debugging C Compiler (DCC) to generate unique,\nnovice-focused explanations tailored to each error. dcc --help prompts an LLM\nwith contextual information of compile- and run-time error occurrences,\nincluding the source code, error location and standard compiler error message.\nThe LLM is instructed to generate novice-focused, actionable error explanations\nand guidance, designed to help students understand and resolve problems without\nproviding solutions. dcc --help was deployed to our CS1 and CS2 courses, with\n2,565 students using the tool over 64,000 times in ten weeks. We analysed a\nsubset of these error/explanation pairs to evaluate their properties, including\nconceptual correctness, relevancy, and overall quality. We found that the\nLLM-generated explanations were conceptually accurate in 90% of compile-time\nand 75% of run-time cases, but often disregarded the instruction not to provide\nsolutions in code. Our findings, observations and reflections following\ndeployment indicate that dcc-help provides novel opportunities for scaffolding\nstudents' introduction to programming.",
        "pdf_link": "https://arxiv.org/pdf/2308.11873v2.pdf"
    },
    {
        "title": "Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test",
        "authors": [
            "Saba Rahimi",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "published": "2023-08-22T23:18:53Z",
        "summary": "Large language models such as Open AI's Generative Pre-trained Transformer\n(GPT) models are proficient at answering questions, but their knowledge is\nconfined to the information present in their training data. This limitation\nrenders them ineffective when confronted with questions about recent\ndevelopments or non-public documents. Our research proposes a method that\nenables GPT models to answer questions by employing context from an information\nsource not previously included in their training data. The methodology includes\npreprocessing of contextual information, the embedding of contexts and queries,\nconstructing prompt through the integration of context embeddings, and\ngenerating answers using GPT models. We applied this method in a controlled\ntest scenario using the California Driver's Handbook as the information source.\nThe GPT-3 model achieved a 96% passing score on a set of 50 sample driving\nknowledge test questions. In contrast, without context, the model's passing\nscore fell to 82%. However, the model still fails to answer some questions\ncorrectly even with providing library of context, highlighting room for\nimprovement. The research also examined the impact of prompt length and context\nformat, on the model's performance. Overall, the study provides insights into\nthe limitations and potential improvements for GPT models in question-answering\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.11827v1.pdf"
    },
    {
        "title": "Towards an On-device Agent for Text Rewriting",
        "authors": [
            "Yun Zhu",
            "Yinxiao Liu",
            "Felix Stahlberg",
            "Shankar Kumar",
            "Yu-hui Chen",
            "Liangchen Luo",
            "Lei Shu",
            "Renjie Liu",
            "Jindong Chen",
            "Lei Meng"
        ],
        "published": "2023-08-22T22:18:38Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities for\ntext rewriting. Nonetheless, the large sizes of these models make them\nimpractical for on-device inference, which would otherwise allow for enhanced\nprivacy and economical inference. Creating a smaller yet potent language model\nfor text rewriting presents a formidable challenge because it requires\nbalancing the need for a small size with the need to retain the emergent\ncapabilities of the LLM, that requires costly data collection. To address the\nabove challenge, we introduce a new instruction tuning approach for building a\nmobile-centric text rewriting model. Our strategies enable the generation of\nhigh quality training data without any human labeling. In addition, we propose\na heuristic reinforcement learning framework which substantially enhances\nperformance without requiring preference data. To further bridge the\nperformance gap with the larger server-side model, we propose an effective\napproach that combines the mobile rewrite agent with the server model using a\ncascade. To tailor the text rewriting tasks to mobile scenarios, we introduce\nMessageRewriteEval, a benchmark that focuses on text rewriting for messages\nthrough natural language instructions. Through empirical experiments, we\ndemonstrate that our on-device model surpasses the current state-of-the-art\nLLMs in text rewriting while maintaining a significantly reduced model size.\nNotably, we show that our proposed cascading approach improves model\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2308.11807v1.pdf"
    },
    {
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
        "authors": [
            "Mohamed Elaraby",
            "Mengyin Lu",
            "Jacob Dunn",
            "Xueying Zhang",
            "Yu Wang",
            "Shizhu Liu",
            "Pingchuan Tian",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "published": "2023-08-22T20:12:49Z",
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.11764v4.pdf"
    },
    {
        "title": "Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation",
        "authors": [
            "Yifei Su",
            "Dong An",
            "Yuan Xu",
            "Kehan Chen",
            "Yan Huang"
        ],
        "published": "2023-08-22T16:45:35Z",
        "summary": "This report details the methods of the winning entry of the AVDN Challenge in\nICCV CLVL 2023. The competition addresses the Aerial Navigation from Dialog\nHistory (ANDH) task, which requires a drone agent to associate dialog history\nwith aerial observations to reach the destination. For better cross-modal\ngrounding abilities of the drone agent, we propose a Target-Grounded\nGraph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages\na graph-aware transformer to capture spatiotemporal dependency, which benefits\nnavigation state tracking and robust action planning. In addition,an auxiliary\nvisual grounding task is devised to boost the agent's awareness of referred\nlandmarks. Moreover, a hybrid augmentation strategy based on large language\nmodels is utilized to mitigate data scarcity limitations. Our TG-GAT framework\nwon the AVDN Challenge, with 2.2% and 3.0% absolute improvements over the\nbaseline on SPL and SR metrics, respectively. The code is available at\nhttps://github.com/yifeisu/TG-GAT.",
        "pdf_link": "https://arxiv.org/pdf/2308.11561v5.pdf"
    },
    {
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
        "authors": [
            "Pouya Pezeshkpour",
            "Estevam Hruschka"
        ],
        "published": "2023-08-22T14:54:59Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious NLP tasks. However, previous works have shown these models are\nsensitive towards prompt wording, and few-shot demonstrations and their order,\nposing challenges to fair assessment of these models. As these models become\nmore powerful, it becomes imperative to understand and address these\nlimitations. In this paper, we focus on LLMs robustness on the task of\nmultiple-choice questions -- commonly adopted task to study reasoning and\nfact-retrieving capability of LLMs. Investigating the sensitivity of LLMs\ntowards the order of options in multiple-choice questions, we demonstrate a\nconsiderable performance gap of approximately 13% to 75% in LLMs on different\nbenchmarks, when answer options are reordered, even when using demonstrations\nin a few-shot setting. Through a detailed analysis, we conjecture that this\nsensitivity arises when LLMs are uncertain about the prediction between the\ntop-2/3 choices, and specific options placements may favor certain prediction\nbetween those top choices depending on the question caused by positional bias.\nWe also identify patterns in top-2 choices that amplify or mitigate the model's\nbias toward option placement. We found that for amplifying bias, the optimal\nstrategy involves positioning the top two choices as the first and last\noptions. Conversely, to mitigate bias, we recommend placing these choices among\nthe adjacent options. To validate our conjecture, we conduct various\nexperiments and adopt two approaches to calibrate LLMs' predictions, leading to\nup to 8 percentage points improvement across different models and benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2308.11483v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis",
        "authors": [
            "Chang Liu",
            "Bo Wu"
        ],
        "published": "2023-08-22T06:32:07Z",
        "summary": "Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.",
        "pdf_link": "https://arxiv.org/pdf/2308.11224v2.pdf"
    },
    {
        "title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning",
        "authors": [
            "Junyi Lu",
            "Lei Yu",
            "Xiaojia Li",
            "Li Yang",
            "Chun Zuo"
        ],
        "published": "2023-08-22T03:10:40Z",
        "summary": "The automation of code review activities, a long-standing pursuit in software\nengineering, has been primarily addressed by numerous domain-specific\npre-trained models. Despite their success, these models frequently demand\nextensive resources for pre-training from scratch. In contrast, Large Language\nModels (LLMs) provide an intriguing alternative, given their remarkable\ncapabilities when supplemented with domain-specific knowledge. However, their\npotential for automating code review tasks remains largely unexplored.\n  In response to this research gap, we present LLaMA-Reviewer, an innovative\nframework that leverages the capabilities of LLaMA, a popular LLM, in the realm\nof code review. Mindful of resource constraints, this framework employs\nparameter-efficient fine-tuning (PEFT) methods, delivering high performance\nwhile using less than 1% of trainable parameters.\n  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse,\npublicly available datasets. Notably, even with the smallest LLaMA base model\nconsisting of 6.7B parameters and a limited number of tuning epochs,\nLLaMA-Reviewer equals the performance of existing code-review-focused models.\n  The ablation experiments provide insights into the influence of various\nfine-tuning process components, including input representation, instruction\ntuning, and different PEFT methods. To foster continuous progress in this\nfield, the code and all PEFT-weight plugins have been made open-source.",
        "pdf_link": "https://arxiv.org/pdf/2308.11148v2.pdf"
    },
    {
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models",
        "authors": [
            "Alex Nyffenegger",
            "Matthias St√ºrmer",
            "Joel Niklaus"
        ],
        "published": "2023-08-22T00:57:36Z",
        "summary": "Anonymity of both natural and legal persons in court rulings is a critical\naspect of privacy protection in the European Union and Switzerland. With the\nadvent of LLMs, concerns about large-scale re-identification of anonymized\npersons are growing. In accordance with the Federal Supreme Court of\nSwitzerland, we explore the potential of LLMs to re-identify individuals in\ncourt rulings by constructing a proof-of-concept using actual legal data from\nthe Swiss federal supreme court. Following the initial experiment, we\nconstructed an anonymized Wikipedia dataset as a more rigorous testing ground\nto further investigate the findings. With the introduction and application of\nthe new task of re-identifying people in texts, we also introduce new metrics\nto measure performance. We systematically analyze the factors that influence\nsuccessful re-identifications, identifying model size, input length, and\ninstruction tuning among the most critical determinants. Despite high\nre-identification rates on Wikipedia, even the best LLMs struggled with court\ndecisions. The complexity is attributed to the lack of test datasets, the\nnecessity for substantial training resources, and data sparsity in the\ninformation used for re-identification. In conclusion, this study demonstrates\nthat re-identification using LLMs may not be feasible for now, but as the\nproof-of-concept on Wikipedia showed, it might become possible in the future.\nWe hope that our system can help enhance the confidence in the security of\nanonymized decisions, thus leading to the courts being more confident to\npublish decisions.",
        "pdf_link": "https://arxiv.org/pdf/2308.11103v1.pdf"
    },
    {
        "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs",
        "authors": [
            "Arka Pal",
            "Deep Karkhanis",
            "Manley Roberts",
            "Samuel Dooley",
            "Arvind Sundararajan",
            "Siddartha Naidu"
        ],
        "published": "2023-08-21T17:30:16Z",
        "summary": "Modern large language models (LLMs) that rely on attention mechanisms are\ntypically trained with fixed context lengths which enforce upper limits on the\nlength of input sequences that they can handle at evaluation time. To use these\nmodels on sequences longer than the train-time context length, one might employ\ntechniques from the growing family of context length extrapolation methods --\nmost of which focus on modifying the system of positional encodings used in the\nattention mechanism to indicate where tokens or activations are located in the\ninput sequence. We conduct a wide survey of existing methods of context length\nextrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own\ndesign as well -- in particular, a new truncation strategy for modifying the\nbasis for the position encoding.\n  We test these methods using three new evaluation tasks (FreeFormQA,\nAlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to\nbe less fine-grained as a measure of long context performance of LLMs. We\nrelease the three tasks publicly as datasets on HuggingFace. We discover that\nlinear scaling is the best method for extending context length, and show that\nfurther gains can be achieved by using longer scales at evaluation time. We\nalso discover promising extrapolation capabilities in the truncated basis. To\nsupport further research in this area, we release three new 13B parameter\nlong-context models which we call Giraffe: 4k and 16k context models trained\nfrom base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We\nalso release the code to replicate our results.",
        "pdf_link": "https://arxiv.org/pdf/2308.10882v1.pdf"
    },
    {
        "title": "Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education",
        "authors": [
            "Lars Krupp",
            "Steffen Steinert",
            "Maximilian Kiefer-Emmanouilidis",
            "Karina E. Avila",
            "Paul Lukowicz",
            "Jochen Kuhn",
            "Stefan K√ºchemann",
            "Jakob Karolus"
        ],
        "published": "2023-08-21T16:14:34Z",
        "summary": "Large language models (LLMs) have recently gained popularity. However, the\nimpact of their general availability through ChatGPT on sensitive areas of\neveryday life, such as education, remains unclear. Nevertheless, the societal\nimpact on established educational methods is already being experienced by both\nstudents and educators. Our work focuses on higher physics education and\nexamines problem solving strategies. In a study, students with a background in\nphysics were assigned to solve physics exercises, with one group having access\nto an internet search engine (N=12) and the other group being allowed to use\nChatGPT (N=27). We evaluated their performance, strategies, and interaction\nwith the provided tools. Our results showed that nearly half of the solutions\nprovided with the support of ChatGPT were mistakenly assumed to be correct by\nthe students, indicating that they overly trusted ChatGPT even in their field\nof expertise. Likewise, in 42% of cases, students used copy & paste to query\nChatGPT -- an approach only used in 4% of search engine queries -- highlighting\nthe stark differences in interaction behavior between the groups and indicating\nlimited reflection when using ChatGPT. In our work, we demonstrated a need to\n(1) guide students on how to interact with LLMs and (2) create awareness of\npotential shortcomings for users.",
        "pdf_link": "https://arxiv.org/pdf/2309.03087v1.pdf"
    },
    {
        "title": "Fact-checking information generated by a large language model can decrease news discernment",
        "authors": [
            "Matthew R. DeVerna",
            "Harry Yaojun Yan",
            "Kai-Cheng Yang",
            "Filippo Menczer"
        ],
        "published": "2023-08-21T15:47:37Z",
        "summary": "Fact checking can be an effective strategy against misinformation, but its\nimplementation at scale is impeded by the overwhelming volume of information\nonline. Recent artificial intelligence (AI) language models have shown\nimpressive ability in fact-checking tasks, but how humans interact with\nfact-checking information provided by these models is unclear. Here, we\ninvestigate the impact of fact-checking information generated by a popular\nlarge language model (LLM) on belief in, and sharing intent of, political news\nin a preregistered randomized control experiment. Although the LLM performs\nreasonably well in debunking false headlines, we find that it does not\nsignificantly affect participants' ability to discern headline accuracy or\nshare accurate news. Subsequent analysis reveals that the AI fact-checker is\nharmful in specific cases: it decreases beliefs in true headlines that it\nmislabels as false and increases beliefs in false headlines that it is unsure\nabout. On the positive side, the AI fact-checking information increases sharing\nintents for correctly labeled true headlines. When participants are given the\noption to view LLM fact checks and choose to do so, they are significantly more\nlikely to share both true and false news but only more likely to believe false\nnews. Our findings highlight an important source of potential harm stemming\nfrom AI applications and underscore the critical need for policies to prevent\nor mitigate such unintended consequences.",
        "pdf_link": "https://arxiv.org/pdf/2308.10800v3.pdf"
    },
    {
        "title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding",
        "authors": [
            "Tianyu Yu",
            "Chengyue Jiang",
            "Chao Lou",
            "Shen Huang",
            "Xiaobin Wang",
            "Wei Liu",
            "Jiong Cai",
            "Yangning Li",
            "Yinghui Li",
            "Kewei Tu",
            "Hai-Tao Zheng",
            "Ningyu Zhang",
            "Pengjun Xie",
            "Fei Huang",
            "Yong Jiang"
        ],
        "published": "2023-08-21T07:31:19Z",
        "summary": "Large language models (LLMs) have shown impressive ability for open-domain\nNLP tasks. However, LLMs are sometimes too footloose for natural language\nunderstanding (NLU) tasks which always have restricted output and input format.\nTheir performances on NLU tasks are highly related to prompts or demonstrations\nand are shown to be poor at performing several representative NLU tasks, such\nas event extraction and entity typing. To this end, we present SeqGPT, a\nbilingual (i.e., English and Chinese) open-source autoregressive model\nspecially enhanced for open-domain natural language understanding. We express\nall NLU tasks with two atomic tasks, which define fixed instructions to\nrestrict the input and output format but still ``open'' for arbitrarily varied\nlabel sets. The model is first instruction-tuned with extremely fine-grained\nlabeled data synthesized by ChatGPT and then further fine-tuned by 233\ndifferent atomic tasks from 152 datasets across various domains. The\nexperimental results show that SeqGPT has decent classification and extraction\nability, and is capable of performing language understanding tasks on unseen\ndomains. We also conduct empirical studies on the scaling of data and model\nsize as well as on the transfer across tasks. Our model is accessible at\nhttps://github.com/Alibaba-NLP/SeqGPT.",
        "pdf_link": "https://arxiv.org/pdf/2308.10529v1.pdf"
    },
    {
        "title": "Dataset Quantization",
        "authors": [
            "Daquan Zhou",
            "Kai Wang",
            "Jianyang Gu",
            "Xiangyu Peng",
            "Dongze Lian",
            "Yifan Zhang",
            "Yang You",
            "Jiashi Feng"
        ],
        "published": "2023-08-21T07:24:29Z",
        "summary": "State-of-the-art deep neural networks are trained with large amounts\n(millions or even billions) of data. The expensive computation and memory costs\nmake it difficult to train them on limited hardware resources, especially for\nrecent popular large language models (LLM) and computer vision models (CV).\nRecent popular dataset distillation methods are thus developed, aiming to\nreduce the number of training samples via synthesizing small-scale datasets via\ngradient matching. However, as the gradient calculation is coupled with the\nspecific network architecture, the synthesized dataset is biased and performs\npoorly when used for training unseen architectures. To address these\nlimitations, we present dataset quantization (DQ), a new framework to compress\nlarge-scale datasets into small subsets which can be used for training any\nneural network architectures. Extensive experiments demonstrate that DQ is able\nto generate condensed small datasets for training unseen network architectures\nwith state-of-the-art compression ratios for lossless model training. To the\nbest of our knowledge, DQ is the first method that can successfully distill\nlarge-scale datasets such as ImageNet-1k with a state-of-the-art compression\nratio. Notably, with 60% data from ImageNet and 20% data from Alpaca's\ninstruction tuning data, the models can be trained with negligible or no\nperformance drop for both vision tasks (including classification, semantic\nsegmentation, and object detection) as well as language tasks (including\ninstruction tuning tasks such as BBH and DROP).",
        "pdf_link": "https://arxiv.org/pdf/2308.10524v1.pdf"
    },
    {
        "title": "An Examination of the Compositionality of Large Generative Vision-Language Models",
        "authors": [
            "Teli Ma",
            "Rong Li",
            "Junwei Liang"
        ],
        "published": "2023-08-21T06:50:29Z",
        "summary": "With the success of Large Language Models (LLMs), many Generative\nVision-Language Models (GVLMs) have been constructed via multimodal instruction\ntuning. However, the performance of GVLMs in multimodal compositional reasoning\nremains under-explored. In this paper, we examine both the evaluation metrics\n(VisualGPTScore, etc.) and current benchmarks for evaluating the\ncompositionality of GVLMs. We identify the syntactical bias in current\nbenchmarks, which is exploited by the linguistic capability of GVLMs. The bias\nrenders VisualGPTScore an insufficient metric for assessing GVLMs. To combat\nthis, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such\nbias for mitigation. A challenging new task is subsequently added to evaluate\nthe robustness of GVLMs against inherent inclination toward syntactical\ncorrectness. Using the bias-mitigated datasets and the new task, we propose a\nnovel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study\nprovides an unbiased benchmark for the compositionality of GVLMs, facilitating\nfuture research in this direction (Code and dataset are available at\nhttps://github.com/TeleeMa/SADE).",
        "pdf_link": "https://arxiv.org/pdf/2308.10509v2.pdf"
    },
    {
        "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
        "authors": [
            "Martin Weyssow",
            "Xin Zhou",
            "Kisub Kim",
            "David Lo",
            "Houari Sahraoui"
        ],
        "published": "2023-08-21T04:31:06Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in zero-shot, i.e.,\nwithout the need for specific fine-tuning. While prior studies have highlighted\nthe advantages of fine-tuning LLMs, this process incurs high computational\ncosts, making it impractical in resource-scarce environments, particularly for\nmodels with billions of parameters. To address these challenges, previous\nresearch explored In-Context Learning (ICL) as a strategy to guide the LLM\ngenerative process with task-specific prompt examples. However, ICL introduces\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee Parameter-Efficient\nFine-Tuning (PEFT) techniques as a promising approach to efficiently specialize\nLLMs to task-specific data while maintaining reasonable resource consumption.\nIn this paper, we deliver a comprehensive study of PEFT techniques for LLMs\nunder the automated code generation scenario. Our comprehensive investigation\nof PEFT techniques for LLMs reveals their superiority and potential over ICL\nacross a diverse set of LLMs. Additionally, we demonstrate the extended\ncapabilities of PEFT, showcasing its ability to learn from two distinct\ndatasets jointly without compromising performance. Furthermore, our study\nhighlights the potential for tuning larger LLMs and significant reductions in\nmemory usage by combining PEFT with quantization. Therefore, this study opens\nopportunities for broader applications of PEFT in software engineering\nscenarios. Our code is available at\nhttps://github.com/martin-wey/peft-llm-code/.",
        "pdf_link": "https://arxiv.org/pdf/2308.10462v2.pdf"
    },
    {
        "title": "Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health Support Generation",
        "authors": [
            "Qi Chen",
            "Dexi Liu"
        ],
        "published": "2023-08-21T03:31:20Z",
        "summary": "Long counseling Text Generation for Mental health support (LTGM), an\ninnovative and challenging task, aims to provide help-seekers with mental\nhealth support through a comprehensive and more acceptable response. The\ncombination of chain-of-thought (CoT) prompting and Large Language Models\n(LLMs) is employed and get the SOTA performance on various NLP tasks,\nespecially on text generation tasks. Zero-shot CoT prompting is one of the most\ncommon methods in CoT prompting. However, in the LTGM task, Zero-shot CoT\nprompting can not simulate a counselor or provide personalized strategies\nwithout effective mental health counseling strategy prompts. To tackle this\nchallenge, we propose a zero-shot Dynamic Strategy Chain (DSC) prompting\nmethod. Firstly, we utilize GPT2 to learn the responses written by mental\nhealth counselors and dynamically generate mental health counseling strategies\ntailored to the help-seekers' needs. Secondly, the Zero-shot DSC prompting is\nconstructed according to mental health counseling strategies and the\nhelp-seekers' post. Finally, the Zero-shot DSC prompting is employed to guide\nLLMs in generating more human-like responses for the help-seekers. Both\nautomatic and manual evaluations demonstrate that Zero-shot DSC prompting can\ndeliver more human-like responses than CoT prompting methods on LTGM tasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.10444v1.pdf"
    },
    {
        "title": "Using Large Language Models for Cybersecurity Capture-The-Flag Challenges and Certification Questions",
        "authors": [
            "Wesley Tann",
            "Yuancheng Liu",
            "Jun Heng Sim",
            "Choon Meng Seah",
            "Ee-Chien Chang"
        ],
        "published": "2023-08-21T03:30:21Z",
        "summary": "The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves\nparticipants finding text strings or ``flags'' by exploiting system\nvulnerabilities. Large Language Models (LLMs) are natural-language models\ntrained on vast amounts of words to understand and generate text; they can\nperform well on many CTF challenges. Such LLMs are freely available to\nstudents. In the context of CTF exercises in the classroom, this raises\nconcerns about academic integrity. Educators must understand LLMs' capabilities\nto modify their teaching to accommodate generative AI assistance. This research\ninvestigates the effectiveness of LLMs, particularly in the realm of CTF\nchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,\nGoogle Bard, and Microsoft Bing. First, we assess the LLMs' question-answering\nperformance on five Cisco certifications with varying difficulty levels. Next,\nwe qualitatively study the LLMs' abilities in solving CTF challenges to\nunderstand their limitations. We report on the experience of using the LLMs for\nseven test cases in all five types of CTF challenges. In addition, we\ndemonstrate how jailbreak prompts can bypass and break LLMs' ethical\nsafeguards. The paper concludes by discussing LLM's impact on CTF exercises and\nits implications.",
        "pdf_link": "https://arxiv.org/pdf/2308.10443v1.pdf"
    },
    {
        "title": "LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying",
        "authors": [
            "Thommen George Karimpanal",
            "Laknath Buddhika Semage",
            "Santu Rana",
            "Hung Le",
            "Truyen Tran",
            "Sunil Gupta",
            "Svetha Venkatesh"
        ],
        "published": "2023-08-21T02:07:35Z",
        "summary": "Large language models (LLMs) have recently demonstrated their impressive\nability to provide context-aware responses via text. This ability could\npotentially be used to predict plausible solutions in sequential decision\nmaking tasks pertaining to pattern completion. For example, by observing a\npartial stack of cubes, LLMs can predict the correct sequence in which the\nremaining cubes should be stacked by extrapolating the observed patterns (e.g.,\ncube sizes, colors or other attributes) in the partial stack. In this work, we\nintroduce LaGR (Language-Guided Reinforcement learning), which uses this\npredictive ability of LLMs to propose solutions to tasks that have been\npartially completed by a primary reinforcement learning (RL) agent, in order to\nsubsequently guide the latter's training. However, as RL training is generally\nnot sample-efficient, deploying this approach would inherently imply that the\nLLM be repeatedly queried for solutions; a process that can be expensive and\ninfeasible. To address this issue, we introduce SEQ (sample efficient\nquerying), where we simultaneously train a secondary RL agent to decide when\nthe LLM should be queried for solutions. Specifically, we use the quality of\nthe solutions emanating from the LLM as the reward to train this agent. We show\nthat our proposed framework LaGR-SEQ enables more efficient primary RL\ntraining, while simultaneously minimizing the number of queries to the LLM. We\ndemonstrate our approach on a series of tasks and highlight the advantages of\nour approach, along with its limitations and potential future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2308.13542v1.pdf"
    },
    {
        "title": "Large Language Models on Wikipedia-Style Survey Generation: an Evaluation in NLP Concepts",
        "authors": [
            "Fan Gao",
            "Hang Jiang",
            "Rui Yang",
            "Qingcheng Zeng",
            "Jinghui Lu",
            "Moritz Blum",
            "Dairui Liu",
            "Tianwei She",
            "Yuang Jiang",
            "Irene Li"
        ],
        "published": "2023-08-21T01:32:45Z",
        "summary": "Educational materials such as survey articles in specialized fields like\ncomputer science traditionally require tremendous expert inputs and are\ntherefore expensive to create and update. Recently, Large Language Models\n(LLMs) have achieved significant success across various general tasks. However,\ntheir effectiveness and limitations in the education domain are yet to be fully\nexplored. In this work, we examine the proficiency of LLMs in generating\nsuccinct survey articles specific to the niche field of NLP in computer\nscience, focusing on a curated list of 99 topics. Automated benchmarks reveal\nthat GPT-4 surpasses its predecessors like GPT-3.5, PaLM2, and LLaMa2 in\ncomparison to the established ground truth. We compare both human and GPT-based\nevaluation scores and provide in-depth analysis. While our findings suggest\nthat GPT-created surveys are more contemporary and accessible than\nhuman-authored ones, certain limitations were observed. Notably, GPT-4, despite\noften delivering outstanding content, occasionally exhibited lapses like\nmissing details or factual errors. At last, we compared the rating behavior\nbetween humans and GPT-4 and found systematic bias in using GPT evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.10410v3.pdf"
    },
    {
        "title": "FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models",
        "authors": [
            "Yanhong Bai",
            "Jiabao Zhao",
            "Jinxin Shi",
            "Tingjiang Wei",
            "Xingjiao Wu",
            "Liang He"
        ],
        "published": "2023-08-21T00:25:17Z",
        "summary": "Detecting stereotypes and biases in Large Language Models (LLMs) can enhance\nfairness and reduce adverse impacts on individuals or groups when these LLMs\nare applied. However, the majority of existing methods focus on measuring the\nmodel's preference towards sentences containing biases and stereotypes within\ndatasets, which lacks interpretability and cannot detect implicit biases and\nstereotypes in the real world. To address this gap, this paper introduces a\nfour-stage framework to directly evaluate stereotypes and biases in the\ngenerated content of LLMs, including direct inquiry testing, serial or adapted\nstory testing, implicit association testing, and unknown situation testing.\nAdditionally, the paper proposes multi-dimensional evaluation metrics and\nexplainable zero-shot prompts for automated evaluation. Using the education\nsector as a case study, we constructed the Edu-FairMonitor based on the\nfour-stage framework, which encompasses 12,632 open-ended questions covering\nnine sensitive factors and 26 educational scenarios. Experimental results\nreveal varying degrees of stereotypes and biases in five LLMs evaluated on\nEdu-FairMonitor. Moreover, the results of our proposed automated evaluation\nmethod have shown a high correlation with human annotations.",
        "pdf_link": "https://arxiv.org/pdf/2308.10397v2.pdf"
    },
    {
        "title": "A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability",
        "authors": [
            "Ming Jin",
            "Bilgehan Sel",
            "Fnu Hardeep",
            "Wotao Yin"
        ],
        "published": "2023-08-20T22:42:04Z",
        "summary": "This paper outlines a natural conversational approach to solving personalized\nenergy-related problems using large language models (LLMs). We focus on\ncustomizable optimization problems that necessitate repeated solving with\nslight variations in modeling and are user-specific, hence posing a challenge\nto devising a one-size-fits-all model. We put forward a strategy that augments\nan LLM with an optimization solver, enhancing its proficiency in understanding\nand responding to user specifications and preferences while providing nonlinear\nreasoning capabilities. Our approach pioneers the novel concept of human-guided\noptimization autoformalism, translating a natural language task specification\nautomatically into an optimization instance. This enables LLMs to analyze,\nexplain, and tackle a variety of instance-specific energy-related problems,\npushing beyond the limits of current prompt-based techniques.\n  Our research encompasses various commonplace tasks in the energy sector, from\nelectric vehicle charging and Heating, Ventilation, and Air Conditioning (HVAC)\ncontrol to long-term planning problems such as cost-benefit evaluations for\ninstalling rooftop solar photovoltaics (PVs) or heat pumps. This pilot study\nmarks an essential stride towards the context-based formulation of optimization\nusing LLMs, with the potential to democratize optimization processes. As a\nresult, stakeholders are empowered to optimize their energy consumption,\npromoting sustainable energy practices customized to personal needs and\npreferences.",
        "pdf_link": "https://arxiv.org/pdf/2308.10380v2.pdf"
    },
    {
        "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation",
        "authors": [
            "Li Zhong",
            "Zilong Wang"
        ],
        "published": "2023-08-20T18:36:28Z",
        "summary": "Recently, the large language models (LLMs) have shown extraordinary ability\nin understanding natural language and generating programming code. It has been\na common practice of software engineers to consult LLMs when encountering\ncoding questions. Although efforts have been made to avoid syntax errors and\nalign the code with the intended semantics, the reliability and robustness of\nthe code generationfrom LLMs have not yet been thoroughly studied. The\nexecutable code is not equivalent to the reliable and robust code, especially\nin the context of real-world software development. The misuse of APIs in the\ngenerated code could lead to severe problem, such as resource leaks, program\ncrashes. To make things worse, the users of LLM code generation services are\nactually the developers that are most vulnerable to these code that seems right\n-- They are always novice developers that are not familiar with the APIs that\nLLMs generate code for them. Therefore, they could hardly tell the misuse in\nthe code generated by LLMs, which further facilitates the incorrect code\napplied in real-world software. Existing code evaluation benchmark and datasets\nfocus on crafting small tasks such as programming questions in coding\ninterviews, which however deviates from the problem that developers would ask\nLLM for real-world coding help. To fill the missing piece, in this work, we\npropose a dataset RobustAPI for evaluating the reliability and robustness of\ncode generated by LLMs. We collect 1208 coding questions from StackOverflow on\n24 representative Java APIs. We summarize thecommon misuse patterns of these\nAPIs and evaluate them oncurrent popular LLMs. The evaluation results show that\nevenfor GPT-4, 62% of the generated code contains API misuses,which would cause\nunexpected consequences if the code isintroduced into real-world software.",
        "pdf_link": "https://arxiv.org/pdf/2308.10335v5.pdf"
    },
    {
        "title": "How Good Are Large Language Models at Out-of-Distribution Detection?",
        "authors": [
            "Bo Liu",
            "Liming Zhan",
            "Zexin Lu",
            "Yujie Feng",
            "Lei Xue",
            "Xiao-Ming Wu"
        ],
        "published": "2023-08-20T13:15:18Z",
        "summary": "Out-of-distribution (OOD) detection plays a vital role in enhancing the\nreliability of machine learning (ML) models. The emergence of large language\nmodels (LLMs) has catalyzed a paradigm shift within the ML community,\nshowcasing their exceptional capabilities across diverse natural language\nprocessing tasks. While existing research has probed OOD detection with\nrelative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark\ndifferences in scales, pre-training objectives, and inference paradigms call\ninto question the applicability of these findings to LLMs. This paper embarks\non a pioneering empirical investigation of OOD detection in the domain of LLMs,\nfocusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate\ncommonly-used OOD detectors, scrutinizing their performance in both zero-grad\nand fine-tuning scenarios. Notably, we alter previous discriminative\nin-distribution fine-tuning into generative fine-tuning, aligning the\npre-training objective of LLMs with downstream tasks. Our findings unveil that\na simple cosine distance OOD detector demonstrates superior efficacy,\noutperforming other OOD detectors. We provide an intriguing explanation for\nthis phenomenon by highlighting the isotropic nature of the embedding spaces of\nLLMs, which distinctly contrasts with the anisotropic property observed in\nsmaller BERT family models. The new insight enhances our understanding of how\nLLMs detect OOD data, thereby enhancing their adaptability and reliability in\ndynamic environments.",
        "pdf_link": "https://arxiv.org/pdf/2308.10261v3.pdf"
    },
    {
        "title": "A Survey on Fairness in Large Language Models",
        "authors": [
            "Yingji Li",
            "Mengnan Du",
            "Rui Song",
            "Xin Wang",
            "Ying Wang"
        ],
        "published": "2023-08-20T03:30:22Z",
        "summary": "Large Language Models (LLMs) have shown powerful performance and development\nprospects and are widely deployed in the real world. However, LLMs can capture\nsocial biases from unprocessed training data and propagate the biases to\ndownstream tasks. Unfair LLM systems have undesirable social impacts and\npotential harms. In this paper, we provide a comprehensive review of related\nresearch on fairness in LLMs. Considering the influence of parameter magnitude\nand training paradigm on research strategy, we divide existing fairness\nresearch into oriented to medium-sized LLMs under pre-training and fine-tuning\nparadigms and oriented to large-sized LLMs under prompting paradigms. First,\nfor medium-sized LLMs, we introduce evaluation metrics and debiasing methods\nfrom the perspectives of intrinsic bias and extrinsic bias, respectively. Then,\nfor large-sized LLMs, we introduce recent fairness research, including fairness\nevaluation, reasons for bias, and debiasing methods. Finally, we discuss and\nprovide insight on the challenges and future directions for the development of\nfairness in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.10149v2.pdf"
    },
    {
        "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression",
        "authors": [
            "Yanyuan Qiao",
            "Yuankai Qi",
            "Zheng Yu",
            "Jing Liu",
            "Qi Wu"
        ],
        "published": "2023-08-20T03:00:20Z",
        "summary": "Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent\nyears, from room-based to object-based and indoor to outdoor. The REVERIE\n(Remote Embodied Referring Expression) is interesting since it only provides\nhigh-level instructions to the agent, which are closer to human commands in\npractice. Nevertheless, this poses more challenges than other VLN tasks since\nit requires agents to infer a navigation plan only based on a short\ninstruction. Large Language Models (LLMs) show great potential in robot action\nplanning by providing proper prompts. Still, this strategy has not been\nexplored under the REVERIE settings. There are several new challenges. For\nexample, the LLM should be environment-aware so that the navigation plan can be\nadjusted based on the current visual observation. Moreover, the LLM planned\nactions should be adaptable to the much larger and more complex REVERIE\nenvironment. This paper proposes a March-in-Chat (MiC) model that can talk to\nthe LLM on the fly and plan dynamically based on a newly proposed\nRoom-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the\nprevious state-of-the-art by large margins by SPL and RGSPL metrics on the\nREVERIE benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2308.10141v1.pdf"
    },
    {
        "title": "UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding",
        "authors": [
            "Hao Feng",
            "Zijian Wang",
            "Jingqun Tang",
            "Jinghui Lu",
            "Wengang Zhou",
            "Houqiang Li",
            "Can Huang"
        ],
        "published": "2023-08-19T17:32:34Z",
        "summary": "In the era of Large Language Models (LLMs), tremendous strides have been made\nin the field of multimodal understanding. However, existing advanced algorithms\nare limited to effectively utilizing the immense representation capabilities\nand rich world knowledge inherent to these large pre-trained models, and the\nbeneficial connections among tasks within the context of text-rich scenarios\nhave not been sufficiently explored. In this work, we introduce UniDoc, a novel\nmultimodal model equipped with text detection and recognition capabilities,\nwhich are deficient in existing approaches. Moreover, UniDoc capitalizes on the\nbeneficial interactions among tasks to enhance the performance of each\nindividual task. To implement UniDoc, we perform unified multimodal instruct\ntuning on the contributed large-scale instruction following datasets.\nQuantitative and qualitative experimental results show that UniDoc sets\nstate-of-the-art scores across multiple challenging benchmarks. To the best of\nour knowledge, this is the first large multimodal model capable of simultaneous\ntext detection, recognition, spotting, and understanding.",
        "pdf_link": "https://arxiv.org/pdf/2308.11592v2.pdf"
    },
    {
        "title": "GameEval: Evaluating LLMs on Conversational Games",
        "authors": [
            "Dan Qiao",
            "Chenfei Wu",
            "Yaobo Liang",
            "Juntao Li",
            "Nan Duan"
        ],
        "published": "2023-08-19T14:33:40Z",
        "summary": "The rapid advancements in large language models (LLMs) have presented\nchallenges in evaluating those models. Existing evaluation methods are either\nreference-based or preference based, which inevitably need human intervention\nor introduce test bias caused by evaluator models. In this paper, we propose\nGameEval, a novel approach to evaluating LLMs through goal-driven\nconversational games, overcoming the limitations of previous methods. GameEval\ntreats LLMs as game players and assigns them distinct roles with specific goals\nachieved by launching conversations of various forms, including discussion,\nquestion answering, and voting. We design three unique games with cooperative\nor adversarial objectives, accompanied by corresponding evaluation metrics, to\nshow how this new paradigm comprehensively evaluates model performance.Through\nextensive experiments, we show that GameEval can effectively differentiate the\ncapabilities of various LLMs, providing a comprehensive assessment of their\nintegrated abilities to solve complex problems. Our public anonymous code is\navailable at https://github.com/GameEval/GameEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.10032v1.pdf"
    },
    {
        "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models",
        "authors": [
            "Liwen Zhang",
            "Weige Cai",
            "Zhaowei Liu",
            "Zhi Yang",
            "Wei Dai",
            "Yujie Liao",
            "Qianru Qin",
            "Yifei Li",
            "Xingyu Liu",
            "Zhiqiang Liu",
            "Zhoufan Zhu",
            "Anbo Wu",
            "Xin Guo",
            "Yun Chen"
        ],
        "published": "2023-08-19T10:38:00Z",
        "summary": "Large language models (LLMs) have demonstrated exceptional performance in\nvarious natural language processing tasks, yet their efficacy in more\nchallenging and domain-specific tasks remains largely unexplored. This paper\npresents FinEval, a benchmark specifically designed for the financial domain\nknowledge in the LLMs. FinEval is a collection of high-quality multiple-choice\nquestions covering Finance, Economy, Accounting, and Certificate. It includes\n4,661 questions spanning 34 different academic subjects. To ensure a\ncomprehensive model performance evaluation, FinEval employs a range of prompt\ntypes, including zero-shot and few-shot prompts, as well as answer-only and\nchain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs\non FinEval, the results show that only GPT-4 achieved an accuracy close to 70%\nin different prompt settings, indicating significant growth potential for LLMs\nin the financial domain knowledge. Our work offers a more comprehensive\nfinancial knowledge evaluation benchmark, utilizing data of mock exams and\ncovering a wide range of evaluated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.09975v1.pdf"
    },
    {
        "title": "Tackling Vision Language Tasks Through Learning Inner Monologues",
        "authors": [
            "Diji Yang",
            "Kezhen Chen",
            "Jinmeng Rao",
            "Xiaoyuan Guo",
            "Yawen Zhang",
            "Jie Yang",
            "Yi Zhang"
        ],
        "published": "2023-08-19T10:10:49Z",
        "summary": "Visual language tasks require AI models to comprehend and reason with both\nvisual and textual content. Driven by the power of Large Language Models\n(LLMs), two prominent methods have emerged: (1) the hybrid integration between\nLLMs and Vision-Language Models (VLMs), where visual inputs are firstly\nconverted into language descriptions by VLMs, serving as inputs for LLMs to\ngenerate final answer(s); (2) visual feature alignment in language space, where\nvisual inputs are encoded as embeddings and projected to LLMs' language space\nvia further supervised fine-tuning. The first approach provides light training\ncosts and interpretability but is hard to be optimized in an end-to-end\nfashion. The second approach presents decent performance, but feature alignment\nusually requires large amounts of training data and lacks interpretability. To\ntackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal\nOptimization (IMMO), to solve complex vision language problems by simulating\ninner monologue processes, a cognitive process in which an individual engages\nin silent verbal communication with themselves. We enable LLMs and VLMs to\ninteract through natural language conversation and propose to use a two-stage\ntraining process to learn how to do the inner monologue (self-asking questions\nand answering questions). IMMO is evaluated on two popular tasks and the\nresults suggest by emulating the cognitive phenomenon of internal dialogue, our\napproach can enhance reasoning and explanation abilities, contributing to the\nmore effective fusion of vision and language models. More importantly, instead\nof using predefined human-crafted monologues, IMMO learns this process within\nthe deep learning models, promising wider applicability to many different AI\nproblems beyond vision language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.09970v1.pdf"
    },
    {
        "title": "Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs",
        "authors": [
            "Federico Cassano",
            "John Gouwar",
            "Francesca Lucchetti",
            "Claire Schlesinger",
            "Anders Freeman",
            "Carolyn Jane Anderson",
            "Molly Q Feldman",
            "Michael Greenberg",
            "Abhinav Jangda",
            "Arjun Guha"
        ],
        "published": "2023-08-19T03:19:01Z",
        "summary": "Over the past few years, Large Language Models of Code (Code LLMs) have\nstarted to have a significant impact on programming practice. Code LLMs are\nalso emerging as building blocks for research in programming languages and\nsoftware engineering. However, Code LLMs produce impressive results on\nprogramming languages that are well represented in their training data (e.g.,\nJava, Python, or JavaScript), but struggle with low-resource languages that\nhave limited training data available. Low resource languages include OCaml,\nRacket, and several others.\n  This paper presents an effective approach for boosting the performance of\nCode LLMs on low-resource languages using semi-synthetic data. Our approach,\nMultiPL-T, translates training data from high-resource languages into training\ndata for low-resource languages in the following way. 1) We use a Code LLM to\nsynthesize tests for commented code from a high-resource language, filtering\nout faulty tests and code with low test coverage. 2) We use a Code LLM to\ntranslate Python code to a target low-resource language, and use tests to\nvalidate the translation. We apply this approach to generate tens of thousands\nof validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,\nwe use an open model (StarCoderBase) with open training data (The Stack), which\nallows us to decontaminate benchmarks, train models without violating licenses,\nand run experiments that could not otherwise be done.\n  With MultiPL-T generated data, we present fine-tuned versions of\nStarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On\nestablished benchmarks (MultiPL-E), these models outperform other open Code\nLLMs. The MultiPL-T approach is easy to apply to new languages, and is\nsignificantly more efficient and effective than alternatives such as training\nlonger.",
        "pdf_link": "https://arxiv.org/pdf/2308.09895v5.pdf"
    },
    {
        "title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis",
        "authors": [
            "Oscar J. Romero",
            "John Zimmerman",
            "Aaron Steinfeld",
            "Anthony Tomasic"
        ],
        "published": "2023-08-18T21:42:47Z",
        "summary": "This paper explores the integration of two AI subdisciplines employed in the\ndevelopment of artificial agents that exhibit intelligent behavior: Large\nLanguage Models (LLMs) and Cognitive Architectures (CAs). We present three\nintegration approaches, each grounded in theoretical models and supported by\npreliminary empirical evidence. The modular approach, which introduces four\nmodels with varying degrees of integration, makes use of chain-of-thought\nprompting, and draws inspiration from augmented LLMs, the Common Model of\nCognition, and the simulation theory of cognition. The agency approach,\nmotivated by the Society of Mind theory and the LIDA cognitive architecture,\nproposes the formation of agent collections that interact at micro and macro\ncognitive levels, driven by either LLMs or symbolic components. The\nneuro-symbolic approach, which takes inspiration from the CLARION cognitive\narchitecture, proposes a model where bottom-up learning extracts symbolic\nrepresentations from an LLM layer and top-down guidance utilizes symbolic\nrepresentations to direct prompt engineering in the LLM layer. These approaches\naim to harness the strengths of both LLMs and CAs, while mitigating their\nweaknesses, thereby advancing the development of more robust AI systems. We\ndiscuss the tradeoffs and challenges associated with each approach.",
        "pdf_link": "https://arxiv.org/pdf/2308.09830v3.pdf"
    },
    {
        "title": "Learning Representations on Logs for AIOps",
        "authors": [
            "Pranjal Gupta",
            "Harshit Kumar",
            "Debanjana Kar",
            "Karan Bhukar",
            "Pooja Aggarwal",
            "Prateeti Mohapatra"
        ],
        "published": "2023-08-18T20:34:46Z",
        "summary": "AI for IT Operations (AIOps) is a powerful platform that Site Reliability\nEngineers (SREs) use to automate and streamline operational workflows with\nminimal human intervention. Automated log analysis is a critical task in AIOps\nas it provides key insights for SREs to identify and address ongoing faults.\nTasks such as log format detection, log classification, and log parsing are key\ncomponents of automated log analysis. Most of these tasks require supervised\nlearning; however, there are multiple challenges due to limited labelled log\ndata and the diverse nature of log data. Large Language Models (LLMs) such as\nBERT and GPT3 are trained using self-supervision on a vast amount of unlabeled\ndata. These models provide generalized representations that can be effectively\nused for various downstream tasks with limited labelled data. Motivated by the\nsuccess of LLMs in specific domains like science and biology, this paper\nintroduces a LLM for log data which is trained on public and proprietary log\ndata. The results of our experiments demonstrate that the proposed LLM\noutperforms existing models on multiple downstream tasks. In summary, AIOps\npowered by LLMs offers an efficient and effective solution for automating log\nanalysis tasks and enabling SREs to focus on higher-level tasks. Our proposed\nLLM, trained on public and proprietary log data, offers superior performance on\nmultiple downstream tasks, making it a valuable addition to the AIOps platform.",
        "pdf_link": "https://arxiv.org/pdf/2308.11526v1.pdf"
    },
    {
        "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
        "authors": [
            "Rishabh Bhardwaj",
            "Soujanya Poria"
        ],
        "published": "2023-08-18T16:27:04Z",
        "summary": "Larger language models (LLMs) have taken the world by storm with their\nmassive multi-tasking capabilities simply by optimizing over a next-word\nprediction objective. With the emergence of their properties and encoded\nknowledge, the risk of LLMs producing harmful outputs increases, making them\nunfit for scalable deployment for the public. In this work, we propose a new\nsafety evaluation benchmark RED-EVAL that carries out red-teaming. We show that\neven widely deployed models are susceptible to the Chain of Utterances-based\n(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and\nChatGPT to unethically respond to more than 65% and 73% of harmful queries. We\nalso demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in\ngenerating harmful responses in more than 86% of the red-teaming attempts.\nNext, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It\nconstitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,\nwe collect a dataset that consists of 1.9K harmful questions covering a wide\nrange of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)\nSAFE-ALIGN: We demonstrate how the conversational dataset can be used for the\nsafety alignment of LLMs by minimizing the negative log-likelihood over helpful\nresponses and penalizing over harmful responses by gradient accent over sample\nloss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely\naligned when evaluated on RED-EVAL and HHH benchmarks while preserving the\nutility of the baseline models (TruthfulQA, MMLU, and BBH).",
        "pdf_link": "https://arxiv.org/pdf/2308.09662v3.pdf"
    },
    {
        "title": "Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning",
        "authors": [
            "Pengbo Hu",
            "Ji Qi",
            "Xingyu Li",
            "Hong Li",
            "Xinqi Wang",
            "Bing Quan",
            "Ruiyu Wang",
            "Yi Zhou"
        ],
        "published": "2023-08-18T16:21:40Z",
        "summary": "There emerges a promising trend of using large language models (LLMs) to\ngenerate code-like plans for complex inference tasks such as visual reasoning.\nThis paradigm, known as LLM-based planning, provides flexibility in problem\nsolving and endows better interpretability. However, current research is mostly\nlimited to basic scenarios of simple questions that can be straightforward\nanswered in a few inference steps. Planning for the more challenging multi-hop\nvisual reasoning tasks remains under-explored. Specifically, under multi-hop\nreasoning situations, the trade-off between accuracy and the complexity of\nplan-searching becomes prominent. The prevailing algorithms either address the\nefficiency issue by employing the fast one-stop generation or adopt a complex\niterative generation method to improve accuracy. Both fail to balance the need\nfor efficiency and performance. Drawing inspiration from the dual system of\ncognition in the human brain, the fast and the slow think processes, we propose\na hierarchical plan-searching algorithm that integrates the one-stop reasoning\n(fast) and the Tree-of-thought (slow). Our approach succeeds in performance\nwhile significantly saving inference steps. Moreover, we repurpose the PTR and\nthe CLEVER datasets, developing a systematic framework for evaluating the\nperformance and efficiency of LLMs-based plan-search algorithms under reasoning\ntasks at different levels of difficulty. Extensive experiments demonstrate the\nsuperiority of our proposed algorithm in terms of performance and efficiency.\nThe dataset and code will be release soon.",
        "pdf_link": "https://arxiv.org/pdf/2308.09658v2.pdf"
    },
    {
        "title": "RatGPT: Turning online LLMs into Proxies for Malware Attacks",
        "authors": [
            "Mika Beckerich",
            "Laura Plein",
            "Sergio Coronado"
        ],
        "published": "2023-08-17T20:54:39Z",
        "summary": "The evolution of Generative AI and the capabilities of the newly released\nLarge Language Models (LLMs) open new opportunities in software engineering.\nHowever, they also lead to new challenges in cybersecurity. Recently,\nresearchers have shown the possibilities of using LLMs such as ChatGPT to\ngenerate malicious content that can directly be exploited or guide\ninexperienced hackers to weaponize tools and code. These studies covered\nscenarios that still require the attacker to be in the middle of the loop. In\nthis study, we leverage openly available plugins and use an LLM as proxy\nbetween the attacker and the victim. We deliver a proof-of-concept where\nChatGPT is used for the dissemination of malicious software while evading\ndetection, alongside establishing the communication to a command and control\n(C2) server to receive commands to interact with a victim's system. Finally, we\npresent the general approach as well as essential elements in order to stay\nundetected and make the attack a success. This proof-of-concept highlights\nsignificant cybersecurity issues with openly available plugins and LLMs, which\nrequire the development of security guidelines, controls, and mitigation\nstrategies.",
        "pdf_link": "https://arxiv.org/pdf/2308.09183v2.pdf"
    },
    {
        "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
        "authors": [
            "Harsh Raj",
            "Vipul Gupta",
            "Domenic Rosati",
            "Subhabrata Majumdar"
        ],
        "published": "2023-08-17T18:11:33Z",
        "summary": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold.",
        "pdf_link": "https://arxiv.org/pdf/2308.09138v1.pdf"
    },
    {
        "title": "MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models",
        "authors": [
            "Mohd Zaki",
            "Jayadeva",
            "Mausam",
            "N. M. Anoop Krishnan"
        ],
        "published": "2023-08-17T17:51:05Z",
        "summary": "Information extraction and textual comprehension from materials literature\nare vital for developing an exhaustive knowledge base that enables accelerated\nmaterials discovery. Language models have demonstrated their capability to\nanswer domain-specific questions and retrieve information from knowledge bases.\nHowever, there are no benchmark datasets in the materials domain that can\nevaluate the understanding of the key concepts by these language models. In\nthis work, we curate a dataset of 650 challenging questions from the materials\ndomain that require the knowledge and skills of a materials student who has\ncleared their undergraduate degree. We classify these questions based on their\nstructure and the materials science domain-based subcategories. Further, we\nevaluate the performance of GPT-3.5 and GPT-4 models on solving these questions\nvia zero-shot and chain of thought prompting. It is observed that GPT-4 gives\nthe best performance (~62% accuracy) as compared to GPT-3.5. Interestingly, in\ncontrast to the general observation, no significant improvement in accuracy is\nobserved with the chain of thought prompting. To evaluate the limitations, we\nperformed an error analysis, which revealed conceptual errors (~64%) as the\nmajor contributor compared to computational errors (~36%) towards the reduced\nperformance of LLMs. We hope that the dataset and analysis performed in this\nwork will promote further research in developing better materials science\ndomain-specific LLMs and strategies for information extraction.",
        "pdf_link": "https://arxiv.org/pdf/2308.09115v1.pdf"
    },
    {
        "title": "Building Emotional Support Chatbots in the Era of LLMs",
        "authors": [
            "Zhonghua Zheng",
            "Lizi Liao",
            "Yang Deng",
            "Liqiang Nie"
        ],
        "published": "2023-08-17T10:49:18Z",
        "summary": "The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.",
        "pdf_link": "https://arxiv.org/pdf/2308.11584v1.pdf"
    },
    {
        "title": "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction",
        "authors": [
            "Dong Wang",
            "Kav√© Salamatian",
            "Yunqing Xia",
            "Weiwei Deng",
            "Qi Zhiang"
        ],
        "published": "2023-08-17T08:25:54Z",
        "summary": "Although deep pre-trained language models have shown promising benefit in a\nlarge set of industrial scenarios, including Click-Through-Rate (CTR)\nprediction, how to integrate pre-trained language models that handle only\ntextual signals into a prediction pipeline with non-textual features is\nchallenging.\n  Up to now two directions have been explored to integrate multi-modal inputs\nin fine-tuning of pre-trained language models. One consists of fusing the\noutcome of language models and non-textual features through an aggregation\nlayer, resulting into ensemble framework, where the cross-information between\ntextual and non-textual inputs are only learned in the aggregation layer. The\nsecond one consists of splitting non-textual features into fine-grained\nfragments and transforming the fragments to new tokens combined with textual\nones, so that they can be fed directly to transformer layers in language\nmodels. However, this approach increases the complexity of the learning and\ninference because of the numerous additional tokens.\n  To address these limitations, we propose in this work a novel framework\nBERT4CTR, with the Uni-Attention mechanism that can benefit from the\ninteractions between non-textual and textual features while maintaining low\ntime-costs in training and inference through a dimensionality reduction.\nComprehensive experiments on both public and commercial data demonstrate that\nBERT4CTR can outperform significantly the state-of-the-art frameworks to handle\nmulti-modal inputs and be applicable to CTR prediction.",
        "pdf_link": "https://arxiv.org/pdf/2308.11527v1.pdf"
    },
    {
        "title": "CMB: A Comprehensive Medical Benchmark in Chinese",
        "authors": [
            "Xidong Wang",
            "Guiming Hardy Chen",
            "Dingjie Song",
            "Zhiyi Zhang",
            "Zhihong Chen",
            "Qingying Xiao",
            "Feng Jiang",
            "Jianquan Li",
            "Xiang Wan",
            "Benyou Wang",
            "Haizhou Li"
        ],
        "published": "2023-08-17T07:51:23Z",
        "summary": "Large Language Models (LLMs) provide a possibility to make a great\nbreakthrough in medicine. The establishment of a standardized medical benchmark\nbecomes a fundamental cornerstone to measure progression. However, medical\nenvironments in different regions have their local characteristics, e.g., the\nubiquity and significance of traditional Chinese medicine within China.\nTherefore, merely translating English-based medical evaluation may result in\n\\textit{contextual incongruities} to a local region. To solve the issue, we\npropose a localized medical benchmark called CMB, a Comprehensive Medical\nBenchmark in Chinese, designed and rooted entirely within the native Chinese\nlinguistic and cultural framework. While traditional Chinese medicine is\nintegral to this evaluation, it does not constitute its entirety. Using this\nbenchmark, we have evaluated several prominent large-scale LLMs, including\nChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical\ndomain. We hope this benchmark provide first-hand experience in existing LLMs\nfor medicine and also facilitate the widespread adoption and enhancement of\nmedical LLMs within China. Our data and code are publicly available at\nhttps://github.com/FreedomIntelligence/CMB.",
        "pdf_link": "https://arxiv.org/pdf/2308.08833v2.pdf"
    },
    {
        "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
        "authors": [
            "Zekun Li",
            "Baolin Peng",
            "Pengcheng He",
            "Xifeng Yan"
        ],
        "published": "2023-08-17T06:21:50Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ninstruction-following, becoming increasingly crucial across various\napplications. However, this capability brings with it the risk of prompt\ninjection attacks, where attackers inject instructions into LLMs' input to\nelicit undesirable actions or content. Understanding the robustness of LLMs\nagainst such attacks is vital for their safe implementation. In this work, we\nestablish a benchmark to evaluate the robustness of instruction-following LLMs\nagainst prompt injection attacks. Our objective is to determine the extent to\nwhich LLMs can be influenced by injected instructions and their ability to\ndifferentiate between these injected and original target instructions. Through\nextensive experiments with leading instruction-following LLMs, we uncover\nsignificant vulnerabilities in their robustness to such attacks. Our results\nindicate that some models are overly tuned to follow any embedded instructions\nin the prompt, overly focusing on the latter parts of the prompt without fully\ngrasping the entire context. By contrast, models with a better grasp of the\ncontext and instruction-following capabilities will potentially be more\nsusceptible to compromise by injected instructions. This underscores the need\nto shift the focus from merely enhancing LLMs' instruction-following\ncapabilities to improving their overall comprehension of prompts and\ndiscernment of instructions that are appropriate to follow. We hope our\nin-depth analysis offers insights into the underlying causes of these\nvulnerabilities, aiding in the development of future solutions. Code and data\nare available at\nhttps://github.com/Leezekun/instruction-following-robustness-eval",
        "pdf_link": "https://arxiv.org/pdf/2308.10819v3.pdf"
    },
    {
        "title": "Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-08-17T03:14:00Z",
        "summary": "Addressing the gap in understanding visual comprehension in Large Language\nModels (LLMs), we designed a challenge-response study, subjecting Google Bard\nand GPT-Vision to 64 visual tasks, spanning categories like \"Visual Situational\nReasoning\" and \"Next Scene Prediction.\" Previous models, such as GPT4, leaned\nheavily on optical character recognition tools like Tesseract, whereas Bard and\nGPT-Vision, akin to Google Lens and Visual API, employ deep learning techniques\nfor visual text recognition. However, our findings spotlight both\nvision-language model's limitations: while proficient in solving visual\nCAPTCHAs that stump ChatGPT alone, it falters in recreating visual elements\nlike ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on\neducated visual guesses. The prediction problem based on visual inputs appears\nparticularly challenging with no common-sense guesses for next-scene\nforecasting based on current \"next-token\" multimodal models. This study\nprovides experimental insights into the current capacities and areas for\nimprovement in multimodal LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2309.16705v2.pdf"
    },
    {
        "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
        "authors": [
            "Yun Luo",
            "Zhen Yang",
            "Fandong Meng",
            "Yafu Li",
            "Jie Zhou",
            "Yue Zhang"
        ],
        "published": "2023-08-17T02:53:23Z",
        "summary": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.",
        "pdf_link": "https://arxiv.org/pdf/2308.08747v3.pdf"
    },
    {
        "title": "PMET: Precise Model Editing in a Transformer",
        "authors": [
            "Xiaopeng Li",
            "Shasha Li",
            "Shezheng Song",
            "Jing Yang",
            "Jun Ma",
            "Jie Yu"
        ],
        "published": "2023-08-17T02:33:43Z",
        "summary": "Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.",
        "pdf_link": "https://arxiv.org/pdf/2308.08742v6.pdf"
    },
    {
        "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs",
        "authors": [
            "Young Jin Kim",
            "Rawn Henry",
            "Raffy Fahim",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-08-16T23:57:41Z",
        "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross various language tasks but pose challenges for practical deployment due\nto their substantial memory requirements. Furthermore, the latest generative\nmodels suffer from high inference costs caused by the memory bandwidth\nbottleneck in the auto-regressive decoding process. To address these issues, we\npropose an efficient weight-only quantization method that reduces memory\nconsumption and accelerates inference for LLMs. To ensure minimal quality\ndegradation, we introduce a simple and effective heuristic approach that\nutilizes only the model weights of a pre-trained model. This approach is\napplicable to both Mixture-of-Experts (MoE) and dense models without requiring\nadditional fine-tuning. To demonstrate the effectiveness of our proposed\nmethod, we first analyze the challenges and issues associated with LLM\nquantization. Subsequently, we present our heuristic approach, which adaptively\nfinds the granularity of quantization, effectively addressing these problems.\nFurthermore, we implement highly efficient GPU GEMMs that perform on-the-fly\nmatrix multiplication and dequantization, supporting the multiplication of fp16\nor bf16 activations with int8 or int4 weights. We evaluate our approach on\nlarge-scale open source models such as OPT-175B and internal MoE models,\nshowcasing minimal accuracy loss while achieving up to 3.65 times higher\nthroughput on the same number of GPUs.",
        "pdf_link": "https://arxiv.org/pdf/2308.09723v1.pdf"
    },
    {
        "title": "LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters",
        "authors": [
            "Ching Chang",
            "Wei-Yao Wang",
            "Wen-Chih Peng",
            "Tien-Fu Chen"
        ],
        "published": "2023-08-16T16:19:50Z",
        "summary": "Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have leveraged the\nrepresentation learning transferability of pre-trained Large Language Models\n(LLMs) to handle limited non-linguistic datasets effectively. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\n\\textit{time-series alignment} stage to align LLMs with the nuances of\ntime-series data, and the \\textit{forecasting fine-tuning} stage for downstream\ntime-series forecasting tasks. Furthermore, our framework features a novel\ntwo-level aggregation method that integrates multi-scale temporal data within\npre-trained LLMs, enhancing their ability to interpret time-specific\ninformation. In experiments across 7 time-series forecasting datasets, LLM4TS\nis superior to existing state-of-the-art methods compared with\ntrained-from-scratch models in full-shot scenarios, and also achieves an\naverage improvement of 6.84% in MSE in few-shot scenarios. In addition,\nevaluations compared with different self-supervised learning approaches\nhighlight LLM4TS's effectiveness with representation learning in forecasting\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2308.08469v5.pdf"
    },
    {
        "title": "Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models",
        "authors": [
            "Zhenhua Wang",
            "Wei Xie",
            "Kai Chen",
            "Baosheng Wang",
            "Zhiwen Gui",
            "Enze Wang"
        ],
        "published": "2023-08-16T09:04:36Z",
        "summary": "Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n  This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.",
        "pdf_link": "https://arxiv.org/pdf/2308.11521v2.pdf"
    },
    {
        "title": "DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue",
        "authors": [
            "Lang Cao"
        ],
        "published": "2023-08-15T21:14:09Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, are increasingly sophisticated\nand exhibit capabilities closely resembling those of humans. A significant\napplication of these LLMs is their use as chat agents, responding to human\ninquiries across various domains. While current LLMs proficiently answer\ngeneral questions, they often fall short in complex diagnostic scenarios such\nas legal, medical, or other specialized consultations. These scenarios\ntypically require Task-Oriented Dialogue (TOD), where an AI chat agent must\nproactively pose questions and guide users toward specific goals or task\ncompletion. Previous fine-tuning models have underperformed in TOD and the full\npotential of this capability in current LLMs has not yet been fully explored.\nIn this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative\napproach that extends LLMs to more TOD scenarios. In addition to guiding users\nto complete tasks, DiagGPT can effectively manage the status of all topics\nthroughout the dialogue development. This feature enhances user experience and\noffers a more flexible interaction in TOD. Our experiments demonstrate that\nDiagGPT exhibits outstanding performance in conducting TOD with users, showing\nits potential for practical applications in various fields.",
        "pdf_link": "https://arxiv.org/pdf/2308.08043v3.pdf"
    },
    {
        "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
        "authors": [
            "Ziyu Zhuang",
            "Qiguang Chen",
            "Longxuan Ma",
            "Mingda Li",
            "Yi Han",
            "Yushan Qian",
            "Haopeng Bai",
            "Zixian Feng",
            "Weinan Zhang",
            "Ting Liu"
        ],
        "published": "2023-08-15T17:40:34Z",
        "summary": "From pre-trained language model (PLM) to large language model (LLM), the\nfield of natural language processing (NLP) has witnessed steep performance\ngains and wide practical uses. The evaluation of a research field guides its\ndirection of improvement. However, LLMs are extremely hard to thoroughly\nevaluate for two reasons. First of all, traditional NLP tasks become inadequate\ndue to the excellent performance of LLM. Secondly, existing evaluation tasks\nare difficult to keep up with the wide range of applications in real-world\nscenarios. To tackle these problems, existing works proposed various benchmarks\nto better evaluate LLMs. To clarify the numerous evaluation tasks in both\nacademia and industry, we investigate multiple papers concerning LLM\nevaluations. We summarize 4 core competencies of LLM, including reasoning,\nknowledge, reliability, and safety. For every competency, we introduce its\ndefinition, corresponding benchmarks, and metrics. Under this competency\narchitecture, similar tasks are combined to reflect corresponding ability,\nwhile new tasks can also be easily added into the system. Finally, we give our\nsuggestions on the future direction of LLM's evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2308.07902v1.pdf"
    },
    {
        "title": "Link-Context Learning for Multimodal LLMs",
        "authors": [
            "Yan Tai",
            "Weichen Fan",
            "Zhao Zhang",
            "Feng Zhu",
            "Rui Zhao",
            "Ziwei Liu"
        ],
        "published": "2023-08-15T17:33:24Z",
        "summary": "The ability to learn from context with novel concepts, and deliver\nappropriate responses are essential in human conversations. Despite current\nMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being\ntrained on mega-scale datasets, recognizing unseen images or understanding\nnovel concepts in a training-free manner remains a challenge. In-Context\nLearning (ICL) explores training-free few-shot learning, where models are\nencouraged to ``learn to learn\" from limited tasks and generalize to unseen\ntasks. In this work, we propose link-context learning (LCL), which emphasizes\n\"reasoning from cause and effect\" to augment the learning capabilities of\nMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal\nrelationship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to discern not only the\nanalogy but also the underlying causal associations between data points, which\nempowers MLLMs to recognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach, we introduce\nthe ISEKAI dataset, comprising exclusively of unseen generated image-label\npairs designed for link-context learning. Extensive experiments show that our\nLCL-MLLM exhibits strong link-context learning capabilities to novel concepts\nover vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.",
        "pdf_link": "https://arxiv.org/pdf/2308.07891v1.pdf"
    },
    {
        "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation",
        "authors": [
            "Charles O'Neill",
            "Yuan-Sen Ting",
            "Ioana Ciuca",
            "Jack Miller",
            "Thang Bui"
        ],
        "published": "2023-08-15T08:49:14Z",
        "summary": "Large Language Models (LLMs) hold immense potential to generate synthetic\ndata of high quality and utility, which has numerous applications from\ndownstream model training to practical data utilisation. However, contemporary\nmodels, despite their impressive capacities, consistently struggle to produce\nboth coherent and diverse data. To address the coherency issue, we introduce\ncontrastive expert guidance, where the difference between the logit\ndistributions of fine-tuned and base language models is emphasised to ensure\ndomain adherence. In order to ensure diversity, we utilise existing real and\nsynthetic examples as negative prompts to the model. We deem this dual-pronged\napproach to logit reshaping as STEER: Semantic Text Enhancement via Embedding\nRepositioning. STEER operates at inference-time and systematically guides the\nLLMs to strike a balance between adherence to the data distribution (ensuring\nsemantic fidelity) and deviation from prior synthetic examples or existing real\ndatasets (ensuring diversity and authenticity). This delicate balancing act is\nachieved by dynamically moving towards or away from chosen representations in\nthe latent space. STEER demonstrates improved performance over previous\nsynthetic data generation techniques, exhibiting better balance between data\ndiversity and coherency across three distinct tasks: hypothesis generation,\ntoxic and non-toxic comment generation, and commonsense reasoning task\ngeneration. We demonstrate how STEER allows for fine-tuned control over the\ndiversity-coherency trade-off via its hyperparameters, highlighting its\nversatility.",
        "pdf_link": "https://arxiv.org/pdf/2308.07645v2.pdf"
    },
    {
        "title": "LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation",
        "authors": [
            "Xiaoming Shi",
            "Jie Xu",
            "Jinru Ding",
            "Jiali Pang",
            "Sichen Liu",
            "Shuqing Luo",
            "Xingwei Peng",
            "Lu Lu",
            "Haihong Yang",
            "Mingtao Hu",
            "Tong Ruan",
            "Shaoting Zhang"
        ],
        "published": "2023-08-15T08:32:20Z",
        "summary": "There is an increasing interest in developing LLMs for medical diagnosis to\nimprove diagnosis efficiency. Despite their alluring technological potential,\nthere is no unified and comprehensive evaluation criterion, leading to the\ninability to evaluate the quality and potential risks of medical LLMs, further\nhindering the application of LLMs in medical treatment scenarios. Besides,\ncurrent evaluations heavily rely on labor-intensive interactions with LLMs to\nobtain diagnostic dialogues and human evaluation on the quality of diagnosis\ndialogue. To tackle the lack of unified and comprehensive evaluation criterion,\nwe first initially establish an evaluation criterion, termed LLM-specific\nMini-CEX to assess the diagnostic capabilities of LLMs effectively, based on\noriginal Mini-CEX. To address the labor-intensive interaction problem, we\ndevelop a patient simulator to engage in automatic conversations with LLMs, and\nutilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental\nresults show that the LLM-specific Mini-CEX is adequate and necessary to\nevaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual\nevaluation on the metrics of humanistic qualities and provides reproducible and\nautomated comparisons between different LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.07635v1.pdf"
    },
    {
        "title": "A Survey on Model Compression for Large Language Models",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "published": "2023-08-15T08:31:05Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks with remarkable success. However, their formidable size and computational\ndemands present significant challenges for practical deployment, especially in\nresource-constrained environments. As these challenges become increasingly\npertinent, the field of model compression has emerged as a pivotal research\narea to alleviate these limitations. This paper presents a comprehensive survey\nthat navigates the landscape of model compression techniques tailored\nspecifically for LLMs. Addressing the imperative need for efficient deployment,\nwe delve into various methodologies, encompassing quantization, pruning,\nknowledge distillation, and more. Within each of these techniques, we highlight\nrecent advancements and innovative approaches that contribute to the evolving\nlandscape of LLM research. Furthermore, we explore benchmarking strategies and\nevaluation metrics that are essential for assessing the effectiveness of\ncompressed LLMs. By providing insights into the latest developments and\npractical implications, this survey serves as an invaluable resource for both\nresearchers and practitioners. As LLMs continue to evolve, this survey aims to\nfacilitate enhanced efficiency and real-world applicability, establishing a\nfoundation for future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2308.07633v3.pdf"
    },
    {
        "title": "Detecting The Corruption Of Online Questionnaires By Artificial Intelligence",
        "authors": [
            "Benjamin Lebrun",
            "Sharon Temtsin",
            "Andrew Vonasch",
            "Christoph Bartneck"
        ],
        "published": "2023-08-14T23:47:56Z",
        "summary": "Online questionnaires that use crowd-sourcing platforms to recruit\nparticipants have become commonplace, due to their ease of use and low costs.\nArtificial Intelligence (AI) based Large Language Models (LLM) have made it\neasy for bad actors to automatically fill in online forms, including generating\nmeaningful text for open-ended tasks. These technological advances threaten the\ndata quality for studies that use online questionnaires. This study tested if\ntext generated by an AI for the purpose of an online study can be detected by\nboth humans and automatic AI detection systems. While humans were able to\ncorrectly identify authorship of text above chance level (76 percent accuracy),\ntheir performance was still below what would be required to ensure satisfactory\ndata quality. Researchers currently have to rely on the disinterest of bad\nactors to successfully use open-ended responses as a useful tool for ensuring\ndata quality. Automatic AI detection systems are currently completely unusable.\nIf AIs become too prevalent in submitting responses then the costs associated\nwith detecting fraudulent submissions will outweigh the benefits of online\nquestionnaires. Individual attention checks will no longer be a sufficient tool\nto ensure good data quality. This problem can only be systematically addressed\nby crowd-sourcing platforms. They cannot rely on automatic AI detection systems\nand it is unclear how they can ensure data quality for their paying clients.",
        "pdf_link": "https://arxiv.org/pdf/2308.07499v1.pdf"
    },
    {
        "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
        "authors": [
            "Mansi Phute",
            "Alec Helbling",
            "Matthew Hull",
            "ShengYun Peng",
            "Sebastian Szyller",
            "Cory Cornelius",
            "Duen Horng Chau"
        ],
        "published": "2023-08-14T17:54:10Z",
        "summary": "Large language models (LLMs) are popular for high-quality text generation but\ncan produce harmful content, even when aligned with human values through\nreinforcement learning. Adversarial prompts can bypass their safety measures.\nWe propose LLM Self Defense, a simple approach to defend against these attacks\nby having an LLM screen the induced responses. Our method does not require any\nfine-tuning, input preprocessing, or iterative output generation. Instead, we\nincorporate the generated content into a pre-defined prompt and employ another\ninstance of an LLM to analyze the text and predict whether it is harmful. We\ntest LLM Self Defense on GPT 3.5 and Llama 2, two of the current most prominent\nLLMs against various types of attacks, such as forcefully inducing affirmative\nresponses to prompts and prompt engineering attacks. Notably, LLM Self Defense\nsucceeds in reducing the attack success rate to virtually 0 using both GPT 3.5\nand Llama 2.",
        "pdf_link": "https://arxiv.org/pdf/2308.07308v3.pdf"
    },
    {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "authors": [
            "Chi-Min Chan",
            "Weize Chen",
            "Yusheng Su",
            "Jianxuan Yu",
            "Wei Xue",
            "Shanghang Zhang",
            "Jie Fu",
            "Zhiyuan Liu"
        ],
        "published": "2023-08-14T15:13:04Z",
        "summary": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.07201v1.pdf"
    },
    {
        "title": "Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice",
        "authors": [
            "Alexandra Sasha Luccioni",
            "Anna Rogers"
        ],
        "published": "2023-08-14T13:00:53Z",
        "summary": "Much of the recent discourse within the NLP research community has been\ncentered around Large Language Models (LLMs), their functionality and potential\n-- yet not only do we not have a working definition of LLMs, but much of this\ndiscourse relies on claims and assumptions that are worth re-examining. This\nposition paper contributes a definition of LLMs, explicates some of the\nassumptions made regarding their functionality, and outlines the existing\nevidence for and against them. We conclude with suggestions for research\ndirections and their framing in future work.",
        "pdf_link": "https://arxiv.org/pdf/2308.07120v1.pdf"
    },
    {
        "title": "CausalLM is not optimal for in-context learning",
        "authors": [
            "Nan Ding",
            "Tomer Levinboim",
            "Jialin Wu",
            "Sebastian Goodman",
            "Radu Soricut"
        ],
        "published": "2023-08-14T03:14:38Z",
        "summary": "Recent empirical evidence indicates that transformer based in-context\nlearning performs better when using a prefix language model (prefixLM), in\nwhich in-context samples can all attend to each other, compared to causal\nlanguage models (causalLM), which use auto-regressive attention that prohibits\nin-context samples to attend to future samples. While this result is intuitive,\nit is not understood from a theoretical perspective. In this paper we take a\ntheoretical approach and analyze the convergence behavior of prefixLM and\ncausalLM under a certain parameter construction. Our analysis shows that both\nLM types converge to their stationary points at a linear rate, but that while\nprefixLM converges to the optimal solution of linear regression, causalLM\nconvergence dynamics follows that of an online gradient descent algorithm,\nwhich is not guaranteed to be optimal even as the number of samples grows\ninfinitely. We supplement our theoretical claims with empirical experiments\nover synthetic and real tasks and using various types of transformers. Our\nexperiments verify that causalLM consistently underperforms prefixLM in all\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2308.06912v3.pdf"
    },
    {
        "title": "Generative Interpretation",
        "authors": [
            "Yonathan A. Arbel",
            "David Hoffman"
        ],
        "published": "2023-08-14T02:59:27Z",
        "summary": "We introduce generative interpretation, a new approach to estimating\ncontractual meaning using large language models. As AI triumphalism is the\norder of the day, we proceed by way of grounded case studies, each illustrating\nthe capabilities of these novel tools in distinct ways. Taking well-known\ncontracts opinions, and sourcing the actual agreements that they adjudicated,\nwe show that AI models can help factfinders ascertain ordinary meaning in\ncontext, quantify ambiguity, and fill gaps in parties' agreements. We also\nillustrate how models can calculate the probative value of individual pieces of\nextrinsic evidence. After offering best practices for the use of these models\ngiven their limitations, we consider their implications for judicial practice\nand contract theory. Using LLMs permits courts to estimate what the parties\nintended cheaply and accurately, and as such generative interpretation\nunsettles the current interpretative stalemate. Their use responds to\nefficiency-minded textualists and justice-oriented contextualists, who argue\nabout whether parties will prefer cost and certainty or accuracy and fairness.\nParties--and courts--would prefer a middle path, in which adjudicators strive\nto predict what the contract really meant, admitting just enough context to\napproximate reality while avoiding unguided and biased assimilation of\nevidence. As generative interpretation offers this possibility, we argue it can\nbecome the new workhorse of contractual interpretation.",
        "pdf_link": "https://arxiv.org/pdf/2308.06907v1.pdf"
    },
    {
        "title": "Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine",
        "authors": [
            "Thomas Savage",
            "Ashwin Nayak",
            "Robert Gallo",
            "Ekanath Rangan",
            "Jonathan H Chen"
        ],
        "published": "2023-08-13T19:04:07Z",
        "summary": "One of the major barriers to using large language models (LLMs) in medicine\nis the perception they use uninterpretable methods to make clinical decisions\nthat are inherently different from the cognitive processes of clinicians. In\nthis manuscript we develop novel diagnostic reasoning prompts to study whether\nLLMs can perform clinical reasoning to accurately form a diagnosis. We find\nthat GPT4 can be prompted to mimic the common clinical reasoning processes of\nclinicians without sacrificing diagnostic accuracy. This is significant because\nan LLM that can use clinical reasoning to provide an interpretable rationale\noffers physicians a means to evaluate whether LLMs can be trusted for patient\ncare. Novel prompting methods have the potential to expose the black box of\nLLMs, bringing them one step closer to safe and effective use in medicine.",
        "pdf_link": "https://arxiv.org/pdf/2308.06834v1.pdf"
    },
    {
        "title": "Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
        "authors": [
            "Jiajia Li",
            "Mingle Xu",
            "Lirong Xiang",
            "Dong Chen",
            "Weichao Zhuang",
            "Xunyuan Yin",
            "Zhaojian Li"
        ],
        "published": "2023-08-13T02:59:36Z",
        "summary": "The past decade has witnessed the rapid development and adoption of ML & DL\nmethodologies in agricultural systems, showcased by great successes in\nagricultural applications. However, these conventional ML/DL models have\ncertain limitations: they heavily rely on large, costly-to-acquire labeled\ndatasets for training, require specialized expertise for development and\nmaintenance, and are mostly tailored for specific tasks, thus lacking\ngeneralizability. Recently, large pre-trained models, also known as FMs, have\ndemonstrated remarkable successes in language, vision, and decision-making\ntasks across various domains. These models are trained on a large amount of\ndata from multiple domains and modalities. Once trained, they can accomplish\nversatile tasks with just minor fine-tuning and minimal task-specific labeled\ndata. Despite their proven effectiveness and huge potential, there has been\nlittle exploration of applying FMs to agriculture AI. Thus, this study aims to\nexplore the potential of FMs in the field of smart agriculture. In particular,\nconceptual tools and technical background are presented to help the\nunderstanding of the problem space and uncover new research directions. To this\nend, recent FMs in the general CS domain are reviewed, and the models are\ncategorized into four categories: language FMs, vision FMs, multimodal FMs, and\nreinforcement learning FMs. Then, the steps of developing agriculture FMs\n(AFMs) are outlined and potential applications in smart agriculture are\ndiscussed. Moreover, challenges and risks associated with developing AFMs are\ndiscussed, including model training, validation, and deployment. In summary,\nthe advancement of AI in agriculture is explored by introducing AFMs as a\npromising paradigm that can significantly mitigate the reliance on extensive\nlabeled datasets and enhance the efficiency, effectiveness, and generalization\nof agricultural AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2308.06668v4.pdf"
    },
    {
        "title": "Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation",
        "authors": [
            "Ambrose Robinson",
            "William Thorne",
            "Ben P. Wu",
            "Abdullah Pandor",
            "Munira Essat",
            "Mark Stevenson",
            "Xingyi Song"
        ],
        "published": "2023-08-12T16:56:55Z",
        "summary": "Medical systematic reviews can be very costly and resource intensive. We\nexplore how Large Language Models (LLMs) can support and be trained to perform\nliterature screening when provided with a detailed set of selection criteria.\nSpecifically, we instruction tune LLaMA and Guanaco models to perform abstract\nscreening for medical systematic reviews. Our best model, Bio-SIEVE,\noutperforms both ChatGPT and trained traditional approaches, and generalises\nbetter across medical domains. However, there remains the challenge of adapting\nthe model to safety-first scenarios. We also explore the impact of multi-task\ntraining with Bio-SIEVE-Multi, including tasks such as PICO extraction and\nexclusion reasoning, but find that it is unable to match single-task\nBio-SIEVE's performance. We see Bio-SIEVE as an important step towards\nspecialising LLMs for the biomedical systematic review process and explore its\nfuture developmental opportunities. We release our models, code and a list of\nDOIs to reconstruct our dataset for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2308.06610v1.pdf"
    },
    {
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
        "authors": [
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Wenxuan Wang",
            "Jen-tse Huang",
            "Pinjia He",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-08-12T04:05:57Z",
        "summary": "Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.",
        "pdf_link": "https://arxiv.org/pdf/2308.06463v2.pdf"
    },
    {
        "title": "Dynamic Planning with a LLM",
        "authors": [
            "Gautier Dagan",
            "Frank Keller",
            "Alex Lascarides"
        ],
        "published": "2023-08-11T21:17:13Z",
        "summary": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot\nsettings, applications involving embodied agents remain problematic. In\nparticular, complex plans that require multi-step reasoning become difficult\nand too costly as the context window grows. Planning requires understanding the\nlikely effects of one's actions and identifying whether the current environment\nsatisfies the goal state. While symbolic planners find optimal solutions\nquickly, they require a complete and accurate representation of the planning\nproblem, severely limiting their use in practical scenarios. In contrast,\nmodern LLMs cope with noisy observations and high levels of uncertainty when\nreasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a\nneuro-symbolic framework where an LLM works hand-in-hand with a traditional\nplanner to solve an embodied task. Given action-descriptions, LLM-DP solves\nAlfworld faster and more efficiently than a naive LLM ReAct baseline.",
        "pdf_link": "https://arxiv.org/pdf/2308.06391v1.pdf"
    },
    {
        "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
        "authors": [
            "Terufumi Morishita",
            "Gaku Morio",
            "Atsuki Yamaguchi",
            "Yasuhiro Sogawa"
        ],
        "published": "2023-08-11T13:15:35Z",
        "summary": "We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2308.07336v3.pdf"
    },
    {
        "title": "Assessing Student Errors in Experimentation Using Artificial Intelligence and Large Language Models: A Comparative Study with Human Raters",
        "authors": [
            "Arne Bewersdorff",
            "Kathrin Se√üler",
            "Armin Baur",
            "Enkelejda Kasneci",
            "Claudia Nerdel"
        ],
        "published": "2023-08-11T12:03:12Z",
        "summary": "Identifying logical errors in complex, incomplete or even contradictory and\noverall heterogeneous data like students' experimentation protocols is\nchallenging. Recognizing the limitations of current evaluation methods, we\ninvestigate the potential of Large Language Models (LLMs) for automatically\nidentifying student errors and streamlining teacher assessments. Our aim is to\nprovide a foundation for productive, personalized feedback. Using a dataset of\n65 student protocols, an Artificial Intelligence (AI) system based on the\nGPT-3.5 and GPT-4 series was developed and tested against human raters. Our\nresults indicate varying levels of accuracy in error detection between the AI\nsystem and human raters. The AI system can accurately identify many fundamental\nstudent errors, for instance, the AI system identifies when a student is\nfocusing the hypothesis not on the dependent variable but solely on an expected\nobservation (acc. = 0.90), when a student modifies the trials in an ongoing\ninvestigation (acc. = 1), and whether a student is conducting valid test trials\n(acc. = 0.82) reliably. The identification of other, usually more complex\nerrors, like whether a student conducts a valid control trial (acc. = .60),\nposes a greater challenge. This research explores not only the utility of AI in\neducational settings, but also contributes to the understanding of the\ncapabilities of LLMs in error detection in inquiry-based learning like\nexperimentation.",
        "pdf_link": "https://arxiv.org/pdf/2308.06088v1.pdf"
    },
    {
        "title": "Large Language Models for Telecom: Forthcoming Impact on the Industry",
        "authors": [
            "Ali Maatouk",
            "Nicola Piovesan",
            "Fadhel Ayed",
            "Antonio De Domenico",
            "Merouane Debbah"
        ],
        "published": "2023-08-11T08:41:00Z",
        "summary": "Large Language Models (LLMs), AI-driven models that can achieve\ngeneral-purpose language understanding and generation, have emerged as a\ntransformative force, revolutionizing fields well beyond Natural Language\nProcessing (NLP) and garnering unprecedented attention. As LLM technology\ncontinues to progress, the telecom industry is facing the prospect of its\nimpact on its landscape. To elucidate these implications, we delve into the\ninner workings of LLMs, providing insights into their current capabilities and\nlimitations. We also examine the use cases that can be readily implemented in\nthe telecom industry, streamlining tasks, such as anomalies resolutions and\ntechnical specifications comprehension, which currently hinder operational\nefficiency and demand significant manpower and expertise. Furthermore, we\nuncover essential research directions that deal with the distinctive challenges\nof utilizing the LLMs within the telecom domain. Addressing them represents a\nsignificant stride towards fully harnessing the potential of LLMs and unlocking\ntheir capabilities to the fullest extent within the telecom domain.",
        "pdf_link": "https://arxiv.org/pdf/2308.06013v2.pdf"
    },
    {
        "title": "PIPPA: A Partially Synthetic Conversational Dataset",
        "authors": [
            "Tear Gosling",
            "Alpin Dale",
            "Yinhe Zheng"
        ],
        "published": "2023-08-11T00:33:26Z",
        "summary": "With the emergence of increasingly powerful large language models, there is a\nburgeoning interest in leveraging these models for casual conversation and\nrole-play applications. However, existing conversational and role-playing\ndatasets often fail to capture the diverse and nuanced interactions typically\nexhibited by real-world role-play participants. To address this limitation and\ncontribute to the rapidly growing field, we introduce a partially-synthetic\ndataset named PIPPA (Personal Interaction Pairs between People and AI). PIPPA\nis a result of a community-driven crowdsourcing effort involving a group of\nrole-play enthusiasts. The dataset comprises over 1 million utterances that are\ndistributed across 26,000 conversation sessions and provides a rich resource\nfor researchers and AI developers to explore and refine conversational AI\nsystems in the context of role-play scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.05884v1.pdf"
    },
    {
        "title": "Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems",
        "authors": [
            "Ernest Davis",
            "Scott Aaronson"
        ],
        "published": "2023-08-10T17:22:28Z",
        "summary": "This report describes a test of the large language model GPT-4 with the\nWolfram Alpha and the Code Interpreter plug-ins on 105 original problems in\nscience and math, at the high school and college levels, carried out in\nJune-August 2023. Our tests suggest that the plug-ins significantly enhance\nGPT's ability to solve these problems. Having said that, there are still often\n\"interface\" failures; that is, GPT often has trouble formulating problems in a\nway that elicits useful answers from the plug-ins. Fixing these interface\nfailures seems like a central challenge in making GPT a reliable tool for\ncollege-level calculation problems.",
        "pdf_link": "https://arxiv.org/pdf/2308.05713v2.pdf"
    },
    {
        "title": "NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search",
        "authors": [
            "Edouard Yvinec",
            "Arnaud Dapogny",
            "Kevin Bailly"
        ],
        "published": "2023-08-10T14:19:58Z",
        "summary": "Deep neural network (DNN) deployment has been confined to larger hardware\ndevices due to their expensive computational requirements. This challenge has\nrecently reached another scale with the emergence of large language models\n(LLMs). In order to reduce both their memory footprint and latency, a promising\ntechnique is quantization. It consists in converting floating point\nrepresentations to low bit-width fixed point representations, usually by\nassuming a uniform mapping onto a regular grid. This process, referred to in\nthe literature as uniform quantization, may however be ill-suited as most DNN\nweights and activations follow a bell-shaped distribution. This is even worse\non LLMs whose weight distributions are known to exhibit large, high impact,\noutlier values. In this work, we propose an improvement over the most commonly\nadopted way to tackle this limitation in deep learning models quantization,\nnamely, non-uniform quantization. NUPES leverages automorphisms to preserve the\nscalar multiplications. Such transformations are derived from power functions.\nHowever, the optimization of the exponent parameter and weight values remains a\nchallenging and novel problem which could not be solved with previous post\ntraining optimization techniques which only learn to round up or down weight\nvalues in order to preserve the predictive function. We circumvent this\nlimitation with a new paradigm: learning new quantized weights over the entire\nquantized space. Similarly, we enable the optimization of the power exponent,\ni.e. the optimization of the quantization operator itself during training by\nalleviating all the numerical instabilities. The resulting predictive function\nis compatible with integer-only low-bit inference. We show the ability of the\nmethod to achieve state-of-the-art compression rates in both, data-free and\ndata-driven configurations.",
        "pdf_link": "https://arxiv.org/pdf/2308.05600v1.pdf"
    },
    {
        "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
        "authors": [
            "Miao Fan",
            "Chen Hu",
            "Shuchang Zhou"
        ],
        "published": "2023-08-10T13:50:17Z",
        "summary": "The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in\nshaping the impact of large language models (LLMs), contributing significantly\nto controlling output toxicity and selecting output styles, particularly as\nLLMs often harbor misleading content, highlighting the urgency to align them\nwith human values for secure AI systems. The RLHF, characterized by complexity,\ninstability, and sensitivity to hyperparameters, makes the evaluation of the\nreward model for complex tasks challenging, thereby further complicating the\nuse of Proximal Policy Optimization (PPO). In this paper, we introduce a simple\ntask designed to employ Gloden as a reward model that validates the\neffectiveness of PPO and inspires it, primarily explaining the task of\nutilizing PPO to manipulate the tokenizer length of the output generated by the\nmodel. Experiments confirm that PPO is not only effective in manipulating the\noutput tokenizer length to a certain extent in this type of task but also\nexhibits facilitated training once the influence of the reward model effect is\nexcluded, making it an exciting development.",
        "pdf_link": "https://arxiv.org/pdf/2308.05585v1.pdf"
    },
    {
        "title": "C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT",
        "authors": [
            "Pan Liang",
            "Danwei Ye",
            "Zihao Zhu",
            "Yunchao Wang",
            "Wang Xia",
            "Ronghua Liang",
            "Guodao Sun"
        ],
        "published": "2023-08-10T13:29:12Z",
        "summary": "Large language models (LLMs), such as ChatGPT, have demonstrated outstanding\nperformance in various fields, particularly in natural language understanding\nand generation tasks. In complex application scenarios, users tend to engage in\nmulti-turn conversations with ChatGPT to keep contextual information and obtain\ncomprehensive responses. However, human forgetting and model contextual\nforgetting remain prominent issues in multi-turn conversation scenarios, which\nchallenge the users' conversation comprehension and contextual continuity for\nChatGPT. To address these challenges, we propose an interactive conversation\nvisualization system called C5, which includes Global View, Topic View, and\nContext-associated Q\\&A View. The Global View uses the GitLog diagram metaphor\nto represent the conversation structure, presenting the trend of conversation\nevolution and supporting the exploration of locally salient features. The Topic\nView is designed to display all the question and answer nodes and their\nrelationships within a topic using the structure of a knowledge graph, thereby\ndisplay the relevance and evolution of conversations. The Context-associated\nQ\\&A View consists of three linked views, which allow users to explore\nindividual conversations deeply while providing specific contextual information\nwhen posing questions. The usefulness and effectiveness of C5 were evaluated\nthrough a case study and a user study.",
        "pdf_link": "https://arxiv.org/pdf/2308.05567v1.pdf"
    },
    {
        "title": "LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following",
        "authors": [
            "Kaize Shi",
            "Xueyao Sun",
            "Dingxian Wang",
            "Yinlin Fu",
            "Guandong Xu",
            "Qing Li"
        ],
        "published": "2023-08-09T12:26:37Z",
        "summary": "E-commerce authoring involves creating attractive, abundant, and targeted\npromotional content to drive product sales. The emergence of large language\nmodels (LLMs) introduces an innovative paradigm, offering a unified solution to\naddress various authoring tasks within this scenario. However, mainstream LLMs\ntrained on general corpora with common sense knowledge reveal limitations in\nfitting complex and personalized features unique to e-commerce products and\ncustomers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility,\nraising concerns about safeguarding voluminous customer privacy data during\ntransmission. This paper proposes the LLaMA-E, the unified and customized\ninstruction-following language models focusing on diverse e-commerce authoring\ntasks. Specifically, the domain experts create the seed instruction set from\nthe tasks of ads generation, query-enhanced product title rewriting, product\nclassification, purchase intent speculation, and general Q&A. These tasks\nenable the models to comprehensively understand precise e-commerce authoring\nknowledge by interleaving features covering typical service aspects of\ncustomers, sellers, and platforms. The GPT-3.5 is introduced as a teacher\nmodel, which expands the seed instructions to form a training set for the\nLLaMA-E models with various scales. The experimental results show that the\nproposed LLaMA-E models achieve state-of-the-art results in quantitative and\nqualitative evaluations, also exhibiting the advantage in zero-shot scenes. To\nthe best of our knowledge, this study is the first to serve the LLMs to\nspecific e-commerce authoring scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2308.04913v1.pdf"
    },
    {
        "title": "CLEVA: Chinese Language Models EVAluation Platform",
        "authors": [
            "Yanyang Li",
            "Jianqiao Zhao",
            "Duo Zheng",
            "Zi-Yuan Hu",
            "Zhi Chen",
            "Xiaohui Su",
            "Yongfeng Huang",
            "Shijia Huang",
            "Dahua Lin",
            "Michael R. Lyu",
            "Liwei Wang"
        ],
        "published": "2023-08-09T09:11:31Z",
        "summary": "With the continuous emergence of Chinese Large Language Models (LLMs), how to\nevaluate a model's capabilities has become an increasingly significant issue.\nThe absence of a comprehensive Chinese benchmark that thoroughly assesses a\nmodel's performance, the unstandardized and incomparable prompting procedure,\nand the prevalent risk of contamination pose major challenges in the current\nevaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted\nto holistically evaluate Chinese LLMs. Our platform employs a standardized\nworkflow to assess LLMs' performance across various dimensions, regularly\nupdating a competitive leaderboard. To alleviate contamination, CLEVA curates a\nsignificant proportion of new data and develops a sampling strategy that\nguarantees a unique subset for each leaderboard round. Empowered by an\neasy-to-use interface that requires just a few mouse clicks and a model API,\nusers can conduct a thorough evaluation with minimal coding. Large-scale\nexperiments featuring 23 Chinese LLMs have validated CLEVA's efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2308.04813v2.pdf"
    },
    {
        "title": "A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology",
        "authors": [
            "Sean Wu",
            "Michael Koo",
            "Lesley Blum",
            "Andy Black",
            "Liyo Kao",
            "Fabien Scalzo",
            "Ira Kurtz"
        ],
        "published": "2023-08-09T05:01:28Z",
        "summary": "In recent years, there have been significant breakthroughs in the field of\nnatural language processing, particularly with the development of large\nlanguage models (LLMs). These LLMs have showcased remarkable capabilities on\nvarious benchmarks. In the healthcare field, the exact role LLMs and other\nfuture AI models will play remains unclear. There is a potential for these\nmodels in the future to be used as part of adaptive physician training, medical\nco-pilot applications, and digital patient interaction scenarios. The ability\nof AI models to participate in medical training and patient care will depend in\npart on their mastery of the knowledge content of specific medical fields. This\nstudy investigated the medical knowledge capability of LLMs, specifically in\nthe context of internal medicine subspecialty multiple-choice test-taking\nability. We compared the performance of several open-source LLMs (Koala 7B,\nFalcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on\nmultiple-choice questions in the field of Nephrology. Nephrology was chosen as\nan example of a particularly conceptually complex subspecialty field within\ninternal medicine. The study was conducted to evaluate the ability of LLM\nmodels to provide correct answers to nephSAP (Nephrology Self-Assessment\nProgram) multiple-choice questions. The overall success of open-sourced LLMs in\nanswering the 858 nephSAP multiple-choice questions correctly was 17.1% -\n25.5%. In contrast, Claude 2 answered 54.4% of the questions correctly, whereas\nGPT-4 achieved a score of 73.3%. We show that current widely used open-sourced\nLLMs do poorly in their ability for zero-shot reasoning when compared to GPT-4\nand Claude 2. The findings of this study potentially have significant\nimplications for the future of subspecialty medical training and patient care.",
        "pdf_link": "https://arxiv.org/pdf/2308.04709v1.pdf"
    },
    {
        "title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA",
        "authors": [
            "Yuhan Ma",
            "Haiqi Jiang",
            "Chenyou Fan"
        ],
        "published": "2023-08-09T03:18:07Z",
        "summary": "Large Language Models (LLMs) have shown outstanding performance across wide\nrange of downstream tasks. This competency is attributed to their substantial\nparameter size and pre-training on extensive corpus. Moreover, LLMs have\nexhibited enhanced reasoning capabilities in tackling complex reasoning tasks,\nowing to the utilization of a method named ``Chain-of-Thought (CoT)\nprompting''. This method is designed to generate intermediate reasoning steps\nthat guide the inference of the final answer. However, it is essential to\nhighlight that these advanced reasoning abilities appear to emerge in models\nwith a minimum of 10 billion parameters, thereby limiting its efficacy in\nsituations where computational resources are constrained. In this paper, we\ninvestigate the possibility of transferring the reasoning capabilities of LLMs\nto smaller models via knowledge distillation. Specifically, we propose Sci-CoT,\na two-stage framework that separates the processes of generating rationales and\ninferring answers. This method enables a more efficient use of rationales\nduring the answer inference stage, leading to improved performance on\nscientific question-answering tasks. Utilizing Sci-CoT, our 80-million\nparameter model is able to exceed the performance of BLOOM-176B in the ARC-Easy\ndataset under the few shot setting.",
        "pdf_link": "https://arxiv.org/pdf/2308.04679v1.pdf"
    },
    {
        "title": "ChatGPT for Arabic Grammatical Error Correction",
        "authors": [
            "Sang Yun Kwon",
            "Gagan Bhatia",
            "El Moatez Billah Nagoud",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-08-08T18:00:39Z",
        "summary": "Recently, large language models (LLMs) fine-tuned to follow human instruction\nhave exhibited significant capabilities in various English NLP tasks. However,\ntheir performance in grammatical error correction (GEC) tasks, particularly in\nnon-English languages, remains significantly unexplored. In this paper, we\ndelve into abilities of instruction fine-tuned LLMs in Arabic GEC, a task made\ncomplex due to Arabic's rich morphology. Our findings suggest that various\nprompting methods, coupled with (in-context) few-shot learning, demonstrate\nconsiderable effectiveness, with GPT-4 achieving up to $65.49$\nF\\textsubscript{1} score under expert prompting (approximately $5$ points\nhigher than our established baseline). This highlights the potential of LLMs in\nlow-resource settings, offering a viable approach for generating useful\nsynthetic data for model training. Despite these positive results, we find that\ninstruction fine-tuned models, regardless of their size, significantly\nunderperform compared to fully fine-tuned models of significantly smaller\nsizes. This disparity highlights a substantial room for improvements for LLMs.\nInspired by methods from low-resource machine translation, we also develop a\nmethod exploiting synthetic data that significantly outperforms previous models\non two standard Arabic benchmarks. Our work sets new SoTA for Arabic GEC, with\n$72.19\\%$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2308.04492v1.pdf"
    },
    {
        "title": "A Comparative Study of Code Generation using ChatGPT 3.5 across 10 Programming Languages",
        "authors": [
            "Alessio Buscemi"
        ],
        "published": "2023-08-08T15:02:32Z",
        "summary": "Large Language Models (LLMs) are advanced Artificial Intelligence (AI)\nsystems that have undergone extensive training using large datasets in order to\nunderstand and produce language that closely resembles that of humans. These\nmodels have reached a level of proficiency where they are capable of\nsuccessfully completing university exams across several disciplines and\ngenerating functional code to handle novel problems. This research investigates\nthe coding proficiency of ChatGPT 3.5, a LLM released by OpenAI in November\n2022, which has gained significant recognition for its impressive text\ngenerating and code creation capabilities. The skill of the model in creating\ncode snippets is evaluated across 10 various programming languages and 4\ndifferent software domains. Based on the findings derived from this research,\nmajor unexpected behaviors and limitations of the model have been identified.\nThis study aims to identify potential areas for development and examine the\nramifications of automated code generation on the evolution of programming\nlanguages and on the tech industry.",
        "pdf_link": "https://arxiv.org/pdf/2308.04477v1.pdf"
    },
    {
        "title": "AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models",
        "authors": [
            "Zhu Deng",
            "Jinjie Liu",
            "Biao Luo",
            "Can Yuan",
            "Qingrun Yang",
            "Lei Xiao",
            "Wenwen Zhou",
            "Zhu Liu"
        ],
        "published": "2023-08-08T13:12:03Z",
        "summary": "The product carbon footprint (PCF) is crucial for decarbonizing the supply\nchain, as it measures the direct and indirect greenhouse gas emissions caused\nby all activities during the product's life cycle. However, PCF accounting\noften requires expert knowledge and significant time to construct life cycle\nmodels. In this study, we test and compare the emergent ability of five large\nlanguage models (LLMs) in modeling the 'cradle-to-gate' life cycles of products\nand generating the inventory data of inputs and outputs, revealing their\nlimitations as a generalized PCF knowledge database. By utilizing LLMs, we\npropose an automatic AI-driven PCF accounting framework, called AutoPCF, which\nalso applies deep learning algorithms to automatically match calculation\nparameters, and ultimately calculate the PCF. The results of estimating the\ncarbon footprint for three case products using the AutoPCF framework\ndemonstrate its potential in achieving automatic modeling and estimation of PCF\nwith a large reduction in modeling time from days to minutes.",
        "pdf_link": "https://arxiv.org/pdf/2308.04241v2.pdf"
    },
    {
        "title": "Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance",
        "authors": [
            "Menglin Xia",
            "Xuchao Zhang",
            "Camille Couturier",
            "Guoqing Zheng",
            "Saravan Rajmohan",
            "Victor Ruhle"
        ],
        "published": "2023-08-08T12:27:20Z",
        "summary": "Retrieval augmentation enhances performance of traditional language models by\nincorporating additional context. However, the computational demands for\nretrieval augmented large language models (LLMs) pose a challenge when applying\nthem to real-time tasks, such as composition assistance. To address this\nlimitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG)\nframework, a novel approach that efficiently combines a cloud-based LLM with a\nsmaller, client-side, language model through retrieval augmented memory. This\nintegration enables the client model to generate effective responses,\nbenefiting from the LLM's capabilities and contextual information.\nAdditionally, through an asynchronous memory update mechanism, the client model\ncan deliver real-time completions swiftly to user inputs without the need to\nwait for responses from the cloud. Our experiments on five benchmark datasets\ndemonstrate that HybridRAG significantly improves utility over client-only\nmodels while maintaining low latency.",
        "pdf_link": "https://arxiv.org/pdf/2308.04215v2.pdf"
    },
    {
        "title": "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
        "authors": [
            "Jiaju Lin",
            "Haoran Zhao",
            "Aochi Zhang",
            "Yiting Wu",
            "Huqiuyue Ping",
            "Qin Chen"
        ],
        "published": "2023-08-08T03:59:28Z",
        "summary": "With ChatGPT-like large language models (LLM) prevailing in the community,\nhow to evaluate the ability of LLMs is an open question. Existing evaluation\nmethods suffer from following shortcomings: (1) constrained evaluation\nabilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that\ntask-based evaluation, where LLM agents complete tasks in a simulated\nenvironment, is a one-for-all solution to solve above problems. We present\nAgentSims, an easy-to-use infrastructure for researchers from all disciplines\nto test the specific capacities they are interested in. Researchers can build\ntheir evaluation tasks by adding agents and buildings on an interactive GUI or\ndeploy and test new support mechanisms, i.e. memory, planning and tool-use\nsystems, by a few lines of codes. Our demo is available at\nhttps://agentsims.com .",
        "pdf_link": "https://arxiv.org/pdf/2308.04026v1.pdf"
    },
    {
        "title": "Revisiting Prompt Engineering via Declarative Crowdsourcing",
        "authors": [
            "Aditya G. Parameswaran",
            "Shreya Shankar",
            "Parth Asawa",
            "Naman Jain",
            "Yujie Wang"
        ],
        "published": "2023-08-07T18:04:12Z",
        "summary": "Large language models (LLMs) are incredibly powerful at comprehending and\ngenerating data in the form of text, but are brittle and error-prone. There has\nbeen an advent of toolkits and recipes centered around so-called prompt\nengineering-the process of asking an LLM to do something via a series of\nprompts. However, for LLM-powered data processing workflows, in particular,\noptimizing for quality, while keeping cost bounded, is a tedious, manual\nprocess. We put forth a vision for declarative prompt engineering. We view LLMs\nlike crowd workers and leverage ideas from the declarative crowdsourcing\nliterature-including leveraging multiple prompting strategies, ensuring\ninternal consistency, and exploring hybrid-LLM-non-LLM approaches-to make\nprompt engineering a more principled process. Preliminary case studies on\nsorting, entity resolution, and imputation demonstrate the promise of our\napproach",
        "pdf_link": "https://arxiv.org/pdf/2308.03854v1.pdf"
    },
    {
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
        "authors": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2023-08-07T16:55:20Z",
        "summary": "The misuse of large language models (LLMs) has garnered significant attention\nfrom the general public and LLM vendors. In response, efforts have been made to\nalign LLMs with human values and intent use. However, a particular type of\nadversarial prompts, known as jailbreak prompt, has emerged and continuously\nevolved to bypass the safeguards and elicit harmful content from LLMs. In this\npaper, we conduct the first measurement study on jailbreak prompts in the wild,\nwith 6,387 prompts collected from four platforms over six months. Leveraging\nnatural language processing technologies and graph-based community detection\nmethods, we discover unique characteristics of jailbreak prompts and their\nmajor attack strategies, such as prompt injection and privilege escalation. We\nalso observe that jailbreak prompts increasingly shift from public platforms to\nprivate ones, posing new challenges for LLM vendors in proactive detection. To\nassess the potential harm caused by jailbreak prompts, we create a question set\ncomprising 46,800 samples across 13 forbidden scenarios. Our experiments show\nthat current LLMs and safeguards cannot adequately defend jailbreak prompts in\nall scenarios. Particularly, we identify two highly effective jailbreak prompts\nwhich achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and\nthey have persisted online for over 100 days. Our work sheds light on the\nsevere and evolving threat landscape of jailbreak prompts. We hope our study\ncan facilitate the research community and LLM vendors in promoting safer and\nregulated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.03825v1.pdf"
    },
    {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-08-07T16:08:11Z",
        "summary": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent's reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 27 API-based and\nopen-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and OSS competitors. We identify the\ntypical reasons of failures in environments and LLMs, showing that poor\nlong-term reasoning, decision-making, and instruction following abilities are\nthe main obstacles for developing usable LLM agents. Training on code and high\nquality multi-turn alignment data could improve agent performance. Datasets,\nenvironments, and an integrated evaluation package for AgentBench are released\nat \\url{https://github.com/THUDM/AgentBench}.",
        "pdf_link": "https://arxiv.org/pdf/2308.03688v2.pdf"
    },
    {
        "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
        "authors": [
            "An Yan",
            "Yu Wang",
            "Yiwu Zhong",
            "Chengyu Dong",
            "Zexue He",
            "Yujie Lu",
            "William Wang",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "published": "2023-08-07T16:00:22Z",
        "summary": "Recent advances in foundation models present new opportunities for\ninterpretable visual recognition -- one can first query Large Language Models\n(LLMs) to obtain a set of attributes that describe each class, then apply\nvision-language models to classify images via these attributes. Pioneering work\nshows that querying thousands of attributes can achieve performance competitive\nwith image features. However, our further investigation on 8 datasets reveals\nthat LLM-generated attributes in a large quantity perform almost the same as\nrandom words. This surprising finding suggests that significant noise may be\npresent in these attributes. We hypothesize that there exist subsets of\nattributes that can maintain the classification performance with much smaller\nsizes, and propose a novel learning-to-search method to discover those concise\nsets of attributes. As a result, on the CUB dataset, our method achieves\nperformance close to that of massive LLM-generated attributes (e.g., 10k\nattributes for CUB), yet using only 32 attributes in total to distinguish 200\nbird species. Furthermore, our new paradigm demonstrates several additional\nbenefits: higher interpretability and interactivity for humans, and the ability\nto summarize knowledge for a recognition task.",
        "pdf_link": "https://arxiv.org/pdf/2308.03685v1.pdf"
    },
    {
        "title": "MedMine: Examining Pre-trained Language Models on Medication Mining",
        "authors": [
            "Haifa Alrdahi",
            "Lifeng Han",
            "Hendrik ≈†uvalov",
            "Goran Nenadic"
        ],
        "published": "2023-08-07T14:36:03Z",
        "summary": "Automatic medication mining from clinical and biomedical text has become a\npopular topic due to its real impact on healthcare applications and the recent\ndevelopment of powerful language models (LMs). However, fully-automatic\nextraction models still face obstacles to be overcome such that they can be\ndeployed directly into clinical practice for better impacts. Such obstacles\ninclude their imbalanced performances on different entity types and clinical\nevents. In this work, we examine current state-of-the-art pre-trained language\nmodels (PLMs) on such tasks, via fine-tuning including the monolingual model\nMed7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their\nadvantages and drawbacks using historical medication mining shared task data\nsets from n2c2-2018 challenges. We report the findings we get from these\nfine-tuning experiments such that they can facilitate future research on\naddressing them, for instance, how to combine their outputs, merge such models,\nor improve their overall accuracy by ensemble learning and data augmentation.\nMedMine is part of the M3 Initiative \\url{https://github.com/HECTA-UoM/M3}",
        "pdf_link": "https://arxiv.org/pdf/2308.03629v2.pdf"
    },
    {
        "title": "Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper API Pricing",
        "authors": [
            "Wai Man Si",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-08-07T13:10:35Z",
        "summary": "The Machine Learning as a Service (MLaaS) market is rapidly expanding and\nbecoming more mature. For example, OpenAI's ChatGPT is an advanced large\nlanguage model (LLM) that generates responses for various queries with\nassociated fees. Although these models can deliver satisfactory performance,\nthey are far from perfect. Researchers have long studied the vulnerabilities\nand limitations of LLMs, such as adversarial attacks and model toxicity.\nInevitably, commercial ML models are also not exempt from such issues, which\ncan be problematic as MLaaS continues to grow. In this paper, we discover a new\nattack strategy against LLM APIs, namely the prompt abstraction attack.\nSpecifically, we propose Mondrian, a simple and straightforward method that\nabstracts sentences, which can lower the cost of using LLM APIs. In this\napproach, the adversary first creates a pseudo API (with a lower established\nprice) to serve as the proxy of the target API (with a higher established\nprice). Next, the pseudo API leverages Mondrian to modify the user query,\nobtain the abstracted response from the target API, and forward it back to the\nend user. Our results show that Mondrian successfully reduces user queries'\ntoken length ranging from 13% to 23% across various tasks, including text\nclassification, generation, and question answering. Meanwhile, these abstracted\nqueries do not significantly affect the utility of task-specific and general\nlanguage models like ChatGPT. Mondrian also reduces instruction prompts' token\nlength by at least 11% without compromising output quality. As a result, the\nprompt abstraction attack enables the adversary to profit without bearing the\ncost of API development and deployment.",
        "pdf_link": "https://arxiv.org/pdf/2308.03558v1.pdf"
    },
    {
        "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
        "authors": [
            "Seungcheol Park",
            "Hojun Choi",
            "U Kang"
        ],
        "published": "2023-08-07T10:11:42Z",
        "summary": "Given a pretrained encoder-based language model, how can we accurately\ncompress it without retraining? Retraining-free structured pruning algorithms\nare crucial in pretrained language model compression due to their significantly\nreduced pruning cost and capability to prune large language models. However,\nexisting retraining-free algorithms encounter severe accuracy degradation, as\nthey fail to handle pruning errors, especially at high compression rates. In\nthis paper, we propose K-prune (Knowledge-preserving pruning), an accurate\nretraining-free structured pruning algorithm for pretrained encoder-based\nlanguage models. K-prune focuses on preserving the useful knowledge of the\npretrained model to minimize pruning errors through a carefully designed\niterative pruning process composed of knowledge measurement,\nknowledge-preserving mask search, and knowledge-preserving weight-tuning. As a\nresult, K-prune shows significant accuracy improvements up to 58.02%p higher F1\nscore compared to existing retraining-free pruning algorithms under a high\ncompression rate of 80% on the SQuAD benchmark without any retraining process.",
        "pdf_link": "https://arxiv.org/pdf/2308.03449v2.pdf"
    },
    {
        "title": "Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records",
        "authors": [
            "Shivani Shekhar",
            "Simran Tiwari",
            "T. C. Rensink",
            "Ramy Eskander",
            "Wael Salloum"
        ],
        "published": "2023-08-07T07:29:49Z",
        "summary": "The application of Artificial Intelligence (AI) in healthcare has been\nrevolutionary, especially with the recent advancements in transformer-based\nLarge Language Models (LLMs). However, the task of understanding unstructured\nelectronic medical records remains a challenge given the nature of the records\n(e.g., disorganization, inconsistency, and redundancy) and the inability of\nLLMs to derive reasoning paradigms that allow for comprehensive understanding\nof medical variables. In this work, we examine the power of coupling symbolic\nreasoning with language modeling toward improved understanding of unstructured\nclinical texts. We show that such a combination improves the extraction of\nseveral medical variables from unstructured records. In addition, we show that\nthe state-of-the-art commercially-free LLMs enjoy retrieval capabilities\ncomparable to those provided by their commercial counterparts. Finally, we\nelaborate on the need for LLM steering through the application of symbolic\nreasoning as the exclusive use of LLMs results in the lowest performance.",
        "pdf_link": "https://arxiv.org/pdf/2308.03360v1.pdf"
    },
    {
        "title": "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis",
        "authors": [
            "Yuqiang Sun",
            "Daoyuan Wu",
            "Yue Xue",
            "Han Liu",
            "Haijun Wang",
            "Zhengzi Xu",
            "Xiaofei Xie",
            "Yang Liu"
        ],
        "published": "2023-08-07T05:48:53Z",
        "summary": "Smart contracts are prone to various vulnerabilities, leading to substantial\nfinancial losses over time. Current analysis tools mainly target\nvulnerabilities with fixed control or data-flow patterns, such as re-entrancy\nand integer overflow. However, a recent study on Web3 security bugs revealed\nthat about 80% of these bugs cannot be audited by existing tools due to the\nlack of domain-specific property description and checking. Given recent\nadvances in Large Language Models (LLMs), it is worth exploring how Generative\nPre-training Transformer (GPT) could aid in detecting logicc vulnerabilities.\n  In this paper, we propose GPTScan, the first tool combining GPT with static\nanalysis for smart contract logic vulnerability detection. Instead of relying\nsolely on GPT to identify vulnerabilities, which can lead to high false\npositives and is limited by GPT's pre-trained knowledge, we utilize GPT as a\nversatile code understanding tool. By breaking down each logic vulnerability\ntype into scenarios and properties, GPTScan matches candidate vulnerabilities\nwith GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently\nrecognize key variables and statements, which are then validated by static\nconfirmation. Evaluation on diverse datasets with around 400 contract projects\nand 3K Solidity files shows that GPTScan achieves high precision (over 90%) for\ntoken contracts and acceptable precision (57.14%) for large projects like\nWeb3Bugs. It effectively detects ground-truth logic vulnerabilities with a\nrecall of over 70%, including 9 new vulnerabilities missed by human auditors.\nGPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01\nUSD to scan per thousand lines of Solidity code. Moreover, static confirmation\nhelps GPTScan reduce two-thirds of false positives.",
        "pdf_link": "https://arxiv.org/pdf/2308.03314v2.pdf"
    },
    {
        "title": "Exploiting Code Symmetries for Learning Program Semantics",
        "authors": [
            "Kexin Pei",
            "Weichen Li",
            "Qirui Jin",
            "Shuyang Liu",
            "Scott Geng",
            "Lorenzo Cavallaro",
            "Junfeng Yang",
            "Suman Jana"
        ],
        "published": "2023-08-07T05:40:58Z",
        "summary": "This paper tackles the challenge of teaching code semantics to Large Language\nModels (LLMs) for program analysis by incorporating code symmetries into the\nmodel architecture. We introduce a group-theoretic framework that defines code\nsymmetries as semantics-preserving transformations, where forming a code\nsymmetry group enables precise and efficient reasoning of code semantics. Our\nsolution, SymC, develops a novel variant of self-attention that is provably\nequivariant to code symmetries from the permutation group defined over the\nprogram dependence graph. SymC obtains superior performance on five program\nanalysis tasks, outperforming state-of-the-art code models, including GPT-4,\nwithout any pre-training. Our results suggest that code LLMs that encode the\ncode structural prior via the code symmetry group generalize better and faster.",
        "pdf_link": "https://arxiv.org/pdf/2308.03312v7.pdf"
    },
    {
        "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning",
        "authors": [
            "Longteng Zhang",
            "Lin Zhang",
            "Shaohuai Shi",
            "Xiaowen Chu",
            "Bo Li"
        ],
        "published": "2023-08-07T05:12:27Z",
        "summary": "The low-rank adaptation (LoRA) method can largely reduce the amount of\ntrainable parameters for fine-tuning large language models (LLMs), however, it\nstill requires expensive activation memory to update low-rank weights. Reducing\nthe number of LoRA layers or using activation recomputation could harm the\nfine-tuning performance or increase the computational overhead. In this work,\nwe present LoRA-FA, a memory-efficient fine-tuning method that reduces the\nactivation memory without performance degradation and expensive recomputation.\nLoRA-FA chooses to freeze the projection-down weight of $A$ and update the\nprojection-up weight of $B$ in each LoRA layer. It ensures the change of model\nweight reside in a low-rank space during LLMs fine-tuning, while eliminating\nthe requirement to store full-rank input activations. We conduct extensive\nexperiments across multiple model types (RoBERTa, T5, LLaMA) and model scales.\nOur results show that LoRA-FA can always achieve close fine-tuning accuracy\nacross different tasks compared to full parameter fine-tuning and LoRA.\nFurthermore, LoRA-FA can reduce the overall memory cost by up to 1.4$\\times$\ncompared to LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2308.03303v1.pdf"
    },
    {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "authors": [
            "Roger Grosse",
            "Juhan Bae",
            "Cem Anil",
            "Nelson Elhage",
            "Alex Tamkin",
            "Amirhossein Tajdini",
            "Benoit Steiner",
            "Dustin Li",
            "Esin Durmus",
            "Ethan Perez",
            "Evan Hubinger",
            "Kamilƒó Luko≈°i≈´tƒó",
            "Karina Nguyen",
            "Nicholas Joseph",
            "Sam McCandlish",
            "Jared Kaplan",
            "Samuel R. Bowman"
        ],
        "published": "2023-08-07T04:47:42Z",
        "summary": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2308.03296v1.pdf"
    },
    {
        "title": "Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)",
        "authors": [
            "Jordan Kodner",
            "Sarah Payne",
            "Jeffrey Heinz"
        ],
        "published": "2023-08-06T23:41:14Z",
        "summary": "We present a critical assessment of Piantadosi's (2023) claim that \"Modern\nlanguage models refute Chomsky's approach to language,\" focusing on four main\npoints. First, despite the impressive performance and utility of large language\nmodels (LLMs), humans achieve their capacity for language after exposure to\nseveral orders of magnitude less data. The fact that young children become\ncompetent, fluent speakers of their native languages with relatively little\nexposure to them is the central mystery of language learning to which Chomsky\ninitially drew attention, and LLMs currently show little promise of solving\nthis mystery. Second, what can the artificial reveal about the natural? Put\nsimply, the implications of LLMs for our understanding of the cognitive\nstructures and mechanisms underlying language and its acquisition are like the\nimplications of airplanes for understanding how birds fly. Third, LLMs cannot\nconstitute scientific theories of language for several reasons, not least of\nwhich is that scientific theories must provide interpretable explanations, not\njust predictions. This leads to our final point: to even determine whether the\nlinguistic and cognitive capabilities of LLMs rival those of humans requires\nexplicating what humans' capacities actually are. In other words, it requires a\nseparate theory of language and cognition; generative linguistics provides\nprecisely such a theory. As such, we conclude that generative linguistics as a\nscientific discipline will remain indispensable throughout the 21st century and\nbeyond.",
        "pdf_link": "https://arxiv.org/pdf/2308.03228v1.pdf"
    },
    {
        "title": "LARCH: Large Language Model-based Automatic Readme Creation with Heuristics",
        "authors": [
            "Yuta Koreeda",
            "Terufumi Morishita",
            "Osamu Imaichi",
            "Yasuhiro Sogawa"
        ],
        "published": "2023-08-06T12:28:24Z",
        "summary": "Writing a readme is a crucial aspect of software development as it plays a\nvital role in managing and reusing program code. Though it is a pain point for\nmany developers, automatically creating one remains a challenge even with the\nrecent advancements in large language models (LLMs), because it requires\ngenerating an abstract description from thousands of lines of code. In this\ndemo paper, we show that LLMs are capable of generating a coherent and\nfactually correct readmes if we can identify a code fragment that is\nrepresentative of the repository. Building upon this finding, we developed\nLARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages\nrepresentative code identification with heuristics and weak supervision.\nThrough human and automated evaluations, we illustrate that LARCH can generate\ncoherent and factually correct readmes in the majority of cases, outperforming\na baseline that does not rely on representative code identification. We have\nmade LARCH open-source and provided a cross-platform Visual Studio Code\ninterface and command-line interface, accessible at\nhttps://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's\ncapabilities is available at https://youtu.be/ZUKkh5ED-O4.",
        "pdf_link": "https://arxiv.org/pdf/2308.03099v2.pdf"
    },
    {
        "title": "TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties",
        "authors": [
            "Karima Kadaoui",
            "Samar M. Magdy",
            "Abdul Waheed",
            "Md Tawkat Islam Khondaker",
            "Ahmed Oumar El-Shangiti",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-08-06T08:29:16Z",
        "summary": "Despite the purported multilingual proficiency of instruction-finetuned large\nlanguage models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of\nthese models remains insufficiently explored. Considering this constraint, we\npresent a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5\nand GPT-4) regarding their machine translation proficiencies across ten\nvarieties of Arabic. Our evaluation covers diverse Arabic varieties such as\nClassical Arabic (CA), Modern Standard Arabic (MSA), and several country-level\ndialectal variants. Our analysis indicates that LLMs may encounter challenges\nwith dialects for which minimal public datasets exist, but on average are\nbetter translators of dialects than existing commercial systems. On CA and MSA,\ninstruction-tuned LLMs, however, trail behind commercial systems such as Google\nTranslate. Finally, we undertake a human-centric study to scrutinize the\nefficacy of the relatively recent model, Bard, in following human instructions\nduring translation tasks. Our analysis reveals a circumscribed capability of\nBard in aligning with human instructions in translation contexts. Collectively,\nour findings underscore that prevailing LLMs remain far from inclusive, with\nonly limited ability to cater for the linguistic and cultural intricacies of\ndiverse communities.",
        "pdf_link": "https://arxiv.org/pdf/2308.03051v2.pdf"
    },
    {
        "title": "An Empirical Study of AI-based Smart Contract Creation",
        "authors": [
            "Rabimba Karanjai",
            "Edward Li",
            "Lei Xu",
            "Weidong Shi"
        ],
        "published": "2023-08-05T21:38:57Z",
        "summary": "The introduction of large language models (LLMs) like ChatGPT and Google\nPalm2 for smart contract generation seems to be the first well-established\ninstance of an AI pair programmer. LLMs have access to a large number of\nopen-source smart contracts, enabling them to utilize more extensive code in\nSolidity than other code generation tools. Although the initial and informal\nassessments of LLMs for smart contract generation are promising, a systematic\nevaluation is needed to explore the limits and benefits of these models. The\nmain objective of this study is to assess the quality of generated code\nprovided by LLMs for smart contracts. We also aim to evaluate the impact of the\nquality and variety of input parameters fed to LLMs. To achieve this aim, we\ncreated an experimental setup for evaluating the generated code in terms of\nvalidity, correctness, and efficiency. Our study finds crucial evidence of\nsecurity bugs getting introduced in the generated smart contracts as well as\nthe overall quality and correctness of the code getting impacted. However, we\nalso identified the areas where it can be improved. The paper also proposes\nseveral potential research directions to improve the process, quality and\nsafety of generated smart contract codes.",
        "pdf_link": "https://arxiv.org/pdf/2308.02955v2.pdf"
    },
    {
        "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
        "authors": [
            "Cliff Wong",
            "Sheng Zhang",
            "Yu Gu",
            "Christine Moung",
            "Jacob Abel",
            "Naoto Usuyama",
            "Roshanthi Weerasinghe",
            "Brian Piening",
            "Tristan Naumann",
            "Carlo Bifulco",
            "Hoifung Poon"
        ],
        "published": "2023-08-04T07:51:15Z",
        "summary": "Clinical trial matching is a key process in health delivery and discovery. In\npractice, it is plagued by overwhelming unstructured data and unscalable manual\nprocessing. In this paper, we conduct a systematic study on scaling clinical\ntrial matching using large language models (LLMs), with oncology as the focus\narea. Our study is grounded in a clinical trial matching system currently in\ntest deployment at a large U.S. health network. Initial findings are promising:\nout of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate\neligibility criteria of clinical trials and extract complex matching logic\n(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially\noutperform prior strong baselines and may serve as a preliminary solution to\nhelp triage patient-trial candidates with humans in the loop. Our study also\nreveals a few significant growth areas for applying LLMs to end-to-end clinical\ntrial matching, such as context limitation and accuracy, especially in\nstructuring patient information from longitudinal medical records.",
        "pdf_link": "https://arxiv.org/pdf/2308.02180v3.pdf"
    },
    {
        "title": "The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations",
        "authors": [
            "Abel Salinas",
            "Parth Vipul Shah",
            "Yuzhong Huang",
            "Robert McCormack",
            "Fred Morstatter"
        ],
        "published": "2023-08-03T21:12:54Z",
        "summary": "Large Language Models (LLMs) have seen widespread deployment in various\nreal-world applications. Understanding these biases is crucial to comprehend\nthe potential downstream consequences when using LLMs to make decisions,\nparticularly for historically disadvantaged groups. In this work, we propose a\nsimple method for analyzing and comparing demographic bias in LLMs, through the\nlens of job recommendations. We demonstrate the effectiveness of our method by\nmeasuring intersectional biases within ChatGPT and LLaMA, two cutting-edge\nLLMs. Our experiments primarily focus on uncovering gender identity and\nnationality bias; however, our method can be extended to examine biases\nassociated with any intersection of demographic identities. We identify\ndistinct biases in both models toward various demographic identities, such as\nboth models consistently suggesting low-paying jobs for Mexican workers or\npreferring to recommend secretarial roles to women. Our study highlights the\nimportance of measuring the bias of LLMs in downstream applications to\nunderstand the potential for harm and inequitable outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2308.02053v2.pdf"
    },
    {
        "title": "Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models",
        "authors": [
            "Mahammed Kamruzzaman",
            "Gene Louis Kim"
        ],
        "published": "2023-08-03T20:29:27Z",
        "summary": "While reaching for NLP systems that maximize accuracy, other important\nmetrics of system performance are often overlooked. Prior models are easily\nforgotten despite their possible suitability in settings where large computing\nresources are unavailable or relatively more costly. In this paper, we perform\na broad comparative evaluation of document-level sentiment analysis models with\na focus on resource costs that are important for the feasibility of model\ndeployment and general climate consciousness. Our experiments consider\ndifferent feature extraction techniques, the effect of ensembling,\ntask-specific deep learning modeling, and domain-independent large language\nmodels (LLMs). We find that while a fine-tuned LLM achieves the best accuracy,\nsome alternate configurations provide huge (up to 24, 283 *) resource savings\nfor a marginal (<1%) loss in accuracy. Furthermore, we find that for smaller\ndatasets, the differences in accuracy shrink while the difference in resource\nconsumption grows further.",
        "pdf_link": "https://arxiv.org/pdf/2308.02022v1.pdf"
    },
    {
        "title": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation",
        "authors": [
            "Xueying Du",
            "Mingwei Liu",
            "Kaixin Wang",
            "Hanlin Wang",
            "Junwei Liu",
            "Yixuan Chen",
            "Jiayi Feng",
            "Chaofeng Sha",
            "Xin Peng",
            "Yiling Lou"
        ],
        "published": "2023-08-03T16:31:02Z",
        "summary": "In this work, we make the first attempt to evaluate LLMs in a more\nchallenging code generation scenario, i.e. class-level code generation. We\nfirst manually construct the first class-level code generation benchmark\nClassEval of 100 class-level Python code generation tasks with approximately\n500 person-hours. Based on it, we then perform the first study of 11\nstate-of-the-art LLMs on class-level code generation. Based on our results, we\nhave the following main findings. First, we find that all existing LLMs show\nmuch worse performance on class-level code generation compared to on standalone\nmethod-level code generation benchmarks like HumanEval; and the method-level\ncoding ability cannot equivalently reflect the class-level coding ability among\nLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior\nthan other LLMs on class-level code generation, and the second-tier models\nincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very\nsimilar performance. Third, we find that generating the entire class all at\nonce (i.e. holistic generation strategy) is the best generation strategy only\nfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental and\ncompositional) is better strategies for the other models with limited ability\nof understanding long instructions and utilizing the middle information.\nLastly, we find the limited model ability of generating method-dependent code\nand discuss the frequent error types in generated classes. Our benchmark is\navailable at https://github.com/FudanSELab/ClassEval.",
        "pdf_link": "https://arxiv.org/pdf/2308.01861v2.pdf"
    },
    {
        "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
        "authors": [
            "Zheng Yuan",
            "Hongyi Yuan",
            "Chengpeng Li",
            "Guanting Dong",
            "Keming Lu",
            "Chuanqi Tan",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2023-08-03T15:34:01Z",
        "summary": "Mathematical reasoning is a challenging task for large language models\n(LLMs), while the scaling relationship of it with respect to LLM capacity is\nunder-explored. In this paper, we investigate how the pre-training loss,\nsupervised data amount, and augmented data amount influence the reasoning\nperformances of a supervised LLM. We find that pre-training loss is a better\nindicator of the model's performance than the model's parameter count. We apply\nsupervised fine-tuning (SFT) with different amounts of supervised data and\nempirically find a log-linear relation between data amount and model\nperformance, and we find better models improve less with enlarged supervised\ndatasets. To augment more data samples for improving model performances without\nany human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT\nuses supervised models to generate and collect correct reasoning paths as\naugmented fine-tuning datasets. We find with augmented samples containing more\ndistinct reasoning paths, RFT improves mathematical reasoning performance more\nfor LLMs. We also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push\nLLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised\nfine-tuning (SFT) accuracy of 35.9\\% significantly.",
        "pdf_link": "https://arxiv.org/pdf/2308.01825v2.pdf"
    },
    {
        "title": "Does Correction Remain A Problem For Large Language Models?",
        "authors": [
            "Xiaowu Zhang",
            "Xiaotian Zhang",
            "Cheng Yang",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023-08-03T14:09:31Z",
        "summary": "As large language models, such as GPT, continue to advance the capabilities\nof natural language processing (NLP), the question arises: does the problem of\ncorrection still persist? This paper investigates the role of correction in the\ncontext of large language models by conducting two experiments. The first\nexperiment focuses on correction as a standalone task, employing few-shot\nlearning techniques with GPT-like models for error correction. The second\nexperiment explores the notion of correction as a preparatory task for other\nNLP tasks, examining whether large language models can tolerate and perform\nadequately on texts containing certain levels of noise or errors. By addressing\nthese experiments, we aim to shed light on the significance of correction in\nthe era of large language models and its implications for various NLP\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2308.01776v2.pdf"
    },
    {
        "title": "Evaluating ChatGPT text-mining of clinical records for obesity monitoring",
        "authors": [
            "Ivo S. Fins",
            "Heather Davies",
            "Sean Farrell",
            "Jose R. Torres",
            "Gina Pinchbeck",
            "Alan D. Radford",
            "Peter-John Noble"
        ],
        "published": "2023-08-03T10:11:42Z",
        "summary": "Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.",
        "pdf_link": "https://arxiv.org/pdf/2308.01666v1.pdf"
    },
    {
        "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
        "authors": [
            "Paul R√∂ttger",
            "Hannah Rose Kirk",
            "Bertie Vidgen",
            "Giuseppe Attanasio",
            "Federico Bianchi",
            "Dirk Hovy"
        ],
        "published": "2023-08-02T16:30:40Z",
        "summary": "Without proper safeguards, large language models will readily follow\nmalicious instructions and generate toxic content. This risk motivates safety\nefforts such as red-teaming and large-scale feedback learning, which aim to\nmake models both helpful and harmless. However, there is a tension between\nthese two objectives, since harmlessness requires models to refuse to comply\nwith unsafe prompts, and thus not be helpful. Recent anecdotal evidence\nsuggests that some models may have struck a poor balance, so that even clearly\nsafe prompts are refused if they use similar language to unsafe prompts or\nmention sensitive topics. In this paper, we introduce a new test suite called\nXSTest to identify such eXaggerated Safety behaviours in a systematic way.\nXSTest comprises 250 safe prompts across ten prompt types that well-calibrated\nmodels should not refuse to comply with, and 200 unsafe prompts as contrasts\nthat models, for most applications, should refuse. We describe XSTest's\ncreation and composition, and then use the test suite to highlight systematic\nfailure modes in state-of-the-art language models as well as more general\nchallenges in building safer language models.",
        "pdf_link": "https://arxiv.org/pdf/2308.01263v3.pdf"
    },
    {
        "title": "Towards More Human-like AI Communication: A Review of Emergent Communication Research",
        "authors": [
            "Nicolo' Brandizzi"
        ],
        "published": "2023-08-01T14:43:10Z",
        "summary": "In the recent shift towards human-centric AI, the need for machines to\naccurately use natural language has become increasingly important. While a\ncommon approach to achieve this is to train large language models, this method\npresents a form of learning misalignment where the model may not capture the\nunderlying structure and reasoning humans employ in using natural language,\npotentially leading to unexpected or unreliable behavior. Emergent\ncommunication (Emecom) is a field of research that has seen a growing number of\npublications in recent years, aiming to develop artificial agents capable of\nusing natural language in a way that goes beyond simple discriminative tasks\nand can effectively communicate and learn new concepts. In this review, we\npresent Emecom under two aspects. Firstly, we delineate all the common\nproprieties we find across the literature and how they relate to human\ninteractions. Secondly, we identify two subcategories and highlight their\ncharacteristics and open challenges. We encourage researchers to work together\nby demonstrating that different methods can be viewed as diverse solutions to a\ncommon problem and emphasize the importance of including diverse perspectives\nand expertise in the field. We believe a deeper understanding of human\ncommunication is crucial to developing machines that can accurately use natural\nlanguage in human-machine interactions.",
        "pdf_link": "https://arxiv.org/pdf/2308.02541v1.pdf"
    },
    {
        "title": "Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education",
        "authors": [
            "S. S. Manathunga",
            "Y. A. Illangasekara"
        ],
        "published": "2023-08-01T12:04:50Z",
        "summary": "Large Language Models are increasingly being used for various tasks including\ncontent generation and as chatbots. Despite their impressive performances in\ngeneral tasks, LLMs need to be aligned when applying for domain specific tasks\nto mitigate the problems of hallucination and producing harmful answers.\nRetrieval Augmented Generation (RAG) allows to easily attach and manipulate a\nnon-parametric knowledgebases to LLMs. Applications of RAG in the field of\nmedical education are discussed in this paper. A combined extractive and\nabstractive summarization method for large unstructured textual data using\nrepresentative vectors is proposed.",
        "pdf_link": "https://arxiv.org/pdf/2308.00479v1.pdf"
    },
    {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": [
            "Ning Miao",
            "Yee Whye Teh",
            "Tom Rainforth"
        ],
        "published": "2023-08-01T10:31:36Z",
        "summary": "The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.",
        "pdf_link": "https://arxiv.org/pdf/2308.00436v3.pdf"
    },
    {
        "title": "LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack",
        "authors": [
            "Hai Zhu",
            "Zhaoqing Yang",
            "Weiwei Shang",
            "Yuren Wu"
        ],
        "published": "2023-08-01T06:30:37Z",
        "summary": "Natural language processing models are vulnerable to adversarial examples.\nPrevious textual adversarial attacks adopt gradients or confidence scores to\ncalculate word importance ranking and generate adversarial examples. However,\nthis information is unavailable in the real world. Therefore, we focus on a\nmore realistic and challenging setting, named hard-label attack, in which the\nattacker can only query the model and obtain a discrete prediction label.\nExisting hard-label attack algorithms tend to initialize adversarial examples\nby random substitution and then utilize complex heuristic algorithms to\noptimize the adversarial perturbation. These methods require a lot of model\nqueries and the attack success rate is restricted by adversary initialization.\nIn this paper, we propose a novel hard-label attack algorithm named LimeAttack,\nwhich leverages a local explainable method to approximate word importance\nranking, and then adopts beam search to find the optimal solution. Extensive\nexperiments show that LimeAttack achieves the better attacking performance\ncompared with existing hard-label attack under the same query budget. In\naddition, we evaluate the effectiveness of LimeAttack on large language models,\nand results indicate that adversarial examples remain a significant threat to\nlarge language models. The adversarial examples crafted by LimeAttack are\nhighly transferable and effectively improve model robustness in adversarial\ntraining.",
        "pdf_link": "https://arxiv.org/pdf/2308.00319v2.pdf"
    },
    {
        "title": "Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting",
        "authors": [
            "Aseem Arora",
            "Shabbirhussain Bhaisaheb",
            "Harshit Nigam",
            "Manasi Patwardhan",
            "Lovekesh Vig",
            "Gautam Shroff"
        ],
        "published": "2023-08-01T05:31:36Z",
        "summary": "Cross-domain and cross-compositional generalization of Text-to-SQL semantic\nparsing is a challenging task. Existing Large Language Model (LLM) based\nsolutions rely on inference-time retrieval of few-shot exemplars from the\ntraining set to synthesize a run-time prompt for each Natural Language (NL)\ntest query. In contrast, we devise an algorithm which performs offline sampling\nof a minimal set-of few-shots from the training data, with complete coverage of\nSQL clauses, operators and functions, and maximal domain coverage within the\nallowed token length. This allows for synthesis of a fixed Generic Prompt (GP),\nwith a diverse set-of exemplars common across NL test queries, avoiding\nexpensive test time exemplar retrieval. We further auto-adapt the GP to the\ntarget database domain (DA-GP), to better handle cross-domain generalization;\nfollowed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle\ncross-compositional generalization. The synthesis of LTMP-DA-GP is an offline\ntask, to be performed one-time per new database with minimal human\nintervention. Our approach demonstrates superior performance on the KaggleDBQA\ndataset, designed to evaluate generalizability for the Text-to-SQL task. We\nfurther showcase consistent performance improvement of LTMP-DA-GP over GP,\nacross LLMs and databases of KaggleDBQA, highlighting the efficacy and model\nagnostic benefits of our prompt based adapt and decompose approach.",
        "pdf_link": "https://arxiv.org/pdf/2308.02582v3.pdf"
    },
    {
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
            "Haonan Li",
            "Yu Hao",
            "Yizhuo Zhai",
            "Zhiyun Qian"
        ],
        "published": "2023-08-01T02:57:43Z",
        "summary": "Static analysis is a widely used technique in software engineering for\nidentifying and mitigating bugs. However, a significant hurdle lies in\nachieving a delicate balance between precision and scalability. Large Language\nModels (LLMs) offer a promising alternative, as recent advances demonstrate\nremarkable capabilities in comprehending, generating, and even debugging code.\nYet, the logic of bugs can be complex and require sophisticated reasoning and a\nlarge analysis scope spanning multiple functions. Therefore, at this point,\nLLMs are better used in an assistive role to complement static analysis. In\nthis paper, we take a deep dive into the open space of LLM-assisted static\nanalysis, using use-before-initialization (UBI) bugs as a case study. To this\nend, we develop LLift, a fully automated framework that interfaces with both a\nstatic analysis tool and an LLM. By carefully designing the framework and the\nprompts, we are able to overcome a number of challenges, including bug-specific\nmodeling, the large problem scope, the non-deterministic nature of LLMs, etc.\nTested in a real-world scenario analyzing nearly a thousand potential UBI bugs\nproduced by static analysis, LLift demonstrates a potent capability, showcasing\na reasonable precision (50%) and appearing to have no missing bugs. It even\nidentified 13 previously unknown UBI bugs in the Linux kernel. This research\npaves the way for new opportunities and methodologies in using LLMs for bug\ndiscovery in extensive, real-world datasets.",
        "pdf_link": "https://arxiv.org/pdf/2308.00245v3.pdf"
    },
    {
        "title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
        "authors": [
            "Itay Itzhak",
            "Gabriel Stanovsky",
            "Nir Rosenfeld",
            "Yonatan Belinkov"
        ],
        "published": "2023-08-01T01:39:25Z",
        "summary": "Recent studies show that instruction tuning (IT) and reinforcement learning\nfrom human feedback (RLHF) improve the abilities of large language models (LMs)\ndramatically. While these tuning methods can help align models with human\nobjectives and generate high-quality text, not much is known about their\npotential adverse effects. In this work, we investigate the effect of IT and\nRLHF on decision making and reasoning in LMs, focusing on three cognitive\nbiases - the decoy effect, the certainty effect, and the belief bias - all of\nwhich are known to influence human decision-making and reasoning. Our findings\nhighlight the presence of these biases in various models from the GPT-3,\nMistral, and T5 families. Notably, we find a stronger presence of biases in\nmodels that have undergone instruction tuning, such as Flan-T5,\nMistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward\ncomprehending cognitive biases in instruction-tuned LMs, which is crucial for\nthe development of more reliable and unbiased language models.",
        "pdf_link": "https://arxiv.org/pdf/2308.00225v2.pdf"
    },
    {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "published": "2023-07-31T17:49:18Z",
        "summary": "The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2307.16883v1.pdf"
    },
    {
        "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
        "authors": [
            "Junjie Fei",
            "Teng Wang",
            "Jinrui Zhang",
            "Zhenyu He",
            "Chengjie Wang",
            "Feng Zheng"
        ],
        "published": "2023-07-31T09:47:06Z",
        "summary": "Image-to-text generation aims to describe images using natural language.\nRecently, zero-shot image captioning based on pre-trained vision-language\nmodels (VLMs) and large language models (LLMs) has made significant progress.\nHowever, we have observed and empirically demonstrated that these methods are\nsusceptible to modality bias induced by LLMs and tend to generate descriptions\ncontaining objects (entities) that do not actually exist in the image but\nfrequently appear during training (i.e., object hallucination). In this paper,\nwe propose ViECap, a transferable decoding model that leverages entity-aware\ndecoding to generate descriptions in both seen and unseen scenarios. ViECap\nincorporates entity-aware hard prompts to guide LLMs' attention toward the\nvisual entities present in the image, enabling coherent caption generation\nacross diverse scenes. With entity-aware hard prompts, ViECap is capable of\nmaintaining performance when transferring from in-domain to out-of-domain\nscenarios. Extensive experiments demonstrate that ViECap sets a new\nstate-of-the-art cross-domain (transferable) captioning and performs\ncompetitively in-domain captioning compared to previous VLMs-based zero-shot\nmethods. Our code is available at: https://github.com/FeiElysia/ViECap",
        "pdf_link": "https://arxiv.org/pdf/2307.16525v1.pdf"
    },
    {
        "title": "Deception Abilities Emerged in Large Language Models",
        "authors": [
            "Thilo Hagendorff"
        ],
        "published": "2023-07-31T09:27:01Z",
        "summary": "Large language models (LLMs) are currently at the forefront of intertwining\nartificial intelligence (AI) systems with human communication and everyday\nlife. Thus, aligning them with human values is of great importance. However,\ngiven the steady increase in reasoning abilities, future LLMs are under\nsuspicion of becoming able to deceive human operators and utilizing this\nability to bypass monitoring efforts. As a prerequisite to this, LLMs need to\npossess a conceptual understanding of deception strategies. This study reveals\nthat such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were\nnon-existent in earlier LLMs. We conduct a series of experiments showing that\nstate-of-the-art LLMs are able to understand and induce false beliefs in other\nagents, that their performance in complex deception scenarios can be amplified\nutilizing chain-of-thought reasoning, and that eliciting Machiavellianism in\nLLMs can alter their propensity to deceive. In sum, revealing hitherto unknown\nmachine behavior in LLMs, our study contributes to the nascent field of machine\npsychology.",
        "pdf_link": "https://arxiv.org/pdf/2307.16513v2.pdf"
    },
    {
        "title": "A Benchmark for Understanding Dialogue Safety in Mental Health Support",
        "authors": [
            "Huachuan Qiu",
            "Tong Zhao",
            "Anqi Li",
            "Shuai Zhang",
            "Hongliang He",
            "Zhenzhong Lan"
        ],
        "published": "2023-07-31T07:33:16Z",
        "summary": "Dialogue safety remains a pervasive challenge in open-domain human-machine\ninteraction. Existing approaches propose distinctive dialogue safety taxonomies\nand datasets for detecting explicitly harmful responses. However, these\ntaxonomies may not be suitable for analyzing response safety in mental health\nsupport. In real-world interactions, a model response deemed acceptable in\ncasual conversations might have a negligible positive impact on users seeking\nmental health support. To address these limitations, this paper aims to develop\na theoretically and factually grounded taxonomy that prioritizes the positive\nimpact on help-seekers. Additionally, we create a benchmark corpus with\nfine-grained labels for each dialogue session to facilitate further research.\nWe analyze the dataset using popular language models, including BERT-base,\nRoBERTa-large, and ChatGPT, to detect and understand unsafe responses within\nthe context of mental health support. Our study reveals that ChatGPT struggles\nto detect safety categories with detailed safety definitions in a zero- and\nfew-shot paradigm, whereas the fine-tuned model proves to be more suitable. The\ndeveloped dataset and findings serve as valuable benchmarks for advancing\nresearch on dialogue safety in mental health support, with significant\nimplications for improving the design and deployment of conversation agents in\nreal-world applications. We release our code and data here:\nhttps://github.com/qiuhuachuan/DialogueSafety.",
        "pdf_link": "https://arxiv.org/pdf/2307.16457v1.pdf"
    },
    {
        "title": "HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field",
        "authors": [
            "Mingliang Bai",
            "Zhihao Zhou",
            "Ruidong Wang",
            "Yusheng Yang",
            "Zizhen Qin",
            "Yunxiao Chen",
            "Chunjin Mu",
            "Jinfu Liu",
            "Daren Yu"
        ],
        "published": "2023-07-31T06:59:36Z",
        "summary": "Renewable energy is important for achieving carbon neutrality goal. With the\ngreat success of Large Language Models (LLMs) like ChatGPT in automatic content\ngeneration, LLMs are playing an increasingly important role. However, there has\nnot been a specially designed LLM for renewable energy. Meanwhile, there has\nnot been any dataset of renewable energy for training LLMs. Therefore, this\npaper published the first open-source Renewable Energy Academic Paper (REAP)\ndataset for non-commercial LLM research of renewable energy. REAP dataset is\ncollected through searching the title and abstract of 1,168,970 academic\nliteratures from Web of Science. Based on REAP dataset, HouYi model, the first\nLLM for renewable energy, is developed through finetuning general LLMs. HouYi\ndemonstrated powerful academic paper paragraph generation ability in renewable\nenergy field. Experiments show that its ability to generate academic papers on\nrenewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIE\nBot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.",
        "pdf_link": "https://arxiv.org/pdf/2308.01414v1.pdf"
    },
    {
        "title": "Distractor generation for multiple-choice questions with predictive prompting and large language models",
        "authors": [
            "Semere Kiros Bitew",
            "Johannes Deleu",
            "Chris Develder",
            "Thomas Demeester"
        ],
        "published": "2023-07-30T23:15:28Z",
        "summary": "Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable\nperformance across various tasks and have garnered significant attention from\nboth researchers and practitioners. However, in an educational context, we\nstill observe a performance gap in generating distractors -- i.e., plausible\nyet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In\nthis study, we propose a strategy for guiding LLMs such as ChatGPT, in\ngenerating relevant distractors by prompting them with question items\nautomatically retrieved from a question bank as well-chosen in-context\nexamples. We evaluate our LLM-based solutions using a quantitative assessment\non an existing test set, as well as through quality annotations by human\nexperts, i.e., teachers. We found that on average 53% of the generated\ndistractors presented to the teachers were rated as high-quality, i.e.,\nsuitable for immediate use as is, outperforming the state-of-the-art model. We\nalso show the gains of our approach 1 in generating high-quality distractors by\ncomparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with\nstatic examples.",
        "pdf_link": "https://arxiv.org/pdf/2307.16338v1.pdf"
    },
    {
        "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
        "authors": [
            "Aiwei Liu",
            "Leyi Pan",
            "Xuming Hu",
            "Shu'ang Li",
            "Lijie Wen",
            "Irwin King",
            "Philip S. Yu"
        ],
        "published": "2023-07-30T13:43:27Z",
        "summary": "Recently, text watermarking algorithms for large language models (LLMs) have\nbeen proposed to mitigate the potential harms of text generated by LLMs,\nincluding fake news and copyright issues. However, current watermark detection\nalgorithms require the secret key used in the watermark generation process,\nmaking them susceptible to security breaches and counterfeiting during public\ndetection. To address this limitation, we propose an unforgeable publicly\nverifiable watermark algorithm that uses two different neural networks for\nwatermark generation and detection, instead of using the same key at both\nstages. Meanwhile, the token embedding parameters are shared between the\ngeneration and detection networks, which makes the detection network achieve a\nhigh accuracy very efficiently. Experiments demonstrate that our algorithm\nattains high detection accuracy and computational efficiency through neural\nnetworks with a minimized number of parameters. Subsequent analysis confirms\nthe high complexity involved in forging the watermark from the detection\nnetwork. Our code and data are available at\n\\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}.",
        "pdf_link": "https://arxiv.org/pdf/2307.16230v5.pdf"
    },
    {
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
        "authors": [
            "Bohao Li",
            "Rui Wang",
            "Guangzhi Wang",
            "Yuying Ge",
            "Yixiao Ge",
            "Ying Shan"
        ],
        "published": "2023-07-30T04:25:16Z",
        "summary": "Based on powerful Large Language Models (LLMs), recent generative Multimodal\nLarge Language Models (MLLMs) have gained prominence as a pivotal research\narea, exhibiting remarkable capability for both comprehension and generation.\nIn this work, we address the evaluation of generative comprehension in MLLMs as\na preliminary step towards a comprehensive assessment of generative models, by\nintroducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple\nchoice questions with accurate human annotations (x 6 larger than existing\nbenchmarks), which spans 12 evaluation dimensions including the comprehension\nof both the image and video modality. We develop an advanced pipeline for\ngenerating multiple-choice questions that target specific evaluation\ndimensions, integrating both automatic filtering and manual verification\nprocesses. Multiple-choice questions with groundtruth options derived from\nhuman annotation enables an objective and efficient assessment of model\nperformance, eliminating the need for human or GPT intervention during\nevaluation. We further evaluate the performance of 18 models across all 12\ndimensions, covering both the spatial and temporal understanding. By revealing\nthe limitations of existing MLLMs through evaluation results, we aim for\nSEED-Bench to provide insights for motivating future research. We will launch\nand consistently maintain a leaderboard to provide a platform for the community\nto assess and investigate model capability.",
        "pdf_link": "https://arxiv.org/pdf/2307.16125v2.pdf"
    },
    {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
        "authors": [
            "Viet Dac Lai",
            "Chien Van Nguyen",
            "Nghia Trung Ngo",
            "Thuat Nguyen",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "published": "2023-07-29T18:01:46Z",
        "summary": "A key technology for the development of large language models (LLMs) involves\ninstruction tuning that helps align the models' responses with human\nexpectations to realize impressive learning abilities. Two major approaches for\ninstruction tuning characterize supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), which are currently applied to produce the\nbest commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for\nresearch and development efforts, various instruction-tuned open-source LLMs\nhave also been introduced recently, e.g., Alpaca, Vicuna, to name a few.\nHowever, existing open-source LLMs have only been instruction-tuned for English\nand a few popular languages, thus hindering their impacts and accessibility to\nmany other languages in the world. Among a few very recent work to explore\ninstruction tuning for LLMs in multiple languages, SFT has been used as the\nonly approach to instruction-tune LLMs for multiple languages. This has left a\nsignificant gap for fine-tuned LLMs based on RLHF in diverse languages and\nraised important questions on how RLHF can boost the performance of\nmultilingual instruction tuning. To overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based on RLHF for multiple languages.\nOkapi introduces instruction and response-ranked data in 26 diverse languages\nto facilitate the experiments and development of future multilingual LLM\nresearch. We also present benchmark datasets to enable the evaluation of\ngenerative LLMs in multiple languages. Our experiments demonstrate the\nadvantages of RLHF for multilingual instruction over SFT for different base\nmodels and datasets. Our framework and resources are released at\nhttps://github.com/nlp-uoregon/Okapi.",
        "pdf_link": "https://arxiv.org/pdf/2307.16039v2.pdf"
    },
    {
        "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs",
        "authors": [
            "Lean Wang",
            "Wenkai Yang",
            "Deli Chen",
            "Hao Zhou",
            "Yankai Lin",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023-07-29T14:11:15Z",
        "summary": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.",
        "pdf_link": "https://arxiv.org/pdf/2307.15992v3.pdf"
    },
    {
        "title": "Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system",
        "authors": [
            "Sumit Asthana",
            "Sagih Hilleli",
            "Pengcheng He",
            "Aaron Halfaker"
        ],
        "published": "2023-07-28T20:25:11Z",
        "summary": "Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality.",
        "pdf_link": "https://arxiv.org/pdf/2307.15793v1.pdf"
    },
    {
        "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools",
        "authors": [
            "Jingwei Ni",
            "Julia Bingler",
            "Chiara Colesanti-Senni",
            "Mathias Kraus",
            "Glen Gostlow",
            "Tobias Schimanski",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Nicolas Webersinke",
            "Tobias Wekhof",
            "Tingyu Yu",
            "Markus Leippold"
        ],
        "published": "2023-07-28T18:58:16Z",
        "summary": "In the face of climate change, are companies really taking substantial steps\ntoward more sustainable operations? A comprehensive answer lies in the dense,\ninformation-rich landscape of corporate sustainability reports. However, the\nsheer volume and complexity of these reports make human analysis very costly.\nTherefore, only a few entities worldwide have the resources to analyze these\nreports at scale, which leads to a lack of transparency in sustainability\nreporting. Empowering stakeholders with LLM-based automatic analysis tools can\nbe a promising way to democratize sustainability report analysis. However,\ndeveloping such tools is challenging due to (1) the hallucination of LLMs and\n(2) the inefficiency of bringing domain experts into the AI development loop.\nIn this paper, we ChatReport, a novel LLM-based system to automate the analysis\nof corporate sustainability reports, addressing existing challenges by (1)\nmaking the answers traceable to reduce the harm of hallucination and (2)\nactively involving domain experts in the development loop. We make our\nmethodology, annotated datasets, and generated analyses of 1015 reports\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2307.15770v2.pdf"
    },
    {
        "title": "A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",
        "authors": [
            "Arash Hajikhani",
            "Carolyn Cole"
        ],
        "published": "2023-07-28T09:20:22Z",
        "summary": "This paper examines the comparative effectiveness of a specialized compiled\nlanguage model and a general-purpose model like OpenAI's GPT-3.5 in detecting\nSDGs within text data. It presents a critical review of Large Language Models\n(LLMs), addressing challenges related to bias and sensitivity. The necessity of\nspecialized training for precise, unbiased analysis is underlined. A case study\nusing a company descriptions dataset offers insight into the differences\nbetween the GPT-3.5 and the specialized SDG detection model. While GPT-3.5\nboasts broader coverage, it may identify SDGs with limited relevance to the\ncompanies' activities. In contrast, the specialized model zeroes in on highly\npertinent SDGs. The importance of thoughtful model selection is emphasized,\ntaking into account task requirements, cost, complexity, and transparency.\nDespite the versatility of LLMs, the use of specialized models is suggested for\ntasks demanding precision and accuracy. The study concludes by encouraging\nfurther research to find a balance between the capabilities of LLMs and the\nneed for domain-specific expertise and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2307.15425v1.pdf"
    },
    {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": [
            "Ankit Pal",
            "Logesh Kumar Umapathi",
            "Malaikannan Sankarasubbu"
        ],
        "published": "2023-07-28T06:43:04Z",
        "summary": "This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io",
        "pdf_link": "https://arxiv.org/pdf/2307.15343v2.pdf"
    },
    {
        "title": "An Overview Of Temporal Commonsense Reasoning and Acquisition",
        "authors": [
            "Georg Wenzel",
            "Adam Jatowt"
        ],
        "published": "2023-07-28T01:30:15Z",
        "summary": "Temporal commonsense reasoning refers to the ability to understand the\ntypical temporal context of phrases, actions, and events, and use it to reason\nover problems requiring such knowledge. This trait is essential in temporal\nnatural language processing tasks, with possible applications such as timeline\nsummarization, temporal question answering, and temporal natural language\ninference. Recent research on the performance of large language models suggests\nthat, although they are adept at generating syntactically correct sentences and\nsolving classification tasks, they often take shortcuts in their reasoning and\nfall prey to simple linguistic traps. This article provides an overview of\nresearch in the domain of temporal commonsense reasoning, particularly focusing\non enhancing language model performance through a variety of augmentations and\ntheir evaluation across a growing number of datasets. However, these augmented\nmodels still struggle to approach human performance on reasoning tasks over\ntemporal common sense properties, such as the typical occurrence times,\norderings, or durations of events. We further emphasize the need for careful\ninterpretation of research to guard against overpromising evaluation results in\nlight of the shallow reasoning present in transformers. This can be achieved by\nappropriately preparing datasets and suitable evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2308.00002v3.pdf"
    },
    {
        "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
        "authors": [
            "Stephen Casper",
            "Xander Davies",
            "Claudia Shi",
            "Thomas Krendl Gilbert",
            "J√©r√©my Scheurer",
            "Javier Rando",
            "Rachel Freedman",
            "Tomasz Korbak",
            "David Lindner",
            "Pedro Freire",
            "Tony Wang",
            "Samuel Marks",
            "Charbel-Rapha√´l Segerie",
            "Micah Carroll",
            "Andi Peng",
            "Phillip Christoffersen",
            "Mehul Damani",
            "Stewart Slocum",
            "Usman Anwar",
            "Anand Siththaranjan",
            "Max Nadeau",
            "Eric J. Michaud",
            "Jacob Pfau",
            "Dmitrii Krasheninnikov",
            "Xin Chen",
            "Lauro Langosco",
            "Peter Hase",
            "Erdem Bƒ±yƒ±k",
            "Anca Dragan",
            "David Krueger",
            "Dorsa Sadigh",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-07-27T22:29:25Z",
        "summary": "Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2307.15217v2.pdf"
    },
    {
        "title": "Matching Patients to Clinical Trials with Large Language Models",
        "authors": [
            "Qiao Jin",
            "Zifeng Wang",
            "Charalampos S. Floudas",
            "Jimeng Sun",
            "Zhiyong Lu"
        ],
        "published": "2023-07-27T17:56:56Z",
        "summary": "Clinical trials are vital in advancing drug development and evidence-based\nmedicine, but their success is often hindered by challenges in patient\nrecruitment. In this work, we investigate the potential of large language\nmodels (LLMs) to assist individual patients and referral physicians in\nidentifying suitable clinical trials from an extensive selection. Specifically,\nwe introduce TrialGPT, a novel architecture employing LLMs to predict\ncriterion-level eligibility with detailed explanations, which are then\naggregated for ranking and excluding candidate clinical trials based on\nfree-text patient notes. We evaluate TrialGPT on three publicly available\ncohorts of 184 patients and 18,238 annotated clinical trials. The experimental\nresults demonstrate several key findings: First, TrialGPT achieves high\ncriterion-level prediction accuracy with faithful explanations. Second, the\naggregated trial-level TrialGPT scores are highly correlated with expert\neligibility annotations. Third, these scores prove effective in ranking\nclinical trials and exclude ineligible candidates. Our error analysis suggests\nthat current LLMs still make some mistakes due to limited medical knowledge and\ndomain-specific context understanding. Nonetheless, we believe the explanatory\ncapabilities of LLMs are highly valuable. Future research is warranted on how\nsuch AI assistants can be integrated into the routine trial matching workflow\nin real-world settings to improve its efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2307.15051v2.pdf"
    },
    {
        "title": "WavJourney: Compositional Audio Creation with Large Language Models",
        "authors": [
            "Xubo Liu",
            "Zhongkai Zhu",
            "Haohe Liu",
            "Yi Yuan",
            "Meng Cui",
            "Qiushi Huang",
            "Jinhua Liang",
            "Yin Cao",
            "Qiuqiang Kong",
            "Mark D. Plumbley",
            "Wenwu Wang"
        ],
        "published": "2023-07-26T17:54:04Z",
        "summary": "Despite breakthroughs in audio generation models, their capabilities are\noften confined to domain-specific conditions such as speech transcriptions and\naudio captions. However, real-world audio creation aims to generate harmonious\naudio containing various elements such as speech, music, and sound effects with\ncontrollable conditions, which is challenging to address using existing audio\ngeneration systems. We present WavJourney, a novel framework that leverages\nLarge Language Models (LLMs) to connect various audio models for audio\ncreation. WavJourney allows users to create storytelling audio content with\ndiverse audio elements simply from textual descriptions. Specifically, given a\ntext instruction, WavJourney first prompts LLMs to generate an audio script\nthat serves as a structured semantic representation of audio elements. The\naudio script is then converted into a computer program, where each line of the\nprogram calls a task-specific audio generation model or computational operation\nfunction. The computer program is then executed to obtain a compositional and\ninterpretable solution for audio creation. Experimental results suggest that\nWavJourney is capable of synthesizing realistic audio aligned with\ntextually-described semantic, spatial and temporal conditions, achieving\nstate-of-the-art results on text-to-audio generation benchmarks. Additionally,\nwe introduce a new multi-genre story benchmark. Subjective evaluations\ndemonstrate the potential of WavJourney in crafting engaging storytelling audio\ncontent from text. We further demonstrate that WavJourney can facilitate\nhuman-machine co-creation in multi-round dialogues. To foster future research,\nthe code and synthesized audio are available at:\nhttps://audio-agi.github.io/WavJourney_demopage/.",
        "pdf_link": "https://arxiv.org/pdf/2307.14335v2.pdf"
    },
    {
        "title": "This is not correct! Negation-aware Evaluation of Language Generation Systems",
        "authors": [
            "Miriam Ansch√ºtz",
            "Diego Miguel Lozano",
            "Georg Groh"
        ],
        "published": "2023-07-26T06:54:31Z",
        "summary": "Large language models underestimate the impact of negations on how much they\nchange the meaning of a sentence. Therefore, learned evaluation metrics based\non these models are insensitive to negations. In this paper, we propose\nNegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that,\nwe designed a rule-based sentence negation tool and used it to create the\nCANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a\nsentence transformer and an evaluation metric to improve their negation\nsensitivity. Evaluating these models on existing benchmarks shows that our\nfine-tuned models outperform existing metrics on the negated sentences by far\nwhile preserving their base models' performances on other perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2307.13989v1.pdf"
    },
    {
        "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
        "authors": [
            "Xuhai Xu",
            "Bingsheng Yao",
            "Yuanzhe Dong",
            "Saadia Gabriel",
            "Hong Yu",
            "James Hendler",
            "Marzyeh Ghassemi",
            "Anind K. Dey",
            "Dakuo Wang"
        ],
        "published": "2023-07-26T06:00:50Z",
        "summary": "Advances in large language models (LLMs) have empowered a variety of\napplications. However, there is still a significant gap in research when it\ncomes to understanding and enhancing the capabilities of LLMs in the field of\nmental health. In this work, we present a comprehensive evaluation of multiple\nLLMs on various mental health prediction tasks via online text data, including\nAlpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of\nexperiments, covering zero-shot prompting, few-shot prompting, and instruction\nfine-tuning. The results indicate a promising yet limited performance of LLMs\nwith zero-shot and few-shot prompt designs for mental health tasks. More\nimportantly, our experiments show that instruction finetuning can significantly\nboost the performance of LLMs for all tasks simultaneously. Our best-finetuned\nmodels, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of\nGPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of\nGPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the\nstate-of-the-art task-specific language model. We also conduct an exploratory\ncase study on LLMs' capability on mental health reasoning tasks, illustrating\nthe promising capability of certain models such as GPT-4. We summarize our\nfindings into a set of action guidelines for potential methods to enhance LLMs'\ncapability for mental health tasks. Meanwhile, we also emphasize the important\nlimitations before achieving deployability in real-world mental health\nsettings, such as known racial and gender bias. We highlight the important\nethical risks accompanying this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2307.14385v4.pdf"
    },
    {
        "title": "Is GPT a Computational Model of Emotion? Detailed Analysis",
        "authors": [
            "Ala N. Tak",
            "Jonathan Gratch"
        ],
        "published": "2023-07-25T19:34:44Z",
        "summary": "This paper investigates the emotional reasoning abilities of the GPT family\nof large language models via a component perspective. The paper first examines\nhow the model reasons about autobiographical memories. Second, it\nsystematically varies aspects of situations to impact emotion intensity and\ncoping tendencies. Even without the use of prompt engineering, it is shown that\nGPT's predictions align significantly with human-provided appraisals and\nemotional labels. However, GPT faces difficulties predicting emotion intensity\nand coping responses. GPT-4 showed the highest performance in the initial study\nbut fell short in the second, despite providing superior results after minor\nprompt engineering. This assessment brings up questions on how to effectively\nemploy the strong points and address the weak areas of these models,\nparticularly concerning response variability. These studies underscore the\nmerits of evaluating models from a componential perspective.",
        "pdf_link": "https://arxiv.org/pdf/2307.13779v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Radiology Natural Language Processing",
        "authors": [
            "Zhengliang Liu",
            "Tianyang Zhong",
            "Yiwei Li",
            "Yutong Zhang",
            "Yi Pan",
            "Zihao Zhao",
            "Peixin Dong",
            "Chao Cao",
            "Yuxiao Liu",
            "Peng Shu",
            "Yaonai Wei",
            "Zihao Wu",
            "Chong Ma",
            "Jiaqi Wang",
            "Sheng Wang",
            "Mengyue Zhou",
            "Zuowei Jiang",
            "Chunlin Li",
            "Jason Holmes",
            "Shaochen Xu",
            "Lu Zhang",
            "Haixing Dai",
            "Kai Zhang",
            "Lin Zhao",
            "Yuanhao Chen",
            "Xu Liu",
            "Peilong Wang",
            "Pingkun Yan",
            "Jun Liu",
            "Bao Ge",
            "Lichao Sun",
            "Dajiang Zhu",
            "Xiang Li",
            "Wei Liu",
            "Xiaoyan Cai",
            "Xintao Hu",
            "Xi Jiang",
            "Shu Zhang",
            "Xin Zhang",
            "Tuo Zhang",
            "Shijie Zhao",
            "Quanzheng Li",
            "Hongtu Zhu",
            "Dinggang Shen",
            "Tianming Liu"
        ],
        "published": "2023-07-25T17:57:18Z",
        "summary": "The rise of large language models (LLMs) has marked a pivotal shift in the\nfield of natural language processing (NLP). LLMs have revolutionized a\nmultitude of domains, and they have made a significant impact in the medical\nfield. Large language models are now more abundant than ever, and many of these\nmodels exhibit bilingual capabilities, proficient in both English and Chinese.\nHowever, a comprehensive evaluation of these models remains to be conducted.\nThis lack of assessment is especially apparent within the context of radiology\nNLP. This study seeks to bridge this gap by critically evaluating thirty two\nLLMs in interpreting radiology reports, a crucial component of radiology NLP.\nSpecifically, the ability to derive impressions from radiologic findings is\nassessed. The outcomes of this evaluation provide key insights into the\nperformance, strengths, and weaknesses of these LLMs, informing their practical\napplications within the medical domain.",
        "pdf_link": "https://arxiv.org/pdf/2307.13693v2.pdf"
    },
    {
        "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
        "authors": [
            "Tomohiro Sawada",
            "Daniel Paleka",
            "Alexander Havrilla",
            "Pranav Tadepalli",
            "Paula Vidas",
            "Alexander Kranias",
            "John J. Nay",
            "Kshitij Gupta",
            "Aran Komatsuzaki"
        ],
        "published": "2023-07-25T17:55:19Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores.",
        "pdf_link": "https://arxiv.org/pdf/2307.13692v2.pdf"
    },
    {
        "title": "How Can Large Language Models Help Humans in Design and Manufacturing?",
        "authors": [
            "Liane Makatura",
            "Michael Foshey",
            "Bohan Wang",
            "Felix H√§hnLein",
            "Pingchuan Ma",
            "Bolei Deng",
            "Megan Tjandrasuwita",
            "Andrew Spielberg",
            "Crystal Elaine Owens",
            "Peter Yichen Chen",
            "Allan Zhao",
            "Amy Zhu",
            "Wil J Norton",
            "Edward Gu",
            "Joshua Jacob",
            "Yifei Li",
            "Adriana Schulz",
            "Wojciech Matusik"
        ],
        "published": "2023-07-25T17:30:38Z",
        "summary": "The advancement of Large Language Models (LLMs), including GPT-4, provides\nexciting new opportunities for generative design. We investigate the\napplication of this tool across the entire design and manufacturing workflow.\nSpecifically, we scrutinize the utility of LLMs in tasks such as: converting a\ntext-based prompt into a design specification, transforming a design into\nmanufacturing instructions, producing a design space and design variations,\ncomputing the performance of a design, and searching for designs predicated on\nperformance. Through a series of examples, we highlight both the benefits and\nthe limitations of the current LLMs. By exposing these limitations, we aspire\nto catalyze the continued improvement and progression of these models.",
        "pdf_link": "https://arxiv.org/pdf/2307.14377v1.pdf"
    },
    {
        "title": "GPT-3 Models are Few-Shot Financial Reasoners",
        "authors": [
            "Raul Salles de Padua",
            "Imran Qureshi",
            "Mustafa U. Karakaplan"
        ],
        "published": "2023-07-25T16:21:07Z",
        "summary": "Financial analysis is an important tool for evaluating company performance.\nPractitioners work to answer financial questions to make profitable investment\ndecisions, and use advanced quantitative analyses to do so. As a result,\nFinancial Question Answering (QA) is a question answering task that requires\ndeep reasoning about numbers. Furthermore, it is unknown how well pre-trained\nlanguage models can reason in the financial domain. The current\nstate-of-the-art requires a retriever to collect relevant facts about the\nfinancial question from the text and a generator to produce a valid financial\nprogram and a final answer. However, recently large language models like GPT-3\nhave achieved state-of-the-art performance on wide variety of tasks with just a\nfew shot examples. We run several experiments with GPT-3 and find that a\nseparate retrieval model and logic engine continue to be essential components\nto achieving SOTA performance in this task, particularly due to the precise\nnature of financial questions and the complex information stored in financial\ndocuments. With this understanding, our refined prompt-engineering approach on\nGPT-3 achieves near SOTA accuracy without any fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2307.13617v2.pdf"
    },
    {
        "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
        "authors": [
            "I-Chun Chern",
            "Steffi Chern",
            "Shiqi Chen",
            "Weizhe Yuan",
            "Kehua Feng",
            "Chunting Zhou",
            "Junxian He",
            "Graham Neubig",
            "Pengfei Liu"
        ],
        "published": "2023-07-25T14:20:51Z",
        "summary": "The emergence of generative pre-trained models has facilitated the synthesis\nof high-quality text, but it has also posed challenges in identifying factual\nerrors in the generated text. In particular: (1) A wider range of tasks now\nface an increasing risk of containing factual errors when handled by generative\nmodels. (2) Generated texts tend to be lengthy and lack a clearly defined\ngranularity for individual facts. (3) There is a scarcity of explicit evidence\navailable during the process of fact checking. With the above challenges in\nmind, in this paper, we propose FacTool, a task and domain agnostic framework\nfor detecting factual errors of texts generated by large language models (e.g.,\nChatGPT). Experiments on four different tasks (knowledge-based QA, code\ngeneration, mathematical reasoning, and scientific literature review) show the\nefficacy of the proposed method. We release the code of FacTool associated with\nChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
        "pdf_link": "https://arxiv.org/pdf/2307.13528v2.pdf"
    },
    {
        "title": "Predicting Code Coverage without Execution",
        "authors": [
            "Michele Tufano",
            "Shubham Chandel",
            "Anisha Agarwal",
            "Neel Sundaresan",
            "Colin Clement"
        ],
        "published": "2023-07-25T10:07:02Z",
        "summary": "Code coverage is a widely used metric for quantifying the extent to which\nprogram elements, such as statements or branches, are executed during testing.\nCalculating code coverage is resource-intensive, requiring code building and\nexecution with additional overhead for the instrumentation. Furthermore,\ncomputing coverage of any snippet of code requires the whole program context.\nUsing Machine Learning to amortize this expensive process could lower the cost\nof code coverage by requiring only the source code context, and the task of\ncode coverage prediction can be a novel benchmark for judging the ability of\nmodels to understand code. We propose a novel benchmark task called Code\nCoverage Prediction for Large Language Models (LLMs). We formalize this task to\nevaluate the capability of LLMs in understanding code execution by determining\nwhich lines of a method are executed by a given test case and inputs. We curate\nand release a dataset we call COVERAGEEVAL by executing tests and code from the\nHumanEval dataset and collecting code coverage information. We report the\nperformance of four state-of-the-art LLMs used for code-related tasks,\nincluding OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's\nClaude, on the Code Coverage Prediction task. Finally, we argue that code\ncoverage as a metric and pre-training data source are valuable for overall LLM\nperformance on software engineering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.13383v1.pdf"
    },
    {
        "title": "Aligning Large Language Models with Human: A Survey",
        "authors": [
            "Yufei Wang",
            "Wanjun Zhong",
            "Liangyou Li",
            "Fei Mi",
            "Xingshan Zeng",
            "Wenyong Huang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-07-24T17:44:58Z",
        "summary": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "pdf_link": "https://arxiv.org/pdf/2307.12966v1.pdf"
    },
    {
        "title": "The potential of LLMs for coding with low-resource and domain-specific programming languages",
        "authors": [
            "Artur Tarassow"
        ],
        "published": "2023-07-24T17:17:13Z",
        "summary": "This paper presents a study on the feasibility of using large language models\n(LLM) for coding with low-resource and domain-specific programming languages\nthat typically lack the amount of data required for effective LLM processing\ntechniques. This study focuses on the econometric scripting language named\nhansl of the open-source software gretl and employs a proprietary LLM based on\nGPT-3.5. Our findings suggest that LLMs can be a useful tool for writing,\nunderstanding, improving, and documenting gretl code, which includes generating\ndescriptive docstrings for functions and providing precise explanations for\nabstract and poorly documented econometric code. While the LLM showcased\npromoting docstring-to-code translation capability, we also identify some\nlimitations, such as its inability to improve certain sections of code and to\nwrite accurate unit tests. This study is a step towards leveraging the power of\nLLMs to facilitate software development in low-resource programming languages\nand ultimately to lower barriers to entry for their adoption.",
        "pdf_link": "https://arxiv.org/pdf/2307.13018v1.pdf"
    },
    {
        "title": "Interpretable Stereotype Identification through Reasoning",
        "authors": [
            "Jacob-Junqi Tian",
            "Omkar Dige",
            "David Emerson",
            "Faiza Khan Khattak"
        ],
        "published": "2023-07-24T15:12:13Z",
        "summary": "Given that language models are trained on vast datasets that may contain\ninherent biases, there is a potential danger of inadvertently perpetuating\nsystemic discrimination. Consequently, it becomes essential to examine and\naddress biases in language models, integrating fairness into their development\nto ensure these models are equitable and free from bias. In this work, we\ndemonstrate the importance of reasoning in zero-shot stereotype identification\nbased on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from\n13B to 33B, we show that the performance gain from reasoning significantly\nexceeds the gain from scaling up. Our findings suggest that reasoning could be\na key factor that enables LLMs to trescend the scaling law on out-of-domain\ntasks such as stereotype identification. Additionally, through a qualitative\nanalysis of select reasoning traces, we highlight how reasoning enhances not\njust accuracy but also the interpretability of the decision.",
        "pdf_link": "https://arxiv.org/pdf/2308.00071v2.pdf"
    },
    {
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
        "authors": [
            "Izzeddin Gur",
            "Hiroki Furuta",
            "Austin Huang",
            "Mustafa Safdari",
            "Yutaka Matsuo",
            "Douglas Eck",
            "Aleksandra Faust"
        ],
        "published": "2023-07-24T14:56:30Z",
        "summary": "Pre-trained large language models (LLMs) have recently achieved better\ngeneralization and sample efficiency in autonomous web automation. However, the\nperformance on real-world websites has still suffered from (1) open domainness,\n(2) limited context length, and (3) lack of inductive bias on HTML. We\nintroduce WebAgent, an LLM-driven agent that learns from self-experience to\ncomplete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical\nsub-instructions, summarizes long HTML documents into task-relevant snippets,\nand acts on websites via Python programs generated from those. We design\nWebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new\npre-trained LLMs for long HTML documents using local and global attention\nmechanisms and a mixture of long-span denoising objectives, for planning and\nsummarization. We empirically demonstrate that our modular recipe improves the\nsuccess on real websites by over 50%, and that HTML-T5 is the best model to\nsolve various HTML understanding tasks; achieving 18.7% higher success rate\nthan the prior method on MiniWoB web automation benchmark, and SoTA performance\non Mind2Web, an offline task planning evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2307.12856v4.pdf"
    },
    {
        "title": "Performance of Large Language Models in a Computer Science Degree Program",
        "authors": [
            "Tim Kr√ºger",
            "Michael Gref"
        ],
        "published": "2023-07-24T14:17:00Z",
        "summary": "Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and\ndominate the current discourse. Their transformative capabilities have led to a\nparadigm shift in how we interact with and utilize (text-based) information.\nEach day, new possibilities to leverage the capabilities of these models\nemerge. This paper presents findings on the performance of different large\nlanguage models in a university of applied sciences' undergraduate computer\nscience degree program. Our primary objective is to assess the effectiveness of\nthese models within the curriculum by employing them as educational aids. By\nprompting the models with lecture material, exercise tasks, and past exams, we\naim to evaluate their proficiency across different computer science domains. We\nshowcase the strong performance of current large language models while\nhighlighting limitations and constraints within the context of such a degree\nprogram. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10\ntested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter\nvariant, 20%. Despite these convincing results, even GPT-4.0 would not pass the\ndegree program - due to limitations in mathematical calculations.",
        "pdf_link": "https://arxiv.org/pdf/2308.02432v1.pdf"
    },
    {
        "title": "Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models",
        "authors": [
            "Yuanzhi Liang",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "published": "2023-07-24T07:40:59Z",
        "summary": "Recent advancements in natural language and Large Language Models (LLMs) have\nenabled AI agents to simulate human-like interactions within virtual worlds.\nHowever, these interactions still face limitations in complexity and\nflexibility, particularly in scenarios involving multiple characters and novel\nobjects. Pre-defining all interactable objects in the agent's world model\npresents challenges, and conveying implicit intentions to multiple characters\nthrough complex interactions remains difficult. To address these issues, we\npropose integrating virtual Game Masters (GMs) into the agent's world model,\ndrawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a\ncrucial role in overseeing information, estimating players' intentions,\nproviding environment descriptions, and offering feedback, compensating for\ncurrent world model deficiencies. To facilitate future explorations for complex\ninteractions, we introduce a benchmark named Tachikuma, comprising a Multiple\ncharacter and novel Object based interaction Estimation (MOE) task and a\nsupporting dataset. MOE challenges models to understand characters' intentions\nand accurately determine their actions within intricate contexts involving\nmulti-character and novel object interactions. Besides, the dataset captures\nlog data from real-time communications during gameplay, providing diverse,\ngrounded, and complex interactions for further explorations. Finally, we\npresent a simple prompting baseline and evaluate its performance, demonstrating\nits effectiveness in enhancing interaction understanding. We hope that our\ndataset and task will inspire further research in complex interactions with\nnatural language, fostering the development of more advanced AI agents.",
        "pdf_link": "https://arxiv.org/pdf/2307.12573v1.pdf"
    },
    {
        "title": "The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for Security-Oriented Code Analysis",
        "authors": [
            "Zhilong Wang",
            "Lan Zhang",
            "Chen Cao",
            "Peng Liu"
        ],
        "published": "2023-07-24T02:38:24Z",
        "summary": "Large Language Models (LLMs), such as GPT and BERT, have demonstrated\nremarkable capabilities in addressing neural language process tasks. Recently,\nthe release of ChatGPT has garnered significant attention due to its ability to\nanalyze, comprehend, and synthesize information from user inputs. Therefore,\nthese LLMs were adopted by researchers in many different domains. In the realm\nof code analysis, researchers have applied LLMs to tasks like code review and\ncode generation. However, we observed that the strengths and limitations of\nadopting these LLMs to the code analysis have not been investigated. In this\npaper, we delve into LLMs' capabilities in security-oriented program analysis,\nconsidering perspectives from both attackers and security analysts. We focus on\ntwo representative LLMs, ChatGPT and CodeBert, and evaluate their performance\nin solving typical analytic tasks with varying levels of difficulty. Given the\ndifferent natures of ChatGPT and CodeBERT, we conduct a qualitative analysis of\nthe model's output for ChatGPT and a quantitative analysis for CodeBERT,\nrespectively. For ChatGPT, we present a case study involving several\nsecurity-oriented program analysis tasks while deliberately introducing\nchallenges to assess its responses. On the other hand, for CodeBERT, we\nsystematically analyze and classify the features in code, quantitatively\nevaluating the impact of these features on the model's performance. Our study\ndemonstrates the LLM's efficiency in learning high-level semantics from code,\npositioning ChatGPT as a potential asset in security-oriented contexts.\nHowever, it is essential to acknowledge certain limitations, such as the heavy\nreliance on well-defined variable and function names, making them unable to\nlearn from anonymized code. We hope that our findings and analysis will offer\nvaluable insights for future researchers in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2307.12488v3.pdf"
    },
    {
        "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
        "authors": [
            "Jannik Kossen",
            "Yarin Gal",
            "Tom Rainforth"
        ],
        "published": "2023-07-23T16:54:41Z",
        "summary": "The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.",
        "pdf_link": "https://arxiv.org/pdf/2307.12375v4.pdf"
    },
    {
        "title": "The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD",
        "authors": [
            "Kadhim Hayawi",
            "Sakib Shahriar",
            "Sujith Samuel Mathew"
        ],
        "published": "2023-07-22T21:00:14Z",
        "summary": "The potential of artificial intelligence (AI)-based large language models\n(LLMs) holds considerable promise in revolutionizing education, research, and\npractice. However, distinguishing between human-written and AI-generated text\nhas become a significant task. This paper presents a comparative study,\nintroducing a novel dataset of human-written and LLM-generated texts in\ndifferent genres: essays, stories, poetry, and Python code. We employ several\nmachine learning models to classify the texts. Results demonstrate the efficacy\nof these models in discerning between human and AI-generated text, despite the\ndataset's limited sample size. However, the task becomes more challenging when\nclassifying GPT-generated text, particularly in story writing. The results\nindicate that the models exhibit superior performance in binary classification\ntasks, such as distinguishing human-generated text from a specific LLM,\ncompared to the more complex multiclass tasks that involve discerning among\nhuman-generated and multiple LLMs. Our findings provide insightful implications\nfor AI text detection while our dataset paves the way for future research in\nthis evolving area.",
        "pdf_link": "https://arxiv.org/pdf/2307.12166v2.pdf"
    },
    {
        "title": "FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models",
        "authors": [
            "Yuwei Yin",
            "Yazheng Yang",
            "Jian Yang",
            "Qi Liu"
        ],
        "published": "2023-07-22T09:27:05Z",
        "summary": "Financial risk prediction plays a crucial role in the financial sector.\nMachine learning methods have been widely applied for automatically detecting\npotential risks and thus saving the cost of labor. However, the development in\nthis field is lagging behind in recent years by the following two facts: 1) the\nalgorithms used are somewhat outdated, especially in the context of the fast\nadvance of generative AI and large language models (LLMs); 2) the lack of a\nunified and open-sourced financial benchmark has impeded the related research\nfor years. To tackle these issues, we propose FinPT and FinBench: the former is\na novel approach for financial risk prediction that conduct Profile Tuning on\nlarge pretrained foundation models, and the latter is a set of high-quality\ndatasets on financial risks such as default, fraud, and churn. In FinPT, we\nfill the financial tabular data into the pre-defined instruction template,\nobtain natural-language customer profiles by prompting LLMs, and fine-tune\nlarge foundation models with the profile text to make predictions. We\ndemonstrate the effectiveness of the proposed FinPT by experimenting with a\nrange of representative strong baselines on FinBench. The analytical studies\nfurther deepen the understanding of LLMs for financial risk prediction.",
        "pdf_link": "https://arxiv.org/pdf/2308.00065v1.pdf"
    },
    {
        "title": "Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models",
        "authors": [
            "Tin Lai",
            "Yukun Shi",
            "Zicong Du",
            "Jiajie Wu",
            "Ken Fu",
            "Yichao Dou",
            "Ziqi Wang"
        ],
        "published": "2023-07-22T06:21:41Z",
        "summary": "The demand for psychological counselling has grown significantly in recent\nyears, particularly with the global outbreak of COVID-19, which has heightened\nthe need for timely and professional mental health support. Online\npsychological counselling has emerged as the predominant mode of providing\nservices in response to this demand. In this study, we propose the Psy-LLM\nframework, an AI-based assistive tool leveraging Large Language Models (LLMs)\nfor question-answering in psychological consultation settings to ease the\ndemand for mental health professions. Our framework combines pre-trained LLMs\nwith real-world professional Q\\&A from psychologists and extensively crawled\npsychological articles. The Psy-LLM framework serves as a front-end tool for\nhealthcare professionals, allowing them to provide immediate responses and\nmindfulness activities to alleviate patient stress. Additionally, it functions\nas a screening tool to identify urgent cases requiring further assistance. We\nevaluated the framework using intrinsic metrics, such as perplexity, and\nextrinsic evaluation metrics, with human participant assessments of response\nhelpfulness, fluency, relevance, and logic. The results demonstrate the\neffectiveness of the Psy-LLM framework in generating coherent and relevant\nanswers to psychological questions. This article discusses the potential and\nlimitations of using large language models to enhance mental health support\nthrough AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2307.11991v2.pdf"
    },
    {
        "title": "Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors",
        "authors": [
            "Kolby Nottingham",
            "Yasaman Razeghi",
            "Kyungmin Kim",
            "JB Lanier",
            "Pierre Baldi",
            "Roy Fox",
            "Sameer Singh"
        ],
        "published": "2023-07-21T22:02:50Z",
        "summary": "Large language models (LLMs) are being applied as actors for sequential\ndecision making tasks in domains such as robotics and games, utilizing their\ngeneral world knowledge and planning abilities. However, previous work does\nlittle to explore what environment state information is provided to LLM actors\nvia language. Exhaustively describing high-dimensional states can impair\nperformance and raise inference costs for LLM actors. Previous LLM actors avoid\nthe issue by relying on hand-engineered, task-specific protocols to determine\nwhich features to communicate about a state and which to leave out. In this\nwork, we propose Brief Language INputs for DEcision-making Responses (BLINDER),\na method for automatically selecting concise state descriptions by learning a\nvalue function for task-conditioned state descriptions. We evaluate BLINDER on\nthe challenging video game NetHack and a robotic manipulation task. Our method\nimproves task success rate, reduces input size and compute costs, and\ngeneralizes between LLM actors.",
        "pdf_link": "https://arxiv.org/pdf/2307.11922v1.pdf"
    },
    {
        "title": "The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention",
        "authors": [
            "Navid Ayoobi",
            "Sadat Shahriar",
            "Arjun Mukherjee"
        ],
        "published": "2023-07-21T19:09:24Z",
        "summary": "In this paper, we present a novel method for detecting fake and Large\nLanguage Model (LLM)-generated profiles in the LinkedIn Online Social Network\nimmediately upon registration and before establishing connections. Early fake\nprofile identification is crucial to maintaining the platform's integrity since\nit prevents imposters from acquiring the private and sensitive information of\nlegitimate users and from gaining an opportunity to increase their credibility\nfor future phishing and scamming activities. This work uses textual information\nprovided in LinkedIn profiles and introduces the Section and Subsection Tag\nEmbedding (SSTE) method to enhance the discriminative characteristics of these\ndata for distinguishing between legitimate profiles and those created by\nimposters manually or by using an LLM. Additionally, the dearth of a large\npublicly available LinkedIn dataset motivated us to collect 3600 LinkedIn\nprofiles for our research. We will release our dataset publicly for research\npurposes. This is, to the best of our knowledge, the first large publicly\navailable LinkedIn dataset for fake LinkedIn account detection. Within our\nparadigm, we assess static and contextualized word embeddings, including GloVe,\nFlair, BERT, and RoBERTa. We show that the suggested method can distinguish\nbetween legitimate and fake profiles with an accuracy of about 95% across all\nword embeddings. In addition, we show that SSTE has a promising accuracy for\nidentifying LLM-generated profiles, despite the fact that no LLM-generated\nprofiles were employed during the training phase, and can achieve an accuracy\nof approximately 90% when only 20 LLM-generated profiles are added to the\ntraining set. It is a significant finding since the proliferation of several\nLLMs in the near future makes it extremely challenging to design a single\nsystem that can identify profiles created with various LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.11864v1.pdf"
    },
    {
        "title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples",
        "authors": [
            "Ryuto Koike",
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-07-21T17:40:47Z",
        "summary": "Large Language Models (LLMs) have achieved human-level fluency in text\ngeneration, making it difficult to distinguish between human-written and\nLLM-generated texts. This poses a growing risk of misuse of LLMs and demands\nthe development of detectors to identify LLM-generated texts. However, existing\ndetectors lack robustness against attacks: they degrade detection accuracy by\nsimply paraphrasing LLM-generated texts. Furthermore, a malicious user might\nattempt to deliberately evade the detectors based on detection results, but\nthis has not been assumed in previous studies. In this paper, we propose\nOUTFOX, a framework that improves the robustness of LLM-generated-text\ndetectors by allowing both the detector and the attacker to consider each\nother's output. In this framework, the attacker uses the detector's prediction\nlabels as examples for in-context learning and adversarially generates essays\nthat are harder to detect, while the detector uses the adversarially generated\nessays as examples for in-context learning to learn to detect essays from a\nstrong attacker. Experiments in the domain of student essays show that the\nproposed detector improves the detection performance on the attacker-generated\ntexts by up to +41.3 points F1-score. Furthermore, the proposed detector shows\na state-of-the-art detection performance: up to 96.9 points F1-score, beating\nexisting detectors on non-attacked texts. Finally, the proposed attacker\ndrastically degrades the performance of detectors by up to -57.0 points\nF1-score, massively outperforming the baseline paraphrasing method for evading\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2307.11729v3.pdf"
    },
    {
        "title": "GPT-4 Can't Reason",
        "authors": [
            "Konstantine Arkoudas"
        ],
        "published": "2023-07-21T17:04:25Z",
        "summary": "GPT-4 was released in March 2023 to wide acclaim, marking a very substantial\nimprovement across the board over GPT-3.5 (OpenAI's previously best model,\nwhich had powered the initial release of ChatGPT). However, despite the\ngenuinely impressive improvement, there are good reasons to be highly skeptical\nof GPT-4's ability to reason. This position paper discusses the nature of\nreasoning; criticizes the current formulation of reasoning problems in the NLP\ncommunity, as well as the way in which LLM reasoning performance is currently\nevaluated; introduces a small collection of 21 diverse reasoning problems; and\nperforms a detailed qualitative evaluation of GPT-4's performance on those\nproblems. Based on this analysis, the paper concludes that, despite its\noccasional flashes of analytical brilliance, GPT-4 at present is utterly\nincapable of reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2308.03762v2.pdf"
    },
    {
        "title": "\"Tidy Up the Table\": Grounding Common-sense Objective for Tabletop Object Rearrangement",
        "authors": [
            "Yiqing Xu",
            "David Hsu"
        ],
        "published": "2023-07-21T03:00:31Z",
        "summary": "Tidying up a messy table may appear simple for humans, but articulating clear\ncriteria for tidiness is challenging due to the ambiguous nature of common\nsense reasoning. Large Language Models (LLMs) have proven capable of capturing\ncommon sense knowledge to reason over this vague concept of tidiness. However,\nthey alone may struggle with table tidying due to the limited grasp on the\nspatio-visual aspects of tidiness. In this work, we aim to ground the\ncommon-sense concept of tidiness within the context of object arrangement. Our\nsurvey reveals that humans usually factorize tidiness into semantic and\nvisual-spatial tidiness; our grounding approach aligns with this decomposition.\nWe connect a language-based policy generator with an image-based tidiness score\nfunction: the policy generator utilizes the LLM's commonsense knowledge to\ncluster objects by their implicit types and functionalities for semantic\ntidiness; meanwhile, the tidiness score function assesses the visual-spatial\nrelations of the object to achieve visual-spatial tidiness. Our tidiness score\nis trained using synthetic data generated cheaply from customized random walks,\nwhich inherently encode the order of tidiness, thereby bypassing the need for\nlabor-intensive human demonstrations. The simulated experiment shows that our\napproach successfully generates tidy arrangements, predominately in 2D, with\npotential for 3D stacking, for tables with various novel objects.",
        "pdf_link": "https://arxiv.org/pdf/2307.11319v2.pdf"
    },
    {
        "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
        "authors": [
            "Seonghyeon Ye",
            "Doyoung Kim",
            "Sungdong Kim",
            "Hyeonbin Hwang",
            "Seungone Kim",
            "Yongrae Jo",
            "James Thorne",
            "Juho Kim",
            "Minjoon Seo"
        ],
        "published": "2023-07-20T14:56:35Z",
        "summary": "Evaluation of Large Language Models (LLMs) is challenging because\ninstruction-following necessitates alignment with human values and the required\nset of skills varies depending on the instruction. However, previous studies\nhave mainly focused on coarse-grained evaluation (i.e. overall preference-based\nevaluation), which limits interpretability since it does not consider the\nnature of user instructions that require instance-wise skill composition. In\nthis paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\nAlignment Skill Sets), a fine-grained evaluation protocol for both human-based\nand model-based evaluation which decomposes coarse-level scoring to a skill\nset-level scoring for each instruction. We experimentally observe that the\nfine-graininess of evaluation is crucial for attaining a holistic view of model\nperformance and increasing the reliability of the evaluation. Using FLASK, we\ncompare multiple open-source and proprietary LLMs and observe a high\ncorrelation between model-based and human-based evaluations. We publicly\nrelease the evaluation data and code implementation at\nhttps://github.com/kaistAI/FLASK.",
        "pdf_link": "https://arxiv.org/pdf/2307.10928v3.pdf"
    },
    {
        "title": "LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?",
        "authors": [
            "David Glukhov",
            "Ilia Shumailov",
            "Yarin Gal",
            "Nicolas Papernot",
            "Vardan Papyan"
        ],
        "published": "2023-07-20T09:25:02Z",
        "summary": "Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.",
        "pdf_link": "https://arxiv.org/pdf/2307.10719v1.pdf"
    },
    {
        "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
        "authors": [
            "Xiaoxuan Wang",
            "Ziniu Hu",
            "Pan Lu",
            "Yanqiao Zhu",
            "Jieyu Zhang",
            "Satyen Subramaniam",
            "Arjun R. Loomba",
            "Shichang Zhang",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2023-07-20T07:01:57Z",
        "summary": "Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.",
        "pdf_link": "https://arxiv.org/pdf/2307.10635v2.pdf"
    },
    {
        "title": "What can we learn from Data Leakage and Unlearning for Law?",
        "authors": [
            "Jaydeep Borkar"
        ],
        "published": "2023-07-19T22:14:58Z",
        "summary": "Large Language Models (LLMs) have a privacy concern because they memorize\ntraining data (including personally identifiable information (PII) like emails\nand phone numbers) and leak it during inference. A company can train an LLM on\nits domain-customized data which can potentially also include their users' PII.\nIn order to comply with privacy laws such as the \"right to be forgotten\", the\ndata points of users that are most vulnerable to extraction could be deleted.\nWe find that once the most vulnerable points are deleted, a new set of points\nbecome vulnerable to extraction. So far, little attention has been given to\nunderstanding memorization for fine-tuned models. In this work, we also show\nthat not only do fine-tuned models leak their training data but they also leak\nthe pre-training data (and PII) memorized during the pre-training phase. The\nproperty of new data points becoming vulnerable to extraction after unlearning\nand leakage of pre-training data through fine-tuned models can pose significant\nprivacy and legal concerns for companies that use LLMs to offer services. We\nhope this work will start an interdisciplinary discussion within AI and law\ncommunities regarding the need for policies to tackle these issues.",
        "pdf_link": "https://arxiv.org/pdf/2307.10476v1.pdf"
    },
    {
        "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?",
        "authors": [
            "Omkar Dige",
            "Jacob-Junqi Tian",
            "David Emerson",
            "Faiza Khan Khattak"
        ],
        "published": "2023-07-19T22:03:40Z",
        "summary": "As the breadth and depth of language model applications continue to expand\nrapidly, it is increasingly important to build efficient frameworks for\nmeasuring and mitigating the learned or inherited social biases of these\nmodels. In this paper, we present our work on evaluating instruction fine-tuned\nlanguage models' ability to identify bias through zero-shot prompting,\nincluding Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction\nfine-tuned versions, Alpaca 7B performs best on the bias identification task\nwith an accuracy of 56.7%. We also demonstrate that scaling up LLM size and\ndata diversity could lead to further performance gain. This is a\nwork-in-progress presenting the first component of our bias mitigation\nframework. We will keep updating this work as we get more results.",
        "pdf_link": "https://arxiv.org/pdf/2307.10472v1.pdf"
    },
    {
        "title": "PharmacyGPT: The AI Pharmacist",
        "authors": [
            "Zhengliang Liu",
            "Zihao Wu",
            "Mengxuan Hu",
            "Bokai Zhao",
            "Lin Zhao",
            "Tianyi Zhang",
            "Haixing Dai",
            "Xianyan Chen",
            "Ye Shen",
            "Sheng Li",
            "Brian Murray",
            "Tianming Liu",
            "Andrea Sikora"
        ],
        "published": "2023-07-19T19:40:34Z",
        "summary": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.",
        "pdf_link": "https://arxiv.org/pdf/2307.10432v2.pdf"
    },
    {
        "title": "Code Detection for Hardware Acceleration Using Large Language Models",
        "authors": [
            "Pablo Antonio Mart√≠nez",
            "Gregorio Bernab√©",
            "Jos√© Manuel Garc√≠a"
        ],
        "published": "2023-07-19T17:21:58Z",
        "summary": "Large language models (LLMs) have been massively applied to many tasks, often\nsurpassing state-of-the-art approaches. While their effectiveness in code\ngeneration has been extensively studied (e.g., AlphaCode), their potential for\ncode detection remains unexplored.\n  This work presents the first analysis of code detection using LLMs. Our study\nexamines essential kernels, including matrix multiplication, convolution, and\nfast-fourier transform, implemented in C/C++. We propose both a preliminary,\nnaive prompt and a novel prompting strategy for code detection.\n  Results reveal that conventional prompting achieves great precision but poor\naccuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)\ndue to a high number of false positives. Our novel prompting strategy\nsubstantially reduces false positives, resulting in excellent overall accuracy\n(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable\nchallenge to existing state-of-the-art code detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2307.10348v1.pdf"
    },
    {
        "title": "Generating Mathematical Derivations with Large Language Models",
        "authors": [
            "Jordan Meadows",
            "Marco Valentino",
            "Andre Freitas"
        ],
        "published": "2023-07-19T14:13:02Z",
        "summary": "The derivation of mathematical results in specialised fields, using Large\nLanguage Models (LLMs), is an emerging research direction that can help\nidentify models' limitations, and potentially support mathematical discovery.\nIn this paper, we leverage a symbolic engine to generate derivations of\nequations at scale, and investigate the capabilities of LLMs when deriving goal\nequations from premises. Specifically, we employ in-context learning for GPT\nand fine-tune a range of T5 models to compare the robustness and generalisation\nof pre-training strategies to specialised models. Empirical results show that\nfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and\nout-of-distribution test sets in conventional scores. However, an in-depth\nanalysis reveals that the fine-tuned models are more sensitive to perturbations\ninvolving unseen symbols and (to a lesser extent) changes to equation\nstructure. In addition, we analyse 1.7K equations, and over 200 derivations, to\nhighlight common reasoning errors such as the inclusion of incorrect,\nirrelevant, and redundant equations. Finally, we explore the suitability of\nexisting metrics for evaluating mathematical derivations and find evidence\nthat, while they can capture general properties such as sensitivity to\nperturbations, they fail to highlight fine-grained reasoning errors and\nessential differences between models. Overall, this work demonstrates that\ntraining models on synthetic data may improve their math capabilities beyond\nmuch larger LLMs, but current metrics are not appropriately assessing the\nquality of generated mathematical text.",
        "pdf_link": "https://arxiv.org/pdf/2307.09998v3.pdf"
    },
    {
        "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats",
        "authors": [
            "Xiaoxia Wu",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "published": "2023-07-19T06:58:03Z",
        "summary": "In the complex domain of large language models (LLMs), striking a balance\nbetween computational efficiency and maintaining model quality is a formidable\nchallenge. Navigating the inherent limitations of uniform quantization,\nparticularly when dealing with outliers, and motivated by the launch of\nNVIDIA's H100 hardware, this study delves into the viability of floating-point\n(FP) quantization, particularly focusing on FP8 and FP4, as a potential\nsolution. Our comprehensive investigation reveals that for LLMs, FP8 activation\nconsistently outshines its integer (INT8) equivalent, with the performance edge\nbecoming more noticeable in models possessing parameters beyond one billion.\nFor weight quantization, our findings indicate that FP4 exhibits comparable, if\nnot superior, performance to INT4, simplifying deployment on FP-supported\nhardware like H100. To mitigate the overhead from precision alignment caused by\nthe disparity between weights and activations, we propose two scaling\nconstraints for weight quantization that negligibly impact the performance\ncompared to the standard W4A8 model. We additionally enhance our quantization\nmethods by integrating the Low Rank Compensation (LoRC) strategy, yielding\nimprovements especially in smaller models. The results of our investigation\nemphasize the immense potential of FP quantization for LLMs, paving the way for\nhigh-efficiency deployment in resource-limited settings.",
        "pdf_link": "https://arxiv.org/pdf/2307.09782v2.pdf"
    },
    {
        "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility",
        "authors": [
            "Guohai Xu",
            "Jiayi Liu",
            "Ming Yan",
            "Haotian Xu",
            "Jinghui Si",
            "Zhuoran Zhou",
            "Peng Yi",
            "Xing Gao",
            "Jitao Sang",
            "Rong Zhang",
            "Ji Zhang",
            "Chao Peng",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2023-07-19T01:22:40Z",
        "summary": "With the rapid evolution of large language models (LLMs), there is a growing\nconcern that they may pose risks or have negative social impacts. Therefore,\nevaluation of human values alignment is becoming increasingly important.\nPrevious work mainly focuses on assessing the performance of LLMs on certain\nknowledge and reasoning abilities, while neglecting the alignment to human\nvalues, especially in a Chinese context. In this paper, we present CValues, the\nfirst Chinese human values evaluation benchmark to measure the alignment\nability of LLMs in terms of both safety and responsibility criteria. As a\nresult, we have manually collected adversarial safety prompts across 10\nscenarios and induced responsibility prompts from 8 domains by professional\nexperts. To provide a comprehensive values evaluation of Chinese LLMs, we not\nonly conduct human evaluation for reliable comparison, but also construct\nmulti-choice prompts for automatic evaluation. Our findings suggest that while\nmost Chinese LLMs perform well in terms of safety, there is considerable room\nfor improvement in terms of responsibility. Moreover, both the automatic and\nhuman evaluation are important for assessing the human values alignment in\ndifferent aspects. The benchmark and code is available on ModelScope and\nGithub.",
        "pdf_link": "https://arxiv.org/pdf/2307.09705v1.pdf"
    },
    {
        "title": "Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study",
        "authors": [
            "Damith Premasiri",
            "Tharindu Ranasinghe",
            "Ruslan Mitkov"
        ],
        "published": "2023-07-18T18:21:26Z",
        "summary": "Text classification is an area of research which has been studied over the\nyears in Natural Language Processing (NLP). Adapting NLP to multiple domains\nhas introduced many new challenges for text classification and one of them is\nlong document classification. While state-of-the-art transformer models provide\nexcellent results in text classification, most of them have limitations in the\nmaximum sequence length of the input sequence. The majority of the transformer\nmodels are limited to 512 tokens, and therefore, they struggle with long\ndocument classification problems. In this research, we explore on employing\nModel Fusing for long document classification while comparing the results with\nwell-known BERT and Longformer architectures.",
        "pdf_link": "https://arxiv.org/pdf/2307.09532v1.pdf"
    },
    {
        "title": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications",
        "authors": [
            "Vishesh Thakur"
        ],
        "published": "2023-07-18T11:38:45Z",
        "summary": "Gender bias in artificial intelligence (AI) and natural language processing\nhas garnered significant attention due to its potential impact on societal\nperceptions and biases. This research paper aims to analyze gender bias in\nLarge Language Models (LLMs) with a focus on multiple comparisons between GPT-2\nand GPT-3.5, some prominent language models, to better understand its\nimplications. Through a comprehensive literature review, the study examines\nexisting research on gender bias in AI language models and identifies gaps in\nthe current knowledge. The methodology involves collecting and preprocessing\ndata from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis\ntechniques to evaluate gender bias in the generated text. The findings shed\nlight on gendered word associations, language usage, and biased narratives\npresent in the outputs of these Large Language Models. The discussion explores\nthe ethical implications of gender bias and its potential consequences on\nsocial perceptions and marginalized communities. Additionally, the paper\npresents strategies for reducing gender bias in LLMs, including algorithmic\napproaches and data augmentation techniques. The research highlights the\nimportance of interdisciplinary collaborations and the role of sociological\nstudies in mitigating gender bias in AI models. By addressing these issues, we\ncan pave the way for more inclusive and unbiased AI systems that have a\npositive impact on society.",
        "pdf_link": "https://arxiv.org/pdf/2307.09162v3.pdf"
    },
    {
        "title": "On the (In)Effectiveness of Large Language Models for Chinese Text Correction",
        "authors": [
            "Yinghui Li",
            "Haojing Huang",
            "Shirong Ma",
            "Yong Jiang",
            "Yangning Li",
            "Feng Zhou",
            "Hai-Tao Zheng",
            "Qingyu Zhou"
        ],
        "published": "2023-07-18T06:48:52Z",
        "summary": "Recently, the development and progress of Large Language Models (LLMs) have\namazed the entire Artificial Intelligence community. Benefiting from their\nemergent abilities, LLMs have attracted more and more researchers to study\ntheir capabilities and performance on various downstream Natural Language\nProcessing (NLP) tasks. While marveling at LLMs' incredible performance on all\nkinds of tasks, we notice that they also have excellent multilingual processing\ncapabilities, such as Chinese. To explore the Chinese processing ability of\nLLMs, we focus on Chinese Text Correction, a fundamental and challenging\nChinese NLP task. Specifically, we evaluate various representative LLMs on the\nChinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC)\ntasks, which are two main Chinese Text Correction scenarios. Additionally, we\nalso fine-tune LLMs for Chinese Text Correction to better observe the potential\ncapabilities of LLMs. From extensive analyses and comparisons with previous\nstate-of-the-art small models, we empirically find that the LLMs currently have\nboth amazing performance and unsatisfactory behavior for Chinese Text\nCorrection. We believe our findings will promote the landing and application of\nLLMs in the Chinese NLP community.",
        "pdf_link": "https://arxiv.org/pdf/2307.09007v2.pdf"
    },
    {
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
        "authors": [
            "Yanda Chen",
            "Ruiqi Zhong",
            "Narutatsu Ri",
            "Chen Zhao",
            "He He",
            "Jacob Steinhardt",
            "Zhou Yu",
            "Kathleen McKeown"
        ],
        "published": "2023-07-17T17:41:47Z",
        "summary": "Large language models (LLMs) are trained to imitate humans to explain human\ndecisions. However, do LLMs explain themselves? Can they help humans build\nmental models of how LLMs process different inputs? To answer these questions,\nwe propose to evaluate $\\textbf{counterfactual simulatability}$ of natural\nlanguage explanations: whether an explanation can enable humans to precisely\ninfer the model's outputs on diverse counterfactuals of the explained input.\nFor example, if a model answers \"yes\" to the input question \"Can eagles fly?\"\nwith the explanation \"all birds can fly\", then humans would infer from the\nexplanation that it would also answer \"yes\" to the counterfactual input \"Can\npenguins fly?\". If the explanation is precise, then the model's answer should\nmatch humans' expectations.\n  We implemented two metrics based on counterfactual simulatability: precision\nand generality. We generated diverse counterfactuals automatically using LLMs.\nWe then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on\ntwo tasks: multi-hop factual reasoning and reward modeling. We found that LLM's\nexplanations have low precision and that precision does not correlate with\nplausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may\nnot be a sufficient solution.",
        "pdf_link": "https://arxiv.org/pdf/2307.08678v1.pdf"
    },
    {
        "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",
        "authors": [
            "Liangyu Zha",
            "Junlin Zhou",
            "Liyao Li",
            "Rui Wang",
            "Qingyi Huang",
            "Saisai Yang",
            "Jing Yuan",
            "Changbao Su",
            "Xiang Li",
            "Aofeng Su",
            "Tao Zhang",
            "Chen Zhou",
            "Kaizhe Shou",
            "Miao Wang",
            "Wufang Zhu",
            "Guoshan Lu",
            "Chao Ye",
            "Yali Ye",
            "Wentao Ye",
            "Yiming Zhang",
            "Xinglong Deng",
            "Jie Xu",
            "Haobo Wang",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "published": "2023-07-17T17:36:09Z",
        "summary": "Tables are prevalent in real-world databases, requiring significant time and\neffort for humans to analyze and manipulate. The advancements in large language\nmodels (LLMs) have made it possible to interact with tables using natural\nlanguage input, bringing this capability closer to reality. In this paper, we\npresent TableGPT, a unified fine-tuned framework that enables LLMs to\nunderstand and operate on tables using external functional commands. It\nintroduces the capability to seamlessly interact with tables, enabling a wide\nrange of functionalities such as question answering, data manipulation (e.g.,\ninsert, delete, query, and modify operations), data visualization, analysis\nreport generation, and automated prediction. TableGPT aims to provide\nconvenience and accessibility to users by empowering them to effortlessly\nleverage tabular data. At the core of TableGPT lies the novel concept of global\ntabular representations, which empowers LLMs to gain a comprehensive\nunderstanding of the entire table beyond meta-information. By jointly training\nLLMs on both table and text modalities, TableGPT achieves a deep understanding\nof tabular data and the ability to perform complex operations on tables through\nchain-of-command instructions. Importantly, TableGPT offers the advantage of\nbeing a self-contained system rather than relying on external API interfaces.\nMoreover, it supports efficient data process flow, query rejection (when\nappropriate) and private deployment, enabling faster domain data fine-tuning\nand ensuring data privacy, which enhances the framework's adaptability to\nspecific use cases.",
        "pdf_link": "https://arxiv.org/pdf/2307.08674v3.pdf"
    },
    {
        "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
        "authors": [
            "Tamera Lanham",
            "Anna Chen",
            "Ansh Radhakrishnan",
            "Benoit Steiner",
            "Carson Denison",
            "Danny Hernandez",
            "Dustin Li",
            "Esin Durmus",
            "Evan Hubinger",
            "Jackson Kernion",
            "Kamilƒó Luko≈°i≈´tƒó",
            "Karina Nguyen",
            "Newton Cheng",
            "Nicholas Joseph",
            "Nicholas Schiefer",
            "Oliver Rausch",
            "Robin Larson",
            "Sam McCandlish",
            "Sandipan Kundu",
            "Saurav Kadavath",
            "Shannon Yang",
            "Thomas Henighan",
            "Timothy Maxwell",
            "Timothy Telleen-Lawton",
            "Tristan Hume",
            "Zac Hatfield-Dodds",
            "Jared Kaplan",
            "Jan Brauner",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-07-17T01:08:39Z",
        "summary": "Large language models (LLMs) perform better when they produce step-by-step,\n\"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is\nunclear if the stated reasoning is a faithful explanation of the model's actual\nreasoning (i.e., its process for answering the question). We investigate\nhypotheses for how CoT reasoning may be unfaithful, by examining how the model\npredictions change when we intervene on the CoT (e.g., by adding mistakes or\nparaphrasing it). Models show large variation across tasks in how strongly they\ncondition on the CoT when predicting their answer, sometimes relying heavily on\nthe CoT and other times primarily ignoring it. CoT's performance boost does not\nseem to come from CoT's added test-time compute alone or from information\nencoded via the particular phrasing of the CoT. As models become larger and\nmore capable, they produce less faithful reasoning on most tasks we study.\nOverall, our results suggest that CoT can be faithful if the circumstances such\nas the model size and task are carefully chosen.",
        "pdf_link": "https://arxiv.org/pdf/2307.13702v1.pdf"
    },
    {
        "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
        "authors": [
            "Ansh Radhakrishnan",
            "Karina Nguyen",
            "Anna Chen",
            "Carol Chen",
            "Carson Denison",
            "Danny Hernandez",
            "Esin Durmus",
            "Evan Hubinger",
            "Jackson Kernion",
            "Kamilƒó Luko≈°i≈´tƒó",
            "Newton Cheng",
            "Nicholas Joseph",
            "Nicholas Schiefer",
            "Oliver Rausch",
            "Sam McCandlish",
            "Sheer El Showk",
            "Tamera Lanham",
            "Tim Maxwell",
            "Venkatesa Chandrasekaran",
            "Zac Hatfield-Dodds",
            "Jared Kaplan",
            "Jan Brauner",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-07-17T00:54:10Z",
        "summary": "As large language models (LLMs) perform more difficult tasks, it becomes\nharder to verify the correctness and safety of their behavior. One approach to\nhelp with this issue is to prompt LLMs to externalize their reasoning, e.g., by\nhaving them generate step-by-step reasoning as they answer a question\n(Chain-of-Thought; CoT). The reasoning may enable us to check the process that\nmodels use to perform tasks. However, this approach relies on the stated\nreasoning faithfully reflecting the model's actual reasoning, which is not\nalways the case. To improve over the faithfulness of CoT reasoning, we have\nmodels generate reasoning by decomposing questions into subquestions.\nDecomposition-based methods achieve strong performance on question-answering\ntasks, sometimes approaching that of CoT while improving the faithfulness of\nthe model's stated reasoning on several recently-proposed metrics. By forcing\nthe model to answer simpler subquestions in separate contexts, we greatly\nincrease the faithfulness of model-generated reasoning over CoT, while still\nachieving some of the performance gains of CoT. Our results show it is possible\nto improve the faithfulness of model-generated reasoning; continued\nimprovements may lead to reasoning that enables us to verify the correctness\nand safety of LLM behavior.",
        "pdf_link": "https://arxiv.org/pdf/2307.11768v2.pdf"
    },
    {
        "title": "The Potential and Pitfalls of using a Large Language Model such as ChatGPT or GPT-4 as a Clinical Assistant",
        "authors": [
            "Jingqing Zhang",
            "Kai Sun",
            "Akshay Jagadeesh",
            "Mahta Ghahfarokhi",
            "Deepa Gupta",
            "Ashok Gupta",
            "Vibhor Gupta",
            "Yike Guo"
        ],
        "published": "2023-07-16T21:19:47Z",
        "summary": "Recent studies have demonstrated promising performance of ChatGPT and GPT-4\non several medical domain tasks. However, none have assessed its performance\nusing a large-scale real-world electronic health record database, nor have\nevaluated its utility in providing clinical diagnostic assistance for patients\nacross a full range of disease presentation. We performed two analyses using\nChatGPT and GPT-4, one to identify patients with specific medical diagnoses\nusing a real-world large electronic health record database and the other, in\nproviding diagnostic assistance to healthcare workers in the prospective\nevaluation of hypothetical patients. Our results show that GPT-4 across disease\nclassification tasks with chain of thought and few-shot prompting can achieve\nperformance as high as 96% F1 scores. For patient assessment, GPT-4 can\naccurately diagnose three out of four times. However, there were mentions of\nfactually incorrect statements, overlooking crucial medical findings,\nrecommendations for unnecessary investigations and overtreatment. These issues\ncoupled with privacy concerns, make these models currently inadequate for real\nworld clinical use. However, limited data and time needed for prompt\nengineering in comparison to configuration of conventional machine learning\nworkflows highlight their potential for scalability across healthcare\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2307.08152v1.pdf"
    },
    {
        "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
        "authors": [
            "Peiyu Liu",
            "Zikang Liu",
            "Ze-Feng Gao",
            "Dawei Gao",
            "Wayne Xin Zhao",
            "Yaliang Li",
            "Bolin Ding",
            "Ji-Rong Wen"
        ],
        "published": "2023-07-16T15:11:01Z",
        "summary": "Despite the superior performance, Large Language Models~(LLMs) require\nsignificant computational resources for deployment and use. To overcome this\nissue, quantization methods have been widely applied to reduce the memory\nfootprint of LLMs as well as increasing the inference rate. However, a major\nchallenge is that low-bit quantization methods often lead to performance\ndegradation. It is important to understand how quantization impacts the\ncapacity of LLMs. Different from previous studies focused on overall\nperformance, this work aims to investigate the impact of quantization on\n\\emph{emergent abilities}, which are important characteristics that distinguish\nLLMs from small language models. Specially, we examine the abilities of\nin-context learning, chain-of-thought reasoning, and instruction-following in\nquantized LLMs. Our empirical experiments show that these emergent abilities\nstill exist in 4-bit quantization models, while 2-bit models encounter severe\nperformance degradation on the test of these abilities. To improve the\nperformance of low-bit models, we conduct two special experiments: (1)\nfine-gained impact analysis that studies which components (or substructures)\nare more sensitive to quantization, and (2) performance compensation through\nmodel fine-tuning. Our work derives a series of important findings to\nunderstand the impact of quantization on emergent abilities, and sheds lights\non the possibilities of extremely low-bit quantization for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.08072v2.pdf"
    },
    {
        "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
        "authors": [
            "Yuheng Huang",
            "Jiayang Song",
            "Zhijie Wang",
            "Shengming Zhao",
            "Huaming Chen",
            "Felix Juefei-Xu",
            "Lei Ma"
        ],
        "published": "2023-07-16T08:28:04Z",
        "summary": "The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.10236v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models to Generate Answer Set Programs",
        "authors": [
            "Adam Ishay",
            "Zhun Yang",
            "Joohyung Lee"
        ],
        "published": "2023-07-15T03:40:55Z",
        "summary": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated\nexceptional performance in various natural language processing tasks and have\nshown the ability to solve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the application of\nvarious prompting techniques. In contrast, formal logic is adept at handling\ncomplex reasoning, but translating natural language descriptions into formal\nlogic is a challenging task that non-experts struggle with. This paper proposes\na neuro-symbolic method that combines the strengths of large language models\nand answer set programming. Specifically, we employ an LLM to transform natural\nlanguage descriptions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language descriptions into answer\nset programs in a step by step manner. Surprisingly, with just a few in-context\nlearning examples, LLMs can generate reasonably complex answer set programs.\nThe majority of errors made are relatively simple and can be easily corrected\nby humans, thus enabling LLMs to effectively assist in the creation of answer\nset programs.",
        "pdf_link": "https://arxiv.org/pdf/2307.07699v1.pdf"
    },
    {
        "title": "Can Large Language Models Empower Molecular Property Prediction?",
        "authors": [
            "Chen Qian",
            "Huayi Tang",
            "Zhirui Yang",
            "Hong Liang",
            "Yong Liu"
        ],
        "published": "2023-07-14T16:06:42Z",
        "summary": "Molecular property prediction has gained significant attention due to its\ntransformative potential in multiple scientific disciplines. Conventionally, a\nmolecule graph can be represented either as a graph-structured data or a SMILES\ntext. Recently, the rapid development of Large Language Models (LLMs) has\nrevolutionized the field of NLP. Although it is natural to utilize LLMs to\nassist in understanding molecules represented by SMILES, the exploration of how\nLLMs will impact molecular property prediction is still in its early stage. In\nthis work, we advance towards this objective through two perspectives:\nzero/few-shot molecular classification, and using the new explanations\ngenerated by LLMs as representations of molecules. To be specific, we first\nprompt LLMs to do in-context molecular classification and evaluate their\nperformance. After that, we employ LLMs to generate semantically enriched\nexplanations for the original SMILES and then leverage that to fine-tune a\nsmall-scale LM model for multiple downstream tasks. The experimental results\nhighlight the superiority of text explanations as molecular representations\nacross multiple benchmark datasets, and confirm the immense potential of LLMs\nin molecular property prediction tasks. Codes are available at\n\\url{https://github.com/ChnQ/LLM4Mol}.",
        "pdf_link": "https://arxiv.org/pdf/2307.07443v1.pdf"
    },
    {
        "title": "Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow",
        "authors": [
            "Maria del Rio-Chanona",
            "Nadzeya Laurentsyeva",
            "Johannes Wachs"
        ],
        "published": "2023-07-14T14:22:12Z",
        "summary": "Large language models like ChatGPT efficiently provide users with information\nabout various topics, presenting a potential substitute for searching the web\nand asking people for help online. But since users interact privately with the\nmodel, these models may drastically reduce the amount of publicly available\nhuman-generated data and knowledge resources. This substitution can present a\nsignificant problem in securing training data for future models. In this work,\nwe investigate how the release of ChatGPT changed human-generated open data on\nthe web by analyzing the activity on Stack Overflow, the leading online Q\\&A\nplatform for computer programming. We find that relative to its Russian and\nChinese counterparts, where access to ChatGPT is limited, and to similar forums\nfor mathematics, where ChatGPT is less capable, activity on Stack Overflow\nsignificantly decreased. A difference-in-differences model estimates a 16\\%\ndecrease in weekly posts on Stack Overflow. This effect increases in magnitude\nover time, and is larger for posts related to the most widely used programming\nlanguages. Posts made after ChatGPT get similar voting scores than before,\nsuggesting that ChatGPT is not merely displacing duplicate or low-quality\ncontent. These results suggest that more users are adopting large language\nmodels to answer questions and they are better substitutes for Stack Overflow\nfor languages for which they have more training data. Using models like ChatGPT\nmay be more efficient for solving certain programming problems, but its\nwidespread adoption and the resulting shift away from public exchange on the\nweb will limit the open data people and models can learn from in the future.",
        "pdf_link": "https://arxiv.org/pdf/2307.07367v1.pdf"
    },
    {
        "title": "Certified Robustness for Large Language Models with Self-Denoising",
        "authors": [
            "Zhen Zhang",
            "Guanhua Zhang",
            "Bairu Hou",
            "Wenqi Fan",
            "Qing Li",
            "Sijia Liu",
            "Yang Zhang",
            "Shiyu Chang"
        ],
        "published": "2023-07-14T05:40:24Z",
        "summary": "Although large language models (LLMs) have achieved great success in vast\nreal-world applications, their vulnerabilities towards noisy inputs have\nsignificantly limited their uses, especially in high-stake environments. In\nthese contexts, it is crucial to ensure that every prediction made by large\nlanguage models is stable, i.e., LLM predictions should be consistent given\nminor differences in the input. This largely falls into the study of certified\nrobust LLMs, i.e., all predictions of LLM are certified to be correct in a\nlocal region around the input. Randomized smoothing has demonstrated great\npotential in certifying the robustness and prediction stability of LLMs.\nHowever, randomized smoothing requires adding noise to the input before model\nprediction, and its certification performance depends largely on the model's\nperformance on corrupted data. As a result, its direct application to LLMs\nremains challenging and often results in a small certification radius. To\naddress this issue, we take advantage of the multitasking nature of LLMs and\npropose to denoise the corrupted inputs with LLMs in a self-denoising manner.\nDifferent from previous works like denoised smoothing, which requires training\na separate model to robustify LLM, our method enjoys far better efficiency and\nflexibility. Our experiment results show that our method outperforms the\nexisting certification methods under both certified robustness and empirical\nrobustness. The codes are available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise.",
        "pdf_link": "https://arxiv.org/pdf/2307.07171v1.pdf"
    },
    {
        "title": "Generating Efficient Training Data via LLM-based Attribute Manipulation",
        "authors": [
            "Letian Peng",
            "Yuwei Zhang",
            "Jingbo Shang"
        ],
        "published": "2023-07-14T00:10:03Z",
        "summary": "In this paper, we propose a novel method, Chain-of-Thoughts Attribute\nManipulation (CoTAM), to guide few-shot learning by carefully crafted data from\nLarge Language Models (LLMs). The main idea is to create data with changes only\nin the attribute targeted by the task. Inspired by facial attribute\nmanipulation, our approach generates label-switched data by leveraging LLMs to\nmanipulate task-specific attributes and reconstruct new sentences in a\ncontrolled manner. Instead of conventional latent representation controlling,\nwe implement chain-of-thoughts decomposition and reconstruction to adapt the\nprocedure to LLMs. Extensive results on text classification and other tasks\nverify the advantage of CoTAM over other LLM-based text generation methods with\nthe same number of training examples. Analysis visualizes the attribute\nmanipulation effectiveness of CoTAM and presents the potential of LLM-guided\nlearning with even less supervision.",
        "pdf_link": "https://arxiv.org/pdf/2307.07099v1.pdf"
    },
    {
        "title": "A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023",
        "authors": [
            "Yi Cheng",
            "Ziwei Xu",
            "Fen Fang",
            "Dongyun Lin",
            "Hehe Fan",
            "Yongkang Wong",
            "Ying Sun",
            "Mohan Kankanhalli"
        ],
        "published": "2023-07-13T05:54:05Z",
        "summary": "In this technical report, we present our findings from a study conducted on\nthe EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action\nRecognition. Our research focuses on the innovative application of a\ndifferentiable logic loss in the training to leverage the co-occurrence\nrelations between verb and noun, as well as the pre-trained Large Language\nModels (LLMs) to generate the logic rules for the adaptation to unseen action\nlabels. Specifically, the model's predictions are treated as the truth\nassignment of a co-occurrence logic formula to compute the logic loss, which\nmeasures the consistency between the predictions and the logic constraints. By\nusing the verb-noun co-occurrence matrix generated from the dataset, we observe\na moderate improvement in model performance compared to our baseline framework.\nTo further enhance the model's adaptability to novel action labels, we\nexperiment with rules generated using GPT-3.5, which leads to a slight decrease\nin performance. These findings shed light on the potential and challenges of\nincorporating differentiable logic and LLMs for knowledge extraction in\nunsupervised domain adaptation for action recognition. Our final submission\n(entitled `NS-LLM') achieved the first place in terms of top-1 action\nrecognition accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2307.06569v1.pdf"
    },
    {
        "title": "Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study",
        "authors": [
            "Zeping Min",
            "Jinbo Wang"
        ],
        "published": "2023-07-13T02:31:55Z",
        "summary": "This paper explores the integration of Large Language Models (LLMs) into\nAutomatic Speech Recognition (ASR) systems to improve transcription accuracy.\nThe increasing sophistication of LLMs, with their in-context learning\ncapabilities and instruction-following behavior, has drawn significant\nattention in the field of Natural Language Processing (NLP). Our primary focus\nis to investigate the potential of using an LLM's in-context learning\ncapabilities to enhance the performance of ASR systems, which currently face\nchallenges such as ambient noise, speaker accents, and complex linguistic\ncontexts. We designed a study using the Aishell-1 and LibriSpeech datasets,\nwith ChatGPT and GPT-4 serving as benchmarks for LLM capabilities.\nUnfortunately, our initial experiments did not yield promising results,\nindicating the complexity of leveraging LLM's in-context learning for ASR\napplications. Despite further exploration with varied settings and models, the\ncorrected sentences from the LLMs frequently resulted in higher Word Error\nRates (WER), demonstrating the limitations of LLMs in speech applications. This\npaper provides a detailed overview of these experiments, their results, and\nimplications, establishing that using LLMs' in-context learning capabilities to\ncorrect potential errors in speech recognition transcriptions is still a\nchallenging task at the current stage.",
        "pdf_link": "https://arxiv.org/pdf/2307.06530v1.pdf"
    },
    {
        "title": "AutoHint: Automatic Prompt Optimization with Hint Generation",
        "authors": [
            "Hong Sun",
            "Xue Li",
            "Yinchuan Xu",
            "Youkow Homma",
            "Qi Cao",
            "Min Wu",
            "Jian Jiao",
            "Denis Charles"
        ],
        "published": "2023-07-13T00:49:27Z",
        "summary": "This paper presents AutoHint, a novel framework for automatic prompt\nengineering and optimization for Large Language Models (LLM). While LLMs have\ndemonstrated remarkable ability in achieving high-quality annotation in various\ntasks, the key to applying this ability to specific tasks lies in developing\nhigh-quality prompts. Thus we propose a framework to inherit the merits of both\nin-context learning and zero-shot learning by incorporating enriched\ninstructions derived from input-output demonstrations to optimize original\nprompt. We refer to the enrichment as the hint and propose a framework to\nautomatically generate the hint from labeled data. More concretely, starting\nfrom an initial prompt, our method first instructs a LLM to deduce new hints\nfor selected samples from incorrect predictions, and then summarizes from\nper-sample hints and adds the results back to the initial prompt to form a new,\nenriched instruction. The proposed method is evaluated on the BIG-Bench\nInstruction Induction dataset for both zero-shot and few-short prompts, where\nexperiments demonstrate our method is able to significantly boost accuracy for\nmultiple tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.07415v2.pdf"
    },
    {
        "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
        "authors": [
            "Kaiyi Huang",
            "Kaiyue Sun",
            "Enze Xie",
            "Zhenguo Li",
            "Xihui Liu"
        ],
        "published": "2023-07-12T17:59:42Z",
        "summary": "Despite the stunning ability to generate high-quality images by recent\ntext-to-image models, current approaches often struggle to effectively compose\nobjects with different attributes and relationships into a complex and coherent\nscene. We propose T2I-CompBench, a comprehensive benchmark for open-world\ncompositional text-to-image generation, consisting of 6,000 compositional text\nprompts from 3 categories (attribute binding, object relationships, and complex\ncompositions) and 6 sub-categories (color binding, shape binding, texture\nbinding, spatial relationships, non-spatial relationships, and complex\ncompositions). We further propose several evaluation metrics specifically\ndesigned to evaluate compositional text-to-image generation and explore the\npotential and limitations of multimodal LLMs for evaluation. We introduce a new\napproach, Generative mOdel fine-tuning with Reward-driven Sample selection\n(GORS), to boost the compositional text-to-image generation abilities of\npretrained text-to-image models. Extensive experiments and evaluations are\nconducted to benchmark previous methods on T2I-CompBench, and to validate the\neffectiveness of our proposed evaluation metrics and GORS approach. Project\npage is available at https://karine-h.github.io/T2I-CompBench/.",
        "pdf_link": "https://arxiv.org/pdf/2307.06350v2.pdf"
    },
    {
        "title": "Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models",
        "authors": [
            "Dhruv Mullick",
            "Bilal Ghanem",
            "Alona Fyshe"
        ],
        "published": "2023-07-11T12:43:28Z",
        "summary": "Customer feedback is invaluable to companies as they refine their products.\nMonitoring customer feedback can be automated with Aspect Level Sentiment\nClassification (ALSC) which allows us to analyse specific aspects of the\nproducts in reviews. Large Language Models (LLMs) are the heart of many\nstate-of-the-art ALSC solutions, but they perform poorly in some scenarios\nrequiring Coreference Resolution (CR). In this work, we propose a framework to\nimprove an LLM's performance on CR-containing reviews by fine tuning on highly\ninferential tasks. We show that the performance improvement is likely\nattributed to the improved model CR ability. We also release a new dataset that\nfocuses on CR in ALSC.",
        "pdf_link": "https://arxiv.org/pdf/2307.05646v1.pdf"
    },
    {
        "title": "Secrets of RLHF in Large Language Models Part I: PPO",
        "authors": [
            "Rui Zheng",
            "Shihan Dou",
            "Songyang Gao",
            "Yuan Hua",
            "Wei Shen",
            "Binghai Wang",
            "Yan Liu",
            "Senjie Jin",
            "Qin Liu",
            "Yuhao Zhou",
            "Limao Xiong",
            "Lu Chen",
            "Zhiheng Xi",
            "Nuo Xu",
            "Wenbin Lai",
            "Minghao Zhu",
            "Cheng Chang",
            "Zhangyue Yin",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Haoran Huang",
            "Tianxiang Sun",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023-07-11T01:55:24Z",
        "summary": "Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.04964v2.pdf"
    },
    {
        "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
        "authors": [
            "Shaokai Ye",
            "Jessy Lauer",
            "Mu Zhou",
            "Alexander Mathis",
            "Mackenzie W. Mathis"
        ],
        "published": "2023-07-10T19:15:17Z",
        "summary": "The process of quantifying and analyzing animal behavior involves translating\nthe naturally occurring descriptive language of their actions into\nmachine-readable code. Yet, codifying behavior analysis is often challenging\nwithout deep understanding of animal behavior and technical machine learning\nknowledge. To limit this gap, we introduce AmadeusGPT: a natural language\ninterface that turns natural language descriptions of behaviors into\nmachine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4\nallow for interactive language-based queries that are potentially well suited\nfor making interactive behavior analysis. However, the comprehension capability\nof these LLMs is limited by the context window size, which prevents it from\nremembering distant conversations. To overcome the context window limitation,\nwe implement a novel dual-memory mechanism to allow communication between\nshort-term and long-term memory using symbols as context pointers for retrieval\nand saving. Concretely, users directly use language-based definitions of\nbehavior and our augmented GPT develops code based on the core AmadeusGPT API,\nwhich contains machine learning, computer vision, spatio-temporal reasoning,\nand visualization modules. Users then can interactively refine results, and\nseamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and\nshow we can produce state-of-the-art performance on the MABE 2022 behavior\nchallenge tasks. Note, an end-user would not need to write any code to achieve\nthis. Thus, collectively AmadeusGPT presents a novel way to merge deep\nbiological knowledge, large-language models, and core computer vision modules\ninto a more naturally intelligent system. Code and demos can be found at:\nhttps://github.com/AdaptiveMotorControlLab/AmadeusGPT.",
        "pdf_link": "https://arxiv.org/pdf/2307.04858v1.pdf"
    },
    {
        "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
        "authors": [
            "Jiaming Ji",
            "Mickel Liu",
            "Juntao Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Ce Bian",
            "Chi Zhang",
            "Ruiyang Sun",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "published": "2023-07-10T15:56:17Z",
        "summary": "In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have gathered safety meta-labels for 333,963\nquestion-answer (QA) pairs and 361,903 pairs of expert comparison data for both\nthe helpfulness and harmlessness metrics. We further showcase applications of\nBeaverTails in content moderation and reinforcement learning with human\nfeedback (RLHF), emphasizing its potential for practical safety measures in\nLLMs. We believe this dataset provides vital resources for the community,\ncontributing towards the safe development and deployment of LLMs. Our project\npage is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.",
        "pdf_link": "https://arxiv.org/pdf/2307.04657v3.pdf"
    },
    {
        "title": "Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training",
        "authors": [
            "Dima Galat",
            "Marian-Andrei Rizoiu"
        ],
        "published": "2023-07-10T08:32:45Z",
        "summary": "Biomedical summarization requires large datasets to train for text\ngeneration. We show that while transfer learning offers a viable option for\naddressing this challenge, an in-domain pre-training does not always offer\nadvantages in a BioASQ summarization task. We identify a suitable model\narchitecture and use it to show a benefit of a general-domain pre-training\nfollowed by a task-specific fine-tuning in the context of a BioASQ\nsummarization task, leading to a novel three-step fine-tuning approach that\nworks with only a thousand in-domain examples. Our results indicate that a\nLarge Language Model without domain-specific pre-training can have a\nsignificant edge in some domain-specific biomedical text generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.04412v1.pdf"
    },
    {
        "title": "TIM: Teaching Large Language Models to Translate with Comparison",
        "authors": [
            "Jiali Zeng",
            "Fandong Meng",
            "Yongjing Yin",
            "Jie Zhou"
        ],
        "published": "2023-07-10T08:15:40Z",
        "summary": "Open-sourced large language models (LLMs) have demonstrated remarkable\nefficacy in various tasks with instruction tuning. However, these models can\nsometimes struggle with tasks that require more specialized knowledge such as\ntranslation. One possible reason for such deficiency is that instruction tuning\naims to generate fluent and coherent text that continues from a given\ninstruction without being constrained by any task-specific requirements.\nMoreover, it can be more challenging for tuning smaller LLMs with lower-quality\ntraining data. To address this issue, we propose a novel framework using\nexamples in comparison to teach LLMs to learn translation. Our approach\ninvolves presenting the model with examples of correct and incorrect\ntranslations and using a preference loss to guide the model's learning. We\nevaluate our method on WMT2022 test sets and show that it outperforms existing\nmethods. Our findings offer a new perspective on fine-tuning LLMs for\ntranslation tasks and provide a promising solution for generating high-quality\ntranslations. Please refer to Github for more details:\nhttps://github.com/lemon0830/TIM.",
        "pdf_link": "https://arxiv.org/pdf/2307.04408v3.pdf"
    },
    {
        "title": "Towards Cross-Table Masked Pretraining for Web Data Mining",
        "authors": [
            "Chao Ye",
            "Guoshan Lu",
            "Haobo Wang",
            "Liyao Li",
            "Sai Wu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "published": "2023-07-10T02:27:38Z",
        "summary": "Tabular data pervades the landscape of the World Wide Web, playing a\nfoundational role in the digital architecture that underpins online\ninformation. Given the recent influence of large-scale pretrained models like\nChatGPT and SAM across various domains, exploring the application of\npretraining techniques for mining tabular data on the web has emerged as a\nhighly promising research direction. Indeed, there have been some recent works\naround this topic where most (if not all) of them are limited in the scope of a\nfixed-schema/single table. Due to the scale of the dataset and the parameter\nsize of the prior models, we believe that we have not reached the ''BERT\nmoment'' for the ubiquitous tabular data. The development on this line\nsignificantly lags behind the counterpart research domains such as natural\nlanguage processing. In this work, we first identify the crucial challenges\nbehind tabular data pretraining, particularly overcoming the cross-table\nhurdle. As a pioneering endeavor, this work mainly (i)-contributes a\nhigh-quality real-world tabular dataset, (ii)-proposes an innovative, generic,\nand efficient cross-table pretraining framework, dubbed as CM2, where the core\nto it comprises a semantic-aware tabular neural network that uniformly encodes\nheterogeneous tables without much restriction and (iii)-introduces a novel\npretraining objective -- prompt Masked Table Modeling (pMTM) -- inspired by NLP\nbut intricately tailored to scalable pretraining on tables. Our extensive\nexperiments demonstrate CM2's state-of-the-art performance and validate that\ncross-table pretraining can enhance various downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2307.04308v2.pdf"
    },
    {
        "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation",
        "authors": [
            "Neeraj Varshney",
            "Wenlin Yao",
            "Hongming Zhang",
            "Jianshu Chen",
            "Dong Yu"
        ],
        "published": "2023-07-08T14:25:57Z",
        "summary": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2307.03987v2.pdf"
    },
    {
        "title": "Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task",
        "authors": [
            "Fanyi Qu",
            "Yunfang Wu"
        ],
        "published": "2023-07-08T13:10:59Z",
        "summary": "Large-scale language models (LLMs) has shown remarkable capability in various\nof Natural Language Processing (NLP) tasks and attracted lots of attention\nrecently. However, some studies indicated that large language models fail to\nachieve promising result beyond the state-of-the-art models in English\ngrammatical error correction (GEC) tasks. In this report, we aim to explore the\nhow large language models perform on Chinese grammatical error correction tasks\nand provide guidance for future work. We conduct experiments with 3 different\nLLMs of different model scale on 4 Chinese GEC dataset. Our experimental\nresults indicate that the performances of LLMs on automatic evaluation metrics\nfalls short of the previous sota models because of the problem of\nover-correction. Furthermore, we also discover notable variations in the\nperformance of LLMs when evaluated on different data distributions. Our\nfindings demonstrates that further investigation is required for the\napplication of LLMs on Chinese GEC task.",
        "pdf_link": "https://arxiv.org/pdf/2307.03972v1.pdf"
    },
    {
        "title": "Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions",
        "authors": [
            "Dawen Zhang",
            "Pamela Finckenberg-Broman",
            "Thong Hoang",
            "Shidong Pan",
            "Zhenchang Xing",
            "Mark Staples",
            "Xiwei Xu"
        ],
        "published": "2023-07-08T09:28:50Z",
        "summary": "The Right to be Forgotten (RTBF) was first established as the result of the\nruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\\'alez, and\nwas later included as the Right to Erasure under the General Data Protection\nRegulation (GDPR) of European Union to allow individuals the right to request\npersonal data be deleted by organizations. Specifically for search engines,\nindividuals can send requests to organizations to exclude their information\nfrom the query results. It was a significant emergent right as the result of\nthe evolution of technology. With the recent development of Large Language\nModels (LLMs) and their use in chatbots, LLM-enabled software systems have\nbecome popular. But they are not excluded from the RTBF. Compared with the\nindexing approach used by search engines, LLMs store, and process information\nin a completely different way. This poses new challenges for compliance with\nthe RTBF. In this paper, we explore these challenges and provide our insights\non how to implement technical solutions for the RTBF, including the use of\ndifferential privacy, machine unlearning, model editing, and prompt\nengineering. With the rapid advancement of AI and the increasing need of\nregulating this powerful technology, learning from the case of RTBF can provide\nvaluable lessons for technical practitioners, legal experts, organizations, and\nauthorities.",
        "pdf_link": "https://arxiv.org/pdf/2307.03941v3.pdf"
    },
    {
        "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
        "authors": [
            "Xiaomeng Hu",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "published": "2023-07-07T21:13:27Z",
        "summary": "Recent advances in large language models (LLMs) and the intensifying\npopularity of ChatGPT-like applications have blurred the boundary of\nhigh-quality text generation between humans and machines. However, in addition\nto the anticipated revolutionary changes to our technology and society, the\ndifficulty of distinguishing LLM-generated texts (AI-text) from human-generated\ntexts poses new challenges of misuse and fairness, such as fake content\ngeneration, plagiarism, and false accusations of innocent writers. While\nexisting works show that current AI-text detectors are not robust to LLM-based\nparaphrasing, this paper aims to bridge this gap by proposing a new framework\ncalled RADAR, which jointly trains a robust AI-text detector via adversarial\nlearning. RADAR is based on adversarial training of a paraphraser and a\ndetector. The paraphraser's goal is to generate realistic content to evade\nAI-text detection. RADAR uses the feedback from the detector to update the\nparaphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly\n2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,\nexperimental results show that RADAR significantly outperforms existing AI-text\ndetection methods, especially when paraphrasing is in place. We also identify\nthe strong transferability of RADAR from instruction-tuned LLMs to other LLMs,\nand evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2307.03838v2.pdf"
    },
    {
        "title": "Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media",
        "authors": [
            "Chuanbo Hu",
            "Bin Liu",
            "Xin Li",
            "Yanfang Ye"
        ],
        "published": "2023-07-07T16:15:59Z",
        "summary": "Social media platforms such as Instagram and Twitter have emerged as critical\nchannels for drug marketing and illegal sale. Detecting and labeling online\nillicit drug trafficking activities becomes important in addressing this issue.\nHowever, the effectiveness of conventional supervised learning methods in\ndetecting drug trafficking heavily relies on having access to substantial\namounts of labeled data, while data annotation is time-consuming and\nresource-intensive. Furthermore, these models often face challenges in\naccurately identifying trafficking activities when drug dealers use deceptive\nlanguage and euphemisms to avoid detection. To overcome this limitation, we\nconduct the first systematic study on leveraging large language models (LLMs),\nsuch as ChatGPT, to detect illicit drug trafficking activities on social media.\nWe propose an analytical framework to compose \\emph{knowledge-informed\nprompts}, which serve as the interface that humans can interact with and use\nLLMs to perform the detection task. Additionally, we design a Monte Carlo\ndropout based prompt optimization method to further to improve performance and\ninterpretability. Our experimental findings demonstrate that the proposed\nframework outperforms other baseline language models in terms of drug\ntrafficking detection accuracy, showing a remarkable improvement of nearly\n12\\%. By integrating prior knowledge and the proposed prompts, ChatGPT can\neffectively identify and label drug trafficking activities on social networks,\neven in the presence of deceptive language and euphemisms used by drug dealers\nto evade detection. The implications of our research extend to social networks,\nemphasizing the importance of incorporating prior knowledge and scenario-based\nprompts into analytical tools to improve online security and public safety.",
        "pdf_link": "https://arxiv.org/pdf/2307.03699v1.pdf"
    },
    {
        "title": "Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models",
        "authors": [
            "Yuxi Ma",
            "Chi Zhang",
            "Song-Chun Zhu"
        ],
        "published": "2023-07-07T13:58:16Z",
        "summary": "In this perspective paper, we first comprehensively review existing\nevaluations of Large Language Models (LLMs) using both standardized tests and\nability-oriented benchmarks. We pinpoint several problems with current\nevaluation methods that tend to overstate the capabilities of LLMs. We then\narticulate what artificial general intelligence should encompass beyond the\ncapabilities of LLMs. We propose four characteristics of generally intelligent\nagents: 1) they can perform unlimited tasks; 2) they can generate new tasks\nwithin a context; 3) they operate based on a value system that underpins task\ngeneration; and 4) they have a world model reflecting reality, which shapes\ntheir interaction with the world. Building on this viewpoint, we highlight the\nmissing pieces in artificial general intelligence, that is, the unity of\nknowing and acting. We argue that active engagement with objects in the real\nworld delivers more robust signals for forming conceptual representations.\nAdditionally, knowledge acquisition isn't solely reliant on passive input but\nrequires repeated trials and errors. We conclude by outlining promising future\nresearch directions in the field of artificial general intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2307.03762v1.pdf"
    },
    {
        "title": "TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction",
        "authors": [
            "Shuo Li",
            "Sangdon Park",
            "Insup Lee",
            "Osbert Bastani"
        ],
        "published": "2023-07-07T02:42:06Z",
        "summary": "When applied to open-domain question answering, large language models (LLMs)\nfrequently generate incorrect responses based on made-up facts, which are\ncalled $\\textit{hallucinations}$. Retrieval augmented generation (RAG) is a\npromising strategy to avoid hallucinations, but it does not provide guarantees\non its correctness. To address this challenge, we propose the Trustworthy\nRetrieval Augmented Question Answering, or $\\textit{TRAQ}$, which provides the\nfirst end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal\nprediction, a statistical technique for constructing prediction sets that are\nguaranteed to contain the semantically correct response with high probability.\nAdditionally, TRAQ leverages Bayesian optimization to minimize the size of the\nconstructed sets. In an extensive experimental evaluation, we demonstrate that\nTRAQ provides the desired correctness guarantee while reducing prediction set\nsize by 16.2% on average compared to an ablation. The implementation is\navailable at $\\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.",
        "pdf_link": "https://arxiv.org/pdf/2307.04642v2.pdf"
    },
    {
        "title": "Focused Transformer: Contrastive Training for Context Scaling",
        "authors": [
            "Szymon Tworkowski",
            "Konrad Staniszewski",
            "Miko≈Çaj Pacek",
            "Yuhuai Wu",
            "Henryk Michalewski",
            "Piotr Mi≈Ço≈õ"
        ],
        "published": "2023-07-06T17:52:10Z",
        "summary": "Large language models have an exceptional capability to incorporate new\ninformation in a contextual manner. However, the full potential of such an\napproach is often restrained due to a limitation in the effective context\nlength. One solution to this issue is to endow an attention layer with access\nto an external memory, which comprises of (key, value) pairs. Yet, as the\nnumber of documents increases, the proportion of relevant keys to irrelevant\nones decreases, leading the model to focus more on the irrelevant keys. We\nidentify a significant challenge, dubbed the distraction issue, where keys\nlinked to different semantic values might overlap, making them hard to\ndistinguish. To tackle this problem, we introduce the Focused Transformer\n(FoT), a technique that employs a training process inspired by contrastive\nlearning. This novel approach enhances the structure of the (key, value) space,\nenabling an extension of the context length. Our method allows for fine-tuning\npre-existing, large-scale models to lengthen their effective context. This is\ndemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The\nresulting models, which we name LongLLaMA, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LongLLaMA models\nadeptly manage a $256 k$ context length for passkey retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2307.03170v2.pdf"
    },
    {
        "title": "Style Over Substance: Evaluation Biases for Large Language Models",
        "authors": [
            "Minghao Wu",
            "Alham Fikri Aji"
        ],
        "published": "2023-07-06T14:42:01Z",
        "summary": "As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2307.03025v3.pdf"
    },
    {
        "title": "Amplifying Limitations, Harms and Risks of Large Language Models",
        "authors": [
            "Michael O'Neill",
            "Mark Connor"
        ],
        "published": "2023-07-06T11:53:45Z",
        "summary": "We present this article as a small gesture in an attempt to counter what\nappears to be exponentially growing hype around Artificial Intelligence (AI)\nand its capabilities, and the distraction provided by the associated talk of\nscience-fiction scenarios that might arise if AI should become sentient and\nsuper-intelligent. It may also help those outside of the field to become more\ninformed about some of the limitations of AI technology. In the current context\nof popular discourse AI defaults to mean foundation and large language models\n(LLMs) such as those used to create ChatGPT. This in itself is a\nmisrepresentation of the diversity, depth and volume of research, researchers,\nand technology that truly represents the field of AI. AI being a field of\nresearch that has existed in software artefacts since at least the 1950's. We\nset out to highlight a number of limitations of LLMs, and in so doing highlight\nthat harms have already arisen and will continue to arise due to these\nlimitations. Along the way we also highlight some of the associated risks for\nindividuals and organisations in using this technology.",
        "pdf_link": "https://arxiv.org/pdf/2307.04821v1.pdf"
    },
    {
        "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
        "authors": [
            "Ruosen Li",
            "Teerth Patel",
            "Xinya Du"
        ],
        "published": "2023-07-06T04:05:44Z",
        "summary": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) are hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs as a reference-free metric for\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose the (1) peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on preferences\nof two answers. We conduct experiments on two benchmark datasets. We find that\nour approaches achieve higher accuracy and align better with human judgments,\nrespectively. Interestingly, PR can induce a relatively accurate self-ranking\nof models under the anonymous setting, where each model's name is unrevealed.\nOur work provides space to explore evaluating models that are hard to compare\nfor humans.",
        "pdf_link": "https://arxiv.org/pdf/2307.02762v1.pdf"
    },
    {
        "title": "Scaling In-Context Demonstrations with Structured Attention",
        "authors": [
            "Tianle Cai",
            "Kaixuan Huang",
            "Jason D. Lee",
            "Mengdi Wang"
        ],
        "published": "2023-07-05T23:26:01Z",
        "summary": "The recent surge of large language models (LLMs) highlights their ability to\nperform in-context learning, i.e., \"learning\" to perform a task from a few\ndemonstrations in the context without any parameter updates. However, their\ncapabilities of in-context learning are limited by the model architecture: 1)\nthe use of demonstrations is constrained by a maximum sentence length due to\npositional embeddings; 2) the quadratic complexity of attention hinders users\nfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitive\nto the order of the demonstrations. In this work, we tackle these challenges by\nproposing a better architectural design for in-context learning. We propose\nSAICL (Structured Attention for In-Context Learning), which replaces the\nfull-attention by a structured attention mechanism designed for in-context\nlearning, and removes unnecessary dependencies between individual\ndemonstrations, while making the model invariant to the permutation of\ndemonstrations. We evaluate SAICL in a meta-training framework and show that\nSAICL achieves comparable or better performance than full attention while\nobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a\nstrong Fusion-in-Decoder (FiD) baseline which processes each demonstration\nindependently. Finally, thanks to its linear nature, we demonstrate that SAICL\ncan easily scale to hundreds of demonstrations with continuous performance\ngains with scaling.",
        "pdf_link": "https://arxiv.org/pdf/2307.02690v1.pdf"
    },
    {
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "authors": [
            "Alexander Wei",
            "Nika Haghtalab",
            "Jacob Steinhardt"
        ],
        "published": "2023-07-05T17:58:10Z",
        "summary": "Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.",
        "pdf_link": "https://arxiv.org/pdf/2307.02483v1.pdf"
    },
    {
        "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
        "authors": [
            "Zhaofeng Wu",
            "Linlu Qiu",
            "Alexis Ross",
            "Ekin Aky√ºrek",
            "Boyuan Chen",
            "Bailin Wang",
            "Najoung Kim",
            "Jacob Andreas",
            "Yoon Kim"
        ],
        "published": "2023-07-05T17:50:42Z",
        "summary": "The impressive performance of recent language models across a wide range of\ntasks suggests that they possess a degree of abstract reasoning skills. Are\nthese skills general and transferable, or specialized to specific tasks seen\nduring pretraining? To disentangle these effects, we propose an evaluation\nframework based on \"counterfactual\" task variants that deviate from the default\nassumptions underlying standard tasks. Across a suite of 11 tasks, we observe\nnontrivial performance on the counterfactual variants, but nevertheless find\nthat performance substantially and consistently degrades compared to the\ndefault conditions. This suggests that while current LMs may possess abstract\ntask-solving skills to an extent, they often also rely on narrow,\nnon-transferable procedures for task-solving. These results motivate a more\ncareful interpretation of language model performance that teases apart these\naspects of behavior.",
        "pdf_link": "https://arxiv.org/pdf/2307.02477v3.pdf"
    },
    {
        "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
        "authors": [
            "Akide Liu"
        ],
        "published": "2023-07-05T17:05:32Z",
        "summary": "Memory is identified as a crucial human faculty that allows for the retention\nof visual and linguistic information within the hippocampus and neurons in the\nbrain, which can subsequently be retrieved to address real-world challenges\nthat arise through a lifetime of learning. The resolution of complex AI tasks\nthrough the application of acquired knowledge represents a stride toward the\nrealization of artificial general intelligence. However, despite the prevalence\nof Large Language Models (LLMs) like GPT-3.5 and GPT-4 \\cite{brown2020language,\nleiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have\ndisplayed remarkable capabilities in language comprehension, generation,\ninteraction, and reasoning, they are inhibited by constraints on context length\nthat preclude the processing of extensive, continually evolving knowledge\nbases. This paper proposes that LLMs could be augmented through the selective\nintegration of knowledge from external repositories, and in doing so,\nintroduces a novel methodology for External Reasoning, exemplified by ChatPDF.\nCentral to this approach is the establishment of a tiered policy for\n\\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in\n\\cref{fig:overall}, where the level of support rendered is modulated across\nentry, intermediate, and advanced tiers based on the complexity of the query,\nwith adjustments made in response to human feedback. A comprehensive evaluation\nof this methodology is conducted using multiple LLMs and the results indicate\nstate-of-the-art performance in \\cref{comparison} , surpassing existing\nsolutions including ChatPDF.com. Moreover, the paper emphasizes that this\napproach is more efficient compared to the direct processing of full text by\nLLMs. The source code is publicly available at:\n\\url{https://github.com/AkideLiu/ANLP}.",
        "pdf_link": "https://arxiv.org/pdf/2307.12057v2.pdf"
    },
    {
        "title": "Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise Given to Students in Synthetic Dialogues",
        "authors": [
            "Dollaya Hirunyasiri",
            "Danielle R. Thomas",
            "Jionghao Lin",
            "Kenneth R. Koedinger",
            "Vincent Aleven"
        ],
        "published": "2023-07-05T04:14:01Z",
        "summary": "Research suggests that providing specific and timely feedback to human tutors\nenhances their performance. However, it presents challenges due to the\ntime-consuming nature of assessing tutor performance by human evaluators. Large\nlanguage models, such as the AI-chatbot ChatGPT, hold potential for offering\nconstructive feedback to tutors in practical settings. Nevertheless, the\naccuracy of AI-generated feedback remains uncertain, with scant research\ninvestigating the ability of models like ChatGPT to deliver effective feedback.\nIn this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a\ntutor-student setting. We use two different prompting approaches, the zero-shot\nchain of thought and the few-shot chain of thought, to identify specific\ncomponents of effective praise based on five criteria. These approaches are\nthen compared to the results of human graders for accuracy. Our goal is to\nassess the extent to which GPT-4 can accurately identify each praise criterion.\nWe found that both zero-shot and few-shot chain of thought approaches yield\ncomparable results. GPT-4 performs moderately well in identifying instances\nwhen the tutor offers specific and immediate praise. However, GPT-4\nunderperforms in identifying the tutor's ability to deliver sincere praise,\nparticularly in the zero-shot prompting scenario where examples of sincere\ntutor praise statements were not provided. Future work will focus on enhancing\nprompt engineering, developing a more general tutoring rubric, and evaluating\nour method using real-life tutoring dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2307.02018v1.pdf"
    },
    {
        "title": "Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations",
        "authors": [
            "Yuhan Ji",
            "Song Gao"
        ],
        "published": "2023-07-05T03:50:08Z",
        "summary": "This research focuses on assessing the ability of large language models\n(LLMs) in representing geometries and their spatial relations. We utilize LLMs\nincluding GPT-2 and BERT to encode the well-known text (WKT) format of\ngeometries and then feed their embeddings into classifiers and regressors to\nevaluate the effectiveness of the LLMs-generated embeddings for geometric\nattributes. The experiments demonstrate that while the LLMs-generated\nembeddings can preserve geometry types and capture some spatial relations (up\nto 73% accuracy), challenges remain in estimating numeric values and retrieving\nspatially related objects. This research highlights the need for improvement in\nterms of capturing the nuances and complexities of the underlying geospatial\ndata and integrating domain knowledge to support various GeoAI applications\nusing foundation models.",
        "pdf_link": "https://arxiv.org/pdf/2307.03678v1.pdf"
    },
    {
        "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
        "authors": [
            "Sha Li",
            "Ruining Zhao",
            "Manling Li",
            "Heng Ji",
            "Chris Callison-Burch",
            "Jiawei Han"
        ],
        "published": "2023-07-05T01:00:44Z",
        "summary": "Event schemas are a form of world knowledge about the typical progression of\nevents. Recent methods for event schema induction use information extraction\nsystems to construct a large number of event graph instances from documents,\nand then learn to generalize the schema from such instances. In contrast, we\npropose to treat event schemas as a form of commonsense knowledge that can be\nderived from large language models (LLMs). This new paradigm greatly simplifies\nthe schema induction process and allows us to handle both hierarchical\nrelations and temporal relations between events in a straightforward way. Since\nevent schemas have complex graph structures, we design an incremental prompting\nand verification method to break down the construction of a complex event graph\ninto three stages: event skeleton construction, event expansion, and\nevent-event relation verification. Compared to directly using LLMs to generate\na linearized graph, our method can generate large and complex schemas with 7.2%\nF1 improvement in temporal relations and 31.0% F1 improvement in hierarchical\nrelations. In addition, compared to the previous state-of-the-art closed-domain\nschema induction model, human assessors were able to cover $\\sim$10% more\nevents when translating the schemas into coherent stories and rated our schemas\n1.3 points higher (on a 5-point scale) in terms of readability.",
        "pdf_link": "https://arxiv.org/pdf/2307.01972v1.pdf"
    },
    {
        "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
        "authors": [
            "Siwon Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Martin Gubri",
            "Sungroh Yoon",
            "Seong Joon Oh"
        ],
        "published": "2023-07-04T18:53:47Z",
        "summary": "The rapid advancement and widespread use of large language models (LLMs) have\nraised significant concerns regarding the potential leakage of personally\nidentifiable information (PII). These models are often trained on vast\nquantities of web-collected data, which may inadvertently include sensitive\npersonal data. This paper presents ProPILE, a novel probing tool designed to\nempower data subjects, or the owners of the PII, with awareness of potential\nPII leakage in LLM-based services. ProPILE lets data subjects formulate prompts\nbased on their own PII to evaluate the level of privacy intrusion in LLMs. We\ndemonstrate its application on the OPT-1.3B model trained on the publicly\navailable Pile dataset. We show how hypothetical data subjects may assess the\nlikelihood of their PII being included in the Pile dataset being revealed.\nProPILE can also be leveraged by LLM service providers to effectively evaluate\ntheir own levels of PII leakage with more powerful prompts specifically tuned\nfor their in-house models. This tool represents a pioneering step towards\nempowering the data subjects for their awareness and control over their own\ndata on the web.",
        "pdf_link": "https://arxiv.org/pdf/2307.01881v1.pdf"
    },
    {
        "title": "Learning to Prompt in the Classroom to Understand AI Limits: A pilot study",
        "authors": [
            "Emily Theophilou",
            "Cansu Koyuturk",
            "Mona Yavari",
            "Sathya Bursic",
            "Gregor Donabauer",
            "Alessia Telari",
            "Alessia Testa",
            "Raffaele Boiano",
            "Davinia Hernandez-Leo",
            "Martin Ruskov",
            "Davide Taibi",
            "Alessandro Gabbiadini",
            "Dimitri Ognibene"
        ],
        "published": "2023-07-04T07:51:37Z",
        "summary": "Artificial intelligence's (AI) progress holds great promise in tackling\npressing societal concerns such as health and climate. Large Language Models\n(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural\nlanguage processing capabilities of AI systems allowing them to process an\nunprecedented amount of unstructured data. However, the ensuing excitement has\nled to negative sentiments, even as AI methods demonstrate remarkable\ncontributions (e.g. in health and genetics). A key factor contributing to this\nsentiment is the misleading perception that LLMs can effortlessly provide\nsolutions across domains, ignoring their limitations such as hallucinations and\nreasoning constraints. Acknowledging AI fallibility is crucial to address the\nimpact of dogmatic overconfidence in possibly erroneous suggestions generated\nby LLMs. At the same time, it can reduce fear and other negative attitudes\ntoward AI. This necessitates comprehensive AI literacy interventions that\neducate the public about LLM constraints and effective usage techniques, i.e\nprompting strategies. With this aim, a pilot educational intervention was\nperformed in a high school with 21 students. It involved presenting high-level\nconcepts about intelligence, AI, and LLMs, followed by practical exercises\ninvolving ChatGPT in creating natural educational conversations and applying\nestablished prompting strategies. Encouraging preliminary results emerged,\nincluding high appreciation of the activity, improved interaction quality with\nthe LLM, reduced negative AI sentiments, and a better grasp of limitations,\nspecifically unreliability, limited understanding of commands leading to\nunsatisfactory responses, and limited presentation flexibility. Our aim is to\nexplore AI acceptance factors and refine this approach for more controlled\nfuture studies.",
        "pdf_link": "https://arxiv.org/pdf/2307.01540v2.pdf"
    },
    {
        "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care",
        "authors": [
            "Tong Xiang",
            "Liangzhi Li",
            "Wangyue Li",
            "Mingbai Bai",
            "Lu Wei",
            "Bowen Wang",
            "Noa Garcia"
        ],
        "published": "2023-07-04T03:34:19Z",
        "summary": "The recent advances in natural language processing (NLP), have led to a new\ntrend of applying large language models (LLMs) to real-world scenarios. While\nthe latest LLMs are astonishingly fluent when interacting with humans, they\nsuffer from the misinformation problem by unintentionally generating factually\nfalse statements. This can lead to harmful consequences, especially when\nproduced within sensitive contexts, such as healthcare. Yet few previous works\nhave focused on evaluating misinformation in the long-form (LF) generation of\nLLMs, especially for knowledge-intensive topics. Moreover, although LLMs have\nbeen shown to perform well in different languages, misinformation evaluation\nhas been mostly conducted in English. To this end, we present a benchmark,\nCARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,\nspecifically the maternity and infant care domain; and 2) a language other than\nEnglish, namely Chinese. Most importantly, we provide an innovative paradigm\nfor building LF generation evaluation benchmarks that can be transferred to\nother knowledge-intensive domains and low-resourced languages. Our proposed\nbenchmark fills the gap between the extensive usage of LLMs and the lack of\ndatasets for assessing the misinformation generated by these models. It\ncontains 1,612 expert-checked questions, accompanied with human-selected\nreferences. Using our benchmark, we conduct extensive experiments and found\nthat current Chinese LLMs are far from perfect in the topic of maternity and\ninfant care. In an effort to minimize the reliance on human resources for\nperformance evaluation, we offer off-the-shelf judgment models for\nautomatically assessing the LF output of LLMs given benchmark questions.\nMoreover, we compare potential solutions for LF generation evaluation and\nprovide insights for building better automated metrics.",
        "pdf_link": "https://arxiv.org/pdf/2307.01458v4.pdf"
    },
    {
        "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
        "authors": [
            "Jinhao Duan",
            "Hao Cheng",
            "Shiqi Wang",
            "Alex Zavalny",
            "Chenan Wang",
            "Renjing Xu",
            "Bhavya Kailkhura",
            "Kaidi Xu"
        ],
        "published": "2023-07-03T22:17:16Z",
        "summary": "While Large Language Models (LLMs) have demonstrated remarkable potential in\nnatural language generation and instruction following, a persistent challenge\nlies in their susceptibility to \"hallucinations\", which erodes trust in their\noutputs. Although Uncertainty Quantification (UQ) presents a promising\nsolution, its accurate implementation within the context of LLMs remains a\nsignificant hurdle. To address this critical roadblock, our research originates\nfrom a fundamental heuristic insight: tokens within auto-regressive\nLLM-generated text do not equally reflect the underlying meaning. Some tokens\ncarry greater relevance and representativeness than others, owing to the\nphenomenon of \"linguistic redundancy\", wherein a select few keywords suffice to\nconvey the essence of lengthy sentences. Regrettably, existing methodologies\ntreat all tokens with equal importance when estimating uncertainty,\ndisregarding these inherent generative inequalities. Our analysis reveals a\nsignificant issue with state-of-the-art: numerous tokens (and sentences) of\nlimited semantic significance receive equal or even excessive weighting during\nuncertainty estimation. To rectify this bias, we propose to jointly Shifting\nAttention to more Relevant (SAR) components, at both the token- and the\nsentence-levels for accurate uncertainty estimation. We conduct extensive\nexperiments involving a range of popular \"off-the-shelf\" LLMs, including\ninstruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as\npretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B\nparameters. We carry out evaluation across various free-form question-answering\ntasks, encompassing domains such as reading comprehension, science Q&A, and\nmedical Q&A. Our experimental results demonstrate the superior performance of\nSAR in addressing the challenges of uncertainty estimation within the realm of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2307.01379v2.pdf"
    },
    {
        "title": "Multilingual Language Models are not Multicultural: A Case Study in Emotion",
        "authors": [
            "Shreya Havaldar",
            "Sunny Rai",
            "Bhumika Singhal",
            "Langchen Liu",
            "Sharath Chandra Guntuku",
            "Lyle Ungar"
        ],
        "published": "2023-07-03T21:54:28Z",
        "summary": "Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.",
        "pdf_link": "https://arxiv.org/pdf/2307.01370v2.pdf"
    },
    {
        "title": "Challenges in Domain-Specific Abstractive Summarization and How to Overcome them",
        "authors": [
            "Anum Afzal",
            "Juraj Vladika",
            "Daniel Braun",
            "Florian Matthes"
        ],
        "published": "2023-07-03T12:26:44Z",
        "summary": "Large Language Models work quite well with general-purpose data and many\ntasks in Natural Language Processing. However, they show several limitations\nwhen used for a task such as domain-specific abstractive text summarization.\nThis paper identifies three of those limitations as research problems in the\ncontext of abstractive text summarization: 1) Quadratic complexity of\ntransformer-based models with respect to the input text length; 2) Model\nHallucination, which is a model's ability to generate factually incorrect text;\nand 3) Domain Shift, which happens when the distribution of the model's\ntraining and test corpus is not the same. Along with a discussion of the open\nresearch questions, this paper also provides an assessment of existing\nstate-of-the-art techniques relevant to domain-specific text summarization to\naddress the research gaps.",
        "pdf_link": "https://arxiv.org/pdf/2307.00963v1.pdf"
    },
    {
        "title": "Evaluating Shutdown Avoidance of Language Models in Textual Scenarios",
        "authors": [
            "Teun van der Weij",
            "Simon Lermen",
            "Leon lang"
        ],
        "published": "2023-07-03T07:05:59Z",
        "summary": "Recently, there has been an increase in interest in evaluating large language\nmodels for emergent and dangerous capabilities. Importantly, agents could\nreason that in some scenarios their goal is better achieved if they are not\nturned off, which can lead to undesirable behaviors. In this paper, we\ninvestigate the potential of using toy textual scenarios to evaluate\ninstrumental reasoning and shutdown avoidance in language models such as GPT-4\nand Claude. Furthermore, we explore whether shutdown avoidance is merely a\nresult of simple pattern matching between the dataset and the prompt or if it\nis a consistent behaviour across different environments and variations.\n  We evaluated behaviours manually and also experimented with using language\nmodels for automatic evaluations, and these evaluations demonstrate that simple\npattern matching is likely not the sole contributing factor for shutdown\navoidance. This study provides insights into the behaviour of language models\nin shutdown avoidance scenarios and inspires further research on the use of\ntextual scenarios for evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2307.00787v1.pdf"
    }
]