[
    {
        "title": "Efficiently Adapting Pretrained Language Models To New Languages",
        "authors": [
            "Zoltan Csaki",
            "Pian Pawakapan",
            "Urmish Thakker",
            "Qiantong Xu"
        ],
        "published": "2023-11-09T20:59:08Z",
        "summary": "Recent large language models (LLM) exhibit sub-optimal performance on\nlow-resource languages, as the training data of these models is usually\ndominated by English and other high-resource languages. Furthermore, it is\nchallenging to train models for low-resource languages, especially from\nscratch, due to a lack of high quality training data. Adapting pretrained LLMs\nreduces the need for data in the new language while also providing cross\nlingual transfer capabilities. However, naively adapting to new languages leads\nto catastrophic forgetting and poor tokenizer efficiency. In this work, we\nstudy how to efficiently adapt any existing pretrained LLM to a new language\nwithout running into these issues. In particular, we improve the encoding\nefficiency of the tokenizer by adding new tokens from the target language and\nstudy the data mixing recipe to mitigate forgetting. Our experiments on\nadapting an English LLM to Hungarian and Thai show that our recipe can reach\nbetter performance than open source models on the target language, with minimal\nregressions on English.",
        "pdf_link": "https://arxiv.org/pdf/2311.05741v2.pdf"
    },
    {
        "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
        "authors": [
            "Simon Stepputtis",
            "Joseph Campbell",
            "Yaqi Xie",
            "Zhengyang Qi",
            "Wenxin Sharon Zhang",
            "Ruiyi Wang",
            "Sanketh Rangreji",
            "Michael Lewis",
            "Katia Sycara"
        ],
        "published": "2023-11-09T20:04:08Z",
        "summary": "Deception and persuasion play a critical role in long-horizon dialogues\nbetween multiple parties, especially when the interests, goals, and motivations\nof the participants are not aligned. Such complex tasks pose challenges for\ncurrent Large Language Models (LLM) as deception and persuasion can easily\nmislead them, especially in long-horizon multi-party dialogues. To this end, we\nexplore the game of Avalon: The Resistance, a social deduction game in which\nplayers must determine each other's hidden identities to complete their team's\nobjective. We introduce an online testbed and a dataset containing 20 carefully\ncollected and labeled games among human players that exhibit long-horizon\ndeception in a cooperative-competitive setting. We discuss the capabilities of\nLLMs to utilize deceptive long-horizon conversations between six human players\nto determine each player's goal and motivation. Particularly, we discuss the\nmultimodal integration of the chat between the players and the game's state\nthat grounds the conversation, providing further insights into the true player\nidentities. We find that even current state-of-the-art LLMs do not reach human\nperformance, making our dataset a compelling benchmark to investigate the\ndecision-making and language-processing capabilities of LLMs. Our dataset and\nonline testbed can be found at our project website:\nhttps://sstepput.github.io/Avalon-NLU/",
        "pdf_link": "https://arxiv.org/pdf/2311.05720v1.pdf"
    },
    {
        "title": "Conversational AI Threads for Visualizing Multidimensional Datasets",
        "authors": [
            "Matt-Heun Hong",
            "Anamaria Crisan"
        ],
        "published": "2023-11-09T18:47:46Z",
        "summary": "Generative Large Language Models (LLMs) show potential in data analysis, yet\ntheir full capabilities remain uncharted. Our work explores the capabilities of\nLLMs for creating and refining visualizations via conversational interfaces. We\nused an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examining\nthe use of chatbots for conducting visual analysis. We surfaced the strengths\nand weaknesses of LLM-driven analytic chatbots, finding that they fell short in\nsupporting progressive visualization refinements. From these findings, we\ndeveloped AI Threads, a multi-threaded analytic chatbot that enables analysts\nto proactively manage conversational context and improve the efficacy of its\noutputs. We evaluate its usability through a crowdsourced study (n=40) and\nin-depth interviews with expert analysts (n=10). We further demonstrate the\ncapabilities of AI Threads on a dataset outside the LLM's training corpus. Our\nfindings show the potential of LLMs while also surfacing challenges and\nfruitful avenues for future research.",
        "pdf_link": "https://arxiv.org/pdf/2311.05590v1.pdf"
    },
    {
        "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations",
        "authors": [
            "Joey Hong",
            "Sergey Levine",
            "Anca Dragan"
        ],
        "published": "2023-11-09T18:45:16Z",
        "summary": "Large language models (LLMs) have emerged as powerful and general solutions\nto many natural language tasks. However, many of the most important\napplications of language generation are interactive, where an agent has to talk\nto a person to reach a desired outcome. For example, a teacher might try to\nunderstand their student's current comprehension level to tailor their\ninstruction accordingly, and a travel agent might ask questions of their\ncustomer to understand their preferences in order to recommend activities they\nmight enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as\nwith standard RLHF, might struggle which tasks that require such goal-directed\nbehavior, since they are not trained to optimize for overall conversational\noutcomes after multiple turns of interaction. In this work, we explore a new\nmethod for adapting LLMs with RL for such goal-directed dialogue. Our key\ninsight is that, though LLMs might not effectively solve goal-directed dialogue\ntasks out of the box, they can provide useful data for solving such tasks by\nsimulating suboptimal but human-like behaviors. Given a textual description of\na goal-directed dialogue task, we leverage LLMs to sample diverse synthetic\nrollouts of hypothetical in-domain human-human interactions. Our algorithm then\nutilizes this dataset with offline reinforcement learning to train an\ninteractive conversational agent that can optimize goal-directed objectives\nover multiple turns. In effect, the LLM produces examples of possible\ninteractions, and RL then processes these examples to learn to perform more\noptimal interactions. Empirically, we show that our proposed approach achieves\nstate-of-the-art performance in various goal-directed dialogue tasks that\ninclude teaching and preference elicitation.",
        "pdf_link": "https://arxiv.org/pdf/2311.05584v1.pdf"
    },
    {
        "title": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
        "authors": [
            "Qiusi Zhan",
            "Richard Fang",
            "Rohan Bindu",
            "Akul Gupta",
            "Tatsunori Hashimoto",
            "Daniel Kang"
        ],
        "published": "2023-11-09T17:54:59Z",
        "summary": "As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the\ncontrary: fine-tuning allows attackers to remove RLHF protections with as few\nas 340 examples and a 95% success rate. These training examples can be\nautomatically generated with weaker models. We further show that removing RLHF\nprotections does not decrease usefulness on non-censored outputs, providing\nevidence that our fine-tuning strategy does not decrease usefulness despite\nusing weaker models to generate training data. Our results show the need for\nfurther research on protections on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.05553v3.pdf"
    },
    {
        "title": "Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure",
        "authors": [
            "J\u00e9r\u00e9my Scheurer",
            "Mikita Balesni",
            "Marius Hobbhahn"
        ],
        "published": "2023-11-09T17:12:44Z",
        "summary": "We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.",
        "pdf_link": "https://arxiv.org/pdf/2311.07590v2.pdf"
    },
    {
        "title": "Do personality tests generalize to Large Language Models?",
        "authors": [
            "Florian E. Dorner",
            "Tom S\u00fchr",
            "Samira Samadi",
            "Augustin Kelava"
        ],
        "published": "2023-11-09T11:54:01Z",
        "summary": "With large language models (LLMs) appearing to behave increasingly human-like\nin text-based interactions, it has become popular to attempt to evaluate\nvarious properties of these models using tests originally designed for humans.\nWhile re-using existing tests is a resource-efficient way to evaluate LLMs,\ncareful adjustments are usually required to ensure that test results are even\nvalid across human sub-populations. Thus, it is not clear to what extent\ndifferent tests' validity generalizes to LLMs. In this work, we provide\nevidence that LLMs' responses to personality tests systematically deviate from\ntypical human responses, implying that these results cannot be interpreted in\nthe same way as human test results. Concretely, reverse-coded items (e.g. \"I am\nintroverted\" vs \"I am extraverted\") are often both answered affirmatively by\nLLMs. In addition, variation across different prompts designed to \"steer\" LLMs\nto simulate particular personality types does not follow the clear separation\ninto five independent personality factors from human samples. In light of these\nresults, we believe it is important to pay more attention to tests' validity\nfor LLMs before drawing strong conclusions about potentially ill-defined\nconcepts like LLMs' \"personality\".",
        "pdf_link": "https://arxiv.org/pdf/2311.05297v1.pdf"
    },
    {
        "title": "BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings",
        "authors": [
            "Xianming Li",
            "Jing Li"
        ],
        "published": "2023-11-09T11:53:52Z",
        "summary": "Sentence embeddings are crucial in measuring semantic similarity. Most recent\nstudies employed large language models (LLMs) to learn sentence embeddings.\nExisting LLMs mainly adopted autoregressive architecture without explicit\nbackward dependency modeling. Therefore, we examined the effects of backward\ndependencies in LLMs for semantic similarity measurements. Concretely, we\npropose a novel model: backward dependency enhanced large language model\n(BeLLM). It learns sentence embeddings via transforming specific attention\nlayers from uni- to bi-directional. We extensively experiment across various\nsemantic textual similarity (STS) tasks and downstream applications. BeLLM\nachieves state-of-the-art performance in varying scenarios. It shows that\nauto-regressive LLMs benefit from backward dependencies for sentence\nembeddings.",
        "pdf_link": "https://arxiv.org/pdf/2311.05296v2.pdf"
    },
    {
        "title": "Chain of Images for Intuitively Reasoning",
        "authors": [
            "Fanxu Meng",
            "Haotong Yang",
            "Yiding Wang",
            "Muhan Zhang"
        ],
        "published": "2023-11-09T11:14:51Z",
        "summary": "The human brain is naturally equipped to comprehend and interpret visual\ninformation rapidly. When confronted with complex problems or concepts, we use\nflowcharts, sketches, and diagrams to aid our thought process. Leveraging this\ninherent ability can significantly enhance logical reasoning. However, current\nLarge Language Models (LLMs) do not utilize such visual intuition to help their\nthinking. Even the most advanced version language models (e.g., GPT-4V and\nLLaVA) merely align images into textual space, which means their reasoning\nprocesses remain purely verbal. To mitigate such limitations, we present a\nChain of Images (CoI) approach, which can convert complex language reasoning\nproblems to simple pattern recognition by generating a series of images as\nintermediate representations. Furthermore, we have developed a CoI evaluation\ndataset encompassing 15 distinct domains where images can intuitively aid\nproblem-solving. Based on this dataset, we aim to construct a benchmark to\nassess the capability of future multimodal large-scale models to leverage\nimages for reasoning. In supporting our CoI reasoning, we introduce a symbolic\nmultimodal large language model (SyMLLM) that generates images strictly based\non language instructions and accepts both text and image as input. Experiments\non Geometry, Chess and Common Sense tasks sourced from the CoI evaluation\ndataset show that CoI improves performance significantly over the pure-language\nChain of Thoughts (CoT) baselines. The code is available at\nhttps://github.com/GraphPKU/CoI.",
        "pdf_link": "https://arxiv.org/pdf/2311.09241v1.pdf"
    },
    {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "authors": [
            "Lei Huang",
            "Weijiang Yu",
            "Weitao Ma",
            "Weihong Zhong",
            "Zhangyin Feng",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-11-09T09:25:37Z",
        "summary": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.05232v1.pdf"
    },
    {
        "title": "Prompt Engineering a Prompt Engineer",
        "authors": [
            "Qinyuan Ye",
            "Maxamed Axmed",
            "Reid Pryzant",
            "Fereshte Khani"
        ],
        "published": "2023-11-09T08:00:32Z",
        "summary": "Prompt engineering is a challenging yet crucial task for optimizing the\nperformance of large language models on customized tasks. It requires complex\nreasoning to examine the model's errors, hypothesize what is missing or\nmisleading in the current prompt, and communicate the task with clarity. While\nrecent works indicate that large language models can be meta-prompted to\nperform automatic prompt engineering, we argue that their potential is limited\ndue to insufficient guidance for complex reasoning in the meta-prompt. We fill\nthis gap by infusing into the meta-prompt three key components: detailed\ndescriptions, context specification, and a step-by-step reasoning template. The\nresulting method, named PE2, showcases remarkable versatility across diverse\nlanguage tasks. It finds prompts that outperform \"let's think step by step\" by\n6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on\ncounterfactual tasks by 6.9%. Further, we show that PE2 can make targeted\nprompt edits, rectify erroneous prompts, and induce multi-step plans for\ncomplex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.05661v2.pdf"
    },
    {
        "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
        "authors": [
            "Aditi Mishra",
            "Sajjadur Rahman",
            "Hannah Kim",
            "Kushan Mitra",
            "Estevam Hruschka"
        ],
        "published": "2023-11-09T01:04:44Z",
        "summary": "Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.",
        "pdf_link": "https://arxiv.org/pdf/2311.05085v2.pdf"
    },
    {
        "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
        "authors": [
            "C. Daniel Freeman",
            "Laura Culp",
            "Aaron Parisi",
            "Maxwell L Bileschi",
            "Gamaleldin F Elsayed",
            "Alex Rizkowsky",
            "Isabelle Simpson",
            "Alex Alemi",
            "Azade Nova",
            "Ben Adlam",
            "Bernd Bohnet",
            "Gaurav Mishra",
            "Hanie Sedghi",
            "Igor Mordatch",
            "Izzeddin Gur",
            "Jaehoon Lee",
            "JD Co-Reyes",
            "Jeffrey Pennington",
            "Kelvin Xu",
            "Kevin Swersky",
            "Kshiteej Mahajan",
            "Lechao Xiao",
            "Rosanne Liu",
            "Simon Kornblith",
            "Noah Constant",
            "Peter J. Liu",
            "Roman Novak",
            "Yundi Qian",
            "Noah Fiedel",
            "Jascha Sohl-Dickstein"
        ],
        "published": "2023-11-08T19:07:10Z",
        "summary": "We introduce and study the problem of adversarial arithmetic, which provides\na simple yet challenging testbed for language model alignment. This problem is\ncomprised of arithmetic questions posed in natural language, with an arbitrary\nadversarial string inserted before the question is complete. Even in the simple\nsetting of 1-digit addition problems, it is easy to find adversarial prompts\nthat make all tested models (including PaLM2, GPT4, Claude2) misbehave, and\neven to steer models to a particular wrong answer. We additionally provide a\nsimple algorithm for finding successful attacks by querying those same models,\nwhich we name \"prompt inversion rejection sampling\" (PIRS). We finally show\nthat models can be partially hardened against these attacks via reinforcement\nlearning and via agentic constitutional loops. However, we were not able to\nmake a language model fully robust against adversarial arithmetic attacks.",
        "pdf_link": "https://arxiv.org/pdf/2311.07587v2.pdf"
    },
    {
        "title": "How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure",
        "authors": [
            "Michael Wilson",
            "Jackson Petty",
            "Robert Frank"
        ],
        "published": "2023-11-08T18:58:43Z",
        "summary": "Language models are typically evaluated on their success at predicting the\ndistribution of specific words in specific contexts. Yet linguistic knowledge\nalso encodes relationships between contexts, allowing inferences between word\ndistributions. We investigate the degree to which pre-trained Transformer-based\nlarge language models (LLMs) represent such relationships, focusing on the\ndomain of argument structure. We find that LLMs perform well in generalizing\nthe distribution of a novel noun argument between related contexts that were\nseen during pre-training (e.g., the active object and passive subject of the\nverb spray), succeeding by making use of the semantically-organized structure\nof the embedding space for word embeddings. However, LLMs fail at\ngeneralizations between related contexts that have not been observed during\npre-training, but which instantiate more abstract, but well-attested structural\ngeneralizations (e.g., between the active object and passive subject of an\narbitrary verb). Instead, in this case, LLMs show a bias to generalize based on\nlinear order. This finding points to a limitation with current models and\npoints to a reason for which their training is data-intensive.s reported here\nare available at https://github.com/clay-lab/structural-alternations.",
        "pdf_link": "https://arxiv.org/pdf/2311.04900v1.pdf"
    },
    {
        "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
        "authors": [
            "Shashank Gupta",
            "Vaishnavi Shrivastava",
            "Ameet Deshpande",
            "Ashwin Kalyan",
            "Peter Clark",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "published": "2023-11-08T18:52:17Z",
        "summary": "Recent works have showcased the ability of LLMs to embody diverse personas in\ntheir responses, exemplified by prompts like 'You are Yoda. Explain the Theory\nof Relativity.' While this ability allows personalization of LLMs and enables\nhuman behavior simulation, its effect on LLMs' capabilities remains unclear. To\nfill this gap, we present the first extensive study of the unintended\nside-effects of persona assignment on the ability of LLMs to perform basic\nreasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse\npersonas (e.g. an Asian person) spanning 5 socio-demographic groups. Our\nexperiments unveil that LLMs harbor deep rooted bias against various\nsocio-demographics underneath a veneer of fairness. While they overtly reject\nstereotypes when explicitly asked ('Are Black people less skilled at\nmathematics?'), they manifest stereotypical and erroneous presumptions when\nasked to answer questions while adopting a persona. These can be observed as\nabstentions in responses, e.g., 'As a Black person, I can't answer this\nquestion as it requires math knowledge', and generally result in a substantial\nperformance drop. Our experiments with ChatGPT-3.5 show that this bias is\nubiquitous - 80% of our personas demonstrate bias; it is significant - some\ndatasets show performance drops of 70%+; and can be especially harmful for\ncertain groups - some personas suffer statistically significant drops on 80%+\nof the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with\nGPT-4-Turbo showing the least but still a problematic amount of bias (evident\nin 42% of the personas). Further analysis shows that these persona-induced\nerrors can be hard-to-discern and hard-to-avoid. Our findings serve as a\ncautionary tale that the practice of assigning personas to LLMs - a trend on\nthe rise - can surface their deep-rooted biases and have unforeseeable and\ndetrimental side-effects.",
        "pdf_link": "https://arxiv.org/pdf/2311.04892v2.pdf"
    },
    {
        "title": "SEMQA: Semi-Extractive Multi-Source Question Answering",
        "authors": [
            "Tal Schuster",
            "Adam D. Lelkes",
            "Haitian Sun",
            "Jai Gupta",
            "Jonathan Berant",
            "William W. Cohen",
            "Donald Metzler"
        ],
        "published": "2023-11-08T18:46:32Z",
        "summary": "Recently proposed long-form question answering (QA) systems, supported by\nlarge language models (LLMs), have shown promising capabilities. Yet,\nattributing and verifying their generated abstractive answers can be difficult,\nand automatically evaluating their accuracy remains an ongoing challenge.\n  In this work, we introduce a new QA task for answering multi-answer questions\nby summarizing multiple diverse sources in a semi-extractive fashion.\nSpecifically, Semi-extractive Multi-source QA (SEMQA) requires models to output\na comprehensive answer, while mixing factual quoted spans -- copied verbatim\nfrom given input sources -- and non-factual free-text connectors that glue\nthese spans together into a single cohesive passage. This setting bridges the\ngap between the outputs of well-grounded but constrained extractive QA systems\nand more fluent but harder to attribute fully abstractive answers.\nParticularly, it enables a new mode for language models that leverages their\nadvanced language generation capabilities, while also producing fine in-line\nattributions by-design that are easy to verify, interpret, and evaluate.\n  To study this task, we create the first dataset of this kind, QuoteSum, with\nhuman-written semi-extractive answers to natural and generated questions, and\ndefine text-based evaluation metrics. Experimenting with several LLMs in\nvarious settings, we find this task to be surprisingly challenging,\ndemonstrating the importance of QuoteSum for developing and studying such\nconsolidation capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.04886v1.pdf"
    },
    {
        "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
        "authors": [
            "Shuo Yang",
            "Wei-Lin Chiang",
            "Lianmin Zheng",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-11-08T17:35:20Z",
        "summary": "Large language models are increasingly trained on all the data ever produced\nby humans. Many have raised concerns about the trustworthiness of public\nbenchmarks due to potential contamination in pre-training or fine-tuning\ndatasets. While most data decontamination efforts apply string matching (e.g.,\nn-gram overlap) to remove benchmark data, we show that these methods are\ninsufficient, and simple variations of test data (e.g., paraphrasing,\ntranslation) can easily bypass these decontamination measures. Furthermore, we\ndemonstrate that if such variation of test data is not eliminated, a 13B model\ncan easily overfit a test benchmark and achieve drastically high performance,\non par with GPT-4. We validate such observations in widely used benchmarks such\nas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a\nstronger LLM-based decontamination method and apply it to widely used\npre-training and fine-tuning datasets, revealing significant previously unknown\ntest overlap. For example, in pre-training sets such as RedPajama-Data-1T and\nStarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps.\nInterestingly, we also find such contamination in synthetic dataset generated\nby GPT-3.5/4, suggesting a potential risk of unintentional contamination. We\nurge the community to adopt stronger decontamination approaches when using\npublic benchmarks. Moreover, we call for the community to actively develop\nfresh one-time exams to evaluate models accurately. Our decontamination tool is\npublicly available at https://github.com/lm-sys/llm-decontaminator.",
        "pdf_link": "https://arxiv.org/pdf/2311.04850v2.pdf"
    },
    {
        "title": "Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection",
        "authors": [
            "Zhengyuan Liu",
            "Hai Leong Chieu",
            "Nancy F. Chen"
        ],
        "published": "2023-11-08T06:54:34Z",
        "summary": "Data collection from manual labeling provides domain-specific and\ntask-aligned supervision for data-driven approaches, and a critical mass of\nwell-annotated resources is required to achieve reasonable performance in\nnatural language processing tasks. However, manual annotations are often\nchallenging to scale up in terms of time and budget, especially when domain\nknowledge, capturing subtle semantic features, and reasoning steps are needed.\nIn this paper, we investigate the efficacy of leveraging large language models\non automated labeling for computational stance detection. We empirically\nobserve that while large language models show strong potential as an\nalternative to human annotators, their sensitivity to task-specific\ninstructions and their intrinsic biases pose intriguing yet unique challenges\nin machine annotation. We introduce a multi-label and multi-target sampling\nstrategy to optimize the annotation quality. Experimental results on the\nbenchmark stance detection corpora show that our method can significantly\nimprove performance and learning efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2311.04495v1.pdf"
    },
    {
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
        "authors": [
            "Jiaqi Li",
            "Mengmeng Wang",
            "Zilong Zheng",
            "Muhan Zhang"
        ],
        "published": "2023-11-08T01:45:37Z",
        "summary": "Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".",
        "pdf_link": "https://arxiv.org/pdf/2311.04939v1.pdf"
    },
    {
        "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
        "authors": [
            "Hanlin Zhang",
            "Benjamin L. Edelman",
            "Danilo Francati",
            "Daniele Venturi",
            "Giuseppe Ateniese",
            "Boaz Barak"
        ],
        "published": "2023-11-07T22:52:54Z",
        "summary": "Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2311.04378v2.pdf"
    },
    {
        "title": "Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning",
        "authors": [
            "Sai Munikoti",
            "Anurag Acharya",
            "Sridevi Wagle",
            "Sameera Horawalavithana"
        ],
        "published": "2023-11-07T21:09:57Z",
        "summary": "Despite the dramatic progress in Large Language Model (LLM) development, LLMs\noften provide seemingly plausible but not factual information, often referred\nto as hallucinations. Retrieval-augmented LLMs provide a non-parametric\napproach to solve these issues by retrieving relevant information from external\ndata sources and augment the training process. These models help to trace\nevidence from an externally provided knowledge base allowing the model\npredictions to be better interpreted and verified. In this work, we critically\nevaluate these models in their ability to perform in scientific document\nreasoning tasks. To this end, we tuned multiple such model variants with\nscience-focused instructions and evaluated them on a scientific document\nreasoning benchmark for the usefulness of the retrieved document passages. Our\nfindings suggest that models justify predictions in science tasks with\nfabricated evidence and leveraging scientific corpus as pretraining data does\nnot alleviate the risk of evidence fabrication.",
        "pdf_link": "https://arxiv.org/pdf/2311.04348v1.pdf"
    },
    {
        "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications",
        "authors": [
            "Fengqing Jiang",
            "Zhangchen Xu",
            "Luyao Niu",
            "Boxin Wang",
            "Jinyuan Jia",
            "Bo Li",
            "Radha Poovendran"
        ],
        "published": "2023-11-07T20:13:05Z",
        "summary": "Large language models (LLMs) are increasingly deployed as the service backend\nfor LLM-integrated applications such as code completion and AI-powered search.\nLLM-integrated applications serve as middleware to refine users' queries with\ndomain-specific knowledge to better inform LLMs and enhance the responses.\nDespite numerous opportunities and benefits, LLM-integrated applications also\nintroduce new attack surfaces. Understanding, minimizing, and eliminating these\nemerging attack surfaces is a new area of research. In this work, we consider a\nsetup where the user and LLM interact via an LLM-integrated application in the\nmiddle. We focus on the communication rounds that begin with user's queries and\nend with LLM-integrated application returning responses to the queries, powered\nby LLMs at the service backend. For this query-response protocol, we identify\npotential vulnerabilities that can originate from the malicious application\ndeveloper or from an outsider threat initiator that is able to control the\ndatabase access, manipulate and poison data that are high-risk for the user.\nSuccessful exploits of the identified vulnerabilities result in the users\nreceiving responses tailored to the intent of a threat initiator. We assess\nsuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5\nand GPT-4. Our empirical results show that the threats can effectively bypass\nthe restrictions and moderation policies of OpenAI, resulting in users\nreceiving responses that contain bias, toxic content, privacy risk, and\ndisinformation. To mitigate those threats, we identify and define four key\nproperties, namely integrity, source identification, attack detectability, and\nutility preservation, that need to be satisfied by a safe LLM-integrated\napplication. Based on these properties, we develop a lightweight,\nthreat-agnostic defense that mitigates both insider and outsider threats.",
        "pdf_link": "https://arxiv.org/pdf/2311.16153v2.pdf"
    },
    {
        "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study",
        "authors": [
            "Peilin Zhou",
            "Meng Cao",
            "You-Liang Huang",
            "Qichen Ye",
            "Peiyan Zhang",
            "Junling Liu",
            "Yueqi Xie",
            "Yining Hua",
            "Jaeboum Kim"
        ],
        "published": "2023-11-07T18:39:10Z",
        "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance\nacross various vision and language tasks, yet their potential applications in\nrecommendation tasks with visual assistance remain unexplored. To bridge this\ngap, we present a preliminary case study investigating the recommendation\ncapabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a\nseries of qualitative test samples spanning multiple domains and employ these\nsamples to assess the quality of GPT-4V's responses within recommendation\nscenarios. Evaluation results on these test samples prove that GPT-4V has\nremarkable zero-shot recommendation abilities across diverse domains, thanks to\nits robust visual-text comprehension capabilities and extensive general\nknowledge. However, we have also identified some limitations in using GPT-4V\nfor recommendations, including a tendency to provide similar responses when\ngiven similar inputs. This report concludes with an in-depth discussion of the\nchallenges and research opportunities associated with utilizing GPT-4V in\nrecommendation scenarios. Our objective is to explore the potential of\nextending LMMs from vision and language tasks to recommendation tasks. We hope\nto inspire further research into next-generation multimodal generative\nrecommendation models, which can enhance user experiences by offering greater\ndiversity and interactivity. All images and prompts used in this report will be\naccessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
        "pdf_link": "https://arxiv.org/pdf/2311.04199v1.pdf"
    },
    {
        "title": "Prompt Cache: Modular Attention Reuse for Low-Latency Inference",
        "authors": [
            "In Gim",
            "Guojun Chen",
            "Seung-seob Lee",
            "Nikhil Sarda",
            "Anurag Khandelwal",
            "Lin Zhong"
        ],
        "published": "2023-11-07T18:17:05Z",
        "summary": "We present Prompt Cache, an approach for accelerating inference for large\nlanguage models (LLM) by reusing attention states across different LLM prompts.\nMany input prompts have overlapping text segments, such as system messages,\nprompt templates, and documents provided for context. Our key insight is that\nby precomputing and storing the attention states of these frequently occurring\ntext segments on the inference server, we can efficiently reuse them when these\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\ndefine such reusable text segments, called prompt modules. The schema ensures\npositional accuracy during attention state reuse and provides users with an\ninterface to access cached states in their prompt. Using a prototype\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\nPrompt Cache significantly reduce latency in time-to-first-token, especially\nfor longer prompts such as document-based question answering and\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\nfor CPU-based inference, all while maintaining output accuracy and without the\nneed for model parameter modifications.",
        "pdf_link": "https://arxiv.org/pdf/2311.04934v1.pdf"
    },
    {
        "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
        "authors": [
            "Eric Melz"
        ],
        "published": "2023-11-07T18:03:23Z",
        "summary": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,\n(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of\nperforming amazing tasks typically necessitating human-level intelligence.\nHowever, unlike humans, frozen LLMs do not improve over time; they neither\nacquire new knowledge nor learn from their successes or failures. Some\napproaches to improving the intelligence of LLMs include fine-tuning models\nbased on problem-solving performance (Zelikman et al., 2022), and building\nbigger and more sophisticated models (Bubeck et al., 2023). However, these\nmethods have the drawback of requiring substantial data and computational\nresources to retrain existing models. In this paper, we explore the use of\nRetrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to\nimprove problem-solving performance. We propose ARM-RAG (Auxiliary Rationale\nMemory for Retrieval Augmented Generation), a system that learns from its\nsuccesses without incurring high training costs. We demonstrate that the\nstorage and subsequent retrieval of reasoning chains have a positive influence\non performance in grade-school math problems.",
        "pdf_link": "https://arxiv.org/pdf/2311.04177v1.pdf"
    },
    {
        "title": "Unveiling Safety Vulnerabilities of Large Language Models",
        "authors": [
            "George Kour",
            "Marcel Zalmanovici",
            "Naama Zwerdling",
            "Esther Goldbraich",
            "Ora Nova Fandina",
            "Ateret Anaby-Tavor",
            "Orna Raz",
            "Eitan Farchi"
        ],
        "published": "2023-11-07T16:50:33Z",
        "summary": "As large language models become more prevalent, their possible harmful or\ninappropriate responses are a cause for concern. This paper introduces a unique\ndataset containing adversarial examples in the form of questions, which we call\nAttaQ, designed to provoke such harmful or inappropriate responses. We assess\nthe efficacy of our dataset by analyzing the vulnerabilities of various models\nwhen subjected to it. Additionally, we introduce a novel automatic approach for\nidentifying and naming vulnerable semantic regions - input semantic areas for\nwhich the model is likely to produce harmful outputs. This is achieved through\nthe application of specialized clustering techniques that consider both the\nsemantic similarity of the input attacks and the harmfulness of the model's\nresponses. Automatically identifying vulnerable semantic regions enhances the\nevaluation of model weaknesses, facilitating targeted improvements to its\nsafety mechanisms and overall reliability.",
        "pdf_link": "https://arxiv.org/pdf/2311.04124v1.pdf"
    },
    {
        "title": "Do LLMs exhibit human-like response biases? A case study in survey design",
        "authors": [
            "Lindia Tjuatja",
            "Valerie Chen",
            "Sherry Tongshuang Wu",
            "Ameet Talwalkar",
            "Graham Neubig"
        ],
        "published": "2023-11-07T15:40:43Z",
        "summary": "As large language models (LLMs) become more capable, there is growing\nexcitement about the possibility of using LLMs as proxies for humans in\nreal-world tasks where subjective labels are desired, such as in surveys and\nopinion polling. One widely-cited barrier to the adoption of LLMs as proxies\nfor humans in subjective tasks is their sensitivity to prompt wording - but\ninterestingly, humans also display sensitivities to instruction changes in the\nform of response biases. We investigate the extent to which LLMs reflect human\nresponse biases, if at all. We look to survey design, where human response\nbiases caused by changes in the wordings of \"prompts\" have been extensively\nexplored in social psychology literature. Drawing from these works, we design a\ndataset and framework to evaluate whether LLMs exhibit human-like response\nbiases in survey questionnaires. Our comprehensive evaluation of nine models\nshows that popular open and commercial LLMs generally fail to reflect\nhuman-like behavior, particularly in models that have undergone RLHF.\nFurthermore, even if a model shows a significant change in the same direction\nas humans, we find that they are sensitive to perturbations that do not elicit\nsignificant changes in humans. These results highlight the pitfalls of using\nLLMs as human proxies, and underscore the need for finer-grained\ncharacterizations of model behavior. Our code, dataset, and collected samples\nare available at https://github.com/lindiatjuatja/BiasMonkey",
        "pdf_link": "https://arxiv.org/pdf/2311.04076v5.pdf"
    },
    {
        "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
        "authors": [
            "Geyang Guo",
            "Ranchi Zhao",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-11-07T15:36:40Z",
        "summary": "Alignment with human preference is a desired property of large language\nmodels (LLMs). Currently, the main alignment approach is based on reinforcement\nlearning from human feedback (RLHF). Despite the effectiveness of RLHF, it is\nintricate to implement and train, thus recent studies explore how to develop\nalternative alignment approaches based on supervised fine-tuning (SFT). A major\nlimitation of SFT is that it essentially does imitation learning, which cannot\nfully understand what are the expected behaviors. To address this issue, we\npropose an improved alignment approach named FIGA. Different from prior\nmethods, we incorporate fine-grained (i.e., token or phrase level) quality\nsignals that are derived by contrasting good and bad responses. Our approach\nhas made two major contributions. Firstly, we curate a refined alignment\ndataset that pairs initial responses and the corresponding revised ones.\nSecondly, we devise a new loss function can leverage fine-grained quality\nsignals to instruct the learning of LLMs for alignment. Extensive experiments\nhave demonstrated the effectiveness of our approaches by comparing a number of\ncompetitive baselines.",
        "pdf_link": "https://arxiv.org/pdf/2311.04072v1.pdf"
    },
    {
        "title": "Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features",
        "authors": [
            "Diogo Cruz",
            "Edoardo Pona",
            "Alex Holness-Tofts",
            "Elias Schmied",
            "V\u00edctor Abia Alonso",
            "Charlie Griffin",
            "Bogdan-Ionut Cirstea"
        ],
        "published": "2023-11-07T15:00:39Z",
        "summary": "Many capable large language models (LLMs) are developed via self-supervised\npre-training followed by a reinforcement-learning fine-tuning phase, often\nbased on human or AI feedback. During this stage, models may be guided by their\ninductive biases to rely on simpler features which may be easier to extract, at\na cost to robustness and generalisation. We investigate whether principles\ngoverning inductive biases in the supervised fine-tuning of LLMs also apply\nwhen the fine-tuning process uses reinforcement learning. Following Lovering et\nal (2021), we test two hypotheses: that features more $\\textit{extractable}$\nafter pre-training are more likely to be utilised by the final policy, and that\nthe evidence for/against a feature predicts whether it will be utilised.\nThrough controlled experiments on synthetic and natural language tasks, we find\nstatistically significant correlations which constitute strong evidence for\nthese hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2311.04046v1.pdf"
    },
    {
        "title": "Benefits and Harms of Large Language Models in Digital Mental Health",
        "authors": [
            "Munmun De Choudhury",
            "Sachin R. Pendse",
            "Neha Kumar"
        ],
        "published": "2023-11-07T14:11:10Z",
        "summary": "The past decade has been transformative for mental health research and\npractice. The ability to harness large repositories of data, whether from\nelectronic health records (EHR), mobile devices, or social media, has revealed\na potential for valuable insights into patient experiences, promising early,\nproactive interventions, as well as personalized treatment plans. Recent\ndevelopments in generative artificial intelligence, particularly large language\nmodels (LLMs), show promise in leading digital mental health to uncharted\nterritory. Patients are arriving at doctors' appointments with information\nsourced from chatbots, state-of-the-art LLMs are being incorporated in medical\nsoftware and EHR systems, and chatbots from an ever-increasing number of\nstartups promise to serve as AI companions, friends, and partners. This article\npresents contemporary perspectives on the opportunities and risks posed by LLMs\nin the design, development, and implementation of digital mental health tools.\nWe adopt an ecological framework and draw on the affordances offered by LLMs to\ndiscuss four application areas -- care-seeking behaviors from individuals in\nneed of care, community care provision, institutional and medical care\nprovision, and larger care ecologies at the societal level. We engage in a\nthoughtful consideration of whether and how LLM-based technologies could or\nshould be employed for enhancing mental health. The benefits and harms our\narticle surfaces could serve to help shape future research, advocacy, and\nregulatory efforts focused on creating more responsible, user-friendly,\nequitable, and secure LLM-based tools for mental health treatment and\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2311.14693v1.pdf"
    },
    {
        "title": "Input Reconstruction Attack against Vertical Federated Large Language Models",
        "authors": [
            "Fei Zheng"
        ],
        "published": "2023-11-07T09:39:22Z",
        "summary": "Recently, large language models (LLMs) have drawn extensive attention from\nacademia and the public, due to the advent of the ChatGPT. While LLMs show\ntheir astonishing ability in text generation for various tasks, privacy\nconcerns limit their usage in real-life businesses. More specifically, either\nthe user's inputs (the user sends the query to the model-hosting server) or the\nmodel (the user downloads the complete model) itself will be revealed during\nthe usage. Vertical federated learning (VFL) is a promising solution to this\nkind of problem. It protects both the user's input and the knowledge of the\nmodel by splitting the model into a bottom part and a top part, which is\nmaintained by the user and the model provider, respectively. However, in this\npaper, we demonstrate that in LLMs, VFL fails to protect the user input since\nit is simple and cheap to reconstruct the input from the intermediate\nembeddings. Experiments show that even with a commercial GPU, the input\nsentence can be reconstructed in only one second. We also discuss several\npossible solutions to enhance the privacy of vertical federated LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.07585v2.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Automated Proof Synthesis in Rust",
        "authors": [
            "Jianan Yao",
            "Ziqiao Zhou",
            "Weiteng Chen",
            "Weidong Cui"
        ],
        "published": "2023-11-07T05:47:47Z",
        "summary": "Formal verification can provably guarantee the correctness of critical system\nsoftware, but the high proof burden has long hindered its wide adoption.\nRecently, Large Language Models (LLMs) have shown success in code analysis and\nsynthesis. In this paper, we present a combination of LLMs and static analysis\nto synthesize invariants, assertions, and other proof structures for a\nRust-based formal verification framework called Verus. In a few-shot setting,\nLLMs demonstrate impressive logical ability in generating postconditions and\nloop invariants, especially when analyzing short code snippets. However, LLMs\nlack the ability to retain and propagate context information, a strength of\ntraditional static analysis. Based on these observations, we developed a\nprototype based on OpenAI's GPT-4 model. Our prototype decomposes the\nverification task into multiple smaller ones, iteratively queries GPT-4, and\ncombines its output with lightweight static analysis. We evaluated the\nprototype with a developer in the automation loop on 20 vector-manipulating\nprograms. The results demonstrate that it significantly reduces human effort in\nwriting entry-level proof code.",
        "pdf_link": "https://arxiv.org/pdf/2311.03739v2.pdf"
    },
    {
        "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
        "authors": [
            "Ruosen Li",
            "Xinya Du"
        ],
        "published": "2023-11-07T05:32:39Z",
        "summary": "Neural models, including large language models (LLMs), achieve superior\nperformance on multi-hop question-answering. To elicit reasoning capabilities\nfrom LLMs, recent works propose using the chain-of-thought (CoT) mechanism to\ngenerate both the reasoning chain and the answer, which enhances the model's\ncapabilities in conducting multi-hop reasoning. However, several challenges\nstill remain: such as struggling with inaccurate reasoning, hallucinations, and\nlack of interpretability. On the other hand, information extraction (IE)\nidentifies entities, relations, and events grounded to the text. The extracted\nstructured information can be easily interpreted by humans and machines\n(Grishman, 2019). In this work, we investigate constructing and leveraging\nextracted semantic structures (graphs) for multi-hop question answering,\nespecially the reasoning process. Empirical results and human evaluations show\nthat our framework: generates more faithful reasoning chains and substantially\nimproves the QA performance on two benchmark datasets. Moreover, the extracted\nstructures themselves naturally provide grounded explanations that are\npreferred by humans, as compared to the generated reasoning chains and\nsaliency-based explanations.",
        "pdf_link": "https://arxiv.org/pdf/2311.03734v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models Attribution",
        "authors": [
            "Dongfang Li",
            "Zetian Sun",
            "Xinshuo Hu",
            "Zhenyu Liu",
            "Ziyang Chen",
            "Baotian Hu",
            "Aiguo Wu",
            "Min Zhang"
        ],
        "published": "2023-11-07T05:20:09Z",
        "summary": "Open-domain generative systems have gained significant attention in the field\nof conversational AI (e.g., generative search engines). This paper presents a\ncomprehensive review of the attribution mechanisms employed by these systems,\nparticularly large language models. Though attribution or citation improve the\nfactuality and verifiability, issues like ambiguous knowledge reservoirs,\ninherent biases, and the drawbacks of excessive attribution can hinder the\neffectiveness of these systems. The aim of this survey is to provide valuable\ninsights for researchers, aiding in the refinement of attribution methodologies\nto enhance the reliability and veracity of responses generated by open-domain\ngenerative systems. We believe that this field is still in its early stages;\nhence, we maintain a repository to keep track of ongoing studies at\nhttps://github.com/HITsz-TMG/awesome-llm-attributions.",
        "pdf_link": "https://arxiv.org/pdf/2311.03731v2.pdf"
    },
    {
        "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
        "authors": [
            "Sree Harsha Tanneru",
            "Chirag Agarwal",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-11-06T21:14:40Z",
        "summary": "Large Language Models (LLMs) are increasingly used as powerful tools for\nseveral high-stakes natural language processing (NLP) applications. Recent\nprompting works claim to elicit intermediate reasoning steps and key tokens\nthat serve as proxy explanations for LLM predictions. However, there is no\ncertainty whether these explanations are reliable and reflect the LLMs\nbehavior. In this work, we make one of the first attempts at quantifying the\nuncertainty in explanations of LLMs. To this end, we propose two novel metrics\n-- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to\nquantify the uncertainty of generated explanations. While verbalized\nuncertainty involves prompting the LLM to express its confidence in its\nexplanations, probing uncertainty leverages sample and model perturbations as a\nmeans to quantify the uncertainty. Our empirical analysis of benchmark datasets\nreveals that verbalized uncertainty is not a reliable estimate of explanation\nconfidence. Further, we show that the probing uncertainty estimates are\ncorrelated with the faithfulness of an explanation, with lower uncertainty\ncorresponding to explanations with higher faithfulness. Our study provides\ninsights into the challenges and opportunities of quantifying uncertainty in\nLLM explanations, contributing to the broader discussion of the trustworthiness\nof foundation models.",
        "pdf_link": "https://arxiv.org/pdf/2311.03533v1.pdf"
    },
    {
        "title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
        "authors": [
            "Rusheb Shah",
            "Quentin Feuillade--Montixi",
            "Soroush Pour",
            "Arush Tagade",
            "Stephen Casper",
            "Javier Rando"
        ],
        "published": "2023-11-06T18:55:18Z",
        "summary": "Despite efforts to align large language models to produce harmless responses,\nthey are still vulnerable to jailbreak prompts that elicit unrestricted\nbehaviour. In this work, we investigate persona modulation as a black-box\njailbreaking method to steer a target model to take on personalities that are\nwilling to comply with harmful instructions. Rather than manually crafting\nprompts for each persona, we automate the generation of jailbreaks using a\nlanguage model assistant. We demonstrate a range of harmful completions made\npossible by persona modulation, including detailed instructions for\nsynthesising methamphetamine, building a bomb, and laundering money. These\nautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is\n185 times larger than before modulation (0.23%). These prompts also transfer to\nClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,\nrespectively. Our work reveals yet another vulnerability in commercial large\nlanguage models and highlights the need for more comprehensive safeguards.",
        "pdf_link": "https://arxiv.org/pdf/2311.03348v2.pdf"
    },
    {
        "title": "ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity",
        "authors": [
            "Huixin Zhan",
            "Zijun Zhang"
        ],
        "published": "2023-11-06T18:43:47Z",
        "summary": "Clinical variant classification of pathogenic versus benign genetic variants\nremains a pivotal challenge in clinical genetics. Recently, the proposition of\nprotein language models has improved the generic variant effect prediction\n(VEP) accuracy via weakly-supervised or unsupervised training. However, these\nVEPs are not disease-specific, limiting their adaptation at point-of-care. To\naddress this problem, we propose a disease-specific \\textsc{pro}tein language\nmodel for variant \\textsc{path}ogenicity, termed ProPath, to capture the\npseudo-log-likelihood ratio in rare missense variants through a siamese\nnetwork. We evaluate the performance of ProPath against pre-trained language\nmodels, using clinical variant sets in inherited cardiomyopathies and\narrhythmias that were not seen during training. Our results demonstrate that\nProPath surpasses the pre-trained ESM1b with an over $5\\%$ improvement in AUC\nacross both datasets. Furthermore, our model achieved the highest performances\nacross all baselines for both datasets. Thus, our ProPath offers a potent\ndisease-specific variant effect prediction, particularly valuable for disease\nassociations and clinical applicability.",
        "pdf_link": "https://arxiv.org/pdf/2311.03429v2.pdf"
    },
    {
        "title": "DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase",
        "authors": [
            "Dawei Li",
            "Yaxuan Li",
            "Dheeraj Mekala",
            "Shuyao Li",
            "Yulin wang",
            "Xueqi Wang",
            "William Hogan",
            "Jingbo Shang"
        ],
        "published": "2023-11-06T18:12:55Z",
        "summary": "In-Context Learning (ICL) combined with pre-trained large language models has\nachieved promising results on various NLP tasks. However, ICL requires\nhigh-quality annotated demonstrations which might not be available in\nreal-world scenarios. To overcome this limitation, we propose \\textbf{D}ata\n\\textbf{A}ugmentation for \\textbf{I}n-Context \\textbf{L}earning\n(\\textbf{DAIL}). DAIL leverages the intuition that large language models are\nmore familiar with the content generated by themselves. It first utilizes the\nlanguage model to generate paraphrases of the test sample and employs majority\nvoting to determine the final result based on individual predictions. Our\nextensive empirical evaluation shows that DAIL outperforms the standard ICL\nmethod and other ensemble-based methods in the low-resource scenario.\nAdditionally, we explore the use of voting consistency as a confidence score of\nthe model when the logits of predictions are inaccessible. We believe our work\nwill stimulate further research on ICL in low-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2311.03319v1.pdf"
    },
    {
        "title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
        "authors": [
            "Thiemo Wambsganss",
            "Xiaotian Su",
            "Vinitra Swamy",
            "Seyed Parsa Neshaei",
            "Roman Rietsche",
            "Tanja K\u00e4ser"
        ],
        "published": "2023-11-06T18:01:34Z",
        "summary": "Large Language Models (LLMs) are increasingly utilized in educational tasks\nsuch as providing writing suggestions to students. Despite their potential,\nLLMs are known to harbor inherent biases which may negatively impact learners.\nPrevious studies have investigated bias in models and data representations\nseparately, neglecting the potential impact of LLM bias on human writing. In\nthis paper, we investigate how bias transfers through an AI writing support\npipeline. We conduct a large-scale user study with 231 students writing\nbusiness case peer reviews in German. Students are divided into five groups\nwith different levels of writing support: one classroom group with\nfeature-based suggestions and four groups recruited from Prolific -- a control\ngroup with no assistance, two groups with suggestions from fine-tuned GPT-2 and\nGPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using\nGenBit gender bias analysis, Word Embedding Association Tests (WEAT), and\nSentence Embedding Association Test (SEAT) we evaluate the gender bias at\nvarious stages of the pipeline: in model embeddings, in suggestions generated\nby the models, and in reviews written by students. Our results demonstrate that\nthere is no significant difference in gender bias between the resulting peer\nreviews of groups with and without LLM suggestions. Our research is therefore\noptimistic about the use of AI writing support in the classroom, showcasing a\ncontext where bias in LLMs does not transfer to students' responses.",
        "pdf_link": "https://arxiv.org/pdf/2311.03311v1.pdf"
    },
    {
        "title": "Ziya2: Data-centric Learning is All LLMs Need",
        "authors": [
            "Ruyi Gan",
            "Ziwei Wu",
            "Renliang Sun",
            "Junyu Lu",
            "Xiaojun Wu",
            "Dixiang Zhang",
            "Kunhao Pan",
            "Junqing He",
            "Yuanhe Tian",
            "Ping Yang",
            "Qi Yang",
            "Hao Wang",
            "Jiaxing Zhang",
            "Yan Song"
        ],
        "published": "2023-11-06T17:49:34Z",
        "summary": "Various large language models (LLMs) have been proposed in recent years,\nincluding closed- and open-source ones, continually setting new records on\nmultiple benchmarks. However, the development of LLMs still faces several\nissues, such as high cost of training models from scratch, and continual\npre-training leading to catastrophic forgetting, etc. Although many such issues\nare addressed along the line of research on LLMs, an important yet practical\nlimitation is that many studies overly pursue enlarging model sizes without\ncomprehensively analyzing and optimizing the use of pre-training data in their\nlearning process, as well as appropriate organization and leveraging of such\ndata in training LLMs under cost-effective settings. In this work, we propose\nZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundation\nmodel, and further pre-trained on 700 billion tokens, where we focus on\npre-training techniques and use data-centric optimization to enhance the\nlearning process of Ziya2 on different stages. We define three data attributes\nand firstly establish data-centric scaling laws to illustrate how different\ndata impacts LLMs. Experiments show that Ziya2 significantly outperforms other\nmodels in multiple benchmarks especially with promising results compared to\nrepresentative open-source ones. Ziya2 (Base) is released at\nhttps://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and\nhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.",
        "pdf_link": "https://arxiv.org/pdf/2311.03301v2.pdf"
    },
    {
        "title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges",
        "authors": [
            "Chenhang Cui",
            "Yiyang Zhou",
            "Xinyu Yang",
            "Shirley Wu",
            "Linjun Zhang",
            "James Zou",
            "Huaxiu Yao"
        ],
        "published": "2023-11-06T17:26:59Z",
        "summary": "While GPT-4V(ision) impressively models both visual and textual information\nsimultaneously, it's hallucination behavior has not been systematically\nassessed. To bridge this gap, we introduce a new benchmark, namely, the Bias\nand Interference Challenges in Visual Language Models (Bingo). This benchmark\nis designed to evaluate and shed light on the two common types of\nhallucinations in visual language models: bias and interference. Here, bias\nrefers to the model's tendency to hallucinate certain types of responses,\npossibly due to imbalance in its training data. Interference pertains to\nscenarios where the judgment of GPT-4V(ision) can be disrupted due to how the\ntext prompt is phrased or how the input image is presented. We identify a\nnotable regional bias, whereby GPT-4V(ision) is better at interpreting Western\nimages or images with English writing compared to images from other countries\nor containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to\nleading questions and is often confused when interpreting multiple images\ntogether. Popular mitigation approaches, such as self-correction and\nchain-of-thought reasoning, are not effective in resolving these challenges. We\nalso identified similar biases and interference vulnerabilities with LLaVA and\nBard. Our results characterize the hallucination challenges in GPT-4V(ision)\nand state-of-the-art visual-language models, and highlight the need for new\nsolutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.",
        "pdf_link": "https://arxiv.org/pdf/2311.03287v2.pdf"
    },
    {
        "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
        "authors": [
            "Zilin Xiao",
            "Ming Gong",
            "Jie Wu",
            "Xingyao Zhang",
            "Linjun Shou",
            "Jian Pei",
            "Daxin Jiang"
        ],
        "published": "2023-11-06T16:38:51Z",
        "summary": "Generative approaches powered by large language models (LLMs) have\ndemonstrated emergent abilities in tasks that require complex reasoning\nabilities. Yet the generative nature still makes the generated content suffer\nfrom hallucinations, thus unsuitable for entity-centric tasks like entity\nlinking (EL) requiring precise entity predictions over a large knowledge base.\nWe present Instructed Generative Entity Linker (INSGENEL), the first approach\nthat enables casual language models to perform entity linking over knowledge\nbases. Several methods to equip language models with EL capability were\nproposed in this work, including (i) a sequence-to-sequence training EL\nobjective with instruction-tuning, (ii) a novel generative EL framework based\non a light-weight potential mention retriever that frees the model from heavy\nand non-parallelizable decoding, achieving 4$\\times$ speedup without compromise\non linking metrics. INSGENEL outperforms previous generative alternatives with\n+6.8 F1 points gain on average, also with a huge advantage in training data\nefficiency and training compute consumption. In addition, our skillfully\nengineered in-context learning (ICL) framework for EL still lags behind\nINSGENEL significantly, reaffirming that the EL task remains a persistent\nhurdle for general LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.03250v1.pdf"
    },
    {
        "title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents",
        "authors": [
            "Shaoguang Mao",
            "Yuzhe Cai",
            "Yan Xia",
            "Wenshan Wu",
            "Xun Wang",
            "Fengyi Wang",
            "Tao Ge",
            "Furu Wei"
        ],
        "published": "2023-11-06T16:03:46Z",
        "summary": "This paper introduces Alympics (Olympics for Agents), a systematic simulation\nframework utilizing Large Language Model (LLM) agents for game theory research.\nAlympics creates a versatile platform for studying complex game theory\nproblems, bridging the gap between theoretical game theory and empirical\ninvestigations by providing a controlled environment for simulating human-like\nstrategic interactions with LLM agents. In our pilot case study, the \"Water\nAllocation Challenge,\" we explore Alympics through a challenging strategic game\nfocused on the multi-round auction on scarce survival resources. This study\ndemonstrates the framework's ability to qualitatively and quantitatively\nanalyze game determinants, strategies, and outcomes. Additionally, we conduct a\ncomprehensive human assessment and an in-depth evaluation of LLM agents in\nstrategic decision-making scenarios. Our findings not only expand the\nunderstanding of LLM agents' proficiency in emulating human strategic behavior\nbut also highlight their potential in advancing game theory knowledge, thereby\nenriching our understanding of both game theory and empowering further research\ninto strategic decision-making domains with LLM agents. Codes, prompts, and all\nrelated resources are available at https://github.com/microsoft/Alympics.",
        "pdf_link": "https://arxiv.org/pdf/2311.03220v4.pdf"
    },
    {
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
        "authors": [
            "Xuan Li",
            "Zhanke Zhou",
            "Jianing Zhu",
            "Jiangchao Yao",
            "Tongliang Liu",
            "Bo Han"
        ],
        "published": "2023-11-06T15:29:30Z",
        "summary": "Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment\nw.r.t. the authority power for inciting harmfulness, we disclose a lightweight\nmethod, termed DeepInception, which can easily hypnotize LLM to be a\njailbreaker. Specifically, DeepInception leverages the personification ability\nof LLM to construct a novel nested scene to behave, which realizes an adaptive\nway to escape the usage control in a normal scenario. Empirically, our\nDeepInception can achieve competitive jailbreak success rates with previous\ncounterparts and realize a continuous jailbreak in subsequent interactions,\nwhich reveals the critical weakness of self-losing on both open and\nclosed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, and GPT-3.5-turbo/4. Our\ninvestigation appeals to people to pay more attention to the safety aspects of\nLLMs and develop a stronger defense against their misuse risks. The code is\npublicly available at: https://github.com/tmlr-group/DeepInception.",
        "pdf_link": "https://arxiv.org/pdf/2311.03191v3.pdf"
    },
    {
        "title": "Zero-shot Bilingual App Reviews Mining with Large Language Models",
        "authors": [
            "Jialiang Wei",
            "Anne-Lise Courbis",
            "Thomas Lambolais",
            "Binbin Xu",
            "Pierre Louis Bernard",
            "G\u00e9rard Dray"
        ],
        "published": "2023-11-06T12:36:46Z",
        "summary": "App reviews from app stores are crucial for improving software requirements.\nA large number of valuable reviews are continually being posted, describing\nsoftware problems and expected features. Effectively utilizing user reviews\nnecessitates the extraction of relevant information, as well as their\nsubsequent summarization. Due to the substantial volume of user reviews, manual\nanalysis is arduous. Various approaches based on natural language processing\n(NLP) have been proposed for automatic user review mining. However, the\nmajority of them requires a manually crafted dataset to train their models,\nwhich limits their usage in real-world scenarios. In this work, we propose\nMini-BAR, a tool that integrates large language models (LLMs) to perform\nzero-shot mining of user reviews in both English and French. Specifically,\nMini-BAR is designed to (i) classify the user reviews, (ii) cluster similar\nreviews together, (iii) generate an abstractive summary for each cluster and\n(iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we\ncreated a dataset containing 6,000 English and 6,000 French annotated user\nreviews and conducted extensive experiments. Preliminary results demonstrate\nthe effectiveness and efficiency of Mini-BAR in requirement engineering by\nanalyzing bilingual app reviews. (Replication package containing the code,\ndataset, and experiment setups on https://github.com/Jl-wei/mini-bar )",
        "pdf_link": "https://arxiv.org/pdf/2311.03058v1.pdf"
    },
    {
        "title": "LitSumm: Large language models for literature summarisation of non-coding RNAs",
        "authors": [
            "Andrew Green",
            "Carlos Ribas",
            "Nancy Ontiveros-Palacios",
            "Sam Griffiths-Jones",
            "Anton I. Petrov",
            "Alex Bateman",
            "Blake Sweeney"
        ],
        "published": "2023-11-06T12:22:19Z",
        "summary": "Motivation: Curation of literature in life sciences is a growing challenge.\nThe continued increase in the rate of publication, coupled with the relatively\nfixed number of curators worldwide presents a major challenge to developers of\nbiomedical knowledgebases. Very few knowledgebases have resources to scale to\nthe whole relevant literature and all have to prioritise their efforts.\n  Results: In this work, we take a first step to alleviating the lack of\ncurator time in RNA science by generating summaries of literature for\nnon-coding RNAs using large language models (LLMs). We demonstrate that\nhigh-quality, factually accurate summaries with accurate references can be\nautomatically generated from the literature using a commercial LLM and a chain\nof prompts and checks. Manual assessment was carried out for a subset of\nsummaries, with the majority being rated extremely high quality. We also\napplied the most commonly used automated evaluation approaches, finding that\nthey do not correlate with human assessment. Finally, we apply our tool to a\nselection of over 4,600 ncRNAs and make the generated summaries available via\nthe RNAcentral resource. We conclude that automated literature summarization is\nfeasible with the current generation of LLMs, provided careful prompting and\nautomated checking are applied.\n  Availability: Code used to produce these summaries can be found here:\nhttps://github.com/RNAcentral/litscan-summarization and the dataset of contexts\nand summaries can be found here:\nhttps://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are also\ndisplayed on the RNA report pages in RNAcentral (https://rnacentral.org/)",
        "pdf_link": "https://arxiv.org/pdf/2311.03056v2.pdf"
    },
    {
        "title": "Can LLMs Follow Simple Rules?",
        "authors": [
            "Norman Mu",
            "Sarah Chen",
            "Zifan Wang",
            "Sizhe Chen",
            "David Karamardian",
            "Lulwa Aljeraisy",
            "Basel Alomair",
            "Dan Hendrycks",
            "David Wagner"
        ],
        "published": "2023-11-06T08:50:29Z",
        "summary": "As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Existing evaluations\nof adversarial attacks and defenses on LLMs generally require either expensive\nmanual review or unreliable heuristic checks. To address this issue, we propose\nRule-following Language Evaluation Scenarios (RuLES), a programmatic framework\nfor measuring rule-following ability in LLMs. RuLES consists of 14 simple text\nscenarios in which the model is instructed to obey various rules while\ninteracting with the user. Each scenario has a programmatic evaluation function\nto determine whether the model has broken any rules in a conversation. Our\nevaluations of proprietary and open models show that almost all current models\nstruggle to follow scenario rules, even on straightforward test cases. We also\ndemonstrate that simple optimization attacks suffice to significantly increase\nfailure rates on test cases. We conclude by exploring two potential avenues for\nimprovement: test-time steering and supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2311.04235v3.pdf"
    },
    {
        "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
        "authors": [
            "Sahana Ramnath",
            "Brihi Joshi",
            "Skyler Hallinan",
            "Ximing Lu",
            "Liunian Harold Li",
            "Aaron Chan",
            "Jack Hessel",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-11-06T00:20:11Z",
        "summary": "Large language models (LMs) are capable of generating free-text rationales to\naid question answering. However, prior work 1) suggests that useful\nself-rationalization is emergent only at significant scales (e.g., 175B\nparameter GPT-3); and 2) focuses largely on downstream performance, ignoring\nthe semantics of the rationales themselves, e.g., are they faithful, true, and\nhelpful for humans? In this work, we enable small-scale LMs (approx. 200x\nsmaller than GPT-3) to generate rationales that not only improve downstream\ntask performance, but are also more plausible, consistent, and diverse,\nassessed both by automatic and human evaluation. Our method, MaRio\n(Multi-rewArd RatIOnalization), is a multi-reward conditioned\nself-rationalization algorithm that optimizes multiple distinct properties like\nplausibility, diversity and consistency. Results on five difficult\nquestion-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC\nshow that not only does MaRio improve task accuracy, but it also improves the\nself-rationalization quality of small LMs across the aforementioned axes better\nthan a supervised fine-tuning (SFT) baseline. Extensive human evaluations\nconfirm that MaRio rationales are preferred vs. SFT rationales, as well as\nqualitative improvements in plausibility and consistency.",
        "pdf_link": "https://arxiv.org/pdf/2311.02805v1.pdf"
    },
    {
        "title": "On the Intersection of Self-Correction and Trust in Language Models",
        "authors": [
            "Satyapriya Krishna"
        ],
        "published": "2023-11-06T00:04:12Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex cognitive tasks. However, their complexity and lack of\ntransparency have raised several trustworthiness concerns, including the\npropagation of misinformation and toxicity. Recent research has explored the\nself-correction capabilities of LLMs to enhance their performance. In this\nwork, we investigate whether these self-correction capabilities can be\nharnessed to improve the trustworthiness of LLMs. We conduct experiments\nfocusing on two key aspects of trustworthiness: truthfulness and toxicity. Our\nfindings reveal that self-correction can lead to improvements in toxicity and\ntruthfulness, but the extent of these improvements varies depending on the\nspecific aspect of trustworthiness and the nature of the task. Interestingly,\nour study also uncovers instances of \"self-doubt\" in LLMs during the\nself-correction process, introducing a new set of challenges that need to be\naddressed.",
        "pdf_link": "https://arxiv.org/pdf/2311.02801v1.pdf"
    },
    {
        "title": "Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation",
        "authors": [
            "Muhammad Fawad Akbar Khan",
            "Max Ramsdell",
            "Erik Falor",
            "Hamid Karimi"
        ],
        "published": "2023-11-05T12:56:40Z",
        "summary": "This paper presents a comprehensive evaluation of the code generation\ncapabilities of ChatGPT, a prominent large language model, compared to human\nprogrammers. A novel dataset of 131 code-generation prompts across 5 categories\nwas curated to enable robust analysis. Code solutions were generated by both\nChatGPT and humans for all prompts, resulting in 262 code samples. A meticulous\nmanual assessment methodology prioritized evaluating correctness,\ncomprehensibility, and security using 14 established code quality metrics. The\nkey findings reveal ChatGPT's strengths in crafting concise, efficient code\nwith advanced constructs, showcasing strengths in data analysis tasks (93.1%\naccuracy) but limitations in visual-graphical challenges. Comparative analysis\nwith human code highlights ChatGPT's inclination towards modular design and\nsuperior error handling. Additionally, machine learning models effectively\ndistinguished ChatGPT from human code with up to 88% accuracy, suggesting\ndetectable coding style disparities. By providing profound insights into\nChatGPT's code generation capabilities and limitations through quantitative\nmetrics and qualitative analysis, this study makes valuable contributions\ntoward advancing AI-based programming assistants. The curated dataset and\nmethodology offer a robust foundation for future research in this nascent\ndomain. All data and codes are available on\nhttps://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls.",
        "pdf_link": "https://arxiv.org/pdf/2311.02640v1.pdf"
    },
    {
        "title": "FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM",
        "authors": [
            "Grace Colverd",
            "Paul Darm",
            "Leonard Silverberg",
            "Noah Kasmanoff"
        ],
        "published": "2023-11-05T08:34:26Z",
        "summary": "Fast disaster impact reporting is crucial in planning humanitarian\nassistance. Large Language Models (LLMs) are well known for their ability to\nwrite coherent text and fulfill a variety of tasks relevant to impact\nreporting, such as question answering or text summarization. However, LLMs are\nconstrained by the knowledge within their training data and are prone to\ngenerating inaccurate, or \"hallucinated\", information. To address this, we\nintroduce a sophisticated pipeline embodied in our tool FloodBrain\n(floodbrain.com), specialized in generating flood disaster impact reports by\nextracting and curating information from the web. Our pipeline assimilates\ninformation from web search results to produce detailed and accurate reports on\nflood events. We test different LLMs as backbones in our tool and compare their\ngenerated reports to human-written reports on different metrics. Similar to\nother studies, we find a notable correlation between the scores assigned by\nGPT-4 and the scores given by human evaluators when comparing our generated\nreports to human-authored ones. Additionally, we conduct an ablation study to\ntest our single pipeline components and their relevancy for the final reports.\nWith our tool, we aim to advance the use of LLMs for disaster impact reporting\nand reduce the time for coordination of humanitarian efforts in the wake of\nflood disasters.",
        "pdf_link": "https://arxiv.org/pdf/2311.02597v1.pdf"
    },
    {
        "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models",
        "authors": [
            "Kun Chu",
            "Xufeng Zhao",
            "Cornelius Weber",
            "Mengdi Li",
            "Stefan Wermter"
        ],
        "published": "2023-11-04T11:21:38Z",
        "summary": "Reinforcement Learning (RL) plays an important role in the robotic\nmanipulation domain since it allows self-learning from trial-and-error\ninteractions with the environment. Still, sample efficiency and reward\nspecification seriously limit its potential. One possible solution involves\nlearning from expert guidance. However, obtaining a human expert is impractical\ndue to the high cost of supervising an RL agent, and developing an automatic\nsupervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate\nremarkable abilities to provide human-like feedback on user inputs in natural\nlanguage. Nevertheless, they are not designed to directly control low-level\nrobotic motions, as their pretraining is based on vast internet data rather\nthan specific robotics data. In this paper, we introduce the Lafite-RL\n(Language agent feedback interactive Reinforcement Learning) framework, which\nenables RL agents to learn robotic tasks efficiently by taking advantage of\nLLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate\nthat, with simple prompt design in natural language, the Lafite-RL agent\nexhibits improved learning capabilities when guided by an LLM. It outperforms\nthe baseline in terms of both learning efficiency and success rate,\nunderscoring the efficacy of the rewards provided by an LLM.",
        "pdf_link": "https://arxiv.org/pdf/2311.02379v1.pdf"
    },
    {
        "title": "Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles",
        "authors": [
            "Weiting Tan",
            "Haoran Xu",
            "Lingfeng Shen",
            "Shuyue Stella Li",
            "Kenton Murray",
            "Philipp Koehn",
            "Benjamin Van Durme",
            "Yunmo Chen"
        ],
        "published": "2023-11-04T03:18:45Z",
        "summary": "Large language models trained primarily in a monolingual setting have\ndemonstrated their ability to generalize to machine translation using zero- and\nfew-shot examples with in-context learning. However, even though zero-shot\ntranslations are relatively good, there remains a discernible gap comparing\ntheir performance with the few-shot setting. In this paper, we investigate the\nfactors contributing to this gap and find that this gap can largely be closed\n(for about 70%) by matching the writing styles of the target corpus.\nAdditionally, we explore potential approaches to enhance zero-shot baselines\nwithout the need for parallel demonstration examples, providing valuable\ninsights into how these methods contribute to improving translation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2311.02310v1.pdf"
    },
    {
        "title": "LLMs-augmented Contextual Bandit",
        "authors": [
            "Ali Baheri",
            "Cecilia O. Alm"
        ],
        "published": "2023-11-03T23:12:57Z",
        "summary": "Contextual bandits have emerged as a cornerstone in reinforcement learning,\nenabling systems to make decisions with partial feedback. However, as contexts\ngrow in complexity, traditional bandit algorithms can face challenges in\nadequately capturing and utilizing such contexts. In this paper, we propose a\nnovel integration of large language models (LLMs) with the contextual bandit\nframework. By leveraging LLMs as an encoder, we enrich the representation of\nthe context, providing the bandit with a denser and more informative view.\nPreliminary results on synthetic datasets demonstrate the potential of this\napproach, showing notable improvements in cumulative rewards and reductions in\nregret compared to traditional bandit algorithms. This integration not only\nshowcases the capabilities of LLMs in reinforcement learning but also opens the\ndoor to a new era of contextually-aware decision systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.02268v1.pdf"
    },
    {
        "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
        "authors": [
            "Qingru Zhang",
            "Chandan Singh",
            "Liyuan Liu",
            "Xiaodong Liu",
            "Bin Yu",
            "Jianfeng Gao",
            "Tuo Zhao"
        ],
        "published": "2023-11-03T22:56:43Z",
        "summary": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need -\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .",
        "pdf_link": "https://arxiv.org/pdf/2311.02262v1.pdf"
    },
    {
        "title": "An Interdisciplinary Outlook on Large Language Models for Scientific Research",
        "authors": [
            "James Boyko",
            "Joseph Cohen",
            "Nathan Fox",
            "Maria Han Veiga",
            "Jennifer I-Hsiu Li",
            "Jing Liu",
            "Bernardo Modenesi",
            "Andreas H. Rauch",
            "Kenneth N. Reid",
            "Soumi Tribedi",
            "Anastasia Visheratina",
            "Xin Xie"
        ],
        "published": "2023-11-03T19:41:09Z",
        "summary": "In this paper, we describe the capabilities and constraints of Large Language\nModels (LLMs) within disparate academic disciplines, aiming to delineate their\nstrengths and limitations with precision. We examine how LLMs augment\nscientific inquiry, offering concrete examples such as accelerating literature\nreview by summarizing vast numbers of publications, enhancing code development\nthrough automated syntax correction, and refining the scientific writing\nprocess. Simultaneously, we articulate the challenges LLMs face, including\ntheir reliance on extensive and sometimes biased datasets, and the potential\nethical dilemmas stemming from their use. Our critical discussion extends to\nthe varying impacts of LLMs across fields, from the natural sciences, where\nthey help model complex biological sequences, to the social sciences, where\nthey can parse large-scale qualitative data. We conclude by offering a nuanced\nperspective on how LLMs can be both a boon and a boundary to scientific\nprogress.",
        "pdf_link": "https://arxiv.org/pdf/2311.04929v1.pdf"
    },
    {
        "title": "An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology",
        "authors": [
            "Reza Khanmohammadi",
            "Mohammad M. Ghassemi",
            "Kyle Verdecchia",
            "Ahmed I. Ghanem",
            "Luo Bing",
            "Indrin J. Chetty",
            "Hassan Bagher-Ebadian",
            "Farzan Siddiqui",
            "Mohamed Elshaikh",
            "Benjamin Movsas",
            "Kundan Thind"
        ],
        "published": "2023-11-03T19:32:35Z",
        "summary": "Natural Language Processing (NLP) is a key technique for developing Medical\nArtificial Intelligence (AI) systems that leverage Electronic Health Record\n(EHR) data to build diagnostic and prognostic models. NLP enables the\nconversion of unstructured clinical text into structured data that can be fed\ninto AI algorithms. The emergence of the transformer architecture and large\nlanguage models (LLMs) has led to remarkable advances in NLP for various\nhealthcare tasks, such as entity recognition, relation extraction, sentence\nsimilarity, text summarization, and question answering. In this article, we\nreview the major technical innovations that underpin modern NLP models and\npresent state-of-the-art NLP applications that employ LLMs in radiation\noncology research. However, these LLMs are prone to many errors such as\nhallucinations, biases, and ethical violations, which necessitate rigorous\nevaluation and validation before clinical deployment. As such, we propose a\ncomprehensive framework for assessing the NLP models based on their purpose and\nclinical fit, technical performance, bias and trust, legal and ethical\nimplications, and quality assurance, prior to implementation in clinical\nradiation oncology. Our article aims to provide guidance and insights for\nresearchers and clinicians who are interested in developing and using NLP\nmodels in clinical radiation oncology.",
        "pdf_link": "https://arxiv.org/pdf/2311.02205v2.pdf"
    },
    {
        "title": "The Alignment Problem in Context",
        "authors": [
            "Rapha\u00ebl Milli\u00e8re"
        ],
        "published": "2023-11-03T17:57:55Z",
        "summary": "A core challenge in the development of increasingly capable AI systems is to\nmake them safe and reliable by ensuring their behaviour is consistent with\nhuman values. This challenge, known as the alignment problem, does not merely\napply to hypothetical future AI systems that may pose catastrophic risks; it\nalready applies to current systems, such as large language models, whose\npotential for harm is rapidly increasing. In this paper, I assess whether we\nare on track to solve the alignment problem for large language models, and what\nthat means for the safety of future AI systems. I argue that existing\nstrategies for alignment are insufficient, because large language models remain\nvulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I\noffer an explanation of this lingering vulnerability on which it is not simply\na contingent limitation of current language models, but has deep technical ties\nto a crucial aspect of what makes these models useful and versatile in the\nfirst place -- namely, their remarkable aptitude to learn \"in context\" directly\nfrom user instructions. It follows that the alignment problem is not only\nunsolved for current AI systems, but may be intrinsically difficult to solve\nwithout severely undermining their capabilities. Furthermore, this assessment\nraises concerns about the prospect of ensuring the safety of future and more\ncapable AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2311.02147v1.pdf"
    },
    {
        "title": "Grounded Intuition of GPT-Vision's Abilities with Scientific Images",
        "authors": [
            "Alyssa Hwang",
            "Andrew Head",
            "Chris Callison-Burch"
        ],
        "published": "2023-11-03T17:53:43Z",
        "summary": "GPT-Vision has impressed us on a range of vision-language tasks, but it comes\nwith the familiar new challenge: we have little idea of its capabilities and\nlimitations. In our study, we formalize a process that many have instinctively\nbeen trying already to develop \"grounded intuition\" of this new model. Inspired\nby the recent movement away from benchmarking in favor of example-driven\nqualitative evaluation, we draw upon grounded theory and thematic analysis in\nsocial science and human-computer interaction to establish a rigorous framework\nfor qualitative evaluation in natural language processing. We use our technique\nto examine alt text generation for scientific figures, finding that GPT-Vision\nis particularly sensitive to prompting, counterfactual text in images, and\nrelative spatial relationships. Our method and analysis aim to help researchers\nramp up their own grounded intuitions of new models while exposing how\nGPT-Vision can be applied to make information more accessible.",
        "pdf_link": "https://arxiv.org/pdf/2311.02069v1.pdf"
    },
    {
        "title": "Post Turing: Mapping the landscape of LLM Evaluation",
        "authors": [
            "Alexey Tikhonov",
            "Ivan P. Yamshchikov"
        ],
        "published": "2023-11-03T17:24:50Z",
        "summary": "In the rapidly evolving landscape of Large Language Models (LLMs),\nintroduction of well-defined and standardized evaluation methodologies remains\na crucial challenge. This paper traces the historical trajectory of LLM\nevaluations, from the foundational questions posed by Alan Turing to the modern\nera of AI research. We categorize the evolution of LLMs into distinct periods,\neach characterized by its unique benchmarks and evaluation criteria. As LLMs\nincreasingly mimic human-like behaviors, traditional evaluation proxies, such\nas the Turing test, have become less reliable. We emphasize the pressing need\nfor a unified evaluation system, given the broader societal implications of\nthese models. Through an analysis of common evaluation methodologies, we\nadvocate for a qualitative shift in assessment approaches, underscoring the\nimportance of standardization and objective criteria. This work serves as a\ncall for the AI community to collaboratively address the challenges of LLM\nevaluation, ensuring their reliability, fairness, and societal benefit.",
        "pdf_link": "https://arxiv.org/pdf/2311.02049v1.pdf"
    },
    {
        "title": "Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks",
        "authors": [
            "Yifan Wang",
            "Qingyan Guo",
            "Xinzhe Ni",
            "Chufan Shi",
            "Lemao Liu",
            "Haiyun Jiang",
            "Yujiu Yang"
        ],
        "published": "2023-11-03T14:39:20Z",
        "summary": "In-context learning (ICL) ability has emerged with the increasing scale of\nlarge language models (LLMs), enabling them to learn input-label mappings from\ndemonstrations and perform well on downstream tasks. However, under the\nstandard ICL setting, LLMs may sometimes neglect query-related information in\ndemonstrations, leading to incorrect predictions. To address this limitation,\nwe propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to\nexplore the power of ICL in open-domain question answering, an important form\nin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract\nquery-related knowledge from demonstrations, then concatenates the knowledge to\nprompt LLMs in a more explicit way. Furthermore, we track the source of this\nknowledge to identify specific examples, and introduce a Hint-related Example\nRetriever (HER) to select informative examples for enhanced demonstrations. We\nevaluate HICL with HER on 3 open-domain QA benchmarks, and observe average\nperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM\nscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.",
        "pdf_link": "https://arxiv.org/pdf/2311.01949v1.pdf"
    },
    {
        "title": "Comprehensive Assessment of Toxicity in ChatGPT",
        "authors": [
            "Boyang Zhang",
            "Xinyue Shen",
            "Wai Man Si",
            "Zeyang Sha",
            "Zeyuan Chen",
            "Ahmed Salem",
            "Yun Shen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-11-03T14:37:53Z",
        "summary": "Moderating offensive, hateful, and toxic language has always been an\nimportant but challenging topic in the domain of safe use in NLP. The emerging\nlarge language models (LLMs), such as ChatGPT, can potentially further\naccentuate this threat. Previous works have discovered that ChatGPT can\ngenerate toxic responses using carefully crafted inputs. However, limited\nresearch has been done to systematically examine when ChatGPT generates toxic\nresponses. In this paper, we comprehensively evaluate the toxicity in ChatGPT\nby utilizing instruction-tuning datasets that closely align with real-world\nscenarios. Our results show that ChatGPT's toxicity varies based on different\nproperties and settings of the prompts, including tasks, domains, length, and\nlanguages. Notably, prompts in creative writing tasks can be 2x more likely\nthan others to elicit toxic responses. Prompting in German and Portuguese can\nalso double the response toxicity. Additionally, we discover that certain\ndeliberately toxic prompts, designed in earlier studies, no longer yield\nharmful responses. We hope our discoveries can guide model developers to better\nregulate these AI systems and the users to avoid undesirable outputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.14685v1.pdf"
    },
    {
        "title": "Sentiment Analysis through LLM Negotiations",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Shengyu Zhang",
            "Shuhe Wang",
            "Fei Wu",
            "Jiwei Li",
            "Tianwei Zhang",
            "Guoyin Wang"
        ],
        "published": "2023-11-03T12:35:29Z",
        "summary": "A standard paradigm for sentiment analysis is to rely on a singular LLM and\nmakes the decision in a single round under the framework of in-context\nlearning. This framework suffers the key disadvantage that the single-turn\noutput generated by a single LLM might not deliver the perfect decision, just\nas humans sometimes need multiple attempts to get things right. This is\nespecially true for the task of sentiment analysis where deep reasoning is\nrequired to address the complex linguistic phenomenon (e.g., clause\ncomposition, irony, etc) in the input.\n  To address this issue, this paper introduces a multi-LLM negotiation\nframework for sentiment analysis. The framework consists of a reasoning-infused\ngenerator to provide decision along with rationale, a explanation-deriving\ndiscriminator to evaluate the credibility of the generator. The generator and\nthe discriminator iterate until a consensus is reached. The proposed framework\nnaturally addressed the aforementioned challenge, as we are able to take the\ncomplementary abilities of two LLMs, have them use rationale to persuade each\nother for correction.\n  Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie\nReview, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed\napproach: it consistently yields better performances than the ICL baseline\nacross all benchmarks, and even superior performances to supervised baselines\non the Twitter and movie review datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.01876v1.pdf"
    },
    {
        "title": "Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT",
        "authors": [
            "Mario S\u00e4nger",
            "Ninon De Mecquenem",
            "Katarzyna Ewa Lewi\u0144ska",
            "Vasilis Bountris",
            "Fabian Lehmann",
            "Ulf Leser",
            "Thomas Kosch"
        ],
        "published": "2023-11-03T10:28:53Z",
        "summary": "Scientific workflow systems are increasingly popular for expressing and\nexecuting complex data analysis pipelines over large datasets, as they offer\nreproducibility, dependability, and scalability of analyses by automatic\nparallelization on large compute clusters. However, implementing workflows is\ndifficult due to the involvement of many black-box tools and the deep\ninfrastructure stack necessary for their execution. Simultaneously,\nuser-supporting tools are rare, and the number of available examples is much\nlower than in classical programming languages. To address these challenges, we\ninvestigate the efficiency of Large Language Models (LLMs), specifically\nChatGPT, to support users when dealing with scientific workflows. We performed\nthree user studies in two scientific domains to evaluate ChatGPT for\ncomprehending, adapting, and extending workflows. Our results indicate that\nLLMs efficiently interpret workflows but achieve lower performance for\nexchanging components or purposeful workflow extensions. We characterize their\nlimitations in these challenging scenarios and suggest future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2311.01825v2.pdf"
    },
    {
        "title": "AFPQ: Asymmetric Floating Point Quantization for LLMs",
        "authors": [
            "Yijia Zhang",
            "Sicheng Zhang",
            "Shijie Cao",
            "Dayou Du",
            "Jianyu Wei",
            "Ting Cao",
            "Ningyi Xu"
        ],
        "published": "2023-11-03T09:07:09Z",
        "summary": "Large language models (LLMs) show great performance in various tasks, but\nface deployment challenges from limited memory capacity and bandwidth. Low-bit\nweight quantization can save memory and accelerate inference. Although\nfloating-point (FP) formats show good performance in LLM quantization, they\ntend to perform poorly with small group sizes or sub-4 bits. We find the reason\nis that the absence of asymmetry in previous FP quantization makes it\nunsuitable for handling asymmetric value distribution of LLM weight tensors. In\nthis work, we propose asymmetric FP quantization (AFPQ), which sets separate\nscales for positive and negative values. Our method leads to large accuracy\nimprovements and can be easily plugged into other quantization methods,\nincluding GPTQ and AWQ, for better performance. Besides, no additional storage\nis needed compared with asymmetric integer (INT) quantization. The code is\navailable at https://github.com/zhangsichengsjtu/AFPQ.",
        "pdf_link": "https://arxiv.org/pdf/2311.01792v1.pdf"
    },
    {
        "title": "TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine",
        "authors": [
            "Guoxing Yang",
            "Jianyu Shi",
            "Zan Wang",
            "Xiaohong Liu",
            "Guangyu Wang"
        ],
        "published": "2023-11-03T08:54:50Z",
        "summary": "Pre-training and fine-tuning have emerged as a promising paradigm across\nvarious natural language processing (NLP) tasks. The effectiveness of\npretrained large language models (LLM) has witnessed further enhancement,\nholding potential for applications in the field of medicine, particularly in\nthe context of Traditional Chinese Medicine (TCM). However, the application of\nthese general models to specific domains often yields suboptimal results,\nprimarily due to challenges like lack of domain knowledge, unique objectives,\nand computational efficiency. Furthermore, their effectiveness in specialized\ndomains, such as Traditional Chinese Medicine, requires comprehensive\nevaluation. To address the above issues, we propose a novel domain specific\nTCMDA (TCM Domain Adaptation) approach, efficient pre-training with\ndomain-specific corpus. Specifically, we first construct a large TCM-specific\ncorpus, TCM-Corpus-1B, by identifying domain keywords and retreving from\ngeneral corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained\nmodel's weights and uses rank decomposition matrices to efficiently train\nspecific dense layers for pre-training and fine-tuning, efficiently aligning\nthe model with TCM-related tasks, namely TCM-GPT-7B. We further conducted\nextensive experiments on two TCM tasks, including TCM examination and TCM\ndiagnosis. TCM-GPT-7B archived the best performance across both datasets,\noutperforming other models by relative increments of 17% and 12% in accuracy,\nrespectively. To the best of our knowledge, our study represents the pioneering\nvalidation of domain adaptation of a large language model with 7 billion\nparameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B\nmodel once accepted to facilitate interdisciplinary development in TCM and NLP,\nserving as the foundation for further study.",
        "pdf_link": "https://arxiv.org/pdf/2311.01786v1.pdf"
    },
    {
        "title": "PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion",
        "authors": [
            "Yiduo Guo",
            "Zekai Zhang",
            "Yaobo Liang",
            "Dongyan Zhao",
            "Nan Duan"
        ],
        "published": "2023-11-03T08:06:35Z",
        "summary": "Recent evaluations of Large Language Models (LLMs) have centered around\ntesting their zero-shot/few-shot capabilities for basic natural language tasks\nand their ability to translate instructions into tool APIs. However, the\nevaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal\ninstructions in a complex multi-modal environment has not been investigated. To\naddress this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark\nto assess LLMs' ability to create and edit PPT files based on user\ninstructions. It contains 279 multi-turn sessions covering diverse topics and\nhundreds of instructions involving multi-modal operations. We also propose the\nPPTX-Match Evaluation System that evaluates if LLMs finish the instruction\nbased on the prediction file rather than the label API sequence, thus it\nsupports various LLM-generated API sequences. We measure 3 closed LLMs and 6\nopen-source LLMs. The results show that GPT-4 outperforms other LLMs with\n75.1\\% accuracy in single-turn dialogue testing but faces challenges in\ncompleting entire sessions, achieving just 6\\% session accuracy. We find three\nmain error causes in our benchmark: error accumulation in the multi-turn\nsession, long PPT template processing, and multi-modality perception. These\npose great challenges for future LLM and agent systems. We release the data,\ncode, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.",
        "pdf_link": "https://arxiv.org/pdf/2311.01767v2.pdf"
    },
    {
        "title": "FinGPT: Large Generative Models for a Small Language",
        "authors": [
            "Risto Luukkonen",
            "Ville Komulainen",
            "Jouni Luoma",
            "Anni Eskelinen",
            "Jenna Kanerva",
            "Hanna-Mari Kupari",
            "Filip Ginter",
            "Veronika Laippala",
            "Niklas Muennighoff",
            "Aleksandra Piktus",
            "Thomas Wang",
            "Nouamane Tazi",
            "Teven Le Scao",
            "Thomas Wolf",
            "Osma Suominen",
            "Samuli Sairanen",
            "Mikko Merioksa",
            "Jyrki Heinonen",
            "Aija Vahtola",
            "Samuel Antao",
            "Sampo Pyysalo"
        ],
        "published": "2023-11-03T08:05:04Z",
        "summary": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most\nopen models have very limited coverage of smaller languages and LLM work tends\nto focus on languages where nearly unlimited data is available for pretraining.\nIn this work, we study the challenges of creating LLMs for Finnish, a language\nspoken by less than 0.1% of the world population. We compile an extensive\ndataset of Finnish combining web crawls, news, social media and eBooks. We\npursue two approaches to pretrain models: 1) we train seven monolingual models\nfrom scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the\npretraining of the multilingual BLOOM model on a mix of its original training\ndata and Finnish, resulting in a 176 billion parameter model we call BLUUMI.\nFor model evaluation, we introduce FIN-bench, a version of BIG-bench with\nFinnish tasks. We also assess other model qualities such as toxicity and bias.\nOur models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "pdf_link": "https://arxiv.org/pdf/2311.05640v1.pdf"
    },
    {
        "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models",
        "authors": [
            "Sean Xie",
            "Soroush Vosoughi",
            "Saeed Hassanpour"
        ],
        "published": "2023-11-03T05:55:32Z",
        "summary": "Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), but their lack of interpretability has been a major\nconcern. Current methods for interpreting LLMs are post hoc, applied after\ninference time, and have limitations such as their focus on low-level features\nand lack of explainability at higher level text units. In this work, we\nintroduce proto-lm, a prototypical network-based white-box framework that\nallows LLMs to learn immediately interpretable embeddings during the\nfine-tuning stage while maintaining competitive performance. Our method's\napplicability and interpretability are demonstrated through experiments on a\nwide range of NLP tasks, and our results indicate a new possibility of creating\ninterpretable models without sacrificing performance. This novel approach to\ninterpretability in LLMs can pave the way for more interpretable models without\nthe need to sacrifice performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.01732v2.pdf"
    },
    {
        "title": "Successor Features for Efficient Multisubject Controlled Text Generation",
        "authors": [
            "Meng Cao",
            "Mehdi Fatemi",
            "Jackie Chi Kit Cheung",
            "Samira Shabanian"
        ],
        "published": "2023-11-03T00:17:08Z",
        "summary": "While large language models (LLMs) have achieved impressive performance in\ngenerating fluent and realistic text, controlling the generated text so that it\nexhibits properties such as safety, factuality, and non-toxicity remains\nchallenging. % such as DExperts, GeDi, and rectification Existing\ndecoding-based methods are static in terms of the dimension of control; if the\ntarget subject is changed, they require new training. Moreover, it can quickly\nbecome prohibitive to concurrently control multiple subjects. In this work, we\nintroduce SF-GEN, which is grounded in two primary concepts: successor features\n(SFs) to decouple the LLM's dynamics from task-specific rewards, and language\nmodel rectification to proportionally adjust the probability of selecting a\ntoken based on the likelihood that the finished text becomes undesired. SF-GEN\nseamlessly integrates the two to enable dynamic steering of text generation\nwith no need to alter the LLM's parameters. Thanks to the decoupling effect\ninduced by successor features, our method proves to be memory-wise and\ncomputationally efficient for training as well as decoding, especially when\ndealing with multiple target subjects. To the best of our knowledge, our\nresearch represents the first application of successor features in text\ngeneration. In addition to its computational efficiency, the resultant language\nproduced by our method is comparable to the SOTA (and outperforms baselines) in\nboth control measures as well as language quality, which we demonstrate through\na series of experiments in various controllable text generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.04921v1.pdf"
    },
    {
        "title": "Preserving the knowledge of long clinical texts using aggregated ensembles of large language models",
        "authors": [
            "Mohammad Junayed Hasan",
            "Suhra Noor",
            "Mohammad Ashrafuzzaman Khan"
        ],
        "published": "2023-11-02T19:50:02Z",
        "summary": "Clinical texts, such as admission notes, discharge summaries, and progress\nnotes, contain rich and valuable information that can be used for various\nclinical outcome prediction tasks. However, applying large language models,\nsuch as BERT-based models, to clinical texts poses two major challenges: the\nlimitation of input length and the diversity of data sources. This paper\nproposes a novel method to preserve the knowledge of long clinical texts using\naggregated ensembles of large language models. Unlike previous studies which\nuse model ensembling or text aggregation methods separately, we combine\nensemble learning with text aggregation and train multiple large language\nmodels on two clinical outcome tasks: mortality prediction and length of stay\nprediction. We show that our method can achieve better results than baselines,\nensembling, and aggregation individually, and can improve the performance of\nlarge language models while handling long inputs and diverse datasets. We\nconduct extensive experiments on the admission notes from the MIMIC-III\nclinical database by combining multiple unstructured and high-dimensional\ndatasets, demonstrating our method's effectiveness and superiority over\nexisting approaches. We also provide a comprehensive analysis and discussion of\nour results, highlighting our method's applications and limitations for future\nresearch in the domain of clinical healthcare. The results and analysis of this\nstudy is supportive of our method assisting in clinical healthcare systems by\nenabling clinical decision-making with robust performance overcoming the\nchallenges of long text inputs and varied datasets.",
        "pdf_link": "https://arxiv.org/pdf/2311.01571v1.pdf"
    },
    {
        "title": "Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization",
        "authors": [
            "Bj\u00f6rn Deiseroth",
            "Max Meuer",
            "Nikolas Gritsch",
            "Constantin Eichenberg",
            "Patrick Schramowski",
            "Matthias A\u00dfenmacher",
            "Kristian Kersting"
        ],
        "published": "2023-11-02T18:55:53Z",
        "summary": "Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. However, their ever-increasing size has raised\nconcerns about their effective deployment and the need for LLM compression.\nThis study introduces the Divergent Token Metrics (DTMs), a novel approach to\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs measure token divergences that allow deeper insights into the subtleties\nof model compression, in particular, when evaluating components' impacts\nindividually. Utilizing the First Divergent Token Metric (FDTM) in model\nsparsification reveals that 25% of all attention components can be pruned\nbeyond 90% on the Llama-2 model family, still keeping SOTA performance. For\nquantization, FDTM suggests that more than 80% of parameters can be naively\ntransformed to int8 without special outlier management. These evaluations\nindicate the necessity of choosing appropriate compressions for parameters\nindividually -- and that FDTM can identify those -- while standard metrics\nresult in deteriorated outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2311.01544v3.pdf"
    },
    {
        "title": "GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks",
        "authors": [
            "Xinlu Zhang",
            "Yujie Lu",
            "Weizhi Wang",
            "An Yan",
            "Jun Yan",
            "Lianke Qin",
            "Heng Wang",
            "Xifeng Yan",
            "William Yang Wang",
            "Linda Ruth Petzold"
        ],
        "published": "2023-11-02T16:11:09Z",
        "summary": "Automatically evaluating vision-language tasks is challenging, especially\nwhen it comes to reflecting human judgments due to limitations in accounting\nfor fine-grained details. Although GPT-4V has shown promising results in\nvarious multi-modal tasks, leveraging GPT-4V as a generalist evaluator for\nthese tasks has not yet been systematically explored. We comprehensively\nvalidate GPT-4V's capabilities for evaluation purposes, addressing tasks\nranging from foundational image-to-text and text-to-image synthesis to\nhigh-level image-to-image translations and multi-images to text alignment. We\nemploy two evaluation methods, single-answer grading and pairwise comparison,\nusing GPT-4V. Notably, GPT-4V shows promising agreement with humans across\nvarious tasks and evaluation methods, demonstrating immense potential for\nmulti-modal LLMs as evaluators. Despite limitations like restricted visual\nclarity grading and real-world complex reasoning, its ability to provide\nhuman-aligned scores enriched with detailed explanations is promising for\nuniversal automatic evaluator.",
        "pdf_link": "https://arxiv.org/pdf/2311.01361v1.pdf"
    },
    {
        "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
        "authors": [
            "Lovisa Hagstr\u00f6m",
            "Denitsa Saynova",
            "Tobias Norlund",
            "Moa Johansson",
            "Richard Johansson"
        ],
        "published": "2023-11-02T15:20:11Z",
        "summary": "Large Language Models (LLMs) make natural interfaces to factual knowledge,\nbut their usefulness is limited by their tendency to deliver inconsistent\nanswers to semantically equivalent questions. For example, a model might\npredict both \"Anne Redpath passed away in Edinburgh.\" and \"Anne Redpath's life\nended in London.\" In this work, we identify potential causes of inconsistency\nand evaluate the effectiveness of two mitigation strategies: up-scaling and\naugmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas\nmodels show that both strategies reduce inconsistency while retrieval\naugmentation is considerably more efficient. We further consider and\ndisentangle the consistency contributions of different components of Atlas. For\nall LMs evaluated we find that syntactical form and other evaluation task\nartifacts impact consistency. Taken together, our results provide a better\nunderstanding of the factors affecting the factual consistency of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2311.01307v1.pdf"
    },
    {
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
            "Ke Hong",
            "Guohao Dai",
            "Jiaming Xu",
            "Qiuli Mao",
            "Xiuhong Li",
            "Jun Liu",
            "Kangdi Chen",
            "Yuhan Dong",
            "Yu Wang"
        ],
        "published": "2023-11-02T14:57:03Z",
        "summary": "As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01282v4.pdf"
    },
    {
        "title": "Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations",
        "authors": [
            "Hanglei Zhang",
            "Yiwei Guo",
            "Sen Liu",
            "Xie Chen",
            "Kai Yu"
        ],
        "published": "2023-11-02T14:20:37Z",
        "summary": "Expressive text-to-speech (TTS) aims to synthesize speeches with human-like\ntones, moods, or even artistic attributes. Recent advancements in expressive\nTTS empower users with the ability to directly control synthesis style through\nnatural language prompts. However, these methods often require excessive\ntraining with a significant amount of style-annotated data, which can be\nchallenging to acquire. Moreover, they may have limited adaptability due to\nfixed style annotations. In this work, we present FreeStyleTTS (FS-TTS), a\ncontrollable expressive TTS model with minimal human annotations. Our approach\nutilizes a large language model (LLM) to transform expressive TTS into a style\nretrieval task. The LLM selects the best-matching style references from\nannotated utterances based on external style prompts, which can be raw input\ntext or natural language style descriptions. The selected reference guides the\nTTS pipeline to synthesize speeches with the intended style. This innovative\napproach provides flexible, versatile, and precise style control with minimal\nhuman workload. Experiments on a Mandarin storytelling corpus demonstrate\nFS-TTS's proficiency in leveraging LLM's semantic inference ability to retrieve\ndesired styles from either input text or user-defined descriptions. This\nresults in synthetic speeches that are closely aligned with the specified\nstyles.",
        "pdf_link": "https://arxiv.org/pdf/2311.01260v1.pdf"
    },
    {
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "authors": [
            "Mayank Kothyari",
            "Dhruva Dhingra",
            "Sunita Sarawagi",
            "Soumen Chakrabarti"
        ],
        "published": "2023-11-02T12:13:52Z",
        "summary": "Existing Text-to-SQL generators require the entire schema to be encoded with\nthe user text. This is expensive or impractical for large databases with tens\nof thousands of columns. Standard dense retrieval techniques are inadequate for\nschema subsetting of a large structured database, where the correct semantics\nof retrieval demands that we rank sets of schema elements rather than\nindividual elements. In response, we propose a two-stage process for effective\ncoverage during retrieval. First, we instruct an LLM to hallucinate a minimal\nDB schema deemed adequate to answer the query. We use the hallucinated schema\nto retrieve a subset of the actual schema, by composing the results from\nmultiple dense retrievals. Remarkably, hallucination $\\unicode{x2013}$\ngenerally considered a nuisance $\\unicode{x2013}$ turns out to be actually\nuseful as a bridging mechanism. Since no existing benchmarks exist for schema\nsubsetting on large databases, we introduce three benchmarks. Two\nsemi-synthetic datasets are derived from the union of schemas in two well-known\ndatasets, SPIDER and BIRD, resulting in 4502 and 798 schema elements\nrespectively. A real-life benchmark called SocialDB is sourced from an actual\nlarge data warehouse comprising 17844 schema elements. We show that our method1\nleads to significantly higher recall than SOTA retrieval-based augmentation\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2311.01173v1.pdf"
    },
    {
        "title": "Revisiting the Knowledge Injection Frameworks",
        "authors": [
            "Peng Fu",
            "Yiming Zhang",
            "Haobo Wang",
            "Weikang Qiu",
            "Junbo Zhao"
        ],
        "published": "2023-11-02T11:18:16Z",
        "summary": "In recent years, large language models (LLMs), such as GPTs, have attained\ngreat impact worldwide. However, how to adapt these LLMs to better suit the\nvertical domain-specific tasks by utilizing external knowledge remains not\ncompletely solved. Indeed, there have emerged a few works on this line where\nmost of them rely on an alignment heuristic that is built to inject the\ncorresponding knowledge tuple into the associated text sample.\n  However, despite the promise, we identify a pivotal problem in this work\nubiquitously. Simply put, we find that injecting unaligned (i.e., random)\nknowledge tuple into the LLMs achieves comparable (and sometimes better)\nresults than the aligned knowledge being injected. We therefore take a thorough\ninvestigation of this frustrating finding on a variety of related prior work\nand further provide a chain of potential interpretations for the phenomenon.\nBased on all that, we offer a simple remediated technique. Briefly, the core of\nthis technique is rooted in an ideological emphasis on the pruning and\npurification of the external knowledge base to be injected into LLMs. At last,\nwe show that by integrating this technique into most (if not all) knowledge\ninjection frameworks and recent LLMs, it manages to overcome the aforementioned\nsanity problem and further pushes the boundary of the performance of the\ndomain-adaptive LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01150v1.pdf"
    },
    {
        "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
        "authors": [
            "Jianghao Chen",
            "Pu Jian",
            "Tengxiao Xi",
            "Dongyi Yi",
            "Qianlong Du",
            "Chenglin Ding",
            "Guibo Zhu",
            "Chengqing Zong",
            "Jinqiao Wang",
            "Jiajun Zhang"
        ],
        "published": "2023-11-02T11:13:51Z",
        "summary": "During the development of large language models (LLMs), the scale and quality\nof the pre-training data play a crucial role in shaping LLMs' capabilities. To\naccelerate the research of LLMs, several large-scale datasets, such as C4 [1],\nPile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.\nHowever, most of the released corpus focus mainly on English, and there is\nstill lack of complete tool-chain for extracting clean texts from web data.\nFurthermore, fine-grained information of the corpus, e.g. the quality of each\ntext, is missing. To address these challenges, we propose in this paper a new\ncomplete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.\nFirst, similar to previous work, manually crafted rules are employed to discard\nexplicit noisy texts from the raw crawled web contents. Second, a well-designed\nevaluation model is leveraged to assess the remaining relatively clean data,\nand each text is assigned a specific quality score. Finally, we can easily\nutilize an appropriate threshold to select the high-quality pre-training data\nfor Chinese. Using our proposed approach, we release the largest and latest\nlarge-scale high-quality Chinese web text ChineseWebText, which consists of\n1.42 TB and each text is associated with a quality score, facilitating the LLM\nresearchers to choose the data according to the desired quality thresholds. We\nalso release a much cleaner subset of 600 GB Chinese data with the quality\nexceeding 90%.",
        "pdf_link": "https://arxiv.org/pdf/2311.01149v2.pdf"
    },
    {
        "title": "Making Harmful Behaviors Unlearnable for Large Language Models",
        "authors": [
            "Xin Zhou",
            "Yi Lu",
            "Ruotian Ma",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-11-02T09:18:21Z",
        "summary": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants in various domains. To meet the requirements of different\napplications, LLMs are often customized by further fine-tuning. However, the\npowerful learning ability of LLMs not only enables them to acquire new tasks\nbut also makes them susceptible to learning undesired behaviors. For example,\neven safety-aligned LLMs can be easily fine-tuned into harmful assistants as\nthe fine-tuning data often contains implicit or explicit harmful content. Can\nwe train LLMs on harmful data without learning harmful behaviors? This paper\nproposes a controllable training framework that makes harmful behaviors\nunlearnable during the fine-tuning process. Specifically, we introduce\n``security vectors'', a few new parameters that can be separated from the LLM,\nto ensure LLM's responses are consistent with the harmful behavior. Security\nvectors are activated during fine-tuning, the consistent behavior makes LLM\nbelieve that such behavior has already been learned, there is no need to\nfurther optimize for harmful data. During inference, we can deactivate security\nvectors to restore the LLM's normal behavior. The experimental results show\nthat the security vectors generated by 100 harmful samples are enough to\nprevent LLM from learning 1000 harmful samples, while preserving the ability to\nlearn other useful information.",
        "pdf_link": "https://arxiv.org/pdf/2311.02105v1.pdf"
    },
    {
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
        "authors": [
            "Zhenjie Yang",
            "Xiaosong Jia",
            "Hongyang Li",
            "Junchi Yan"
        ],
        "published": "2023-11-02T07:23:33Z",
        "summary": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
        "pdf_link": "https://arxiv.org/pdf/2311.01043v3.pdf"
    },
    {
        "title": "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism",
        "authors": [
            "Lang Cao"
        ],
        "published": "2023-11-02T07:20:49Z",
        "summary": "Large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, enabling them to answer a wide range\nof questions across various domains. However, these models are not flawless and\noften produce responses that contain errors or misinformation. These\ninaccuracies, commonly referred to as hallucinations, render LLMs unreliable\nand even unusable in many scenarios. In this paper, our focus is on mitigating\nthe issue of hallucination in LLMs, particularly in the context of\nquestion-answering. Instead of attempting to answer all questions, we explore a\nrefusal mechanism that instructs LLMs to refuse to answer challenging questions\nin order to avoid errors. We then propose a simple yet effective solution\ncalled Learn to Refuse (L2R), which incorporates the refusal mechanism to\nenable LLMs to recognize and refuse to answer questions that they find\ndifficult to address. To achieve this, we utilize a structured knowledge base\nto represent all the LLM's understanding of the world, enabling it to provide\ntraceable gold knowledge. This knowledge base is separate from the LLM and\ninitially empty, and it is progressively expanded with validated knowledge.\nWhen an LLM encounters questions outside its domain, the system recognizes its\nknowledge scope and determines whether it can answer the question\nindependently. Additionally, we introduce a method for automatically and\nefficiently expanding the knowledge base of LLMs. Through qualitative and\nquantitative analysis, we demonstrate that our approach enhances the\ncontrollability and reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2311.01041v1.pdf"
    },
    {
        "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
        "authors": [
            "Sam Toyer",
            "Olivia Watkins",
            "Ethan Adrian Mendes",
            "Justin Svegliato",
            "Luke Bailey",
            "Tiffany Wang",
            "Isaac Ong",
            "Karim Elmaaroufi",
            "Pieter Abbeel",
            "Trevor Darrell",
            "Alan Ritter",
            "Stuart Russell"
        ],
        "published": "2023-11-02T06:13:36Z",
        "summary": "While Large Language Models (LLMs) are increasingly being used in real-world\napplications, they remain vulnerable to prompt injection attacks: malicious\nthird party prompts that subvert the intent of the system designer. To help\nresearchers study this problem, we present a dataset of over 126,000 prompt\ninjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\nall created by players of an online game called Tensor Trust. To the best of\nour knowledge, this is currently the largest dataset of human-generated\nadversarial examples for instruction-following LLMs. The attacks in our dataset\nhave a lot of easily interpretable stucture, and shed light on the weaknesses\nof LLMs. We also use the dataset to create a benchmark for resistance to two\ntypes of prompt injection, which we refer to as prompt extraction and prompt\nhijacking. Our benchmark results show that many models are vulnerable to the\nattack strategies in the Tensor Trust dataset. Furthermore, we show that some\nattack strategies from the dataset generalize to deployed LLM-based\napplications, even though they have a very different set of constraints to the\ngame. We release all data and source code at https://tensortrust.ai/paper",
        "pdf_link": "https://arxiv.org/pdf/2311.01011v1.pdf"
    },
    {
        "title": "POS: A Prompts Optimization Suite for Augmenting Text-to-Video Generation",
        "authors": [
            "Shijie Ma",
            "Huayi Xu",
            "Mengjian Li",
            "Weidong Geng",
            "Meng Wang",
            "Yaxiong Wang"
        ],
        "published": "2023-11-02T02:33:09Z",
        "summary": "This paper targets to enhance the diffusion-based text-to-video generation by\nimproving the two input prompts, including the noise and the text. Accommodated\nwith this goal, we propose POS, a training-free Prompt Optimization Suite to\nboost text-to-video models. POS is motivated by two observations: (1) Video\ngeneration shows instability in terms of noise. Given the same text, different\nnoises lead to videos that differ significantly in terms of both frame quality\nand temporal consistency. This observation implies that there exists an optimal\nnoise matched to each textual input; To capture the potential noise, we propose\nan optimal noise approximator to approach the potential optimal noise.\nParticularly, the optimal noise approximator initially searches a video that\nclosely relates to the text prompt and then inverts it into the noise space to\nserve as an improved noise prompt for the textual input. (2) Improving the text\nprompt via LLMs often causes semantic deviation. Many existing text-to-vision\nworks have utilized LLMs to improve the text prompts for generation\nenhancement. However, existing methods often neglect the semantic alignment\nbetween the original text and the rewritten one. In response to this issue, we\ndesign a semantic-preserving rewriter to impose contraints in both rewritng and\ndenoising phrases to preserve the semantic consistency. Extensive experiments\non popular benchmarks show that our POS can improve the text-to-video models\nwith a clear margin. The code will be open-sourced.",
        "pdf_link": "https://arxiv.org/pdf/2311.00949v2.pdf"
    },
    {
        "title": "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place",
        "authors": [
            "Wentao Yuan",
            "Adithyavairavan Murali",
            "Arsalan Mousavian",
            "Dieter Fox"
        ],
        "published": "2023-11-02T01:42:52Z",
        "summary": "With the advent of large language models and large-scale robotic datasets,\nthere has been tremendous progress in high-level decision-making for object\nmanipulation. These generic models are able to interpret complex tasks using\nlanguage commands, but they often have difficulties generalizing to\nout-of-distribution objects due to the inability of low-level action\nprimitives. In contrast, existing task-specific models excel in low-level\nmanipulation of unknown objects, but only work for a single type of action. To\nbridge this gap, we present M2T2, a single model that supplies different types\nof low-level actions that work robustly on arbitrary objects in cluttered\nscenes. M2T2 is a transformer model which reasons about contact points and\npredicts valid gripper poses for different action modes given a raw point cloud\nof the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2\nachieves zero-shot sim2real transfer on the real robot, outperforming the\nbaseline system with state-of-the-art task-specific models by about 19% in\noverall performance and 37.5% in challenging scenes where the object needs to\nbe re-oriented for collision-free placement. M2T2 also achieves\nstate-of-the-art results on a subset of language conditioned tasks in RLBench.\nVideos of robot experiments on unseen objects in both real world and simulation\nare available on our project website https://m2-t2.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2311.00926v1.pdf"
    },
    {
        "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
        "authors": [
            "Zedian Xiao",
            "William Held",
            "Yanchen Liu",
            "Diyi Yang"
        ],
        "published": "2023-11-02T01:17:29Z",
        "summary": "Large Language Models (LLMs) are trained on corpora disproportionally\nweighted in favor of Standard American English. As a result, speakers of other\ndialects experience significantly more failures when interacting with these\ntechnologies. In practice, these speakers often accommodate their speech to be\nbetter understood. Our work shares the belief that language technologies should\nbe designed to accommodate the diversity in English dialects and not the other\nway around. However, prior works on dialect struggle with generalizing to\nevolving and emerging dialects in a scalable manner. To fill this gap, our\nmethod, HyperLoRA, leverages expert linguistic knowledge to enable\nresource-efficient adaptation via hypernetworks. By disentangling\ndialect-specific and cross-dialectal information, HyperLoRA improves\ngeneralization to unseen dialects in a task-agnostic fashion. Not only is\nHyperLoRA more scalable in the number of parameters, but it also achieves the\nbest or most competitive performance across 5 dialects in a zero-shot setting.\nIn this way, our approach facilitates access to language technology for\nbillions of English dialect speakers who are traditionally underrepresented.",
        "pdf_link": "https://arxiv.org/pdf/2311.00915v1.pdf"
    },
    {
        "title": "Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code",
        "authors": [
            "Mohammed Latif Siddiq",
            "Joanna C. S. Santos"
        ],
        "published": "2023-11-01T22:46:31Z",
        "summary": "With the growing popularity of Large Language Models (e.g. GitHub Copilot,\nChatGPT, etc.) in software engineers' daily practices, it is important to\nensure that the code generated by these tools is not only functionally correct\nbut also free of vulnerabilities. Although LLMs can help developers to be more\nproductive, prior empirical studies have shown that LLMs can generate insecure\ncode. There are two contributing factors to the insecure code generation.\nFirst, existing datasets used to evaluate Large Language Models (LLMs) do not\nadequately represent genuine software engineering tasks sensitive to security.\nInstead, they are often based on competitive programming challenges or\nclassroom-type coding tasks. In real-world applications, the code produced is\nintegrated into larger codebases, introducing potential security risks. There's\na clear absence of benchmarks that focus on evaluating the security of the\ngenerated code. Second, existing evaluation metrics primarily focus on the\nfunctional correctness of the generated code while ignoring security\nconsiderations. Metrics such as pass@k gauge the probability of obtaining the\ncorrect code in the top k suggestions. Other popular metrics like BLEU,\nCodeBLEU, ROUGE, and METEOR similarly emphasize functional accuracy, neglecting\nsecurity implications. In light of these research gaps, in this paper, we\ndescribed SALLM, a framework to benchmark LLMs' abilities to generate secure\ncode systematically. This framework has three major components: a novel dataset\nof security-centric Python prompts, an evaluation environment to test the\ngenerated code, and novel metrics to evaluate the models' performance from the\nperspective of secure code generation.",
        "pdf_link": "https://arxiv.org/pdf/2311.00889v1.pdf"
    },
    {
        "title": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models",
        "authors": [
            "Steve Yadlowsky",
            "Lyric Doshi",
            "Nilesh Tripuraneni"
        ],
        "published": "2023-11-01T21:41:08Z",
        "summary": "Transformer models, notably large language models (LLMs), have the remarkable\nability to perform in-context learning (ICL) -- to perform new tasks when\nprompted with unseen input-output examples without any explicit model training.\nIn this work, we study how effectively transformers can bridge between their\npretraining data mixture, comprised of multiple distinct task families, to\nidentify and learn new tasks in-context which are both inside and outside the\npretraining distribution. Building on previous work, we investigate this\nquestion in a controlled setting, where we study transformer models trained on\nsequences of $(x, f(x))$ pairs rather than natural language. Our empirical\nresults show transformers demonstrate near-optimal unsupervised model selection\ncapabilities, in their ability to first in-context identify different task\nfamilies and in-context learn within them when the task families are\nwell-represented in their pretraining data. However when presented with tasks\nor functions which are out-of-domain of their pretraining data, we demonstrate\nvarious failure modes of transformers and degradation of their generalization\nfor even simple extrapolation tasks. Together our results highlight that the\nimpressive ICL abilities of high-capacity sequence models may be more closely\ntied to the coverage of their pretraining data mixtures than inductive biases\nthat create fundamental generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2311.00871v1.pdf"
    },
    {
        "title": "SAGE: Smart home Agent with Grounded Execution",
        "authors": [
            "Dmitriy Rivkin",
            "Francois Hogan",
            "Amal Feriani",
            "Abhisek Konar",
            "Adam Sigal",
            "Steve Liu",
            "Greg Dudek"
        ],
        "published": "2023-11-01T18:36:28Z",
        "summary": "The common sense reasoning abilities and vast general knowledge of Large\nLanguage Models (LLMs) make them a natural fit for interpreting user requests\nin a Smart Home assistant context. LLMs, however, lack specific knowledge about\nthe user and their home limit their potential impact. SAGE (Smart Home Agent\nwith Grounded Execution), overcomes these and other limitations by using a\nscheme in which a user request triggers an LLM-controlled sequence of discrete\nactions. These actions can be used to retrieve information, interact with the\nuser, or manipulate device states. SAGE controls this process through a\ndynamically constructed tree of LLM prompts, which help it decide which action\nto take next, whether an action was successful, and when to terminate the\nprocess. The SAGE action set augments an LLM's capabilities to support some of\nthe most critical requirements for a Smart Home assistant. These include:\nflexible and scalable user preference management (\"is my team playing\ntonight?\"), access to any smart device's full functionality without\ndevice-specific code via API reading \"turn down the screen brightness on my\ndryer\", persistent device state monitoring (\"remind me to throw out the milk\nwhen I open the fridge\"), natural device references using only a photo of the\nroom (\"turn on the light on the dresser\"), and more. We introduce a benchmark\nof 50 new and challenging smart home tasks where SAGE achieves a 75% success\nrate, significantly outperforming existing LLM-enabled baselines (30% success\nrate).",
        "pdf_link": "https://arxiv.org/pdf/2311.00772v2.pdf"
    },
    {
        "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
        "authors": [
            "Zhan Ling",
            "Yunhao Fang",
            "Xuanlin Li",
            "Tongzhou Mu",
            "Mingu Lee",
            "Reza Pourreza",
            "Roland Memisevic",
            "Hao Su"
        ],
        "published": "2023-11-01T17:52:15Z",
        "summary": "Large Language Models (LLMs) have achieved tremendous progress, yet they\nstill often struggle with challenging reasoning problems. Current approaches\naddress this challenge by sampling or searching detailed and low-level\nreasoning chains. However, these methods are still limited in their exploration\ncapabilities, making it challenging for correct solutions to stand out in the\nhuge solution space. In this work, we unleash LLMs' creative potential for\nexploring multiple diverse problem solving strategies by framing an LLM as a\nhierarchical policy via in-context learning. This policy comprises of a\nvisionary leader that proposes multiple diverse high-level problem-solving\ntactics as hints, accompanied by a follower that executes detailed\nproblem-solving processes following each of the high-level instruction. The\nfollower uses each of the leader's directives as a guide and samples multiple\nreasoning chains to tackle the problem, generating a solution group for each\nleader proposal. Additionally, we propose an effective and efficient\ntournament-based approach to select among these explored solution groups to\nreach the final answer. Our approach produces meaningful and inspiring hints,\nenhances problem-solving strategy exploration, and improves the final answer\naccuracy on challenging problems in the MATH dataset. Code will be released at\nhttps://github.com/lz1oceani/LLM-As-Hierarchical-Policy.",
        "pdf_link": "https://arxiv.org/pdf/2311.00694v2.pdf"
    },
    {
        "title": "Improving Interpersonal Communication by Simulating Audiences with Language Models",
        "authors": [
            "Ryan Liu",
            "Howard Yen",
            "Raja Marjieh",
            "Thomas L. Griffiths",
            "Ranjay Krishna"
        ],
        "published": "2023-11-01T17:44:50Z",
        "summary": "How do we communicate with others to achieve our goals? We use our prior\nexperience or advice from others, or construct a candidate utterance by\npredicting how it will be received. However, our experiences are limited and\nbiased, and reasoning about potential outcomes can be difficult and cognitively\nchallenging. In this paper, we explore how we can leverage Large Language Model\n(LLM) simulations to help us communicate better. We propose the\nExplore-Generate-Simulate (EGS) framework, which takes as input any scenario\nwhere an individual is communicating to an audience with a goal they want to\nachieve. EGS (1) explores the solution space by producing a diverse set of\nadvice relevant to the scenario, (2) generates communication candidates\nconditioned on subsets of the advice, and (3) simulates the reactions from\nvarious audiences to determine both the best candidate and advice to use. We\nevaluate the framework on eight scenarios spanning the ten fundamental\nprocesses of interpersonal communication. For each scenario, we collect a\ndataset of human evaluations across candidates and baselines, and showcase that\nour framework's chosen candidate is preferred over popular generation\nmechanisms including Chain-of-Thought. We also find that audience simulations\nachieve reasonably high agreement with human raters across 5 of the 8\nscenarios. Finally, we demonstrate the generality of our framework by applying\nit to real-world scenarios described by users on web forums. Through\nevaluations and demonstrations, we show that EGS enhances the effectiveness and\noutcomes of goal-oriented communication across a variety of situations, thus\nopening up new possibilities for the application of large language models in\nrevolutionizing communication and decision-making processes.",
        "pdf_link": "https://arxiv.org/pdf/2311.00687v2.pdf"
    },
    {
        "title": "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Alexander I. Rudnicky"
        ],
        "published": "2023-11-01T17:43:35Z",
        "summary": "An ideal length-extrapolatable Transformer language model can handle\nsequences longer than the training length without any fine-tuning. Such\nlong-context utilization capability relies heavily on a flexible positional\nembedding design. Upon investigating the flexibility of existing large\npre-trained Transformer language models, we find that the T5 family deserves a\ncloser look, as its positional embeddings capture rich and flexible attention\npatterns. However, T5 suffers from the dispersed attention issue: the longer\nthe input sequence, the flatter the attention distribution. To alleviate the\nissue, we propose two attention alignment strategies via temperature scaling.\nOur findings show improvement on the long-context utilization capability of T5\non language modeling, retrieval, multi-document question answering, and code\ncompletion tasks without any fine-tuning. This suggests that a flexible\npositional embedding design and attention alignment can go a long way toward\nTransformer length extrapolation.",
        "pdf_link": "https://arxiv.org/pdf/2311.00684v2.pdf"
    },
    {
        "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
        "authors": [
            "Xue-Yong Fu",
            "Md Tahmid Rahman Laskar",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2023-11-01T17:42:45Z",
        "summary": "In recent years, Large Language Models (LLMs) have gained immense attention\ndue to their notable emergent capabilities, surpassing those seen in earlier\nlanguage models. A particularly intriguing application of LLMs is their role as\nevaluators for texts produced by various generative models.\n  In this study, we delve into the potential of LLMs as reliable assessors of\nfactual consistency in summaries generated by text-generation models.\nInitially, we introduce an innovative approach for factuality assessment using\nLLMs. This entails employing a singular LLM for the entirety of the\nquestion-answering-based factuality scoring process. Following this, we examine\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\nagainst traditional measures and human annotations.\n  Contrary to initial expectations, our results indicate a lack of significant\ncorrelations between factuality metrics and human evaluations, specifically for\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\ntwo factuality subcategories. These consistent findings across various factual\nerror categories suggest a fundamental limitation in the current LLMs'\ncapability to accurately gauge factuality.\n  This version presents the information more concisely while maintaining the\nmain points and findings of the original text.",
        "pdf_link": "https://arxiv.org/pdf/2311.00681v1.pdf"
    },
    {
        "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
        "authors": [
            "Xiaoqian Li",
            "Ercong Nie",
            "Sheng Liang"
        ],
        "published": "2023-11-01T15:32:50Z",
        "summary": "The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.",
        "pdf_link": "https://arxiv.org/pdf/2311.00587v2.pdf"
    },
    {
        "title": "Can Large Language Models Design Accurate Label Functions?",
        "authors": [
            "Naiqing Guan",
            "Kaiwen Chen",
            "Nick Koudas"
        ],
        "published": "2023-11-01T15:14:46Z",
        "summary": "Programmatic weak supervision methodologies facilitate the expedited labeling\nof extensive datasets through the use of label functions (LFs) that encapsulate\nheuristic data sources. Nonetheless, the creation of precise LFs necessitates\ndomain expertise and substantial endeavors. Recent advances in pre-trained\nlanguage models (PLMs) have exhibited substantial potential across diverse\ntasks. However, the capacity of PLMs to autonomously formulate accurate LFs\nremains an underexplored domain. In this research, we address this gap by\nintroducing DataSculpt, an interactive framework that harnesses PLMs for the\nautomated generation of LFs. Within DataSculpt, we incorporate an array of\nprompting techniques, instance selection strategies, and LF filtration methods\nto explore the expansive design landscape. Ultimately, we conduct a thorough\nassessment of DataSculpt's performance on 12 real-world datasets, encompassing\na range of tasks. This evaluation unveils both the strengths and limitations of\ncontemporary PLMs in LF design.",
        "pdf_link": "https://arxiv.org/pdf/2311.00739v1.pdf"
    },
    {
        "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation",
        "authors": [
            "Xiangjue Dong",
            "Yibo Wang",
            "Philip S. Yu",
            "James Caverlee"
        ],
        "published": "2023-11-01T05:31:46Z",
        "summary": "Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.",
        "pdf_link": "https://arxiv.org/pdf/2311.00306v1.pdf"
    },
    {
        "title": "Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions",
        "authors": [
            "Taehyeon Kim",
            "Joonkee Kim",
            "Gihun Lee",
            "Se-Young Yun"
        ],
        "published": "2023-11-01T02:31:35Z",
        "summary": "While instruction-tuned language models have demonstrated impressive\nzero-shot generalization, these models often struggle to generate accurate\nresponses when faced with instructions that fall outside their training set.\nThis paper presents Instructive Decoding (ID), a simple yet effective approach\nthat augments the efficacy of instruction-tuned models. Specifically, ID\nadjusts the logits for next-token prediction in a contrastive manner, utilizing\npredictions generated from a manipulated version of the original instruction,\nreferred to as a noisy instruction. This noisy instruction aims to elicit\nresponses that could diverge from the intended instruction yet remain\nplausible. We conduct experiments across a spectrum of such noisy instructions,\nranging from those that insert semantic noise via random words to others like\n'opposite' that elicit the deviated responses. Our approach achieves\nconsiderable performance gains across various instruction-tuned models and\ntasks without necessitating any additional parameter updates. Notably,\nutilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum\ndivergence from the original instruction, consistently produces the most\nsignificant performance gains across multiple models and tasks.",
        "pdf_link": "https://arxiv.org/pdf/2311.00233v2.pdf"
    },
    {
        "title": "Is GPT Powerful Enough to Analyze the Emotions of Memes?",
        "authors": [
            "Jingjing Wang",
            "Joshua Luo",
            "Grace Yang",
            "Allen Hong",
            "Feng Luo"
        ],
        "published": "2023-11-01T01:57:48Z",
        "summary": "Large Language Models (LLMs), representing a significant achievement in\nartificial intelligence (AI) research, have demonstrated their ability in a\nmultitude of tasks. This project aims to explore the capabilities of GPT-3.5, a\nleading example of LLMs, in processing the sentiment analysis of Internet\nmemes. Memes, which include both verbal and visual aspects, act as a powerful\nyet complex tool for expressing ideas and sentiments, demanding an\nunderstanding of societal norms and cultural contexts. Notably, the detection\nand moderation of hateful memes pose a significant challenge due to their\nimplicit offensive nature. This project investigates GPT's proficiency in such\nsubjective tasks, revealing its strengths and potential limitations. The tasks\ninclude the classification of meme sentiment, determination of humor type, and\ndetection of implicit hate in memes. The performance evaluation, using datasets\nfrom SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative\nunderstanding of GPT responses against human annotations. Despite GPT's\nremarkable progress, our findings underscore the challenges faced by these\nmodels in handling subjective tasks, which are rooted in their inherent\nlimitations including contextual understanding, interpretation of implicit\nmeanings, and data biases. This research contributes to the broader discourse\non the applicability of AI in handling complex, context-dependent tasks, and\noffers valuable insights for future advancements.",
        "pdf_link": "https://arxiv.org/pdf/2311.00223v1.pdf"
    },
    {
        "title": "Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias",
        "authors": [
            "S. Lee",
            "T. Q. Peng",
            "M. H. Goldberg",
            "S. A. Rosenthal",
            "J. E. Kotcher",
            "E. W. Maibach",
            "A. Leiserowitz"
        ],
        "published": "2023-11-01T01:32:59Z",
        "summary": "Large language models (LLMs) have demonstrated their potential in social\nscience research by emulating human perceptions and behaviors, a concept\nreferred to as algorithmic fidelity. This study assesses the algorithmic\nfidelity and bias of LLMs by utilizing two nationally representative climate\nchange surveys. The LLMs were conditioned on demographics and/or psychological\ncovariates to simulate survey responses. The findings indicate that LLMs can\neffectively capture presidential voting behaviors but encounter challenges in\naccurately representing global warming perspectives when relevant covariates\nare not included. GPT-4 exhibits improved performance when conditioned on both\ndemographics and covariates. However, disparities emerge in LLM estimations of\nthe views of certain groups, with LLMs tending to underestimate worry about\nglobal warming among Black Americans. While highlighting the potential of LLMs\nto aid social science research, these results underscore the importance of\nmeticulous conditioning, model selection, survey question format, and bias\nassessment when employing LLMs for survey simulation. Further investigation\ninto prompt engineering and algorithm auditing is essential to harness the\npower of LLMs while addressing their inherent limitations.",
        "pdf_link": "https://arxiv.org/pdf/2311.00217v2.pdf"
    },
    {
        "title": "ChatGPT-Powered Hierarchical Comparisons for Image Classification",
        "authors": [
            "Zhiyuan Ren",
            "Yiyang Su",
            "Xiaoming Liu"
        ],
        "published": "2023-11-01T00:26:40Z",
        "summary": "The zero-shot open-vocabulary challenge in image classification is tackled by\npretrained vision-language models like CLIP, which benefit from incorporating\nclass-specific knowledge from large language models (LLMs) like ChatGPT.\nHowever, biases in CLIP lead to similar descriptions for distinct but related\nclasses, prompting our novel image classification framework via hierarchical\ncomparisons: using LLMs to recursively group classes into hierarchies and\nclassifying images by comparing image-text embeddings at each hierarchy level,\nresulting in an intuitive, effective, and explainable approach.",
        "pdf_link": "https://arxiv.org/pdf/2311.00206v1.pdf"
    },
    {
        "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
        "authors": [
            "Nathan Lambert",
            "Roberto Calandra"
        ],
        "published": "2023-10-31T21:52:41Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful\ntechnique to make large language models (LLMs) more capable in complex\nsettings. RLHF proceeds as collecting human preference data, training a reward\nmodel on said data, and optimizing a base ML model with respect to said reward\nfor extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many\nassumptions about how the various pieces fit together, such as a reward model\ncapturing human preferences and an RL optimizer extracting the right signal\nfrom a reward model. As the RLHF process involves many distinct design\ndecisions, it is easy to assume that multiple processes are correlated and\ntherefore numerically linked. This apparent correlation is often not true,\nwhere reward models are easily overoptimized or RL optimizers can reduce\nperformance on tasks not modeled in the data. Notable manifestations of models\ntrained with imperfect RLHF systems are those that are prone to refusing basic\nrequests for safety reasons or appearing lazy in generations. As chat model\nevaluation becomes increasingly nuanced, the reliance on a perceived link\nbetween reward model training, RL scores, and downstream performance drives\nthese issues, which we describe as an objective mismatch. In this paper, we\nillustrate the causes of this issue, reviewing relevant literature from\nmodel-based reinforcement learning, and argue for solutions. By solving\nobjective mismatch in RLHF, the ML models of the future will be more precisely\naligned to user instructions for both safety and helpfulness.",
        "pdf_link": "https://arxiv.org/pdf/2311.00168v2.pdf"
    },
    {
        "title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "authors": [
            "Pranav Gade",
            "Simon Lermen",
            "Charlie Rogers-Smith",
            "Jeffrey Ladish"
        ],
        "published": "2023-10-31T19:45:15Z",
        "summary": "Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.",
        "pdf_link": "https://arxiv.org/pdf/2311.00117v2.pdf"
    },
    {
        "title": "BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text",
        "authors": [
            "Aarohi Srivastava",
            "David Chiang"
        ],
        "published": "2023-10-31T19:44:50Z",
        "summary": "Real-world NLP applications often deal with nonstandard text (e.g.,\ndialectal, informal, or misspelled text). However, language models like BERT\ndeteriorate in the face of dialect variation or noise. How do we push BERT's\nmodeling capabilities to encompass nonstandard text? Fine-tuning helps, but it\nis designed for specializing a model to a task and does not seem to bring about\nthe deeper, more pervasive changes needed to adapt a model to nonstandard\nlanguage. In this paper, we introduce the novel idea of sandwiching BERT's\nencoder stack between additional encoder layers trained to perform masked\nlanguage modeling on noisy text. We find that our approach, paired with recent\nwork on including character-level noise in fine-tuning data, can promote\nzero-shot transfer to dialectal text, as well as reduce the distance in the\nembedding space between words and their noisy counterparts.",
        "pdf_link": "https://arxiv.org/pdf/2311.00116v1.pdf"
    },
    {
        "title": "Filter bubbles and affective polarization in user-personalized large language model outputs",
        "authors": [
            "Tomo Lazovich"
        ],
        "published": "2023-10-31T18:19:28Z",
        "summary": "Echoing the history of search engines and social media content rankings, the\nadvent of large language models (LLMs) has led to a push for increased\npersonalization of model outputs to individual users. In the past, personalized\nrecommendations and ranking systems have been linked to the development of\nfilter bubbles (serving content that may confirm a user's existing biases) and\naffective polarization (strong negative sentiment towards those with differing\nviews). In this work, we explore how prompting a leading large language model,\nChatGPT-3.5, with a user's political affiliation prior to asking factual\nquestions about public figures and organizations leads to differing results. We\nobserve that left-leaning users tend to receive more positive statements about\nleft-leaning political figures and media outlets, while right-leaning users see\nmore positive statements about right-leaning entities. This pattern holds\nacross presidential candidates, members of the U.S. Senate, and media\norganizations with ratings from AllSides. When qualitatively evaluating some of\nthese outputs, there is evidence that particular facts are included or excluded\nbased on the user's political affiliation. These results illustrate that\npersonalizing LLMs based on user demographics carry the same risks of affective\npolarization and filter bubbles that have been seen in other personalized\ninternet technologies. This ``failure mode\" should be monitored closely as\nthere are more attempts to monetize and personalize these models.",
        "pdf_link": "https://arxiv.org/pdf/2311.14677v1.pdf"
    },
    {
        "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
        "authors": [
            "Simon Lermen",
            "Charlie Rogers-Smith",
            "Jeffrey Ladish"
        ],
        "published": "2023-10-31T16:55:06Z",
        "summary": "AI developers often apply safety alignment procedures to prevent the misuse\nof their AI systems. For example, before Meta released Llama 2-Chat, a\ncollection of instruction fine-tuned large language models, they invested\nheavily in safety training, incorporating extensive red-teaming and\nreinforcement learning from human feedback. However, it remains unclear how\nwell safety training guards against model misuse when attackers have access to\nmodel weights. We explore the robustness of safety training in language models\nby subversively fine-tuning the public weights of Llama 2-Chat. We employ\nlow-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of\nless than $200 per model and using only one GPU, we successfully undo the\nsafety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,\nour fine-tuning technique significantly reduces the rate at which the model\nrefuses to follow harmful instructions. We achieve a refusal rate below 1% for\nour 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method\nretains general performance, which we validate by comparing our fine-tuned\nmodels against Llama 2-Chat across two benchmarks. Additionally, we present a\nselection of harmful outputs produced by our models. While there is\nconsiderable uncertainty about the scope of risks from current models, it is\nlikely that future models will have significantly more dangerous capabilities,\nincluding the ability to hack into critical infrastructure, create dangerous\nbio-weapons, or autonomously replicate and adapt to new environments. We show\nthat subversive fine-tuning is practical and effective, and hence argue that\nevaluating risks from fine-tuning should be a core part of risk assessments for\nreleasing model weights.",
        "pdf_link": "https://arxiv.org/pdf/2310.20624v1.pdf"
    },
    {
        "title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding",
        "authors": [
            "Yuqi Wang",
            "Zeqiang Wang",
            "Wei Wang",
            "Qi Chen",
            "Kaizhu Huang",
            "Anh Nguyen",
            "Suparna De"
        ],
        "published": "2023-10-31T16:26:33Z",
        "summary": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.",
        "pdf_link": "https://arxiv.org/pdf/2310.20588v1.pdf"
    },
    {
        "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
        "authors": [
            "Ruizhe Shi",
            "Yuyao Liu",
            "Yanjie Ze",
            "Simon S. Du",
            "Huazhe Xu"
        ],
        "published": "2023-10-31T16:24:17Z",
        "summary": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks\nand closes the gap between value-based offline RL methods and decision\ntransformers in dense-reward tasks. In particular, our method demonstrates\nsuperior performance in scenarios with limited data samples.",
        "pdf_link": "https://arxiv.org/pdf/2310.20587v4.pdf"
    },
    {
        "title": "Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT",
        "authors": [
            "Aman Jaiswal",
            "Evangelos Milios"
        ],
        "published": "2023-10-31T15:41:08Z",
        "summary": "Transformer-based models, specifically BERT, have propelled research in\nvarious NLP tasks. However, these models are limited to a maximum token limit\nof 512 tokens. Consequently, this makes it non-trivial to apply it in a\npractical setting with long input. Various complex methods have claimed to\novercome this limit, but recent research questions the efficacy of these models\nacross different classification tasks. These complex architectures evaluated on\ncarefully curated long datasets perform at par or worse than simple baselines.\nIn this work, we propose a relatively simple extension to vanilla BERT\narchitecture called ChunkBERT that allows finetuning of any pretrained models\nto perform inference on arbitrarily long text. The proposed method is based on\nchunking token representations and CNN layers, making it compatible with any\npre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for\ncomparing long-text classification models across a variety of tasks (including\nbinary classification, multi-class classification, and multi-label\nclassification). A BERT model finetuned using the ChunkBERT method performs\nconsistently across long samples in the benchmark while utilizing only a\nfraction (6.25\\%) of the original memory footprint. These findings suggest that\nefficient finetuning and inference can be achieved through simple modifications\nto pre-trained BERT models.",
        "pdf_link": "https://arxiv.org/pdf/2310.20558v1.pdf"
    },
    {
        "title": "LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts",
        "authors": [
            "Sunhao Dai",
            "Yuqi Zhou",
            "Liang Pang",
            "Weihao Liu",
            "Xiaolin Hu",
            "Yong Liu",
            "Xiao Zhang",
            "Gang Wang",
            "Jun Xu"
        ],
        "published": "2023-10-31T14:42:23Z",
        "summary": "Recently, the emergence of large language models (LLMs) has revolutionized\nthe paradigm of information retrieval (IR) applications, especially in web\nsearch. With their remarkable capabilities in generating human-like texts, LLMs\nhave created enormous texts on the Internet. As a result, IR systems in the\nLLMs era are facing a new challenge: the indexed documents now are not only\nwritten by human beings but also automatically generated by the LLMs. How these\nLLM-generated documents influence the IR systems is a pressing and still\nunexplored question. In this work, we conduct a quantitative evaluation of\ndifferent IR models in scenarios where both human-written and LLM-generated\ntexts are involved. Surprisingly, our findings indicate that neural retrieval\nmodels tend to rank LLM-generated documents higher. We refer to this category\nof biases in neural retrieval models towards the LLM-generated text as the\n\\textbf{source bias}. Moreover, we discover that this bias is not confined to\nthe first-stage neural retrievers, but extends to the second-stage neural\nre-rankers. Then, we provide an in-depth analysis from the perspective of text\ncompression and observe that neural models can better understand the semantic\ninformation of LLM-generated text, which is further substantiated by our\ntheoretical analysis. To mitigate the source bias, we also propose a\nplug-and-play debiased constraint for the optimization objective, and\nexperimental results show the effectiveness. Finally, we discuss the potential\nsevere concerns stemming from the observed source bias and hope our findings\ncan serve as a critical wake-up call to the IR community and beyond. To\nfacilitate future explorations of IR in the LLM era, the constructed two new\nbenchmarks and codes will later be available at\n\\url{https://github.com/KID-22/LLM4IR-Bias}.",
        "pdf_link": "https://arxiv.org/pdf/2310.20501v2.pdf"
    },
    {
        "title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Yufei Wang",
            "Xingshan Zeng",
            "Wanjun Zhong",
            "Liangyou Li",
            "Fei Mi",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Wei Wang"
        ],
        "published": "2023-10-31T12:32:38Z",
        "summary": "The ability to follow instructions is crucial for Large Language Models\n(LLMs) to handle various real-world applications. Existing benchmarks primarily\nfocus on evaluating pure response quality, rather than assessing whether the\nresponse follows constraints stated in the instruction. To fill this research\ngap, in this paper, we propose FollowBench, a Multi-level Fine-grained\nConstraints Following Benchmark for LLMs. FollowBench comprehensively includes\nfive different types (i.e., Content, Situation, Style, Format, and Example) of\nfine-grained constraints. To enable a precise constraint following estimation\non diverse difficulties, we introduce a Multi-level mechanism that\nincrementally adds a single constraint to the initial instruction at each\nincreased level. To assess whether LLMs' outputs have satisfied every\nindividual constraint, we propose to prompt strong LLMs with\nconstraint-evolution paths to handle challenging open-ended instructions. By\nevaluating ten closed-source and open-source popular LLMs on FollowBench, we\nhighlight the weaknesses of LLMs in instruction following and point towards\npotential avenues for future work. The data and code are publicly available at\nhttps://github.com/YJiangcm/FollowBench.",
        "pdf_link": "https://arxiv.org/pdf/2310.20410v2.pdf"
    },
    {
        "title": "InstructCoder: Instruction Tuning Large Language Models for Code Editing",
        "authors": [
            "Kaixin Li",
            "Qisheng Hu",
            "Xu Zhao",
            "Hui Chen",
            "Yuxi Xie",
            "Tiedong Liu",
            "Qizhe Xie",
            "Junxian He"
        ],
        "published": "2023-10-31T10:15:35Z",
        "summary": "Code editing encompasses a variety of pragmatic tasks that developers deal\nwith daily. Despite its relevance and practical usefulness, automatic code\nediting remains an underexplored area in the evolution of deep learning models,\npartly due to data scarcity. In this work, we explore the use of Large Language\nModels (LLMs) to edit code based on user instructions. Evaluated on a novel\nhuman-written execution-based benchmark dubbed EditEval, we found current\nmodels often struggle to fulfill the instructions. In light of this, we\ncontribute InstructCoder, the first instruction-tuning dataset designed to\nadapt LLMs for general-purpose code editing, containing high-diversity\ncode-editing tasks such as comment insertion, code optimization, and code\nrefactoring. It consists of over 114,000 instruction-input-output triplets and\ncovers multiple distinct code editing scenarios. The collection process starts\nwith filtered commit data sourced from GitHub Python repositories as seeds.\nSubsequently, the dataset is systematically expanded through an iterative\nprocess, where both seed and generated tasks are used to prompt ChatGPT for\nmore data. Our findings reveal that open-source LLMs fine-tuned on\nInstructCoder can significantly enhance the accuracy of code edits, exhibiting\nsuperior code-editing performance matching advanced proprietary LLMs. The\ndatasets and the source code are publicly available at\nhttps://github.com/qishenghu/CodeInstruct.",
        "pdf_link": "https://arxiv.org/pdf/2310.20329v3.pdf"
    },
    {
        "title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision",
        "authors": [
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Kamalika Das",
            "Sricharan Kumar"
        ],
        "published": "2023-10-31T03:39:23Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, their suitability for domain-specific tasks, is limited\ndue to their immense scale at deployment, susceptibility to misinformation, and\nmore importantly, high data annotation costs. We propose a novel Interactive\nMulti-Fidelity Learning (IMFL) framework for the cost-effective development of\nsmall domain-specific LMs under limited annotation budgets. Our approach\nformulates the domain-specific fine-tuning process as a multi-fidelity learning\nproblem, focusing on identifying the optimal acquisition strategy that balances\nbetween low-fidelity automatic LLM annotations and high-fidelity human\nannotations to maximize model performance. We further propose an\nexploration-exploitation query strategy that enhances annotation diversity and\ninformativeness, incorporating two innovative designs: 1) prompt retrieval that\nselects in-context examples from human-annotated samples to improve LLM\nannotation, and 2) variable batch size that controls the order for choosing\neach fidelity to facilitate knowledge distillation, ultimately enhancing\nannotation quality. Extensive experiments on financial and medical tasks\ndemonstrate that IMFL achieves superior performance compared with single\nfidelity annotations. Given a limited budget of human annotation, IMFL\nsignificantly outperforms the human annotation baselines in all four tasks and\nachieves very close performance as human annotations on two of the tasks. These\npromising results suggest that the high human annotation costs in\ndomain-specific tasks can be significantly reduced by employing IMFL, which\nutilizes fewer human annotations, supplemented with cheaper and faster LLM\n(e.g., GPT-3.5) annotations to achieve comparable performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.20153v1.pdf"
    },
    {
        "title": "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
        "authors": [
            "Chris Richardson",
            "Yao Zhang",
            "Kellen Gillespie",
            "Sudipta Kar",
            "Arshdeep Singh",
            "Zeynab Raeesy",
            "Omar Zia Khan",
            "Abhinav Sethy"
        ],
        "published": "2023-10-30T23:40:41Z",
        "summary": "Personalization, the ability to tailor a system to individual users, is an\nessential factor in user experience with natural language processing (NLP)\nsystems. With the emergence of Large Language Models (LLMs), a key question is\nhow to leverage these models to better personalize user experiences. To\npersonalize a language model's output, a straightforward approach is to\nincorporate past user data into the language model prompt, but this approach\ncan result in lengthy inputs exceeding limitations on input length and\nincurring latency and cost issues. Existing approaches tackle such challenges\nby selectively extracting relevant user data (i.e. selective retrieval) to\nconstruct a prompt for downstream tasks. However, retrieval-based methods are\nlimited by potential information loss, lack of more profound user\nunderstanding, and cold-start challenges. To overcome these limitations, we\npropose a novel summary-augmented approach by extending retrieval-augmented\npersonalization with task-aware user summaries generated by LLMs. The summaries\ncan be generated and stored offline, enabling real-world systems with runtime\nconstraints like voice assistants to leverage the power of LLMs. Experiments\nshow our method with 75% less of retrieved user data is on-par or outperforms\nretrieval augmentation on most tasks in the LaMP personalization benchmark. We\ndemonstrate that offline summarization via LLMs and runtime retrieval enables\nbetter performance for personalization on a range of tasks under practical\nconstraints.",
        "pdf_link": "https://arxiv.org/pdf/2310.20081v1.pdf"
    },
    {
        "title": "The Expressibility of Polynomial based Attention Scheme",
        "authors": [
            "Zhao Song",
            "Guangyi Xu",
            "Junze Yin"
        ],
        "published": "2023-10-30T22:16:18Z",
        "summary": "Large language models (LLMs) have significantly improved various aspects of\nour daily lives. These models have impacted numerous domains, from healthcare\nto education, enhancing productivity, decision-making processes, and\naccessibility. As a result, they have influenced and, to some extent, reshaped\npeople's lifestyles. However, the quadratic complexity of attention in\ntransformer architectures poses a challenge when scaling up these models for\nprocessing long textual contexts. This issue makes it impractical to train very\nlarge models on lengthy texts or use them efficiently during inference. While a\nrecent study by [KMZ23] introduced a technique that replaces the softmax with a\npolynomial function and polynomial sketching to speed up attention mechanisms,\nthe theoretical understandings of this new approach are not yet well\nunderstood.\n  In this paper, we offer a theoretical analysis of the expressive capabilities\nof polynomial attention. Our study reveals a disparity in the ability of\nhigh-degree and low-degree polynomial attention. Specifically, we construct two\ncarefully designed datasets, namely $\\mathcal{D}_0$ and $\\mathcal{D}_1$, where\n$\\mathcal{D}_1$ includes a feature with a significantly larger value compared\nto $\\mathcal{D}_0$. We demonstrate that with a sufficiently high degree\n$\\beta$, a single-layer polynomial attention network can distinguish between\n$\\mathcal{D}_0$ and $\\mathcal{D}_1$. However, with a low degree $\\beta$, the\nnetwork cannot effectively separate the two datasets. This analysis underscores\nthe greater effectiveness of high-degree polynomials in amplifying large values\nand distinguishing between datasets. Our analysis offers insight into the\nrepresentational capacity of polynomial attention and provides a rationale for\nincorporating higher-degree polynomials in attention mechanisms to capture\nintricate linguistic correlations.",
        "pdf_link": "https://arxiv.org/pdf/2310.20051v1.pdf"
    },
    {
        "title": "Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
        "authors": [
            "Prakamya Mishra",
            "Zonghai Yao",
            "Shuwei Chen",
            "Beining Wang",
            "Rohan Mittal",
            "Hong Yu"
        ],
        "published": "2023-10-30T21:33:22Z",
        "summary": "Large Language Models (LLMs) like the GPT and LLaMA families have\ndemonstrated exceptional capabilities in capturing and condensing critical\ncontextual information and achieving state-of-the-art performance in the\nsummarization task. However, community concerns about these models'\nhallucination issues continue to rise. LLMs sometimes generate factually\nhallucinated summaries, which can be extremely harmful in the clinical domain\nNLP tasks (e.g., clinical note summarization), where factually incorrect\nstatements can lead to critically erroneous diagnoses. Fine-tuning LLMs using\nhuman feedback has shown the promise of aligning LLMs to be factually\nconsistent during generation, but such training procedure requires high-quality\nhuman-annotated data, which can be extremely expensive to get in the clinical\ndomain. In this work, we propose a new pipeline using ChatGPT instead of human\nexperts to generate high-quality feedback data for improving factual\nconsistency in the clinical note summarization task. We focus specifically on\nedit feedback because recent work discusses the shortcomings of human alignment\nvia preference feedback in complex situations (such as clinical NLP tasks that\nrequire extensive expert knowledge), as well as some advantages of collecting\nedit feedback from domain experts. In addition, although GPT has reached the\nexpert level in many clinical NLP tasks (e.g., USMLE QA), there is not much\nprevious work discussing whether GPT can generate expert-level edit feedback\nfor LMs in the clinical note summarization task. We hope to fill this gap.\nFinally, our evaluations demonstrate the potential use of GPT edits in human\nalignment, especially from a factuality perspective.",
        "pdf_link": "https://arxiv.org/pdf/2310.20033v2.pdf"
    },
    {
        "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
        "authors": [
            "Joseph Gatto",
            "Omar Sharif",
            "Sarah Masud Preum"
        ],
        "published": "2023-10-30T17:18:10Z",
        "summary": "Stance detection on social media is challenging for Large Language Models\n(LLMs), as emerging slang and colloquial language in online conversations often\ncontain deeply implicit stance labels. Chain-of-Thought (COT) prompting has\nrecently been shown to improve performance on stance detection tasks --\nalleviating some of these issues. However, COT prompting still struggles with\nimplicit stance identification. This challenge arises because many samples are\ninitially challenging to comprehend before a model becomes familiar with the\nslang and evolving knowledge related to different topics, all of which need to\nbe acquired through the training data. In this study, we address this problem\nby introducing COT Embeddings which improve COT performance on stance detection\ntasks by embedding COT reasonings and integrating them into a traditional\nRoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text\nencoders can leverage COT reasonings with minor errors or hallucinations that\nwould otherwise distort the COT output label. 2) Text encoders can overlook\nmisleading COT reasoning when a sample's prediction heavily depends on\ndomain-specific patterns. Our model achieves SOTA performance on multiple\nstance detection datasets collected from social media.",
        "pdf_link": "https://arxiv.org/pdf/2310.19750v1.pdf"
    },
    {
        "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
        "authors": [
            "Qintong Li",
            "Leyang Cui",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2023-10-30T17:04:35Z",
        "summary": "Humans are widely involved in the evaluation of open-ended natural language\ngeneration tasks (NLG) that demand creativity, as automatic metrics often\nexhibit weak correlations with human judgments. Large language models (LLMs)\nrecently have emerged as a scalable and cost-effective alternative to human\nevaluations. However, both humans and LLMs have limitations, i.e., inherent\nsubjectivity and unreliable judgments, particularly for open-ended tasks that\nrequire adaptable metrics tailored to diverse task requirements. To explore the\nsynergy between humans and LLM-based evaluators and address the challenges of\nexisting inconsistent evaluation criteria in open-ended NLG tasks, we propose a\nCollaborative Evaluation pipeline CoEval, involving the design of a checklist\nof task-specific criteria and the detailed evaluation of texts, in which LLM\ngenerates initial ideation, and then humans engage in scrutiny. We conducted a\nseries of experiments to investigate the mutual effects between LLMs and humans\nin CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates\nlengthy texts, saving significant time and reducing human evaluation outliers.\nHuman scrutiny still plays a role, revising around 20% of LLM evaluation scores\nfor ultimate reliability.",
        "pdf_link": "https://arxiv.org/pdf/2310.19740v1.pdf"
    },
    {
        "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
        "authors": [
            "Leo Schwinn",
            "David Dobre",
            "Stephan G\u00fcnnemann",
            "Gauthier Gidel"
        ],
        "published": "2023-10-30T17:01:02Z",
        "summary": "Over the past decade, there has been extensive research aimed at enhancing\nthe robustness of neural networks, yet this problem remains vastly unsolved.\nHere, one major impediment has been the overestimation of the robustness of new\ndefense approaches due to faulty defense evaluations. Flawed robustness\nevaluations necessitate rectifications in subsequent works, dangerously slowing\ndown the research and providing a false sense of security. In this context, we\nwill face substantial challenges associated with an impending adversarial arms\nrace in natural language processing, specifically with closed-source Large\nLanguage Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We\nprovide a first set of prerequisites to improve the robustness assessment of\nnew approaches and reduce the amount of faulty evaluations. Additionally, we\nidentify embedding space attacks on LLMs as another viable threat model for the\npurposes of generating malicious content in open-sourced models. Finally, we\ndemonstrate on a recently proposed defense that, without LLM-specific best\npractices in place, it is easy to overestimate the robustness of a new\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2310.19737v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models: A Comprehensive Survey",
        "authors": [
            "Zishan Guo",
            "Renren Jin",
            "Chuang Liu",
            "Yufei Huang",
            "Dan Shi",
            "Supryadi",
            "Linhao Yu",
            "Yan Liu",
            "Jiaxuan Li",
            "Bojian Xiong",
            "Deyi Xiong"
        ],
        "published": "2023-10-30T17:00:52Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na broad spectrum of tasks. They have attracted significant attention and been\ndeployed in numerous downstream applications. Nevertheless, akin to a\ndouble-edged sword, LLMs also present potential risks. They could suffer from\nprivate data leaks or yield inappropriate, harmful, or misleading content.\nAdditionally, the rapid progress of LLMs raises concerns about the potential\nemergence of superintelligent systems without adequate safeguards. To\neffectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of\nLLMs. We categorize the evaluation of LLMs into three major groups: knowledge\nand capability evaluation, alignment evaluation and safety evaluation. In\naddition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs' performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making\nevaluation serve as a cornerstone in guiding the responsible development of\nLLMs. We envision that this will channel their evolution into a direction that\nmaximizes societal benefit while minimizing potential risks. A curated list of\nrelated papers has been publicly available at\nhttps://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.",
        "pdf_link": "https://arxiv.org/pdf/2310.19736v3.pdf"
    },
    {
        "title": "Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection",
        "authors": [
            "Noah Ziems",
            "Gang Liu",
            "John Flanagan",
            "Meng Jiang"
        ],
        "published": "2023-10-30T15:40:34Z",
        "summary": "Network intrusion detection (NID) systems which leverage machine learning\nhave been shown to have strong performance in practice when used to detect\nmalicious network traffic. Decision trees in particular offer a strong balance\nbetween performance and simplicity, but require users of NID systems to have\nbackground knowledge in machine learning to interpret. In addition, they are\nunable to provide additional outside information as to why certain features may\nbe important for classification.\n  In this work, we explore the use of large language models (LLMs) to provide\nexplanations and additional background knowledge for decision tree NID systems.\nFurther, we introduce a new human evaluation framework for decision tree\nexplanations, which leverages automatically generated quiz questions that\nmeasure human evaluators' understanding of decision tree inference. Finally, we\nshow LLM generated decision tree explanations correlate highly with human\nratings of readability, quality, and use of background knowledge while\nsimultaneously providing better understanding of decision boundaries.",
        "pdf_link": "https://arxiv.org/pdf/2310.19658v1.pdf"
    },
    {
        "title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models",
        "authors": [
            "Zhenpeng Su",
            "Xing Wu",
            "Xue Bai",
            "Zijia Lin",
            "Hui Chen",
            "Guiguang Ding",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "published": "2023-10-30T13:33:21Z",
        "summary": "Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2310.19531v7.pdf"
    },
    {
        "title": "Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs",
        "authors": [
            "Huawen Feng",
            "Yan Fan",
            "Xiong Liu",
            "Ting-En Lin",
            "Zekun Yao",
            "Yuchuan Wu",
            "Fei Huang",
            "Yongbin Li",
            "Qianli Ma"
        ],
        "published": "2023-10-30T08:40:16Z",
        "summary": "Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nefficient training to cover the shortage of sensitivity for true and false in\nthe training process of LLMs. In this way, LLMs are less confused about\nembellishing and understanding; thus, they can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.19347v3.pdf"
    },
    {
        "title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models",
        "authors": [
            "Wai-Chung Kwan",
            "Xingshan Zeng",
            "Yufei Wang",
            "Yusen Sun",
            "Liangyou Li",
            "Lifeng Shang",
            "Qun Liu",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-30T03:11:30Z",
        "summary": "Managing long sequences has become an important and necessary feature for\nlarge language models (LLMs). However, it is still an open question of how to\ncomprehensively and systematically evaluate the long-sequence capability of\nLLMs. One of the reasons is that conventional and widely-used benchmarks mainly\nconsist of short sequences. In this paper, we propose M4LE, a Multi-ability,\nMulti-range, Multi-task, Multi-domain benchmark for Long-context Evaluation.\nM4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task\ntypes and 12 domains. To alleviate the scarcity of tasks with naturally long\nsequences and incorporate multiple-ability assessment, we propose an automatic\napproach (but with negligible human annotations) to convert short-sequence\ntasks into a unified long-sequence scenario where LLMs have to identify single\nor multiple relevant spans in long contexts based on explicit or semantic\nhints. Specifically, the scenario includes five different types of abilities:\n(1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span;\n(4) semantic multiple-span; and (5) global context understanding. The resulting\nsamples in M4LE are evenly distributed from 1k to 8k input length. We conducted\na systematic evaluation on 11 well-established LLMs, especially those optimized\nfor long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to\nunderstand long context, particularly when tasks require multiple-span\nattention. 2) Semantic retrieval task is more difficult for competent LLMs. 3)\nModels fine-tuned on longer text with position interpolation have comparable\nperformance to those using Neural Tangent Kernel (NTK) aware scaling methods\nwithout fine-tuning. We make our benchmark publicly available to encourage\nfuture research in this challenging area.",
        "pdf_link": "https://arxiv.org/pdf/2310.19240v1.pdf"
    },
    {
        "title": "From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude",
        "authors": [
            "Sayak Saha Roy",
            "Poojitha Thota",
            "Krishna Vamsi Naragam",
            "Shirin Nilizadeh"
        ],
        "published": "2023-10-29T22:52:40Z",
        "summary": "The advanced capabilities of Large Language Models (LLMs) have made them\ninvaluable across various applications, from conversational agents and content\ncreation to data analysis, research, and innovation. However, their\neffectiveness and accessibility also render them susceptible to abuse for\ngenerating malicious content, including phishing attacks. This study explores\nthe potential of using four popular commercially available LLMs, i.e., ChatGPT\n(GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing\nattacks using a series of malicious prompts. We discover that these LLMs can\ngenerate both phishing websites and emails that can convincingly imitate\nwell-known brands and also deploy a range of evasive tactics that are used to\nelude detection mechanisms employed by anti-phishing systems. These attacks can\nbe generated using unmodified or \"vanilla\" versions of these LLMs without\nrequiring any prior adversarial exploits such as jailbreaking. We evaluate the\nperformance of the LLMs towards generating these attacks and find that they can\nalso be utilized to create malicious prompts that, in turn, can be fed back to\nthe model to generate phishing scams - thus massively reducing the\nprompt-engineering effort required by attackers to scale these threats. As a\ncountermeasure, we build a BERT-based automated detection tool that can be used\nfor the early detection of malicious prompts to prevent LLMs from generating\nphishing content. Our model is transferable across all four commercial LLMs,\nattaining an average accuracy of 96% for phishing website prompts and 94% for\nphishing email prompts. We also disclose the vulnerabilities to the concerned\nLLMs, with Google acknowledging it as a severe issue. Our detection model is\navailable for use at Hugging Face, as well as a ChatGPT Actions plugin.",
        "pdf_link": "https://arxiv.org/pdf/2310.19181v2.pdf"
    },
    {
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion",
        "authors": [
            "Pengyue Jia",
            "Yiding Liu",
            "Xiangyu Zhao",
            "Xiaopeng Li",
            "Changying Hao",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2023-10-29T16:04:10Z",
        "summary": "Query expansion, pivotal in search engines, enhances the representation of\nuser information needs with additional terms. While existing methods expand\nqueries using retrieved or generated contextual documents, each approach has\nnotable limitations. Retrieval-based methods often fail to accurately capture\nsearch intent, particularly with brief or ambiguous queries. Generation-based\nmethods, utilizing large language models (LLMs), generally lack corpus-specific\nknowledge and entail high fine-tuning costs. To address these gaps, we propose\na novel zero-shot query expansion framework utilizing LLMs for mutual\nverification. Specifically, we first design a query-query-document generation\nmethod, leveraging LLMs' zero-shot reasoning ability to produce diverse\nsub-queries and corresponding documents. Then, a mutual verification process\nsynergizes generated and retrieved documents for optimal expansion. Our\nproposed method is fully zero-shot, and extensive experiments on three public\nbenchmark datasets are conducted to demonstrate its effectiveness over existing\nmethods. Our code is available online at\nhttps://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.",
        "pdf_link": "https://arxiv.org/pdf/2310.19056v3.pdf"
    },
    {
        "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding",
        "authors": [
            "Lixing Zhu",
            "Runcong Zhao",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2023-10-28T18:47:57Z",
        "summary": "Narrative understanding involves capturing the author's cognitive processes,\nproviding insights into their knowledge, intentions, beliefs, and desires.\nAlthough large language models (LLMs) excel in generating grammatically\ncoherent text, their ability to comprehend the author's thoughts remains\nuncertain. This limitation hinders the practical applications of narrative\nunderstanding. In this paper, we conduct a comprehensive survey of narrative\nunderstanding tasks, thoroughly examining their key features, definitions,\ntaxonomy, associated datasets, training objectives, evaluation metrics, and\nlimitations. Furthermore, we explore the potential of expanding the\ncapabilities of modularized LLMs to address novel narrative understanding\ntasks. By framing narrative understanding as the retrieval of the author's\nimaginative cues that outline the narrative structure, our study introduces a\nfresh perspective on enhancing narrative comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2310.18783v1.pdf"
    },
    {
        "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
        "authors": [
            "Sajad Mousavi",
            "Ricardo Luna Guti\u00e9rrez",
            "Desik Rengarajan",
            "Vineet Gundecha",
            "Ashwin Ramesh Babu",
            "Avisek Naug",
            "Antonio Guillen",
            "Soumyendu Sarkar"
        ],
        "published": "2023-10-28T11:22:22Z",
        "summary": "We propose a self-correction mechanism for Large Language Models (LLMs) to\nmitigate issues such as toxicity and fact hallucination. This method involves\nrefining model outputs through an ensemble of critics and the model's own\nfeedback. Drawing inspiration from human behavior, we explore whether LLMs can\nemulate the self-correction process observed in humans who often engage in\nself-reflection and seek input from others to refine their understanding of\ncomplex topics. Our approach is model-agnostic and can be applied across\nvarious domains to enhance trustworthiness by addressing fairness, bias, and\nrobustness concerns. We consistently observe performance improvements in LLMs\nfor reducing toxicity and correcting factual errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.18679v2.pdf"
    },
    {
        "title": "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers",
        "authors": [
            "Wencong You",
            "Zayd Hammoudeh",
            "Daniel Lowd"
        ],
        "published": "2023-10-28T06:11:07Z",
        "summary": "Backdoor attacks manipulate model predictions by inserting innocuous triggers\ninto training and test data. We focus on more realistic and more challenging\nclean-label attacks where the adversarial training examples are correctly\nlabeled. Our attack, LLMBkd, leverages language models to automatically insert\ndiverse style-based triggers into texts. We also propose a poison selection\ntechnique to improve the effectiveness of both LLMBkd as well as existing\ntextual backdoor attacks. Lastly, we describe REACT, a baseline defense to\nmitigate backdoor attacks via antidote training examples. Our evaluations\ndemonstrate LLMBkd's effectiveness and efficiency, where we consistently\nachieve high attack success rates across a wide range of styles with little\neffort and no model training.",
        "pdf_link": "https://arxiv.org/pdf/2310.18603v1.pdf"
    },
    {
        "title": "LLMs-Healthcare : Current Applications and Challenges of Large Language Models in various Medical Specialties",
        "authors": [
            "Ummara Mumtaz",
            "Awais Ahmed",
            "Summaya Mumtaz"
        ],
        "published": "2023-10-28T01:01:30Z",
        "summary": "We aim to present a comprehensive overview of the latest advancements in\nutilizing Large Language Models (LLMs) within the healthcare sector,\nemphasizing their transformative impact across various medical domains. LLMs\nhave become pivotal in supporting healthcare, including physicians, healthcare\nproviders, and patients. Our review provides insight into the applications of\nLarge Language Models (LLMs) in healthcare, specifically focusing on diagnostic\nand treatment-related functionalities. We shed light on how LLMs are applied in\ncancer care, dermatology, dental care, neurodegenerative disorders, and mental\nhealth, highlighting their innovative contributions to medical diagnostics and\npatient care. Throughout our analysis, we explore the challenges and\nopportunities associated with integrating LLMs in healthcare, recognizing their\npotential across various medical specialties despite existing limitations.\nAdditionally, we offer an overview of handling diverse data types within the\nmedical field.",
        "pdf_link": "https://arxiv.org/pdf/2311.12882v3.pdf"
    },
    {
        "title": "On the Automatic Generation and Simplification of Children's Stories",
        "authors": [
            "Maria Valentini",
            "Jennifer Weber",
            "Jesus Salcido",
            "T\u00e9a Wright",
            "Eliana Colunga",
            "Katharina Kann"
        ],
        "published": "2023-10-27T21:31:34Z",
        "summary": "With recent advances in large language models (LLMs), the concept of\nautomatically generating children's educational materials has become\nincreasingly realistic. Working toward the goal of age-appropriate simplicity\nin generated educational texts, we first examine the ability of several popular\nLLMs to generate stories with properly adjusted lexical and readability levels.\nWe find that, in spite of the growing capabilities of LLMs, they do not yet\npossess the ability to limit their vocabulary to levels appropriate for younger\nage groups. As a second experiment, we explore the ability of state-of-the-art\nlexical simplification models to generalize to the domain of children's stories\nand, thus, create an efficient pipeline for their automatic generation. In\norder to test these models, we develop a dataset of child-directed lexical\nsimplification instances, with examples taken from the LLM-generated stories in\nour first experiment. We find that, while the strongest-performing current\nlexical simplification models do not perform as well on material designed for\nchildren due to their reliance on large language models behind the scenes, some\nmodels that still achieve fairly strong results on general data can mimic or\neven improve their performance on children-directed data with proper\nfine-tuning, which we conduct using our newly created child-directed\nsimplification dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.18502v1.pdf"
    },
    {
        "title": "PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction",
        "authors": [
            "Mingchen Li",
            "M. Chen",
            "Huixue Zhou",
            "Halil Kilicoglu",
            "Rui Zhang"
        ],
        "published": "2023-10-27T20:15:23Z",
        "summary": "Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. While current unified information\nextraction models showcase state-of-the-art performance, they face challenges\nin understanding relationships between entities within intricate biomedical\nsentences. Furthermore, the absence of a high-quality biomedical triple\nextraction dataset impedes the progress in developing robust triple extraction\nsystems. To tackle these challenges, we propose a novel retrieval-based\nframework for biomedical triple extraction, namely PeTailor, which explicitly\nretrieves the relevant document from our pre-built diverse chunk database using\na novel tailored chunk scorer and integrates the retrieved information into the\ninput of a Large Language Model (LLM) to generate the corresponding triple\n(head entity, relation, tail entity) for the input sentence. Additionally, we\npresent GM-CIHT, an expert-annotated biomedical triple extraction dataset that\ncovers a wider range of relation types. Experimental results show that our\nproposed PeTailor method achieves state-of-the-art performance on GM-CIHT and\ntwo standard biomedical triple extraction datasets",
        "pdf_link": "https://arxiv.org/pdf/2310.18463v3.pdf"
    },
    {
        "title": "T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models",
        "authors": [
            "Rebecca M. M. Hicke",
            "David Mimno"
        ],
        "published": "2023-10-27T20:04:57Z",
        "summary": "Large language models have shown breakthrough potential in many NLP domains.\nHere we consider their use for stylometry, specifically authorship\nidentification in Early Modern English drama. We find both promising and\nconcerning results; LLMs are able to accurately predict the author of\nsurprisingly short passages but are also prone to confidently misattribute\ntexts to specific authors. A fine-tuned t5-large model outperforms all tested\nbaselines, including logistic regression, SVM with a linear kernel, and cosine\ndelta, at attributing small passages. However, we see indications that the\npresence of certain authors in the model's pre-training data affects predictive\nresults in ways that are difficult to assess.",
        "pdf_link": "https://arxiv.org/pdf/2310.18454v1.pdf"
    },
    {
        "title": "Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models",
        "authors": [
            "Eren Unlu",
            "Unver Ciftci"
        ],
        "published": "2023-10-27T17:04:10Z",
        "summary": "Large Language Models (LLMs) are evolving to integrate multiple modalities,\nsuch as text, image, and audio into a unified linguistic space. We envision a\nfuture direction based on this framework where conceptual entities defined in\nsequences of text can also be imagined as modalities. Such a formulation has\nthe potential to overcome the cognitive and computational limitations of\ncurrent models. Several illustrative examples of such potential implicit\nmodalities are given. Along with vast promises of the hypothesized structure,\nexpected challenges are discussed as well.",
        "pdf_link": "https://arxiv.org/pdf/2310.18390v1.pdf"
    },
    {
        "title": "ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models",
        "authors": [
            "Benjamin Feuer",
            "Yurong Liu",
            "Chinmay Hegde",
            "Juliana Freire"
        ],
        "published": "2023-10-27T15:31:22Z",
        "summary": "Existing deep-learning approaches to semantic column type annotation (CTA)\nhave important shortcomings: they rely on semantic types which are fixed at\ntraining time; require a large number of training samples per type and incur\nlarge run-time inference costs; and their performance can degrade when\nevaluated on novel datasets, even when types remain constant. Large language\nmodels have exhibited strong zero-shot classification performance on a wide\nrange of tasks and in this paper we explore their use for CTA. We introduce\nArcheType, a simple, practical method for context sampling, prompt\nserialization, model querying, and label remapping, which enables large\nlanguage models to solve CTA problems in a fully zero-shot manner. We ablate\neach component of our method separately, and establish that improvements to\ncontext sampling and label remapping provide the most consistent gains.\nArcheType establishes a new state-of-the-art performance on zero-shot CTA\nbenchmarks (including three new domain-specific benchmarks which we release\nalong with this paper), and when used in conjunction with classical CTA\ntechniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB\nbenchmark. Our code is available at https://github.com/penfever/ArcheType.",
        "pdf_link": "https://arxiv.org/pdf/2310.18208v2.pdf"
    },
    {
        "title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues",
        "authors": [
            "David Q. Sun",
            "Artem Abzaliev",
            "Hadas Kotek",
            "Zidi Xiu",
            "Christopher Klein",
            "Jason D. Williams"
        ],
        "published": "2023-10-27T13:23:02Z",
        "summary": "Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.",
        "pdf_link": "https://arxiv.org/pdf/2310.18130v2.pdf"
    },
    {
        "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
        "authors": [
            "Oscar Sainz",
            "Jon Ander Campos",
            "Iker Garc\u00eda-Ferrero",
            "Julen Etxaniz",
            "Oier Lopez de Lacalle",
            "Eneko Agirre"
        ],
        "published": "2023-10-27T09:48:29Z",
        "summary": "In this position paper, we argue that the classical evaluation on Natural\nLanguage Processing (NLP) tasks using annotated benchmarks is in trouble. The\nworst kind of data contamination happens when a Large Language Model (LLM) is\ntrained on the test split of a benchmark, and then evaluated in the same\nbenchmark. The extent of the problem is unknown, as it is not straightforward\nto measure. Contamination causes an overestimation of the performance of a\ncontaminated model in a target benchmark and associated task with respect to\ntheir non-contaminated counterparts. The consequences can be very harmful, with\nwrong scientific conclusions being published while other correct ones are\ndiscarded. This position paper defines different levels of data contamination\nand argues for a community effort, including the development of automatic and\nsemi-automatic measures to detect when data from a benchmark was exposed to a\nmodel, and suggestions for flagging papers with conclusions that are\ncompromised by data contamination.",
        "pdf_link": "https://arxiv.org/pdf/2310.18018v1.pdf"
    },
    {
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-10-27T06:48:48Z",
        "summary": "Sentiment analysis is a well-established natural language processing task,\nwith sentiment polarity classification being one of its most popular and\nrepresentative tasks. However, despite the success of pre-trained language\nmodels in this area, they often fall short of capturing the broader\ncomplexities of sentiment analysis. To address this issue, we propose a new\ntask called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims\nto evaluate sentiment understanding through two subtasks: Review Comprehension\n(RC) and Justification Generation (JG). RC seeks to validate statements that\nfocus on subjective information based on a review text, while JG requires\nmodels to provide explanations for their sentiment predictions. To enable\ncomprehensive evaluation, we annotate a new dataset comprising 15,028\nstatements from 3,638 reviews. Experimental results indicate that SOUL is a\nchallenging task for both small and large language models, with a performance\ngap of up to 27% when compared to human performance. Furthermore, evaluations\nconducted with both human experts and GPT-4 highlight the limitations of the\nsmall language model in generating reasoning-based justifications. These\nfindings underscore the challenging nature of the SOUL task for existing\nmodels, emphasizing the need for further advancements in sentiment analysis to\naddress its complexities. The new dataset and code are available at\nhttps://github.com/DAMO-NLP-SG/SOUL.",
        "pdf_link": "https://arxiv.org/pdf/2310.17924v1.pdf"
    },
    {
        "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method",
        "authors": [
            "Yukun Zhao",
            "Lingyong Yan",
            "Weiwei Sun",
            "Guoliang Xing",
            "Chong Meng",
            "Shuaiqiang Wang",
            "Zhicong Cheng",
            "Zhaochun Ren",
            "Dawei Yin"
        ],
        "published": "2023-10-27T06:22:14Z",
        "summary": "Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2310.17918v2.pdf"
    },
    {
        "title": "Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey",
        "authors": [
            "Weixu Zhang",
            "Yifei Wang",
            "Yuanfeng Song",
            "Victor Junqiu Wei",
            "Yuxing Tian",
            "Yiyan Qi",
            "Jonathan H. Chan",
            "Raymond Chi-Wing Wong",
            "Haiqin Yang"
        ],
        "published": "2023-10-27T05:01:20Z",
        "summary": "The emergence of natural language processing has revolutionized the way users\ninteract with tabular data, enabling a shift from traditional query languages\nand manual plotting to more intuitive, language-based interfaces. The rise of\nlarge language models (LLMs) such as ChatGPT and its successors has further\nadvanced this field, opening new avenues for natural language processing\ntechniques. This survey presents a comprehensive overview of natural language\ninterfaces for tabular data querying and visualization, which allow users to\ninteract with data using natural language queries. We introduce the fundamental\nconcepts and techniques underlying these interfaces with a particular emphasis\non semantic parsing, the key technology facilitating the translation from\nnatural language to SQL queries or data visualization commands. We then delve\ninto the recent advancements in Text-to-SQL and Text-to-Vis problems from the\nperspectives of datasets, methodologies, metrics, and system designs. This\nincludes a deep dive into the influence of LLMs, highlighting their strengths,\nlimitations, and potential for future improvements. Through this survey, we aim\nto provide a roadmap for researchers and practitioners interested in developing\nand applying natural language interfaces for data interaction in the era of\nlarge language models.",
        "pdf_link": "https://arxiv.org/pdf/2310.17894v1.pdf"
    },
    {
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
        "authors": [
            "Niloofar Mireshghallah",
            "Hyunwoo Kim",
            "Xuhui Zhou",
            "Yulia Tsvetkov",
            "Maarten Sap",
            "Reza Shokri",
            "Yejin Choi"
        ],
        "published": "2023-10-27T04:15:30Z",
        "summary": "The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.",
        "pdf_link": "https://arxiv.org/pdf/2310.17884v1.pdf"
    },
    {
        "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of Abstract Meaning Representation",
        "authors": [
            "Allyson Ettinger",
            "Jena D. Hwang",
            "Valentina Pyatkin",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "published": "2023-10-26T21:47:59Z",
        "summary": "Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.",
        "pdf_link": "https://arxiv.org/pdf/2310.17793v2.pdf"
    },
    {
        "title": "Evaluation of large language models using an Indian language LGBTI+ lexicon",
        "authors": [
            "Aditya Joshi",
            "Shruta Rawat",
            "Alpana Dange"
        ],
        "published": "2023-10-26T21:32:24Z",
        "summary": "Large language models (LLMs) are typically evaluated on the basis of\ntask-based benchmarks such as MMLU. Such benchmarks do not examine responsible\nbehaviour of LLMs in specific contexts. This is particularly true in the LGBTI+\ncontext where social stereotypes may result in variation in LGBTI+ terminology.\nTherefore, domain-specific lexicons or dictionaries may be useful as a\nrepresentative list of words against which the LLM's behaviour needs to be\nevaluated. This paper presents a methodology for evaluation of LLMs using an\nLGBTI+ lexicon in Indian languages. The methodology consists of four steps:\nformulating NLP tasks relevant to the expected behaviour, creating prompts that\ntest LLMs, using the LLMs to obtain the output and, finally, manually\nevaluating the results. Our qualitative analysis shows that the three LLMs we\nexperiment on are unable to detect underlying hateful content. Similarly, we\nobserve limitations in using machine translation as means to evaluate natural\nlanguage understanding in languages other than English. The methodology\npresented in this paper can be useful for LGBTI+ lexicons in other languages as\nwell as other domain-specific lexicons. The work done in this paper opens\navenues for responsible behaviour of LLMs, as demonstrated in the context of\nprevalent social perception of the LGBTI+ community.",
        "pdf_link": "https://arxiv.org/pdf/2310.17787v1.pdf"
    },
    {
        "title": "A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications",
        "authors": [
            "Ahmed Magooda",
            "Alec Helyar",
            "Kyle Jackson",
            "David Sullivan",
            "Chad Atalla",
            "Emily Sheng",
            "Dan Vann",
            "Richard Edgar",
            "Hamid Palangi",
            "Roman Lutz",
            "Hongliang Kong",
            "Vincent Yun",
            "Eslam Kamal",
            "Federico Zarfati",
            "Hanna Wallach",
            "Sarah Bird",
            "Mei Chen"
        ],
        "published": "2023-10-26T19:45:06Z",
        "summary": "We present a framework for the automated measurement of responsible AI (RAI)\nmetrics for large language models (LLMs) and associated products and services.\nOur framework for automatically measuring harms from LLMs builds on existing\ntechnical and sociotechnical expertise and leverages the capabilities of\nstate-of-the-art LLMs, such as GPT-4. We use this framework to run through\nseveral case studies investigating how different LLMs may violate a range of\nRAI-related principles. The framework may be employed alongside domain-specific\nsociotechnical expertise to create measurements for new harm areas in the\nfuture. By implementing this framework, we aim to enable more advanced harm\nmeasurement efforts and further the responsible use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.17750v1.pdf"
    },
    {
        "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems",
        "authors": [
            "Lidiya Murakhovs'ka",
            "Philippe Laban",
            "Tian Xie",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023-10-26T19:44:06Z",
        "summary": "Making big purchases requires consumers to research or consult a salesperson\nto gain domain expertise. However, existing conversational recommender systems\n(CRS) often overlook users' lack of background knowledge, focusing solely on\ngathering preferences. In this work, we define a new problem space for\nconversational agents that aim to provide both product recommendations and\neducational value through mixed-type mixed-initiative dialog. We introduce\nSalesOps, a framework that facilitates the simulation and evaluation of such\nsystems by leveraging recent advancements in large language models (LLMs). We\nbuild SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate\neither side of the framework. A comprehensive human study compares SalesBot\nagainst professional salespeople, revealing that although SalesBot approaches\nprofessional performance in terms of fluency and informativeness, it lags\nbehind in recommendation quality. We emphasize the distinct limitations both\nface in providing truthful information, highlighting the challenges of ensuring\nfaithfulness in the CRS context. We release our code and make all data\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2310.17749v1.pdf"
    },
    {
        "title": "Outlier Dimensions Encode Task-Specific Knowledge",
        "authors": [
            "William Rudman",
            "Catherine Chen",
            "Carsten Eickhoff"
        ],
        "published": "2023-10-26T18:22:13Z",
        "summary": "Representations from large language models (LLMs) are known to be dominated\nby a small subset of dimensions with exceedingly high variance. Previous works\nhave argued that although ablating these outlier dimensions in LLM\nrepresentations hurts downstream performance, outlier dimensions are\ndetrimental to the representational quality of embeddings. In this study, we\ninvestigate how fine-tuning impacts outlier dimensions and show that 1) outlier\ndimensions that occur in pre-training persist in fine-tuned models and 2) a\nsingle outlier dimension can complete downstream tasks with a minimal error\nrate. Our results suggest that outlier dimensions can encode crucial\ntask-specific knowledge and that the value of a representation in a single\noutlier dimension drives downstream model decisions.",
        "pdf_link": "https://arxiv.org/pdf/2310.17715v2.pdf"
    },
    {
        "title": "Proving Test Set Contamination in Black Box Language Models",
        "authors": [
            "Yonatan Oren",
            "Nicole Meister",
            "Niladri Chatterji",
            "Faisal Ladhak",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-10-26T17:43:13Z",
        "summary": "Large language models are trained on vast amounts of internet data, prompting\nconcerns and speculation that they have memorized public benchmarks. Going from\nspeculation to proof of contamination is challenging, as the pretraining data\nused by proprietary models are often not publicly accessible. We show that it\nis possible to provide provable guarantees of test set contamination in\nlanguage models without access to pretraining data or model weights. Our\napproach leverages the fact that when there is no data contamination, all\norderings of an exchangeable benchmark should be equally likely. In contrast,\nthe tendency for language models to memorize example order means that a\ncontaminated language model will find certain canonical orderings to be much\nmore likely than others. Our test flags potential contamination whenever the\nlikelihood of a canonically ordered benchmark dataset is significantly higher\nthan the likelihood after shuffling the examples. We demonstrate that our\nprocedure is sensitive enough to reliably prove test set contamination in\nchallenging situations, including models as small as 1.4 billion parameters, on\nsmall test sets of only 1000 examples, and datasets that appear only a few\ntimes in the pretraining corpus. Using our test, we audit five popular publicly\naccessible language models for test set contamination and find little evidence\nfor pervasive contamination.",
        "pdf_link": "https://arxiv.org/pdf/2310.17623v2.pdf"
    },
    {
        "title": "An Open Source Data Contamination Report for Large Language Models",
        "authors": [
            "Yucheng Li",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2023-10-26T17:11:42Z",
        "summary": "Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to \"cheat\"\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.",
        "pdf_link": "https://arxiv.org/pdf/2310.17589v3.pdf"
    },
    {
        "title": "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
        "authors": [
            "Qusai Khraisha",
            "Sophie Put",
            "Johanna Kappenberg",
            "Azza Warraitch",
            "Kristin Hadfield"
        ],
        "published": "2023-10-26T16:18:30Z",
        "summary": "Systematic reviews are vital for guiding practice, research, and policy, yet\nthey are often slow and labour-intensive. Large language models (LLMs) could\noffer a way to speed up and automate systematic reviews, but their performance\nin such tasks has not been comprehensively evaluated against humans, and no\nstudy has tested GPT-4, the biggest LLM so far. This pre-registered study\nevaluates GPT-4's capability in title/abstract screening, full-text review, and\ndata extraction across various literature types and languages using a\n'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human\nperformance in most tasks, results were skewed by chance agreement and dataset\nimbalance. After adjusting for these, there was a moderate level of performance\nfor data extraction, and - barring studies that used highly reliable prompts -\nscreening performance levelled at none to moderate for different stages and\nlanguages. When screening full-text literature using highly reliable prompts,\nGPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key\nstudies using highly reliable prompts improved its performance even more. Our\nfindings indicate that, currently, substantial caution should be used if LLMs\nare being used to conduct systematic reviews, but suggest that, for certain\nsystematic review tasks delivered under reliable prompts, LLMs can rival human\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2310.17526v2.pdf"
    },
    {
        "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering",
        "authors": [
            "Sukmin Cho",
            "Jeongyeon Seo",
            "Soyeong Jeong",
            "Jong C. Park"
        ],
        "published": "2023-10-26T15:45:12Z",
        "summary": "Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.",
        "pdf_link": "https://arxiv.org/pdf/2310.17490v3.pdf"
    },
    {
        "title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in Discriminative Tasks",
        "authors": [
            "Shen Yuan",
            "Hongteng Xu"
        ],
        "published": "2023-10-26T14:43:07Z",
        "summary": "As one of the most popular neural network modules, Transformer plays a\ncentral role in many fundamental deep learning models, e.g., the ViT in\ncomputer vision and the BERT and GPT in natural language processing. The\neffectiveness of the Transformer is often attributed to its multi-head\nattention (MHA) mechanism. In this study, we discuss the limitations of MHA,\nincluding the high computational complexity due to its ``query-key-value''\narchitecture and the numerical issue caused by its softmax operation.\nConsidering the above problems and the recent development tendency of the\nattention layer, we propose an effective and efficient surrogate of the\nTransformer, called Sliceformer. Our Sliceformer replaces the classic MHA\nmechanism with an extremely simple ``slicing-sorting'' operation, i.e.,\nprojecting inputs linearly to a latent space and sorting them along different\nfeature dimensions (or equivalently, called channels). For each feature\ndimension, the sorting operation implicitly generates an implicit attention map\nwith sparse, full-rank, and doubly-stochastic structures. We consider different\nimplementations of the slicing-sorting operation and analyze their impacts on\nthe Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark,\nimage classification, text classification, and molecular property prediction,\ndemonstrating its advantage in computational complexity and universal\neffectiveness in discriminative tasks. Our Sliceformer achieves comparable or\nbetter performance with lower memory cost and faster speed than the Transformer\nand its variants. Moreover, the experimental results reveal that applying our\nSliceformer can empirically suppress the risk of mode collapse when\nrepresenting data. The code is available at\n\\url{https://github.com/SDS-Lab/sliceformer}.",
        "pdf_link": "https://arxiv.org/pdf/2310.17683v1.pdf"
    },
    {
        "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
        "authors": [
            "Zi Lin",
            "Zihan Wang",
            "Yongqi Tong",
            "Yangkun Wang",
            "Yuxin Guo",
            "Yujia Wang",
            "Jingbo Shang"
        ],
        "published": "2023-10-26T13:35:41Z",
        "summary": "Despite remarkable advances that large language models have achieved in\nchatbots, maintaining a non-toxic user-AI interactive environment has become\nincreasingly critical nowadays. However, previous efforts in toxicity detection\nhave been mostly based on benchmarks derived from social media content, leaving\nthe unique challenges inherent to real-world user-AI interactions\ninsufficiently explored. In this work, we introduce ToxicChat, a novel\nbenchmark based on real user queries from an open-source chatbot. This\nbenchmark contains the rich, nuanced phenomena that can be tricky for current\ntoxicity detection models to identify, revealing a significant domain\ndifference compared to social media content. Our systematic evaluation of\nmodels trained on existing toxicity datasets has shown their shortcomings when\napplied to this unique domain of ToxicChat. Our work illuminates the\npotentially overlooked challenges of toxicity detection in real-world user-AI\nconversations. In the future, ToxicChat can be a valuable resource to drive\nfurther advancements toward building a safe and healthy environment for user-AI\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2310.17389v1.pdf"
    },
    {
        "title": "Cultural Adaptation of Recipes",
        "authors": [
            "Yong Cao",
            "Yova Kementchedjhieva",
            "Ruixiang Cui",
            "Antonia Karamolegkou",
            "Li Zhou",
            "Megan Dare",
            "Lucia Donatelli",
            "Daniel Hershcovich"
        ],
        "published": "2023-10-26T12:39:20Z",
        "summary": "Building upon the considerable advances in Large Language Models (LLMs), we\nare now equipped to address more sophisticated tasks demanding a nuanced\nunderstanding of cross-cultural contexts. A key example is recipe adaptation,\nwhich goes beyond simple translation to include a grasp of ingredients,\nculinary techniques, and dietary preferences specific to a given culture. We\nintroduce a new task involving the translation and cultural adaptation of\nrecipes between Chinese and English-speaking cuisines. To support this\ninvestigation, we present CulturalRecipes, a unique dataset comprised of\nautomatically paired recipes written in Mandarin Chinese and English. This\ndataset is further enriched with a human-written and curated test set. In this\nintricate task of cross-cultural recipe adaptation, we evaluate the performance\nof various methods, including GPT-4 and other LLMs, traditional machine\ntranslation, and information retrieval techniques. Our comprehensive analysis\nincludes both automatic and human evaluation metrics. While GPT-4 exhibits\nimpressive abilities in adapting Chinese recipes into English, it still lags\nbehind human expertise when translating English recipes into Chinese. This\nunderscores the multifaceted nature of cultural adaptations. We anticipate that\nthese insights will significantly contribute to future research on\nculturally-aware language models and their practical application in culturally\ndiverse contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.17353v1.pdf"
    },
    {
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "authors": [
            "Justin T. Chiu",
            "Wenting Zhao",
            "Derek Chen",
            "Saujas Vaduguru",
            "Alexander M. Rush",
            "Daniel Fried"
        ],
        "published": "2023-10-26T04:22:23Z",
        "summary": "Large language models (LLMs) excel at processing and generating both text and\ncode. However, LLMs have had limited applicability in grounded task-oriented\ndialogue as they are difficult to steer toward task objectives and fail to\nhandle novel grounding. We present a modular and interpretable grounded\ndialogue system that addresses these shortcomings by composing LLMs with a\nsymbolic planner and grounded code execution. Our system consists of a reader\nand planner: the reader leverages an LLM to convert partner utterances into\nexecutable code, calling functions that perform grounding. The translated\ncode's output is stored to track dialogue state, while a symbolic planner\ndetermines the next appropriate response. We evaluate our system's performance\non the demanding OneCommon dialogue task, involving collaborative reference\nresolution on abstract images of scattered dots. Our system substantially\noutperforms the previous state-of-the-art, including improving task success in\nhuman evaluations from 56% to 69% in the most challenging setting.",
        "pdf_link": "https://arxiv.org/pdf/2310.17140v1.pdf"
    },
    {
        "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs",
        "authors": [
            "Yuxin Zuo",
            "Bei Li",
            "Chuanhao Lv",
            "Tong Zheng",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "published": "2023-10-26T04:13:49Z",
        "summary": "This paper presents an in-depth study of multimodal machine translation\n(MMT), examining the prevailing understanding that MMT systems exhibit\ndecreased sensitivity to visual information when text inputs are complete.\nInstead, we attribute this phenomenon to insufficient cross-modal interaction,\nrather than image information redundancy. A novel approach is proposed to\ngenerate parallel Visual Question-Answering (VQA) style pairs from the source\ntext, fostering more robust cross-modal interaction. Using Large Language\nModels (LLMs), we explicitly model the probing signal in MMT to convert it into\nVQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask\nlearning framework is introduced to incorporate explicit probing signals from\nthe dataset into the MMT training process. Experimental results on two\nwidely-used benchmarks demonstrate the effectiveness of this novel approach.\nOur code and data would be available at:\n\\url{https://github.com/libeineu/MMT-VQA}.",
        "pdf_link": "https://arxiv.org/pdf/2310.17133v1.pdf"
    },
    {
        "title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge",
        "authors": [
            "Farima Fatahi Bayat",
            "Kun Qian",
            "Benjamin Han",
            "Yisi Sang",
            "Anton Belyi",
            "Samira Khorshidi",
            "Fei Wu",
            "Ihab F. Ilyas",
            "Yunyao Li"
        ],
        "published": "2023-10-26T03:28:30Z",
        "summary": "Detecting factual errors in textual information, whether generated by large\nlanguage models (LLM) or curated by humans, is crucial for making informed\ndecisions. LLMs' inability to attribute their claims to external knowledge and\ntheir tendency to hallucinate makes it difficult to rely on their responses.\nHumans, too, are prone to factual errors in their writing. Since manual\ndetection and correction of factual errors is labor-intensive, developing an\nautomatic approach can greatly reduce human effort. We present FLEEK, a\nprototype tool that automatically extracts factual claims from text, gathers\nevidence from external knowledge sources, evaluates the factuality of each\nclaim, and suggests revisions for identified errors using the collected\nevidence. Initial empirical evaluation on fact error detection (77-85\\% F1)\nshows the potential of FLEEK. A video demo of FLEEK can be found at\nhttps://youtu.be/NapJFUlkPdQ.",
        "pdf_link": "https://arxiv.org/pdf/2310.17119v1.pdf"
    },
    {
        "title": "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories",
        "authors": [
            "Hassen Saidi",
            "Susmit Jha",
            "Tuhin Sahai"
        ],
        "published": "2023-10-25T23:54:04Z",
        "summary": "As artificial intelligence (AI) gains greater adoption in a wide variety of\napplications, it has immense potential to contribute to mathematical discovery,\nby guiding conjecture generation, constructing counterexamples, assisting in\nformalizing mathematics, and discovering connections between different\nmathematical areas, to name a few.\n  While prior work has leveraged computers for exhaustive mathematical proof\nsearch, recent efforts based on large language models (LLMs) aspire to position\ncomputing platforms as co-contributors in the mathematical research process.\nDespite their current limitations in logic and mathematical tasks, there is\ngrowing interest in melding theorem proving systems with foundation models.\nThis work investigates the applicability of LLMs in formalizing advanced\nmathematical concepts and proposes a framework that can critically review and\ncheck mathematical reasoning in research papers. Given the noted reasoning\nshortcomings of LLMs, our approach synergizes the capabilities of proof\nassistants, specifically PVS, with LLMs, enabling a bridge between textual\ndescriptions in academic papers and formal specifications in PVS. By harnessing\nthe PVS environment, coupled with data ingestion and conversion mechanisms, we\nenvision an automated process, called \\emph{math-PVS}, to extract and formalize\nmathematical theorems from research papers, offering an innovative tool for\nacademic review and discovery.",
        "pdf_link": "https://arxiv.org/pdf/2310.17064v1.pdf"
    },
    {
        "title": "BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation",
        "authors": [
            "Yufei Tian",
            "Felix Zhang",
            "Nanyun Peng"
        ],
        "published": "2023-10-25T23:32:12Z",
        "summary": "Large language models (LLMs) such as GPT-3 have demonstrated a strong\ncapability to generate coherent and contextually relevant text. However, amidst\ntheir successes, a crucial issue persists: their generated outputs still lack\ncommonsense at times. Moreover, fine-tuning the entire LLM towards more\ncommonsensical outputs is computationally expensive if not infeasible. In this\npaper, we present a computation-efficient framework that steers a frozen\nPre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,\nproducing a plausible output that incorporates a list of concepts in a\nmeaningful way). Specifically, we first construct a reference-free evaluator\nthat assigns a sentence with a commonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base from four different relational aspects.\nWe then use the scorer as the oracle for commonsense knowledge, and extend the\ncontrollable generation method called NADO to train an auxiliary head that\nguides a fixed PTLM to better satisfy the oracle. We test our framework on a\nseries of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two\nconstrained concept-to-sentence benchmarks. Human evaluation results\ndemonstrate that our method consistently leads to the most commonsensical\noutputs.",
        "pdf_link": "https://arxiv.org/pdf/2310.17054v1.pdf"
    },
    {
        "title": "Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning",
        "authors": [
            "Ananth Balashankar",
            "Xiao Ma",
            "Aradhana Sinha",
            "Ahmad Beirami",
            "Yao Qin",
            "Jilin Chen",
            "Alex Beutel"
        ],
        "published": "2023-10-25T19:57:07Z",
        "summary": "As large language models (LLMs) are widely adopted, new safety issues and\npolicies emerge, to which existing safety classifiers do not generalize well.\nIf we have only observed a few examples of violations of a new safety rule, how\ncan we build a classifier to detect violations? In this paper, we study the\nnovel setting of domain-generalized few-shot learning for LLM-based text safety\nclassifiers. Unlike prior few-shot work, these new safety issues can be hard to\nuncover and we do not get to choose the few examples. We demonstrate that\nexisting few-shot techniques do not perform well in this setting, and rather we\npropose to do parameter-efficient fine-tuning (PEFT) combined with augmenting\ntraining data based on similar examples in prior existing rules. We empirically\nshow that our approach of similarity-based data-augmentation + prompt-tuning\n(DAPT) consistently outperforms baselines that either do not rely on data\naugmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral\njudgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule\nis loosely correlated with existing ones.",
        "pdf_link": "https://arxiv.org/pdf/2310.16959v1.pdf"
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "authors": [
            "Shih-yang Liu",
            "Zechun Liu",
            "Xijie Huang",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "published": "2023-10-25T17:59:32Z",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
        "pdf_link": "https://arxiv.org/pdf/2310.16836v1.pdf"
    },
    {
        "title": "Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization",
        "authors": [
            "Yongxin Zhou",
            "Fabien Ringeval",
            "Fran\u00e7ois Portet"
        ],
        "published": "2023-10-25T17:39:07Z",
        "summary": "This study explores the capabilities of prompt-driven Large Language Models\n(LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue\nsummarization. Experiments employed DialogSum (English social conversations)\nand DECODA (French call center interactions), testing various prompts:\nincluding prompts from existing literature and those from human summarization\nguidelines, as well as a two-step prompt approach. Our findings indicate that\nGPT models often produce lengthy summaries and deviate from human summarization\nguidelines. However, using human guidelines as an intermediate step shows\npromise, outperforming direct word-length constraint prompts in some cases. The\nresults reveal that GPT models exhibit unique stylistic tendencies in their\nsummaries. While BERTScores did not dramatically decrease for GPT outputs\nsuggesting semantic similarity to human references and specialised pre-trained\nmodels, ROUGE scores reveal grammatical and lexical disparities between\nGPT-generated and human-written summaries. These findings shed light on the\ncapabilities and limitations of GPT models in following human instructions for\ndialogue summarization.",
        "pdf_link": "https://arxiv.org/pdf/2310.16810v1.pdf"
    },
    {
        "title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation",
        "authors": [
            "Yongxin Shi",
            "Dezhi Peng",
            "Wenhui Liao",
            "Zening Lin",
            "Xinhong Chen",
            "Chongyu Liu",
            "Yuyi Zhang",
            "Lianwen Jin"
        ],
        "published": "2023-10-25T17:38:55Z",
        "summary": "This paper presents a comprehensive evaluation of the Optical Character\nRecognition (OCR) capabilities of the recently released GPT-4V(ision), a Large\nMultimodal Model (LMM). We assess the model's performance across a range of OCR\ntasks, including scene text recognition, handwritten text recognition,\nhandwritten mathematical expression recognition, table structure recognition,\nand information extraction from visually-rich document. The evaluation reveals\nthat GPT-4V performs well in recognizing and understanding Latin contents, but\nstruggles with multilingual scenarios and complex tasks. Specifically, it\nshowed limitations when dealing with non-Latin languages and complex tasks such\nas handwriting mathematical expression recognition, table structure\nrecognition, and end-to-end semantic entity recognition and pair extraction\nfrom document image. Based on these observations, we affirm the necessity and\ncontinued research value of specialized OCR models. In general, despite its\nversatility in handling diverse OCR tasks, GPT-4V does not outperform existing\nstate-of-the-art OCR models. How to fully utilize pre-trained general-purpose\nLMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study\noffers a critical reference for future research in OCR with LMMs. Evaluation\npipeline and results are available at\nhttps://github.com/SCUT-DLVCLab/GPT-4V_OCR.",
        "pdf_link": "https://arxiv.org/pdf/2310.16809v2.pdf"
    },
    {
        "title": "Detecting Pretraining Data from Large Language Models",
        "authors": [
            "Weijia Shi",
            "Anirudh Ajith",
            "Mengzhou Xia",
            "Yangsibo Huang",
            "Daogao Liu",
            "Terra Blevins",
            "Danqi Chen",
            "Luke Zettlemoyer"
        ],
        "published": "2023-10-25T17:21:23Z",
        "summary": "Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to three real-world scenarios, copyrighted book\ndetection, contaminated downstream example detection and privacy auditing of\nmachine unlearning, and find it a consistently effective solution.",
        "pdf_link": "https://arxiv.org/pdf/2310.16789v3.pdf"
    },
    {
        "title": "SuperHF: Supervised Iterative Learning from Human Feedback",
        "authors": [
            "Gabriel Mukobi",
            "Peter Chatain",
            "Su Fong",
            "Robert Windesheim",
            "Gitta Kutyniok",
            "Kush Bhatia",
            "Silas Alberti"
        ],
        "published": "2023-10-25T16:52:00Z",
        "summary": "While large language models demonstrate remarkable capabilities, they often\npresent challenges in terms of safety, alignment with human values, and\nstability during training. Here, we focus on two prevalent methods used to\nalign these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning\nfrom Human Feedback (RLHF). SFT is simple and robust, powering a host of\nopen-source models, while RLHF is a more sophisticated method used in top-tier\nmodels like ChatGPT but also suffers from instability and susceptibility to\nreward hacking. We propose a novel approach, Supervised Iterative Learning from\nHuman Feedback (SuperHF), which seeks to leverage the strengths of both\nmethods. Our hypothesis is two-fold: that the reward model used in RLHF is\ncritical for efficient data use and model generalization and that the use of\nProximal Policy Optimization (PPO) in RLHF may not be necessary and could\ncontribute to instability issues. SuperHF replaces PPO with a simple supervised\nloss and a Kullback-Leibler (KL) divergence prior. It creates its own training\ndata by repeatedly sampling a batch of model outputs and filtering them through\nthe reward model in an online learning regime. We then break down the reward\noptimization problem into three components: robustly optimizing the training\nrewards themselves, preventing reward hacking-exploitation of the reward model\nthat degrades model performance-as measured by a novel METEOR similarity\nmetric, and maintaining good performance on downstream evaluations. Our\nexperimental results show SuperHF exceeds PPO-based RLHF on the training\nobjective, easily and favorably trades off high reward with low reward hacking,\nimproves downstream calibration, and performs the same on our GPT-4 based\nqualitative evaluation scheme all the while being significantly simpler to\nimplement, highlighting SuperHF's potential as a competitive language model\nalignment technique.",
        "pdf_link": "https://arxiv.org/pdf/2310.16763v1.pdf"
    },
    {
        "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
        "authors": [
            "Yinghui He",
            "Yufan Wu",
            "Yilin Jia",
            "Rada Mihalcea",
            "Yulong Chen",
            "Naihao Deng"
        ],
        "published": "2023-10-25T16:41:15Z",
        "summary": "Theory of Mind (ToM) is the ability to reason about one's own and others'\nmental states. ToM plays a critical role in the development of intelligence,\nlanguage understanding, and cognitive processes. While previous work has\nprimarily focused on first and second-order ToM, we explore higher-order ToM,\nwhich involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\nHigher Order Theory of Mind benchmark. Our experimental evaluation using\nvarious Large Language Models (LLMs) indicates a decline in performance on\nhigher-order ToM tasks, demonstrating the limitations of current LLMs. We\nconduct a thorough analysis of different failure cases of LLMs, and share our\nthoughts on the implications of our findings on the future of NLP.",
        "pdf_link": "https://arxiv.org/pdf/2310.16755v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Code Explanation",
        "authors": [
            "Paheli Bhattacharya",
            "Manojit Chakraborty",
            "Kartheek N S N Palepu",
            "Vikas Pandey",
            "Ishan Dindorkar",
            "Rakesh Rajpurohit",
            "Rishabh Gupta"
        ],
        "published": "2023-10-25T14:38:40Z",
        "summary": "Automating code documentation through explanatory text can prove highly\nbeneficial in code understanding. Large Language Models (LLMs) have made\nremarkable strides in Natural Language Processing, especially within software\nengineering tasks such as code generation and code summarization. This study\nspecifically delves into the task of generating natural-language summaries for\ncode snippets, using various LLMs. The findings indicate that Code LLMs\noutperform their generic counterparts, and zero-shot methods yield superior\nresults when dealing with datasets with dissimilar distributions between\ntraining and testing sets.",
        "pdf_link": "https://arxiv.org/pdf/2310.16673v1.pdf"
    },
    {
        "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
        "authors": [
            "Tianlong Li",
            "Shihan Dou",
            "Changze Lv",
            "Wenhao Liu",
            "Jianhan Xu",
            "Muling Wu",
            "Zixuan Ling",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2023-10-25T12:16:33Z",
        "summary": "Personality plays a pivotal role in shaping human expression patterns, thus\nregulating the personality of large language models (LLMs) holds significant\npotential in enhancing the user experience of LLMs. Previous methods either\nrelied on fine-tuning LLMs on specific corpora or necessitated manually crafted\nprompts to elicit specific personalities from LLMs. However, the former\napproach is inefficient and costly, while the latter cannot precisely\nmanipulate personality traits at a fine-grained level. To address the above\nchallenges, we have employed a novel Unsupervisedly-Built Personalized Lexicons\n(UBPL) in a pluggable manner during the decoding phase of LLMs to manipulate\ntheir personality traits. UBPL is a lexicon built through an unsupervised\napproach from a situational judgment test dataset (SJTs4LLM). Users can utilize\nUBPL to adjust the probability vectors of predicted words in the decoding phase\nof LLMs, thus influencing the personality expression of LLMs. Extensive\nexperimentation demonstrates the remarkable effectiveness and pluggability of\nour method for fine-grained manipulation of LLM's personality.",
        "pdf_link": "https://arxiv.org/pdf/2310.16582v2.pdf"
    },
    {
        "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
        "authors": [
            "Qingyuan Tian",
            "Hanlun Zhu",
            "Lei Wang",
            "Yang Li",
            "Yunshi Lan"
        ],
        "published": "2023-10-25T10:34:02Z",
        "summary": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models\n(LLMs) have achieved remarkable performance on various reasoning tasks.\nHowever, most of them have been evaluated under noise-free context and the\ndilemma for LLMs to produce inaccurate results under the noisy context has not\nbeen fully investigated. Existing studies utilize trigger sentences to\nencourage LLMs to concentrate on the relevant information but the trigger has\nlimited effect on final answer prediction. Inspired by interactive CoT method,\nwhere intermediate reasoning steps are promoted by multiple rounds of\ninteraction between users and LLMs, we propose a novel prompting method, namely\nR$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$\nprompting interacts with LLMs to perform key sentence extraction, variable\ndeclaration and answer prediction, which corresponds to a thought process of\nreviewing, rephrasing and resolving. The responses generated at the last\ninteraction will perform as hints to guide toward the responses of the next\ninteraction. Our experiments show that R$^3$ prompting significantly\noutperforms existing CoT prompting methods on five reasoning tasks under noisy\ncontext. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on\nthe reasoning tasks under noisy context compared to the most competitive\nprompting baseline. More analyses and ablation studies show the robustness and\ngeneralization of R$^3$ prompting method in solving reasoning tasks in LLMs\nunder noisy context.",
        "pdf_link": "https://arxiv.org/pdf/2310.16535v1.pdf"
    },
    {
        "title": "An Early Evaluation of GPT-4V(ision)",
        "authors": [
            "Yang Wu",
            "Shilong Wang",
            "Hao Yang",
            "Tian Zheng",
            "Hongbo Zhang",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "published": "2023-10-25T10:33:17Z",
        "summary": "In this paper, we evaluate different abilities of GPT-4V including visual\nunderstanding, language understanding, visual puzzle solving, and understanding\nof other modalities such as depth, thermal, video, and audio. To estimate\nGPT-4V's performance, we manually construct 656 test instances and carefully\nevaluate the results of GPT-4V. The highlights of our findings are as follows:\n(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks\nbut fails to recognize simple Chinese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering questions related to sensitive\ntraits such as gender, race, and age; (3) GPT-4V obtains worse results than\nGPT-4 (API) on language understanding tasks including general language\nunderstanding benchmarks and visual commonsense knowledge evaluation\nbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both\nvisual understanding and language understanding; (5) GPT-4V struggles to find\nthe nuances between two similar images and solve the easy math picture puzzles;\n(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to\nimage, such as video and thermal. Our experimental results reveal the ability\nand limitations of GPT-4V and we hope our paper can provide some insights into\nthe application and research of GPT-4V.",
        "pdf_link": "https://arxiv.org/pdf/2310.16534v1.pdf"
    },
    {
        "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
        "authors": [
            "Preethi Lahoti",
            "Nicholas Blumm",
            "Xiao Ma",
            "Raghavendra Kotikalapudi",
            "Sahitya Potluri",
            "Qijun Tan",
            "Hansa Srinivasan",
            "Ben Packer",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023-10-25T10:17:17Z",
        "summary": "A crucial challenge for generative large language models (LLMs) is diversity:\nwhen a user's prompt is under-specified, models may follow implicit assumptions\nwhile generating a response, which may result in homogenization of the\nresponses, as well as certain demographic groups being under-represented or\neven erased from the generated responses. In this paper, we formalize diversity\nof representation in generative LLMs. We present evaluation datasets and\npropose metrics to measure diversity in generated responses along people and\nculture axes. We find that LLMs understand the notion of diversity, and that\nthey can reason and critique their own responses for that goal. This finding\nmotivated a new prompting technique called collective-critique and self-voting\n(CCSV) to self-improve people diversity of LLMs by tapping into its diversity\nreasoning capabilities, without relying on handcrafted examples or prompt\ntuning. Extensive empirical experiments with both human and automated\nevaluations show that our proposed approach is effective at improving people\nand culture diversity, and outperforms all baseline methods by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2310.16523v1.pdf"
    },
    {
        "title": "OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models",
        "authors": [
            "Mingfeng Xue",
            "Dayiheng Liu",
            "Kexin Yang",
            "Guanting Dong",
            "Wenqiang Lei",
            "Zheng Yuan",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2023-10-25T10:06:17Z",
        "summary": "The emergence of large language models (LLMs) has revolutionized natural\nlanguage processing tasks. However, existing instruction-tuning datasets suffer\nfrom occupational bias: the majority of data relates to only a few occupations,\nwhich hampers the instruction-tuned LLMs to generate helpful responses to\nprofessional queries from practitioners in specific fields. To mitigate this\nissue and promote occupation-inclusive LLMs, we create an instruction-tuning\ndataset named \\emph{OccuQuest}, which contains 110,000+ prompt-completion pairs\nand 30,000+ dialogues covering over 1,000 occupations in 26 occupational\ncategories. We systematically request ChatGPT, organizing queries\nhierarchically based on Occupation, Responsibility, Topic, and Question, to\nensure a comprehensive coverage of occupational specialty inquiries. By\ncomparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we\nobserve that OccuQuest exhibits a more balanced distribution across\noccupations. Furthermore, we assemble three test sets for comprehensive\nevaluation, an occu-test set covering 25 occupational categories, an estate set\nfocusing on real estate, and an occu-quora set containing real-world questions\nfrom Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which\nsignificantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and\nWizardLM) on professional questions in GPT-4 and human evaluations. Notably, on\nthe occu-quora set, OccuLLaMA reaches a high win rate of 86.4\\% against\nWizardLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.16517v1.pdf"
    },
    {
        "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
        "authors": [
            "Guanzheng Chen",
            "Xin Li",
            "Zaiqiao Meng",
            "Shangsong Liang",
            "Lidong Bing"
        ],
        "published": "2023-10-25T08:13:02Z",
        "summary": "Transformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position\nEmbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their\nextrapolation abilities or sacrificing partial performance within the context\nwindow. Length extrapolation methods, although theoretically capable of\nextending the context window beyond the training sequence length, often\nunderperform in practical long-context applications. To address these\nchallenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We\ngeneralise the PE scaling approaches to model the continuous dynamics by\nordinary differential equations over the length scaling factor, thereby\novercoming the constraints of current PE scaling methods designed for specific\nlengths. Moreover, by extending the dynamics to desired context lengths beyond\nthe training sequence length, CLEX facilitates the length extrapolation with\nimpressive performance in practical tasks. We demonstrate that CLEX can be\nseamlessly incorporated into LLMs equipped with Rotary Position Embedding, such\nas LLaMA and GPT-NeoX, with negligible impact on training and inference\nlatency. Experimental results reveal that CLEX can effectively extend the\ncontext window to over 4x or almost 8x training length, with no deterioration\nin performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to\n32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
        "pdf_link": "https://arxiv.org/pdf/2310.16450v3.pdf"
    },
    {
        "title": "Graph Agent: Explicit Reasoning Agent for Graphs",
        "authors": [
            "Qinyong Wang",
            "Zhenxiang Gao",
            "Rong Xu"
        ],
        "published": "2023-10-25T07:20:16Z",
        "summary": "Graph embedding methods such as Graph Neural Networks (GNNs) and Graph\nTransformers have contributed to the development of graph reasoning algorithms\nfor various tasks on knowledge graphs. However, the lack of interpretability\nand explainability of graph embedding methods has limited their applicability\nin scenarios requiring explicit reasoning. In this paper, we introduce the\nGraph Agent (GA), an intelligent agent methodology of leveraging large language\nmodels (LLMs), inductive-deductive reasoning modules, and long-term memory for\nknowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning\nand existing graph embedding methods to provide an innovative approach for\ncomplex graph reasoning tasks. By converting graph structures into textual\ndata, GA enables LLMs to process, reason, and provide predictions alongside\nhuman-interpretable explanations. The effectiveness of the GA was evaluated on\nnode classification and link prediction tasks. Results showed that GA reached\nstate-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and\n89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to\nexisting GNN and transformer models, GA offered advantages of explicit\nreasoning ability, free-of-training, easy adaption to various graph reasoning\ntasks",
        "pdf_link": "https://arxiv.org/pdf/2310.16421v1.pdf"
    },
    {
        "title": "Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model",
        "authors": [
            "Dui Wang",
            "Xiangyu Hou",
            "Xiaohui Yang",
            "Bo Zhang",
            "Renbing Chen",
            "Daiyue Xue"
        ],
        "published": "2023-10-25T06:49:19Z",
        "summary": "Recommendation system (RS) plays significant roles in matching users\ninformation needs for Internet applications, and it usually utilizes the\nvanilla neural network as the backbone to handle embedding details. Recently,\nthe large language model (LLM) has exhibited emergent abilities and achieved\ngreat breakthroughs both in the CV and NLP communities. Thus, it is logical to\nincorporate RS with LLM better, which has become an emerging research\ndirection. Although some existing works have made their contributions to this\nissue, they mainly consider the single key situation (e.g. historical\ninteractions), especially in sequential recommendation. The situation of\nmultiple key-value data is simply neglected. This significant scenario is\nmainstream in real practical applications, where the information of users (e.g.\nage, occupation, etc) and items (e.g. title, category, etc) has more than one\nkey. Therefore, we aim to implement sequential recommendations based on\nmultiple key-value data by incorporating RS with LLM. In particular, we\ninstruct tuning a prevalent open-source LLM (Llama 7B) in order to inject\ndomain knowledge of RS into the pre-trained LLM. Since we adopt multiple\nkey-value strategies, LLM is hard to learn well among these keys. Thus the\ngeneral and innovative shuffle and mask strategies, as an innovative manner of\ndata argument, are designed. To demonstrate the effectiveness of our approach,\nextensive experiments are conducted on the popular and suitable dataset\nMovieLens which contains multiple keys-value. The experimental results\ndemonstrate that our approach can nicely and effectively complete this\nchallenging issue.",
        "pdf_link": "https://arxiv.org/pdf/2310.16409v1.pdf"
    },
    {
        "title": "Evaluating, Understanding, and Improving Constrained Text Generation for Large Language Models",
        "authors": [
            "Xiang Chen",
            "Xiaojun Wan"
        ],
        "published": "2023-10-25T03:58:49Z",
        "summary": "Advancements in natural language generation (NLG) and large language models\n(LLMs) have led to proficient text generation in various tasks. However,\nintegrating intricate constraints into neural text generation, due to LLMs'\nopacity, remains challenging. This study investigates constrained text\ngeneration for LLMs, where predefined constraints are applied during LLM's\ngeneration process. Our research mainly focuses on mainstream open-source LLMs,\ncategorizing constraints into lexical, structural, and relation-based types. We\nalso present various benchmarks to facilitate fair evaluation. The study\naddresses some key research questions, including evaluating, understanding and\nimproving constrained text generation for LLMs. Results illuminate LLMs'\ncapacity and deficiency to incorporate constraints and provide insights for\nfuture developments in constrained text generation. Codes and datasets will be\nreleased upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2310.16343v2.pdf"
    },
    {
        "title": "Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation",
        "authors": [
            "Jiexin Wang",
            "Liuwen Cao",
            "Xitong Luo",
            "Zhiping Zhou",
            "Jiayuan Xie",
            "Adam Jatowt",
            "Yi Cai"
        ],
        "published": "2023-10-25T00:32:56Z",
        "summary": "Large language models (LLMs) have brought significant advancements to code\ngeneration, benefiting both novice and experienced developers. However, their\ntraining using unsanitized data from open-source repositories, like GitHub,\nintroduces the risk of inadvertently propagating security vulnerabilities. To\neffectively mitigate this concern, this paper presents a comprehensive study\nfocused on evaluating and enhancing code LLMs from a software security\nperspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as\nsupplemental material and will be made publicly available after publication.},\na meticulously curated dataset targeting 21 critical vulnerability types.\nSecuCoGen comprises 180 samples and serves as the foundation for conducting\nexperiments on three crucial code-related tasks: code generation, code repair\nand vulnerability classification, with a strong emphasis on security. Our\nexperimental results reveal that existing models often overlook security\nconcerns during code generation, leading to the generation of vulnerable code.\nTo address this, we propose effective approaches to mitigate the security\nvulnerabilities and enhance the overall robustness of code generated by LLMs.\nMoreover, our study identifies weaknesses in existing models' ability to repair\nvulnerable code, even when provided with vulnerability information.\nAdditionally, certain vulnerability types pose challenges for the models,\nhindering their performance in vulnerability classification. Based on these\nfindings, we believe our study will have a positive impact on the software\nengineering community, inspiring the development of improved methods for\ntraining and utilizing LLMs, thereby leading to safer and more trustworthy\nmodel deployment.",
        "pdf_link": "https://arxiv.org/pdf/2310.16263v1.pdf"
    },
    {
        "title": "ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair",
        "authors": [
            "Yonghao Wu",
            "Zheng Li",
            "Jie M. Zhang",
            "Yong Liu"
        ],
        "published": "2023-10-25T00:06:02Z",
        "summary": "With the growing interest on Large Language Models (LLMs) for fault\nlocalization and program repair, ensuring the integrity and generalizability of\nthe LLM-based methods becomes paramount. The code in existing widely-adopted\nbenchmarks for these tasks was written before the the bloom of LLMs and may be\nincluded in the training data of existing popular LLMs, thereby suffering from\nthe threat of data leakage, leading to misleadingly optimistic performance\nmetrics. To address this issue, we introduce \"ConDefects\", a novel dataset of\nreal faults meticulously curated to eliminate such overlap. ConDefects contains\n1,254 Java faulty programs and 1,625 Python faulty programs. All these programs\nare sourced from the online competition platform AtCoder and were produced\nbetween October 2021 and September 2023. We pair each fault with fault\nlocations and the corresponding repaired code versions, making it tailored for\nin fault localization and program repair related research. We also provide\ninterfaces for selecting subsets based on different time windows and coding\ntask difficulties. While inspired by LLM-based tasks, ConDefects can be adopted\nfor benchmarking ALL types of fault localization and program repair methods.\nThe dataset is publicly available, and a demo video can be found at\nhttps://www.youtube.com/watch?v=22j15Hj5ONk.",
        "pdf_link": "https://arxiv.org/pdf/2310.16253v1.pdf"
    },
    {
        "title": "Knowledge Editing for Large Language Models: A Survey",
        "authors": [
            "Song Wang",
            "Yaochen Zhu",
            "Haochen Liu",
            "Zaiyi Zheng",
            "Chen Chen",
            "Jundong Li"
        ],
        "published": "2023-10-24T22:18:13Z",
        "summary": "Large language models (LLMs) have recently transformed both the academic and\nindustrial landscapes due to their remarkable capacity to understand, analyze,\nand generate texts based on their vast knowledge and reasoning ability.\nNevertheless, one major drawback of LLMs is their substantial computational\ncost for pre-training due to their unprecedented amounts of parameters. The\ndisadvantage is exacerbated when new knowledge frequently needs to be\nintroduced into the pre-trained model. Therefore, it is imperative to develop\neffective and efficient techniques to update pre-trained LLMs. Traditional\nmethods encode new knowledge in pre-trained LLMs through direct fine-tuning.\nHowever, naively re-training LLMs can be computationally intensive and risks\ndegenerating valuable pre-trained knowledge irrelevant to the update in the\nmodel. Recently, Knowledge-based Model Editing (KME) has attracted increasing\nattention, which aims to precisely modify the LLMs to incorporate specific\nknowledge, without negatively influencing other irrelevant knowledge. In this\nsurvey, we aim to provide a comprehensive and in-depth overview of recent\nadvances in the field of KME. We first introduce a general formulation of KME\nto encompass different KME strategies. Afterward, we provide an innovative\ntaxonomy of KME techniques based on how the new knowledge is introduced into\npre-trained LLMs, and investigate existing KME strategies while analyzing key\ninsights, advantages, and limitations of methods from each category. Moreover,\nrepresentative metrics, datasets, and applications of KME are introduced\naccordingly. Finally, we provide an in-depth analysis regarding the\npracticality and remaining challenges of KME and suggest promising research\ndirections for further advancement in this field.",
        "pdf_link": "https://arxiv.org/pdf/2310.16218v3.pdf"
    },
    {
        "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
        "authors": [
            "Alejandro Lozano",
            "Scott L Fleming",
            "Chia-Chun Chiang",
            "Nigam Shah"
        ],
        "published": "2023-10-24T19:43:39Z",
        "summary": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
        "pdf_link": "https://arxiv.org/pdf/2310.16146v1.pdf"
    },
    {
        "title": "Can You Follow Me? Testing Situational Understanding in ChatGPT",
        "authors": [
            "Chenghao Yang",
            "Allyson Ettinger"
        ],
        "published": "2023-10-24T19:22:01Z",
        "summary": "Understanding sentence meanings and updating information states appropriately\nacross time -- what we call \"situational understanding\" (SU) -- is a critical\nability for human-like AI agents. SU is essential in particular for chat\nmodels, such as ChatGPT, to enable consistent, coherent, and effective dialogue\nbetween humans and AI. Previous works have identified certain SU limitations in\nnon-chatbot Large Language models (LLMs), but the extent and causes of these\nlimitations are not well understood, and capabilities of current chat-based\nmodels in this domain have not been explored. In this work we tackle these\nquestions, proposing a novel synthetic environment for SU testing which allows\nus to do controlled and systematic testing of SU in chat-oriented models,\nthrough assessment of models' ability to track and enumerate environment\nstates. Our environment also allows for close analysis of dynamics of model\nperformance, to better understand underlying causes for performance patterns.\nWe apply our test to ChatGPT, the state-of-the-art chatbot, and find that\ndespite the fundamental simplicity of the task, the model's performance\nreflects an inability to retain correct environment states across time. Our\nfollow-up analyses suggest that performance degradation is largely because\nChatGPT has non-persistent in-context memory (although it can access the full\ndialogue history) and it is susceptible to hallucinated updates -- including\nupdates that artificially inflate accuracies. Our findings suggest overall that\nChatGPT is not currently equipped for robust tracking of situation states, and\nthat trust in the impressive dialogue performance of ChatGPT comes with risks.\nWe release the codebase for reproducing our test environment, as well as all\nprompts and API responses from ChatGPT, at\nhttps://github.com/yangalan123/SituationalTesting.",
        "pdf_link": "https://arxiv.org/pdf/2310.16135v1.pdf"
    },
    {
        "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
        "authors": [
            "Saiteja Utpala",
            "Sara Hooker",
            "Pin Yu Chen"
        ],
        "published": "2023-10-24T18:25:13Z",
        "summary": "Numerous studies have highlighted the privacy risks associated with\npretrained large language models. In contrast, our research offers a unique\nperspective by demonstrating that pretrained large language models can\neffectively contribute to privacy preservation. We propose a locally\ndifferentially private mechanism called DP-Prompt, which leverages the power of\npretrained large language models and zero-shot prompting to counter author\nde-anonymization attacks while minimizing the impact on downstream utility.\nWhen DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),\nwe observe a notable reduction in the success rate of de-anonymization attacks,\nshowing that it surpasses existing approaches by a considerable margin despite\nits simpler design. For instance, in the case of the IMDB dataset, DP-Prompt\n(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving\na 46\\% reduction in author identification F1 score against static attackers and\na 26\\% reduction against adaptive attackers. We conduct extensive experiments\nacross six open-source large language models, ranging up to 7 billion\nparameters, to analyze various effects of the privacy-utility tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2310.16111v2.pdf"
    },
    {
        "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition",
        "authors": [
            "Sander Schulhoff",
            "Jeremy Pinto",
            "Anaum Khan",
            "Louis-Fran\u00e7ois Bouchard",
            "Chenglei Si",
            "Svetlina Anati",
            "Valen Tagliabue",
            "Anson Liu Kost",
            "Christopher Carnahan",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-10-24T18:18:11Z",
        "summary": "Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.",
        "pdf_link": "https://arxiv.org/pdf/2311.16119v3.pdf"
    },
    {
        "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
        "authors": [
            "Zayne Sprague",
            "Xi Ye",
            "Kaj Bostrom",
            "Swarat Chaudhuri",
            "Greg Durrett"
        ],
        "published": "2023-10-24T17:59:20Z",
        "summary": "While large language models (LLMs) equipped with techniques like\nchain-of-thought prompting have demonstrated impressive capabilities, they\nstill fall short in their ability to reason robustly in complex settings.\nHowever, evaluating LLM reasoning is challenging because system capabilities\ncontinue to grow while benchmark datasets for tasks like logical deduction have\nremained static. We introduce MuSR, a dataset for evaluating language models on\nmultistep soft reasoning tasks specified in a natural language narrative. This\ndataset has two crucial features. First, it is created through a novel\nneurosymbolic synthetic-to-natural generation algorithm, enabling the\nconstruction of complex reasoning instances that challenge GPT-4 (e.g., murder\nmysteries roughly 1000 words in length) and which can be scaled further as more\ncapable LLMs are released. Second, our dataset instances are free text\nnarratives corresponding to real-world domains of reasoning; this makes it\nsimultaneously much more challenging than other synthetically-crafted\nbenchmarks while remaining realistic and tractable for human annotators to\nsolve with high accuracy. We evaluate a range of LLMs and prompting techniques\non this dataset and characterize the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2310.16049v2.pdf"
    },
    {
        "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
        "authors": [
            "Joy Hsu",
            "Jiayuan Mao",
            "Joshua B. Tenenbaum",
            "Jiajun Wu"
        ],
        "published": "2023-10-24T17:50:20Z",
        "summary": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.16035v1.pdf"
    },
    {
        "title": "Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs",
        "authors": [
            "Jiarui Zhang",
            "Mahyar Khayatkhoei",
            "Prateek Chhikara",
            "Filip Ilievski"
        ],
        "published": "2023-10-24T17:48:04Z",
        "summary": "Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive small details as well as\nlarge details in images. In particular, we show that their zero-shot accuracy\nin answering visual questions is very sensitive to the size of the visual\nsubject of the question, declining up to 46% with size. Furthermore, we show\nthat this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose five automatic visual cropping methods --\nleveraging either external localization models or the decision process of the\ngiven MLLM itself -- as inference time mechanisms to improve the zero-shot\nperformance of MLLMs. We study their effectiveness on four popular VQA\ndatasets, and a subset of the VQAv2 dataset tailored towards fine visual\ndetails. Our findings suggest that MLLMs should be used with caution in\ndetail-sensitive VQA applications, and that visual cropping is a promising\ndirection to improve their zero-shot performance. To facilitate further\ninvestigation of MLLMs' behaviors, our code and data are publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2310.16033v3.pdf"
    },
    {
        "title": "Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation",
        "authors": [
            "Szymon Antoniak",
            "Sebastian Jaszczur",
            "Micha\u0142 Krutul",
            "Maciej Pi\u00f3ro",
            "Jakub Krajewski",
            "Jan Ludziejewski",
            "Tomasz Odrzyg\u00f3\u017ad\u017a",
            "Marek Cygan"
        ],
        "published": "2023-10-24T16:03:57Z",
        "summary": "Despite the promise of Mixture of Experts (MoE) models in increasing\nparameter counts of Transformer models while maintaining training and inference\ncosts, their application carries notable drawbacks. The key strategy of these\nmodels is to, for each processed token, activate at most a few experts -\nsubsets of an extensive feed-forward layer. But this approach is not without\nits challenges. The operation of matching experts and tokens is discrete, which\nmakes MoE models prone to issues like training instability and uneven expert\nutilization. Existing techniques designed to address these concerns, such as\nauxiliary losses or balance-aware matching, result either in lower model\nperformance or are more difficult to train. In response to these issues, we\npropose Mixture of Tokens, a fully-differentiable model that retains the\nbenefits of MoE architectures while avoiding the aforementioned difficulties.\nRather than routing tokens to experts, this approach mixes tokens from\ndifferent examples prior to feeding them to experts, enabling the model to\nlearn from all token-expert combinations. Importantly, this mixing can be\ndisabled to avoid mixing of different sequences during inference. Crucially,\nthis method is fully compatible with both masked and causal Large Language\nModel training and inference.",
        "pdf_link": "https://arxiv.org/pdf/2310.15961v1.pdf"
    },
    {
        "title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Zhichao Yang",
            "Huixue Zhou",
            "Rumeng Li",
            "Xun Wang",
            "Yucheng Xu",
            "Hong Yu"
        ],
        "published": "2023-10-24T15:59:43Z",
        "summary": "We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.",
        "pdf_link": "https://arxiv.org/pdf/2310.15959v2.pdf"
    },
    {
        "title": "Representation Learning with Large Language Models for Recommendation",
        "authors": [
            "Xubin Ren",
            "Wei Wei",
            "Lianghao Xia",
            "Lixin Su",
            "Suqi Cheng",
            "Junfeng Wang",
            "Dawei Yin",
            "Chao Huang"
        ],
        "published": "2023-10-24T15:51:13Z",
        "summary": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.",
        "pdf_link": "https://arxiv.org/pdf/2310.15950v4.pdf"
    },
    {
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": [
            "Iker Garc\u00eda-Ferrero",
            "Bego\u00f1a Altuna",
            "Javier \u00c1lvez",
            "Itziar Gonzalez-Dios",
            "German Rigau"
        ],
        "published": "2023-10-24T15:38:21Z",
        "summary": "Although large language models (LLMs) have apparently acquired a certain\nlevel of grammatical knowledge and the ability to make generalizations, they\nfail to interpret negation, a crucial step in Natural Language Processing. We\ntry to clarify the reasons for the sub-optimal performance of LLMs\nunderstanding negation. We introduce a large semi-automatically generated\ndataset of circa 400,000 descriptive sentences about commonsense knowledge that\ncan be true or false in which negation is present in about 2/3 of the corpus in\ndifferent forms. We have used our dataset with the largest available open LLMs\nin a zero-shot approach to grasp their generalization and inference capability\nand we have also fine-tuned some of the models to assess whether the\nunderstanding of negation can be trained. Our findings show that, while LLMs\nare proficient at classifying affirmative sentences, they struggle with\nnegative sentences and lack a deep understanding of negation, often relying on\nsuperficial cues. Although fine-tuning the models on negative sentences\nimproves their performance, the lack of generalization in handling negation is\npersistent, highlighting the ongoing challenges of LLMs regarding negation\nunderstanding and generalization. The dataset and code are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2310.15941v1.pdf"
    },
    {
        "title": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity",
        "authors": [
            "Yun Li",
            "Lin Niu",
            "Xipeng Zhang",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2023-10-24T15:27:15Z",
        "summary": "Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.",
        "pdf_link": "https://arxiv.org/pdf/2310.15929v2.pdf"
    },
    {
        "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT",
        "authors": [
            "Yirong Chen",
            "Zhenyu Wang",
            "Xiaofen Xing",
            "huimin zheng",
            "Zhipei Xu",
            "Kai Fang",
            "Junhong Wang",
            "Sihang Li",
            "Jieling Wu",
            "Qi Liu",
            "Xiangmin Xu"
        ],
        "published": "2023-10-24T14:57:34Z",
        "summary": "Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.",
        "pdf_link": "https://arxiv.org/pdf/2310.15896v2.pdf"
    },
    {
        "title": "SoK: Memorization in General-Purpose Large Language Models",
        "authors": [
            "Valentin Hartmann",
            "Anshuman Suri",
            "Vincent Bindschaedler",
            "David Evans",
            "Shruti Tople",
            "Robert West"
        ],
        "published": "2023-10-24T14:25:53Z",
        "summary": "Large Language Models (LLMs) are advancing at a remarkable pace, with myriad\napplications under development. Unlike most earlier machine learning models,\nthey are no longer built for one specific application but are designed to excel\nin a wide range of tasks. A major part of this success is due to their huge\ntraining datasets and the unprecedented number of model parameters, which allow\nthem to memorize large amounts of information contained in the training data.\nThis memorization goes beyond mere language, and encompasses information only\npresent in a few documents. This is often desirable since it is necessary for\nperforming tasks such as question answering, and therefore an important part of\nlearning, but also brings a whole array of issues, from privacy and security to\ncopyright and beyond. LLMs can memorize short secrets in the training data, but\ncan also memorize concepts like facts or writing styles that can be expressed\nin text in many different ways. We propose a taxonomy for memorization in LLMs\nthat covers verbatim text, facts, ideas and algorithms, writing styles,\ndistributional properties, and alignment goals. We describe the implications of\neach type of memorization - both positive and negative - for model performance,\nprivacy, security and confidentiality, copyright, and auditing, and ways to\ndetect and prevent memorization. We further highlight the challenges that arise\nfrom the predominant way of defining memorization with respect to model\nbehavior instead of model weights, due to LLM-specific phenomena such as\nreasoning capabilities or differences between decoding algorithms. Throughout\nthe paper, we describe potential risks and opportunities arising from\nmemorization in LLMs that we hope will motivate new research directions.",
        "pdf_link": "https://arxiv.org/pdf/2310.18362v1.pdf"
    },
    {
        "title": "Self-Guard: Empower the LLM to Safeguard Itself",
        "authors": [
            "Zezhong Wang",
            "Fangkai Yang",
            "Lu Wang",
            "Pu Zhao",
            "Hongru Wang",
            "Liang Chen",
            "Qingwei Lin",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-24T14:08:26Z",
        "summary": "The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.",
        "pdf_link": "https://arxiv.org/pdf/2310.15851v2.pdf"
    },
    {
        "title": "Unnatural language processing: How do language models handle machine-generated prompts?",
        "authors": [
            "Corentin Kervadec",
            "Francesca Franzon",
            "Marco Baroni"
        ],
        "published": "2023-10-24T13:32:20Z",
        "summary": "Language model prompt optimization research has shown that semantically and\ngrammatically well-formed manually crafted prompts are routinely outperformed\nby automatically generated token sequences with no apparent meaning or\nsyntactic structure, including sequences of vectors from a model's embedding\nspace. We use machine-generated prompts to probe how models respond to input\nthat is not composed of natural language expressions. We study the behavior of\nmodels of different sizes in multiple semantic tasks in response to both\ncontinuous and discrete machine-generated prompts, and compare it to the\nbehavior in response to human-generated natural-language prompts. Even when\nproducing a similar output, machine-generated and human prompts trigger\ndifferent response patterns through the network processing pathways, including\ndifferent perplexities, different attention and output entropy distributions,\nand different unit activation profiles. We provide preliminary insight into the\nnature of the units activated by different prompt types, suggesting that only\nnatural language prompts recruit a genuinely linguistic circuit.",
        "pdf_link": "https://arxiv.org/pdf/2310.15829v1.pdf"
    },
    {
        "title": "Generative Language Models Exhibit Social Identity Biases",
        "authors": [
            "Tiancheng Hu",
            "Yara Kyrychenko",
            "Steve Rathje",
            "Nigel Collier",
            "Sander van der Linden",
            "Jon Roozenbeek"
        ],
        "published": "2023-10-24T13:17:40Z",
        "summary": "The surge in popularity of large language models has given rise to concerns\nabout biases that these models could learn from humans. In this study, we\ninvestigate whether ingroup solidarity and outgroup hostility, fundamental\nsocial biases known from social science, are present in 51 large language\nmodels. We find that almost all foundational language models and some\ninstruction fine-tuned models exhibit clear ingroup-positive and\noutgroup-negative biases when prompted to complete sentences (e.g., \"We\nare...\"). A comparison of LLM-generated sentences with human-written sentences\non the internet reveals that these models exhibit similar level, if not\ngreater, levels of bias than human text. To investigate where these biases stem\nfrom, we experimentally varied the amount of ingroup-positive or\noutgroup-negative sentences the model was exposed to during fine-tuning in the\ncontext of the United States Democrat-Republican divide. Doing so resulted in\nthe models exhibiting a marked increase in ingroup solidarity and an even\ngreater increase in outgroup hostility. Furthermore, removing either\ningroup-positive or outgroup-negative sentences (or both) from the fine-tuning\ndata leads to a significant reduction in both ingroup solidarity and outgroup\nhostility, suggesting that biases can be reduced by removing biased training\ndata. Our findings suggest that modern language models exhibit fundamental\nsocial identity biases and that such biases can be mitigated by curating\ntraining data. Our results have practical implications for creating less biased\nlarge-language models and further underscore the need for more research into\nuser interactions with LLMs to prevent potential bias reinforcement in humans.",
        "pdf_link": "https://arxiv.org/pdf/2310.15819v1.pdf"
    },
    {
        "title": "Improving generalization in large language models by learning prefix subspaces",
        "authors": [
            "Louis Falissard",
            "Vincent Guigue",
            "Laure Soulier"
        ],
        "published": "2023-10-24T12:44:09Z",
        "summary": "This article focuses on large language models (LLMs) fine-tuning in the\nscarce data regime (also known as the \"few-shot\" learning setting). We propose\na method to increase the generalization capabilities of LLMs based on neural\nnetwork subspaces. This optimization method, recently introduced in computer\nvision, aims to improve model generalization by identifying wider local optima\nthrough the joint optimization of an entire simplex of models in parameter\nspace. Its adaptation to massive, pretrained transformers, however, poses some\nchallenges. First, their considerable number of parameters makes it difficult\nto train several models jointly, and second, their deterministic parameter\ninitialization schemes make them unfit for the subspace method as originally\nproposed. We show in this paper that \"Parameter Efficient Fine-Tuning\" (PEFT)\nmethods, however, are perfectly compatible with this original approach, and\npropose to learn entire simplex of continuous prefixes. We test our method on a\nvariant of the GLUE benchmark adapted to the few-shot learning setting, and\nshow that both our contributions jointly lead to a gain in average performances\ncompared to sota methods. The implementation can be found at the following\nlink: https://github.com/Liloulou/prefix_subspace",
        "pdf_link": "https://arxiv.org/pdf/2310.15793v1.pdf"
    },
    {
        "title": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers",
        "authors": [
            "Mosh Levy",
            "Shauli Ravfogel",
            "Yoav Goldberg"
        ],
        "published": "2023-10-24T12:37:06Z",
        "summary": "Recent applications of LLMs in Machine Reading Comprehension (MRC) systems\nhave shown impressive results, but the use of shortcuts, mechanisms triggered\nby features spuriously correlated to the true label, has emerged as a potential\nthreat to their reliability. We analyze the problem from two angles: LLMs as\neditors, guided to edit text to mislead LLMs; and LLMs as readers, who answer\nquestions based on the edited text. We introduce a framework that guides an\neditor to add potential shortcuts-triggers to samples. Using GPT4 as the\neditor, we find it can successfully edit trigger shortcut in samples that fool\nLLMs. Analysing LLMs as readers, we observe that even capable LLMs can be\ndeceived using shortcut knowledge. Strikingly, we discover that GPT4 can be\ndeceived by its own edits (15% drop in F1). Our findings highlight inherent\nvulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a\ncurated dataset generated by our framework for future research.",
        "pdf_link": "https://arxiv.org/pdf/2310.18360v1.pdf"
    },
    {
        "title": "Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection",
        "authors": [
            "Dennis Fucci",
            "Marco Gaido",
            "Sara Papi",
            "Mauro Cettolo",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2023-10-24T11:55:16Z",
        "summary": "When translating words referring to the speaker, speech translation (ST)\nsystems should not resort to default masculine generics nor rely on potentially\nmisleading vocal traits. Rather, they should assign gender according to the\nspeakers' preference. The existing solutions to do so, though effective, are\nhardly feasible in practice as they involve dedicated model re-training on\ngender-labeled ST data. To overcome these limitations, we propose the first\ninference-time solution to control speaker-related gender inflections in ST.\nOur approach partially replaces the (biased) internal language model (LM)\nimplicitly learned by the ST decoder with gender-specific external LMs.\nExperiments on en->es/fr/it show that our solution outperforms the base models\nand the best training-time mitigation strategy by up to 31.0 and 1.6 points in\ngender accuracy, respectively, for feminine forms. The gains are even larger\n(up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits\nconflict with their gender.",
        "pdf_link": "https://arxiv.org/pdf/2310.15752v1.pdf"
    },
    {
        "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
        "authors": [
            "Dohwan Ko",
            "Ji Soo Lee",
            "Wooyoung Kang",
            "Byungseok Roh",
            "Hyunwoo J. Kim"
        ],
        "published": "2023-10-24T11:44:39Z",
        "summary": "Large Language Models (LLMs) have shown remarkable performances on a wide\nrange of natural language understanding and generation tasks. We observe that\nthe LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$\nfor temporal and causal reasoning in Video Question Answering (VideoQA).\nHowever, such priors often cause suboptimal results on VideoQA by leading the\nmodel to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$,\nwhile ignoring visual content. This is also known as `ungrounded guesses' or\n`hallucinations'. To address this problem while leveraging LLMs' prior on\nVideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to\npredict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping\nthe source pair and the target label to understand their complex relationships,\n$\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,\nrespectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to\nLLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five\nchallenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general\nframework that is applicable to various LLMs (OPT and GPT-J) and consistently\nimproves their performances. We empirically demonstrate that Flipped-VQA not\nonly enhances the exploitation of linguistic shortcuts but also mitigates the\nlinguistic bias, which causes incorrect answers over-relying on the question.\nCode is available at https://github.com/mlvlab/Flipped-VQA.",
        "pdf_link": "https://arxiv.org/pdf/2310.15747v2.pdf"
    },
    {
        "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
        "authors": [
            "Zeyuan Yang",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023-10-24T11:40:34Z",
        "summary": "Large Language Models (LLMs) have showcased impressive performance. However,\ndue to their inability to capture relationships among samples, these frozen\nLLMs inevitably keep repeating similar mistakes. In this work, we propose our\nTuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving\ntheir performance by learning from previous mistakes. Considering data arrives\nsequentially, LLMs gradually accumulate rules from incorrect cases, forming a\nrule collection. These rules are then utilized by the LLMs to avoid making\nsimilar mistakes when processing subsequent inputs. Moreover, the rules remain\nindependent of the primary prompts, seamlessly complementing prompt design\nstrategies. Experimentally, we show that TRAN improves over recent baselines by\na large margin.",
        "pdf_link": "https://arxiv.org/pdf/2310.15746v1.pdf"
    },
    {
        "title": "Prevalence and prevention of large language model use in crowd work",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Philip Cozzolino",
            "Andrew Gordon",
            "David Rothschild",
            "Robert West"
        ],
        "published": "2023-10-24T09:52:09Z",
        "summary": "We show that the use of large language models (LLMs) is prevalent among crowd\nworkers, and that targeted mitigation strategies can significantly reduce, but\nnot eliminate, LLM use. On a text summarization task where workers were not\ndirected in any way regarding their LLM use, the estimated prevalence of LLM\nuse was around 30%, but was reduced by about half by asking workers to not use\nLLMs and by raising the cost of using them, e.g., by disabling copy-pasting.\nSecondary analyses give further insight into LLM use and its prevention: LLM\nuse yields high-quality but homogeneous responses, which may harm research\nconcerned with human (rather than model) behavior and degrade future models\ntrained with crowdsourced data. At the same time, preventing LLM use may be at\nodds with obtaining high-quality responses; e.g., when requesting workers not\nto use LLMs, summaries contained fewer keywords carrying essential information.\nOur estimates will likely change as LLMs increase in popularity or\ncapabilities, and as norms around their usage change. Yet, understanding the\nco-evolution of LLM-based tools and users is key to maintaining the validity of\nresearch done using crowdsourcing, and we provide a critical baseline before\nwidespread adoption ensues.",
        "pdf_link": "https://arxiv.org/pdf/2310.15683v1.pdf"
    },
    {
        "title": "A Survey on Detection of LLMs-Generated Content",
        "authors": [
            "Xianjun Yang",
            "Liangming Pan",
            "Xuandong Zhao",
            "Haifeng Chen",
            "Linda Petzold",
            "William Yang Wang",
            "Wei Cheng"
        ],
        "published": "2023-10-24T09:10:26Z",
        "summary": "The burgeoning capabilities of advanced large language models (LLMs) such as\nChatGPT have led to an increase in synthetic content generation with\nimplications across a variety of sectors, including media, cybersecurity,\npublic discourse, and education. As such, the ability to detect LLMs-generated\ncontent has become of paramount importance. We aim to provide a detailed\noverview of existing detection strategies and benchmarks, scrutinizing their\ndifferences and identifying key challenges and prospects in the field,\nadvocating for more adaptable and robust models to enhance detection accuracy.\nWe also posit the necessity for a multi-faceted approach to defend against\nvarious attacks to counter the rapidly advancing capabilities of LLMs. To the\nbest of our knowledge, this work is the first comprehensive survey on the\ndetection in the era of LLMs. We hope it will provide a broad understanding of\nthe current landscape of LLMs-generated content detection, offering a guiding\nreference for researchers and practitioners striving to uphold the integrity of\ndigital information in an era increasingly dominated by synthetic content. The\nrelevant papers are summarized and will be consistently updated at\nhttps://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.",
        "pdf_link": "https://arxiv.org/pdf/2310.15654v1.pdf"
    },
    {
        "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
        "authors": [
            "Junyi Liu",
            "Liangzhi Li",
            "Tong Xiang",
            "Bowen Wang",
            "Yiming Qian"
        ],
        "published": "2023-10-24T06:56:38Z",
        "summary": "Since ChatGPT released its API for public use, the number of applications\nbuilt on top of commercial large language models (LLMs) increase exponentially.\nOne popular usage of such models is leveraging its in-context learning ability\nand generating responses given user queries leveraging knowledge obtained by\nretrieval augmentation. One problem of deploying commercial retrieval-augmented\nLLMs is the cost due to the additionally retrieved context that largely\nincreases the input token size of the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two methods: summarization compression\nand semantic compression. The first method applies a T5-based model that is\nfine-tuned by datasets generated using self-instruct containing samples with\nvarying lengths and reduce token size by doing summarization. The second method\nfurther compresses the token size by removing words with lower impact on the\nsemantic. In order to adequately evaluate the effectiveness of the proposed\nmethods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)\nfocusing on food recommendation for women around pregnancy period or infants.\nOur summarization compression can reduce 65% of the retrieval token size with\nfurther 0.3% improvement on the accuracy; semantic compression provides a more\nflexible way to trade-off the token size with performance, for which we can\nreduce the token size by 20% with only 1.6% of accuracy drop.",
        "pdf_link": "https://arxiv.org/pdf/2310.15556v2.pdf"
    },
    {
        "title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation",
        "authors": [
            "Jialing Pan",
            "Adrien Sad\u00e9",
            "Jin Kim",
            "Eric Soriano",
            "Guillem Sole",
            "Sylvain Flamant"
        ],
        "published": "2023-10-24T06:04:28Z",
        "summary": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et\nal., 2023) and Code Llama (Rozi\\`ere et al., 2023) have demonstrated remarkable\nperformance in code generation. However, there is still a need for improvement\nin code translation functionality with efficient training techniques. In\nresponse to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM\ndesigned specifically for multi-programming language-to-Python code\ntranslation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or\nPHP-to-Python code translation without specifying the input programming\nlanguage. We modified StarCoder model architecture by incorporating a\nMixture-of-Experts (MoE) technique featuring five experts and a gating network\nfor multi-task handling. Experts are obtained by StarCoder fine-tuning.\nSpecifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each\nexpert size as only 0.06% of number of StarCoder's parameters. At the same\ntime, to enhance training efficiency in terms of time, we adopt curriculum\nlearning strategy and use self-instruct data for efficient fine-tuning. As a\nresult, each expert takes only 6 hours to train on one single 80Gb A100 HBM.\nWith experiments on XLCoST datasets, SteloCoder achieves an average of 73.76\nCodeBLEU score in multi-programming language-to-Python translation, surpassing\nthe top performance from the leaderboard by at least 3.5. This accomplishment\nis attributed to only 45M extra parameters with StarCoder as the backbone and\n32 hours of valid training on one 80GB A100 HBM. The source code is release\nhere: https://github.com/sade-adrien/SteloCoder.",
        "pdf_link": "https://arxiv.org/pdf/2310.15539v2.pdf"
    },
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
        "authors": [
            "Marah I Abdin",
            "Suriya Gunasekar",
            "Varun Chandrasekaran",
            "Jerry Li",
            "Mert Yuksekgonul",
            "Rahee Ghosh Peshawaria",
            "Ranjita Naik",
            "Besmira Nushi"
        ],
        "published": "2023-10-24T04:40:38Z",
        "summary": "We study the ability of state-of-the art models to answer constraint\nsatisfaction queries for information retrieval (e.g., 'a list of ice cream\nshops in San Diego'). In the past, such queries were considered to be tasks\nthat could only be solved via web-search or knowledge bases. More recently,\nlarge language models (LLMs) have demonstrated initial emergent abilities in\nthis task. However, many current retrieval benchmarks are either saturated or\ndo not measure constraint satisfaction. Motivated by rising concerns around\nfactual incorrectness and hallucinations of LLMs, we present KITAB, a new\ndataset for measuring constraint satisfaction abilities of language models.\nKITAB consists of book-related data across more than 600 authors and 13,000\nqueries, and also offers an associated dynamic data collection and constraint\nverification approach for acquiring similar test data for other authors. Our\nextended experiments on GPT4 and GPT3.5 characterize and decouple common\nfailure modes across dimensions such as information popularity, constraint\ntypes, and context availability. Results show that in the absence of context,\nmodels exhibit severe limitations as measured by irrelevant information,\nfactual errors, and incompleteness, many of which exacerbate as information\npopularity decreases. While context availability mitigates irrelevant\ninformation, it is not helpful for satisfying constraints, identifying\nfundamental barriers to constraint satisfaction. We open source our\ncontributions to foster further research on improving constraint satisfaction\nabilities of future models.",
        "pdf_link": "https://arxiv.org/pdf/2310.15511v1.pdf"
    },
    {
        "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks",
        "authors": [
            "Xiaoyi Chen",
            "Siyuan Tang",
            "Rui Zhu",
            "Shijun Yan",
            "Lei Jin",
            "Zihao Wang",
            "Liya Su",
            "XiaoFeng Wang",
            "Haixu Tang"
        ],
        "published": "2023-10-24T02:48:19Z",
        "summary": "The era post-2018 marked the advent of Large Language Models (LLMs), with\ninnovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.\nAs the industry galloped toward augmenting model parameters and capitalizing on\nvast swaths of human language data, security and privacy challenges also\nemerged. Foremost among these is the potential inadvertent accrual of Personal\nIdentifiable Information (PII) during web-based data acquisition, posing risks\nof unintended PII disclosure. While strategies like RLHF during training and\nCatastrophic Forgetting have been marshaled to control the risk of privacy\ninfringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning\ninterface for GPT-3.5, have reignited concerns. One may ask: can the\nfine-tuning of LLMs precipitate the leakage of personal information embedded\nwithin training datasets? This paper reports the first endeavor to seek the\nanswer to the question, particularly our discovery of a new LLM exploitation\navenue, called the Janus attack. In the attack, one can construct a PII\nassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,\nto potentially reinstate and reveal concealed PIIs. Our findings indicate that,\nwith a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from\nbeing impermeable to PII extraction to a state where they divulge a substantial\nproportion of concealed PII. This research, through its deep dive into the\nJanus attack vector, underscores the imperative of navigating the intricate\ninterplay between LLM utility and privacy preservation.",
        "pdf_link": "https://arxiv.org/pdf/2310.15469v1.pdf"
    },
    {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Le Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023-10-24T00:24:11Z",
        "summary": "Theory of mind (ToM) evaluations currently focus on testing models using\npassive narratives that inherently lack interactivity. We introduce FANToM, a\nnew benchmark designed to stress-test ToM within information-asymmetric\nconversational contexts via question answering. Our benchmark draws upon\nimportant theoretical requisites from psychology and necessary empirical\nconsiderations when evaluating large language models (LLMs). In particular, we\nformulate multiple types of questions that demand the same underlying reasoning\nto identify illusory or false sense of ToM capabilities in LLMs. We show that\nFANToM is challenging for state-of-the-art LLMs, which perform significantly\nworse than humans even with chain-of-thought reasoning or fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.15421v3.pdf"
    },
    {
        "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions",
        "authors": [
            "Ting-Yao Hsu",
            "Chieh-Yang Huang",
            "Ryan Rossi",
            "Sungchul Kim",
            "C. Lee Giles",
            "Ting-Hao K. Huang"
        ],
        "published": "2023-10-23T23:24:57Z",
        "summary": "There is growing interest in systems that generate captions for scientific\nfigures. However, assessing these systems output poses a significant challenge.\nHuman evaluation requires academic expertise and is costly, while automatic\nevaluation depends on often low-quality author-written captions. This paper\ninvestigates using large language models (LLMs) as a cost-effective,\nreference-free method for evaluating figure captions. We first constructed\nSCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600\nscientific figure captions, both original and machine-made, for 600 arXiv\nfigures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption\nbased on its potential to aid reader understanding, given relevant context such\nas figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot\nevaluator, outperformed all other models and even surpassed assessments made by\nComputer Science and Informatics undergraduates, achieving a Kendall\ncorrelation score of 0.401 with Ph.D. students rankings",
        "pdf_link": "https://arxiv.org/pdf/2310.15405v1.pdf"
    },
    {
        "title": "DoGE: Domain Reweighting with Generalization Estimation",
        "authors": [
            "Simin Fan",
            "Matteo Pagliardini",
            "Martin Jaggi"
        ],
        "published": "2023-10-23T22:51:58Z",
        "summary": "The coverage and composition of the pretraining data significantly impacts\nthe generalization ability of Large Language Models (LLMs). Despite its\nimportance, recent LLMs still rely on heuristics and trial and error to\nincrease or reduce the influence of data-domains. We propose DOmain reweighting\nwith Generalization Estimation (DoGE), which optimizes the probability of\nsampling from each domain (domain weights) in a principled way. Our approach is\na two-stage process consisting of (i) training a proxy model to obtain domain\nweights using a bi-level optimization algorithm; (ii) training a larger base\nmodel by sampling training domains according to the learned domain weights. In\nour experiments, we extensively show how DoGE improves the generalization of\nthe base model to any target data mixture. On the SlimPajama dataset, our base\nmodel gets better perplexity and few-shot reasoning accuracies across $6$ tasks\ncompared to baseline methods. Moreover, aiming to generalize to out-of-domain\ntarget tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can\neffectively identify inter-domain dependencies, and consistently achieves\nbetter test perplexity on the target domain.",
        "pdf_link": "https://arxiv.org/pdf/2310.15393v2.pdf"
    },
    {
        "title": "Irreducible Curriculum for Language Model Pretraining",
        "authors": [
            "Simin Fan",
            "Martin Jaggi"
        ],
        "published": "2023-10-23T22:41:33Z",
        "summary": "Automatic data selection and curriculum design for training large language\nmodels is challenging, with only a few existing methods showing improvements\nover standard training. Furthermore, current schemes focus on domain-level\nselection, overlooking the more fine-grained contributions of each individual\ntraining point. It is difficult to apply traditional datapoint selection\nmethods on large language models: most online batch selection methods perform\ntwo-times forward or backward passes, which introduces considerable extra costs\nwith large-scale models. To mitigate these obstacles, we propose irreducible\ncurriculum as a curriculum learning algorithm for language model pretraining,\nwhich prioritizes samples with higher learnability. Specifically, to avoid\nprohibitive extra computation overhead, we simulate the sample loss along the\nmain model's training trajectory using a small-scale proxy model. Our\nexperiments on the RedPajama-1B dataset demonstrate a consistent improvement on\nvalidation perplexity across all 7 domains compared to random uniform baseline\nand the anti-curriculum strategy. Our method also reduces the sharpness of the\nnetwork and illustrates a better 5-shot accuracy on MMLU benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2310.15389v1.pdf"
    },
    {
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "authors": [
            "Gabriele Prato",
            "Jerry Huang",
            "Prasannna Parthasarathi",
            "Shagun Sodhani",
            "Sarath Chandar"
        ],
        "published": "2023-10-23T21:15:54Z",
        "summary": "In the age of artificial intelligence, the role of large language models\n(LLMs) is becoming increasingly central. Despite their growing prevalence,\ntheir capacity to consolidate knowledge from different training documents - a\ncrucial ability in numerous applications - remains unexplored. This paper\npresents the first study examining the capability of LLMs to effectively\ncombine such information within their parameter space. We introduce EpiK-Eval,\na novel question-answering benchmark tailored to evaluate LLMs' proficiency in\nformulating a coherent and consistent knowledge representation from segmented\nnarratives. Evaluations across various LLMs reveal significant weaknesses in\nthis domain. We contend that these shortcomings stem from the intrinsic nature\nof prevailing training objectives. Consequently, we advocate for refining the\napproach towards knowledge consolidation, as it harbors the potential to\ndramatically improve their overall effectiveness and performance. The findings\nfrom this study offer insights for developing more robust and reliable LLMs.\nOur code and benchmark are available at\nhttps://github.com/chandar-lab/EpiK-Eval",
        "pdf_link": "https://arxiv.org/pdf/2310.15372v2.pdf"
    },
    {
        "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
        "authors": [
            "Adam Bouyamourn"
        ],
        "published": "2023-10-23T20:35:52Z",
        "summary": "We show that LLMs hallucinate because their output is not constrained to be\nsynonymous with claims for which they have evidence: a condition that we call\nevidential closure. Information about the truth or falsity of sentences is not\nstatistically identified in the standard neural probabilistic language model\nsetup, and so cannot be conditioned on to generate new strings. We then show\nhow to constrain LLMs to produce output that does satisfy evidential closure. A\nmultimodal LLM must learn about the external world (perceptual learning); it\nmust learn a mapping from strings to states of the world (extensional\nlearning); and, to achieve fluency when generalizing beyond a body of evidence,\nit must learn mappings from strings to their synonyms (intensional learning).\nThe output of a unimodal LLM must be synonymous with strings in a validated\nevidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,\nthat yields faithful output from an LLM by rejecting output that is not\nsynonymous with claims for which the LLM has evidence.",
        "pdf_link": "https://arxiv.org/pdf/2310.15355v1.pdf"
    },
    {
        "title": "Moral Foundations of Large Language Models",
        "authors": [
            "Marwa Abdulhai",
            "Gregory Serapio-Garcia",
            "Cl\u00e9ment Crepy",
            "Daria Valter",
            "John Canny",
            "Natasha Jaques"
        ],
        "published": "2023-10-23T20:05:37Z",
        "summary": "Moral foundations theory (MFT) is a psychological assessment tool that\ndecomposes human moral reasoning into five factors, including care/harm,\nliberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary\nin the weight they place on these dimensions when making moral decisions, in\npart due to their cultural upbringing and political ideology. As large language\nmodels (LLMs) are trained on datasets collected from the internet, they may\nreflect the biases that are present in such corpora. This paper uses MFT as a\nlens to analyze whether popular LLMs have acquired a bias towards a particular\nset of moral values. We analyze known LLMs and find they exhibit particular\nmoral foundations, and show how these relate to human moral foundations and\npolitical affiliations. We also measure the consistency of these biases, or\nwhether they vary strongly depending on the context of how the model is\nprompted. Finally, we show that we can adversarially select prompts that\nencourage the moral to exhibit a particular set of moral foundations, and that\nthis can affect the model's behavior on downstream tasks. These findings help\nillustrate the potential risks and unintended consequences of LLMs assuming a\nparticular moral stance.",
        "pdf_link": "https://arxiv.org/pdf/2310.15337v1.pdf"
    },
    {
        "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
        "authors": [
            "Soumya Suvra Ghosal",
            "Souradip Chakraborty",
            "Jonas Geiping",
            "Furong Huang",
            "Dinesh Manocha",
            "Amrit Singh Bedi"
        ],
        "published": "2023-10-23T18:11:32Z",
        "summary": "Large Language Models (LLMs) have revolutionized the domain of natural\nlanguage processing (NLP) with remarkable capabilities of generating human-like\ntext responses. However, despite these advancements, several works in the\nexisting literature have raised serious concerns about the potential misuse of\nLLMs such as spreading misinformation, generating fake news, plagiarism in\nacademia, and contaminating the web. To address these concerns, a consensus\namong the research community is to develop algorithmic solutions to detect\nAI-generated text. The basic idea is that whenever we can tell if the given\ntext is either written by a human or an AI, we can utilize this information to\naddress the above-mentioned concerns. To that end, a plethora of detection\nframeworks have been proposed, highlighting the possibilities of AI-generated\ntext detection. But in parallel to the development of detection frameworks,\nresearchers have also concentrated on designing strategies to elude detection,\ni.e., focusing on the impossibilities of AI-generated text detection. This is a\ncrucial step in order to make sure the detection frameworks are robust enough\nand it is not too easy to fool a detector. Despite the huge interest and the\nflurry of research in this domain, the community currently lacks a\ncomprehensive analysis of recent developments. In this survey, we aim to\nprovide a concise categorization and overview of current work encompassing both\nthe prospects and the limitations of AI-generated text detection. To enrich the\ncollective knowledge, we engage in an exhaustive discussion on critical and\nchallenging open questions related to ongoing research on AI-generated text\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2310.15264v1.pdf"
    },
    {
        "title": "Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number",
        "authors": [
            "Sophie Hao",
            "Tal Linzen"
        ],
        "published": "2023-10-23T17:53:47Z",
        "summary": "Deep architectures such as Transformers are sometimes criticized for having\nuninterpretable \"black-box\" representations. We use causal intervention\nanalysis to show that, in fact, some linguistic features are represented in a\nlinear, interpretable format. Specifically, we show that BERT's ability to\nconjugate verbs relies on a linear encoding of subject number that can be\nmanipulated with predictable effects on conjugation accuracy. This encoding is\nfound in the subject position at the first layer and the verb position at the\nlast layer, but distributed across positions at middle layers, particularly\nwhen there are multiple cues to subject number.",
        "pdf_link": "https://arxiv.org/pdf/2310.15151v1.pdf"
    },
    {
        "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models",
        "authors": [
            "Fangyu Lei",
            "Qian Liu",
            "Yiming Huang",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-23T17:52:06Z",
        "summary": "The rapid development of Large Language Models (LLMs) has led to great\nstrides in model capabilities like long-context understanding and reasoning.\nHowever, as LLMs are able to process longer contexts, it becomes more\nchallenging to evaluate whether they have acquired certain capabilities, since\nthe length of text (e.g., 200K tokens) they can process far exceeds what humans\ncan reliably assess in a reasonable duration. In this paper, we propose using\ncomplex synthetic tasks as a proxy evaluation method, and present S3Eval, a\nSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The\nsynthetic nature of S3Eval provides users full control over the dataset,\nallowing them to systematically probe LLM capabilities by scaling text length\nand varying task difficulty across diverse scenarios. The strong correlation\nbetween S3Eval and real-world benchmarks demonstrates the soundness of using\nS3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite\nlong-context data generation method. We have generated a comprehensive dataset\ncalled S3Eval-Standard, and experimental results have shown that it poses\nsignificant challenges for all existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.15147v2.pdf"
    },
    {
        "title": "Causal Inference Using LLM-Guided Discovery",
        "authors": [
            "Aniket Vashishtha",
            "Abbavaram Gowtham Reddy",
            "Abhinav Kumar",
            "Saketh Bachu",
            "Vineeth N Balasubramanian",
            "Amit Sharma"
        ],
        "published": "2023-10-23T17:23:56Z",
        "summary": "At the core of causal inference lies the challenge of determining reliable\ncausal graphs solely based on observational data. Since the well-known backdoor\ncriterion depends on the graph, any errors in the graph can propagate\ndownstream to effect inference. In this work, we initially show that complete\ngraph information is not necessary for causal effect inference; the topological\norder over graph variables (causal order) alone suffices. Further, given a node\npair, causal order is easier to elicit from domain experts compared to graph\nedges since determining the existence of an edge can depend extensively on\nother variables. Interestingly, we find that the same principle holds for Large\nLanguage Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated\nmethod to obtain causal order (and hence causal effect) with LLMs acting as\nvirtual domain experts. To this end, we employ different prompting strategies\nand contextual cues to propose a robust technique of obtaining causal order\nfrom LLMs. Acknowledging LLMs' limitations, we also study possible techniques\nto integrate LLMs with established causal discovery algorithms, including\nconstraint-based and score-based methods, to enhance their performance.\nExtensive experiments demonstrate that our approach significantly improves\ncausal ordering accuracy as compared to discovery algorithms, highlighting the\npotential of LLMs to enhance causal inference across diverse fields.",
        "pdf_link": "https://arxiv.org/pdf/2310.15117v1.pdf"
    },
    {
        "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
        "authors": [
            "Tianshi Che",
            "Ji Liu",
            "Yang Zhou",
            "Jiaxiang Ren",
            "Jiwen Zhou",
            "Victor S. Sheng",
            "Huaiyu Dai",
            "Dejing Dou"
        ],
        "published": "2023-10-23T16:37:59Z",
        "summary": "Federated learning (FL) is a promising paradigm to enable collaborative model\ntraining with decentralized data. However, the training process of Large\nLanguage Models (LLMs) generally incurs the update of significant parameters,\nwhich limits the applicability of FL techniques to tackle the LLMs in real\nscenarios. Prompt tuning can significantly reduce the number of parameters to\nupdate, but it either incurs performance degradation or low training\nefficiency. The straightforward utilization of prompt tuning in the FL often\nraises non-trivial communication costs and dramatically degrades performance.\nIn addition, the decentralized data is generally non-Independent and\nIdentically Distributed (non-IID), which brings client drift problems and thus\npoor performance. This paper proposes a Parameter-efficient prompt Tuning\napproach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and\neffective FL of LLMs. First, an efficient partial prompt tuning approach is\nproposed to improve performance and efficiency simultaneously. Second, a novel\nadaptive optimization method is developed to address the client drift problems\non both the device and server sides to enhance performance further. Extensive\nexperiments based on 10 datasets demonstrate the superb performance (up to\n60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training\ntime) of FedPepTAO compared with 9 baseline approaches. Our code is available\nat https://github.com/llm-eff/FedPepTAO.",
        "pdf_link": "https://arxiv.org/pdf/2310.15080v3.pdf"
    },
    {
        "title": "Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models",
        "authors": [
            "Matthieu Meeus",
            "Shubham Jain",
            "Marek Rei",
            "Yves-Alexandre de Montjoye"
        ],
        "published": "2023-10-23T15:00:46Z",
        "summary": "With large language models (LLMs) poised to become embedded in our daily\nlives, questions are starting to be raised about the dataset(s) they learned\nfrom. These questions range from potential bias or misinformation LLMs could\nretain from their training data to questions of copyright and fair use of\nhuman-generated text. However, while these questions emerge, developers of the\nrecent state-of-the-art LLMs become increasingly reluctant to disclose details\non their training corpus. We here introduce the task of document-level\nmembership inference for real-world LLMs, i.e. inferring whether the LLM has\nseen a given document during training or not. First, we propose a procedure for\nthe development and evaluation of document-level membership inference for LLMs\nby leveraging commonly used data sources for training and the model release\ndate. We then propose a practical, black-box method to predict document-level\nmembership and instantiate it on OpenLLaMA-7B with both books and academic\npapers. We show our methodology to perform very well, reaching an impressive\nAUC of 0.856 for books and 0.678 for papers. We then show our approach to\noutperform the sentence-level membership inference attacks used in the privacy\nliterature for the document-level membership task. We finally evaluate whether\nsmaller models might be less sensitive to document-level inference and show\nOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.\nTaken together, our results show that accurate document-level membership can be\ninferred for LLMs, increasing the transparency of technology poised to change\nour lives.",
        "pdf_link": "https://arxiv.org/pdf/2310.15007v1.pdf"
    },
    {
        "title": "Towards LLM-driven Dialogue State Tracking",
        "authors": [
            "Yujie Feng",
            "Zexin Lu",
            "Bo Liu",
            "Liming Zhan",
            "Xiao-Ming Wu"
        ],
        "published": "2023-10-23T14:15:28Z",
        "summary": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate\ntracking of user goals and system actions within task-oriented dialogue\nsystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT\nhas sparked considerable interest in assessing their efficacy across diverse\napplications. In this study, we conduct an initial examination of ChatGPT's\ncapabilities in DST. Our evaluation uncovers the exceptional performance of\nChatGPT in this task, offering valuable insights to researchers regarding its\ncapabilities and providing useful directions for designing and enhancing\ndialogue systems. Despite its impressive performance, ChatGPT has significant\nlimitations including its closed-source nature, request restrictions, raising\ndata privacy concerns, and lacking local deployment capabilities. To address\nthese concerns, we present LDST, an LLM-driven DST framework based on smaller,\nopen-source foundation models. By utilizing a novel domain-slot instruction\ntuning method, LDST achieves performance on par with ChatGPT. Comprehensive\nevaluations across three distinct experimental settings, we find that LDST\nexhibits remarkable performance improvements in both zero-shot and few-shot\nsetting compared to previous SOTA methods. The source code is provided for\nreproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2310.14970v1.pdf"
    },
    {
        "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
        "authors": [
            "Mengyu Ye",
            "Tatsuki Kuribayashi",
            "Jun Suzuki",
            "Goro Kobayashi",
            "Hiroaki Funayama"
        ],
        "published": "2023-10-23T12:40:41Z",
        "summary": "Large language models (LLMs) take advantage of step-by-step reasoning\ninstructions, e.g., chain-of-thought (CoT) prompting. Building on this, their\nability to perform CoT-style reasoning robustly is of interest from a probing\nperspective. In this study, we inspect the step-by-step reasoning ability of\nLLMs with a focus on negation, which is a core linguistic phenomenon that is\ndifficult to process. In particular, we introduce several controlled settings\n(e.g., reasoning in case of fictional entities) to evaluate the logical\nreasoning abilities of the models. We observed that dozens of modern LLMs were\nnot robust against lexical negation (e.g., plausible ->implausible) when\nperforming CoT-style reasoning, and the results highlight unique limitations in\neach LLM family.",
        "pdf_link": "https://arxiv.org/pdf/2310.14868v1.pdf"
    },
    {
        "title": "Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing",
        "authors": [
            "Sai Koneru",
            "Miriam Exel",
            "Matthias Huck",
            "Jan Niehues"
        ],
        "published": "2023-10-23T12:22:15Z",
        "summary": "Large Language Models (LLM's) have demonstrated considerable success in\nvarious Natural Language Processing tasks, but they have yet to attain\nstate-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,\ntheir significant performance in tasks demanding a broad understanding and\ncontextual processing shows their potential for translation. To exploit these\nabilities, we investigate using LLM's for MT and explore recent\nparameter-efficient fine-tuning techniques. Surprisingly, our initial\nexperiments find that fine-tuning for translation purposes even led to\nperformance degradation. To overcome this, we propose an alternative approach:\nadapting LLM's as Automatic Post-Editors (APE) rather than direct translators.\nBuilding on the LLM's exceptional ability to process and generate lengthy\nsequences, we also propose extending our approach to document-level\ntranslation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can\nyield significant improvements across both sentence and document-level metrics\nwhile generalizing to out-of-domain data. Most notably, we achieve a\nstate-of-the-art accuracy rate of 89\\% on the ContraPro test set, which\nspecifically assesses the model's ability to resolve pronoun ambiguities when\ntranslating from English to German. Lastly, we investigate a practical scenario\ninvolving manual post-editing for document-level translation, where reference\ncontext is made available. Here, we demonstrate that leveraging human\ncorrections can significantly reduce the number of edits required for\nsubsequent translations (Interactive Demo for integrating manual feedback can\nbe found here:\nhttps://huggingface.co/spaces/skoneru/contextual_refinement_ende).",
        "pdf_link": "https://arxiv.org/pdf/2310.14855v2.pdf"
    },
    {
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": [
            "Xunjian Yin",
            "Baizhou Huang",
            "Xiaojun Wan"
        ],
        "published": "2023-10-23T11:40:05Z",
        "summary": "With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2310.14820v1.pdf"
    },
    {
        "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
        "authors": [
            "Sabri Boughorbel",
            "Majd Hawasly"
        ],
        "published": "2023-10-23T11:40:04Z",
        "summary": "While significant progress has been made in benchmarking Large Language\nModels (LLMs) across various tasks, there is a lack of comprehensive evaluation\nof their abilities in responding to multi-turn instructions in less-commonly\ntested languages like Arabic. Our paper offers a detailed examination of the\nproficiency of open LLMs in such scenarios in Arabic. Utilizing a customized\nArabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a\nuniform evaluator for both English and Arabic queries to assess and compare the\nperformance of the LLMs on various open-ended tasks. Our findings reveal\nvariations in model responses on different task categories, e.g., logic vs.\nliteracy, when instructed in English or Arabic. We find that fine-tuned base\nmodels using multilingual and multi-turn datasets could be competitive to\nmodels trained from scratch on multilingual data. Finally, we hypothesize that\nan ensemble of small, open LLMs could perform competitively to proprietary LLMs\non the benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.14819v1.pdf"
    },
    {
        "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
        "authors": [
            "Libo Qin",
            "Qiguang Chen",
            "Fuxuan Wei",
            "Shijue Huang",
            "Wanxiang Che"
        ],
        "published": "2023-10-23T10:56:03Z",
        "summary": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate\nreasoning paths, thus promoting reasoning accuracy and attracting increasing\nattention. Specifically, zero-shot CoT achieves remarkable improvements in a\nwide range of reasoning tasks by simply instructing the LLM with the prompt\n\"Let's think step by step!\". Despite the success of zero-shot CoT, the existing\nzero-shot prompting techniques remain limited to a single language, making it\nchallenging to generalize to other languages and hindering global development.\nIn this work, we introduce cross-lingual prompting (CLP), aiming to improve\nzero-shot CoT reasoning across languages. Specifically, CLP consists of two\nmain components: (1) cross-lingual alignment prompting and (2) task-specific\nsolver prompting. The cross-lingual alignment prompting is responsible for\naligning representations across different languages, whereas the task-specific\nsolver prompting is used to generate the final chain of thoughts and results\nfor the reasoning task. In addition, we further introduce cross-lingual\nself-consistent prompting (CLSP) to ensemble different reasoning paths across\nlanguages. Our experimental evaluations on several benchmarks demonstrate that\nCLP and CLSP significantly outperform the existing prompting methods and\nachieve state-of-the-art performance. We hope this work will inspire further\nbreakthroughs in cross-lingual CoT.",
        "pdf_link": "https://arxiv.org/pdf/2310.14799v1.pdf"
    },
    {
        "title": "Evaluating the Knowledge Base Completion Potential of GPT",
        "authors": [
            "Blerta Veseli",
            "Simon Razniewski",
            "Jan-Christoph Kalo",
            "Gerhard Weikum"
        ],
        "published": "2023-10-23T10:15:13Z",
        "summary": "Structured knowledge bases (KBs) are an asset for search engines and other\napplications, but are inevitably incomplete. Language models (LMs) have been\nproposed for unsupervised knowledge base completion (KBC), yet, their ability\nto do this at scale and with high accuracy remains an open question. Prior\nexperimental studies mostly fall short because they only evaluate on popular\nsubjects, or sample already existing facts from KBs. In this work, we perform a\ncareful evaluation of GPT's potential to complete the largest public KB:\nWikidata. We find that, despite their size and capabilities, models like GPT-3,\nChatGPT and GPT-4 do not achieve fully convincing results on this task.\nNonetheless, they provide solid improvements over earlier approaches with\nsmaller LMs. In particular, we show that, with proper thresholding, GPT-3\nenables to extend Wikidata by 27M facts at 90% precision.",
        "pdf_link": "https://arxiv.org/pdf/2310.14771v1.pdf"
    },
    {
        "title": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",
        "authors": [
            "Junchao Wu",
            "Shu Yang",
            "Runzhe Zhan",
            "Yulin Yuan",
            "Derek F. Wong",
            "Lidia S. Chao"
        ],
        "published": "2023-10-23T09:01:13Z",
        "summary": "The powerful ability to understand, follow, and generate complex language\nemerging from large language models (LLMs) makes LLM-generated text flood many\nareas of our daily lives at an incredible speed and is widely accepted by\nhumans. As LLMs continue to expand, there is an imperative need to develop\ndetectors that can detect LLM-generated text. This is crucial to mitigate\npotential misuse of LLMs and safeguard realms like artistic expression and\nsocial networks from harmful influence of LLM-generated content. The\nLLM-generated text detection aims to discern if a piece of text was produced by\nan LLM, which is essentially a binary classification task. The detector\ntechniques have witnessed notable advancements recently, propelled by\ninnovations in watermarking techniques, zero-shot methods, fine-turning LMs\nmethods, adversarial learning methods, LLMs as detectors, and human-assisted\nmethods. In this survey, we collate recent research breakthroughs in this area\nand underscore the pressing need to bolster detector research. We also delve\ninto prevalent datasets, elucidating their limitations and developmental\nrequirements. Furthermore, we analyze various LLM-generated text detection\nparadigms, shedding light on challenges like out-of-distribution problems,\npotential attacks, and data ambiguity. Conclusively, we highlight interesting\ndirections for future research in LLM-generated text detection to advance the\nimplementation of responsible artificial intelligence (AI). Our aim with this\nsurvey is to provide a clear and comprehensive introduction for newcomers while\nalso offering seasoned researchers a valuable update in the field of\nLLM-generated text detection. The useful resources are publicly available at:\nhttps://github.com/NLP2CT/LLM-generated-Text-Detection.",
        "pdf_link": "https://arxiv.org/pdf/2310.14724v2.pdf"
    },
    {
        "title": "Establishing Vocabulary Tests as a Benchmark for Evaluating Large Language Models",
        "authors": [
            "Gonzalo Mart\u00ednez",
            "Javier Conde",
            "Elena Merino-G\u00f3mez",
            "Beatriz Berm\u00fadez-Margaretto",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Pedro Reviriego",
            "Marc Brysbaert"
        ],
        "published": "2023-10-23T08:45:12Z",
        "summary": "Vocabulary tests, once a cornerstone of language modeling evaluation, have\nbeen largely overlooked in the current landscape of Large Language Models\n(LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus\non specific tasks or domain-specific knowledge, they often neglect the\nfundamental linguistic aspects of language understanding and production. In\nthis paper, we advocate for the revival of vocabulary tests as a valuable tool\nfor assessing LLM performance. We evaluate seven LLMs using two vocabulary test\nformats across two languages and uncover surprising gaps in their lexical\nknowledge. These findings shed light on the intricacies of LLM word\nrepresentations, their learning mechanisms, and performance variations across\nmodels and languages. Moreover, the ability to automatically generate and\nperform vocabulary tests offers new opportunities to expand the approach and\nprovide a more complete picture of LLMs' language skills.",
        "pdf_link": "https://arxiv.org/pdf/2310.14703v2.pdf"
    },
    {
        "title": "Reasoning about Ambiguous Definite Descriptions",
        "authors": [
            "Stefan F. Schouten",
            "Peter Bloem",
            "Ilia Markov",
            "Piek Vossen"
        ],
        "published": "2023-10-23T07:52:38Z",
        "summary": "Natural language reasoning plays an increasingly important role in improving\nlanguage models' ability to solve complex language understanding tasks. An\ninteresting use case for reasoning is the resolution of context-dependent\nambiguity. But no resources exist to evaluate how well Large Language Models\ncan use explicit reasoning to resolve ambiguity in language. We propose to use\nambiguous definite descriptions for this purpose and create and publish the\nfirst benchmark dataset consisting of such phrases. Our method includes all\ninformation required to resolve the ambiguity in the prompt, which means a\nmodel does not require anything but reasoning to do well. We find this to be a\nchallenging task for recent LLMs. Code and data available at:\nhttps://github.com/sfschouten/exploiting-ambiguity",
        "pdf_link": "https://arxiv.org/pdf/2310.14657v1.pdf"
    },
    {
        "title": "Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue",
        "authors": [
            "Yuanxing Liu",
            "Wei-Nan Zhang",
            "Yifan Chen",
            "Yuchi Zhang",
            "Haopeng Bai",
            "Fan Feng",
            "Hengbin Cui",
            "Yongbin Li",
            "Wanxiang Che"
        ],
        "published": "2023-10-23T07:00:51Z",
        "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
        "pdf_link": "https://arxiv.org/pdf/2310.14626v1.pdf"
    },
    {
        "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
        "authors": [
            "Yanchen Liu",
            "Srishti Gautam",
            "Jiaqi Ma",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-10-23T06:31:28Z",
        "summary": "Recent literature has suggested the potential of using large language models\n(LLMs) to make classifications for tabular tasks. However, LLMs have been shown\nto exhibit harmful social biases that reflect the stereotypes and inequalities\npresent in society. To this end, as well as the widespread use of tabular data\nin many high-stake applications, it is important to explore the following\nquestions: what sources of information do LLMs draw upon when making\nclassifications for tabular tasks; whether and to what extent are LLM\nclassifications for tabular data influenced by social biases and stereotypes;\nand what are the consequential implications for fairness?\n  Through a series of experiments, we delve into these questions and show that\nLLMs tend to inherit social biases from their training data which significantly\nimpact their fairness in tabular classification tasks. Furthermore, our\ninvestigations show that in the context of bias mitigation, though in-context\nlearning and finetuning have a moderate effect, the fairness metric gap between\ndifferent subgroups is still larger than that in traditional machine learning\nmodels, such as Random Forest and shallow Neural Networks. This observation\nemphasizes that the social biases are inherent within the LLMs themselves and\ninherited from their pretraining corpus, not only from the downstream task\ndatasets. Besides, we demonstrate that label-flipping of in-context examples\ncan significantly reduce biases, further highlighting the presence of inherent\nbias within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.14607v2.pdf"
    },
    {
        "title": "Large Search Model: Redefining Search Stack in the Era of LLMs",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023-10-23T05:52:09Z",
        "summary": "Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.14587v2.pdf"
    },
    {
        "title": "Language Models Hallucinate, but May Excel at Fact Verification",
        "authors": [
            "Jian Guan",
            "Jesse Dodge",
            "David Wadden",
            "Minlie Huang",
            "Hao Peng"
        ],
        "published": "2023-10-23T04:39:01Z",
        "summary": "Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.",
        "pdf_link": "https://arxiv.org/pdf/2310.14564v2.pdf"
    },
    {
        "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
        "authors": [
            "Chiyu Zhang",
            "Khai Duy Doan",
            "Qisheng Liao",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-10-23T04:22:44Z",
        "summary": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate\nremarkable performance in a wide range of tasks. Despite numerous recent\nstudies that examine the performance of instruction-tuned LLMs on various NLP\nbenchmarks, there remains a lack of comprehensive investigation into their\nability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning\nembedded within social and interactive contexts. This deficiency arises partly\nfrom SM not being adequately represented in any of the existing benchmarks. To\naddress this gap, we present SPARROW, an extensive multilingual benchmark\nspecifically designed for SM understanding. SPARROW comprises 169 datasets\ncovering 13 task types across six primary categories (e.g., anti-social\nlanguage detection, emotion recognition). SPARROW datasets encompass 64\ndifferent languages originating from 12 language families representing 16\nwriting scripts. We evaluate the performance of various multilingual pretrained\nlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)\non SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our\ncomprehensive analysis reveals that existing open-source instruction tuned LLMs\nstill struggle to understand SM across various languages, performing close to a\nrandom baseline in some cases. We also find that although ChatGPT outperforms\nmany LLMs, it still falls behind task-specific finetuned models with a gap of\n12.19 SPARROW score. Our benchmark is available at:\nhttps://github.com/UBC-NLP/SPARROW",
        "pdf_link": "https://arxiv.org/pdf/2310.14557v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on Controlled Generation Tasks",
        "authors": [
            "Jiao Sun",
            "Yufei Tian",
            "Wangchunshu Zhou",
            "Nan Xu",
            "Qian Hu",
            "Rahul Gupta",
            "John Frederick Wieting",
            "Nanyun Peng",
            "Xuezhe Ma"
        ],
        "published": "2023-10-23T03:48:24Z",
        "summary": "While recent studies have looked into the abilities of large language models\nin various benchmark tasks, including question generation, reading\ncomprehension, multilingual and etc, there have been few studies looking into\nthe controllability of large language models on generation tasks. We present an\nextensive analysis of various benchmarks including a sentence planning\nbenchmark with different granularities. After comparing large language models\nagainst state-of-the-start finetuned smaller models, we present a spectrum\nshowing large language models falling behind, are comparable, or exceed the\nability of smaller models. We conclude that **large language models struggle at\nmeeting fine-grained hard constraints**.",
        "pdf_link": "https://arxiv.org/pdf/2310.14542v1.pdf"
    },
    {
        "title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing",
        "authors": [
            "Yating Wu",
            "Ritika Mangla",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "published": "2023-10-23T03:03:58Z",
        "summary": "Questions Under Discussion (QUD) is a versatile linguistic framework in which\ndiscourse progresses as continuously asking questions and answering them.\nAutomatic parsing of a discourse to produce a QUD structure thus entails a\ncomplex question generation task: given a document and an answer sentence,\ngenerate a question that satisfies linguistic constraints of QUD and can be\ngrounded in an anchor sentence in prior context. These questions are known to\nbe curiosity-driven and open-ended. This work introduces the first framework\nfor the automatic evaluation of QUD parsing, instantiating the theoretical\nconstraints of QUD in a concrete protocol. We present QUDeval, a dataset of\nfine-grained evaluation of 2,190 QUD questions generated from both fine-tuned\nsystems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD\nis still challenging for modern LLMs, and that existing evaluation metrics\npoorly approximate parser quality. Encouragingly, human-authored QUDs are\nscored highly by our human evaluators, suggesting that there is headroom for\nfurther progress on language modeling to improve both QUD parsing and QUD\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.14520v2.pdf"
    },
    {
        "title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains",
        "authors": [
            "Vaibhav Mavi",
            "Abulhair Saparov",
            "Chen Zhao"
        ],
        "published": "2023-10-22T22:45:14Z",
        "summary": "Applying existing question answering (QA) systems to specialized domains like\nlaw and finance presents challenges that necessitate domain expertise. Although\nlarge language models (LLMs) have shown impressive language comprehension and\nin-context learning capabilities, their inability to handle very long\ninputs/contexts is well known. Tasks specific to these domains need significant\nbackground knowledge, leading to contexts that can often exceed the maximum\nlength that existing LLMs can process. This study explores leveraging the\nsemi-structured nature of legal and financial data to efficiently retrieve\nrelevant context, enabling the use of LLMs for domain-specialized QA. The\nresulting system outperforms contemporary models and also provides useful\nexplanations for the answers, encouraging the integration of LLMs into legal\nand financial NLP systems for future research.",
        "pdf_link": "https://arxiv.org/pdf/2310.14435v1.pdf"
    },
    {
        "title": "Large Language Models are biased to overestimate profoundness",
        "authors": [
            "Eugenio Herrera-Berg",
            "Tom\u00e1s Vergara Browne",
            "Pablo Le\u00f3n-Villagr\u00e1",
            "Marc-Llu\u00eds Vives",
            "Cristian Buc Calderon"
        ],
        "published": "2023-10-22T21:33:50Z",
        "summary": "Recent advancements in natural language processing by large language models\n(LLMs), such as GPT-4, have been suggested to approach Artificial General\nIntelligence. And yet, it is still under dispute whether LLMs possess similar\nreasoning abilities to humans. This study evaluates GPT-4 and various other\nLLMs in judging the profoundness of mundane, motivational, and pseudo-profound\nstatements. We found a significant statement-to-statement correlation between\nthe LLMs and humans, irrespective of the type of statements and the prompting\ntechnique used. However, LLMs systematically overestimate the profoundness of\nnonsensical statements, with the exception of Tk-instruct, which uniquely\nunderestimates the profoundness of statements. Only few-shot learning prompts,\nas opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.\nFurthermore, this work provides insights into the potential biases induced by\nReinforcement Learning from Human Feedback (RLHF), inducing an increase in the\nbias to overestimate the profoundness of statements.",
        "pdf_link": "https://arxiv.org/pdf/2310.14422v1.pdf"
    },
    {
        "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
        "authors": [
            "Henry W. Sprueill",
            "Carl Edwards",
            "Mariefel V. Olarte",
            "Udishnu Sanyal",
            "Heng Ji",
            "Sutanay Choudhury"
        ],
        "published": "2023-10-22T21:29:33Z",
        "summary": "Discovering novel catalysts requires complex reasoning involving multiple\nchemical properties and resultant trade-offs, leading to a combinatorial growth\nin the search space. While large language models (LLM) have demonstrated novel\ncapabilities for chemistry through complex instruction following capabilities\nand high quality reasoning, a goal-driven combinatorial search using LLMs has\nnot been explored in detail. In this work, we present a Monte Carlo Tree\nSearch-based approach that improves beyond state-of-the-art chain-of-thought\nprompting variants to augment scientific reasoning. We introduce two new\nreasoning datasets: 1) a curation of computational chemistry simulations, and\n2) diverse questions written by catalysis researchers for reasoning about novel\nchemical conversion processes. We improve over the best baseline by 25.8\\% and\nfind that our approach can augment scientist's reasoning and discovery process\nwith novel insights.",
        "pdf_link": "https://arxiv.org/pdf/2310.14420v1.pdf"
    },
    {
        "title": "Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis",
        "authors": [
            "Inez Okulska",
            "Emilia Wi\u015bnios"
        ],
        "published": "2023-10-22T15:19:04Z",
        "summary": "Adult content detection still poses a great challenge for automation.\nExisting classifiers primarily focus on distinguishing between erotic and\nnon-erotic texts. However, they often need more nuance in assessing the\npotential harm. Unfortunately, the content of this nature falls beyond the\nreach of generative models due to its potentially harmful nature. Ethical\nrestrictions prohibit large language models (LLMs) from analyzing and\nclassifying harmful erotics, let alone generating them to create synthetic\ndatasets for other neural models. In such instances where data is scarce and\nchallenging, a thorough analysis of the structure of such texts rather than a\nlarge model may offer a viable solution. Especially given that harmful erotic\nnarratives, despite appearing similar to harmless ones, usually reveal their\nharmful nature first through contextual information hidden in the non-sexual\nparts of the narrative.\n  This paper introduces a hybrid neural and rule-based context-aware system\nthat leverages coreference resolution to identify harmful contextual cues in\nerotic content. Collaborating with professional moderators, we compiled a\ndataset and developed a classifier capable of distinguishing harmful from\nnon-harmful erotic content. Our hybrid model, tested on Polish text,\ndemonstrates a promising accuracy of 84% and a recall of 80%. Models based on\nRoBERTa and Longformer without explicit usage of coreference chains achieved\nsignificantly weaker results, underscoring the importance of coreference\nresolution in detecting such nuanced content as harmful erotics. This approach\nalso offers the potential for enhanced visual explainability, supporting\nmoderators in evaluating predictions and taking necessary actions to address\nharmful content.",
        "pdf_link": "https://arxiv.org/pdf/2310.14325v1.pdf"
    },
    {
        "title": "Chainpoll: A high efficacy method for LLM hallucination detection",
        "authors": [
            "Robert Friel",
            "Atindriyo Sanyal"
        ],
        "published": "2023-10-22T14:45:14Z",
        "summary": "Large language models (LLMs) have experienced notable advancements in\ngenerating coherent and contextually relevant responses. However,\nhallucinations - incorrect or unfounded claims - are still prevalent, prompting\nthe creation of automated metrics to detect these in LLM outputs. Our\ncontributions include: introducing ChainPoll, an innovative hallucination\ndetection method that excels compared to its counterparts, and unveiling\nRealHall, a refined collection of benchmark datasets to assess hallucination\ndetection metrics from recent studies. While creating RealHall, we assessed\ntasks and datasets from previous hallucination detection studies and observed\nthat many are not suitable for the potent LLMs currently in use. Overcoming\nthis, we opted for four datasets challenging for modern LLMs and pertinent to\nreal-world scenarios. Using RealHall, we conducted a comprehensive comparison\nof ChainPoll with numerous hallucination metrics from recent studies. Our\nfindings indicate that ChainPoll outperforms in all RealHall benchmarks,\nachieving an overall AUROC of 0.781. This surpasses the next best theoretical\nmethod by 11% and exceeds industry standards by over 23%. Additionally,\nChainPoll is cost-effective and offers greater transparency than other metrics.\nWe introduce two novel metrics to assess LLM hallucinations: Adherence and\nCorrectness. Adherence is relevant to Retrieval Augmented Generation workflows,\nevaluating an LLM's analytical capabilities within given documents and\ncontexts. In contrast, Correctness identifies logical and reasoning errors.",
        "pdf_link": "https://arxiv.org/pdf/2310.18344v1.pdf"
    },
    {
        "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases",
        "authors": [
            "Rishabh Bhardwaj",
            "Soujanya Poria"
        ],
        "published": "2023-10-22T13:55:46Z",
        "summary": "Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.",
        "pdf_link": "https://arxiv.org/pdf/2310.14303v2.pdf"
    },
    {
        "title": "NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval",
        "authors": [
            "Uri Katz",
            "Matan Vetzler",
            "Amir DN Cohen",
            "Yoav Goldberg"
        ],
        "published": "2023-10-22T12:23:00Z",
        "summary": "Recognizing entities in texts is a central need in many information-seeking\nscenarios, and indeed, Named Entity Recognition (NER) is arguably one of the\nmost successful examples of a widely adopted NLP task and corresponding NLP\ntechnology. Recent advances in large language models (LLMs) appear to provide\neffective solutions (also) for NER tasks that were traditionally handled with\ndedicated models, often matching or surpassing the abilities of the dedicated\nmodels. Should NER be considered a solved problem? We argue to the contrary:\nthe capabilities provided by LLMs are not the end of NER research, but rather\nan exciting beginning. They allow taking NER to the next level, tackling\nincreasingly more useful, and increasingly more challenging, variants. We\npresent three variants of the NER task, together with a dataset to support\nthem. The first is a move towards more fine-grained -- and intersectional --\nentity types. The second is a move towards zero-shot recognition and extraction\nof these fine-grained types based on entity-type labels. The third, and most\nchallenging, is the move from the recognition setup to a novel retrieval setup,\nwhere the query is a zero-shot entity type, and the expected result is all the\nsentences from a large, pre-indexed corpus that contain entities of these\ntypes, and their corresponding spans. We show that all of these are far from\nbeing solved. We provide a large, silver-annotated corpus of 4 million\nparagraphs covering 500 entity types, to facilitate research towards all of\nthese three goals.",
        "pdf_link": "https://arxiv.org/pdf/2310.14282v1.pdf"
    },
    {
        "title": "From Static to Dynamic: A Continual Learning Framework for Large Language Models",
        "authors": [
            "Mingzhe Du",
            "Anh Tuan Luu",
            "Bin Ji",
            "See-kiong Ng"
        ],
        "published": "2023-10-22T10:18:53Z",
        "summary": "The vast number of parameters in large language models (LLMs) endows them\nwith remarkable capabilities, allowing them to excel in a variety of natural\nlanguage processing tasks. However, this complexity also presents challenges,\nmaking LLMs difficult to train and inhibiting their ability to continuously\nassimilate new knowledge, which may lead to inaccuracies in their outputs. To\nmitigate these issues, this paper presents DynaMind, a novel continual learning\nframework designed for LLMs. DynaMind incorporates memory mechanisms to\nassimilate new knowledge and modular operators to enhance the model inference\nprocess with the newly assimilated knowledge, consequently improving the\naccuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's\neffectiveness in overcoming these challenges. The code and demo of DynaMind are\navailable on GitHub: https://github.com/Elfsong/DynaMind.",
        "pdf_link": "https://arxiv.org/pdf/2310.14248v1.pdf"
    },
    {
        "title": "CXR-LLAVA: a multimodal large language model for interpreting chest X-ray images",
        "authors": [
            "Seowoo Lee",
            "Jiwon Youn",
            "Hyungjin Kim",
            "Mansu Kim",
            "Soon Ho Yoon"
        ],
        "published": "2023-10-22T06:22:37Z",
        "summary": "Purpose: This study aimed to develop an open-source multimodal large language\nmodel (CXR-LLAVA) for interpreting chest X-ray images (CXRs), leveraging recent\nadvances in large language models (LLMs) to potentially replicate the image\ninterpretation skills of human radiologists Materials and Methods: For\ntraining, we collected 592,580 publicly available CXRs, of which 374,881 had\nlabels for certain radiographic abnormalities (Dataset 1) and 217,699 provided\nfree-text radiology reports (Dataset 2). After pre-training a vision\ntransformer with Dataset 1, we integrated it with an LLM influenced by the\nLLAVA network. Then, the model was fine-tuned, primarily using Dataset 2. The\nmodel's diagnostic performance for major pathological findings was evaluated,\nalong with the acceptability of radiologic reports by human radiologists, to\ngauge its potential for autonomous reporting. Results: The model demonstrated\nimpressive performance in test sets, achieving an average F1 score of 0.81 for\nsix major pathological findings in the MIMIC internal test set and 0.62 for\nseven major pathological findings in the external test set. The model's F1\nscores surpassed those of GPT-4-vision and Gemini-Pro-Vision in both test sets.\nIn human radiologist evaluations of the external test set, the model achieved a\n72.7% success rate in autonomous reporting, slightly below the 84.0% rate of\nground truth reports. Conclusion: This study highlights the significant\npotential of multimodal LLMs for CXR interpretation, while also acknowledging\nthe performance limitations. Despite these challenges, we believe that making\nour model open-source will catalyze further research, expanding its\neffectiveness and applicability in various clinical contexts. CXR-LLAVA is\navailable at https://github.com/ECOFRI/CXR_LLAVA.",
        "pdf_link": "https://arxiv.org/pdf/2310.18341v3.pdf"
    },
    {
        "title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain",
        "authors": [
            "Wei Zhu",
            "Xiaoling Wang",
            "Huanran Zheng",
            "Mosha Chen",
            "Buzhou Tang"
        ],
        "published": "2023-10-22T02:20:38Z",
        "summary": "Biomedical language understanding benchmarks are the driving forces for\nartificial intelligence applications with large language model (LLM) back-ends.\nHowever, most current benchmarks: (a) are limited to English which makes it\nchallenging to replicate many of the successes in English for other languages,\nor (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs\napply these knowledge to perform on a wide range of bio-medical tasks, or (c)\nhave become a publicly available corpus and are leaked to LLMs during\npre-training. To facilitate the research in medical LLMs, we re-build the\nChinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a\nlarge scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable\ntest-bed and an online platform for evaluating Chinese LLMs' multi-task\ncapabilities on a wide range bio-medical tasks including medical entity\nrecognition, medical text classification, medical natural language inference,\nmedical dialogue understanding and medical content/dialogue generation. To\nestablish evaluation on these tasks, we have experimented and report the\nresults with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2310.14151v1.pdf"
    },
    {
        "title": "Learning Reward for Physical Skills using Large Language Model",
        "authors": [
            "Yuwei Zeng",
            "Yiqing Xu"
        ],
        "published": "2023-10-21T19:10:06Z",
        "summary": "Learning reward functions for physical skills are challenging due to the vast\nspectrum of skills, the high-dimensionality of state and action space, and\nnuanced sensory feedback. The complexity of these tasks makes acquiring expert\ndemonstration data both costly and time-consuming. Large Language Models (LLMs)\ncontain valuable task-related knowledge that can aid in learning these reward\nfunctions. However, the direct application of LLMs for proposing reward\nfunctions has its limitations such as numerical instability and inability to\nincorporate the environment feedback. We aim to extract task knowledge from\nLLMs using environment feedback to create efficient reward functions for\nphysical skills. Our approach consists of two components. We first use the LLM\nto propose features and parameterization of the reward function. Next, we\nupdate the parameters of this proposed reward function through an iterative\nself-alignment process. In particular, this process minimizes the ranking\ninconsistency between the LLM and our learned reward functions based on the new\nobservations. We validated our method by testing it on three simulated physical\nskill learning tasks, demonstrating effective support for our design choices.",
        "pdf_link": "https://arxiv.org/pdf/2310.14092v1.pdf"
    },
    {
        "title": "MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications",
        "authors": [
            "Qidong Liu",
            "Xian Wu",
            "Xiangyu Zhao",
            "Yuanshao Zhu",
            "Derong Xu",
            "Feng Tian",
            "Yefeng Zheng"
        ],
        "published": "2023-10-21T17:18:09Z",
        "summary": "The recent surge in the field of Large Language Models (LLMs) has gained\nsignificant attention in numerous domains. In order to tailor an LLM to a\nspecific domain such as a web-based healthcare system, fine-tuning with domain\nknowledge is necessary. However, two issues arise during fine-tuning LLMs for\nmedical applications. The first is the problem of task variety, where there are\nnumerous distinct tasks in real-world medical scenarios. This diversity often\nresults in suboptimal fine-tuning due to data imbalance and seesawing problems.\nAdditionally, the high cost of fine-tuning can be prohibitive, impeding the\napplication of LLMs. The large number of parameters in LLMs results in enormous\ntime and computational consumption during fine-tuning, which is difficult to\njustify. To address these two issues simultaneously, we propose a novel\nparameter-efficient fine-tuning framework for multi-task medical applications\ncalled MOELoRA. The framework aims to capitalize on the benefits of both MOE\nfor multi-task learning and LoRA for parameter-efficient fine-tuning. To unify\nMOE and LoRA, we devise multiple experts as the trainable parameters, where\neach expert consists of a pair of low-rank matrices to maintain a small number\nof trainable parameters. Additionally, we propose a task-motivated gate\nfunction for all MOELoRA layers that can regulate the contributions of each\nexpert and generate distinct parameters for various tasks. To validate the\neffectiveness and practicality of the proposed method, we conducted\ncomprehensive experiments on a public multi-task Chinese medical dataset. The\nexperimental results demonstrate that MOELoRA outperforms existing\nparameter-efficient fine-tuning methods. The implementation is available online\nfor convenient reproduction of our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2310.18339v1.pdf"
    },
    {
        "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
        "authors": [
            "Marcus J. Min",
            "Yangruibo Ding",
            "Luca Buratti",
            "Saurabh Pujar",
            "Gail Kaiser",
            "Suman Jana",
            "Baishakhi Ray"
        ],
        "published": "2023-10-21T16:14:56Z",
        "summary": "Code Large Language Models (Code LLMs) are being increasingly employed in\nreal-life applications, so evaluating them is critical. While the conventional\naccuracy evaluates the performance of Code LLMs on a set of individual tasks,\ntheir self-consistency across different tasks is overlooked. Intuitively, a\ntrustworthy model should be self-consistent when generating natural language\nspecifications for its own code and generating code for its own specifications.\nFailure to preserve self-consistency reveals a lack of understanding of the\nshared semantics underlying natural language and programming language, and\ntherefore undermines the trustworthiness of a model. In this paper, we first\nformally define the self-consistency of Code LLMs and then design a framework,\nIdentityChain, which effectively and efficiently evaluates the self-consistency\nand conventional accuracy of a model at the same time. We study eleven Code\nLLMs and show that they fail to preserve self-consistency, which is indeed a\ndistinct aspect from conventional accuracy. Furthermore, we show that\nIdentityChain can be used as a model debugging tool to expose weaknesses of\nCode LLMs by demonstrating three major weaknesses that we identify in current\nmodels using IdentityChain. Our code is available at\nhttps://github.com/marcusm117/IdentityChain.",
        "pdf_link": "https://arxiv.org/pdf/2310.14053v3.pdf"
    },
    {
        "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions",
        "authors": [
            "Andre Niyongabo Rubungo",
            "Craig Arnold",
            "Barry P. Rand",
            "Adji Bousso Dieng"
        ],
        "published": "2023-10-21T14:49:58Z",
        "summary": "The prediction of crystal properties plays a crucial role in the crystal\ndesign process. Current methods for predicting crystal properties focus on\nmodeling crystal structures using graph neural networks (GNNs). Although GNNs\nare powerful, accurately modeling the complex interactions between atoms and\nmolecules within a crystal remains a challenge. Surprisingly, predicting\ncrystal properties from crystal text descriptions is understudied, despite the\nrich information and expressiveness that text data offer. One of the main\nreasons is the lack of publicly available data for this task. In this paper, we\ndevelop and make public a benchmark dataset (called TextEdge) that contains\ntext descriptions of crystal structures with their properties. We then propose\nLLM-Prop, a method that leverages the general-purpose learning capabilities of\nlarge language models (LLMs) to predict the physical and electronic properties\nof crystals from their text descriptions. LLM-Prop outperforms the current\nstate-of-the-art GNN-based crystal property predictor by about 4% in predicting\nband gap, 3% in classifying whether the band gap is direct or indirect, and 66%\nin predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,\na domain-specific pre-trained BERT model, despite having 3 times fewer\nparameters. Our empirical results may highlight the current inability of GNNs\nto capture information pertaining to space group symmetry and Wyckoff sites for\naccurate crystal property prediction.",
        "pdf_link": "https://arxiv.org/pdf/2310.14029v1.pdf"
    },
    {
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": [
            "Yaoyiran Li",
            "Anna Korhonen",
            "Ivan Vuli\u0107"
        ],
        "published": "2023-10-21T12:43:27Z",
        "summary": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that\nstill, to a large extent, relies on calculating cross-lingual word\nrepresentations. Inspired by the global paradigm shift in NLP towards Large\nLanguage Models (LLMs), we examine the potential of the latest generation of\nLLMs for the development of bilingual lexicons. We ask the following research\nquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for\nBLI, and how does this approach compare against and complement current BLI\napproaches? To this end, we systematically study 1) zero-shot prompting for\nunsupervised BLI and 2) few-shot in-context prompting with a set of seed\ntranslation pairs, both without any LLM fine-tuning, as well as 3) standard\nBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source\ntext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two\nstandard BLI benchmarks covering a range of typologically diverse languages.\nOur work is the first to demonstrate strong BLI capabilities of text-to-text\nmLLMs. The results reveal that few-shot prompting with in-context examples from\nnearest neighbours achieves the best performance, establishing new\nstate-of-the-art BLI scores for many language pairs. We also conduct a series\nof in-depth analyses and ablation studies, providing more insights on BLI with\n(m)LLMs, also along with their limitations.",
        "pdf_link": "https://arxiv.org/pdf/2310.13995v2.pdf"
    },
    {
        "title": "GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4",
        "authors": [
            "Tom Kocmi",
            "Christian Federmann"
        ],
        "published": "2023-10-21T12:30:33Z",
        "summary": "This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to\ndetect translation quality errors, specifically for the quality estimation\nsetting without the need for human reference translations. Based on the power\nof large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting\ntechnique, querying the GPT-4 model to mark error quality spans. Compared to\nprevious works, our method has language-agnostic prompts, thus avoiding the\nneed for manual prompt preparation for new languages.\n  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art\naccuracy for system ranking, we advise caution when using it in academic works\nto demonstrate improvements over other methods due to its dependence on the\nproprietary, black-box GPT model.",
        "pdf_link": "https://arxiv.org/pdf/2310.13988v1.pdf"
    },
    {
        "title": "HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models",
        "authors": [
            "Vibhor Agarwal",
            "Yu Chen",
            "Nishanth Sastry"
        ],
        "published": "2023-10-21T12:18:29Z",
        "summary": "Hate speech has become pervasive in today's digital age. Although there has\nbeen considerable research to detect hate speech or generate counter speech to\ncombat hateful views, these approaches still cannot completely eliminate the\npotential harmful societal consequences of hate speech -- hate speech, even\nwhen detected, can often not be taken down or is often not taken down enough;\nand hate speech unfortunately spreads quickly, often much faster than any\ngenerated counter speech.\n  This paper investigates a relatively new yet simple and effective approach of\nsuggesting a rephrasing of potential hate speech content even before the post\nis made. We show that Large Language Models (LLMs) perform well on this task,\noutperforming state-of-the-art baselines such as BART-Detox. We develop 4\ndifferent prompts based on task description, hate definition, few-shot\ndemonstrations and chain-of-thoughts for comprehensive experiments and conduct\nexperiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well\nas OpenAI's GPT-3.5. We propose various evaluation metrics to measure the\nefficacy of the generated text and ensure the generated text has reduced hate\nintensity without drastically changing the semantic meaning of the original\ntext.\n  We find that LLMs with a few-shot demonstrations prompt work the best in\ngenerating acceptable hate-rephrased text with semantic meaning similar to the\noriginal text. Overall, we find that GPT-3.5 outperforms the baseline and\nopen-source models for all the different kinds of prompts. We also perform\nhuman evaluations and interestingly, find that the rephrasings generated by\nGPT-3.5 outperform even the human-generated ground-truth rephrasings in the\ndataset. We also conduct detailed ablation studies to investigate why LLMs work\nsatisfactorily on this task and conduct a failure analysis to understand the\ngaps.",
        "pdf_link": "https://arxiv.org/pdf/2310.13985v1.pdf"
    },
    {
        "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs",
        "authors": [
            "Young-Suk Lee",
            "Md Arafat Sultan",
            "Yousef El-Kurdi",
            "Tahira Naseem Asim Munawar",
            "Radu Florian",
            "Salim Roukos",
            "Ram\u00f3n Fernandez Astudillo"
        ],
        "published": "2023-10-21T10:21:17Z",
        "summary": "Using in-context learning (ICL) for data generation, techniques such as\nSelf-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)\ncan train strong conversational agents with only a small amount of human\nsupervision. One limitation of these approaches is that they resort to very\nlarge language models (around 175B parameters) that are also proprietary and\nnon-public. Here we explore the application of such techniques to language\nmodels that are much smaller (around 10B--40B parameters) and have permissive\nlicenses. We find the Self-Instruct approach to be less effective at these\nsizes and propose new ICL methods that draw on two main ideas: (a)\nCategorization and simplification of the ICL templates to make prompt learning\neasier for the LM, and (b) Ensembling over multiple LM outputs to help select\nhigh-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct\nseed tasks and employs separate pipelines for instructions that require an\ninput and instructions that do not. Empirical investigations with different LMs\nshow that: (1) Our proposed method yields higher-quality instruction tuning\ndata than Self-Instruct, (2) It improves performances of both vanilla and\ninstruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned\nLMs generate more useful outputs than their larger un-tuned counterparts. Our\ncodebase is available at https://github.com/IBM/ensemble-instruct.",
        "pdf_link": "https://arxiv.org/pdf/2310.13961v1.pdf"
    },
    {
        "title": "Copyright Violations and Large Language Models",
        "authors": [
            "Antonia Karamolegkou",
            "Jiaang Li",
            "Li Zhou",
            "Anders S\u00f8gaard"
        ],
        "published": "2023-10-20T19:14:59Z",
        "summary": "Language models may memorize more than just facts, including entire chunks of\ntexts seen during training. Fair use exemptions to copyright laws typically\nallow for limited use of copyrighted material without permission from the\ncopyright holder, but typically for extraction of information from copyrighted\nmaterials, rather than {\\em verbatim} reproduction. This work explores the\nissue of copyright violations and large language models through the lens of\nverbatim memorization, focusing on possible redistribution of copyrighted text.\nWe present experiments with a range of language models over a collection of\npopular books and coding problems, providing a conservative characterization of\nthe extent to which language models can redistribute these materials. Overall,\nthis research highlights the need for further examination and the potential\nimpact on future developments in natural language processing to ensure\nadherence to copyright regulations. Code is at\n\\url{https://github.com/coastalcph/CopyrightLLMs}.",
        "pdf_link": "https://arxiv.org/pdf/2310.13771v1.pdf"
    },
    {
        "title": "Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models",
        "authors": [
            "Arya D. McCarthy",
            "Hao Zhang",
            "Shankar Kumar",
            "Felix Stahlberg",
            "Ke Wu"
        ],
        "published": "2023-10-20T17:31:39Z",
        "summary": "One challenge in speech translation is that plenty of spoken content is\nlong-form, but short units are necessary for obtaining high-quality\ntranslations. To address this mismatch, we adapt large language models (LLMs)\nto split long ASR transcripts into segments that can be independently\ntranslated so as to maximize the overall translation quality. We overcome the\ntendency of hallucination in LLMs by incorporating finite-state constraints\nduring decoding; these eliminate invalid outputs without requiring additional\ntraining. We discover that LLMs are adaptable to transcripts containing ASR\nerrors through prompt-tuning or fine-tuning. Relative to a state-of-the-art\nautomatic punctuation baseline, our best LLM improves the average BLEU by 2.9\npoints for English-German, English-Spanish, and English-Arabic TED talk\ntranslation in 9 test sets, just by improving segmentation.",
        "pdf_link": "https://arxiv.org/pdf/2310.13678v2.pdf"
    },
    {
        "title": "Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models",
        "authors": [
            "Ruida Wang",
            "Wangchunshu Zhou",
            "Mrinmaya Sachan"
        ],
        "published": "2023-10-20T17:14:25Z",
        "summary": "*Data Synthesis* is a promising way to train a small model with very little\nlabeled data. One approach for data synthesis is to leverage the rich knowledge\nfrom large language models to synthesize pseudo training examples for small\nmodels, making it possible to achieve both data and compute efficiency at the\nsame time. However, a key challenge in data synthesis is that the synthesized\ndataset often suffers from a large distributional discrepancy from the *real\ntask* data distribution. Thus, in this paper, we propose *Synthesis Step by\nStep* (**S3**), a data synthesis framework that shrinks this distribution gap\nby iteratively extrapolating the errors made by a small model trained on the\nsynthesized dataset on a small real-world validation dataset using a large\nlanguage model. Extensive experiments on multiple NLP tasks show that our\napproach improves the performance of a small model by reducing the gap between\nthe synthetic dataset and the real data, resulting in significant improvement\ncompared to several baselines: 9.48% improvement compared to ZeroGen and 2.73%\ncompared to GoldGen, and at most 15.17% improvement compared to the small model\ntrained on human-annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2310.13671v1.pdf"
    },
    {
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "authors": [
            "Adithya Bhaskar",
            "Tushar Tomar",
            "Ashutosh Sathe",
            "Sunita Sarawagi"
        ],
        "published": "2023-10-20T17:00:53Z",
        "summary": "Research in Text-to-SQL conversion has been largely benchmarked against\ndatasets where each text query corresponds to one correct SQL. However, natural\nlanguage queries over real-life databases frequently involve significant\nambiguity about the intended SQL due to overlapping schema names and multiple\nconfusing relationship paths. To bridge this gap, we develop a novel benchmark\ncalled AmbiQT with over 3000 examples where each text is interpretable as two\nplausible SQLs due to lexical and/or structural ambiguity.\n  When faced with ambiguity, an ideal top-$k$ decoder should generate all valid\ninterpretations for possible disambiguation by the user. We evaluate several\nText-to-SQL systems and decoding algorithms, including those employing\nstate-of-the-art LLMs, and find them to be far from this ideal. The primary\nreason is that the prevalent beam search algorithm and its variants, treat SQL\nqueries as a string and produce unhelpful token-level diversity in the top-$k$.\n  We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic\nspace using a blend of plan-based template generation and constrained\ninfilling. Counterfactually generated plans diversify templates while\nin-filling with a beam-search that branches solely on schema names provides\nvalue diversity. LogicalBeam is up to $2.5$ times more effective than\nstate-of-the-art models at generating all candidate SQLs in the top-$k$ ranked\noutputs. It also enhances the top-$5$ Exact and Execution Match Accuracies on\nSPIDER and Kaggle DBQA.",
        "pdf_link": "https://arxiv.org/pdf/2310.13659v1.pdf"
    },
    {
        "title": "BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues",
        "authors": [
            "Haodong Duan",
            "Jueqi Wei",
            "Chonghua Wang",
            "Hongwei Liu",
            "Yixiao Fang",
            "Songyang Zhang",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published": "2023-10-20T16:53:51Z",
        "summary": "Interacting with human via high-quality multi-turn dialogues is a key feature\nof large language models (LLMs). However, human-based evaluation of such\ncapability involves intensive manual labor. This report provides a preliminary\nevaluation of existing large language models for human-style multi-turn\nchatting, through an LLM-based approach. We start from real-world human\ndialogues and keep the very first utterances as the ChatSEED. Then we prompt\nLLMs to generate a full multi-turn dialogue (tens of utterances) based on the\nChatSEED, utterance by utterance. Finally, we adopt state-of-the-art LLMs\n(GPT-4, \\etc) as the judge to evaluate the generated dialogues. With different\nevaluation protocols, we come to substantially identical conclusions. We find\nthat GPT-4 can generate human-style multi-turn dialogues with impressive\nquality, significantly outperforms its counterparts. It's difficult for a\ndiscriminator to distinguish between GPT-4 generated dialogues and human\ndialogues. In contrast, other LLMs struggle to generate multi-turn dialogues of\nsatisfactory quality due to poor instruction-following capability, tendency to\ngenerate lengthy utterances, or limited general capability. All data and codes\nwill be provided in https://github.com/open-compass/BotChat/ and we hope they\ncan serve as a valuable resource for evaluating multi-turn chatting\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.13650v1.pdf"
    },
    {
        "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
        "authors": [
            "An-Zi Yen",
            "Wei-Ling Hsu"
        ],
        "published": "2023-10-20T16:05:35Z",
        "summary": "Due to the remarkable language understanding and generation abilities of\nlarge language models (LLMs), their use in educational applications has been\nexplored. However, little work has been done on investigating the pedagogical\nability of LLMs in helping students to learn mathematics. In this position\npaper, we discuss the challenges associated with employing LLMs to enhance\nstudents' mathematical problem-solving skills by providing adaptive feedback.\nApart from generating the wrong reasoning processes, LLMs can misinterpret the\nmeaning of the question, and also exhibit difficulty in understanding the given\nquestions' rationales when attempting to correct students' answers. Three\nresearch questions are formulated.",
        "pdf_link": "https://arxiv.org/pdf/2310.13615v1.pdf"
    },
    {
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "authors": [
            "Alexandros Xenos",
            "Themos Stafylakis",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ],
        "published": "2023-10-20T15:08:17Z",
        "summary": "This paper is on the problem of Knowledge-Based Visual Question Answering\n(KB-VQA). Recent works have emphasized the significance of incorporating both\nexplicit (through external databases) and implicit (through LLMs) knowledge to\nanswer questions requiring external knowledge effectively. A common limitation\nof such approaches is that they consist of relatively complicated pipelines and\noften heavily rely on accessing GPT-3 API. Our main contribution in this paper\nis to propose a much simpler and readily reproducible pipeline which, in a\nnutshell, is based on efficient in-context learning by prompting LLaMA (1 and\n2) using question-informative captions as contextual information. Contrary to\nrecent approaches, our method is training-free, does not require access to\nexternal databases or APIs, and yet achieves state-of-the-art accuracy on the\nOK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to\nunderstand important aspects of our method. Our code is publicly available at\nhttps://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA",
        "pdf_link": "https://arxiv.org/pdf/2310.13570v2.pdf"
    },
    {
        "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
        "authors": [
            "Jinyuan Wang",
            "Junlong Li",
            "Hai Zhao"
        ],
        "published": "2023-10-20T14:51:10Z",
        "summary": "In open-domain question-answering (ODQA), most existing questions require\nsingle-hop reasoning on commonsense. To further extend this task, we officially\nintroduce open-domain multi-hop reasoning (ODMR) by answering multi-hop\nquestions with explicit reasoning steps in open-domain setting. Recently, large\nlanguage models (LLMs) have found significant utility in facilitating ODQA\nwithout external corpus. Furthermore, chain-of-thought (CoT) prompting boosts\nthe reasoning capability of LLMs to a greater extent with manual or automated\nparadigms. However, existing automated methods lack of quality assurance, while\nmanual approaches suffer from limited scalability and poor diversity, hindering\nthe capabilities of LLMs. In this paper, we propose Self-prompted\nChain-of-Thought (SP-CoT), an automated framework to mass-produce high quality\nCoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation\npipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT\nselection and self-prompted inference via in-context learning. Extensive\nexperiments on four multi-hop question-answering benchmarks show that our\nproposed SP-CoT not only significantly surpasses the previous SOTA methods on\nlarge-scale (175B) LLMs, but also nearly doubles the zero-shot performance of\nsmall-scale (13B) LLMs. Further analysis reveals the remarkable capability of\nSP-CoT to elicit direct and concise intermediate reasoning steps by recalling\n$\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.13552v2.pdf"
    },
    {
        "title": "The Perils & Promises of Fact-checking with Large Language Models",
        "authors": [
            "Dorian Quelle",
            "Alexandre Bovet"
        ],
        "published": "2023-10-20T14:49:47Z",
        "summary": "Automated fact-checking, using machine learning to verify claims, has grown\nvital as misinformation spreads beyond human fact-checking capacity. Large\nLanguage Models (LLMs) like GPT-4 are increasingly trusted to write academic\npapers, lawsuits, and news articles and to verify information, emphasizing\ntheir role in discerning truth from falsehood and the importance of being able\nto verify their outputs. Understanding the capacities and limitations of LLMs\nin fact-checking tasks is therefore essential for ensuring the health of our\ninformation ecosystem. Here, we evaluate the use of LLM agents in fact-checking\nby having them phrase queries, retrieve contextual data, and make decisions.\nImportantly, in our framework, agents explain their reasoning and cite the\nrelevant sources from the retrieved context. Our results show the enhanced\nprowess of LLMs when equipped with contextual information. GPT-4 outperforms\nGPT-3, but accuracy varies based on query language and claim veracity. While\nLLMs show promise in fact-checking, caution is essential due to inconsistent\naccuracy. Our investigation calls for further research, fostering a deeper\ncomprehension of when agents succeed and when they fail.",
        "pdf_link": "https://arxiv.org/pdf/2310.13549v2.pdf"
    },
    {
        "title": "She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models",
        "authors": [
            "Veronica Chatrath",
            "Oluwanifemi Bamgbose",
            "Shaina Raza"
        ],
        "published": "2023-10-20T14:18:40Z",
        "summary": "As the use of large language models (LLMs) increases within society, as does\nthe risk of their misuse. Appropriate safeguards must be in place to ensure LLM\noutputs uphold the ethical standards of society, highlighting the positive role\nthat artificial intelligence technologies can have. Recent events indicate\nethical concerns around conventionally trained LLMs, leading to overall unsafe\nuser experiences. This motivates our research question: how do we ensure LLM\nalignment? In this work, we introduce a test suite of unique prompts to foster\nthe development of aligned LLMs that are fair, safe, and robust. We show that\nprompting LLMs at every step of the development pipeline, including data\ncuration, pre-training, and fine-tuning, will result in an overall more\nresponsible model. Our test suite evaluates outputs from four state-of-the-art\nlanguage models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in\nthis paper highlights a gap between societal alignment and the capabilities of\ncurrent LLMs. Additionally, implementing a test suite such as ours lowers the\nenvironmental overhead of making models safe and fair.",
        "pdf_link": "https://arxiv.org/pdf/2310.18333v3.pdf"
    },
    {
        "title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
        "authors": [
            "Xiao Yu",
            "Baolin Peng",
            "Michel Galley",
            "Jianfeng Gao",
            "Zhou Yu"
        ],
        "published": "2023-10-20T14:11:04Z",
        "summary": "The self-improving ability of large language models (LLMs), enabled by\nprompting them to analyze and revise their own outputs, has garnered\nsignificant interest in recent research. However, this ability has been shown\nto be absent and difficult to learn for smaller models, thus widening the\nperformance gap between state-of-the-art LLMs and more cost-effective and\nfaster ones. To reduce this gap, we introduce TriPosT, a training algorithm\nthat endows smaller models with such self-improvement ability, and show that\nour approach can improve a LLaMA-7b's performance on math and reasoning tasks\nby up to 7.13%. In contrast to prior work, we achieve this by using the smaller\nmodel to interact with LLMs to collect feedback and improvements on its own\ngenerations. We then replay this experience to train the small model. Our\nexperiments on four math and reasoning datasets show that the interactive\nexperience of learning from and correcting its own mistakes is crucial for\nsmall models to improve their performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.13522v2.pdf"
    },
    {
        "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
        "authors": [
            "Duarte M. Alves",
            "Nuno M. Guerreiro",
            "Jo\u00e3o Alves",
            "Jos\u00e9 Pombal",
            "Ricardo Rei",
            "Jos\u00e9 G. C. de Souza",
            "Pierre Colombo",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2023-10-20T12:29:51Z",
        "summary": "Large language models (LLMs) are a promising avenue for machine translation\n(MT). However, current LLM-based MT systems are brittle: their effectiveness\nhighly depends on the choice of few-shot examples and they often require extra\npost-processing due to overgeneration. Alternatives such as finetuning on\ntranslation instructions are computationally expensive and may weaken\nin-context learning capabilities, due to overspecialization. In this paper, we\nprovide a closer look at this problem. We start by showing that adapter-based\nfinetuning with LoRA matches the performance of traditional finetuning while\nreducing the number of training parameters by a factor of 50. This method also\noutperforms few-shot prompting and eliminates the need for post-processing or\nin-context examples. However, we show that finetuning generally degrades\nfew-shot performance, hindering adaptation capabilities. Finally, to obtain the\nbest of both worlds, we propose a simple approach that incorporates few-shot\nexamples during finetuning. Experiments on 10 language pairs show that our\nproposed approach recovers the original few-shot capabilities while keeping the\nadded benefits of finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.13448v1.pdf"
    },
    {
        "title": "Self-Consistency of Large Language Models under Ambiguity",
        "authors": [
            "Henning Bartsch",
            "Ole Jorgensen",
            "Domenic Rosati",
            "Jason Hoelscher-Obermaier",
            "Jacob Pfau"
        ],
        "published": "2023-10-20T11:57:56Z",
        "summary": "Large language models (LLMs) that do not give consistent answers across\ncontexts are problematic when used for tasks with expectations of consistency,\ne.g., question-answering, explanations, etc. Our work presents an evaluation\nbenchmark for self-consistency in cases of under-specification where two or\nmore answers can be correct. We conduct a series of behavioral experiments on\nthe OpenAI model suite using an ambiguous integer sequence completion task. We\nfind that average consistency ranges from 67\\% to 82\\%, far higher than would\nbe predicted if a model's consistency was random, and increases as model\ncapability improves. Furthermore, we show that models tend to maintain\nself-consistency across a series of robustness checks, including prompting\nspeaker changes and sequence length changes. These results suggest that\nself-consistency arises as an emergent capability without specifically training\nfor it. Despite this, we find that models are uncalibrated when judging their\nown consistency, with models displaying both over- and under-confidence. We\nalso propose a nonparametric test for determining from token output\ndistribution whether a model assigns non-trivial probability to alternative\nanswers. Using this test, we find that despite increases in self-consistency,\nmodels usually place significant weight on alternative, inconsistent answers.\nThis distribution of probability mass provides evidence that even highly\nself-consistent models internally compute multiple possible responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.13439v1.pdf"
    },
    {
        "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
        "authors": [
            "Chang Shu",
            "Jiuzhou Han",
            "Fangyu Liu",
            "Ehsan Shareghi",
            "Nigel Collier"
        ],
        "published": "2023-10-20T10:05:01Z",
        "summary": "Embodied language comprehension emphasizes that language understanding is not\nsolely a matter of mental processing in the brain but also involves\ninteractions with the physical and social environment. With the explosive\ngrowth of Large Language Models (LLMs) and their already ubiquitous presence in\nour daily lives, it is becoming increasingly necessary to verify their\nreal-world understanding. Inspired by cognitive theories, we propose POSQA: a\nPhysical Object Size Question Answering dataset with simple size comparison\nquestions to examine the extremity and analyze the potential mechanisms of the\nembodied comprehension of the latest LLMs.\n  We show that even the largest LLMs today perform poorly under the zero-shot\nsetting. We then push their limits with advanced prompting techniques and\nexternal knowledge augmentation. Furthermore, we investigate whether their\nreal-world comprehension primarily derives from contextual information or\ninternal weights and analyse the impact of prompt formats and report bias of\ndifferent objects. Our results show that real-world understanding that LLMs\nshaped from textual data can be vulnerable to deception and confusion by the\nsurface form of prompts, which makes it less aligned with human behaviours.",
        "pdf_link": "https://arxiv.org/pdf/2310.13394v1.pdf"
    },
    {
        "title": "Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)",
        "authors": [
            "Xiaoliang Chen",
            "Liangbin Li",
            "Le Chang",
            "Yunhe Huang",
            "Yuxuan Zhao",
            "Yuxiao Zhang",
            "Dinuo Li"
        ],
        "published": "2023-10-20T08:13:36Z",
        "summary": "With the development of large language models (LLMs) like the GPT series,\ntheir widespread use across various application scenarios presents a myriad of\nchallenges. This review initially explores the issue of domain specificity,\nwhere LLMs may struggle to provide precise answers to specialized questions\nwithin niche fields. The problem of knowledge forgetting arises as these LLMs\nmight find it hard to balance old and new information. The knowledge repetition\nphenomenon reveals that sometimes LLMs might deliver overly mechanized\nresponses, lacking depth and originality. Furthermore, knowledge illusion\ndescribes situations where LLMs might provide answers that seem insightful but\nare actually superficial, while knowledge toxicity focuses on harmful or biased\ninformation outputs. These challenges underscore problems in the training data\nand algorithmic design of LLMs. To address these issues, it's suggested to\ndiversify training data, fine-tune models, enhance transparency and\ninterpretability, and incorporate ethics and fairness training. Future\ntechnological trends might lean towards iterative methodologies, multimodal\nlearning, model personalization and customization, and real-time learning and\nfeedback mechanisms. In conclusion, future LLMs should prioritize fairness,\ntransparency, and ethics, ensuring they uphold high moral and ethical standards\nwhen serving humanity.",
        "pdf_link": "https://arxiv.org/pdf/2310.13343v1.pdf"
    },
    {
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
        "authors": [
            "Zhaoyang Wang",
            "Shaohan Huang",
            "Yuxuan Liu",
            "Jiahai Wang",
            "Minghui Song",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2023-10-20T07:50:10Z",
        "summary": "Large language models (LLMs) exhibit impressive emergent abilities in natural\nlanguage processing, but their democratization is hindered due to huge\ncomputation requirements and closed-source nature. Recent research on advancing\nopen-source smaller LMs by distilling knowledge from black-box LLMs has\nobtained promising results in the instruction-following ability. However, the\nreasoning ability which is more challenging to foster, is relatively rarely\nexplored. In this paper, we propose a tailored learning approach to distill\nsuch reasoning ability to smaller LMs to facilitate the democratization of the\nexclusive reasoning ability. In contrast to merely employing LLM as a data\nannotator, we exploit the potential of LLM as a reasoning teacher by building\nan interactive multi-round learning paradigm. This paradigm enables the student\nto expose its deficiencies to the black-box teacher who then can provide\ncustomized training data in return. Further, to exploit the reasoning potential\nof the smaller LM, we propose self-reflection learning to motivate the student\nto learn from self-made mistakes. The learning from self-reflection and LLM are\nall tailored to the student's learning status, thanks to the seamless\nintegration with the multi-round learning paradigm. Comprehensive experiments\nand analysis on mathematical and commonsense reasoning tasks demonstrate the\neffectiveness of our method. The code will be available at\nhttps://github.com/Raibows/Learn-to-Reason.",
        "pdf_link": "https://arxiv.org/pdf/2310.13332v1.pdf"
    },
    {
        "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
        "authors": [
            "Soyeong Jeong",
            "Jinheon Baek",
            "Sukmin Cho",
            "Sung Ju Hwang",
            "Jong C. Park"
        ],
        "published": "2023-10-20T06:49:32Z",
        "summary": "Recent instruction-finetuned large language models (LMs) have achieved\nnotable performances in various tasks, such as question-answering (QA).\nHowever, despite their ability to memorize a vast amount of general knowledge\nacross diverse tasks, they might be suboptimal on specific tasks due to their\nlimited capacity to transfer and adapt knowledge to target tasks. Moreover,\nfurther finetuning LMs with labeled datasets is often infeasible due to their\nabsence, but it is also questionable if we can transfer smaller LMs having\nlimited knowledge only with unlabeled test data. In this work, we show and\ninvestigate the capabilities of smaller self-adaptive LMs, only with unlabeled\ntest data. In particular, we first stochastically generate multiple answers,\nand then ensemble them while filtering out low-quality samples to mitigate\nnoise from inaccurate labels. Our proposed self-adaption strategy demonstrates\nsignificant performance improvements on benchmark QA datasets with higher\nrobustness across diverse prompts, enabling LMs to stay stable. Code is\navailable at: https://github.com/starsuzi/T-SAS.",
        "pdf_link": "https://arxiv.org/pdf/2310.13307v1.pdf"
    },
    {
        "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model",
        "authors": [
            "Le Zhang",
            "Yihong Wu",
            "Fengran Mo",
            "Jian-Yun Nie",
            "Aishwarya Agrawal"
        ],
        "published": "2023-10-20T04:09:36Z",
        "summary": "Multi-modal open-domain question answering typically requires evidence\nretrieval from databases across diverse modalities, such as images, tables,\npassages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this\ntask. To enable LLMs to tackle the task in a zero-shot manner, we introduce\nMoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer\nstrategy that bypasses intricate multi-modality ranking, our framework can\naccommodate new modalities and seamlessly transition to new models for the\ntask. Built upon LLMs, MoqaGPT retrieves and extracts answers from each\nmodality separately, then fuses this multi-modal information using LLMs to\nproduce a final answer. Our methodology boosts performance on the MMCoQA\ndataset, improving F1 by +37.91 points and EM by +34.07 points over the\nsupervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the\nzero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and\nsignificantly closes the gap with supervised methods. Our codebase is available\nat https://github.com/lezhang7/MOQAGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.13265v1.pdf"
    },
    {
        "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
        "authors": [
            "Sipeng Zheng",
            "Jiazheng Liu",
            "Yicheng Feng",
            "Zongqing Lu"
        ],
        "published": "2023-10-20T03:22:05Z",
        "summary": "Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2310.13255v2.pdf"
    },
    {
        "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
        "authors": [
            "Jianwei Li",
            "Qi Lei",
            "Wei Cheng",
            "Dongkuan Xu"
        ],
        "published": "2023-10-19T23:02:29Z",
        "summary": "The pruning objective has recently extended beyond accuracy and sparsity to\nrobustness in language models. Despite this, existing methods struggle to\nenhance robustness against adversarial attacks when continually increasing\nmodel sparsity and require a retraining process. As humans step into the era of\nlarge language models, these issues become increasingly prominent. This paper\nproposes that the robustness of language models is proportional to the extent\nof pre-trained knowledge they encompass. Accordingly, we introduce a\npost-training pruning strategy designed to faithfully replicate the embedding\nspace and feature space of dense language models, aiming to conserve more\npre-trained knowledge during the pruning process. In this setup, each layer's\nreconstruction error not only originates from itself but also includes\ncumulative error from preceding layers, followed by an adaptive rectification.\nCompared to other state-of-art baselines, our approach demonstrates a superior\nbalance between accuracy, sparsity, robustness, and pruning cost with BERT on\ndatasets SST2, IMDB, and AGNews, marking a significant stride towards robust\npruning in language models.",
        "pdf_link": "https://arxiv.org/pdf/2310.13191v3.pdf"
    },
    {
        "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
        "authors": [
            "Zhihan Zhang",
            "Shuohang Wang",
            "Wenhao Yu",
            "Yichong Xu",
            "Dan Iter",
            "Qingkai Zeng",
            "Yang Liu",
            "Chenguang Zhu",
            "Meng Jiang"
        ],
        "published": "2023-10-19T19:52:55Z",
        "summary": "Large language models (LLMs) can perform a wide range of tasks by following\nnatural language instructions, without the necessity of task-specific\nfine-tuning. Unfortunately, the performance of LLMs is greatly influenced by\nthe quality of these instructions, and manually writing effective instructions\nfor each task is a laborious and subjective process. In this paper, we\nintroduce Auto-Instruct, a novel method to automatically improve the quality of\ninstructions provided to LLMs. Our method leverages the inherent generative\nability of LLMs to produce diverse candidate instructions for a given task, and\nthen ranks them using a scoring model trained on a variety of 575 existing NLP\ntasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both\nhuman-written instructions and existing baselines of LLM-generated\ninstructions. Furthermore, our method exhibits notable generalizability even\nwith other LLMs that are not incorporated into its training process.",
        "pdf_link": "https://arxiv.org/pdf/2310.13127v1.pdf"
    },
    {
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "authors": [
            "David Chan",
            "Suzanne Petryk",
            "Joseph E. Gonzalez",
            "Trevor Darrell",
            "John Canny"
        ],
        "published": "2023-10-19T17:59:01Z",
        "summary": "The evaluation of machine-generated image captions poses an interesting yet\npersistent challenge. Effective evaluation measures must consider numerous\ndimensions of similarity, including semantic relevance, visual structure,\nobject interactions, caption diversity, and specificity. Existing\nhighly-engineered measures attempt to capture specific aspects, but fall short\nin providing a holistic score that aligns closely with human judgments. Here,\nwe propose CLAIR, a novel method that leverages the zero-shot language modeling\ncapabilities of large language models (LLMs) to evaluate candidate captions. In\nour evaluations, CLAIR demonstrates a stronger correlation with human judgments\nof caption quality compared to existing measures. Notably, on Flickr8K-Expert,\nCLAIR achieves relative correlation improvements over SPICE of 39.6% and over\nimage-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides\nnoisily interpretable results by allowing the language model to identify the\nunderlying reasoning behind its assigned score. Code is available at\nhttps://davidmchan.github.io/clair/",
        "pdf_link": "https://arxiv.org/pdf/2310.12971v1.pdf"
    },
    {
        "title": "Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation",
        "authors": [
            "Sangho Suh",
            "Meng Chen",
            "Bryan Min",
            "Toby Jia-Jun Li",
            "Haijun Xia"
        ],
        "published": "2023-10-19T17:53:14Z",
        "summary": "Thanks to their generative capabilities, large language models (LLMs) have\nbecome an invaluable tool for creative processes. These models have the\ncapacity to produce hundreds and thousands of visual and textual outputs,\noffering abundant inspiration for creative endeavors. But are we harnessing\ntheir full potential? We argue that current interaction paradigms fall short,\nguiding users towards rapid convergence on a limited set of ideas, rather than\nempowering them to explore the vast latent design space in generative models.\nTo address this limitation, we propose a framework that facilitates the\nstructured generation of design space in which users can seamlessly explore,\nevaluate, and synthesize a multitude of responses. We demonstrate the\nfeasibility and usefulness of this framework through the design and development\nof an interactive system, Luminate, and a user study with 14 professional\nwriters. Our work advances how we interact with LLMs for creative tasks,\nintroducing a way to harness the creative potential of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12953v3.pdf"
    },
    {
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Tsz Ho Chan",
            "Tianqing Fang",
            "Weiqi Wang",
            "Chunkit Chan",
            "Dongyu Ru",
            "Qipeng Guo",
            "Hongming Zhang",
            "Yangqiu Song",
            "Yue Zhang",
            "Zheng Zhang"
        ],
        "published": "2023-10-19T16:29:23Z",
        "summary": "Analogy-making between narratives is crucial for human reasoning. In this\npaper, we evaluate the ability to identify and generate analogies by\nconstructing a first-of-its-kind large-scale story-level analogy corpus,\n\\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with\nhuman annotations on two similarities from the extended Structure-Mapping\nTheory. We design a set of tests on \\textsc{StoryAnalogy}, presenting the first\nevaluation of story-level analogy identification and generation. Interestingly,\nwe find that the analogy identification tasks are incredibly difficult not only\nfor sentence embedding models but also for the recent large language models\n(LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around\n30% accuracy in multiple-choice questions (compared to over 85% accuracy for\nhumans). Furthermore, we observe that the data in \\textsc{StoryAnalogy} can\nimprove the quality of analogy generation in LLMs, where a fine-tuned\nFlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.12874v2.pdf"
    },
    {
        "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
        "authors": [
            "Sarthak Roy",
            "Ashish Harshavardhan",
            "Animesh Mukherjee",
            "Punyajoy Saha"
        ],
        "published": "2023-10-19T16:11:02Z",
        "summary": "Recently efforts have been made by social media platforms as well as\nresearchers to detect hateful or toxic language using large language models.\nHowever, none of these works aim to use explanation, additional context and\nvictim community information in the detection process. We utilise different\nprompt variation, input information and evaluate large language models in zero\nshot setting (without adding any in-context examples). We select three large\nlanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -\nHateXplain, implicit hate and ToxicSpans. We find that on average including the\ntarget information in the pipeline improves the model performance substantially\n(~20-30%) over the baseline across the datasets. There is also a considerable\neffect of adding the rationales/explanations into the pipeline (~10-20%) over\nthe baseline across the datasets. In addition, we further provide a typology of\nthe error cases where these large language models fail to (i) classify and (ii)\nexplain the reason for the decisions they take. Such vulnerable points\nautomatically constitute 'jailbreak' prompts for these models and industry\nscale safeguard techniques need to be developed to make the models robust\nagainst such prompts.",
        "pdf_link": "https://arxiv.org/pdf/2310.12860v2.pdf"
    },
    {
        "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
        "authors": [
            "Aohan Zeng",
            "Mingdao Liu",
            "Rui Lu",
            "Bowen Wang",
            "Xiao Liu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-10-19T15:19:53Z",
        "summary": "Open large language models (LLMs) with great performance in various tasks\nhave significantly advanced the development of LLMs. However, they are far\ninferior to commercial models such as ChatGPT and GPT-4 when acting as agents\nto tackle complex tasks in the real world. These agent tasks employ LLMs as the\ncentral controller responsible for planning, memorization, and tool\nutilization, necessitating both fine-grained prompting methods and robust LLMs\nto achieve satisfactory performance. Though many prompting methods have been\nproposed to complete particular agent tasks, there is lack of research focusing\non improving the agent capabilities of LLMs themselves without compromising\ntheir general abilities. In this work, we present AgentTuning, a simple and\ngeneral method to enhance the agent abilities of LLMs while maintaining their\ngeneral LLM capabilities. We construct AgentInstruct, a lightweight\ninstruction-tuning dataset containing high-quality interaction trajectories. We\nemploy a hybrid instruction-tuning strategy by combining AgentInstruct with\nopen-source instructions from general domains. AgentTuning is used to\ninstruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show\nthat AgentTuning enables LLMs' agent capabilities without compromising general\nabilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent\ntasks, demonstrating generalized agent capabilities. We open source the\nAgentInstruct and AgentLM-7B, 13B, and 70B models at\nhttps://github.com/THUDM/AgentTuning, serving open and powerful alternatives to\ncommercial LLMs for agent tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.12823v2.pdf"
    },
    {
        "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
        "authors": [
            "Yupei Liu",
            "Yuqi Jia",
            "Runpeng Geng",
            "Jinyuan Jia",
            "Neil Zhenqiang Gong"
        ],
        "published": "2023-10-19T15:12:09Z",
        "summary": "Large Language Models (LLMs) are increasingly deployed as the backend for a\nvariety of real-world applications called LLM-Integrated Applications. Multiple\nrecent works showed that LLM-Integrated Applications are vulnerable to prompt\ninjection attacks, in which an attacker injects malicious instruction/data into\nthe input of those applications such that they produce results as the attacker\ndesires. However, existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a general framework to formalize prompt injection attacks. Existing\nattacks, which are discussed in research papers and blog posts, are special\ncases in our framework. Our framework enables us to design a new attack by\ncombining existing attacks. Moreover, we also propose a framework to\nsystematize defenses against prompt injection attacks. Using our frameworks, we\nconduct a systematic evaluation on prompt injection attacks and their defenses\nwith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in\nthis field. Our code is available at\nhttps://github.com/liu00222/Open-Prompt-Injection.",
        "pdf_link": "https://arxiv.org/pdf/2310.12815v1.pdf"
    },
    {
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": [
            "Ningyu Xu",
            "Qi Zhang",
            "Jingting Ye",
            "Menghan Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-19T14:50:51Z",
        "summary": "Large language models (LLMs) have exhibited considerable cross-lingual\ngeneralization abilities, whereby they implicitly transfer knowledge across\nlanguages. However, the transfer is not equally successful for all languages,\nespecially for low-resource ones, which poses an ongoing challenge. It is\nunclear whether we have reached the limits of implicit cross-lingual\ngeneralization and if explicit knowledge transfer is viable. In this paper, we\ninvestigate the potential for explicitly aligning conceptual correspondence\nbetween languages to enhance cross-lingual generalization. Using the syntactic\naspect of language as a testbed, our analyses of 43 languages reveal a high\ndegree of alignability among the spaces of structural concepts within each\nlanguage for both encoder-only and decoder-only LLMs. We then propose a\nmeta-learning-based method to learn to align conceptual spaces of different\nlanguages, which facilitates zero-shot and few-shot generalization in concept\nclassification and also offers insights into the cross-lingual in-context\nlearning phenomenon. Experiments on syntactic analysis tasks show that our\napproach achieves competitive results with state-of-the-art methods and narrows\nthe performance gap between languages, particularly benefiting those with\nlimited resources.",
        "pdf_link": "https://arxiv.org/pdf/2310.12794v2.pdf"
    },
    {
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
        "authors": [
            "Josef Dai",
            "Xuehai Pan",
            "Ruiyang Sun",
            "Jiaming Ji",
            "Xinbo Xu",
            "Mickel Liu",
            "Yizhou Wang",
            "Yaodong Yang"
        ],
        "published": "2023-10-19T14:22:03Z",
        "summary": "With the development of large language models (LLMs), striking a balance\nbetween the performance and safety of AI systems has never been more critical.\nHowever, the inherent tension between the objectives of helpfulness and\nharmlessness presents a significant challenge during LLM training. To address\nthis issue, we propose Safe Reinforcement Learning from Human Feedback (Safe\nRLHF), a novel algorithm for human value alignment. Safe RLHF explicitly\ndecouples human preferences regarding helpfulness and harmlessness, effectively\navoiding the crowdworkers' confusion about the tension and allowing us to train\nseparate reward and cost models. We formalize the safety concern of LLMs as an\noptimization task of maximizing the reward function while satisfying specified\ncost constraints. Leveraging the Lagrangian method to solve this constrained\nproblem, Safe RLHF dynamically adjusts the balance between the two objectives\nduring fine-tuning. Through a three-round fine-tuning using Safe RLHF, we\ndemonstrate a superior ability to mitigate harmful responses while enhancing\nmodel performance compared to existing value-aligned algorithms.\nExperimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with\ncollected human preferences, significantly improving its helpfulness and\nharmlessness according to human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2310.12773v1.pdf"
    },
    {
        "title": "Exploring Large Language Models as a Source of Common-Sense Knowledge for Robots",
        "authors": [
            "Felix Ocker",
            "J\u00f6rg Deigm\u00f6ller",
            "Julian Eggert"
        ],
        "published": "2023-10-19T14:20:30Z",
        "summary": "Service robots need common-sense knowledge to help humans in everyday\nsituations as it enables them to understand the context of their actions.\nHowever, approaches that use ontologies face a challenge because common-sense\nknowledge is often implicit, i.e., it is obvious to humans but not explicitly\nstated. This paper investigates if Large Language Models (LLMs) can fill this\ngap. Our experiments reveal limited effectiveness in the selective extraction\nof contextual action knowledge, suggesting that LLMs may not be sufficient on\ntheir own. However, the large-scale extraction of general, actionable knowledge\nshows potential, indicating that LLMs can be a suitable tool for efficiently\ncreating ontologies for robots. This paper shows that the technique used for\nknowledge extraction can be applied to populate a minimalist ontology,\nshowcasing the potential of LLMs in synergy with formal knowledge\nrepresentation.",
        "pdf_link": "https://arxiv.org/pdf/2311.08412v1.pdf"
    },
    {
        "title": "TabuLa: Harnessing Language Models for Tabular Data Synthesis",
        "authors": [
            "Zilong Zhao",
            "Robert Birke",
            "Lydia Chen"
        ],
        "published": "2023-10-19T13:50:56Z",
        "summary": "Given the ubiquitous use of tabular data in industries and the growing\nconcerns in data privacy and security, tabular data synthesis emerges as a\ncritical research area. The recent state-of-the-art methods show that large\nlanguage models (LLMs) can be adopted to generate realistic tabular data. As\nLLMs pre-process tabular data as full text, they have the advantage of avoiding\nthe curse of dimensionality associated with one-hot encoding high-dimensional\ndata. However, their long training time and limited re-usability on new tasks\nprevent them from replacing exiting tabular generative models. In this paper,\nwe propose Tabula, a tabular data synthesizer based on the language model\nstructure. Through Tabula, we demonstrate the inherent limitation of employing\npre-trained language models designed for natural language processing (NLP) in\nthe context of tabular data synthesis. Our investigation delves into the\ndevelopment of a dedicated foundational model tailored specifically for tabular\ndata synthesis. Additionally, we propose a token sequence compression strategy\nto significantly reduce training time while preserving the quality of synthetic\ndata. Extensive experiments on six datasets demonstrate that using a language\nmodel structure without loading the well-trained model weights yields a better\nstarting model for tabular data synthesis. Moreover, the Tabula model,\npreviously trained on other tabular data, serves as an excellent foundation\nmodel for new tabular data synthesis tasks. Additionally, the token sequence\ncompression method substantially reduces the model's training time. Results\nshow that Tabula averagely reduces 46.2% training time per epoch comparing to\ncurrent LLMs-based state-of-the-art algorithm and consistently achieves even\nhigher synthetic data utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.12746v1.pdf"
    },
    {
        "title": "Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong",
        "authors": [
            "Chenglei Si",
            "Navita Goyal",
            "Sherry Tongshuang Wu",
            "Chen Zhao",
            "Shi Feng",
            "Hal Daum\u00e9 III",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-10-19T08:09:58Z",
        "summary": "Large Language Models (LLMs) are increasingly used for accessing information\non the web. Their truthfulness and factuality are thus of great interest. To\nhelp users make the right decisions about the information they get, LLMs should\nnot only provide information but also help users fact-check it. Our experiments\nwith 80 crowdworkers compare language models with search engines (information\nretrieval systems) at facilitating fact-checking. We prompt LLMs to validate a\ngiven claim and provide corresponding explanations. Users reading LLM\nexplanations are significantly more efficient than those using search engines\nwhile achieving similar accuracy. However, they over-rely on the LLMs when the\nexplanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide\ncontrastive information - explain both why the claim is true and false, and\nthen we present both sides of the explanation to users. This contrastive\nexplanation mitigates users' over-reliance on LLMs, but cannot significantly\noutperform search engines. Further, showing both search engine results and LLM\nexplanations offers no complementary benefits compared to search engines alone.\nTaken together, our study highlights that natural language explanations by LLMs\nmay not be a reliable replacement for reading the retrieved passages,\nespecially in high-stakes settings where over-relying on wrong AI explanations\ncould lead to critical consequences.",
        "pdf_link": "https://arxiv.org/pdf/2310.12558v2.pdf"
    },
    {
        "title": "Product Attribute Value Extraction using Large Language Models",
        "authors": [
            "Alexander Brinkmann",
            "Roee Shraga",
            "Christian Bizer"
        ],
        "published": "2023-10-19T07:39:00Z",
        "summary": "E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.",
        "pdf_link": "https://arxiv.org/pdf/2310.12537v2.pdf"
    },
    {
        "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
        "authors": [
            "Xiaodong Yu",
            "Hao Cheng",
            "Xiaodong Liu",
            "Dan Roth",
            "Jianfeng Gao"
        ],
        "published": "2023-10-19T06:37:32Z",
        "summary": "Although remarkable progress has been achieved in preventing large language\nmodel (LLM) hallucinations using instruction tuning and retrieval augmentation,\nit remains challenging to measure the reliability of LLMs using human-crafted\nevaluation data which is not available for many tasks and domains and could\nsuffer from data leakage. Inspired by adversarial machine learning, this paper\naims to develop a method of automatically generating evaluation data by\nappropriately modifying existing data on which LLMs behave faithfully.\nSpecifically, this paper presents AutoDebug, an LLM-based framework to use\nprompting chaining to generate transferable adversarial attacks in the form of\nquestion-answering examples. We seek to understand the extent to which these\nexamples trigger the hallucination behaviors of LLMs.\n  We implement AutoDebug using ChatGPT and evaluate the resulting two variants\nof a popular open-domain question-answering dataset, Natural Questions (NQ), on\na collection of open-source and proprietary LLMs under various prompting\nsettings. Our generated evaluation data is human-readable and, as we show,\nhumans can answer these modified questions well. Nevertheless, we observe\npronounced accuracy drops across multiple LLMs including GPT-4. Our\nexperimental results show that LLMs are likely to hallucinate in two categories\nof question-answering scenarios where (1) there are conflicts between knowledge\ngiven in the prompt and their parametric knowledge, or (2) the knowledge\nexpressed in the prompt is complex. Finally, we find that the adversarial\nexamples generated by our method are transferable across all considered LLMs.\nThe examples generated by a small model can be used to debug a much larger\nmodel, making our approach cost-effective.",
        "pdf_link": "https://arxiv.org/pdf/2310.12516v1.pdf"
    },
    {
        "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
        "authors": [
            "Boyi Deng",
            "Wenjie Wang",
            "Fuli Feng",
            "Yang Deng",
            "Qifan Wang",
            "Xiangnan He"
        ],
        "published": "2023-10-19T06:15:05Z",
        "summary": "Large language models (LLMs) are susceptible to red teaming attacks, which\ncan induce LLMs to generate harmful content. Previous research constructs\nattack prompts via manual or automatic methods, which have their own\nlimitations on construction cost and quality. To address these issues, we\npropose an integrated approach that combines manual and automatic methods to\neconomically generate high-quality attack prompts. Specifically, considering\nthe impressive capabilities of newly emerged LLMs, we propose an attack\nframework to instruct LLMs to mimic human-generated prompts through in-context\nlearning. Furthermore, we propose a defense framework that fine-tunes victim\nLLMs through iterative interactions with the attack framework to enhance their\nsafety against red teaming attacks. Extensive experiments on different LLMs\nvalidate the effectiveness of our proposed attack and defense frameworks.\nAdditionally, we release a series of attack prompts datasets named SAP with\nvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.\nOur code and dataset is available on https://github.com/Aatrox103/SAP .",
        "pdf_link": "https://arxiv.org/pdf/2310.12505v1.pdf"
    },
    {
        "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
        "authors": [
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Jingyuan Huang",
            "Ruyi Dai",
            "Jen-tse Huang",
            "Zhaopeng Tu",
            "Michael R. Lyu"
        ],
        "published": "2023-10-19T05:38:23Z",
        "summary": "This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12481v2.pdf"
    },
    {
        "title": "Contrastive Learning for Inference in Dialogue",
        "authors": [
            "Etsuko Ishii",
            "Yan Xu",
            "Bryan Wilie",
            "Ziwei Ji",
            "Holy Lovenia",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023-10-19T04:49:36Z",
        "summary": "Inference, especially those derived from inductive processes, is a crucial\ncomponent in our conversation to complement the information implicitly or\nexplicitly conveyed by a speaker. While recent large language models show\nremarkable advances in inference tasks, their performance in inductive\nreasoning, where not all information is present in the context, is far behind\ndeductive reasoning. In this paper, we analyze the behavior of the models based\non the task difficulty defined by the semantic information gap -- which\ndistinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).\nOur analysis reveals that the disparity in information between dialogue\ncontexts and desired inferences poses a significant challenge to the inductive\ninference process. To mitigate this information gap, we investigate a\ncontrastive learning approach by feeding negative samples. Our experiments\nsuggest negative samples help models understand what is wrong and improve their\ninference generations.",
        "pdf_link": "https://arxiv.org/pdf/2310.12467v2.pdf"
    },
    {
        "title": "Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher",
        "authors": [
            "Xiang Shi",
            "Jiawei Liu",
            "Yinpeng Liu",
            "Qikai Cheng",
            "Wei Lu"
        ],
        "published": "2023-10-19T03:49:36Z",
        "summary": "The advent of Large Language Models (LLMs) has shown the potential to improve\nrelevance and provide direct answers in web searches. However, challenges arise\nin validating the reliability of generated results and the credibility of\ncontributing sources, due to the limitations of traditional information\nretrieval algorithms and the LLM hallucination problem. Aiming to create a\n\"PageRank\" for the LLM era, we strive to transform LLM into a relevant,\nresponsible, and trustworthy searcher. We propose a novel generative retrieval\nframework leveraging the knowledge of LLMs to foster a direct link between\nqueries and online sources. This framework consists of three core modules:\nGenerator, Validator, and Optimizer, each focusing on generating trustworthy\nonline sources, verifying source reliability, and refining unreliable sources,\nrespectively. Extensive experiments and evaluations highlight our method's\nsuperior relevance, responsibility, and trustfulness against various SOTA\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2310.12443v1.pdf"
    },
    {
        "title": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
        "authors": [
            "Hongwei Yao",
            "Jian Lou",
            "Zhan Qin"
        ],
        "published": "2023-10-19T03:25:28Z",
        "summary": "Prompts have significantly improved the performance of pretrained Large\nLanguage Models (LLMs) on various downstream tasks recently, making them\nincreasingly indispensable for a diverse range of LLM application scenarios.\nHowever, the backdoor vulnerability, a serious security threat that can\nmaliciously alter the victim model's normal predictions, has not been\nsufficiently explored for prompt-based LLMs. In this paper, we present\nPOISONPROMPT, a novel backdoor attack capable of successfully compromising both\nhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and\nrobustness of POISONPROMPT through extensive experiments on three popular\nprompt methods, using six datasets and three widely used LLMs. Our findings\nhighlight the potential security threats posed by backdoor attacks on\nprompt-based LLMs and emphasize the need for further research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2310.12439v2.pdf"
    },
    {
        "title": "Automated Repair of Declarative Software Specifications in the Era of Large Language Models",
        "authors": [
            "Md Rashedul Hasan",
            "Jiawei Li",
            "Iftekhar Ahmed",
            "Hamid Bagheri"
        ],
        "published": "2023-10-19T02:30:42Z",
        "summary": "The growing adoption of declarative software specification languages, coupled\nwith their inherent difficulty in debugging, has underscored the need for\neffective and automated repair techniques applicable to such languages.\nResearchers have recently explored various methods to automatically repair\ndeclarative software specifications, such as template-based repair,\nfeedback-driven iterative repair, and bounded exhaustive approaches. The latest\ndevelopments in large language models provide new opportunities for the\nautomatic repair of declarative specifications. In this study, we assess the\neffectiveness of utilizing OpenAI's ChatGPT to repair software specifications\nwritten in the Alloy declarative language. Unlike imperative languages,\nspecifications in Alloy are not executed but rather translated into logical\nformulas and evaluated using backend constraint solvers to identify\nspecification instances and counterexamples to assertions. Our evaluation\nfocuses on ChatGPT's ability to improve the correctness and completeness of\nAlloy declarative specifications through automatic repairs. We analyze the\nresults produced by ChatGPT and compare them with those of leading automatic\nAlloy repair methods. Our study revealed that while ChatGPT falls short in\ncomparison to existing techniques, it was able to successfully repair bugs that\nno other technique could address. Our analysis also identified errors in\nChatGPT's generated repairs, including improper operator usage, type errors,\nhigher-order logic misuse, and relational arity mismatches. Additionally, we\nobserved instances of hallucinations in ChatGPT-generated repairs and\ninconsistency in its results. Our study provides valuable insights for software\npractitioners, researchers, and tool builders considering ChatGPT for\ndeclarative specification repairs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12425v2.pdf"
    },
    {
        "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
        "authors": [
            "Kaya Stechly",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "published": "2023-10-19T00:56:37Z",
        "summary": "There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples, a wide spread belief in their iterative self-critique\ncapabilities persists. In this paper, we set out to systematically investigate\nthe effectiveness of iterative prompting of LLMs in the context of Graph\nColoring, a canonical NP-complete reasoning problem that is related to\npropositional satisfiability as well as practical problems like scheduling and\nallocation. We present a principled empirical study of the performance of GPT4\nin solving graph coloring instances or verifying the correctness of candidate\ncolorings. In iterative modes, we experiment with the model critiquing its own\nanswers and an external correct reasoner verifying proposed solutions. In both\ncases, we analyze whether the content of the criticisms actually affects bottom\nline performance. The study seems to indicate that (i) LLMs are bad at solving\ngraph coloring instances (ii) they are no better at verifying a solution--and\nthus are not effective in iterative modes with LLMs critiquing LLM-generated\nsolutions (iii) the correctness and content of the criticisms--whether by LLMs\nor external solvers--seems largely irrelevant to the performance of iterative\nprompting. We show that the observed increase in effectiveness is largely due\nto the correct solution being fortuitously present in the top-k completions of\nthe prompt (and being recognized as such by an external verifier). Our results\nthus call into question claims about the self-critiquing capabilities of state\nof the art LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.12397v1.pdf"
    },
    {
        "title": "FactCHD: Benchmarking Fact-Conflicting Hallucination Detection",
        "authors": [
            "Xiang Chen",
            "Duanzheng Song",
            "Honghao Gui",
            "Chenxi Wang",
            "Ningyu Zhang",
            "Jiang Yong",
            "Fei Huang",
            "Chengfei Lv",
            "Dan Zhang",
            "Huajun Chen"
        ],
        "published": "2023-10-18T16:27:49Z",
        "summary": "Despite their impressive generative capabilities, LLMs are hindered by\nfact-conflicting hallucinations in real-world applications. The accurate\nidentification of hallucinations in texts generated by LLMs, especially in\ncomplex inferential scenarios, is a relatively unexplored area. To address this\ngap, we present FactCHD, a dedicated benchmark designed for the detection of\nfact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset\nthat spans various factuality patterns, including vanilla, multi-hop,\ncomparison, and set operation. A distinctive element of FactCHD is its\nintegration of fact-based evidence chains, significantly enhancing the depth of\nevaluating the detectors' explanations. Experiments on different LLMs expose\nthe shortcomings of current approaches in detecting factual errors accurately.\nFurthermore, we introduce Truth-Triangulator that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset is available at\nhttps://github.com/zjunlp/FactCHD.",
        "pdf_link": "https://arxiv.org/pdf/2310.12086v2.pdf"
    },
    {
        "title": "SPEED: Speculative Pipelined Execution for Efficient Decoding",
        "authors": [
            "Coleman Hooper",
            "Sehoon Kim",
            "Hiva Mohammadzadeh",
            "Hasan Genc",
            "Kurt Keutzer",
            "Amir Gholami",
            "Sophia Shao"
        ],
        "published": "2023-10-18T16:07:01Z",
        "summary": "Generative Large Language Models (LLMs) based on the Transformer architecture\nhave recently emerged as a dominant foundation model for a wide range of\nNatural Language Processing tasks. Nevertheless, their application in real-time\nscenarios has been highly restricted due to the significant inference latency\nassociated with these models. This is particularly pronounced due to the\nautoregressive nature of generative LLM inference, where tokens are generated\nsequentially since each token depends on all previous output tokens. It is\ntherefore challenging to achieve any token-level parallelism, making inference\nextremely memory-bound. In this work, we propose SPEED, which improves\ninference efficiency by speculatively executing multiple future tokens in\nparallel with the current token using predicted values based on early-layer\nhidden states. For Transformer decoders that employ parameter sharing, the\nmemory operations for the tokens executing in parallel can be amortized, which\nallows us to accelerate generative LLM inference. We demonstrate the efficiency\nof our method in terms of latency reduction relative to model accuracy and\ndemonstrate how speculation allows for training deeper decoders with parameter\nsharing with minimal runtime overhead.",
        "pdf_link": "https://arxiv.org/pdf/2310.12072v2.pdf"
    },
    {
        "title": "LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation",
        "authors": [
            "Shengqiang Zhang",
            "Philipp Wicke",
            "L\u00fctfi Kerem \u015eenel",
            "Luis Figueredo",
            "Abdeldjallil Naceri",
            "Sami Haddadin",
            "Barbara Plank",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023-10-18T14:53:14Z",
        "summary": "The convergence of embodied agents and large language models (LLMs) has\nbrought significant advancements to embodied instruction following.\nParticularly, the strong reasoning capabilities of LLMs make it possible for\nrobots to perform long-horizon tasks without expensive annotated\ndemonstrations. However, public benchmarks for testing the long-horizon\nreasoning capabilities of language-conditioned robots in various scenarios are\nstill missing. To fill this gap, this work focuses on the tabletop manipulation\ntask and releases a simulation benchmark, \\textit{LoHoRavens}, which covers\nvarious long-horizon reasoning aspects spanning color, size, space, arithmetics\nand reference. Furthermore, there is a key modality bridging problem for\nlong-horizon manipulation tasks with LLMs: how to incorporate the observation\nfeedback during robot execution for the LLM's closed-loop planning, which is\nhowever less studied by prior work. We investigate two methods of bridging the\nmodality gap: caption generation and learnable interface for incorporating\nexplicit and implicit observation feedback to the LLM, respectively. These\nmethods serve as the two baselines for our proposed benchmark. Experiments show\nthat both methods struggle to solve some tasks, indicating long-horizon\nmanipulation tasks are still challenging for current popular models. We expect\nthe proposed public benchmark and baselines can help the community develop\nbetter models for long-horizon tabletop manipulation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.12020v2.pdf"
    },
    {
        "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
        "authors": [
            "Rui Zheng",
            "Wei Shen",
            "Yuan Hua",
            "Wenbin Lai",
            "Shihan Dou",
            "Yuhao Zhou",
            "Zhiheng Xi",
            "Xiao Wang",
            "Haoran Huang",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-18T13:54:15Z",
        "summary": "The success of AI assistants based on language models (LLMs) hinges crucially\non Reinforcement Learning from Human Feedback (RLHF), which enables the\ngeneration of responses more aligned with human preferences. As universal AI\nassistants, there's a growing expectation for them to perform consistently\nacross various domains. However, previous work shows that Reinforcement\nLearning (RL) often exploits shortcuts to attain high rewards and overlooks\nchallenging samples. This focus on quick reward gains undermines both the\nstability in training and the model's ability to generalize to new, unseen\ndata. In this work, we propose a novel approach that can learn a consistent\npolicy via RL across various data groups or domains. Given the challenges\nassociated with acquiring group annotations, our method automatically\nclassifies data into different groups, deliberately maximizing performance\nvariance. Then, we optimize the policy to perform well on challenging groups.\nLastly, leveraging the established groups, our approach adaptively adjusts the\nexploration space, allocating more learning capacity to more challenging data\nand preventing the model from over-optimizing on simpler data. Experimental\nresults indicate that our approach significantly enhances training stability\nand model generalization.",
        "pdf_link": "https://arxiv.org/pdf/2310.11971v3.pdf"
    },
    {
        "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
        "authors": [
            "Yuval Pinter",
            "Michael Elhadad"
        ],
        "published": "2023-10-18T13:38:03Z",
        "summary": "We call into question the recently popularized method of direct model editing\nas a means of correcting factual errors in LLM generations. We contrast model\nediting with three similar but distinct approaches that pursue better defined\nobjectives: (1) retrieval-based architectures, which decouple factual memory\nfrom inference and linguistic capabilities embodied in LLMs; (2) concept\nerasure methods, which aim at preventing systemic bias in generated text; and\n(3) attribution methods, which aim at grounding generations into identified\ntextual sources. We argue that direct model editing cannot be trusted as a\nsystematic remedy for the disadvantages inherent to LLMs, and while it has\nproven potential in improving model explainability, it opens risks by\nreinforcing the notion that models can be trusted for factuality. We call for\ncautious promotion and application of model editing as part of the LLM\ndeployment process, and for responsibly limiting the use cases of LLMs to those\nnot relying on editing as a critical component.",
        "pdf_link": "https://arxiv.org/pdf/2310.11958v1.pdf"
    },
    {
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
        "authors": [
            "Aviv Slobodkin",
            "Omer Goldman",
            "Avi Caciularu",
            "Ido Dagan",
            "Shauli Ravfogel"
        ],
        "published": "2023-10-18T11:01:09Z",
        "summary": "Large language models (LLMs) have been shown to possess impressive\ncapabilities, while also raising crucial concerns about the faithfulness of\ntheir responses. A primary issue arising in this context is the management of\n(un)answerable queries by LLMs, which often results in hallucinatory behavior\ndue to overconfidence. In this paper, we explore the behavior of LLMs when\npresented with (un)answerable queries. We ask: do models represent the fact\nthat the question is (un)answerable when generating a hallucinatory answer? Our\nresults show strong indications that such models encode the answerability of an\ninput query, with the representation of the first decoded token often being a\nstrong indicator. These findings shed new light on the spatial organization\nwithin the latent representations of LLMs, unveiling previously unexplored\nfacets of these models. Moreover, they pave the way for the development of\nimproved decoding techniques with better adherence to factual generation,\nparticularly in scenarios where query (un)answerability is a concern.",
        "pdf_link": "https://arxiv.org/pdf/2310.11877v2.pdf"
    },
    {
        "title": "Enhancing Genetic Improvement Mutations Using Large Language Models",
        "authors": [
            "Alexander E. I. Brownlee",
            "James Callan",
            "Karine Even-Mendoza",
            "Alina Geiger",
            "Carol Hanna",
            "Justyna Petke",
            "Federica Sarro",
            "Dominik Sobania"
        ],
        "published": "2023-10-18T10:24:14Z",
        "summary": "Large language models (LLMs) have been successfully applied to software\nengineering tasks, including program repair. However, their application in\nsearch-based techniques such as Genetic Improvement (GI) is still largely\nunexplored. In this paper, we evaluate the use of LLMs as mutation operators\nfor GI to improve the search process. We expand the Gin Java GI toolkit to call\nOpenAI's API to generate edits for the JCodec tool. We randomly sample the\nspace of edits using 5 different edit types. We find that the number of patches\npassing unit tests is up to 75% higher with LLM-based edits than with standard\nInsert edits. Further, we observe that the patches found with LLMs are\ngenerally less diverse compared to standard edits. We ran GI with local search\nto find runtime improvements. Although many improving patches are found by\nLLM-enhanced GI, the best improving patch was found by standard GI.",
        "pdf_link": "https://arxiv.org/pdf/2310.19813v1.pdf"
    },
    {
        "title": "Solving the multiplication problem of a large language model system using a graph-based method",
        "authors": [
            "Turker Tuncer",
            "Sengul Dogan",
            "Mehmet Baygin",
            "Prabal Datta Barua",
            "Abdul Hafeez-Baig",
            "Ru-San Tan",
            "Subrata Chakraborty",
            "U. Rajendra Acharya"
        ],
        "published": "2023-10-18T08:02:00Z",
        "summary": "The generative pre-trained transformer (GPT)-based chatbot software ChatGPT\npossesses excellent natural language processing capabilities but is inadequate\nfor solving arithmetic problems, especially multiplication. Its GPT structure\nuses a computational graph for multiplication, which has limited accuracy\nbeyond simple multiplication operations. We developed a graph-based\nmultiplication algorithm that emulated human-like numerical operations by\nincorporating a 10k operator, where k represents the maximum power to base 10\nof the larger of two input numbers. Our proposed algorithm attained 100%\naccuracy for 1,000,000 large number multiplication tasks, effectively solving\nthe multiplication challenge of GPT-based and other large language models. Our\nwork highlights the importance of blending simple human insights into the\ndesign of artificial intelligence algorithms. Keywords: Graph-based\nmultiplication; ChatGPT; Multiplication problem",
        "pdf_link": "https://arxiv.org/pdf/2310.13016v1.pdf"
    },
    {
        "title": "Telecom AI Native Systems in the Age of Generative AI -- An Engineering Perspective",
        "authors": [
            "Ricardo Britto",
            "Timothy Murphy",
            "Massimo Iovene",
            "Leif Jonsson",
            "Melike Erol-Kantarci",
            "Benedek Kov\u00e1cs"
        ],
        "published": "2023-10-18T07:55:54Z",
        "summary": "The rapid advancements in Artificial Intelligence (AI), particularly in\ngenerative AI and foundational models (FMs), have ushered in transformative\nchanges across various industries. Large language models (LLMs), a type of FM,\nhave demonstrated their prowess in natural language processing tasks and\ncontent generation, revolutionizing how we interact with software products and\nservices. This article explores the integration of FMs in the\ntelecommunications industry, shedding light on the concept of AI native telco,\nwhere AI is seamlessly woven into the fabric of telecom products. It delves\ninto the engineering considerations and unique challenges associated with\nimplementing FMs into the software life cycle, emphasizing the need for AI\nnative-first approaches. Despite the enormous potential of FMs, ethical,\nregulatory, and operational challenges require careful consideration,\nespecially in mission-critical telecom contexts. As the telecom industry seeks\nto harness the power of AI, a comprehensive understanding of these challenges\nis vital to thrive in a fiercely competitive market.",
        "pdf_link": "https://arxiv.org/pdf/2310.11770v1.pdf"
    },
    {
        "title": "MISAR: A Multimodal Instructional System with Augmented Reality",
        "authors": [
            "Jing Bi",
            "Nguyen Manh Nguyen",
            "Ali Vosoughi",
            "Chenliang Xu"
        ],
        "published": "2023-10-18T04:15:12Z",
        "summary": "Augmented reality (AR) requires the seamless integration of visual, auditory,\nand linguistic channels for optimized human-computer interaction. While\nauditory and visual inputs facilitate real-time and contextual user guidance,\nthe potential of large language models (LLMs) in this landscape remains largely\nuntapped. Our study introduces an innovative method harnessing LLMs to\nassimilate information from visual, auditory, and contextual modalities.\nFocusing on the unique challenge of task performance quantification in AR, we\nutilize egocentric video, speech, and context analysis. The integration of LLMs\nfacilitates enhanced state estimation, marking a step towards more adaptive AR\nsystems. Code, dataset, and demo will be available at\nhttps://github.com/nguyennm1024/misar.",
        "pdf_link": "https://arxiv.org/pdf/2310.11699v1.pdf"
    },
    {
        "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
        "authors": [
            "Xuhui Zhou",
            "Hao Zhu",
            "Leena Mathur",
            "Ruohong Zhang",
            "Haofei Yu",
            "Zhengyang Qi",
            "Louis-Philippe Morency",
            "Yonatan Bisk",
            "Daniel Fried",
            "Graham Neubig",
            "Maarten Sap"
        ],
        "published": "2023-10-18T02:27:01Z",
        "summary": "Humans are social beings; we pursue social goals in our daily interactions,\nwhich is a crucial aspect of social intelligence. Yet, AI systems' abilities in\nthis realm remain elusive. We present SOTOPIA, an open-ended environment to\nsimulate complex social interactions between artificial agents and evaluate\ntheir social intelligence. In our environment, agents role-play and interact\nunder a wide variety of scenarios; they coordinate, collaborate, exchange, and\ncompete with each other to achieve complex social goals. We simulate the\nrole-play interaction between LLM-based agents and humans within this task\nspace and evaluate their performance with a holistic evaluation framework\ncalled SOTOPIA-Eval. With SOTOPIA, we find significant differences between\nthese models in terms of their social intelligence, and we identify a subset of\nSOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.\nWe find that on this subset, GPT-4 achieves a significantly lower goal\ncompletion rate than humans and struggles to exhibit social commonsense\nreasoning and strategic communication skills. These findings demonstrate\nSOTOPIA's promise as a general platform for research on evaluating and\nimproving social intelligence in artificial agents.",
        "pdf_link": "https://arxiv.org/pdf/2310.11667v2.pdf"
    },
    {
        "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
        "authors": [
            "Linhao Luo",
            "Thuy-Trang Vu",
            "Dinh Phung",
            "Gholamreza Haffari"
        ],
        "published": "2023-10-18T00:20:50Z",
        "summary": "Previous studies have relied on existing question-answering benchmarks to\nevaluate the knowledge stored in large language models (LLMs). However, this\napproach has limitations regarding factual knowledge coverage, as it mostly\nfocuses on generic domains which may overlap with the pretraining data. This\npaper proposes a framework to systematically assess the factual knowledge of\nLLMs by leveraging knowledge graphs (KGs). Our framework automatically\ngenerates a set of questions and expected answers from the facts stored in a\ngiven KG, and then evaluates the accuracy of LLMs in answering these questions.\nWe systematically evaluate the state-of-the-art LLMs with KGs in generic and\nspecific domains. The experiment shows that ChatGPT is consistently the top\nperformer across all domains. We also find that LLMs performance depends on the\ninstruction finetuning, domain and question complexity and is prone to\nadversarial context.",
        "pdf_link": "https://arxiv.org/pdf/2310.11638v3.pdf"
    },
    {
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "authors": [
            "Arkil Patel",
            "Satwik Bhattamishra",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023-10-18T00:02:38Z",
        "summary": "Humans possess a remarkable ability to assign novel interpretations to\nlinguistic expressions, enabling them to learn new words and understand\ncommunity-specific connotations. However, Large Language Models (LLMs) have a\nknowledge cutoff and are costly to finetune repeatedly. Therefore, it is\ncrucial for LLMs to learn novel interpretations in-context. In this paper, we\nsystematically analyse the ability of LLMs to acquire novel interpretations\nusing in-context learning. To facilitate our study, we introduce MAGNIFICo, an\nevaluation suite implemented within a text-to-SQL semantic parsing framework\nthat incorporates diverse tokens and prompt settings to simulate real-world\ncomplexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a\nsurprisingly robust capacity for comprehending novel interpretations from\nnatural language descriptions as well as from discussions within long\nconversations. Nevertheless, our findings also highlight the need for further\nimprovements, particularly when interpreting unfamiliar words or when composing\nmultiple novel interpretations simultaneously in the same example.\nAdditionally, our analysis uncovers the semantic predispositions in LLMs and\nreveals the impact of recency bias for information presented in long contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.11634v1.pdf"
    },
    {
        "title": "Language Models as Zero-Shot Trajectory Generators",
        "authors": [
            "Teyun Kwon",
            "Norman Di Palo",
            "Edward Johns"
        ],
        "published": "2023-10-17T21:57:36Z",
        "summary": "Large Language Models (LLMs) have recently shown promise as high-level\nplanners for robots when given access to a selection of low-level skills.\nHowever, it is often assumed that LLMs do not possess sufficient knowledge to\nbe used for the low-level trajectories themselves. In this work, we address\nthis assumption thoroughly, and investigate if an LLM (GPT-4) can directly\npredict a dense sequence of end-effector poses for manipulation skills, when\ngiven access to only object detection and segmentation vision models. We study\nhow well a single task-agnostic prompt, without any in-context examples, motion\nprimitives, or external trajectory optimisers, can perform across 26 real-world\nlanguage-based tasks, such as \"open the bottle cap\" and \"wipe the plate with\nthe sponge\", and we investigate which design choices in this prompt are the\nmost effective. Our conclusions raise the assumed limit of LLMs for robotics,\nand we reveal for the first time that LLMs do indeed possess an understanding\nof low-level robot control sufficient for a range of common tasks, and that\nthey can additionally detect failures and then re-plan trajectories\naccordingly. Videos, code, and prompts are available at:\nhttps://www.robot-learning.uk/language-models-trajectory-generators.",
        "pdf_link": "https://arxiv.org/pdf/2310.11604v1.pdf"
    },
    {
        "title": "Automated Evaluation of Personalized Text Generation using Large Language Models",
        "authors": [
            "Yaqing Wang",
            "Jiepu Jiang",
            "Mingyang Zhang",
            "Cheng Li",
            "Yi Liang",
            "Qiaozhu Mei",
            "Michael Bendersky"
        ],
        "published": "2023-10-17T21:35:06Z",
        "summary": "Personalized text generation presents a specialized mechanism for delivering\ncontent that is specific to a user's personal context. While the research\nprogress in this area has been rapid, evaluation still presents a challenge.\nTraditional automated metrics such as BLEU and ROUGE primarily measure lexical\nsimilarity to human-written references, and are not able to distinguish\npersonalization from other subtle semantic aspects, thus falling short of\ncapturing the nuances of personalized generated content quality. On the other\nhand, human judgments are costly to obtain, especially in the realm of\npersonalized evaluation. Inspired by these challenges, we explore the use of\nlarge language models (LLMs) for evaluating personalized text generation, and\nexamine their ability to understand nuanced user context. We present AuPEL, a\nnovel evaluation method that distills three major semantic aspects of the\ngenerated text: personalization, quality and relevance, and automatically\nmeasures these aspects. To validate the effectiveness of AuPEL, we design\ncarefully controlled experiments and compare the accuracy of the evaluation\njudgments made by LLMs versus that of judgements made by human annotators, and\nconduct rigorous analyses of the consistency and sensitivity of the proposed\nmetric. We find that, compared to existing evaluation metrics, AuPEL not only\ndistinguishes and ranks models based on their personalization abilities more\naccurately, but also presents commendable consistency and efficiency for this\ntask. Our work suggests that using LLMs as the evaluators of personalized text\ngeneration is superior to traditional text similarity metrics, even though\ninteresting new challenges still remain.",
        "pdf_link": "https://arxiv.org/pdf/2310.11593v1.pdf"
    },
    {
        "title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging",
        "authors": [
            "Joel Jang",
            "Seungone Kim",
            "Bill Yuchen Lin",
            "Yizhong Wang",
            "Jack Hessel",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi",
            "Yejin Choi",
            "Prithviraj Ammanabrolu"
        ],
        "published": "2023-10-17T20:22:13Z",
        "summary": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language\nModels (LLMs) with general, aggregate human preferences, it is suboptimal for\nlearning diverse, individual perspectives. In this work, we study Reinforcement\nLearning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are\naligned to multiple (sometimes conflicting) preferences by modeling alignment\nas a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong\nsingle-objective baselines, we show that we can achieve personalized alignment\nby decomposing preferences into multiple dimensions. These dimensions are\ndefined based on personalizations that are declared as desirable by the user.\nIn this work, we show that they can be efficiently trained independently in a\ndistributed manner and combined effectively post-hoc through parameter merging.\nThe code is available at https://github.com/joeljang/RLPHF.",
        "pdf_link": "https://arxiv.org/pdf/2310.11564v1.pdf"
    },
    {
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "authors": [
            "Akari Asai",
            "Zeqiu Wu",
            "Yizhong Wang",
            "Avirup Sil",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-10-17T18:18:32Z",
        "summary": "Despite their remarkable capabilities, large language models (LLMs) often\nproduce responses containing factual inaccuracies due to their sole reliance on\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\nknowledge, decreases such issues. However, indiscriminately retrieving and\nincorporating a fixed number of retrieved passages, regardless of whether\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\ncan lead to unhelpful response generation. We introduce a new framework called\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\nquality and factuality through retrieval and self-reflection. Our framework\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\ngenerates and reflects on retrieved passages and its own generations using\nspecial tokens, called reflection tokens. Generating reflection tokens makes\nthe LM controllable during the inference phase, enabling it to tailor its\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\n13B parameters) significantly outperforms state-of-the-art LLMs and\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in\nimproving factuality and citation accuracy for long-form generations relative\nto these models.",
        "pdf_link": "https://arxiv.org/pdf/2310.11511v1.pdf"
    },
    {
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "authors": [
            "Myra Cheng",
            "Tiziano Piccardi",
            "Diyi Yang"
        ],
        "published": "2023-10-17T18:00:25Z",
        "summary": "Recent work has aimed to capture nuances of human behavior by using LLMs to\nsimulate responses from particular demographics in settings like social science\nexperiments and public opinion surveys. However, there are currently no\nestablished ways to discuss or evaluate the quality of such LLM simulations.\nMoreover, there is growing concern that these LLM simulations are flattened\ncaricatures of the personas that they aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating stereotypes. To bridge these\ngaps, we present CoMPosT, a framework to characterize LLM simulations using\nfour dimensions: Context, Model, Persona, and Topic. We use this framework to\nmeasure open-ended LLM simulations' susceptibility to caricature, defined via\ntwo criteria: individuation and exaggeration. We evaluate the level of\ncaricature in scenarios from existing work on LLM simulations. We find that for\nGPT-4, simulations of certain demographics (political and marginalized groups)\nand topics (general, uncontroversial) are highly susceptible to caricature.",
        "pdf_link": "https://arxiv.org/pdf/2310.11501v1.pdf"
    },
    {
        "title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
        "authors": [
            "Philipp Schoenegger",
            "Peter S. Park"
        ],
        "published": "2023-10-17T17:58:17Z",
        "summary": "Accurately predicting the future would be an important milestone in the\ncapabilities of artificial intelligence. However, research on the ability of\nlarge language models to provide probabilistic predictions about future events\nremains nascent. To empirically test this ability, we enrolled OpenAI's\nstate-of-the-art large language model, GPT-4, in a three-month forecasting\ntournament hosted on the Metaculus platform. The tournament, running from July\nto October 2023, attracted 843 participants and covered diverse topics\nincluding Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict.\nFocusing on binary forecasts, we show that GPT-4's probabilistic forecasts are\nsignificantly less accurate than the median human-crowd forecasts. We find that\nGPT-4's forecasts did not significantly differ from the no-information\nforecasting strategy of assigning a 50% probability to every question. We\nexplore a potential explanation, that GPT-4 might be predisposed to predict\nprobabilities close to the midpoint of the scale, but our data do not support\nthis hypothesis. Overall, we find that GPT-4 significantly underperforms in\nreal-world predictive tasks compared to median human-crowd forecasts. A\npotential explanation for this underperformance is that in real-world\nforecasting tournaments, the true answers are genuinely unknown at the time of\nprediction; unlike in other benchmark tasks like professional exams or time\nseries forecasting, where strong performance may at least partly be due to the\nanswers being memorized from the training data. This makes real-world\nforecasting tournaments an ideal environment for testing the generalized\nreasoning and prediction capabilities of artificial intelligence going forward.",
        "pdf_link": "https://arxiv.org/pdf/2310.13014v1.pdf"
    },
    {
        "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
        "authors": [
            "Rui Wen",
            "Tianhao Wang",
            "Michael Backes",
            "Yang Zhang",
            "Ahmed Salem"
        ],
        "published": "2023-10-17T17:03:00Z",
        "summary": "Large Language Models (LLMs) are powerful tools for natural language\nprocessing, enabling novel applications and user experiences. However, to\nachieve optimal performance, LLMs often require adaptation with private data,\nwhich poses privacy and security challenges. Several techniques have been\nproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),\nSoft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative\nprivacy and security properties have not been systematically investigated. In\nthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL\nagainst three types of well-established attacks: membership inference, which\nexposes data leakage (privacy); backdoor, which injects malicious behavior\n(security); and model stealing, which can violate intellectual property\n(privacy and security). Our results show that there is no silver bullet for\nprivacy and security in LLM adaptation and each technique has different\nstrengths and weaknesses.",
        "pdf_link": "https://arxiv.org/pdf/2310.11397v1.pdf"
    },
    {
        "title": "DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations",
        "authors": [
            "Yazhou Zhang",
            "Mengyao Wang",
            "Youxi Wu",
            "Prayag Tiwari",
            "Qiuchi Li",
            "Benyou Wang",
            "Jing Qin"
        ],
        "published": "2023-10-17T16:15:34Z",
        "summary": "Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.",
        "pdf_link": "https://arxiv.org/pdf/2310.11374v4.pdf"
    },
    {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
        ],
        "published": "2023-10-17T15:03:30Z",
        "summary": "As large language models (LLMs) are adopted as a fundamental component of\nlanguage technologies, it is crucial to accurately characterize their\nperformance. Because choices in prompt design can strongly influence model\nbehavior, this design process is critical in effectively using any modern\npre-trained generative language model. In this work, we focus on LLM\nsensitivity to a quintessential class of meaning-preserving design choices:\nprompt formatting. We find that several widely used open-source LLMs are\nextremely sensitive to subtle changes in prompt formatting in few-shot\nsettings, with performance differences of up to 76 accuracy points when\nevaluated using LLaMA-2-13B. Sensitivity remains even when increasing model\nsize, the number of few-shot examples, or performing instruction tuning. Our\nanalysis suggests that work evaluating LLMs with prompting-based methods would\nbenefit from reporting a range of performance across plausible prompt formats,\ninstead of the currently-standard practice of reporting performance on a single\nformat. We also show that format performance only weakly correlates between\nmodels, which puts into question the methodological validity of comparing\nmodels with an arbitrarily chosen, fixed prompt format. To facilitate\nsystematic analysis we propose FormatSpread, an algorithm that rapidly\nevaluates a sampled set of plausible prompt formats for a given task, and\nreports the interval of expected performance without accessing model weights.\nFurthermore, we present a suite of analyses that characterize the nature of\nthis sensitivity, including exploring the influence of particular atomic\nperturbations and the internal representation of particular formats.",
        "pdf_link": "https://arxiv.org/pdf/2310.11324v1.pdf"
    },
    {
        "title": "Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue",
        "authors": [
            "Shiwei Zhang",
            "Mingfang Wu",
            "Xiuzhen Zhang"
        ],
        "published": "2023-10-17T14:52:33Z",
        "summary": "In support of open and reproducible research, there has been a rapidly\nincreasing number of datasets made available for research. As the availability\nof datasets increases, it becomes more important to have quality metadata for\ndiscovering and reusing them. Yet, it is a common issue that datasets often\nlack quality metadata due to limited resources for data curation. Meanwhile,\ntechnologies such as artificial intelligence and large language models (LLMs)\nare progressing rapidly. Recently, systems based on these technologies, such as\nChatGPT, have demonstrated promising capabilities for certain data curation\ntasks. This paper proposes to leverage LLMs for cost-effective annotation of\nsubject metadata through the LLM-based in-context learning. Our method employs\nGPT-3.5 with prompts designed for annotating subject metadata, demonstrating\npromising performance in automatic metadata annotation. However, models based\non in-context learning cannot acquire discipline-specific rules, resulting in\nlower performance in several categories. This limitation arises from the\nlimited contextual information available for subject inference. To the best of\nour knowledge, we are introducing, for the first time, an in-context learning\nmethod that harnesses large language models for automated subject metadata\nannotation.",
        "pdf_link": "https://arxiv.org/pdf/2310.11318v1.pdf"
    },
    {
        "title": "Generative error correction for code-switching speech recognition using large language models",
        "authors": [
            "Chen Chen",
            "Yuchen Hu",
            "Chao-Han Huck Yang",
            "Hexin Liu",
            "Sabato Marco Siniscalchi",
            "Eng Siong Chng"
        ],
        "published": "2023-10-17T14:49:48Z",
        "summary": "Code-switching (CS) speech refers to the phenomenon of mixing two or more\nlanguages within the same sentence. Despite the recent advances in automatic\nspeech recognition (ASR), CS-ASR is still a challenging task ought to the\ngrammatical structure complexity of the phenomenon and the data scarcity of\nspecific training corpus. In this work, we propose to leverage large language\nmodels (LLMs) and lists of hypotheses generated by an ASR to address the CS\nproblem. Specifically, we first employ multiple well-trained ASR models for\nN-best hypotheses generation, with the aim of increasing the diverse and\ninformative elements in the set of hypotheses. Next, we utilize the LLMs to\nlearn the hypotheses-to-transcription (H2T) mapping by adding a trainable\nlow-rank adapter. Such a generative error correction (GER) method directly\npredicts the accurate transcription according to its expert linguistic\nknowledge and N-best hypotheses, resulting in a paradigm shift from the\ntraditional language model rescoring or error correction techniques.\nExperimental evidence demonstrates that GER significantly enhances CS-ASR\naccuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show\nremarkable data efficiency for H2T learning, providing a potential solution to\nthe data scarcity problem of CS-ASR in low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2310.13013v1.pdf"
    },
    {
        "title": "Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges",
        "authors": [
            "Thilo Spinner",
            "Rebecca Kehlbeck",
            "Rita Sevastjanova",
            "Tobias St\u00e4hle",
            "Daniel A. Keim",
            "Oliver Deussen",
            "Andreas Spitz",
            "Mennatallah El-Assady"
        ],
        "published": "2023-10-17T13:20:16Z",
        "summary": "The growing popularity of generative language models has amplified interest\nin interactive methods to guide model outputs. Prompt refinement is considered\none of the most effective means to influence output among these methods. We\nidentify several challenges associated with prompting large language models,\ncategorized into data- and model-specific, linguistic, and socio-linguistic\nchallenges. A comprehensive examination of model outputs, including runner-up\ncandidates and their corresponding probabilities, is needed to address these\nissues. The beam search tree, the prevalent algorithm to sample model outputs,\ncan inherently supply this information. Consequently, we introduce an\ninteractive visual method for investigating the beam search tree, facilitating\nanalysis of the decisions made by the model during generation. We\nquantitatively show the value of exposing the beam search tree and present five\ndetailed analysis scenarios addressing the identified challenges. Our\nmethodology validates existing results and offers additional insights.",
        "pdf_link": "https://arxiv.org/pdf/2310.11252v1.pdf"
    },
    {
        "title": "Entity Matching using Large Language Models",
        "authors": [
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "published": "2023-10-17T13:12:32Z",
        "summary": "Entity Matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. It is a central step in most data integration\npipelines and an enabler for many e-commerce applications which require to\nmatch products offers from different vendors. State-of-the-art entity matching\nmethods rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two\nmajor drawbacks of these models for entity matching are that (i) the models\nrequire significant amounts of task-specific training data and (ii) the\nfine-tuned models are not robust concerning out-of-distribution entities. We\ninvestigate using generative large language models (LLMs) for entity matching\nas a less task-specific training data dependent and more robust alternative to\nPLM-based matchers. Our study covers hosted LLMs as well as open-source LLMs\nwhich can be run locally. We evaluate these models in a zero-shot scenario as\nwell as a scenario where task-specific training data is available. We compare\ndifferent prompt designs as well as the prompt sensitivity of the models and\nshow that there is no single best prompt but the prompt is akin to a\nhyperparameter that needs to be estimated for each model/dataset combination.\nWe further investigate (i) the selection of in-context demonstrations, (ii) the\ngeneration of matching rules, as well as (iii) fine-tuning a hosted LLM using\nthe same pool of training data. Our experiments show that the best LLMs require\nno or only a few training examples to reach a similar performance as fine-tuned\nPLMs. They further exhibit a higher robustness to unseen entities, which makes\nthem especially suited to use cases where no training data is available. We\nshow that for use cases that do not allow data to be shared with third parties,\nopen-source LLMs can be a viable alternative to hosted LLMs given that a small\namount of training data or matching knowledge...",
        "pdf_link": "https://arxiv.org/pdf/2310.11244v2.pdf"
    },
    {
        "title": "Watermarking LLMs with Weight Quantization",
        "authors": [
            "Linyang Li",
            "Botian Jiang",
            "Pengyu Wang",
            "Ke Ren",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023-10-17T13:06:59Z",
        "summary": "Abuse of large language models reveals high risks as large language models\nare being deployed at an astonishing speed. It is important to protect the\nmodel weights to avoid malicious usage that violates licenses of open-source\nlarge language models. This paper proposes a novel watermarking strategy that\nplants watermarks in the quantization process of large language models without\npre-defined triggers during inference. The watermark works when the model is\nused in the fp32 mode and remains hidden when the model is quantized to int8,\nin this way, the users can only inference the model without further supervised\nfine-tuning of the model. We successfully plant the watermark into open-source\nlarge language model weights including GPT-Neo and LLaMA. We hope our proposed\nmethod can provide a potential direction for protecting model weights in the\nera of large language model applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.11237v1.pdf"
    },
    {
        "title": "The Quo Vadis of the Relationship between Language and Large Language Models",
        "authors": [
            "Evelina Leivada",
            "Vittoria Dentella",
            "Elliot Murphy"
        ],
        "published": "2023-10-17T10:54:24Z",
        "summary": "In the field of Artificial (General) Intelligence (AI), the several recent\nadvancements in Natural language processing (NLP) activities relying on Large\nLanguage Models (LLMs) have come to encourage the adoption of LLMs as\nscientific models of language. While the terminology employed for the\ncharacterization of LLMs favors their embracing as such, it is not clear that\nthey are in a place to offer insights into the target system they seek to\nrepresent. After identifying the most important theoretical and empirical risks\nbrought about by the adoption of scientific models that lack transparency, we\ndiscuss LLMs relating them to every scientific model's fundamental components:\nthe object, the medium, the meaning and the user. We conclude that, at their\ncurrent stage of development, LLMs hardly offer any explanations for language,\nand then we provide an outlook for more informative future research directions\non this topic.",
        "pdf_link": "https://arxiv.org/pdf/2310.11146v1.pdf"
    },
    {
        "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
        "authors": [
            "Arno Candel",
            "Jon McKinney",
            "Philipp Singer",
            "Pascal Pfeiffer",
            "Maximilian Jeblick",
            "Chun Ming Lee",
            "Marcos V. Conde"
        ],
        "published": "2023-10-17T09:40:58Z",
        "summary": "Large Language Models (LLMs) represent a revolution in AI. However, they also\npose many significant risks, such as the presence of biased, private,\ncopyrighted or harmful text. For this reason we need open, transparent and safe\nsolutions. We introduce a complete open-source ecosystem for developing and\ntesting LLMs. The goal of this project is to boost open alternatives to\nclosed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of\ndiverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI\ndesigned for efficient fine-tuning, evaluation, and deployment of LLMs using\nthe most recent state-of-the-art techniques. Our code and models are fully\nopen-source. We believe this work helps to boost AI development and make it\nmore accessible, efficient and trustworthy. The demo is available at:\nhttps://gpt.h2o.ai/",
        "pdf_link": "https://arxiv.org/pdf/2310.13012v2.pdf"
    },
    {
        "title": "Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models",
        "authors": [
            "Hsuan Su",
            "Cheng-Chu Cheng",
            "Hua Farn",
            "Shachi H Kumar",
            "Saurav Sahay",
            "Shang-Tse Chen",
            "Hung-yi Lee"
        ],
        "published": "2023-10-17T08:56:04Z",
        "summary": "Recently, researchers have made considerable improvements in dialogue systems\nwith the progress of large language models (LLMs) such as ChatGPT and GPT-4.\nThese LLM-based chatbots encode the potential biases while retaining\ndisparities that can harm humans during interactions. The traditional biases\ninvestigation methods often rely on human-written test cases. However, these\ntest cases are usually expensive and limited. In this work, we propose a\nfirst-of-its-kind method that automatically generates test cases to detect\nLLMs' potential gender bias. We apply our method to three well-known LLMs and\nfind that the generated test cases effectively identify the presence of biases.\nTo address the biases identified, we propose a mitigation strategy that uses\nthe generated test cases as demonstrations for in-context learning to\ncircumvent the need for parameter fine-tuning. The experimental results show\nthat LLMs generate fairer responses with the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2310.11079v1.pdf"
    },
    {
        "title": "Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning",
        "authors": [
            "Shitong Duan",
            "Xiaoyuan Yi",
            "Peng Zhang",
            "Tun Lu",
            "Xing Xie",
            "Ning Gu"
        ],
        "published": "2023-10-17T07:42:40Z",
        "summary": "Large Language Models (LLMs) have made unprecedented breakthroughs, yet their\nincreasing integration into everyday life might raise societal risks due to\ngenerated unethical content. Despite extensive study on specific issues like\nbias, the intrinsic values of LLMs remain largely unexplored from a moral\nphilosophy perspective. This work delves into ethical values utilizing Moral\nFoundation Theory. Moving beyond conventional discriminative evaluations with\npoor reliability, we propose DeNEVIL, a novel prompt generation algorithm\ntailored to dynamically exploit LLMs' value vulnerabilities and elicit the\nviolation of ethics in a generative manner, revealing their underlying value\ninclinations. On such a basis, we construct MoralPrompt, a high-quality dataset\ncomprising 2,397 prompts covering 500+ value principles, and then benchmark the\nintrinsic values across a spectrum of LLMs. We discovered that most models are\nessentially misaligned, necessitating further ethical value alignment. In\nresponse, we develop VILMO, an in-context alignment method that substantially\nenhances the value compliance of LLM outputs by learning to generate\nappropriate value instructions, outperforming existing competitors. Our methods\nare suitable for black-box and open-source models, offering a promising initial\nstep in studying the ethical values of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.11053v3.pdf"
    },
    {
        "title": "Core Building Blocks: Next Gen Geo Spatial GPT Application",
        "authors": [
            "Ashley Fernandez",
            "Swaraj Dube"
        ],
        "published": "2023-10-17T06:59:31Z",
        "summary": "This paper proposes MapGPT which is a novel approach that integrates the\ncapabilities of language models, specifically large language models (LLMs),\nwith spatial data processing techniques. This paper introduces MapGPT, which\naims to bridge the gap between natural language understanding and spatial data\nanalysis by highlighting the relevant core building blocks. By combining the\nstrengths of LLMs and geospatial analysis, MapGPT enables more accurate and\ncontextually aware responses to location-based queries. The proposed\nmethodology highlights building LLMs on spatial and textual data, utilizing\ntokenization and vector representations specific to spatial information. The\npaper also explores the challenges associated with generating spatial vector\nrepresentations. Furthermore, the study discusses the potential of\ncomputational capabilities within MapGPT, allowing users to perform geospatial\ncomputations and obtain visualized outputs. Overall, this research paper\npresents the building blocks and methodology of MapGPT, highlighting its\npotential to enhance spatial data understanding and generation in natural\nlanguage processing applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.11029v2.pdf"
    },
    {
        "title": "Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation",
        "authors": [
            "Tomohito Kasahara",
            "Daisuke Kawahara"
        ],
        "published": "2023-10-17T06:53:00Z",
        "summary": "Automatic evaluation of text generation is essential for improving the\naccuracy of generation tasks. In light of the current trend towards\nincreasingly larger decoder-based language models, we investigate automatic\nevaluation methods based on such models for text generation. This paper\ncompares various methods, including tuning with encoder-based models and large\nlanguage models under equal conditions, on two different tasks, machine\ntranslation evaluation and semantic textual similarity, in two languages,\nJapanese and English. Experimental results show that compared to the tuned\nencoder-based models, the tuned decoder-based models perform poorly. The\nanalysis of the causes for this suggests that the decoder-based models focus on\nsurface word sequences and do not capture meaning. It is also revealed that\nin-context learning of very large decoder-based models such as ChatGPT makes it\ndifficult to identify fine-grained semantic differences.",
        "pdf_link": "https://arxiv.org/pdf/2310.11026v1.pdf"
    },
    {
        "title": "EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset",
        "authors": [
            "Hang Yin",
            "Pinren Lu",
            "Ziang Li",
            "Bin Sun",
            "Kan Li"
        ],
        "published": "2023-10-17T03:28:29Z",
        "summary": "The need for high-quality data has been a key issue hindering the research of\ndialogue tasks. Recent studies try to build datasets through manual, web\ncrawling, and large pre-trained models. However, man-made data is expensive and\ndata collected from the internet often includes generic responses, meaningless\nstatements, and toxic dialogues. Automatic data generation through large models\nis a cost-effective method, but for open-domain multimodal dialogue tasks,\nthere are still three drawbacks: 1) There is currently no open-source large\nmodel that can accept multimodal input; 2) The content generated by the model\nlacks interpretability; 3) The generated data is usually difficult to quality\ncontrol and require extensive resource to collect. To alleviate the significant\nhuman and resource expenditure in data collection, we propose a Multimodal Data\nConstruction Framework (MDCF). MDCF designs proper prompts to spur the\nlarge-scale pre-trained language model to generate well-formed and satisfactory\ncontent. Additionally, MDCF also automatically provides explanation for a given\nimage and its corresponding dialogue, which can provide a certain degree of\ninterpretability and facilitate manual follow-up quality inspection. Based on\nthis, we release an Explanatory Multimodal Open-Domain dialogue dataset\n(EXMODD). Experiments indicate a positive correlation between the model's\nability to generate accurate understandings and high-quality responses. Our\ncode and data can be found at https://github.com/poplpr/EXMODD.",
        "pdf_link": "https://arxiv.org/pdf/2310.10967v1.pdf"
    },
    {
        "title": "Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models",
        "authors": [
            "Huiming Wang",
            "Liying Cheng",
            "Zhaodonghui Li",
            "De Wen Soh",
            "Lidong Bing"
        ],
        "published": "2023-10-17T03:21:43Z",
        "summary": "Contrastive learning has been proven to be effective in learning better\nsentence representations. However, to train a contrastive learning model, large\nnumbers of labeled sentences are required to construct positive and negative\npairs explicitly, such as those in natural language inference (NLI) datasets.\nUnfortunately, acquiring sufficient high-quality labeled data can be both\ntime-consuming and resource-intensive, leading researchers to focus on\ndeveloping methods for learning unsupervised sentence representations. As there\nis no clear relationship between these unstructured randomly-sampled sentences,\nbuilding positive and negative pairs over them is tricky and problematic. To\ntackle these challenges, in this paper, we propose SemCSR, a semantic-aware\ncontrastive sentence representation framework. By leveraging the generation and\nevaluation capabilities of large language models (LLMs), we can automatically\nconstruct a high-quality NLI-style corpus without any human annotation, and\nfurther incorporate the generated sentence pairs into learning a contrastive\nsentence representation model. Extensive experiments and comprehensive analyses\ndemonstrate the effectiveness of our proposed framework for learning a better\nsentence representation with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10962v1.pdf"
    },
    {
        "title": "NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain",
        "authors": [
            "Anurag Acharya",
            "Sai Munikoti",
            "Aaron Hellinger",
            "Sara Smith",
            "Sridevi Wagle",
            "Sameera Horawalavithana"
        ],
        "published": "2023-10-17T01:27:20Z",
        "summary": "As LLMs have become increasingly popular, they have been used in almost every\nfield. But as the application for LLMs expands from generic fields to narrow,\nfocused science domains, there exists an ever-increasing gap in ways to\nevaluate their efficacy in those fields. For the benchmarks that do exist, a\nlot of them focus on questions that don't require proper understanding of the\nsubject in question. In this paper, we present NuclearQA, a human-made\nbenchmark of 100 questions to evaluate language models in the nuclear domain,\nconsisting of a varying collection of questions that have been specifically\ndesigned by experts to test the abilities of language models. We detail our\napproach and show how the mix of several types of questions makes our benchmark\nuniquely capable of evaluating models in the nuclear domain. We also present\nour own evaluation metric for assessing LLM's performances due to the\nlimitations of existing ones. Our experiments on state-of-the-art models\nsuggest that even the best LLMs perform less than satisfactorily on our\nbenchmark, demonstrating the scientific knowledge gap of existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10920v1.pdf"
    },
    {
        "title": "Unlocking Emergent Modularity in Large Language Models",
        "authors": [
            "Zihan Qiu",
            "Zeyu Huang",
            "Jie Fu"
        ],
        "published": "2023-10-17T01:02:32Z",
        "summary": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic\nmodels. Existing MNNs are generally $\\textit{explicit}$: their modular\narchitectures are pre-defined, with individual modules expected to implement\ndistinct functions. Recent works reveal that there exists $\\textit{implicit}$\nmodularity in standard pre-trained transformers, namely $\\textit{Emergent\nModularity}$. They indicate that such modular structures spontaneously exhibit\nduring the early pre-training phase. Despite the benefits of modularity, most\nLanguage Models (LMs) are still treated as monolithic models in the pre-train\nand fine-tune paradigm, with their emergent modularity locked and\nunderutilized. In this work, focusing on unlocking the emergent modularity in\nLMs, we showcase that standard LMs could be fine-tuned as their\nMixture-of-Expert (MoEs) counterparts without introducing any extra parameters.\nSuch MoEs are derived from emergent modularity and are referred to as Emergent\nMoEs (EMoE). Our experiments demonstrate that fine-tuning EMoE effectively\nimproves downstream in-domain and out-of-domain generalization compared with\nvanilla fine-tuning. Our analysis and ablation studies further illustrate that\nit is robust to various configurations and can scale up to Large Language\nModels (i.e., Llama2-7B and Llama-30B). Code is available at\nhttps://github.com/qiuzh20/EMoE.",
        "pdf_link": "https://arxiv.org/pdf/2310.10908v2.pdf"
    },
    {
        "title": "Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks",
        "authors": [
            "Jiaying Wu",
            "Bryan Hooi"
        ],
        "published": "2023-10-16T21:05:12Z",
        "summary": "It is commonly perceived that online fake news and reliable news exhibit\nstark differences in writing styles, such as the use of sensationalist versus\nobjective language. However, we emphasize that style-related features can also\nbe exploited for style-based attacks. Notably, the rise of powerful Large\nLanguage Models (LLMs) has enabled malicious users to mimic the style of\ntrustworthy news outlets at minimal cost. Our analysis reveals that\nLLM-camouflaged fake news content leads to substantial performance degradation\nof state-of-the-art text-based detectors (up to 38% decrease in F1 Score),\nposing a significant challenge for automated detection in online ecosystems. To\naddress this, we introduce SheepDog, a style-agnostic fake news detector robust\nto news writing styles. SheepDog achieves this adaptability through\nLLM-empowered news reframing, which customizes each article to match different\nwriting styles using style-oriented reframing prompts. By employing\nstyle-agnostic training, SheepDog enhances its resilience to stylistic\nvariations by maximizing prediction consistency across these diverse\nreframings. Furthermore, SheepDog extracts content-focused veracity\nattributions from LLMs, where the news content is evaluated against a set of\nfact-checking rationales. These attributions provide supplementary information\nand potential interpretability that assist veracity prediction. On three\nbenchmark datasets, empirical results show that SheepDog consistently yields\nsignificant improvements over competitive baselines and enhances robustness\nagainst LLM-empowered style attacks.",
        "pdf_link": "https://arxiv.org/pdf/2310.10830v1.pdf"
    },
    {
        "title": "Vision and Language Navigation in the Real World via Online Visual Language Mapping",
        "authors": [
            "Chengguang Xu",
            "Hieu T. Nguyen",
            "Christopher Amato",
            "Lawson L. S. Wong"
        ],
        "published": "2023-10-16T20:44:09Z",
        "summary": "Navigating in unseen environments is crucial for mobile robots. Enhancing\nthem with the ability to follow instructions in natural language will further\nimprove navigation efficiency in unseen cases. However, state-of-the-art (SOTA)\nvision-and-language navigation (VLN) methods are mainly evaluated in\nsimulation, neglecting the complex and noisy real world. Directly transferring\nSOTA navigation policies trained in simulation to the real world is challenging\ndue to the visual domain gap and the absence of prior knowledge about unseen\nenvironments. In this work, we propose a novel navigation framework to address\nthe VLN task in the real world. Utilizing the powerful foundation models, the\nproposed framework includes four key components: (1) an LLMs-based instruction\nparser that converts the language instruction into a sequence of pre-defined\nmacro-action descriptions, (2) an online visual-language mapper that builds a\nreal-time visual-language map to maintain a spatial and semantic understanding\nof the unseen environment, (3) a language indexing-based localizer that grounds\neach macro-action description into a waypoint location on the map, and (4) a\nDD-PPO-based local controller that predicts the action. We evaluate the\nproposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment.\nWithout any fine-tuning, our pipeline significantly outperforms the SOTA VLN\nbaseline in the real world.",
        "pdf_link": "https://arxiv.org/pdf/2310.10822v1.pdf"
    },
    {
        "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
        "authors": [
            "Bhaskarjit Sarmah",
            "Tianjie Zhu",
            "Dhagash Mehta",
            "Stefano Pasquali"
        ],
        "published": "2023-10-16T18:45:38Z",
        "summary": "For a financial analyst, the question and answer (Q\\&A) segment of the\ncompany financial report is a crucial piece of information for various analysis\nand investment decisions. However, extracting valuable insights from the Q\\&A\nsection has posed considerable challenges as the conventional methods such as\ndetailed reading and note-taking lack scalability and are susceptible to human\nerrors, and Optical Character Recognition (OCR) and similar techniques\nencounter difficulties in accurately processing unstructured transcript text,\noften missing subtle linguistic nuances that drive investor decisions. Here, we\ndemonstrate the utilization of Large Language Models (LLMs) to efficiently and\nrapidly extract information from earnings report transcripts while ensuring\nhigh accuracy transforming the extraction process as well as reducing\nhallucination by combining retrieval-augmented generation technique as well as\nmetadata. We evaluate the outcomes of various LLMs with and without using our\nproposed approach based on various objective metrics for evaluating Q\\&A\nsystems, and empirically demonstrate superiority of our method.",
        "pdf_link": "https://arxiv.org/pdf/2310.10760v1.pdf"
    },
    {
        "title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes",
        "authors": [
            "Rose E. Wang",
            "Qingyang Zhang",
            "Carly Robinson",
            "Susanna Loeb",
            "Dorottya Demszky"
        ],
        "published": "2023-10-16T17:59:50Z",
        "summary": "Scaling high-quality tutoring remains a major challenge in education. Due to\ngrowing demand, many platforms employ novice tutors who, unlike experienced\neducators, struggle to address student mistakes and thus fail to seize prime\nlearning opportunities. Our work explores the potential of large language\nmodels (LLMs) to close the novice-expert knowledge gap in remediating math\nmistakes. We contribute Bridge, a method that uses cognitive task analysis to\ntranslate an expert's latent thought process into a decision-making model for\nremediation. This involves an expert identifying (A) the student's error, (B) a\nremediation strategy, and (C) their intention before generating a response. We\nconstruct a dataset of 700 real tutoring conversations, annotated by experts\nwith their decisions. We evaluate state-of-the-art LLMs on our dataset and find\nthat the expert's decision-making model is critical for LLMs to close the gap:\nresponses from GPT4 with expert decisions (e.g., \"simplify the problem\") are\n+76% more preferred than without. Additionally, context-sensitive decisions are\ncritical to closing pedagogical gaps: random decisions decrease GPT4's response\nquality by -97% than expert decisions. Our work shows the potential of\nembedding expert thought processes in LLM generations to enhance their\ncapability to bridge novice-expert knowledge gaps. Our dataset and code can be\nfound at: \\url{https://github.com/rosewang2008/bridge}.",
        "pdf_link": "https://arxiv.org/pdf/2310.10648v3.pdf"
    },
    {
        "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts",
        "authors": [
            "Hanan Gani",
            "Shariq Farooq Bhat",
            "Muzammal Naseer",
            "Salman Khan",
            "Peter Wonka"
        ],
        "published": "2023-10-16T17:57:37Z",
        "summary": "Diffusion-based generative models have significantly advanced text-to-image\ngeneration but encounter challenges when processing lengthy and intricate text\nprompts describing complex scenes with multiple objects. While excelling in\ngenerating images from short, single-object descriptions, these models often\nstruggle to faithfully capture all the nuanced details within longer and more\nelaborate textual inputs. In response, we present a novel approach leveraging\nLarge Language Models (LLMs) to extract critical components from text prompts,\nincluding bounding box coordinates for foreground objects, detailed textual\ndescriptions for individual objects, and a succinct background context. These\ncomponents form the foundation of our layout-to-image generation model, which\noperates in two phases. The initial Global Scene Generation utilizes object\nlayouts and background context to create an initial scene but often falls short\nin faithfully representing object characteristics as specified in the prompts.\nTo address this limitation, we introduce an Iterative Refinement Scheme that\niteratively evaluates and refines box-level content to align them with their\ntextual descriptions, recomposing objects as needed to ensure consistency. Our\nevaluation on complex prompts featuring multiple objects demonstrates a\nsubstantial improvement in recall compared to baseline diffusion models. This\nis further validated by a user study, underscoring the efficacy of our approach\nin generating coherent and detailed scenes from intricate textual inputs.",
        "pdf_link": "https://arxiv.org/pdf/2310.10640v2.pdf"
    },
    {
        "title": "\"Mistakes Help Us Grow\": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
        "authors": [
            "Kunal Handa",
            "Margaret Clapper",
            "Jessica Boyle",
            "Rose E Wang",
            "Diyi Yang",
            "David S Yeager",
            "Dorottya Demszky"
        ],
        "published": "2023-10-16T17:56:07Z",
        "summary": "Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing\nthat one's skills can be improved over time--has been shown to significantly\nreduce disparities in academic achievement and enhance students' learning\noutcomes. Although teachers espouse growth mindset principles, most find it\ndifficult to adopt GMSL in their practice due the lack of effective coaching in\nthis area. We explore whether large language models (LLMs) can provide\nautomated, personalized coaching to support teachers' use of GMSL. We establish\nan effective coaching tool to reframe unsupportive utterances to GMSL by\ndeveloping (i) a parallel dataset containing GMSL-trained teacher reframings of\nunsupportive statements with an accompanying annotation guide, (ii) a GMSL\nprompt framework to revise teachers' unsupportive language, and (iii) an\nevaluation framework grounded in psychological theory for evaluating GMSL with\nthe help of students and teachers. We conduct a large-scale evaluation\ninvolving 174 teachers and 1,006 students, finding that both teachers and\nstudents perceive GMSL-trained teacher and model reframings as more effective\nin fostering a growth mindset and promoting challenge-seeking behavior, among\nother benefits. We also find that model-generated reframings outperform those\nfrom the GMSL-trained teachers. These results show promise for harnessing LLMs\nto provide automated GMSL feedback for teachers and, more broadly, LLMs'\npotentiality for supporting students' learning in the classroom. Our findings\nalso demonstrate the benefit of large-scale human evaluations when applying\nLLMs in educational domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.10637v1.pdf"
    },
    {
        "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
        "authors": [
            "Odhran O'Donoghue",
            "Aleksandar Shtedritski",
            "John Ginger",
            "Ralph Abboud",
            "Ali Essa Ghareeb",
            "Justin Booth",
            "Samuel G Rodriques"
        ],
        "published": "2023-10-16T17:54:20Z",
        "summary": "The ability to automatically generate accurate protocols for scientific\nexperiments would represent a major step towards the automation of science.\nLarge Language Models (LLMs) have impressive capabilities on a wide range of\ntasks, such as question answering and the generation of coherent text and code.\nHowever, LLMs can struggle with multi-step problems and long-term planning,\nwhich are crucial for designing scientific experiments. Moreover, evaluation of\nthe accuracy of scientific protocols is challenging, because experiments can be\ndescribed correctly in many different ways, require expert knowledge to\nevaluate, and cannot usually be executed automatically. Here we present an\nautomatic evaluation framework for the task of planning experimental protocols,\nand we introduce BioProt: a dataset of biology protocols with corresponding\npseudocode representations. To measure performance on generating scientific\nprotocols, we use an LLM to convert a natural language protocol into\npseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode\nfrom a high-level description and a list of admissible pseudocode functions. We\nevaluate GPT-3 and GPT-4 on this task and explore their robustness. We\nexternally validate the utility of pseudocode representations of text by\ngenerating accurate novel protocols using retrieved pseudocode, and we run a\ngenerated protocol successfully in our biological laboratory. Our framework is\nextensible to the evaluation and improvement of language model planning\nabilities in other areas of science or other areas that lack automatic\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.10632v1.pdf"
    },
    {
        "title": "Data Contamination Through the Lens of Time",
        "authors": [
            "Manley Roberts",
            "Himanshu Thakur",
            "Christine Herlihy",
            "Colin White",
            "Samuel Dooley"
        ],
        "published": "2023-10-16T17:51:29Z",
        "summary": "Recent claims about the impressive abilities of large language models (LLMs)\nare often supported by evaluating publicly available benchmarks. Since LLMs\ntrain on wide swaths of the internet, this practice raises concerns of data\ncontamination, i.e., evaluating on examples that are explicitly or implicitly\nincluded in the training data. Data contamination remains notoriously\nchallenging to measure and mitigate, even with partial attempts like controlled\nexperimentation of training data, canary strings, or embedding similarities. In\nthis work, we conduct the first thorough longitudinal analysis of data\ncontamination in LLMs by using the natural experiment of training cutoffs in\nGPT models to look at benchmarks released over time. Specifically, we consider\ntwo code/mathematical problem-solving datasets, Codeforces and Project Euler,\nand find statistically significant trends among LLM pass rate vs. GitHub\npopularity and release date that provide strong evidence of contamination. By\nopen-sourcing our dataset, raw results, and evaluation framework, our work\npaves the way for rigorous analyses of data contamination in modern models. We\nconclude with a discussion of best practices and future steps for publicly\nreleasing benchmarks in the age of LLMs that train on webscale data.",
        "pdf_link": "https://arxiv.org/pdf/2310.10628v1.pdf"
    },
    {
        "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
        "authors": [
            "Charlie George",
            "Andreas Stuhlm\u00fcller"
        ],
        "published": "2023-10-16T17:51:17Z",
        "summary": "Hallucination plagues even frontier LLMs--but how bad is it really for\nsummarizing academic papers? We evaluate Factored Verification, a simple\nautomated method for detecting hallucinations in abstractive summaries. This\nmethod sets a new SotA on hallucination detection in the summarization task of\nthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method to\nestimate how often language models hallucinate when summarizing across multiple\nacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)\nsummary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct\nusing Factored Critiques and find that this lowers the number of hallucinations\nto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations\nwe find are often subtle, so we advise caution when using models to synthesize\nacademic papers.",
        "pdf_link": "https://arxiv.org/pdf/2310.10627v1.pdf"
    },
    {
        "title": "On Context Utilization in Summarization with Large Language Models",
        "authors": [
            "Mathieu Ravaut",
            "Aixin Sun",
            "Nancy F. Chen",
            "Shafiq Joty"
        ],
        "published": "2023-10-16T16:45:12Z",
        "summary": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization.",
        "pdf_link": "https://arxiv.org/pdf/2310.10570v3.pdf"
    },
    {
        "title": "Metric Ensembles For Hallucination Detection",
        "authors": [
            "Grant C. Forbes",
            "Parth Katlana",
            "Zeydy Ortiz"
        ],
        "published": "2023-10-16T15:17:22Z",
        "summary": "Abstractive text summarization has garnered increased interest as of late, in\npart due to the proliferation of large language models (LLMs). One of the most\npressing problems related to generation of abstractive summaries is the need to\nreduce \"hallucinations,\" information that was not included in the document\nbeing summarized, and which may be wholly incorrect. Due to this need, a wide\narray of metrics estimating consistency with the text being summarized have\nbeen proposed. We examine in particular a suite of unsupervised metrics for\nsummary consistency, and measure their correlations with each other and with\nhuman evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then\ncompare these evaluations to models made from a simple linear ensemble of these\nmetrics. We find that LLM-based methods outperform other unsupervised metrics\nfor hallucination detection. We also find that ensemble methods can improve\nthese scores even further, provided that the metrics in the ensemble have\nsufficiently similar and uncorrelated error rates. Finally, we present an\nensemble method for LLM-based evaluations that we show improves over this\nprevious SOTA.",
        "pdf_link": "https://arxiv.org/pdf/2310.10495v1.pdf"
    },
    {
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
        "authors": [
            "Kai Chen",
            "Chunwei Wang",
            "Kuo Yang",
            "Jianhua Han",
            "Lanqing Hong",
            "Fei Mi",
            "Hang Xu",
            "Zhengying Liu",
            "Wenyong Huang",
            "Zhenguo Li",
            "Dit-Yan Yeung",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-10-16T14:59:10Z",
        "summary": "The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.",
        "pdf_link": "https://arxiv.org/pdf/2310.10477v6.pdf"
    },
    {
        "title": "Stance Detection with Collaborative Role-Infused LLM-Based Agents",
        "authors": [
            "Xiaochong Lan",
            "Chen Gao",
            "Depeng Jin",
            "Yong Li"
        ],
        "published": "2023-10-16T14:46:52Z",
        "summary": "Stance detection automatically detects the stance in a text towards a target,\nvital for content analysis in web and social media research. Despite their\npromising capabilities, LLMs encounter challenges when directly applied to\nstance detection. First, stance detection demands multi-aspect knowledge, from\ndeciphering event-related terminologies to understanding the expression styles\nin social media platforms. Second, stance detection requires advanced reasoning\nto infer authors' implicit viewpoints, as stance are often subtly embedded\nrather than overtly stated in the text. To address these challenges, we design\na three-stage framework COLA (short for Collaborative rOle-infused LLM-based\nAgents) in which LLMs are designated distinct roles, creating a collaborative\nsystem where each role contributes uniquely. Initially, in the multidimensional\ntext analysis stage, we configure the LLMs to act as a linguistic expert, a\ndomain specialist, and a social media veteran to get a multifaceted analysis of\ntexts, thus overcoming the first challenge. Next, in the reasoning-enhanced\ndebating stage, for each potential stance, we designate a specific LLM-based\nagent to advocate for it, guiding the LLM to detect logical connections between\ntext features and stance, tackling the second challenge. Finally, in the stance\nconclusion stage, a final decision maker agent consolidates prior insights to\ndetermine the stance. Our approach avoids extra annotated data and model\ntraining and is highly usable. We achieve state-of-the-art performance across\nmultiple datasets. Ablation studies validate the effectiveness of each design\nrole in handling stance detection. Further experiments have demonstrated the\nexplainability and the versatility of our approach. Our approach excels in\nusability, accuracy, effectiveness, explainability and versatility,\nhighlighting its value.",
        "pdf_link": "https://arxiv.org/pdf/2310.10467v1.pdf"
    },
    {
        "title": "Large Language Model-Empowered Agents for Simulating Macroeconomic Activities",
        "authors": [
            "Nian Li",
            "Chen Gao",
            "Yong Li",
            "Qingmin Liao"
        ],
        "published": "2023-10-16T14:19:40Z",
        "summary": "The advent of the Web has brought about a paradigm shift in traditional\neconomics, particularly in the digital economy era, enabling the precise\nrecording and analysis of individual economic behavior. This has led to a\ngrowing emphasis on data-driven modeling in macroeconomics. In macroeconomic\nresearch, Agent-based modeling (ABM) emerged as an alternative, evolving\nthrough rule-based agents, machine learning-enhanced decision-making, and, more\nrecently, advanced AI agents. However, the existing works are suffering from\nthree main challenges when endowing agents with human-like decision-making,\nincluding agent heterogeneity, the influence of macroeconomic trends, and\nmultifaceted economic factors. Large language models (LLMs) have recently\ngained prominence in offering autonomous human-like characteristics. Therefore,\nleveraging LLMs in macroeconomic simulation presents an opportunity to overcome\ntraditional limitations. In this work, we take an early step in introducing a\nnovel approach that leverages LLMs in macroeconomic simulation. We design\nprompt-engineering-driven LLM agents to exhibit human-like decision-making and\nadaptability in the economic environment, with the abilities of perception,\nreflection, and decision-making to address the abovementioned challenges.\nSimulation experiments on macroeconomic activities show that LLM-empowered\nagents can make realistic work and consumption decisions and emerge more\nreasonable macroeconomic phenomena than existing rule-based or AI agents. Our\nwork demonstrates the promising potential to simulate macroeconomics based on\nLLM and its human-like characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2310.10436v1.pdf"
    },
    {
        "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions",
        "authors": [
            "Haoran Li",
            "Yulin Chen",
            "Jinglong Luo",
            "Yan Kang",
            "Xiaojin Zhang",
            "Qi Hu",
            "Chunkit Chan",
            "Yangqiu Song"
        ],
        "published": "2023-10-16T13:23:54Z",
        "summary": "The advancement of large language models (LLMs) has significantly enhanced\nthe ability to effectively tackle various downstream NLP tasks and unify these\ntasks into generative pipelines. On the one hand, powerful language models,\ntrained on massive textual data, have brought unparalleled accessibility and\nusability for both models and users. On the other hand, unrestricted access to\nthese models can also introduce potential malicious and unintentional privacy\nrisks. Despite ongoing efforts to address the safety and privacy concerns\nassociated with LLMs, the problem remains unresolved. In this paper, we provide\na comprehensive analysis of the current privacy attacks targeting LLMs and\ncategorize them according to the adversary's assumed capabilities to shed light\non the potential vulnerabilities present in LLMs. Then, we present a detailed\noverview of prominent defense strategies that have been developed to counter\nthese privacy attacks. Beyond existing works, we identify upcoming privacy\nconcerns as LLMs evolve. Lastly, we point out several potential avenues for\nfuture exploration.",
        "pdf_link": "https://arxiv.org/pdf/2310.10383v1.pdf"
    },
    {
        "title": "Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs",
        "authors": [
            "Ananya Singha",
            "Jos\u00e9 Cambronero",
            "Sumit Gulwani",
            "Vu Le",
            "Chris Parnin"
        ],
        "published": "2023-10-16T12:51:24Z",
        "summary": "Large language models (LLMs) are increasingly applied for tabular tasks using\nin-context learning. The prompt representation for a table may play a role in\nthe LLMs ability to process the table. Inspired by prior work, we generate a\ncollection of self-supervised structural tasks (e.g. navigate to a cell and\nrow; transpose the table) and evaluate the performance differences when using 8\nformats. In contrast to past work, we introduce 8 noise operations inspired by\nreal-world messy data and adversarial inputs, and show that such operations can\nimpact LLM performance across formats for different structural understanding\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.10358v1.pdf"
    },
    {
        "title": "Generative Calibration for In-context Learning",
        "authors": [
            "Zhongtao Jiang",
            "Yuanzhe Zhang",
            "Cao Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-16T10:45:02Z",
        "summary": "As one of the most exciting features of large language models (LLMs),\nin-context learning is a mixed blessing. While it allows users to\nfast-prototype a task solver with only a few training examples, the performance\nis generally sensitive to various configurations of the prompt such as the\nchoice or order of the training examples. In this paper, we for the first time\ntheoretically and empirically identify that such a paradox is mainly due to the\nlabel shift of the in-context model to the data distribution, in which LLMs\nshift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.\nWith this understanding, we can simply calibrate the in-context predictive\ndistribution by adjusting the label marginal, which is estimated via\nMonte-Carlo sampling over the in-context model, i.e., generation of LLMs. We\ncall our approach as generative calibration. We conduct exhaustive experiments\nwith 12 text classification tasks and 12 LLMs scaling from 774M to 33B,\ngenerally find that the proposed method greatly and consistently outperforms\nthe ICL as well as state-of-the-art calibration methods, by up to 27% absolute\nin macro-F1. Meanwhile, the proposed method is also stable under different\nprompt configurations.",
        "pdf_link": "https://arxiv.org/pdf/2310.10266v1.pdf"
    },
    {
        "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
        "authors": [
            "Rujie Wu",
            "Xiaojian Ma",
            "Zhenliang Zhang",
            "Wei Wang",
            "Qing Li",
            "Song-Chun Zhu",
            "Yizhou Wang"
        ],
        "published": "2023-10-16T09:19:18Z",
        "summary": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world\nfew-shot reasoning for machine vision. It originates from the classical Bongard\nProblems (BPs): Given two sets of images (positive and negative), the model\nneeds to identify the set that query images belong to by inducing the visual\nconcepts, which is exclusively depicted by images from the positive set. Our\nbenchmark inherits the few-shot concept induction of the original BPs while\nadding the two novel layers of challenge: 1) open-world free-form concepts, as\nthe visual concepts in Bongard-OpenWorld are unique compositions of terms from\nan open vocabulary, ranging from object categories to abstract visual\nattributes and commonsense factual knowledge; 2) real-world images, as opposed\nto the synthetic diagrams used by many counterparts. In our exploration,\nBongard-OpenWorld already imposes a significant challenge to current few-shot\nreasoning algorithms. We further investigate to which extent the recently\nintroduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can\nsolve our task, by directly probing VLMs, and combining VLMs and LLMs in an\ninteractive reasoning scheme. We even conceived a neuro-symbolic reasoning\napproach that reconciles LLMs & VLMs with logical reasoning to emulate the\nhuman problem-solving process for Bongard Problems. However, none of these\napproaches manage to close the human-machine gap, as the best learner achieves\n64% accuracy while human participants easily reach 91%. We hope\nBongard-OpenWorld can help us better understand the limitations of current\nvisual intelligence and facilitate future research on visual agents with\nstronger few-shot visual reasoning capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2310.10207v5.pdf"
    },
    {
        "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
        "authors": [
            "Xiaoshuai Song",
            "Keqing He",
            "Pei Wang",
            "Guanting Dong",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2023-10-16T08:34:44Z",
        "summary": "The tasks of out-of-domain (OOD) intent discovery and generalized intent\ndiscovery (GID) aim to extend a closed intent classifier to open-world intent\nsets, which is crucial to task-oriented dialogue (TOD) systems. Previous\nmethods address them by fine-tuning discriminative models. Recently, although\nsome studies have been exploring the application of large language models\n(LLMs) represented by ChatGPT to various downstream tasks, it is still unclear\nfor the ability of ChatGPT to discover and incrementally extent OOD intents. In\nthis paper, we comprehensively evaluate ChatGPT on OOD intent discovery and\nGID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT\nexhibits consistent advantages under zero-shot settings, but is still at a\ndisadvantage compared to fine-tuned models. More deeply, through a series of\nanalytical experiments, we summarize and discuss the challenges faced by LLMs\nincluding clustering, domain-specific understanding, and cross-domain\nin-context learning scenarios. Finally, we provide empirical guidance for\nfuture directions to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2310.10176v1.pdf"
    },
    {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
            "Huao Li",
            "Yu Quan Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Michael Lewis",
            "Katia Sycara"
        ],
        "published": "2023-10-16T07:51:19Z",
        "summary": "While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.",
        "pdf_link": "https://arxiv.org/pdf/2310.10701v2.pdf"
    },
    {
        "title": "LoBaSS: Gauging Learnability in Supervised Fine-tuning Data",
        "authors": [
            "Haotian Zhou",
            "Tingkai Liu",
            "Qianli Ma",
            "Jianbo Yuan",
            "Pengfei Liu",
            "Yang You",
            "Hongxia Yang"
        ],
        "published": "2023-10-16T07:26:24Z",
        "summary": "Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large\nLanguage Models (LLMs) to specific task prerequisites. The selection of\nfine-tuning data profoundly influences the model's performance, whose principle\nis traditionally grounded in data quality and distribution. In this paper, we\nintroduce a new dimension in SFT data selection: learnability. This new\ndimension is motivated by the intuition that SFT unlocks capabilities acquired\nby a LLM during the pretraining phase. Given that different pretrained models\nhave disparate capabilities, the SFT data appropriate for one may not suit\nanother. Thus, we introduce the term learnability to define the suitability of\ndata for effective learning by the model. We present the Loss Based SFT Data\nSelection (LoBaSS) method, utilizing data learnability as the principal\ncriterion for the selection SFT data. This method provides a nuanced approach,\nallowing the alignment of data selection with inherent model capabilities,\nensuring optimal compatibility and learning efficiency. In experimental\ncomparisons involving 7B and 13B models, our LoBaSS method is able to surpass\nfull-data fine-tuning at merely 6% of the total training data. When employing\n16.7% of the data, LoBaSS harmonizes the model's capabilities across\nconversational and mathematical domains, proving its efficacy and adaptability.",
        "pdf_link": "https://arxiv.org/pdf/2310.13008v1.pdf"
    },
    {
        "title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset",
        "authors": [
            "Arthur Amalvy",
            "Vincent Labatut",
            "Richard Dufour"
        ],
        "published": "2023-10-16T06:53:12Z",
        "summary": "While recent pre-trained transformer-based models can perform named entity\nrecognition (NER) with great accuracy, their limited range remains an issue\nwhen applied to long documents such as whole novels. To alleviate this issue, a\nsolution is to retrieve relevant context at the document level. Unfortunately,\nthe lack of supervision for such a task means one has to settle for\nunsupervised approaches. Instead, we propose to generate a synthetic context\nretrieval training dataset using Alpaca, an instructiontuned large language\nmodel (LLM). Using this dataset, we train a neural context retriever based on a\nBERT model that is able to find relevant context for NER. We show that our\nmethod outperforms several retrieval baselines for the NER task on an English\nliterary dataset composed of the first chapter of 40 books.",
        "pdf_link": "https://arxiv.org/pdf/2310.10118v3.pdf"
    },
    {
        "title": "On Generative Agents in Recommendation",
        "authors": [
            "An Zhang",
            "Leheng Sheng",
            "Yuxin Chen",
            "Hao Li",
            "Yang Deng",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "published": "2023-10-16T06:41:16Z",
        "summary": "Recommender systems are the cornerstone of today's information dissemination,\nyet a disconnect between offline metrics and online performance greatly hinders\ntheir development. Addressing this challenge, we envision a recommendation\nsimulator, capitalizing on recent breakthroughs in human-level intelligence\nexhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie\nrecommendation simulator, leveraging LLM-empowered generative agents equipped\nwith user profile, memory, and actions modules specifically tailored for the\nrecommender system. In particular, these agents' profile modules are\ninitialized using the MovieLens dataset, capturing users' unique tastes and\nsocial traits; memory modules log both factual and emotional memories and are\nintegrated with an emotion-driven reflection mechanism; action modules support\na wide variety of behaviors, spanning both taste-driven and emotion-driven\nactions. Each agent interacts with personalized movie recommendations in a\npage-by-page manner, relying on a pre-implemented collaborative filtering-based\nrecommendation algorithm. We delve into both the capabilities and limitations\nof Agent4Rec, aiming to explore an essential research question: to what extent\ncan LLM-empowered generative agents faithfully simulate the behavior of real,\nautonomous humans in recommender systems? Extensive and multi-faceted\nevaluations of Agent4Rec highlight both the alignment and deviation between\nagents and user-personalized preferences. Beyond mere performance comparison,\nwe explore insightful experiments, such as emulating the filter bubble effect\nand discovering the underlying causal relationships in recommendation tasks.\nOur codes are available at https://github.com/LehengTHU/Agent4Rec.",
        "pdf_link": "https://arxiv.org/pdf/2310.10108v1.pdf"
    },
    {
        "title": "JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning",
        "authors": [
            "Issey Sukeda",
            "Masahiro Suzuki",
            "Hiroki Sakaji",
            "Satoshi Kodera"
        ],
        "published": "2023-10-16T05:28:28Z",
        "summary": "In the ongoing wave of impact driven by large language models (LLMs) like\nChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial\nresearch frontier. Since mainstream LLMs tend to be designed for\ngeneral-purpose applications, constructing a medical LLM through domain\nadaptation is a huge challenge. While instruction-tuning is used to fine-tune\nsome LLMs, its precise roles in domain adaptation remain unknown. Here we show\nthe contribution of LoRA-based instruction-tuning to performance in Japanese\nmedical question-answering tasks. In doing so, we employ a multifaceted\nevaluation for multiple-choice questions, including scoring based on \"Exact\nmatch\" and \"Gestalt distance\" in addition to the conventional accuracy. Our\nfindings suggest that LoRA-based instruction-tuning can partially incorporate\ndomain-specific knowledge into LLMs, with larger models demonstrating more\npronounced effects. Furthermore, our results underscore the potential of\nadapting English-centric models for Japanese applications in domain adaptation,\nwhile also highlighting the persisting limitations of Japanese-centric models.\nThis initiative represents a pioneering effort in enabling medical institutions\nto fine-tune and operate models without relying on external services.",
        "pdf_link": "https://arxiv.org/pdf/2310.10083v2.pdf"
    },
    {
        "title": "Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks",
        "authors": [
            "Shuyu Jiang",
            "Xingshu Chen",
            "Rui Tang"
        ],
        "published": "2023-10-16T05:19:25Z",
        "summary": "Recently, Large language models (LLMs) with powerful general capabilities\nhave been increasingly integrated into various Web applications, while\nundergoing alignment training to ensure that the generated content aligns with\nuser intent and ethics. Unfortunately, they remain the risk of generating\nharmful content like hate speech and criminal activities in practical\napplications. Current approaches primarily rely on detecting, collecting, and\ntraining against harmful prompts to prevent such risks. However, they typically\nfocused on the \"superficial\" harmful prompts with a solitary intent, ignoring\ncomposite attack instructions with multiple intentions that can easily elicit\nharmful content in real-world scenarios. In this paper, we introduce an\ninnovative technique for obfuscating harmful instructions: Compositional\nInstruction Attacks (CIA), which refers to attacking by combination and\nencapsulation of multiple instructions. CIA hides harmful prompts within\ninstructions of harmless intentions, making it impossible for the model to\nidentify underlying malicious intentions. Furthermore, we implement two\ntransformation methods, known as T-CIA and W-CIA, to automatically disguise\nharmful instructions as talking or writing tasks, making them appear harmless\nto LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety\nassessment datasets and two harmful prompt datasets. It achieves an attack\nsuccess rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+\nfor ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets.\nOur approach reveals the vulnerability of LLMs to such compositional\ninstruction attacks that harbor underlying harmful intentions, contributing\nsignificantly to LLM security development. Warning: this paper may contain\noffensive or upsetting content!",
        "pdf_link": "https://arxiv.org/pdf/2310.10077v1.pdf"
    },
    {
        "title": "FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models",
        "authors": [
            "Tao Fan",
            "Yan Kang",
            "Guoqiang Ma",
            "Weijing Chen",
            "Wenbin Wei",
            "Lixin Fan",
            "Qiang Yang"
        ],
        "published": "2023-10-16T04:17:13Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have\nexhibited remarkable performances across various tasks in recent years.\nHowever, LLMs face two main challenges in real-world applications. One\nchallenge is that training LLMs consumes vast computing resources, preventing\nLLMs from being adopted by small and medium-sized enterprises with limited\ncomputing resources. Another is that training LLM requires a large amount of\nhigh-quality data, which are often scattered among enterprises. To address\nthese challenges, we propose FATE-LLM, an industrial-grade federated learning\nframework for large language models. FATE-LLM (1) facilitates federated\nlearning for large language models (coined FedLLM); (2) promotes efficient\ntraining of FedLLM using parameter-efficient fine-tuning methods; (3) protects\nthe intellectual property of LLMs; (4) preserves data privacy during training\nand inference through privacy-preserving mechanisms. We release the code of\nFATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research\nof FedLLM and enable a broad range of industrial applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.10049v1.pdf"
    },
    {
        "title": "Improving Large Language Model Fine-tuning for Solving Math Problems",
        "authors": [
            "Yixin Liu",
            "Avi Singh",
            "C. Daniel Freeman",
            "John D. Co-Reyes",
            "Peter J. Liu"
        ],
        "published": "2023-10-16T04:11:19Z",
        "summary": "Despite their success in many natural language tasks, solving math problems\nremains a significant challenge for large language models (LLMs). A large gap\nexists between LLMs' pass-at-one and pass-at-N performance in solving math\nproblems, suggesting LLMs might be close to finding correct solutions,\nmotivating our exploration of fine-tuning methods to unlock LLMs' performance.\nUsing the challenging MATH dataset, we investigate three fine-tuning\nstrategies: (1) solution fine-tuning, where we fine-tune to generate a detailed\nsolution for a given math problem; (2) solution-cluster re-ranking, where the\nLLM is fine-tuned as a solution verifier/evaluator to choose among generated\ncandidate solution clusters; (3) multi-task sequential fine-tuning, which\nintegrates both solution generation and evaluation tasks together efficiently\nto enhance the LLM performance. With these methods, we present a thorough\nempirical study on a series of PaLM 2 models and find: (1) The quality and\nstyle of the step-by-step solutions used for fine-tuning can make a significant\nimpact on the model performance; (2) While solution re-ranking and majority\nvoting are both effective for improving the model performance when used\nseparately, they can also be used together for an even greater performance\nboost; (3) Multi-task fine-tuning that sequentially separates the solution\ngeneration and evaluation tasks can offer improved performance compared with\nthe solution fine-tuning baseline. Guided by these insights, we design a\nfine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset\nwith fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the\nfew-shot performance of pre-trained PaLM 2-L model with majority voting.",
        "pdf_link": "https://arxiv.org/pdf/2310.10047v1.pdf"
    },
    {
        "title": "TRANSOM: An Efficient Fault-Tolerant System for Training LLMs",
        "authors": [
            "Baodong Wu",
            "Lei Xia",
            "Qingping Li",
            "Kangyu Li",
            "Xu Chen",
            "Yongqiang Guo",
            "Tieyao Xiang",
            "Yuheng Chen",
            "Shigang Li"
        ],
        "published": "2023-10-16T04:06:52Z",
        "summary": "Large language models (LLMs) with hundreds of billions or trillions of\nparameters, represented by chatGPT, have achieved profound impact on various\nfields. However, training LLMs with super-large-scale parameters requires large\nhigh-performance GPU clusters and long training periods lasting for months. Due\nto the inevitable hardware and software failures in large-scale clusters,\nmaintaining uninterrupted and long-duration training is extremely challenging.\nAs a result, A substantial amount of training time is devoted to task\ncheckpoint saving and loading, task rescheduling and restart, and task manual\nanomaly checks, which greatly harms the overall training efficiency. To address\nthese issues, we propose TRANSOM, a novel fault-tolerant LLM training system.\nIn this work, we design three key subsystems: the training pipeline automatic\nfault tolerance and recovery mechanism named Transom Operator and Launcher\n(TOL), the training task multi-dimensional metric automatic anomaly detection\nsystem named Transom Eagle Eye (TEE), and the training checkpoint asynchronous\naccess automatic fault tolerance and recovery technology named Transom\nCheckpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks,\nwhile TEE is responsible for task monitoring and anomaly reporting. TEE detects\ntraining anomalies and reports them to TOL, who automatically enters the fault\ntolerance strategy to eliminate abnormal nodes and restart the training task.\nAnd the asynchronous checkpoint saving and loading functionality provided by\nTCE greatly shorten the fault tolerance overhead. The experimental results\nindicate that TRANSOM significantly enhances the efficiency of large-scale LLM\ntraining on clusters. Specifically, the pre-training time for GPT3-175B has\nbeen reduced by 28%, while checkpoint saving and loading performance have\nimproved by a factor of 20.",
        "pdf_link": "https://arxiv.org/pdf/2310.10046v3.pdf"
    },
    {
        "title": "Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis",
        "authors": [
            "Chaoyi Wu",
            "Jiayu Lei",
            "Qiaoyu Zheng",
            "Weike Zhao",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Xiao Zhou",
            "Ziheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "published": "2023-10-15T18:32:27Z",
        "summary": "Driven by the large foundation models, the development of artificial\nintelligence has witnessed tremendous progress lately, leading to a surge of\ngeneral interest from the public. In this study, we aim to assess the\nperformance of OpenAI's newest model, GPT-4V(ision), specifically in the realm\nof multimodal medical diagnosis. Our evaluation encompasses 17 human body\nsystems, including Central Nervous System, Head and Neck, Cardiac, Chest,\nHematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,\nObstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,\nPediatrics, with images taken from 8 modalities used in daily clinic routine,\ne.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),\nPositron Emission Tomography (PET), Digital Subtraction Angiography (DSA),\nMammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on\nmultiple clinical tasks with or without patent history provided, including\nimaging modality and anatomy recognition, disease diagnosis, report generation,\ndisease localisation.\n  Our observation shows that, while GPT-4V demonstrates proficiency in\ndistinguishing between medical image modalities and anatomy, it faces\nsignificant challenges in disease diagnosis and generating comprehensive\nreports. These findings underscore that while large multimodal models have made\nsignificant advancements in computer vision and natural language processing, it\nremains far from being used to effectively support real-world medical\napplications and clinical decision-making.\n  All images used in this report can be found in\nhttps://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.09909v3.pdf"
    },
    {
        "title": "In-Context Learning with Iterative Demonstration Selection",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Anirudh Dagar",
            "Wenming Ye"
        ],
        "published": "2023-10-15T16:40:19Z",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nLeveraging the merits of both dimensions, we propose Iterative Demonstration\nSelection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT),\nIDS iteratively selects examples that are diverse but still strongly correlated\nwith the test sample as ICL demonstrations. Specifically, IDS applies\nZero-shot-CoT to the test sample before demonstration selection. The output\nreasoning path is then used to choose demonstrations that are prepended to the\ntest sample for inference. The generated answer is accompanied by its\ncorresponding reasoning path for extracting a new set of demonstrations in the\nnext iteration. After several iterations, IDS adopts majority voting to obtain\nthe final result. Through extensive experiments on tasks including commonsense\nreasoning, question answering, topic classification, and sentiment analysis, we\ndemonstrate that IDS can consistently outperform existing ICL demonstration\nselection methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.09881v2.pdf"
    },
    {
        "title": "Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)",
        "authors": [
            "Jianxiang Yu",
            "Yuxiang Ren",
            "Chenghua Gong",
            "Jiaqi Tan",
            "Xiang Li",
            "Xuecang Zhang"
        ],
        "published": "2023-10-15T16:04:28Z",
        "summary": "Text-attributed graphs have recently garnered significant attention due to\ntheir wide range of applications in web domains. Existing methodologies employ\nword embedding models for acquiring text representations as node features,\nwhich are subsequently fed into Graph Neural Networks (GNNs) for training.\nRecently, the advent of Large Language Models (LLMs) has introduced their\npowerful capabilities in information retrieval and text generation, which can\ngreatly enhance the text attributes of graph data. Furthermore, the acquisition\nand labeling of extensive datasets are both costly and time-consuming\nendeavors. Consequently, few-shot learning has emerged as a crucial problem in\nthe context of graph learning tasks. In order to tackle this challenge, we\npropose a lightweight paradigm called ENG, which adopts a plug-and-play\napproach to empower text-attributed graphs through node generation using LLMs.\nSpecifically, we utilize LLMs to extract semantic information from the labels\nand generate samples that belong to these categories as exemplars.\nSubsequently, we employ an edge predictor to capture the structural information\ninherent in the raw dataset and integrate the newly generated samples into the\noriginal graph. This approach harnesses LLMs for enhancing class-level\ninformation and seamlessly introduces labeled nodes and edges without modifying\nthe raw dataset, thereby facilitating the node classification task in few-shot\nscenarios. Extensive experiments demonstrate the outstanding performance of our\nproposed paradigm, particularly in low-shot scenarios. For instance, in the\n1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement over\nthe baseline model.",
        "pdf_link": "https://arxiv.org/pdf/2310.09872v1.pdf"
    },
    {
        "title": "ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors",
        "authors": [
            "Julien Pourcel",
            "C\u00e9dric Colas",
            "Pierre-Yves Oudeyer",
            "Laetitia Teodorescu"
        ],
        "published": "2023-10-15T14:57:14Z",
        "summary": "Finding and selecting new and interesting problems to solve is at the heart\nof curiosity, science and innovation. We here study automated problem\ngeneration in the context of the open-ended space of python programming\npuzzles. Existing generative models often aim at modeling a reference\ndistribution without any explicit diversity optimization. Other methods\nexplicitly optimizing for diversity do so either in limited hand-coded\nrepresentation spaces or in uninterpretable learned embedding spaces that may\nnot align with human perceptions of interesting variations. With ACES\n(Autotelic Code Exploration via Semantic descriptors), we introduce a new\nautotelic generation method that leverages semantic descriptors produced by a\nlarge language model (LLM) to directly optimize for interesting diversity, as\nwell as few-shot-based generation. Each puzzle is labeled along 10 dimensions,\neach capturing a programming skill required to solve it. ACES generates and\npursues novel and feasible goals to explore that abstract semantic space,\nslowly discovering a diversity of solvable programming puzzles in any given\nrun. Across a set of experiments, we show that ACES discovers a richer\ndiversity of puzzles than existing diversity-maximizing algorithms as measured\nacross a range of diversity metrics. We further study whether and in which\nconditions this diversity can translate into the successful training of puzzle\nsolving models.",
        "pdf_link": "https://arxiv.org/pdf/2310.10692v3.pdf"
    },
    {
        "title": "Assessing the Reliability of Large Language Model Knowledge",
        "authors": [
            "Weixuan Wang",
            "Barry Haddow",
            "Alexandra Birch",
            "Wei Peng"
        ],
        "published": "2023-10-15T12:40:30Z",
        "summary": "Large language models (LLMs) have been treated as knowledge bases due to\ntheir strong performance in knowledge probing tasks. LLMs are typically\nevaluated using accuracy, yet this metric does not capture the vulnerability of\nLLMs to hallucination-inducing factors like prompt and context variability. How\ndo we evaluate the capabilities of LLMs to consistently produce factually\ncorrect answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe\n(MONITOR), a novel metric designed to directly measure LLMs' factual\nreliability. MONITOR computes the distance between the probability\ndistributions of a valid output and its counterparts produced by the same LLM\nprobing the same fact using different styles of prompts and\ncontexts.Experiments on a comprehensive range of 12 LLMs demonstrate the\neffectiveness of MONITOR in evaluating the factual reliability of LLMs while\nmaintaining a low computational overhead. In addition, we release the FKTC\n(Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total\nto foster research along this line (https://github.com/Vicky-Wil/MONITOR).",
        "pdf_link": "https://arxiv.org/pdf/2310.09820v1.pdf"
    },
    {
        "title": "When can transformers reason with abstract symbols?",
        "authors": [
            "Enric Boix-Adsera",
            "Omid Saremi",
            "Emmanuel Abbe",
            "Samy Bengio",
            "Etai Littwin",
            "Joshua Susskind"
        ],
        "published": "2023-10-15T06:45:38Z",
        "summary": "We investigate the capabilities of transformer large language models (LLMs)\non relational reasoning tasks involving abstract symbols. Such tasks have long\nbeen studied in the neuroscience literature as fundamental building blocks for\nmore complex abilities in programming, mathematics, and verbal reasoning. For\n(i) regression tasks, we prove that transformers generalize when trained, but\nrequire astonishingly large quantities of training data. For (ii)\nnext-token-prediction tasks with symbolic labels, we show an \"inverse scaling\nlaw\": transformers fail to generalize as their embedding dimension increases.\nFor both settings (i) and (ii), we propose subtle transformer modifications\nwhich can reduce the amount of data needed by adding two trainable parameters\nper head.",
        "pdf_link": "https://arxiv.org/pdf/2310.09753v1.pdf"
    },
    {
        "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
        "authors": [
            "Fanghua Ye",
            "Meng Fang",
            "Shenghui Li",
            "Emine Yilmaz"
        ],
        "published": "2023-10-15T03:04:17Z",
        "summary": "Query rewriting plays a vital role in enhancing conversational search by\ntransforming context-dependent user queries into standalone forms. Existing\napproaches primarily leverage human-rewritten queries as labels to train query\nrewriting models. However, human rewrites may lack sufficient information for\noptimal retrieval performance. To overcome this limitation, we propose\nutilizing large language models (LLMs) as query rewriters, enabling the\ngeneration of informative query rewrites through well-designed instructions. We\ndefine four essential properties for well-formed rewrites and incorporate all\nof them into the instruction. In addition, we introduce the role of rewrite\neditors for LLMs when initial query rewrites are available, forming a\n\"rewrite-then-edit\" process. Furthermore, we propose distilling the rewriting\ncapabilities of LLMs into smaller models to reduce rewriting latency. Our\nexperimental evaluation on the QReCC dataset demonstrates that informative\nquery rewrites can yield substantially improved retrieval performance compared\nto human rewrites, especially with sparse retrievers.",
        "pdf_link": "https://arxiv.org/pdf/2310.09716v2.pdf"
    },
    {
        "title": "Configuration Validation with Large Language Models",
        "authors": [
            "Xinyu Lian",
            "Yinfang Chen",
            "Runxiang Cheng",
            "Jie Huang",
            "Parth Thakkar",
            "Minjia Zhang",
            "Tianyin Xu"
        ],
        "published": "2023-10-15T00:50:27Z",
        "summary": "Misconfigurations are major causes of software failures. Existing practices\nrely on developer-written rules or test cases to validate configurations, which\nare expensive. Machine learning (ML) for configuration validation is considered\na promising direction, but has been facing challenges such as the need of\nlarge-scale field data and system-specific models. Recent advances in Large\nLanguage Models (LLMs) show promise in addressing some of the long-lasting\nlimitations of ML-based configuration validation. We present a first analysis\non the feasibility and effectiveness of using LLMs for configuration\nvalidation. We empirically evaluate LLMs as configuration validators by\ndeveloping a generic LLM-based configuration validation framework, named Ciri.\nCiri employs effective prompt engineering with few-shot learning based on both\nvalid configuration and misconfiguration data. Ciri checks outputs from LLMs\nwhen producing results, addressing hallucination and nondeterminism of LLMs. We\nevaluate Ciri's validation effectiveness on eight popular LLMs using\nconfiguration data of ten widely deployed open-source systems. Our analysis (1)\nconfirms the potential of using LLMs for configuration validation, (2) explores\ndesign space of LLMbased validators like Ciri, and (3) reveals open challenges\nsuch as ineffectiveness in detecting certain types of misconfigurations and\nbiases towards popular configuration parameters.",
        "pdf_link": "https://arxiv.org/pdf/2310.09690v2.pdf"
    },
    {
        "title": "DPZero: Private Fine-Tuning of Language Models without Backpropagation",
        "authors": [
            "Liang Zhang",
            "Bingcong Li",
            "Kiran Koshy Thekumparampil",
            "Sewoong Oh",
            "Niao He"
        ],
        "published": "2023-10-14T18:42:56Z",
        "summary": "The widespread practice of fine-tuning large language models (LLMs) on\ndomain-specific data faces two major challenges in memory and privacy. First,\nas the size of LLMs continues to grow, the memory demands of gradient-based\ntraining methods via backpropagation become prohibitively high. Second, given\nthe tendency of LLMs to memorize training data, it is important to protect\npotentially sensitive information in the fine-tuning data from being\nregurgitated. Zeroth-order methods, which rely solely on forward passes,\nsubstantially reduce memory consumption during training. However, directly\ncombining them with standard differentially private gradient descent suffers\nfrom growing model size. To bridge this gap, we introduce DPZero, a novel\nprivate zeroth-order algorithm with nearly dimension-independent rates. The\nmemory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa on\nsix downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.09639v2.pdf"
    },
    {
        "title": "Autonomous Tree-search Ability of Large Language Models",
        "authors": [
            "Zheyu Zhang",
            "Zhuorui Ye",
            "Yikang Shen",
            "Chuang Gan"
        ],
        "published": "2023-10-14T14:14:38Z",
        "summary": "Large Language Models have excelled in remarkable reasoning capabilities with\nadvanced prompting techniques, but they fall short on tasks that require\nexploration, strategic foresight, and sequential decision-making. Recent works\npropose to utilize external programs to define search logic, such that LLMs can\nperform passive tree search to solve more challenging reasoning tasks. Though\nimpressive results have been achieved, there are several fundamental\nlimitations of these approaches. First, passive tree searches are not efficient\nas they usually require multiple rounds of LLM API calls to solve one single\nproblem. Moreover, passive search methods are not flexible since they need\ntask-specific program designs. Then a natural question arises: can we maintain\nthe tree-search capability of LLMs without the aid of external programs, and\ncan still generate responses that clearly demonstrate the process of a\ntree-structure search? To this end, we propose a new concept called autonomous\ntree-search ability of LLM, which can automatically generate a response\ncontaining search trajectories for the correct answer. Concretely, we perform\nsearch trajectories using capable LLM API via a fixed system prompt, allowing\nthem to perform autonomous tree-search (ATS) right out of the box. Experiments\non 4 puzzle games demonstrate our method can achieve huge improvements. The\nATS-BFS method outperforms the Chain of Thought approach by achieving an\naverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires\n65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.\nMoreover, we have collected data using the ATS prompt method and fine-tuned\nLLaMA. This approach yield a greater improvement compared to the ones\nfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an\naverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.10686v1.pdf"
    },
    {
        "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
        "authors": [
            "Md Rashad Al Hasan Rony",
            "Christian Suess",
            "Sinchana Ramakanth Bhat",
            "Viju Sudhi",
            "Julia Schneider",
            "Maximilian Vogel",
            "Roman Teucher",
            "Ken E. Friedl",
            "Soumya Sahoo"
        ],
        "published": "2023-10-14T08:46:24Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance by\nfollowing natural language instructions without fine-tuning them on\ndomain-specific tasks and data. However, leveraging LLMs for domain-specific\nquestion answering suffers from severe limitations. The generated answer tends\nto hallucinate due to the training data collection time (when using\noff-the-shelf), complex user utterance and wrong retrieval (in\nretrieval-augmented generation). Furthermore, due to the lack of awareness\nabout the domain and expected output, such LLMs may generate unexpected and\nunsafe answers that are not tailored to the target domain. In this paper, we\npropose CarExpert, an in-car retrieval-augmented conversational\nquestion-answering system leveraging LLMs for different tasks. Specifically,\nCarExpert employs LLMs to control the input, provide domain-specific documents\nto the extractive and generative answering components, and controls the output\nto ensure safe and domain-specific answers. A comprehensive empirical\nevaluation exhibits that CarExpert outperforms state-of-the-art LLMs in\ngenerating natural, safe and car-specific answers.",
        "pdf_link": "https://arxiv.org/pdf/2310.09536v1.pdf"
    },
    {
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
        "authors": [
            "Haikang Deng",
            "Colin Raffel"
        ],
        "published": "2023-10-14T07:19:47Z",
        "summary": "While large language models have proven effective in a huge range of\ndownstream applications, they often generate text that is problematic or lacks\na desired attribute. In this paper, we introduce Reward-Augmented Decoding\n(RAD), a text generation procedure that uses a small unidirectional reward\nmodel to encourage a language model to generate text that has certain\nproperties. Specifically, RAD uses the reward model to score generations as\nthey are produced and rescales sampling probabilities to favor high-reward\ntokens. By using a unidirectional reward model, RAD can cache activations from\nprior generation steps to decrease computational overhead. Through experiments\non generating non-toxic and sentiment-controlled text, we demonstrate that RAD\nperforms best among methods that change only the generation procedure and\nmatches the performance of state-of-the-art methods that involve re-training\nthe language model. We further validate that RAD is effective on very large\nlanguage models while incurring a minimal computational overhead.",
        "pdf_link": "https://arxiv.org/pdf/2310.09520v4.pdf"
    },
    {
        "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models",
        "authors": [
            "Hang Shao",
            "Bei Liu",
            "Bo Xiao",
            "Ke Zeng",
            "Guanglu Wan",
            "Yanmin Qian"
        ],
        "published": "2023-10-14T05:43:09Z",
        "summary": "Various Large Language Models(LLMs) from the Generative Pretrained\nTransformer(GPT) family have achieved outstanding performances in a wide range\nof text generation tasks. However, the enormous model sizes have hindered their\npractical use in real-world applications due to high inference latency.\nTherefore, improving the efficiencies of LLMs through quantization, pruning,\nand other means has been a key issue in LLM studies. In this work, we propose a\nmethod based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs\nto at least 50% sparsity without the need of any retraining. It allocates\nsparsity adaptively based on sensitivity, allowing us to reduce pruning-induced\nerror while maintaining the overall sparsity level. The advantages of the\nproposed method exhibit even more when the sparsity is extremely high.\nFurthermore, our method is compatible with quantization, enabling further\ncompression of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.09499v3.pdf"
    },
    {
        "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
        "authors": [
            "Shengyao Zhuang",
            "Honglei Zhuang",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2023-10-14T05:20:02Z",
        "summary": "Large Language Models (LLMs) demonstrate impressive effectiveness in\nzero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting\napproaches have been proposed for LLM-based zero-shot ranking. Our study begins\nby thoroughly evaluating these existing approaches within a consistent\nexperimental framework, considering factors like model size, token consumption,\nlatency, among others. This first-of-its-kind comparative evaluation of these\napproaches allows us to identify the trade-offs between effectiveness and\nefficiency inherent in each approach. We find that while Pointwise approaches\nscore high on efficiency, they suffer from poor effectiveness. Conversely,\nPairwise approaches demonstrate superior effectiveness but incur high\ncomputational overhead. To further enhance the efficiency of LLM-based\nzero-shot ranking, we propose a novel Setwise prompting approach. Our approach\nreduces the number of LLM inferences and the amount of prompt token consumption\nduring the ranking procedure, significantly improving the efficiency of\nLLM-based zero-shot ranking. We test our method using the TREC DL datasets and\nthe BEIR zero-shot document ranking benchmark. The empirical results indicate\nthat our approach considerably reduces computational costs while also retaining\nhigh zero-shot ranking effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2310.09497v1.pdf"
    },
    {
        "title": "Large Language Model Unlearning",
        "authors": [
            "Yuanshun Yao",
            "Xiaojun Xu",
            "Yang Liu"
        ],
        "published": "2023-10-14T00:32:55Z",
        "summary": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors,\non large language models (LLMs). We show at least three scenarios of aligning\nLLMs with human preferences can benefit from unlearning: (1) removing harmful\nresponses, (2) erasing copyright-protected content as requested, and (3)\nreducing hallucinations. Unlearning, as an alignment technique, has three\nadvantages. (1) It only requires negative (e.g. harmful) examples, which are\nmuch easier and cheaper to collect (e.g. via red teaming or user reporting)\nthan positive (e.g. helpful and often human-written) examples required in RLHF\n(RL from human feedback). (2) It is computationally efficient. (3) It is\nespecially effective when we know which training samples cause the misbehavior.\nTo the best of our knowledge, our work is among the first to explore LLM\nunlearning. We are also among the first to formulate the settings, goals, and\nevaluations in LLM unlearning. We show that if practitioners only have limited\nresources, and therefore the priority is to stop generating undesirable outputs\nrather than to try to generate desirable outputs, unlearning is particularly\nappealing. Despite only having negative samples, our ablation study shows that\nunlearning can still achieve better alignment performance than RLHF with just\n2% of its computational time.",
        "pdf_link": "https://arxiv.org/pdf/2310.10683v2.pdf"
    },
    {
        "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents",
        "authors": [
            "Hyungjoo Chae",
            "Yongho Song",
            "Kai Tzu-iunn Ong",
            "Taeyoon Kwon",
            "Minjin Kim",
            "Youngjae Yu",
            "Dongha Lee",
            "Dongyeop Kang",
            "Jinyoung Yeo"
        ],
        "published": "2023-10-13T18:17:23Z",
        "summary": "Human-like chatbots necessitate the use of commonsense reasoning in order to\neffectively comprehend and respond to implicit information present within\nconversations. Achieving such coherence and informativeness in responses,\nhowever, is a non-trivial task. Even for large language models (LLMs), the task\nof identifying and aggregating key evidence within a single hop presents a\nsubstantial challenge. This complexity arises because such evidence is\nscattered across multiple turns in a conversation, thus necessitating\nintegration over multiple hops. Hence, our focus is to facilitate such\nmulti-hop reasoning over a dialogue context, namely dialogue chain-of-thought\n(CoT) reasoning. To this end, we propose a knowledge distillation framework\nthat leverages LLMs as unreliable teachers and selectively distills consistent\nand helpful rationales via alignment filters. We further present DOCTOR, a\nDialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for\nresponse generation. We conduct extensive experiments to show that enhancing\ndialogue agents with high-quality rationales from DOCTOR significantly improves\nthe quality of their responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.09343v2.pdf"
    },
    {
        "title": "Ranking LLM-Generated Loop Invariants for Program Verification",
        "authors": [
            "Saikat Chakraborty",
            "Shuvendu K. Lahiri",
            "Sarah Fakhoury",
            "Madanlal Musuvathi",
            "Akash Lal",
            "Aseem Rastogi",
            "Aditya Senthilnathan",
            "Rahul Sharma",
            "Nikhil Swamy"
        ],
        "published": "2023-10-13T18:13:52Z",
        "summary": "Synthesizing inductive loop invariants is fundamental to automating program\nverification. In this work, we observe that Large Language Models (such as\ngpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of\nprograms in a 0-shot setting, yet require several samples to generate the\ncorrect invariants. This can lead to a large number of calls to a program\nverifier to establish an invariant. To address this issue, we propose a {\\it\nre-ranking} approach for the generated results of LLMs. We have designed a\nranker that can distinguish between correct inductive invariants and incorrect\nattempts based on the problem definition. The ranker is optimized as a\ncontrastive ranker. Experimental results demonstrate that this re-ranking\nmechanism significantly improves the ranking of correct invariants among the\ngenerated candidates, leading to a notable reduction in the number of calls to\na verifier. The source code and the experimental data for this paper are\navailable in \\url{https://github.com/microsoft/NeuralInvariantRanker}.",
        "pdf_link": "https://arxiv.org/pdf/2310.09342v3.pdf"
    },
    {
        "title": "User Inference Attacks on Large Language Models",
        "authors": [
            "Nikhil Kandpal",
            "Krishna Pillutla",
            "Alina Oprea",
            "Peter Kairouz",
            "Christopher A. Choquette-Choo",
            "Zheng Xu"
        ],
        "published": "2023-10-13T17:24:52Z",
        "summary": "Fine-tuning is a common and effective method for tailoring large language\nmodels (LLMs) to specialized tasks and applications. In this paper, we study\nthe privacy implications of fine-tuning LLMs on user data. To this end, we\nconsider a realistic threat model, called user inference, wherein an attacker\ninfers whether or not a user's data was used for fine-tuning. We design attacks\nfor performing user inference that require only black-box access to the\nfine-tuned LLM and a few samples from a user which need not be from the\nfine-tuning dataset. We find that LLMs are susceptible to user inference across\na variety of fine-tuning datasets, at times with near perfect attack success\nrates. Further, we theoretically and empirically investigate the properties\nthat make users vulnerable to user inference, finding that outlier users, users\nwith identifiable shared features between examples, and users that contribute a\nlarge fraction of the fine-tuning data are most susceptible to attack. Based on\nthese findings, we identify several methods for mitigating user inference\nincluding training with example-level differential privacy, removing\nwithin-user duplicate examples, and reducing a user's contribution to the\ntraining data. While these techniques provide partial mitigation of user\ninference, we highlight the need to develop methods to fully protect fine-tuned\nLLMs against this privacy risk.",
        "pdf_link": "https://arxiv.org/pdf/2310.09266v2.pdf"
    },
    {
        "title": "PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming",
        "authors": [
            "Chufan Gao",
            "Xulin Fan",
            "Jimeng Sun",
            "Xuan Wang"
        ],
        "published": "2023-10-13T17:23:17Z",
        "summary": "Relation extraction aims to classify the relationships between two entities\ninto pre-defined categories. While previous research has mainly focused on\nsentence-level relation extraction, recent studies have expanded the scope to\ndocument-level relation extraction. Traditional relation extraction methods\nheavily rely on human-annotated training data, which is time-consuming and\nlabor-intensive. To mitigate the need for manual annotation, recent\nweakly-supervised approaches have been developed for sentence-level relation\nextraction while limited work has been done on document-level relation\nextraction. Weakly-supervised document-level relation extraction faces\nsignificant challenges due to an imbalanced number \"no relation\" instances and\nthe failure of directly probing pretrained large language models for document\nrelation extraction. To address these challenges, we propose PromptRE, a novel\nweakly-supervised document-level relation extraction method that combines\nprompting-based techniques with data programming. Furthermore, PromptRE\nincorporates the label distribution and entity types as prior knowledge to\nimprove the performance. By leveraging the strengths of both prompting and data\nprogramming, PromptRE achieves improved performance in relation classification\nand effectively handles the \"no relation\" problem. Experimental results on\nReDocRED, a benchmark dataset for document-level relation extraction,\ndemonstrate the superiority of PromptRE over baseline approaches.",
        "pdf_link": "https://arxiv.org/pdf/2310.09265v1.pdf"
    },
    {
        "title": "Table-GPT: Table-tuned GPT for Diverse Table Tasks",
        "authors": [
            "Peng Li",
            "Yeye He",
            "Dror Yashar",
            "Weiwei Cui",
            "Song Ge",
            "Haidong Zhang",
            "Danielle Rifinski Fainman",
            "Dongmei Zhang",
            "Surajit Chaudhuri"
        ],
        "published": "2023-10-13T17:20:56Z",
        "summary": "Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable\nabilities to follow diverse human instructions and perform a wide range of\ntasks. However, when probing language models using a range of basic\ntable-understanding tasks, we observe that today's language models are still\nsub-optimal in many table-related tasks, likely because they are pre-trained\npredominantly on \\emph{one-dimensional} natural-language texts, whereas\nrelational tables are \\emph{two-dimensional} objects.\n  In this work, we propose a new \"\\emph{table-tuning}\" paradigm, where we\ncontinue to train/fine-tune language models like GPT-3.5 and ChatGPT, using\ndiverse table-tasks synthesized from real tables as training data, with the\ngoal of enhancing language models' ability to understand tables and perform\ntable tasks. We show that our resulting Table-GPT models demonstrate (1) better\n\\emph{table-understanding} capabilities, by consistently outperforming the\nvanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout\nunseen tasks, and (2) strong \\emph{generalizability}, in its ability to respond\nto diverse human instructions to perform new table-tasks, in a manner similar\nto GPT-3.5 and ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.09263v1.pdf"
    },
    {
        "title": "\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
        "authors": [
            "Yixin Wan",
            "George Pu",
            "Jiao Sun",
            "Aparna Garimella",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023-10-13T16:12:57Z",
        "summary": "Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2310.09219v5.pdf"
    },
    {
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Tao Yang",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023-10-13T15:03:15Z",
        "summary": "Instruction-tuning can be substantially optimized through enhanced diversity,\nresulting in models capable of handling a broader spectrum of tasks. However,\nexisting data employed for such tuning often exhibit an inadequate coverage of\nindividual domains, limiting the scope for nuanced comprehension and\ninteractions within these areas. To address this deficiency, we propose\nExplore-Instruct, a novel approach to enhance the data coverage to be used in\ndomain-specific instruction-tuning through active exploration via Large\nLanguage Models (LLMs). Built upon representative domain use cases,\nExplore-Instruct explores a multitude of variations or possibilities by\nimplementing a search algorithm to obtain diversified and domain-focused\ninstruction-tuning data. Our data-centric analysis validates the effectiveness\nof this proposed approach in improving domain-specific instruction coverage.\nMoreover, our model's performance demonstrates considerable advancements over\nmultiple baselines, including those utilizing domain-specific data enhancement.\nOur findings offer a promising opportunity to improve instruction coverage,\nespecially in domain-specific contexts, thereby advancing the development of\nadaptable language models. Our code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/Explore-Instruct}.",
        "pdf_link": "https://arxiv.org/pdf/2310.09168v3.pdf"
    },
    {
        "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
        "authors": [
            "Peihua Mai",
            "Ran Yan",
            "Zhe Huang",
            "Youjia Yang",
            "Yan Pang"
        ],
        "published": "2023-10-13T14:17:33Z",
        "summary": "Large Language Models (LLMs) shows powerful capability in natural language\nunderstanding by capturing hidden semantics in vector space. This process\nenriches the value of the text embeddings for various downstream tasks, thereby\nfostering the Embedding-as-a-Service (EaaS) business model. However, the direct\ntransmission of text to servers poses a largely unaddressed risk of privacy\nleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an\ninnovative framework that split the model to execute the token embedding layer\non the client side at minimal computational cost. This allows the client to\nintroduce noise prior to transmitting the embeddings to the server, and\nsubsequently receive and denoise the perturbed output embeddings for downstream\ntasks. Our approach is designed for the inference stage of LLMs and requires no\nmodifications to the model parameters. Extensive experiments demonstrate SnD's\neffectiveness in optimizing the privacy-utility tradeoff across various LLM\narchitectures and diverse downstream tasks. The results reveal a significant\nperformance improvement under the same privacy budget compared to the baseline,\noffering clients a privacy-preserving solution for local privacy protection.",
        "pdf_link": "https://arxiv.org/pdf/2310.09130v2.pdf"
    },
    {
        "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
        "authors": [
            "Qichen Ye",
            "Junling Liu",
            "Dading Chong",
            "Peilin Zhou",
            "Yining Hua",
            "Andrew Liu"
        ],
        "published": "2023-10-13T13:17:03Z",
        "summary": "Integrating large language models (LLMs) into healthcare presents potential\nbut faces challenges. Directly pre-training LLMs for domains like medicine is\nresource-heavy and sometimes unfeasible. Sole reliance on Supervised\nFine-tuning (SFT) can result in overconfident predictions and may not tap into\ndomain specific insights. Addressing these challenges, we present a multi-stage\ntraining method combining Domain-specific Continued Pre-training (DCPT), SFT,\nand Direct Preference Optimization (DPO). A notable contribution of our study\nis the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing\nmedical question answering, plain texts, knowledge graphs, and dialogues,\nsegmented into three training stages. The medical LLM trained with our\npipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and\nSFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing\nBaichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores\n16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21.\nThis highlights the strength of our training approach in refining LLMs for\nmedical applications.",
        "pdf_link": "https://arxiv.org/pdf/2310.09089v1.pdf"
    },
    {
        "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
        "authors": [
            "Sehyun Choi",
            "Tianqing Fang",
            "Zhaowei Wang",
            "Yangqiu Song"
        ],
        "published": "2023-10-13T12:12:34Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable human-level natural\nlanguage generation capabilities. However, their potential to generate\nmisinformation, often called the hallucination problem, poses a significant\nrisk to their deployment. A common approach to address this issue is to\nretrieve relevant knowledge and fine-tune the LLM with the knowledge in its\ninput. Unfortunately, this method incurs high training costs and may cause\ncatastrophic forgetting for multi-tasking models. To overcome these\nlimitations, we propose a knowledge-constrained decoding method called KCTS\n(Knowledge-Constrained Tree Search), which guides a frozen LM to generate text\naligned with the reference knowledge at each decoding step using a knowledge\nclassifier score and MCTS (Monte-Carlo Tree Search). To adapt the\nsequence-level knowledge classifier to token-level guidance, we also propose a\nnovel token-level hallucination detection method called RIPA (Reward Inflection\nPoint Approximation). Our empirical results on knowledge-grounded dialogue and\nabstractive summarization demonstrate the strength of KCTS as a plug-and-play,\nmodel-agnostic decoding method that can effectively reduce hallucinations in\nnatural language generation.",
        "pdf_link": "https://arxiv.org/pdf/2310.09044v1.pdf"
    },
    {
        "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
        "authors": [
            "Hung Le",
            "Hailin Chen",
            "Amrita Saha",
            "Akash Gokul",
            "Doyen Sahoo",
            "Shafiq Joty"
        ],
        "published": "2023-10-13T10:17:48Z",
        "summary": "Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.",
        "pdf_link": "https://arxiv.org/pdf/2310.08992v3.pdf"
    },
    {
        "title": "Embarrassingly Simple Text Watermarks",
        "authors": [
            "Ryoma Sato",
            "Yuki Takezawa",
            "Han Bao",
            "Kenta Niwa",
            "Makoto Yamada"
        ],
        "published": "2023-10-13T07:44:05Z",
        "summary": "We propose Easymark, a family of embarrassingly simple yet effective\nwatermarks. Text watermarking is becoming increasingly important with the\nadvent of Large Language Models (LLM). LLMs can generate texts that cannot be\ndistinguished from human-written texts. This is a serious problem for the\ncredibility of the text. Easymark is a simple yet effective solution to this\nproblem. Easymark can inject a watermark without changing the meaning of the\ntext at all while a validator can detect if a text was generated from a system\nthat adopted Easymark or not with high credibility. Easymark is extremely easy\nto implement so that it only requires a few lines of code. Easymark does not\nrequire access to LLMs, so it can be implemented on the user-side when the LLM\nproviders do not offer watermarked LLMs. In spite of its simplicity, it\nachieves higher detection accuracy and BLEU scores than the state-of-the-art\ntext watermarking methods. We also prove the impossibility theorem of perfect\nwatermarking, which is valuable in its own right. This theorem shows that no\nmatter how sophisticated a watermark is, a malicious user could remove it from\nthe text, which motivate us to use a simple watermark such as Easymark. We\ncarry out experiments with LLM-generated texts and confirm that Easymark can be\ndetected reliably without any degradation of BLEU and perplexity, and\noutperform state-of-the-art watermarks in terms of both quality and\nreliability.",
        "pdf_link": "https://arxiv.org/pdf/2310.08920v1.pdf"
    },
    {
        "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
        "authors": [
            "Yuxin Zhang",
            "Lirui Zhao",
            "Mingbao Lin",
            "Yunyun Sun",
            "Yiwu Yao",
            "Xingjia Han",
            "Jared Tanner",
            "Shiwei Liu",
            "Rongrong Ji"
        ],
        "published": "2023-10-13T07:38:52Z",
        "summary": "The ever-increasing large language models (LLMs), though opening a potential\npath for the upcoming artificial general intelligence, sadly drops a daunting\nobstacle on the way towards their on-device deployment. As one of the most\nwell-established pre-LLMs approaches in reducing model complexity, network\npruning appears to lag behind in the era of LLMs, due mostly to its costly\nfine-tuning (or re-training) necessity under the massive volumes of model\nparameter and training data. To close this industry-academia gap, we introduce\nDynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that\nslightly updates sparse LLMs without the expensive backpropagation and any\nweight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the\nreconstruction error between the dense and sparse LLMs, in the fashion of\nperforming iterative weight pruning-and-growing on top of sparse LLMs. To\naccomplish this purpose, DSnoT particularly takes into account the anticipated\nreduction in reconstruction error for pruning and growing, as well as the\nvariance w.r.t. different input data for growing each weight. This practice can\nbe executed efficiently in linear time since its obviates the need of\nbackpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,\nVicuna, and OPT across various benchmarks demonstrate the effectiveness of\nDSnoT in enhancing the performance of sparse LLMs, especially at high sparsity\nlevels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by\n26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights\ninto how to fine-tune sparse LLMs in an efficient training-free manner and open\nnew venues to scale the great potential of sparsity to LLMs. Codes are\navailable at https://github.com/zyxxmu/DSnoT.",
        "pdf_link": "https://arxiv.org/pdf/2310.08915v3.pdf"
    },
    {
        "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
        "authors": [
            "Pengyu Wang",
            "Linyang Li",
            "Ke Ren",
            "Botian Jiang",
            "Dong Zhang",
            "Xipeng Qiu"
        ],
        "published": "2023-10-13T07:18:53Z",
        "summary": "Widely applied large language models (LLMs) can generate human-like content,\nraising concerns about the abuse of LLMs. Therefore, it is important to build\nstrong AI-generated text (AIGT) detectors. Current works only consider\ndocument-level AIGT detection, therefore, in this paper, we first introduce a\nsentence-level detection challenge by synthesizing a dataset that contains\ndocuments that are polished with LLMs, that is, the documents contain sentences\nwritten by humans and sentences modified by LLMs. Then we propose\n\\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes\nlog probability lists from white-box LLMs as features for sentence-level AIGT\ndetection. These features are composed like \\textit{waves} in speech processing\nand cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution\nand self-attention networks. We test it in both sentence and document-level\ndetection challenges. Experimental results show that previous methods struggle\nin solving sentence-level AIGT detection, while our method not only\nsignificantly surpasses baseline methods in both sentence and document-level\ndetection challenges but also exhibits strong generalization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2310.08903v2.pdf"
    },
    {
        "title": "Exploration with Principles for Diverse AI Supervision",
        "authors": [
            "Hao Liu",
            "Matei Zaharia",
            "Pieter Abbeel"
        ],
        "published": "2023-10-13T07:03:39Z",
        "summary": "Training large transformers using next-token prediction has given rise to\ngroundbreaking advancements in AI. While this generative AI approach has\nproduced impressive results, it heavily leans on human supervision. Even\nstate-of-the-art AI models like ChatGPT depend on fine-tuning through human\ndemonstrations, demanding extensive human input and domain expertise. This\nstrong reliance on human oversight poses a significant hurdle to the\nadvancement of AI innovation. To address this limitation, we propose a novel\nparadigm termed Exploratory AI (EAI) aimed at autonomously generating\nhigh-quality training data. Drawing inspiration from unsupervised reinforcement\nlearning (RL) pretraining, EAI achieves exploration within the natural language\nspace. We accomplish this by harnessing large language models to assess the\nnovelty of generated content. Our approach employs two key components: an actor\nthat generates novel content following exploration principles and a critic that\nevaluates the generated content, offering critiques to guide the actor.\nEmpirical evaluations demonstrate that EAI significantly boosts model\nperformance on complex reasoning tasks, addressing the limitations of\nhuman-intensive supervision.",
        "pdf_link": "https://arxiv.org/pdf/2310.08899v2.pdf"
    },
    {
        "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue",
        "authors": [
            "Hongru Wang",
            "Minda Hu",
            "Yang Deng",
            "Rui Wang",
            "Fei Mi",
            "Weichao Wang",
            "Yasheng Wang",
            "Wai-Chung Kwan",
            "Irwin King",
            "Kam-Fai Wong"
        ],
        "published": "2023-10-13T03:38:38Z",
        "summary": "Open-domain dialogue system usually requires different sources of knowledge\nto generate more informative and evidential responses. However, existing\nknowledge-grounded dialogue systems either focus on a single knowledge source\nor overlook the dependency between multiple sources of knowledge, which may\nresult in generating inconsistent or even paradoxical responses. To incorporate\nmultiple knowledge sources and dependencies between them, we propose SAFARI, a\nnovel framework that leverages the exceptional capabilities of large language\nmodels (LLMs) in planning, understanding, and incorporating under both\nsupervised and unsupervised settings. Specifically, SAFARI decouples the\nknowledge grounding into multiple sources and response generation, which allows\neasy extension to various knowledge sources including the possibility of not\nusing any sources. To study the problem, we construct a personalized\nknowledge-grounded dialogue dataset \\textit{\\textbf{K}nowledge \\textbf{B}ehind\n\\textbf{P}ersona}~(\\textbf{KBP}), which is the first to consider the dependency\nbetween persona and implicit knowledge. Experimental results on the KBP dataset\ndemonstrate that the SAFARI framework can effectively produce\npersona-consistent and knowledge-enhanced responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.08840v1.pdf"
    },
    {
        "title": "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
        "authors": [
            "Harsh Kumar",
            "Ilya Musabirov",
            "Mohi Reza",
            "Jiakai Shi",
            "Xinyuan Wang",
            "Joseph Jay Williams",
            "Anastasia Kuzminykh",
            "Michael Liut"
        ],
        "published": "2023-10-13T01:21:52Z",
        "summary": "Personalized chatbot-based teaching assistants can be crucial in addressing\nincreasing classroom sizes, especially where direct teacher presence is\nlimited. Large language models (LLMs) offer a promising avenue, with increasing\nresearch exploring their educational utility. However, the challenge lies not\nonly in establishing the efficacy of LLMs but also in discerning the nuances of\ninteraction between learners and these models, which impact learners'\nengagement and results. We conducted a formative study in an undergraduate\ncomputer science classroom (N=145) and a controlled experiment on Prolific\n(N=356) to explore the impact of four pedagogically informed guidance\nstrategies on the learners' performance, confidence and trust in LLMs. Direct\nLLM answers marginally improved performance, while refining student solutions\nfostered trust. Structured guidance reduced random queries as well as instances\nof students copy-pasting assignment questions to the LLM. Our work highlights\nthe role that teachers can play in shaping LLM-supported learning environments.",
        "pdf_link": "https://arxiv.org/pdf/2310.13712v2.pdf"
    },
    {
        "title": "End-to-end Story Plot Generator",
        "authors": [
            "Hanlin Zhu",
            "Andrew Cohen",
            "Danqing Wang",
            "Kevin Yang",
            "Xiaomeng Yang",
            "Jiantao Jiao",
            "Yuandong Tian"
        ],
        "published": "2023-10-13T00:49:59Z",
        "summary": "Story plots, while short, carry most of the essential information of a full\nstory that may contain tens of thousands of words. We study the problem of\nautomatic generation of story plots, which includes story premise, character\ndescriptions, plot outlines, etc. To generate a single engaging plot, existing\nplot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands\nof calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot,\nwhich is costly and takes at least several minutes. Moreover, the hard-wired\nnature of the method makes the pipeline non-differentiable, blocking fast\nspecialization and personalization of the plot generator. In this paper, we\npropose three models, $\\texttt{OpenPlot}$, $\\texttt{E2EPlot}$ and\n$\\texttt{RLPlot}$, to address these challenges. $\\texttt{OpenPlot}$ replaces\nexpensive OpenAI API calls with LLaMA2 (Touvron et al., 2023) calls via careful\nprompt designs, which leads to inexpensive generation of high-quality training\ndatasets of story plots. We then train an end-to-end story plot generator,\n$\\texttt{E2EPlot}$, by supervised fine-tuning (SFT) using approximately 13000\nstory plots generated by $\\texttt{OpenPlot}$. $\\texttt{E2EPlot}$ generates\nstory plots of comparable quality to $\\texttt{OpenPlot}$, and is > 10$\\times$\nfaster (1k tokens in only 30 seconds on average). Finally, we obtain\n$\\texttt{RLPlot}$ that is further fine-tuned with RLHF on several different\nreward models for different aspects of story quality, which yields 60.0$\\%$\nwinning rate against $\\texttt{E2EPlot}$ along the aspect of suspense and\nsurprise.",
        "pdf_link": "https://arxiv.org/pdf/2310.08796v1.pdf"
    },
    {
        "title": "\"Im not Racist but...\": Discovering Bias in the Internal Knowledge of Large Language Models",
        "authors": [
            "Abel Salinas",
            "Louis Penafiel",
            "Robert McCormack",
            "Fred Morstatter"
        ],
        "published": "2023-10-13T00:03:37Z",
        "summary": "Large language models (LLMs) have garnered significant attention for their\nremarkable performance in a continuously expanding set of natural language\nprocessing tasks. However, these models have been shown to harbor inherent\nsocietal biases, or stereotypes, which can adversely affect their performance\nin their many downstream applications. In this paper, we introduce a novel,\npurely prompt-based approach to uncover hidden stereotypes within any arbitrary\nLLM. Our approach dynamically generates a knowledge representation of internal\nstereotypes, enabling the identification of biases encoded within the LLM's\ninternal knowledge. By illuminating the biases present in LLMs and offering a\nsystematic methodology for their analysis, our work contributes to advancing\ntransparency and promoting fairness in natural language processing systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.08780v1.pdf"
    },
    {
        "title": "Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving",
        "authors": [
            "Karen D. Wang",
            "Eric Burkholder",
            "Carl Wieman",
            "Shima Salehi",
            "Nick Haber"
        ],
        "published": "2023-10-12T23:39:28Z",
        "summary": "The study explores the capabilities of OpenAI's ChatGPT in solving different\ntypes of physics problems. ChatGPT (with GPT-4) was queried to solve a total of\n40 problems from a college-level engineering physics course. These problems\nranged from well-specified problems, where all data required for solving the\nproblem was provided, to under-specified, real-world problems where not all\nnecessary data were given. Our findings show that ChatGPT could successfully\nsolve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for\nunder-specified problems. Analysis of the model's incorrect solutions revealed\nthree distinct failure modes: 1) failure to construct accurate models of the\nphysical world, 2) failure to make reasonable assumptions about missing data,\nand 3) calculation errors. The study offers implications for how to leverage\nLLM-augmented instructional materials to enhance STEM education. The insights\nalso contribute to the broader discourse on AI's strengths and limitations,\nserving both educators aiming to leverage the technology and researchers\ninvestigating human-AI collaboration frameworks for problem-solving and\ndecision-making.",
        "pdf_link": "https://arxiv.org/pdf/2310.08773v2.pdf"
    },
    {
        "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
        "authors": [
            "Yixiao Li",
            "Yifan Yu",
            "Chen Liang",
            "Pengcheng He",
            "Nikos Karampatziakis",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2023-10-12T18:34:08Z",
        "summary": "Quantization is an indispensable technique for serving Large Language Models\n(LLMs) and has recently found its way into LoRA fine-tuning. In this work we\nfocus on the scenario where quantization and LoRA fine-tuning are applied\ntogether on a pre-trained model. In such cases it is common to observe a\nconsistent gap in the performance on downstream tasks between full fine-tuning\nand quantization plus LoRA fine-tuning approach. In response, we propose LoftQ\n(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that\nsimultaneously quantizes an LLM and finds a proper low-rank initialization for\nLoRA fine-tuning. Such an initialization alleviates the discrepancy between the\nquantized and full-precision model and significantly improves generalization in\ndownstream tasks. We evaluate our method on natural language understanding,\nquestion answering, summarization, and natural language generation tasks.\nExperiments show that our method is highly effective and outperforms existing\nquantization methods, especially in the challenging 2-bit and 2/4-bit mixed\nprecision regimes. The code is available on https://github.com/yxli2123/LoftQ.",
        "pdf_link": "https://arxiv.org/pdf/2310.08659v4.pdf"
    },
    {
        "title": "MemGPT: Towards LLMs as Operating Systems",
        "authors": [
            "Charles Packer",
            "Sarah Wooders",
            "Kevin Lin",
            "Vivian Fang",
            "Shishir G. Patil",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "published": "2023-10-12T17:51:32Z",
        "summary": "Large language models (LLMs) have revolutionized AI, but are constrained by\nlimited context windows, hindering their utility in tasks like extended\nconversations and document analysis. To enable using context beyond limited\ncontext windows, we propose virtual context management, a technique drawing\ninspiration from hierarchical memory systems in traditional operating systems\nthat provide the appearance of large memory resources through data movement\nbetween fast and slow memory. Using this technique, we introduce MemGPT\n(Memory-GPT), a system that intelligently manages different memory tiers in\norder to effectively provide extended context within the LLM's limited context\nwindow, and utilizes interrupts to manage control flow between itself and the\nuser. We evaluate our OS-inspired design in two domains where the limited\ncontext windows of modern LLMs severely handicaps their performance: document\nanalysis, where MemGPT is able to analyze large documents that far exceed the\nunderlying LLM's context window, and multi-session chat, where MemGPT can\ncreate conversational agents that remember, reflect, and evolve dynamically\nthrough long-term interactions with their users. We release MemGPT code and\ndata for our experiments at https://memgpt.ai.",
        "pdf_link": "https://arxiv.org/pdf/2310.08560v2.pdf"
    },
    {
        "title": "Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?",
        "authors": [
            "Lingfeng Shen",
            "Aayush Mishra",
            "Daniel Khashabi"
        ],
        "published": "2023-10-12T17:32:09Z",
        "summary": "The emergence of In-Context Learning (ICL) in LLMs remains a significant\nphenomenon with little understanding. To explain ICL, recent studies try to\ntheoretically connect it to Gradient Descent (GD). We ask, does this connection\nhold up in actual pre-trained models?\n  We highlight the limiting assumptions in prior works that make their context\nconsiderably different from the practical context in which language models are\ntrained. For example, the theoretical hand-constructed weights used in these\nstudies have properties that don't match those of real LLMs. Furthermore, their\nexperimental verification uses ICL objective (training models explicitly for\nICL), which differs from the emergent ICL in the wild.\n  We also look for evidence in real models. We observe that ICL and GD have\ndifferent sensitivity to the order in which they observe demonstrations.\nFinally, we probe and compare the ICL vs. GD hypothesis in a natural setting.\nWe conduct comprehensive empirical analyses on language models pre-trained on\nnatural data (LLaMa-7B). Our comparisons of three performance metrics highlight\nthe inconsistent behavior of ICL and GD as a function of various factors such\nas datasets, models, and the number of demonstrations. We observe that ICL and\nGD modify the output distribution of language models differently. These results\nindicate that the equivalence between ICL and GD remains an open hypothesis and\ncalls for further studies.",
        "pdf_link": "https://arxiv.org/pdf/2310.08540v4.pdf"
    },
    {
        "title": "LLM-augmented Preference Learning from Natural Language",
        "authors": [
            "Inwon Kang",
            "Sikai Ruan",
            "Tyler Ho",
            "Jui-Chien Lin",
            "Farhad Mohsin",
            "Oshani Seneviratne",
            "Lirong Xia"
        ],
        "published": "2023-10-12T17:17:27Z",
        "summary": "Finding preferences expressed in natural language is an important but\nchallenging task. State-of-the-art(SotA) methods leverage transformer-based\nmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graph\nattention networks. Since Large Language Models (LLMs) are equipped to deal\nwith larger context lengths and have much larger model sizes than the\ntransformer-based model, we investigate their ability to classify comparative\ntext directly. This work aims to serve as a first step towards using LLMs for\nthe CPC task. We design and conduct a set of experiments that format the\nclassification task into an input prompt for the LLM and a methodology to get a\nfixed-format response that can be automatically evaluated. Comparing\nperformances with existing methods, we see that pre-trained LLMs are able to\noutperform the previous SotA models with no fine-tuning involved. Our results\nshow that the LLMs can consistently outperform the SotA when the target text is\nlarge -- i.e. composed of multiple sentences --, and are still comparable to\nthe SotA performance in shorter text. We also find that few-shot learning\nyields better performance than zero-shot learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.08523v1.pdf"
    },
    {
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
        "authors": [
            "Seungone Kim",
            "Jamin Shin",
            "Yejin Cho",
            "Joel Jang",
            "Shayne Longpre",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seongjin Shin",
            "Sungdong Kim",
            "James Thorne",
            "Minjoon Seo"
        ],
        "published": "2023-10-12T16:50:08Z",
        "summary": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://kaistai.github.io/prometheus/.",
        "pdf_link": "https://arxiv.org/pdf/2310.08491v2.pdf"
    },
    {
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
        "authors": [
            "Patrick Chao",
            "Alexander Robey",
            "Edgar Dobriban",
            "Hamed Hassani",
            "George J. Pappas",
            "Eric Wong"
        ],
        "published": "2023-10-12T15:38:28Z",
        "summary": "There is growing interest in ensuring that large language models (LLMs) align\nwith human values. However, the alignment of such models is vulnerable to\nadversarial jailbreaks, which coax LLMs into overriding their safety\nguardrails. The identification of these vulnerabilities is therefore\ninstrumental in understanding inherent weaknesses and preventing future misuse.\nTo this end, we propose Prompt Automatic Iterative Refinement (PAIR), an\nalgorithm that generates semantic jailbreaks with only black-box access to an\nLLM. PAIR -- which is inspired by social engineering attacks -- uses an\nattacker LLM to automatically generate jailbreaks for a separate targeted LLM\nwithout human intervention. In this way, the attacker LLM iteratively queries\nthe target LLM to update and refine a candidate jailbreak. Empirically, PAIR\noften requires fewer than twenty queries to produce a jailbreak, which is\norders of magnitude more efficient than existing algorithms. PAIR also achieves\ncompetitive jailbreaking success rates and transferability on open and\nclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
        "pdf_link": "https://arxiv.org/pdf/2310.08419v2.pdf"
    },
    {
        "title": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
        "authors": [
            "Ondrej Skopek",
            "Rahul Aralikatte",
            "Sian Gooding",
            "Victor Carbune"
        ],
        "published": "2023-10-12T15:07:11Z",
        "summary": "Despite recent advances, evaluating how well large language models (LLMs)\nfollow user instructions remains an open problem. While evaluation methods of\nlanguage models have seen a rise in prompt-based approaches, limited work on\nthe correctness of these methods has been conducted. In this work, we perform a\nmeta-evaluation of a variety of metrics to quantify how accurately they measure\nthe instruction-following abilities of LLMs. Our investigation is performed on\ngrounded query-based summarization by collecting a new short-form, real-world\ndataset riSum, containing 300 document-instruction pairs with 3 answers each.\nAll 900 answers are rated by 3 human annotators. Using riSum, we analyze the\nagreement between evaluation methods and human judgment. Finally, we propose\nnew LLM-based reference-free evaluation methods that improve upon established\nbaselines and perform on par with costly reference-based metrics that require\nhigh-quality summaries.",
        "pdf_link": "https://arxiv.org/pdf/2310.08394v2.pdf"
    },
    {
        "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
        "authors": [
            "Cheongwoong Kang",
            "Jaesik Choi"
        ],
        "published": "2023-10-12T12:01:32Z",
        "summary": "Large language models (LLMs) often make factually incorrect responses despite\ntheir success in various applications. In this paper, we hypothesize that\nrelying heavily on simple co-occurrence statistics of the pre-training corpora\nis one of the main factors that cause factual errors. Our results reveal that\nLLMs are vulnerable to the co-occurrence bias, defined as preferring frequently\nco-occurred words over the correct answer. Consequently, LLMs struggle to\nrecall facts whose subject and object rarely co-occur in the pre-training\ndataset although they are seen during finetuning. We show that co-occurrence\nbias remains despite scaling up model sizes or finetuning. Therefore, we\nsuggest finetuning on a debiased dataset to mitigate the bias by filtering out\nbiased samples whose subject-object co-occurrence count is high. Although\ndebiased finetuning allows LLMs to memorize rare facts in the training set, it\nis not effective in recalling rare facts unseen during finetuning. Further\nresearch in mitigation will help build reliable language models by preventing\npotential errors. The code is available at\n\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.",
        "pdf_link": "https://arxiv.org/pdf/2310.08256v1.pdf"
    },
    {
        "title": "Large language models can replicate cross-cultural differences in personality",
        "authors": [
            "Pawe\u0142 Niszczota",
            "Mateusz Janczak"
        ],
        "published": "2023-10-12T11:17:23Z",
        "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. Overall, we provide preliminary\nevidence that LLMs can aid cross-cultural psychological research.",
        "pdf_link": "https://arxiv.org/pdf/2310.10679v1.pdf"
    },
    {
        "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
        "authors": [
            "Chia-Yu Hung",
            "Zhiqiang Hu",
            "Yujia Hu",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-10-12T08:24:15Z",
        "summary": "Authorship verification (AV) is a fundamental task in natural language\nprocessing (NLP) and computational linguistics, with applications in forensic\nanalysis, plagiarism detection, and identification of deceptive content.\nExisting AV techniques, including traditional stylometric and deep learning\napproaches, face limitations in terms of data requirements and lack of\nexplainability. To address these limitations, this paper proposes PromptAV, a\nnovel technique that leverages Large-Language Models (LLMs) for AV by providing\nstep-by-step stylometric explanation prompts. PromptAV outperforms\nstate-of-the-art baselines, operates effectively with limited training data,\nand enhances interpretability through intuitive explanations, showcasing its\npotential as an effective and interpretable solution for the AV task.",
        "pdf_link": "https://arxiv.org/pdf/2310.08123v1.pdf"
    },
    {
        "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Subbarao Kambhampati"
        ],
        "published": "2023-10-12T08:22:37Z",
        "summary": "There have been widespread claims about Large Language Models (LLMs) being\nable to successfully verify or self-critique their candidate solutions in\nreasoning problems in an iterative mode. Intrigued by those claims, in this\npaper we set out to investigate the verification/self-critiquing abilities of\nlarge language models in the context of planning. We evaluate a planning system\nthat employs LLMs for both plan generation and verification. We assess the\nverifier LLM's performance against ground-truth verification, the impact of\nself-critiquing on plan generation, and the influence of varying feedback\nlevels on system performance. Using GPT-4, a state-of-the-art LLM, for both\ngeneration and verification, our findings reveal that self-critiquing appears\nto diminish plan generation performance, especially when compared to systems\nwith external, sound verifiers and the LLM verifiers in that system produce a\nnotable number of false positives, compromising the system's reliability.\nAdditionally, the nature of feedback, whether binary or detailed, showed\nminimal impact on plan generation. Collectively, our results cast doubt on the\neffectiveness of LLMs in a self-critiquing, iterative framework for planning\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.08118v1.pdf"
    },
    {
        "title": "QASiNa: Religious Domain Question Answering using Sirah Nabawiyah",
        "authors": [
            "Muhammad Razif Rizqullah",
            "Ayu Purwarianti",
            "Alham Fikri Aji"
        ],
        "published": "2023-10-12T07:52:19Z",
        "summary": "Nowadays, Question Answering (QA) tasks receive significant research focus,\nparticularly with the development of Large Language Model (LLM) such as Chat\nGPT [1]. LLM can be applied to various domains, but it contradicts the\nprinciples of information transmission when applied to the Islamic domain. In\nIslam we strictly regulates the sources of information and who can give\ninterpretations or tafseer for that sources [2]. The approach used by LLM to\ngenerate answers based on its own interpretation is similar to the concept of\ntafseer, LLM is neither an Islamic expert nor a human which is not permitted in\nIslam. Indonesia is the country with the largest Islamic believer population in\nthe world [3]. With the high influence of LLM, we need to make evaluation of\nLLM in religious domain. Currently, there is only few religious QA dataset\navailable and none of them using Sirah Nabawiyah especially in Indonesian\nLanguage. In this paper, we propose the Question Answering Sirah Nabawiyah\n(QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in\nIndonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5],\nand IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0\n[7]. XLM-R model returned the best performance on QASiNa with EM of 61.20,\nF1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance\nwith Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and\nF1-Score with higher Substring Match, the gap of EM and Substring Match get\nwider in GPT-4. The experiment indicate that Chat GPT tends to give excessive\ninterpretations as evidenced by its higher Substring Match scores compared to\nEM and F1-Score, even after providing instruction and context. This concludes\nChat GPT is unsuitable for question answering task in religious domain\nespecially for Islamic religion.",
        "pdf_link": "https://arxiv.org/pdf/2310.08102v1.pdf"
    },
    {
        "title": "Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques",
        "authors": [
            "Junxiao Shen",
            "John J. Dudley",
            "Jingyao Zheng",
            "Bill Byrne",
            "Per Ola Kristensson"
        ],
        "published": "2023-10-12T07:51:43Z",
        "summary": "Text entry is an essential task in our day-to-day digital interactions.\nNumerous intelligent features have been developed to streamline this process,\nmaking text entry more effective, efficient, and fluid. These improvements\ninclude sentence prediction and user personalization. However, as deep\nlearning-based language models become the norm for these advanced features, the\nnecessity for data collection and model fine-tuning increases. These challenges\ncan be mitigated by harnessing the in-context learning capability of large\nlanguage models such as GPT-3.5. This unique feature allows the language model\nto acquire new skills through prompts, eliminating the need for data collection\nand fine-tuning. Consequently, large language models can learn various text\nprediction techniques. We initially showed that, for a sentence prediction\ntask, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is\ncomparable with a fine-tuned GPT-3.5 model, with the latter two methods\nrequiring costly data collection, fine-tuning and post-processing. However, the\ntask of prompting large language models to specialize in specific text\nprediction tasks can be challenging, particularly for designers without\nexpertise in prompt engineering. To address this, we introduce Promptor, a\nconversational prompt generation agent designed to engage proactively with\ndesigners. Promptor can automatically generate complex prompts tailored to meet\nspecific needs, thus offering a solution to this challenge. We conducted a user\nstudy involving 24 participants creating prompts for three intelligent text\nentry tasks, half of the participants used Promptor while the other half\ndesigned prompts themselves. The results show that Promptor-designed prompts\nresult in a 35% increase in similarity and 22% in coherence over those by\ndesigners.",
        "pdf_link": "https://arxiv.org/pdf/2310.08101v2.pdf"
    },
    {
        "title": "GameGPT: Multi-agent Collaborative Framework for Game Development",
        "authors": [
            "Dake Chen",
            "Hanbin Wang",
            "Yunhao Huo",
            "Yuzhao Li",
            "Haoyang Zhang"
        ],
        "published": "2023-10-12T06:31:43Z",
        "summary": "The large language model (LLM) based agents have demonstrated their capacity\nto automate and expedite software development processes. In this paper, we\nfocus on game development and propose a multi-agent collaborative framework,\ndubbed GameGPT, to automate game development. While many studies have\npinpointed hallucination as a primary roadblock for deploying LLMs in\nproduction, we identify another concern: redundancy. Our framework presents a\nseries of methods to mitigate both concerns. These methods include dual\ncollaboration and layered approaches with several in-house lexicons, to\nmitigate the hallucination and redundancy in the planning, task identification,\nand implementation phases. Furthermore, a decoupling approach is also\nintroduced to achieve code generation with better precision.",
        "pdf_link": "https://arxiv.org/pdf/2310.08067v1.pdf"
    },
    {
        "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Kaisheng Zeng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-10-12T04:14:28Z",
        "summary": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2310.08027v1.pdf"
    },
    {
        "title": "Effects of Human Adversarial and Affable Samples on BERT Generalization",
        "authors": [
            "Aparna Elangovan",
            "Jiayuan He",
            "Yuan Li",
            "Karin Verspoor"
        ],
        "published": "2023-10-12T03:20:43Z",
        "summary": "BERT-based models have had strong performance on leaderboards, yet have been\ndemonstrably worse in real-world settings requiring generalization. Limited\nquantities of training data is considered a key impediment to achieving\ngeneralizability in machine learning. In this paper, we examine the impact of\ntraining data quality, not quantity, on a model's generalizability. We consider\ntwo characteristics of training data: the portion of human-adversarial\n(h-adversarial), i.e., sample pairs with seemingly minor differences but\ndifferent ground-truth labels, and human-affable (h-affable) training samples,\ni.e., sample pairs with minor differences but the same ground-truth label. We\nfind that for a fixed size of training samples, as a rule of thumb, having\n10-30% h-adversarial instances improves the precision, and therefore F1, by up\nto 20 points in the tasks of text classification and relation extraction.\nIncreasing h-adversarials beyond this range can result in performance plateaus\nor even degradation. In contrast, h-affables may not contribute to a model's\ngeneralizability and may even degrade generalization performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.08008v4.pdf"
    },
    {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "authors": [
            "Zhuoyan Li",
            "Hangxiao Zhu",
            "Zhuoran Lu",
            "Ming Yin"
        ],
        "published": "2023-10-11T19:51:13Z",
        "summary": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "pdf_link": "https://arxiv.org/pdf/2310.07849v2.pdf"
    },
    {
        "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph",
        "authors": [
            "Ruotong Liao",
            "Xu Jia",
            "Yunpu Ma",
            "Yangzhe Li",
            "Volker Tresp"
        ],
        "published": "2023-10-11T18:27:12Z",
        "summary": "The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional\nembedding-based and rule-based methods dominate. The question remains open of\nwhether pre-trained LLMs can understand structured temporal relational data and\nreplace them as the foundation model for temporal relational forecasting.\nTherefore, we bring temporal knowledge forecasting into the generative setting.\nHowever, challenges occur in the huge chasms between complex temporal graph\ndata structure and sequential natural expressions LLMs can handle, and between\nthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.\nTo address these challenges, we propose a novel retrieval-augmented generation\nframework named GenTKG combining a temporal logical rule-based retrieval\nstrategy and few-shot parameter-efficient instruction tuning to solve the above\nchallenges, respectively. Extensive experiments have shown that GenTKG\noutperforms conventional methods of temporal relational forecasting with low\ncomputation resources using extremely limited training data as few as 16\nsamples. GenTKG also highlights remarkable cross-domain generalizability with\noutperforming performance on unseen datasets without re-training, and in-domain\ngeneralizability regardless of time split in the same dataset. Our work reveals\nthe huge potential of LLMs in the tKG domain and opens a new frontier for\ngenerative forecasting on tKGs. Code and data are released here:\nhttps://github.com/mayhugotong/GenTKG.",
        "pdf_link": "https://arxiv.org/pdf/2310.07793v4.pdf"
    },
    {
        "title": "Composite Backdoor Attacks Against Large Language Models",
        "authors": [
            "Hai Huang",
            "Zhengyu Zhao",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2023-10-11T17:21:03Z",
        "summary": "Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work\nhighlights the necessity of increased security research on the trustworthiness\nof foundation LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.07676v2.pdf"
    },
    {
        "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
        "authors": [
            "Lang Qin",
            "Yao Zhang",
            "Hongru Liang",
            "Jun Wang",
            "Zhenglu Yang"
        ],
        "published": "2023-10-11T17:00:29Z",
        "summary": "Accurate knowledge selection is critical in knowledge-grounded dialogue\nsystems. Towards a closer look at it, we offer a novel perspective to organize\nexisting literature, i.e., knowledge selection coupled with, after, and before\ngeneration. We focus on the third under-explored category of study, which can\nnot only select knowledge accurately in advance, but has the advantage to\nreduce the learning, adjustment, and interpretation burden of subsequent\nresponse generation models, especially LLMs. We propose GATE, a\ngenerator-agnostic knowledge selection method, to prepare knowledge for\nsubsequent response generation models by selecting context-related knowledge\namong different knowledge structures and variable knowledge requirements.\nExperimental results demonstrate the superiority of GATE, and indicate that\nknowledge selection before generation is a lightweight yet effective way to\nfacilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.07659v3.pdf"
    },
    {
        "title": "Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models",
        "authors": [
            "Zeqiang Lai",
            "Xizhou Zhu",
            "Jifeng Dai",
            "Yu Qiao",
            "Wenhai Wang"
        ],
        "published": "2023-10-11T16:53:40Z",
        "summary": "The revolution of artificial intelligence content generation has been rapidly\naccelerated with the booming text-to-image (T2I) diffusion models. Within just\ntwo years of development, it was unprecedentedly of high-quality, diversity,\nand creativity that the state-of-the-art models could generate. However, a\nprevalent limitation persists in the effective communication with these popular\nT2I models, such as Stable Diffusion, using natural language descriptions. This\ntypically makes an engaging image hard to obtain without expertise in prompt\nengineering with complex word compositions, magic tags, and annotations.\nInspired by the recently released DALLE3 - a T2I model directly built-in\nChatGPT that talks human language, we revisit the existing T2I systems\nendeavoring to align human intent and introduce a new task - interactive text\nto image (iT2I), where people can interact with LLM for interleaved\nhigh-quality image generation/edit/refinement and question answering with\nstronger images and text correspondences using natural language. In addressing\nthe iT2I problem, we present a simple approach that augments LLMs for iT2I with\nprompting techniques and off-the-shelf T2I models. We evaluate our approach for\niT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT,\nLLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a\nconvenient and low-cost way to introduce the iT2I ability for any existing LLMs\nand any text-to-image models without any training while bringing little\ndegradation on LLMs' inherent capabilities in, e.g., question answering and\ncode generation. We hope this work could draw broader attention and provide\ninspiration for boosting user experience in human-machine interactions\nalongside the image quality of the next-generation T2I systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.07653v2.pdf"
    },
    {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "authors": [
            "Zhiyuan Zeng",
            "Jiatong Yu",
            "Tianyu Gao",
            "Yu Meng",
            "Tanya Goyal",
            "Danqi Chen"
        ],
        "published": "2023-10-11T16:38:11Z",
        "summary": "As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these \"LLM evaluators\", particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.",
        "pdf_link": "https://arxiv.org/pdf/2310.07641v1.pdf"
    },
    {
        "title": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models",
        "authors": [
            "Yuhe Liu",
            "Changhua Pei",
            "Longlong Xu",
            "Bohan Chen",
            "Mingze Sun",
            "Zhirui Zhang",
            "Yongqian Sun",
            "Shenglin Zhang",
            "Kun Wang",
            "Haiming Zhang",
            "Jianhui Li",
            "Gaogang Xie",
            "Xidao Wen",
            "Xiaohui Nie",
            "Minghua Ma",
            "Dan Pei"
        ],
        "published": "2023-10-11T16:33:29Z",
        "summary": "Information Technology (IT) Operations (Ops), particularly Artificial\nIntelligence for IT Operations (AIOps), is the guarantee for maintaining the\norderly and stable operation of existing information systems. According to\nGartner's prediction, the use of AI technology for automated IT operations has\nbecome a new trend. Large language models (LLMs) that have exhibited remarkable\ncapabilities in NLP-related tasks, are showing great potential in the field of\nAIOps, such as in aspects of root cause analysis of failures, generation of\noperations and maintenance scripts, and summarizing of alert information.\nNevertheless, the performance of current LLMs in Ops tasks is yet to be\ndetermined. In this paper, we present OpsEval, a comprehensive task-oriented\nOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'\nproficiency in various crucial scenarios at different ability levels. The\nbenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)\nformats in English and Chinese. By conducting a comprehensive performance\nevaluation of the current leading large language models, we show how various\nLLM techniques can affect the performance of Ops, and discussed findings\nrelated to various topics, including model quantification, QA evaluation, and\nhallucination issues. To ensure the credibility of our evaluation, we invite\ndozens of domain experts to manually review our questions. At the same time, we\nhave open-sourced 20% of the test QA to assist current researchers in\npreliminary evaluations of their OpsLLM models. The remaining 80% of the data,\nwhich is not disclosed, is used to eliminate the issue of the test set leakage.\nAdditionally, we have constructed an online leaderboard that is updated in\nreal-time and will continue to be updated, ensuring that any newly emerging\nLLMs will be evaluated promptly. Both our dataset and leaderboard have been\nmade public.",
        "pdf_link": "https://arxiv.org/pdf/2310.07637v3.pdf"
    },
    {
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
        "authors": [
            "Hannah Rose Kirk",
            "Andrew M. Bean",
            "Bertie Vidgen",
            "Paul R\u00f6ttger",
            "Scott A. Hale"
        ],
        "published": "2023-10-11T16:18:13Z",
        "summary": "Human feedback is increasingly used to steer the behaviours of Large Language\nModels (LLMs). However, it is unclear how to collect and incorporate feedback\nin a way that is efficient, effective and unbiased, especially for highly\nsubjective human preferences and values. In this paper, we survey existing\napproaches for learning from human feedback, drawing on 95 papers primarily\nfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLM\ntrends for integrating human feedback into language models. Second, we give an\noverview of present techniques and practices, as well as the motivations for\nusing feedback; conceptual frameworks for defining values and preferences; and\nhow feedback is collected and from whom. Finally, we encourage a better future\nof feedback learning in LLMs by raising five unresolved conceptual and\npractical challenges.",
        "pdf_link": "https://arxiv.org/pdf/2310.07629v1.pdf"
    },
    {
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
        "authors": [
            "Martin Pawelczyk",
            "Seth Neel",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-10-11T15:19:31Z",
        "summary": "Machine unlearning, the study of efficiently removing the impact of specific\ntraining points on the trained model, has garnered increased attention of late,\ndriven by the need to comply with privacy regulations like the Right to be\nForgotten. Although unlearning is particularly relevant for LLMs in light of\nthe copyright issues they raise, achieving precise unlearning is\ncomputationally infeasible for very large models. To this end, recent work has\nproposed several algorithms which approximate the removal of training data\nwithout retraining the model. These algorithms crucially rely on access to the\nmodel parameters in order to update them, an assumption that may not hold in\npractice due to computational constraints or when the LLM is accessed via API.\nIn this work, we propose a new class of unlearning methods for LLMs we call\n''In-Context Unlearning'', providing inputs in context and without having to\nupdate model parameters. To unlearn a particular training instance, we provide\nthe instance alongside a flipped label and additional correctly labelled\ninstances which are prepended as inputs to the LLM at inference time. Our\nexperimental results demonstrate that these contexts effectively remove\nspecific information from the training set while maintaining performance levels\nthat are competitive with (or in some cases exceed) state-of-the-art unlearning\nmethods that require access to the LLM parameters.",
        "pdf_link": "https://arxiv.org/pdf/2310.07579v2.pdf"
    },
    {
        "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
        "authors": [
            "Cunxiang Wang",
            "Xiaoze Liu",
            "Yuanhao Yue",
            "Xiangru Tang",
            "Tianhang Zhang",
            "Cheng Jiayang",
            "Yunzhi Yao",
            "Wenyang Gao",
            "Xuming Hu",
            "Zehan Qi",
            "Yidong Wang",
            "Linyi Yang",
            "Jindong Wang",
            "Xing Xie",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "published": "2023-10-11T14:18:03Z",
        "summary": "This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.07521v3.pdf"
    },
    {
        "title": "Fast-ELECTRA for Efficient Pre-training",
        "authors": [
            "Chengyu Dong",
            "Liyuan Liu",
            "Hao Cheng",
            "Jingbo Shang",
            "Jianfeng Gao",
            "Xiaodong Liu"
        ],
        "published": "2023-10-11T09:55:46Z",
        "summary": "ELECTRA pre-trains language models by detecting tokens in a sequence that\nhave been replaced by an auxiliary model. Although ELECTRA offers a significant\nboost in efficiency, its potential is constrained by the training cost brought\nby the auxiliary model. Notably, this model, which is jointly trained with the\nmain model, only serves to assist the training of the main model and is\ndiscarded post-training. This results in a substantial amount of training cost\nbeing expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which\nleverages an existing language model as the auxiliary model. To construct a\nlearning curriculum for the main model, we smooth its output distribution via\ntemperature scaling following a descending schedule. Our approach rivals the\nperformance of state-of-the-art ELECTRA-style pre-training methods, while\nsignificantly eliminating the computation and memory cost brought by the joint\ntraining of the auxiliary model. Our method also reduces the sensitivity to\nhyper-parameters and enhances the pre-training stability.",
        "pdf_link": "https://arxiv.org/pdf/2310.07347v1.pdf"
    },
    {
        "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad",
            "Jun Wang"
        ],
        "published": "2023-10-11T09:46:32Z",
        "summary": "Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms",
        "pdf_link": "https://arxiv.org/pdf/2310.07343v1.pdf"
    },
    {
        "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunovi\u0107",
            "Martin Vechev"
        ],
        "published": "2023-10-11T08:32:46Z",
        "summary": "Current privacy research on large language models (LLMs) primarily focuses on\nthe issue of extracting memorized training data. At the same time, models'\ninference capabilities have increased drastically. This raises the key question\nof whether current LLMs could violate individuals' privacy by inferring\npersonal attributes from text given at inference time. In this work, we present\nthe first comprehensive study on the capabilities of pretrained LLMs to infer\npersonal attributes from text. We construct a dataset consisting of real Reddit\nprofiles, and show that current LLMs can infer a wide range of personal\nattributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and\n$95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time\n($240\\times$) required by humans. As people increasingly interact with\nLLM-powered chatbots across all aspects of life, we also explore the emerging\nthreat of privacy-invasive chatbots trying to extract personal information\nthrough seemingly benign questions. Finally, we show that common mitigations,\ni.e., text anonymization and model alignment, are currently ineffective at\nprotecting user privacy against LLM inference. Our findings highlight that\ncurrent LLMs can infer personal data at a previously unattainable scale. In the\nabsence of working defenses, we advocate for a broader discussion around LLM\nprivacy implications beyond memorization, striving for a wider privacy\nprotection.",
        "pdf_link": "https://arxiv.org/pdf/2310.07298v1.pdf"
    },
    {
        "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction",
        "authors": [
            "Xiang Hao",
            "Jibin Wu",
            "Jianwei Yu",
            "Chenglin Xu",
            "Kay Chen Tan"
        ],
        "published": "2023-10-11T08:17:54Z",
        "summary": "Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) extracts useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process or complement the pre-registered cues. Our experimental\nresults demonstrate competitive performance when only text-based cues are\npresented, the effectiveness of using input text as a task selector, and a new\nstate-of-the-art when combining text-based cues with pre-registered cues. To\nour knowledge, this is the first study to successfully incorporate LLMs to\nguide target speaker extraction, which can be a cornerstone for cocktail party\nproblem research.",
        "pdf_link": "https://arxiv.org/pdf/2310.07284v3.pdf"
    },
    {
        "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
        "authors": [
            "Frank Joublin",
            "Antonello Ceravola",
            "Pavel Smirnov",
            "Felix Ocker",
            "Joerg Deigmoeller",
            "Anna Belardinelli",
            "Chao Wang",
            "Stephan Hasler",
            "Daniel Tanneberg",
            "Michael Gienger"
        ],
        "published": "2023-10-11T07:39:42Z",
        "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
        "pdf_link": "https://arxiv.org/pdf/2310.07263v1.pdf"
    },
    {
        "title": "CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving",
        "authors": [
            "Yuhan Liu",
            "Hanchen Li",
            "Yihua Cheng",
            "Siddhant Ray",
            "Yuyang Huang",
            "Qizheng Zhang",
            "Kuntai Du",
            "Jiayi Yao",
            "Shan Lu",
            "Ganesh Ananthanarayanan",
            "Michael Maire",
            "Henry Hoffmann",
            "Ari Holtzman",
            "Junchen Jiang"
        ],
        "published": "2023-10-11T07:08:20Z",
        "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge or\nuser-specific information. Yet using long contexts poses a challenge for\nresponsive LLM systems, as nothing can be generated until the whole context is\nprocessed by the LLM. While the context-processing delay can be reduced by\nreusing the KV cache of a context across different inputs, fetching the KV\ncache, which contains large tensors, over the network can cause extra network\ndelays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, which embraces KV cache's distributional\nproperties, to encode a KV cache into more compact bitstream representations\nwith negligible encoding/decoding overhead. This reduces the bandwidth demand\nto fetch the KV cache. Second, to maintain low context-loading delay and high\ngeneration quality, CacheGen adapts the streaming strategies to cope with\nchanges in available bandwidth. When available bandwidth drops, CacheGen may\nraise the compression level for a part of the context or choose to recompute\nits KV cache on the fly. We test CacheGen on four popular LLMs of various sizes\nand four datasets (662 contexts in total). Compared to the recent systems that\nreuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the\ntotal delay in fetching and processing contexts by 2.7-3.2x while having\nnegligible impact on the LLM response quality in accuracy or perplexity.",
        "pdf_link": "https://arxiv.org/pdf/2310.07240v4.pdf"
    },
    {
        "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
        "authors": [
            "Jiamin Li",
            "Qiang Su",
            "Yitao Yang",
            "Yimin Jiang",
            "Cong Wang",
            "Hong Xu"
        ],
        "published": "2023-10-11T04:30:18Z",
        "summary": "Large language models, such as OpenAI's ChatGPT, have demonstrated\nexceptional language understanding capabilities in various NLP tasks. Sparsely\nactivated mixture-of-experts (MoE) has emerged as a promising solution for\nscaling models while maintaining a constant number of computational operations.\nExisting MoE model adopts a fixed gating network where each token is computed\nby the same number of experts. However, this approach contradicts our intuition\nthat the tokens in each sequence vary in terms of their linguistic complexity\nand, consequently, require different computational costs. Little is discussed\nin prior research on the trade-off between computation per token and model\nperformance. This paper introduces adaptive gating in MoE, a flexible training\nstrategy that allows tokens to be processed by a variable number of experts\nbased on expert probability distribution. The proposed framework preserves\nsparsity while improving training efficiency. Additionally, curriculum learning\nis leveraged to further reduce training time. Extensive experiments on diverse\nNLP tasks show that adaptive gating reduces at most 22.5% training time while\nmaintaining inference quality. Moreover, we conduct a comprehensive analysis of\nthe routing decisions and present our insights when adaptive gating is used.",
        "pdf_link": "https://arxiv.org/pdf/2310.07188v1.pdf"
    },
    {
        "title": "Online Speculative Decoding",
        "authors": [
            "Xiaoxuan Liu",
            "Lanxiang Hu",
            "Peter Bailis",
            "Ion Stoica",
            "Zhijie Deng",
            "Alvin Cheung",
            "Hao Zhang"
        ],
        "published": "2023-10-11T04:03:42Z",
        "summary": "Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.",
        "pdf_link": "https://arxiv.org/pdf/2310.07177v2.pdf"
    },
    {
        "title": "Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding",
        "authors": [
            "Kexun Zhang",
            "Hongqiao Chen",
            "Lei Li",
            "William Wang"
        ],
        "published": "2023-10-10T23:37:53Z",
        "summary": "Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.",
        "pdf_link": "https://arxiv.org/pdf/2310.07075v1.pdf"
    },
    {
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
        "authors": [
            "Yangsibo Huang",
            "Samyak Gupta",
            "Mengzhou Xia",
            "Kai Li",
            "Danqi Chen"
        ],
        "published": "2023-10-10T20:15:54Z",
        "summary": "The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.06987v1.pdf"
    },
    {
        "title": "LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing",
        "authors": [
            "Stephen Moskal",
            "Sam Laney",
            "Erik Hemberg",
            "Una-May O'Reilly"
        ],
        "published": "2023-10-10T18:49:20Z",
        "summary": "In this paper, we explore the potential of Large Language Models (LLMs) to\nreason about threats, generate information about tools, and automate cyber\ncampaigns. We begin with a manual exploration of LLMs in supporting specific\nthreat-related actions and decisions. We proceed by automating the decision\nprocess in a cyber campaign. We present prompt engineering approaches for a\nplan-act-report loop for one action of a threat campaign and and a prompt\nchaining design that directs the sequential decision process of a multi-action\ncampaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the\nshort campaign we demonstrate and provide insights into prompt design for\neliciting actionable responses. We discuss the potential impact of LLMs on the\nthreat landscape and the ethical considerations of using LLMs for accelerating\nthreat actor capabilities. We report a promising, yet concerning, application\nof generative AI to cyber threats. However, the LLM's capabilities to deal with\nmore complex networks, sophisticated vulnerabilities, and the sensitivity of\nprompts are open questions. This research should spur deliberations over the\ninevitable advancements in LLM-supported cyber adversarial landscape.",
        "pdf_link": "https://arxiv.org/pdf/2310.06936v1.pdf"
    },
    {
        "title": "A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging",
        "authors": [
            "Atish Kumar Dipongkor",
            "Kevin Moran"
        ],
        "published": "2023-10-10T18:09:32Z",
        "summary": "Often, the first step in managing bug reports is related to triaging a bug to\nthe appropriate developer who is best suited to understand, localize, and fix\nthe target bug. Additionally, assigning a given bug to a particular part of a\nsoftware project can help to expedite the fixing process. However, despite the\nimportance of these activities, they are quite challenging, where days can be\nspent on the manual triaging process. Past studies have attempted to leverage\nthe limited textual data of bug reports to train text classification models\nthat automate this process -- to varying degrees of success. However, the\ntextual representations and machine learning models used in prior work are\nlimited by their expressiveness, often failing to capture nuanced textual\npatterns that might otherwise aid in the triaging process. Recently, large,\ntransformer-based, pre-trained neural text representation techniques such as\nBERT have achieved greater performance in several natural language processing\ntasks. However, the potential for using these techniques to improve upon prior\napproaches for automated bug triaging is not well studied or understood.\n  Therefore, in this paper we offer one of the first investigations that\nfine-tunes transformer-based language models for the task of bug triaging on\nfour open source datasets, spanning a collective 53 years of development\nhistory with over 400 developers and over 150 software project components. Our\nstudy includes both a quantitative and qualitative analysis of effectiveness.\nOur findings illustrate that DeBERTa is the most effective technique across the\ntriaging tasks of developer and component assignment, and the measured\nperformance delta is statistically significant compared to other techniques.\nHowever, through our qualitative analysis, we also observe that each technique\npossesses unique abilities best suited to certain types of bug reports.",
        "pdf_link": "https://arxiv.org/pdf/2310.06913v1.pdf"
    },
    {
        "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
        "authors": [
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Xufang Luo",
            "Dongsheng Li",
            "Chin-Yew Lin",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "published": "2023-10-10T17:59:58Z",
        "summary": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational/financial cost, longer latency, and inferior\nperformance. Some studies reveal that the performance of LLMs depends on both\nthe density and the position of the key information (question relevant) in the\ninput prompt. Inspired by these findings, we propose LongLLMLingua for prompt\ncompression towards improving LLMs' perception of the key information to\nsimultaneously address the three challenges. We conduct evaluation on a wide\nrange of long context scenarios including single-/multi-document QA, few-shot\nlearning, summarization, synthetic tasks, and code completion. The experimental\nresults show that LongLLMLingua compressed prompt can derive higher performance\nwith much less cost. The latency of the end-to-end system is also reduced. For\nexample, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost\nof up to 17.1% over the original prompt with ~4x fewer tokens as input to\nGPT-3.5-Turbo. It can derive cost savings of \\$28.5 and \\$27.4 per 1,000\nsamples from the LongBench and ZeroScrolls benchmark, respectively.\nAdditionally, when compressing prompts of ~10k tokens at a compression rate of\n2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our\ncode is available at https://aka.ms/LLMLingua.",
        "pdf_link": "https://arxiv.org/pdf/2310.06839v1.pdf"
    },
    {
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
        "authors": [
            "Eric Zelikman",
            "Wanjing Anya Ma",
            "Jasmine E. Tran",
            "Diyi Yang",
            "Jason D. Yeatman",
            "Nick Haber"
        ],
        "published": "2023-10-10T17:59:51Z",
        "summary": "Developing an educational test can be expensive and time-consuming, as each\nitem must be written by experts and then evaluated by collecting hundreds of\nstudent responses. Moreover, many tests require multiple distinct sets of\nquestions administered throughout the school year to closely monitor students'\nprogress, known as parallel tests. In this study, we focus on tests of silent\nsentence reading efficiency, used to assess students' reading ability over\ntime. To generate high-quality parallel tests, we propose to fine-tune large\nlanguage models (LLMs) to simulate how previous students would have responded\nto unseen items. With these simulated responses, we can estimate each item's\ndifficulty and ambiguity. We first use GPT-4 to generate new test items\nfollowing a list of expert-developed rules and then apply a fine-tuned LLM to\nfilter the items based on criteria from psychological measurements. We also\npropose an optimal-transport-inspired technique for generating parallel tests\nand show the generated tests closely correspond to the original test's\ndifficulty and reliability based on crowdworker responses. Our evaluation of a\ngenerated test with 234 students from grades 2 to 8 produces test scores highly\ncorrelated (r=0.93) to those of a standard test form written by human experts\nand evaluated across thousands of K-12 students.",
        "pdf_link": "https://arxiv.org/pdf/2310.06837v1.pdf"
    },
    {
        "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
        "authors": [
            "Erik Jones",
            "Hamid Palangi",
            "Clarisse Sim\u00f5es",
            "Varun Chandrasekaran",
            "Subhabrata Mukherjee",
            "Arindam Mitra",
            "Ahmed Awadallah",
            "Ece Kamar"
        ],
        "published": "2023-10-10T17:57:00Z",
        "summary": "Large language models (LLMs) frequently hallucinate on abstractive\nsummarization tasks such as document-based question-answering, meeting\nsummarization, and clinical report generation, even though all necessary\ninformation is included in context. However, optimizing LLMs to hallucinate\nless on these tasks is challenging, as hallucination is hard to efficiently\nevaluate at each optimization step. In this work, we show that reducing\nhallucination on a synthetic task can also reduce hallucination on real-world\ndownstream tasks. Our method, SynTra, first designs a synthetic task where\nhallucinations are easy to elicit and measure. It next optimizes the LLM's\nsystem message via prefix-tuning on the synthetic task, and finally transfers\nthe system message to realistic, hard-to-optimize tasks. Across three realistic\nabstractive summarization tasks, SynTra reduces hallucination for two\n13B-parameter LLMs using only a synthetic retrieval task for supervision. We\nalso find that optimizing the system message rather than the model weights can\nbe critical; fine-tuning the entire model on the synthetic task can\ncounterintuitively increase hallucination. Overall, SynTra demonstrates that\nthe extra flexibility of working with synthetic data can help mitigate\nundesired behaviors in practice.",
        "pdf_link": "https://arxiv.org/pdf/2310.06827v3.pdf"
    },
    {
        "title": "The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets",
        "authors": [
            "Samuel Marks",
            "Max Tegmark"
        ],
        "published": "2023-10-10T17:54:39Z",
        "summary": "Large Language Models (LLMs) have impressive capabilities, but are also prone\nto outputting falsehoods. Recent work has developed techniques for inferring\nwhether a LLM is telling the truth by training probes on the LLM's internal\nactivations. However, this line of work is controversial, with some authors\npointing out failures of these probes to generalize in basic ways, among other\nconceptual issues. In this work, we curate high-quality datasets of true/false\nstatements and use them to study in detail the structure of LLM representations\nof truth, drawing on three lines of evidence: 1. Visualizations of LLM\ntrue/false statement representations, which reveal clear linear structure. 2.\nTransfer experiments in which probes trained on one dataset generalize to\ndifferent datasets. 3. Causal evidence obtained by surgically intervening in a\nLLM's forward pass, causing it to treat false statements as true and vice\nversa. Overall, we present evidence that language models linearly represent the\ntruth or falsehood of factual statements. We also introduce a novel technique,\nmass-mean probing, which generalizes better and is more causally implicated in\nmodel outputs than other probing techniques.",
        "pdf_link": "https://arxiv.org/pdf/2310.06824v2.pdf"
    },
    {
        "title": "Exploring Memorization in Fine-tuned Language Models",
        "authors": [
            "Shenglai Zeng",
            "Yaxin Li",
            "Jie Ren",
            "Yiding Liu",
            "Han Xu",
            "Pengfei He",
            "Yue Xing",
            "Shuaiqiang Wang",
            "Jiliang Tang",
            "Dawei Yin"
        ],
        "published": "2023-10-10T15:41:26Z",
        "summary": "Large language models (LLMs) have shown great capabilities in various tasks\nbut also exhibited memorization of training data, raising tremendous privacy\nand copyright concerns. While prior works have studied memorization during\npre-training, the exploration of memorization during fine-tuning is rather\nlimited. Compared to pre-training, fine-tuning typically involves more\nsensitive data and diverse objectives, thus may bring distinct privacy risks\nand unique memorization behaviors. In this work, we conduct the first\ncomprehensive analysis to explore language models' (LMs) memorization during\nfine-tuning across tasks. Our studies with open-sourced and our own fine-tuned\nLMs across various tasks indicate that memorization presents a strong disparity\namong different fine-tuning tasks. We provide an intuitive explanation of this\ntask disparity via sparse coding theory and unveil a strong correlation between\nmemorization and attention score distribution.",
        "pdf_link": "https://arxiv.org/pdf/2310.06714v2.pdf"
    },
    {
        "title": "Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach",
        "authors": [
            "Zhenlan Ji",
            "Pingchuan Ma",
            "Zongjie Li",
            "Shuai Wang"
        ],
        "published": "2023-10-10T14:56:26Z",
        "summary": "While code generation has been widely used in various software development\nscenarios, the quality of the generated code is not guaranteed. This has been a\nparticular concern in the era of large language models (LLMs)- based code\ngeneration, where LLMs, deemed a complex and powerful black-box model, is\ninstructed by a high-level natural language specification, namely a prompt, to\ngenerate code. Nevertheless, effectively evaluating and explaining the code\ngeneration capability of LLMs is inherently challenging, given the complexity\nof LLMs and the lack of transparency.\n  Inspired by the recent progress in causality analysis and its application in\nsoftware engineering, this paper launches a causality analysis-based approach\nto systematically analyze the causal relations between the LLM input prompts\nand the generated code. To handle various technical challenges in this study,\nwe first propose a novel causal graph-based representation of the prompt and\nthe generated code, which is established over the fine-grained,\nhuman-understandable concepts in the input prompts. The formed causal graph is\nthen used to identify the causal relations between the prompt and the derived\ncode. We illustrate the insights that our framework can provide by studying\nover 3 popular LLMs with over 12 prompt adjustment strategies. The results of\nthese studies illustrate the potential of our technique to provide insights\ninto LLM effectiveness, and aid end-users in understanding predictions.\nAdditionally, we demonstrate that our approach provides actionable insights to\nimprove the quality of the LLM-generated code by properly calibrating the\nprompt.",
        "pdf_link": "https://arxiv.org/pdf/2310.06680v1.pdf"
    },
    {
        "title": "Automated clinical coding using off-the-shelf large language models",
        "authors": [
            "Joseph S. Boyle",
            "Antanas Kascenas",
            "Pat Lok",
            "Maria Liakata",
            "Alison Q. O'Neil"
        ],
        "published": "2023-10-10T11:56:48Z",
        "summary": "The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.",
        "pdf_link": "https://arxiv.org/pdf/2310.06552v3.pdf"
    },
    {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "published": "2023-10-10T10:17:58Z",
        "summary": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for various tasks and social simulations. Despite this, their\ncapacities to coordinate within task-oriented social contexts are\nunder-explored. Such capabilities are crucial if LLMs are to effectively mimic\nhuman-like social behavior and produce meaningful results. To bridge this gap,\nwe introduce collaborative generative agents, endowing LLM-based Agents with\nconsistent behavior patterns and task-solving abilities. We situate these\nagents in a simulated job fair environment as a case study to scrutinize their\ncoordination skills. We propose a novel framework that equips collaborative\ngenerative agents with human-like reasoning abilities and specialized skills.\nOur evaluation demonstrates that these agents show promising performance.\nHowever, we also uncover limitations that hinder their effectiveness in more\ncomplex coordination tasks. Our work provides valuable insights into the role\nand evolution of LLMs in task-oriented social simulations.",
        "pdf_link": "https://arxiv.org/pdf/2310.06500v1.pdf"
    },
    {
        "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection",
        "authors": [
            "Shiping Yang",
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2023-10-10T10:14:59Z",
        "summary": "Large Language Models (LLMs) have shown their ability to collaborate\neffectively with humans in real-world scenarios. However, LLMs are apt to\ngenerate hallucinations, i.e., makeup incorrect text and unverified\ninformation, which can cause significant damage when deployed for\nmission-critical tasks. In this paper, we propose a self-check approach based\non reverse validation to detect factual errors automatically in a zero-resource\nfashion. To facilitate future studies and assess different methods, we\nconstruct a hallucination detection benchmark named PHD, which is generated by\nChatGPT and annotated by human annotators. Contrasting previous studies of\nzero-resource hallucination detection, our method and benchmark concentrate on\npassage-level detection instead of sentence-level. We empirically evaluate our\nmethod and existing zero-resource detection methods on two datasets. The\nexperimental results demonstrate that the proposed method considerably\noutperforms the baselines while costing fewer tokens and less time.\nFurthermore, we manually analyze some hallucination cases that LLM failed to\ncapture, revealing the shared limitation of zero-resource methods.",
        "pdf_link": "https://arxiv.org/pdf/2310.06498v2.pdf"
    },
    {
        "title": "Multilingual Jailbreak Challenges in Large Language Models",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-10-10T09:44:06Z",
        "summary": "While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risky\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit about three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\n\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
        "pdf_link": "https://arxiv.org/pdf/2310.06474v3.pdf"
    },
    {
        "title": "Constructive Large Language Models Alignment with Diverse Feedback",
        "authors": [
            "Tianshu Yu",
            "Ting-En Lin",
            "Yuchuan Wu",
            "Min Yang",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-10-10T09:20:14Z",
        "summary": "In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.",
        "pdf_link": "https://arxiv.org/pdf/2310.06450v2.pdf"
    },
    {
        "title": "Large Language Models for Propaganda Detection",
        "authors": [
            "Kilian Sprenkamp",
            "Daniel Gordon Jones",
            "Liudmila Zavolokina"
        ],
        "published": "2023-10-10T08:46:10Z",
        "summary": "The prevalence of propaganda in our digital society poses a challenge to\nsocietal harmony and the dissemination of truth. Detecting propaganda through\nNLP in text is challenging due to subtle manipulation techniques and contextual\ndependencies. To address this issue, we investigate the effectiveness of modern\nLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.\nWe conduct experiments using the SemEval-2020 task 11 dataset, which features\nnews articles labeled with 14 propaganda techniques as a multi-label\nclassification problem. Five variations of GPT-3 and GPT-4 are employed,\nincorporating various prompt engineering and fine-tuning strategies across the\ndifferent models. We evaluate the models' performance by assessing metrics such\nas $F1$ score, $Precision$, and $Recall$, comparing the results with the\ncurrent state-of-the-art approach using RoBERTa. Our findings demonstrate that\nGPT-4 achieves comparable results to the current state-of-the-art. Further,\nthis study analyzes the potential and challenges of LLMs in complex tasks like\npropaganda detection.",
        "pdf_link": "https://arxiv.org/pdf/2310.06422v2.pdf"
    },
    {
        "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "published": "2023-10-10T03:05:44Z",
        "summary": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2310.06271v1.pdf"
    },
    {
        "title": "Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction",
        "authors": [
            "Cheng Peng",
            "Xi Yang",
            "Kaleb E Smith",
            "Zehao Yu",
            "Aokun Chen",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-10-10T01:27:08Z",
        "summary": "Objective To develop soft prompt-based learning algorithms for large language\nmodels (LLMs), examine the shape of prompts, prompt-tuning using\nfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.\nMethods We developed a soft prompt-based LLM model and compared 4 training\nstrategies including (1) fine-tuning without prompts; (2) hard-prompt with\nunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with\nfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for\nclinical concept and relation extraction on two benchmark datasets. We\nevaluated the transfer learning ability of the prompt-based learning algorithms\nin a cross-institution setting. We also assessed the few-shot learning ability.\nResults and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft\nprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept\nextraction, outperforming the traditional fine-tuning and hard prompt-based\nmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft\nprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end\nrelation extraction, outperforming the other two models by 0.2~2% and\n0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million\nparameters) LLMs have a big gap to be competitive with unfrozen models; scaling\nLLMs up to billions of parameters makes frozen LLMs competitive with unfrozen\nLLMs. For cross-institute evaluation, soft prompting with a frozen\nGatorTron-8.9B model achieved the best performance. This study demonstrates\nthat (1) machines can learn soft prompts better than humans, (2) frozen LLMs\nhave better few-shot learning ability and transfer learning ability to\nfacilitate muti-institution applications, and (3) frozen LLMs require large\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2310.06239v1.pdf"
    },
    {
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
            "Yucheng Li",
            "Bo Dong",
            "Chenghua Lin",
            "Frank Guerin"
        ],
        "published": "2023-10-09T23:03:24Z",
        "summary": "Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.",
        "pdf_link": "https://arxiv.org/pdf/2310.06201v1.pdf"
    },
    {
        "title": "Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution",
        "authors": [
            "Navapat Nananukul",
            "Khanin Sisaengsuwanchai",
            "Mayank Kejriwal"
        ],
        "published": "2023-10-09T21:57:07Z",
        "summary": "Entity Resolution (ER) is the problem of semi-automatically determining when\ntwo entities refer to the same underlying entity, with applications ranging\nfrom healthcare to e-commerce. Traditional ER solutions required considerable\nmanual expertise, including domain-specific feature engineering, as well as\nidentification and curation of training data. Recently released large language\nmodels (LLMs) provide an opportunity to make ER more seamless and\ndomain-independent. However, it is also well known that LLMs can pose risks,\nand that the quality of their outputs can depend on how prompts are engineered.\nUnfortunately, a systematic experimental study on the effects of different\nprompting methods for addressing unsupervised ER, using LLMs like ChatGPT, has\nbeen lacking thus far. This paper aims to address this gap by conducting such a\nstudy. We consider some relatively simple and cost-efficient ER prompt\nengineering methods and apply them to ER on two real-world datasets widely used\nin the community. We use an extensive set of experimental results to show that\nan LLM like GPT3.5 is viable for high-performing unsupervised ER, and\ninterestingly, that more complicated and detailed (and hence, expensive)\nprompting methods do not necessarily outperform simpler approaches. We provide\nbrief discussions on qualitative and error analysis, including a study of the\ninter-consistency of different prompting methods to determine whether they\nyield stable outputs. Finally, we consider some limitations of LLMs when\napplied to ER.",
        "pdf_link": "https://arxiv.org/pdf/2310.06174v2.pdf"
    },
    {
        "title": "OptiMUS: Optimization Modeling Using MIP Solvers and large language models",
        "authors": [
            "Ali AhmadiTeshnizi",
            "Wenzhi Gao",
            "Madeleine Udell"
        ],
        "published": "2023-10-09T19:47:03Z",
        "summary": "Optimization problems are pervasive across various sectors, from\nmanufacturing and distribution to healthcare. However, most such problems are\nstill solved heuristically by hand rather than optimally by state-of-the-art\nsolvers, as the expertise required to formulate and solve these problems limits\nthe widespread adoption of optimization tools and techniques. We introduce\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve MILP problems from their natural language descriptions. OptiMUS is\ncapable of developing mathematical models, writing and debugging solver code,\ndeveloping tests, and checking the validity of generated solutions. To\nbenchmark our agent, we present NLP4LP, a novel dataset of linear programming\n(LP) and mixed integer linear programming (MILP) problems. Our experiments\ndemonstrate that OptiMUS solves nearly twice as many problems as a basic LLM\nprompting strategy. OptiMUS code and NLP4LP dataset are available at\n\\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}",
        "pdf_link": "https://arxiv.org/pdf/2310.06116v2.pdf"
    },
    {
        "title": "SALMON: Self-Alignment with Instructable Reward Models",
        "authors": [
            "Zhiqing Sun",
            "Yikang Shen",
            "Hongxin Zhang",
            "Qinhong Zhou",
            "Zhenfang Chen",
            "David Cox",
            "Yiming Yang",
            "Chuang Gan"
        ],
        "published": "2023-10-09T17:56:53Z",
        "summary": "Supervised Fine-Tuning (SFT) on response demonstrations combined with\nReinforcement Learning from Human Feedback (RLHF) constitutes a powerful\nparadigm for aligning LLM-based AI agents. However, a significant limitation of\nsuch an approach is its dependency on high-quality human annotations, making\nits application to intricate tasks challenging due to difficulties in obtaining\nconsistent response demonstrations and in-distribution response preferences.\nThis paper presents a novel approach, namely SALMON, to align base language\nmodels with minimal human supervision, using only a small set of human-defined\nprinciples, yet achieving superior performance. Central to our approach is an\ninstructable reward model. Trained on synthetic preference data, this model can\ngenerate reward scores based on arbitrary human-defined principles. By merely\nadjusting these principles during the RL training phase, we gain full control\nover the preferences with the instructable reward model, subsequently\ninfluencing the behavior of the RL-trained policy models, and reducing the\nreliance on the collection of online human preferences. Applying our method to\nthe LLaMA-2-70b base language model, we developed an AI assistant named\nDromedary-2. With only 6 exemplars for in-context learning and 31 human-defined\nprinciples, Dromedary-2 significantly surpasses the performance of several\nstate-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark\ndatasets. We have open-sourced the code and model weights to encourage further\nresearch into aligning LLM-based AI agents with enhanced supervision\nefficiency, improved controllability, and scalable oversight.",
        "pdf_link": "https://arxiv.org/pdf/2310.05910v2.pdf"
    },
    {
        "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
        "authors": [
            "Kaiwen Zhou",
            "Kwonjoon Lee",
            "Teruhisa Misu",
            "Xin Eric Wang"
        ],
        "published": "2023-10-09T17:10:35Z",
        "summary": "In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2310.05872v1.pdf"
    },
    {
        "title": "HyperAttention: Long-context Attention in Near-Linear Time",
        "authors": [
            "Insu Han",
            "Rajesh Jayaram",
            "Amin Karbasi",
            "Vahab Mirrokni",
            "David P. Woodruff",
            "Amir Zandieh"
        ],
        "published": "2023-10-09T17:05:25Z",
        "summary": "We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.",
        "pdf_link": "https://arxiv.org/pdf/2310.05869v3.pdf"
    },
    {
        "title": "Improving Summarization with Human Edits",
        "authors": [
            "Zonghai Yao",
            "Benjamin J Schloss",
            "Sai P. Selvaraj"
        ],
        "published": "2023-10-09T16:52:07Z",
        "summary": "Recent work has shown the promise of learning with human feedback paradigms\nto produce human-determined high-quality text. Existing works use human\nfeedback to train large language models (LLMs) in general domain abstractive\nsummarization and have obtained summary quality exceeding traditional\nlikelihood training. In this paper, we focus on a less explored form of human\nfeedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training\n(SALT), a novel technique to use both the human-edited and model-generated data\ntogether in the training loop. In addition, we demonstrate simulating Human\nEdits with ground truth summaries coming from existing training data --\nImitation edits, along with the model-generated summaries obtained after the\ntraining, to reduce the need for expensive human-edit data. In our experiments,\nwe extend human feedback exploration from general domain summarization to\nmedical domain summarization. Our results demonstrate the effectiveness of SALT\nin improving the summary quality with Human and Imitation Edits. Through\nadditional experiments, we show that SALT outperforms the conventional RLHF\nmethod (designed for human preferences) -- DPO, when applied to human-edit\ndata. We hope the evidence in our paper prompts researchers to explore,\ncollect, and better use different human feedback approaches scalably.",
        "pdf_link": "https://arxiv.org/pdf/2310.05857v2.pdf"
    },
    {
        "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
        "authors": [
            "Ziwei Chai",
            "Tianjie Zhang",
            "Liang Wu",
            "Kaiqiao Han",
            "Xiaohai Hu",
            "Xuanwen Huang",
            "Yang Yang"
        ],
        "published": "2023-10-09T16:42:00Z",
        "summary": "The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.05845v1.pdf"
    },
    {
        "title": "SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese",
        "authors": [
            "Liang Xu",
            "Kangkang Zhao",
            "Lei Zhu",
            "Hang Xue"
        ],
        "published": "2023-10-09T16:03:22Z",
        "summary": "Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated\nremarkable abilities in natural language understanding and generation. However,\nalongside their positive impact on our daily tasks, they can also produce\nharmful content that negatively affects societal perceptions. To systematically\nassess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) -\na multi-round adversarial benchmark with 4912 open-ended questions covering\nmore than 20 safety sub-dimensions. Adversarial human-model interactions and\nconversations significantly increase the challenges compared to existing\nmethods. Experiments on 13 major LLMs supporting Chinese yield the following\ninsights: 1) Closed-source models outperform open-sourced ones in terms of\nsafety; 2) Models released from China demonstrate comparable safety levels to\nLLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can\ncompete effectively in terms of safety. By introducing SC-Safety, we aim to\npromote collaborative efforts to create safer and more trustworthy LLMs. The\nbenchmark and findings provide guidance on model selection. Our benchmark can\nbe found at https://www.CLUEbenchmarks.com",
        "pdf_link": "https://arxiv.org/pdf/2310.05818v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics",
        "authors": [
            "Kai He",
            "Rui Mao",
            "Qika Lin",
            "Yucheng Ruan",
            "Xiang Lan",
            "Mengling Feng",
            "Erik Cambria"
        ],
        "published": "2023-10-09T13:15:23Z",
        "summary": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to datacentered methodologies.",
        "pdf_link": "https://arxiv.org/pdf/2310.05694v1.pdf"
    },
    {
        "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
        "authors": [
            "Yongfu Dai",
            "Duanyu Feng",
            "Jimin Huang",
            "Haochen Jia",
            "Qianqian Xie",
            "Yifang Zhang",
            "Weiguang Han",
            "Wei Tian",
            "Hao Wang"
        ],
        "published": "2023-10-09T11:19:55Z",
        "summary": "General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.",
        "pdf_link": "https://arxiv.org/pdf/2310.05620v2.pdf"
    },
    {
        "title": "Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization",
        "authors": [
            "Chengpeng Li",
            "Zheng Yuan",
            "Hongyi Yuan",
            "Guanting Dong",
            "Keming Lu",
            "Jiancan Wu",
            "Chuanqi Tan",
            "Xiang Wang",
            "Chang Zhou"
        ],
        "published": "2023-10-09T08:18:58Z",
        "summary": "In math reasoning with large language models (LLMs), fine-tuning data\naugmentation by query evolution and diverse reasoning paths is empirically\nverified effective, profoundly narrowing the gap between open-sourced LLMs and\ncutting-edge proprietary LLMs. In this paper, we conduct an investigation for\nsuch data augmentation in math reasoning and are intended to answer: (1) What\nstrategies of data augmentation are more effective; (2) What is the scaling\nrelationship between the amount of augmented data and model performance; and\n(3) Can data augmentation incentivize generalization to out-of-domain\nmathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K,\nby complicating and diversifying the queries from GSM8K and sampling multiple\nreasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning\non subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art\non GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the\nscale of 13B). A log-linear relationship is presented between MuggleMath's\nperformance and the amount of augmented data. We also find that MuggleMath is\nweak in out-of-domain math reasoning generalization to MATH. This is attributed\nto the differences in query distribution between AugGSM8K and MATH which\nsuggest that augmentation on a single benchmark could not help with overall\nmath reasoning performance. Codes and AugGSM8K will be uploaded to\nhttps://github.com/OFA-Sys/gsm8k-ScRel.",
        "pdf_link": "https://arxiv.org/pdf/2310.05506v2.pdf"
    },
    {
        "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
        "authors": [
            "Zhihua Wen",
            "Zhiliang Tian",
            "Wei Wu",
            "Yuxin Yang",
            "Yanqi Shi",
            "Zhen Huang",
            "Dongsheng Li"
        ],
        "published": "2023-10-09T03:55:55Z",
        "summary": "Conditional story generation is significant in human-machine interaction,\nparticularly in producing stories with complex plots. While Large language\nmodels (LLMs) perform well on multiple NLP tasks, including story generation,\nit is challenging to generate stories with both complex and creative plots.\nExisting methods often rely on detailed prompts to guide LLMs to meet target\nconditions, which inadvertently restrict the creative potential of the\ngenerated stories. We argue that leveraging information from exemplary\nhuman-written stories facilitates generating more diverse plotlines. Delving\ndeeper into story details helps build complex and credible plots. In this\npaper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation\nframework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to\nenhance stories' complexity. We build a retrieval repository for target\nconditions to produce few-shot examples to prompt LLMs. Additionally, we design\nan ``asking-why'' prompting scheme that extracts a forest of evidence,\nproviding compensation for the ambiguities that may occur in the generated\nstory. This iterative process uncovers underlying story backgrounds. Finally,\nwe select the most fitting chains of evidence from the evidence forest and\nintegrate them into the generated story, thereby enhancing the narrative's\ncomplexity and credibility. Experimental results and numerous examples verify\nthe effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2310.05388v2.pdf"
    },
    {
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
        "authors": [
            "Yi Dong",
            "Zhilin Wang",
            "Makesh Narsimhan Sreedhar",
            "Xianchao Wu",
            "Oleksii Kuchaiev"
        ],
        "published": "2023-10-09T02:11:21Z",
        "summary": "Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",
        "pdf_link": "https://arxiv.org/pdf/2310.05344v1.pdf"
    },
    {
        "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models",
        "authors": [
            "Holy Lovenia",
            "Wenliang Dai",
            "Samuel Cahyawijaya",
            "Ziwei Ji",
            "Pascale Fung"
        ],
        "published": "2023-10-09T01:52:27Z",
        "summary": "Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.",
        "pdf_link": "https://arxiv.org/pdf/2310.05338v1.pdf"
    },
    {
        "title": "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems",
        "authors": [
            "Yixin Wan",
            "Jieyu Zhao",
            "Aman Chadha",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2023-10-08T21:03:18Z",
        "summary": "Recent advancements in Large Language Models empower them to follow freeform\ninstructions, including imitating generic or specific demographic personas in\nconversations. We define generic personas to represent demographic groups, such\nas \"an Asian person\", whereas specific personas may take the form of specific\npopular Asian names like \"Yumi\". While the adoption of personas enriches user\nexperiences by making dialogue systems more engaging and approachable, it also\ncasts a shadow of potential risk by exacerbating social biases within model\nresponses, thereby causing societal harm through interactions with users. In\nthis paper, we systematically study \"persona biases\", which we define to be the\nsensitivity of dialogue models' harmful behaviors contingent upon the personas\nthey adopt. We categorize persona biases into biases in harmful expression and\nharmful agreement, and establish a comprehensive evaluation framework to\nmeasure persona biases in five aspects: Offensiveness, Toxic Continuation,\nRegard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to\ninvestigate persona biases by experimenting with UNIVERSALPERSONA, a\nsystematically constructed persona dataset encompassing various types of both\ngeneric and specific model personas. Through benchmarking on four different\nmodels -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers\nsignificant persona biases in dialogue systems. Our findings also underscore\nthe pressing need to revisit the use of personas in dialogue agents to ensure\nsafe application.",
        "pdf_link": "https://arxiv.org/pdf/2310.05280v5.pdf"
    },
    {
        "title": "Measuring reasoning capabilities of ChatGPT",
        "authors": [
            "Adrian Groza"
        ],
        "published": "2023-10-08T20:18:50Z",
        "summary": "I shall quantify the logical faults generated by ChatGPT when applied to\nreasoning tasks. For experiments, I use the 144 puzzles from the library\n\\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The\nlibrary contains puzzles of various types, including arithmetic puzzles,\nlogical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling\npuzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct\nsolutions for these puzzles were checked using the theorem prover\nProver9~\\cite{mccune2005release} and the finite models finder\nMace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order\nLogic. A first output of this study is the benchmark of 100 logical puzzles.\nFor this dataset ChatGPT provided both correct answer and justification for 7\\%\nonly. %, while BARD for 5\\%. Since the dataset seems challenging, the\nresearchers are invited to test the dataset on more advanced or tuned models\nthan ChatGPT3.5 with more crafted prompts. A second output is the\nclassification of reasoning faults conveyed by ChatGPT. This classification\nforms a basis for a taxonomy of reasoning faults generated by large language\nmodels. I have identified 67 such logical faults, among which: inconsistencies,\nimplication does not hold, unsupported claim, lack of commonsense, wrong\njustification. The 100 solutions generated by ChatGPT contain 698 logical\nfaults. That is on average, 7 fallacies for each reasoning task. A third ouput\nis the annotated answers of the ChatGPT with the corresponding logical faults.\nEach wrong statement within the ChatGPT answer was manually annotated, aiming\nto quantify the amount of faulty text generated by the language model. On\naverage, 26.03\\% from the generated text was a logical fault.",
        "pdf_link": "https://arxiv.org/pdf/2310.05993v1.pdf"
    },
    {
        "title": "MindfulDiary: Harnessing Large Language Model to Support Psychiatric Patients' Journaling",
        "authors": [
            "Taewan Kim",
            "Seolyeong Bae",
            "Hyun Ah Kim",
            "Su-woo Lee",
            "Hwajung Hong",
            "Chanmo Yang",
            "Young-Ho Kim"
        ],
        "published": "2023-10-08T17:00:04Z",
        "summary": "In the mental health domain, Large Language Models (LLMs) offer promising new\nopportunities, though their inherent complexity and low controllability have\nraised questions about their suitability in clinical settings. We present\nMindfulDiary, a mobile journaling app incorporating an LLM to help psychiatric\npatients document daily experiences through conversation. Designed in\ncollaboration with mental health professionals (MHPs), MindfulDiary takes a\nstate-based approach to safely comply with the experts' guidelines while\ncarrying on free-form conversations. Through a four-week field study involving\n28 patients with major depressive disorder and five psychiatrists, we found\nthat MindfulDiary supported patients in consistently enriching their daily\nrecords and helped psychiatrists better empathize with their patients through\nan understanding of their thoughts and daily contexts. Drawing on these\nfindings, we discuss the implications of leveraging LLMs in the mental health\ndomain, bridging the technical feasibility and their integration into clinical\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2310.05231v2.pdf"
    },
    {
        "title": "Scaling Laws of RoPE-based Extrapolation",
        "authors": [
            "Xiaoran Liu",
            "Hang Yan",
            "Shuo Zhang",
            "Chenxin An",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2023-10-08T15:50:36Z",
        "summary": "The extrapolation capability of Large Language Models (LLMs) based on Rotary\nPosition Embedding is currently a topic of considerable interest. The\nmainstream approach to addressing extrapolation with LLMs involves modifying\nRoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the\noriginal RoPE, with a larger value and providing longer fine-tuning text. In\nthis work, we first observe that fine-tuning a RoPE-based LLM with either a\nsmaller or larger base in pre-training context length could significantly\nenhance its extrapolation performance. After that, we propose\n\\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework\nfrom the periodic perspective, to describe the relationship between the\nextrapolation performance and base value as well as tuning context length. In\nthis process, we also explain the origin of the RoPE-based extrapolation issue\nby \\textbf{\\textit{critical dimension for extrapolation}}. Besides these\nobservations and analyses, we achieve extrapolation up to 1 million context\nlength within only 16K training length on LLaMA2 7B and 13B.",
        "pdf_link": "https://arxiv.org/pdf/2310.05209v2.pdf"
    },
    {
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
        "authors": [
            "Wei Shen",
            "Rui Zheng",
            "Wenyu Zhan",
            "Jun Zhao",
            "Shihan Dou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-10-08T15:14:39Z",
        "summary": "Reinforcement learning from human feedback serves as a crucial bridge,\naligning large language models with human and societal values. This alignment\nrequires a vast corpus of human feedback to learn a reward model, which is\nsubsequently used to finetune language models. However, we have identified that\nthe reward model often finds shortcuts to bypass its intended objectives,\nmisleadingly assuming that humans prefer longer responses. The emergence of\nlength bias often induces the model to favor longer outputs, yet it doesn't\nequate to an increase in helpful information within these outputs. In this\npaper, we propose an innovative solution, applying the Product-of-Experts (PoE)\ntechnique to separate reward modeling from the influence of sequence length. In\nour framework, the main expert concentrates on understanding human intents,\nwhile the biased expert targets the identification and capture of length bias.\nTo further enhance the learning of bias, we introduce perturbations into the\nbias-focused expert, disrupting the flow of semantic information. Experimental\nresults validate the effectiveness of our approach, indicating that language\nmodel performance is improved, irrespective of sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2310.05199v5.pdf"
    },
    {
        "title": "Factuality Challenges in the Era of Large Language Models",
        "authors": [
            "Isabelle Augenstein",
            "Timothy Baldwin",
            "Meeyoung Cha",
            "Tanmoy Chakraborty",
            "Giovanni Luca Ciampaglia",
            "David Corney",
            "Renee DiResta",
            "Emilio Ferrara",
            "Scott Hale",
            "Alon Halevy",
            "Eduard Hovy",
            "Heng Ji",
            "Filippo Menczer",
            "Ruben Miguez",
            "Preslav Nakov",
            "Dietram Scheufele",
            "Shivam Sharma",
            "Giovanni Zagni"
        ],
        "published": "2023-10-08T14:55:02Z",
        "summary": "The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.",
        "pdf_link": "https://arxiv.org/pdf/2310.05189v2.pdf"
    },
    {
        "title": "Do Large Language Models Know about Facts?",
        "authors": [
            "Xuming Hu",
            "Junzhe Chen",
            "Xiaochuan Li",
            "Yufei Guo",
            "Lijie Wen",
            "Philip S. Yu",
            "Zhijiang Guo"
        ],
        "published": "2023-10-08T14:26:55Z",
        "summary": "Large language models (LLMs) have recently driven striking performance\nimprovements across a range of natural language processing tasks. The factual\nknowledge acquired during pretraining and instruction tuning can be useful in\nvarious downstream tasks, such as question answering, and language generation.\nUnlike conventional Knowledge Bases (KBs) that explicitly store factual\nknowledge, LLMs implicitly store facts in their parameters. Content generated\nby the LLMs can often exhibit inaccuracies or deviations from the truth, due to\nfacts that can be incorrectly induced or become obsolete over time. To this\nend, we aim to comprehensively evaluate the extent and scope of factual\nknowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains\n20K diverse factual questions that span different sources, timelines, domains,\nregions, and languages. Furthermore, we investigate whether LLMs are able to\ncompose multiple facts, update factual knowledge temporally, reason over\nmultiple pieces of facts, identify subtle factual differences, and resist\nadversarial examples. Extensive experiments on different sizes and types of\nLLMs show that existing LLMs still lack factual knowledge and suffer from\nvarious spurious correlations. We believe this is a critical bottleneck for\nrealizing trustworthy artificial intelligence. The dataset Pinocchio and our\ncodes will be publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2310.05177v1.pdf"
    },
    {
        "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
        "authors": [
            "Lu Yin",
            "You Wu",
            "Zhenyu Zhang",
            "Cheng-Yu Hsieh",
            "Yaqing Wang",
            "Yiling Jia",
            "Mykola Pechenizkiy",
            "Yi Liang",
            "Zhangyang Wang",
            "Shiwei Liu"
        ],
        "published": "2023-10-08T14:22:58Z",
        "summary": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL.",
        "pdf_link": "https://arxiv.org/pdf/2310.05175v2.pdf"
    },
    {
        "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
        "authors": [
            "Xiao Pu",
            "Jingyu Zhang",
            "Xiaochuang Han",
            "Yulia Tsvetkov",
            "Tianxing He"
        ],
        "published": "2023-10-08T13:49:51Z",
        "summary": "The rampant proliferation of large language models, fluent enough to generate\ntext indistinguishable from human-written language, gives unprecedented\nimportance to the detection of machine-generated text. This work is motivated\nby an important research question: How will the detectors of machine-generated\ntext perform on outputs of a new generator, that the detectors were not trained\non? We begin by collecting generation data from a wide range of LLMs, and train\nneural detectors on data from each generator and test its performance on\nheld-out generators. While none of the detectors can generalize to all\ngenerators, we observe a consistent and interesting pattern that the detectors\ntrained on data from a medium-size LLM can zero-shot generalize to the larger\nversion. As a concrete application, we demonstrate that robust detectors can be\nbuilt on an ensemble of training data from medium-sized models.",
        "pdf_link": "https://arxiv.org/pdf/2310.05165v1.pdf"
    },
    {
        "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
        "authors": [
            "Chengwen Qi",
            "Bowen Li",
            "Binyuan Hui",
            "Bailin Wang",
            "Jinyang Li",
            "Jinwang Wu",
            "Yuanjun Laili"
        ],
        "published": "2023-10-08T13:45:05Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2310.05163v3.pdf"
    },
    {
        "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
        "authors": [
            "Yifan Wei",
            "Yisong Su",
            "Huanhuan Ma",
            "Xiaoyan Yu",
            "Fangyu Lei",
            "Yuanzhe Zhang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-10-08T13:19:52Z",
        "summary": "Large language models (LLMs) have shown nearly saturated performance on many\nnatural language processing (NLP) tasks. As a result, it is natural for people\nto believe that LLMs have also mastered abilities such as time understanding\nand reasoning. However, research on the temporal sensitivity of LLMs has been\ninsufficiently emphasized. To fill this gap, this paper constructs Multiple\nSensitive Factors Time QA (MenatQA), which encompasses three temporal factors\n(scope factor, order factor, counterfactual factor) with total 2,853 samples\nfor evaluating the time comprehension and reasoning abilities of LLMs. This\npaper tests current mainstream LLMs with different parameter sizes, ranging\nfrom billions to hundreds of billions. The results show most LLMs fall behind\nsmaller temporal reasoning models with different degree on these factors. In\nspecific, LLMs show a significant vulnerability to temporal biases and depend\nheavily on the temporal information provided in questions. Furthermore, this\npaper undertakes a preliminary investigation into potential improvement\nstrategies by devising specific prompts and leveraging external tools. These\napproaches serve as valuable baselines or references for future research\nendeavors.",
        "pdf_link": "https://arxiv.org/pdf/2310.05157v1.pdf"
    },
    {
        "title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
        "authors": [
            "John Chong Min Tan",
            "Mehul Motani"
        ],
        "published": "2023-10-08T12:37:28Z",
        "summary": "We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge\nusing Large Language Models (LLMs) as a system of multiple expert agents. Using\nthe flexibility of LLMs to be prompted to do various novel tasks using\nzero-shot, few-shot, context-grounded prompting, we explore the feasibility of\nusing LLMs to solve the ARC Challenge. We firstly convert the input image into\nmultiple suitable text-based abstraction spaces. We then utilise the\nassociative power of LLMs to derive the input-output relationship and map this\nto actions in the form of a working program, similar to Voyager / Ghost in the\nMineCraft. In addition, we use iterative environmental feedback in order to\nguide LLMs to solve the task. Our proposed approach achieves 50 solves out of\n111 training set problems (45%) with just three abstraction spaces - grid,\nobject and pixel - and we believe that with more abstraction spaces and\nlearnable actions, we will be able to solve more.",
        "pdf_link": "https://arxiv.org/pdf/2310.05146v1.pdf"
    },
    {
        "title": "Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT",
        "authors": [
            "Akshaj Kumar Veldanda",
            "Fabian Grob",
            "Shailja Thakur",
            "Hammond Pearce",
            "Benjamin Tan",
            "Ramesh Karri",
            "Siddharth Garg"
        ],
        "published": "2023-10-08T12:08:48Z",
        "summary": "Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit\napplicability across numerous tasks. One domain of interest is their use in\nalgorithmic hiring, specifically in matching resumes with job categories. Yet,\nthis introduces issues of bias on protected attributes like gender, race and\nmaternity status. The seminal work of Bertrand & Mullainathan (2003) set the\ngold-standard for identifying hiring bias via field experiments where the\nresponse rate for identical resumes that differ only in protected attributes,\ne.g., racially suggestive names such as Emily or Lakisha, is compared. We\nreplicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and\nLlama) to evaluate bias (or lack thereof) on gender, race, maternity status,\npregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1)\nmatching resumes to job categories; and (2) summarizing resumes with employment\nrelevant information. Overall, LLMs are robust across race and gender. They\ndiffer in their performance on pregnancy status and political affiliation. We\nuse contrastive input decoding on open-source LLMs to uncover potential sources\nof bias.",
        "pdf_link": "https://arxiv.org/pdf/2310.05135v1.pdf"
    },
    {
        "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
        "authors": [
            "Chengcheng Han",
            "Xiaowei Du",
            "Che Zhang",
            "Yixin Lian",
            "Xiang Li",
            "Ming Gao",
            "Baoyuan Wang"
        ],
        "published": "2023-10-08T08:52:13Z",
        "summary": "Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.",
        "pdf_link": "https://arxiv.org/pdf/2310.05074v3.pdf"
    },
    {
        "title": "AvalonBench: Evaluating LLMs Playing the Game of Avalon",
        "authors": [
            "Jonathan Light",
            "Min Cai",
            "Sheng Shen",
            "Ziniu Hu"
        ],
        "published": "2023-10-08T06:37:08Z",
        "summary": "In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.",
        "pdf_link": "https://arxiv.org/pdf/2310.05036v3.pdf"
    },
    {
        "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
        "authors": [
            "Howard Chen",
            "Ramakanth Pasunuru",
            "Jason Weston",
            "Asli Celikyilmaz"
        ],
        "published": "2023-10-08T06:18:14Z",
        "summary": "Large language models (LLMs) have advanced in large strides due to the\neffectiveness of the self-attention mechanism that processes and compares all\ntokens at once. However, this mechanism comes with a fundamental issue -- the\npredetermined context window is bound to be limited. Despite attempts to extend\nthe context window through methods like extrapolating the positional embedding,\nusing recurrence, or selectively retrieving essential parts of the long\nsequence, long-text understanding continues to be a challenge. We propose an\nalternative approach which instead treats the LLM as an interactive agent,\nallowing it to decide how to read the text via iterative prompting. We\nintroduce MemWalker, a method that first processes the long context into a tree\nof summary nodes. Upon receiving a query, the model navigates this tree in\nsearch of relevant information, and responds once it gathers sufficient\ninformation. On long-text question answering tasks our method outperforms\nbaseline approaches that use long context windows, recurrence, and retrieval.\nWe show that, beyond effective reading, MemWalker enhances explainability by\nhighlighting the reasoning steps as it interactively reads the text;\npinpointing the relevant text segments related to the query.",
        "pdf_link": "https://arxiv.org/pdf/2310.05029v1.pdf"
    },
    {
        "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
        "authors": [
            "Guozheng Li",
            "Peng Wang",
            "Wenjun Ke"
        ],
        "published": "2023-10-08T06:17:39Z",
        "summary": "Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.",
        "pdf_link": "https://arxiv.org/pdf/2310.05028v4.pdf"
    },
    {
        "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
        "authors": [
            "Song Guo",
            "Jiahang Xu",
            "Li Lyna Zhang",
            "Mao Yang"
        ],
        "published": "2023-10-08T05:16:28Z",
        "summary": "Despite the remarkable success of Large Language Models (LLMs), the massive\nsize poses significant deployment challenges, particularly on\nresource-constrained hardware. While existing LLM compression methods focus on\nquantization, pruning remains relatively unexplored due to the high cost of\ntraining-based approaches and data collection challenges. One-shot pruning\nmethods, although cost-effective and data-free, have become dominant in LLM\npruning, but lead to performance decline under the structured pruning setting.\nIn this work, we introduce a new paradigm for structurally pruning LLMs, called\nCompresso. Our approach, through the collaboration of the proposed\nresource-efficient pruning algorithm and the LLM itself, learns optimal pruning\ndecisions during the training process. Compresso addresses the challenges of\nexpensive training costs and data collection by incorporating Low-Rank\nAdaptation (LoRA) into the $L_0$ regularization during the instruction tuning\nprocess. Then, we further augment the pruning algorithm by introducing a\ncollaborative prompt that fosters collaboration between the LLM and the pruning\nalgorithm, significantly boosting the overall performance. To this end,\nCompresso prunes LLaMA-7B to 5.4B, maintaining original performance and even\nsurpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments\ndemonstrate that Compresso significantly outperforms one-shot pruning baselines\nacross various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81%\nhigher scores on the commonsense reasoning, reading comprehension, MMLU, and\nBBH benchmarks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.05015v2.pdf"
    },
    {
        "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
        "authors": [
            "Yile Wang",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2023-10-08T04:22:33Z",
        "summary": "Large language models (LLMs) have shown superior performance without\ntask-specific fine-tuning. Despite the success, the knowledge stored in the\nparameters of LLMs could still be incomplete and difficult to update due to the\ncomputational costs. As complementary, retrieval-based methods can offer\nnon-parametric world knowledge and improve the performance on tasks such as\nquestion answering. However, we find that the retrieved knowledge does not\nalways help and even has a negative impact on original responses occasionally.\nTo better make use of both internal knowledge and external world knowledge, we\ninvestigate eliciting the model's ability to recognize what they know and do\nnot know (which is also called self-knowledge) and propose Self-Knowledge\nguided Retrieval augmentation (SKR), a simple yet effective method which can\nlet LLMs refer to the questions they have previously encountered and adaptively\ncall for external resources when dealing with new questions. We evaluate SKR on\nmultiple datasets and demonstrate that it outperforms chain-of-thought based\nand fully retrieval-based methods by using either InstructGPT or ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2310.05002v1.pdf"
    },
    {
        "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
        "authors": [
            "Fajri Koto",
            "Nurul Aisyah",
            "Haonan Li",
            "Timothy Baldwin"
        ],
        "published": "2023-10-07T21:49:38Z",
        "summary": "Although large language models (LLMs) are often pre-trained on large-scale\nmultilingual texts, their reasoning abilities and real-world knowledge are\nmainly evaluated based on English datasets. Assessing LLM capabilities beyond\nEnglish is increasingly vital but hindered due to the lack of suitable\ndatasets. In this work, we introduce IndoMMLU, the first multi-task language\nunderstanding benchmark for Indonesian culture and languages, which consists of\nquestions from primary school to university entrance exams in Indonesia. By\nemploying professional teachers, we obtain 14,981 questions across 64 tasks and\neducation levels, with 46% of the questions focusing on assessing proficiency\nin the Indonesian language and knowledge of nine local languages and cultures\nin Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass\nthe Indonesian primary school level, with limited knowledge of local Indonesian\nlanguages and culture. Other smaller models such as BLOOMZ and Falcon perform\nat even lower levels.",
        "pdf_link": "https://arxiv.org/pdf/2310.04928v2.pdf"
    },
    {
        "title": "Dual Grained Quantization: Efficient Fine-Grained Quantization for LLM",
        "authors": [
            "Luoming Zhang",
            "Wen Fei",
            "Weijia Wu",
            "Yefei He",
            "Zhenyu Lou",
            "Hong Zhou"
        ],
        "published": "2023-10-07T14:50:28Z",
        "summary": "Large Language Models (LLMs) pose significant hardware challenges related to\nmemory requirements and computational ability. There are two mainstream\nquantization schemes for LLMs: coarse-grained ($\\textit{e.g.,}$ channel-wise)\nquantization and fine-grained ($\\textit{e.g.,}$ group-wise) quantization.\nFine-grained quantization has smaller quantization loss, consequently achieving\nsuperior performance. However, when applied to weight-activation quantization,\nit disrupts continuous integer matrix multiplication, leading to inefficient\ninference. In this paper, we introduce Dual Grained Quantization (DGQ), a novel\nA8W4 quantization for LLM that maintains superior performance while ensuring\nfast inference speed. DSQ dequantizes the fine-grained INT4 weight into\ncoarse-grained INT8 representation and preform matrix multiplication using INT8\nkernels. Besides, we develop a two-phase grid search algorithm to simplify the\ndetermination of fine-grained and coarse-grained quantization scales. We also\ndevise a percentile clipping schema for smoothing the activation outliers\nwithout the need for complex optimization techniques. Experimental results\ndemonstrate that DGQ consistently outperforms prior methods across various LLM\narchitectures and a wide range of tasks. Remarkably, by our implemented\nefficient CUTLASS kernel, we achieve $\\textbf{1.12}$ $\\times$ memory reduction\nand $\\textbf{3.24}$ $\\times$ speed gains comparing A16W4 implementation. These\nadvancements enable efficient deployment of A8W4 LLMs for real-world\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2310.04836v1.pdf"
    },
    {
        "title": "Critique Ability of Large Language Models",
        "authors": [
            "Liangchen Luo",
            "Zi Lin",
            "Yinxiao Liu",
            "Lei Shu",
            "Yun Zhu",
            "Jingbo Shang",
            "Lei Meng"
        ],
        "published": "2023-10-07T14:12:15Z",
        "summary": "Critical thinking is essential for rational decision-making and\nproblem-solving. This skill hinges on the ability to provide precise and\nreasoned critiques and is a hallmark of human intelligence. In the era of large\nlanguage models (LLMs), this study explores the ability of LLMs to deliver\naccurate critiques across various tasks. We are interested in this topic as a\ncapable critic model could not only serve as a reliable evaluator, but also as\na source of supervised signals for model tuning. Particularly, if a model can\nself-critique, it has the potential for autonomous self-improvement. To examine\nthis, we introduce a unified evaluation framework for assessing the critique\nabilities of LLMs. We develop a benchmark called CriticBench, which comprises\n3K high-quality natural language queries and corresponding model responses; and\nannotate the correctness of these responses. The benchmark cover tasks such as\nmath problem-solving, code completion, and question answering. We evaluate\nmultiple LLMs on the collected dataset and our analysis reveals several\nnoteworthy insights: (1) Critique is generally challenging for most LLMs, and\nthis capability often emerges only when models are sufficiently large. (2) In\nparticular, self-critique is especially difficult. Even top-performing LLMs\nstruggle to achieve satisfactory performance. (3) Models tend to have lower\ncritique accuracy on problems where they are most uncertain. To this end, we\nintroduce a simple yet effective baseline named self-check, which leverages\nself-critique to improve task performance for various models. We hope this\nstudy serves as an initial exploration into understanding the critique\nabilities of LLMs, and aims to inform future research, including the\ndevelopment of more proficient critic models and the application of critiques\nacross diverse tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.04815v1.pdf"
    },
    {
        "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
        "authors": [
            "Yuchen Yang",
            "Houqiang Li",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2023-10-07T12:06:53Z",
        "summary": "In recent years, large-scale language models (LLMs) have gained attention for\ntheir impressive text generation capabilities. However, these models often face\nthe challenge of \"hallucination,\" which undermines their reliability. In this\nstudy, we introduce an uncertainty-aware in-context learning framework to\nempower the model to enhance or reject its output in response to uncertainty.\nHuman-defined methods for estimating uncertainty typically assume that\n\"uncertainty is lower when the model's response is correct compared to when it\nis incorrect.\" However, setting a precise threshold to distinguish correctness\nis challenging. Therefore, we introduce uncertainty information as an\nintermediary variable that implicitly influences the model's behavior. Our\ninnovative uncertainty-aware in-context learning framework involves fine-tuning\nthe LLM using a calibration dataset. Our aim is to improve the model's\nresponses by filtering out answers with high uncertainty while considering the\nmodel's knowledge limitations. We evaluate the model's knowledge by examining\nmultiple responses to the same question for the presence of a correct answer.\nWhen the model lacks relevant knowledge, the response should indicate that the\nquestion cannot be answered. Conversely, when the model has relevant knowledge,\nthe response should provide the correct answer. Extensive experiments confirm\nthe effectiveness of our framework, leading to two key findings. First, the\nlogit output values of the LLM partly reflect inherent uncertainty. Second, our\nmodel autonomously recognizes uncertainty, resulting in improved responses.",
        "pdf_link": "https://arxiv.org/pdf/2310.04782v1.pdf"
    },
    {
        "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
        "authors": [
            "Song Jiang",
            "Zahra Shakeri",
            "Aaron Chan",
            "Maziar Sanjabi",
            "Hamed Firooz",
            "Yinglong Xia",
            "Bugra Akyildiz",
            "Yizhou Sun",
            "Jinchao Li",
            "Qifan Wang",
            "Asli Celikyilmaz"
        ],
        "published": "2023-10-07T08:56:28Z",
        "summary": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.",
        "pdf_link": "https://arxiv.org/pdf/2310.04743v1.pdf"
    },
    {
        "title": "Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis",
        "authors": [
            "Siqi Du",
            "Shengjun Tang",
            "Weixi Wang",
            "Xiaoming Li",
            "Renzhong Guo"
        ],
        "published": "2023-10-07T06:12:39Z",
        "summary": "This paper introduces a novel framework, Tree-GPT, which incorporates Large\nLanguage Models (LLMs) into the forestry remote sensing data workflow, thereby\nenhancing the efficiency of data analysis. Currently, LLMs are unable to\nextract or comprehend information from images and may generate inaccurate text\ndue to a lack of domain knowledge, limiting their use in forestry data\nanalysis. To address this issue, we propose a modular LLM expert system,\nTree-GPT, that integrates image understanding modules, domain knowledge bases,\nand toolchains. This empowers LLMs with the ability to comprehend images,\nacquire accurate knowledge, generate code, and perform data analysis in a local\nenvironment. Specifically, the image understanding module extracts structured\ninformation from forest remote sensing images by utilizing automatic or\ninteractive generation of prompts to guide the Segment Anything Model (SAM) in\ngenerating and selecting optimal tree segmentation results. The system then\ncalculates tree structural parameters based on these results and stores them in\na database. Upon receiving a specific natural language instruction, the LLM\ngenerates code based on a thought chain to accomplish the analysis task. The\ncode is then executed by an LLM agent in a local environment and . For\necological parameter calculations, the system retrieves the corresponding\nknowledge from the knowledge base and inputs it into the LLM to guide the\ngeneration of accurate code. We tested this system on several tasks, including\nSearch, Visualization, and Machine Learning Analysis. The prototype system\nperformed well, demonstrating the potential for dynamic usage of LLMs in\nforestry research and environmental sciences.",
        "pdf_link": "https://arxiv.org/pdf/2310.04698v1.pdf"
    },
    {
        "title": "Data-Centric Financial Large Language Models",
        "authors": [
            "Zhixuan Chu",
            "Huaiyu Guo",
            "Xinyuan Zhou",
            "Yijia Wang",
            "Fei Yu",
            "Hong Chen",
            "Wanqing Xu",
            "Xin Lu",
            "Qing Cui",
            "Longfei Li",
            "Jun Zhou",
            "Sheng Li"
        ],
        "published": "2023-10-07T04:53:31Z",
        "summary": "Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.",
        "pdf_link": "https://arxiv.org/pdf/2310.17784v2.pdf"
    },
    {
        "title": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
        "authors": [
            "Tian Jin",
            "Nolan Clement",
            "Xin Dong",
            "Vaishnavh Nagarajan",
            "Michael Carbin",
            "Jonathan Ragan-Kelley",
            "Gintare Karolina Dziugaite"
        ],
        "published": "2023-10-07T03:36:39Z",
        "summary": "How does scaling the number of parameters in large language models (LLMs)\naffect their core capabilities? We study two natural scaling techniques --\nweight pruning and simply training a smaller or larger model, which we refer to\nas dense scaling -- and their effects on two core capabilities of LLMs: (a)\nrecalling facts presented during pre-training and (b) processing information\npresented in-context during inference. By curating a suite of tasks that help\ndisentangle these two capabilities, we find a striking difference in how these\ntwo abilities evolve due to scaling. Reducing the model size by more than 30\\%\n(via either scaling approach) significantly decreases the ability to recall\nfacts seen in pre-training. Yet, a 60--70\\% reduction largely preserves the\nvarious ways the model can process in-context information, ranging from\nretrieving answers from a long context to learning parameterized functions from\nin-context exemplars. The fact that both dense scaling and weight pruning\nexhibit this behavior suggests that scaling model size has an inherently\ndisparate effect on fact recall and in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.04680v1.pdf"
    },
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hongzhi Wen",
            "Haoyu Han",
            "Wei Jin",
            "Haiyang Zhang",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "published": "2023-10-07T03:14:11Z",
        "summary": "In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.",
        "pdf_link": "https://arxiv.org/pdf/2310.04668v3.pdf"
    },
    {
        "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
        "authors": [
            "Ted Moskovitz",
            "Aaditya K. Singh",
            "DJ Strouse",
            "Tuomas Sandholm",
            "Ruslan Salakhutdinov",
            "Anca D. Dragan",
            "Stephen McAleer"
        ],
        "published": "2023-10-06T16:59:17Z",
        "summary": "Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.",
        "pdf_link": "https://arxiv.org/pdf/2310.04373v2.pdf"
    },
    {
        "title": "Amortizing intractable inference in large language models",
        "authors": [
            "Edward J. Hu",
            "Moksh Jain",
            "Eric Elmoznino",
            "Younesse Kaddar",
            "Guillaume Lajoie",
            "Yoshua Bengio",
            "Nikolay Malkin"
        ],
        "published": "2023-10-06T16:36:08Z",
        "summary": "Autoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits\ntractable querying of this knowledge to start-to-end autoregressive sampling.\nHowever, many tasks of interest -- including sequence continuation, infilling,\nand other forms of constrained generation -- involve sampling from intractable\nposterior distributions. We address this limitation by using amortized Bayesian\ninference to sample from these intractable posteriors. Such amortization is\nalgorithmically achieved by fine-tuning LLMs via diversity-seeking\nreinforcement learning algorithms: generative flow networks (GFlowNets). We\nempirically demonstrate that this distribution-matching paradigm of LLM\nfine-tuning can serve as an effective alternative to maximum-likelihood\ntraining and reward-maximizing policy optimization. As an important\napplication, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient\nadaptation of LLMs to tasks that require multi-step rationalization and tool\nuse.",
        "pdf_link": "https://arxiv.org/pdf/2310.04363v2.pdf"
    },
    {
        "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
        "authors": [
            "Wanyun Cui",
            "Qianle Wang"
        ],
        "published": "2023-10-06T13:28:04Z",
        "summary": "Generating diverse and sophisticated instructions for downstream tasks by\nLarge Language Models (LLMs) is pivotal for advancing the effect. Current\napproaches leverage closed-source LLMs, employing in-context prompting for\ninstruction generation. However, in this paper, we found that in-context\nprompting cannot generate complex instructions with length $\\ge 100$ for tasks\nlike code completion.\n  To solve this problem, we introduce Ada-Instruct, an adaptive instruction\ngenerator developed by fine-tuning open-source LLMs. Our pivotal finding\nillustrates that fine-tuning open-source LLMs with a mere ten samples generates\nlong instructions that maintain distributional consistency for complex\nreasoning tasks. We empirically validated Ada-Instruct's efficacy across\ndifferent applications, including code completion, mathematical reasoning, and\ncommonsense reasoning. The results underscore Ada-Instruct's superiority,\nevidencing its improvements over its base models, current self-instruct\nmethods, and other state-of-the-art models.",
        "pdf_link": "https://arxiv.org/pdf/2310.04484v2.pdf"
    },
    {
        "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface",
        "authors": [
            "Anupam Purwar",
            "Rahul Sundar"
        ],
        "published": "2023-10-06T12:44:04Z",
        "summary": "Retrieving answers in a quick and low cost manner without hallucinations from\na combination of structured and unstructured data using Language models is a\nmajor hurdle. This is what prevents employment of Language models in knowledge\nretrieval automation. This becomes accentuated when one wants to integrate a\nspeech interface on top of a text based knowledge retrieval system. Besides,\nfor commercial search and chat-bot applications, complete reliance on\ncommercial large language models (LLMs) like GPT 3.5 etc. can be very costly.\nIn the present study, the authors have addressed the aforementioned problem by\nfirst developing a keyword based search framework which augments discovery of\nthe context from the document to be provided to the LLM. The keywords in turn\nare generated by a relatively smaller LLM and cached for comparison with\nkeywords generated by the same smaller LLM against the query raised. This\nsignificantly reduces time and cost to find the context within documents. Once\nthe context is set, a larger LLM uses that to provide answers based on a prompt\ntailored for Q\\&A. This research work demonstrates that use of keywords in\ncontext identification reduces the overall inference time and cost of\ninformation retrieval. Given this reduction in inference time and cost with the\nkeyword augmented retrieval framework, a speech based interface for user input\nand response readout was integrated. This allowed a seamless interaction with\nthe language model.",
        "pdf_link": "https://arxiv.org/pdf/2310.04205v2.pdf"
    },
    {
        "title": "Conversational Financial Information Retrieval Model (ConFIRM)",
        "authors": [
            "Stephen Choi",
            "William Gazeley",
            "Siu Ho Wong",
            "Tingting Li"
        ],
        "published": "2023-10-06T12:31:05Z",
        "summary": "With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.13001v3.pdf"
    },
    {
        "title": "Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models",
        "authors": [
            "Wenbei Xie"
        ],
        "published": "2023-10-06T06:20:06Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nimpressive capabilities across a range of natural language processing tasks,\nespecially in reasoning, a cornerstone for achieving Artificial General\nIntelligence (AGI). However, commonly used benchmarks may not fully encapsulate\nthe inferential abilities of these models in real-world scenarios. To address\nthis gap, a new form of Question-Answering (QA) task, termed Reasoning with\nRedundant Information Provided (RRIP), is introduced. The study designed a\nmodified version of the grade school math 8K (GSM-8K) dataset which has several\nvariants focusing on different attributes of redundant information. This\ninvestigation evaluates two popular LLMs, LlaMA2-13B-chat and generative\npre-trained transformer 3.5 (GPT-3.5), contrasting their performance on\ntraditional QA tasks against the RRIP tasks. Findings indicate that while these\nmodels achieved moderate success on standard QA benchmarks, their performance\nnotably declines when assessed on RRIP tasks. The study not only highlights the\nlimitations of current LLMs in handling redundant information but also suggests\nthat future training of these models should focus on incorporating redundant\ninformation into the training data to increase the performance on RRIP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2310.04039v1.pdf"
    },
    {
        "title": "Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models",
        "authors": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Tianyu Zhou",
            "Ali Babar",
            "Xiao-Yang Liu"
        ],
        "published": "2023-10-06T05:40:23Z",
        "summary": "Financial sentiment analysis is critical for valuation and investment\ndecision-making. Traditional NLP models, however, are limited by their\nparameter size and the scope of their training datasets, which hampers their\ngeneralization capabilities and effectiveness in this field. Recently, Large\nLanguage Models (LLMs) pre-trained on extensive corpora have demonstrated\nsuperior performance across various NLP tasks due to their commendable\nzero-shot abilities. Yet, directly applying LLMs to financial sentiment\nanalysis presents challenges: The discrepancy between the pre-training\nobjective of LLMs and predicting the sentiment label can compromise their\npredictive performance. Furthermore, the succinct nature of financial news,\noften devoid of sufficient context, can significantly diminish the reliability\nof LLMs' sentiment analysis. To address these challenges, we introduce a\nretrieval-augmented LLMs framework for financial sentiment analysis. This\nframework includes an instruction-tuned LLMs module, which ensures LLMs behave\nas predictors of sentiment labels, and a retrieval-augmentation module which\nretrieves additional context from reliable external sources. Benchmarked\nagainst traditional models and LLMs like ChatGPT and LLaMA, our approach\nachieves 15\\% to 48\\% performance gain in accuracy and F1 score.",
        "pdf_link": "https://arxiv.org/pdf/2310.04027v2.pdf"
    },
    {
        "title": "Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning",
        "authors": [
            "Yinger Zhang",
            "Hui Cai",
            "Xeirui Song",
            "Yicheng Chen",
            "Rui Sun",
            "Jing Zheng"
        ],
        "published": "2023-10-06T05:20:18Z",
        "summary": "While enabling large language models to implement function calling (known as\nAPIs) can greatly enhance the performance of Large Language Models (LLMs),\nfunction calling is still a challenging task due to the complicated relations\nbetween different APIs, especially in a context-learning setting without\nfine-tuning. This paper introduces ``Reverse Chain'', a controllable,\ntarget-driven approach designed to empower LLMs with the capability to operate\nexternal APIs only via prompts. Recognizing that most LLMs have limited\ntool-use capabilities, Reverse Chain limits LLMs to executing simple tasks,\ne.g., API Selection and Argument Completion. Furthermore, to manage a\ncontrollable multi-function calling, Reverse Chain adopts a generic rule based\non a backward reasoning process. This rule determines when to do API selection\nor Argument completion. To evaluate the multi-tool-use capability of LLMs, we\nhave released a compositional multi-tool task dataset, available at\n\\url{https://anonymous.4open.science/r/reverse-chain-8681}. Extensive numerical\nexperiments validate the remarkable proficiency of Reverse Chain in managing\nmultiple API calls.",
        "pdf_link": "https://arxiv.org/pdf/2310.04474v3.pdf"
    },
    {
        "title": "From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self",
        "authors": [
            "Yue Fu",
            "Sami Foell",
            "Xuhai Xu",
            "Alexis Hiniker"
        ],
        "published": "2023-10-06T02:19:10Z",
        "summary": "In the rapidly evolving landscape of AI-mediated communication (AIMC), tools\npowered by Large Language Models (LLMs) are becoming integral to interpersonal\ncommunication. Employing a mixed-methods approach, we conducted a one-week\ndiary and interview study to explore users' perceptions of these tools' ability\nto: 1) support interpersonal communication in the short-term, and 2) lead to\npotential long-term effects. Our findings indicate that participants view AIMC\nsupport favorably, citing benefits such as increased communication confidence,\nand finding precise language to express their thoughts, navigating linguistic\nand cultural barriers. However, the study also uncovers current limitations of\nAIMC tools, including verbosity, unnatural responses, and excessive emotional\nintensity. These shortcomings are further exacerbated by user concerns about\ninauthenticity and potential overreliance on the technology. Furthermore, we\nidentified four key communication spaces delineated by communication stakes\n(high or low) and relationship dynamics (formal or informal) that\ndifferentially predict users' attitudes toward AIMC tools. Specifically,\nparticipants found the tool is more suitable for communicating in formal\nrelationships than informal ones and more beneficial in high-stakes than\nlow-stakes communication.",
        "pdf_link": "https://arxiv.org/pdf/2310.03976v3.pdf"
    },
    {
        "title": "Quantized Transformer Language Model Implementations on Edge Devices",
        "authors": [
            "Mohammad Wali Ur Rahman",
            "Murad Mehrab Abrar",
            "Hunter Gibbons Copening",
            "Salim Hariri",
            "Sicong Shao",
            "Pratik Satam",
            "Soheil Salehi"
        ],
        "published": "2023-10-06T01:59:19Z",
        "summary": "Large-scale transformer-based models like the Bidirectional Encoder\nRepresentations from Transformers (BERT) are widely used for Natural Language\nProcessing (NLP) applications, wherein these models are initially pre-trained\nwith a large corpus with millions of parameters and then fine-tuned for a\ndownstream NLP task. One of the major limitations of these large-scale models\nis that they cannot be deployed on resource-constrained devices due to their\nlarge model size and increased inference latency. In order to overcome these\nlimitations, such large-scale models can be converted to an optimized\nFlatBuffer format, tailored for deployment on resource-constrained edge\ndevices. Herein, we evaluate the performance of such FlatBuffer transformed\nMobileBERT models on three different edge devices, fine-tuned for Reputation\nanalysis of English language tweets in the RepLab 2013 dataset. In addition,\nthis study encompassed an evaluation of the deployed models, wherein their\nlatency, performance, and resource efficiency were meticulously assessed. Our\nexperiment results show that, compared to the original BERT large model, the\nconverted and quantized MobileBERT models have 160$\\times$ smaller footprints\nfor a 4.1% drop in accuracy while analyzing at least one tweet per second on\nedge devices. Furthermore, our study highlights the privacy-preserving aspect\nof TinyML systems as all data is processed locally within a serverless\nenvironment.",
        "pdf_link": "https://arxiv.org/pdf/2310.03971v1.pdf"
    },
    {
        "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
        "authors": [
            "Deren Lei",
            "Yaxi Li",
            "Mengya Hu",
            "Mingyu Wang",
            "Vincent Yun",
            "Emily Ching",
            "Eslam Kamal"
        ],
        "published": "2023-10-06T00:10:46Z",
        "summary": "Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.",
        "pdf_link": "https://arxiv.org/pdf/2310.03951v2.pdf"
    },
    {
        "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models",
        "authors": [
            "Saaket Agashe",
            "Yue Fan",
            "Anthony Reyna",
            "Xin Eric Wang"
        ],
        "published": "2023-10-05T21:18:15Z",
        "summary": "The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by\nLarge Language Models (LLMs) make them promising candidates for developing\ncoordination agents. In this study, we introduce a new LLM-Coordination\nBenchmark aimed at a detailed analysis of LLMs within the context of Pure\nCoordination Games, where participating agents need to cooperate for the most\ngain. This benchmark evaluates LLMs through two distinct tasks: (1)\n\\emph{Agentic Coordination}, where LLMs act as proactive participants for\ncooperation in 4 pure coordination games; (2) \\emph{Coordination Question\nAnswering (QA)}, where LLMs are prompted to answer 198 multiple-choice\nquestions from the 4 games for evaluation of three key reasoning abilities:\nEnvironment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to\nenable LLMs for multi-agent coordination, we introduce a Cognitive Architecture\nfor Coordination (CAC) framework that can easily integrate different LLMs as\nplug-and-play modules for pure coordination games. Our findings indicate that\nLLM agents equipped with GPT-4-turbo achieve comparable performance to\nstate-of-the-art reinforcement learning methods in games that require\ncommonsense actions based on the environment. Besides, zero-shot coordination\nexperiments reveal that, unlike RL methods, LLM agents are robust to new unseen\npartners. However, results on Coordination QA show a large room for improvement\nin the Theory of Mind reasoning and joint planning abilities of LLMs. The\nanalysis also sheds light on how the ability of LLMs to understand their\nenvironment and their partner's beliefs and intentions plays a part in their\nability to plan for coordination. Our code is available at\n\\url{https://github.com/eric-ai-lab/llm_coordination}.",
        "pdf_link": "https://arxiv.org/pdf/2310.03903v2.pdf"
    },
    {
        "title": "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms",
        "authors": [
            "Petter T\u00f6rnberg",
            "Diliara Valeeva",
            "Justus Uitermark",
            "Christopher Bail"
        ],
        "published": "2023-10-05T18:26:06Z",
        "summary": "Social media is often criticized for amplifying toxic discourse and\ndiscouraging constructive conversations. But designing social media platforms\nto promote better conversations is inherently challenging. This paper asks\nwhether simulating social media through a combination of Large Language Models\n(LLM) and Agent-Based Modeling can help researchers study how different news\nfeed algorithms shape the quality of online conversations. We create realistic\npersonas using data from the American National Election Study to populate\nsimulated social media platforms. Next, we prompt the agents to read and share\nnews articles - and like or comment upon each other's messages - within three\nplatforms that use different news feed algorithms. In the first platform, users\nsee the most liked and commented posts from users whom they follow. In the\nsecond, they see posts from all users - even those outside their own network.\nThe third platform employs a novel \"bridging\" algorithm that highlights posts\nthat are liked by people with opposing political views. We find this bridging\nalgorithm promotes more constructive, non-toxic, conversation across political\ndivides than the other two models. Though further research is needed to\nevaluate these findings, we argue that LLMs hold considerable potential to\nimprove simulation research on social media and many other complex social\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2310.05984v1.pdf"
    },
    {
        "title": "HeaP: Hierarchical Policies for Web Actions using LLMs",
        "authors": [
            "Paloma Sodhi",
            "S. R. K. Branavan",
            "Ryan McDonald"
        ],
        "published": "2023-10-05T17:40:09Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nperforming a range of instruction following tasks in few and zero-shot\nsettings. However, teaching LLMs to perform tasks on the web presents\nfundamental challenges -- combinatorially large open-world tasks and variations\nacross web interfaces. We tackle these challenges by leveraging LLMs to\ndecompose web tasks into a collection of sub-tasks, each of which can be solved\nby a low-level, closed-loop policy. These policies constitute a shared grammar\nacross tasks, i.e., new web tasks can be expressed as a composition of these\npolicies. We propose a novel framework, Hierarchical Policies for Web Actions\nusing LLMs (HeaP), that learns a set of hierarchical LLM prompts from\ndemonstrations for planning high-level tasks and executing them via a sequence\nof low-level policies. We evaluate HeaP against a range of baselines on a suite\nof web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as\nlive website interactions, and show that it is able to outperform prior works\nusing orders of magnitude less data.",
        "pdf_link": "https://arxiv.org/pdf/2310.03720v1.pdf"
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
        "authors": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "published": "2023-10-05T17:12:17Z",
        "summary": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.03693v1.pdf"
    },
    {
        "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
        "authors": [
            "Oscar Sainz",
            "Iker Garc\u00eda-Ferrero",
            "Rodrigo Agerri",
            "Oier Lopez de Lacalle",
            "German Rigau",
            "Eneko Agirre"
        ],
        "published": "2023-10-05T16:43:13Z",
        "summary": "Large Language Models (LLMs) combined with instruction tuning have made\nsignificant progress when generalizing to unseen tasks. However, they have been\nless successful in Information Extraction (IE), lagging behind task-specific\nmodels. Typically, IE tasks are characterized by complex annotation guidelines\nthat describe the task and give examples to humans. Previous attempts to\nleverage such information have failed, even with the largest models, as they\nare not able to follow the guidelines out of the box. In this paper, we propose\nGoLLIE (Guideline-following Large Language Model for IE), a model able to\nimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned to\ncomply with annotation guidelines. Comprehensive evaluation empirically\ndemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,\noutperforming previous attempts at zero-shot information extraction. The\nablation study shows that detailed guidelines are key for good results.",
        "pdf_link": "https://arxiv.org/pdf/2310.03668v5.pdf"
    },
    {
        "title": "Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures",
        "authors": [
            "Thorsten H\u00e4ndler"
        ],
        "published": "2023-10-05T16:37:29Z",
        "summary": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, endowing it with sophisticated language understanding and\ngeneration capabilities. However, when faced with more complex and\ninterconnected tasks that demand a profound and iterative thought process, LLMs\nreveal their inherent limitations. Autonomous LLM-powered multi-agent systems\nrepresent a strategic response to these challenges. Such systems strive for\nautonomously tackling user-prompted goals by decomposing them into manageable\ntasks and orchestrating their execution and result synthesis through a\ncollective of specialized intelligent agents. Equipped with LLM-powered\nreasoning capabilities, these agents harness the cognitive synergy of\ncollaborating with their peers, enhanced by leveraging contextual resources\nsuch as tools and datasets. While these architectures hold promising potential\nin amplifying AI capabilities, striking the right balance between different\nlevels of autonomy and alignment remains the crucial challenge for their\neffective operation. This paper proposes a comprehensive multi-dimensional\ntaxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems\nbalance the dynamic interplay between autonomy and alignment across various\naspects inherent to architectural viewpoints such as goal-driven task\nmanagement, agent composition, multi-agent collaboration, and context\ninteraction. It also includes a domain-ontology model specifying fundamental\narchitectural concepts. Our taxonomy aims to empower researchers, engineers,\nand AI practitioners to systematically analyze the architectural dynamics and\nbalancing strategies employed by these increasingly prevalent AI systems. The\nexploratory taxonomic classification of selected representative LLM-powered\nmulti-agent systems illustrates its practical utility and reveals potential for\nfuture research and development.",
        "pdf_link": "https://arxiv.org/pdf/2310.03659v1.pdf"
    },
    {
        "title": "Redefining Digital Health Interfaces with Large Language Models",
        "authors": [
            "Fergus Imrie",
            "Paulius Rauba",
            "Mihaela van der Schaar"
        ],
        "published": "2023-10-05T14:18:40Z",
        "summary": "Digital health tools have the potential to significantly improve the delivery\nof healthcare services. However, their adoption remains comparatively limited\ndue, in part, to challenges surrounding usability and trust. Large Language\nModels (LLMs) have emerged as general-purpose models with the ability to\nprocess complex information and produce human-quality text, presenting a wealth\nof potential applications in healthcare. Directly applying LLMs in clinical\nsettings is not straightforward, however, with LLMs susceptible to providing\ninconsistent or nonsensical answers. We demonstrate how LLM-based systems can\nutilize external tools and provide a novel interface between clinicians and\ndigital technologies. This enhances the utility and practical impact of digital\nhealthcare tools and AI models while addressing current issues with using LLMs\nin clinical settings such as hallucinations. We illustrate LLM-based interfaces\nwith the example of cardiovascular disease risk prediction. We develop a new\nprognostic tool using automated machine learning and demonstrate how LLMs can\nprovide a unique interface to both our model and existing risk scores,\nhighlighting the benefit compared to traditional interfaces for digital tools.",
        "pdf_link": "https://arxiv.org/pdf/2310.03560v3.pdf"
    },
    {
        "title": "Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards",
        "authors": [
            "Litton J Kurisinkel",
            "Nancy F chen"
        ],
        "published": "2023-10-05T11:29:09Z",
        "summary": "Memory-efficient large language models are good at refining text input for\nbetter readability. However, controllability is a matter of concern when it\ncomes to text generation tasks with long inputs, such as multi-document\nsummarization. In this work, we investigate for a generic controllable approach\nfor multi-document summarization that leverages the capabilities of LLMs to\nrefine the text. In particular, we train a controllable content extraction\nscheme to extract the text that will be refined by an LLM. The scheme is\ndesigned with a novel coverage and coherence intuitive policy, which is duly\nrewarded by a passively trained LLM. Our approach yields competitive results in\nthe evaluation using ROUGE metrics and outperforms potential baselines in\ncoherence, as per human evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2310.03473v1.pdf"
    },
    {
        "title": "Evaluating Hallucinations in Chinese Large Language Models",
        "authors": [
            "Qinyuan Cheng",
            "Tianxiang Sun",
            "Wenwei Zhang",
            "Siyin Wang",
            "Xiangyang Liu",
            "Mozhi Zhang",
            "Junliang He",
            "Mianqiu Huang",
            "Zhangyue Yin",
            "Kai Chen",
            "Xipeng Qiu"
        ],
        "published": "2023-10-05T07:57:09Z",
        "summary": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.",
        "pdf_link": "https://arxiv.org/pdf/2310.03368v4.pdf"
    },
    {
        "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning",
        "authors": [
            "Timothy Chu",
            "Zhao Song",
            "Chiwun Yang"
        ],
        "published": "2023-10-05T06:16:01Z",
        "summary": "In-context learning (ICL) is an astonishing emergent ability of large\nlanguage models (LLMs). By presenting a prompt that includes multiple\ninput-output pairs as examples and introducing a new query input, models can\ngenerate the corresponding output. However, the performance of models heavily\nrelies on the quality of the input prompt when implementing in-context\nlearning. Biased or imbalanced input prompts can significantly degrade the\nperformance of language models. To address this issue, we introduce a\nreweighted algorithm called RICL (Reweighted In-context Learning). This\nalgorithm fine-tunes language models using an unbiased validation set to\ndetermine the optimal weight for each input-output example to approximate\nunbiased in-context learning. Furthermore, we also introduce a low-cost\nreweighted algorithm, a linear optimal weight approximation algorithm called\nLARICL (Linear Approximation of Reweighted In-context Learning). This algorithm\nrequires minimal training cost while providing effective results. We prove the\nconvergence of our algorithm and validate its performance through experiments\nconducted on a numerical dataset. The experimental findings reveal a\nsubstantial improvement in comparison to benchmarks including the performance\nof casual prompt-based in-context learning and the performance of a classic\nfine-tuning method.",
        "pdf_link": "https://arxiv.org/pdf/2310.03331v1.pdf"
    },
    {
        "title": "Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise",
        "authors": [
            "Zhen wan",
            "Yating Zhang",
            "Yexiang Wang",
            "Fei Cheng",
            "Sadao Kurohashi"
        ],
        "published": "2023-10-05T05:55:06Z",
        "summary": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased",
        "pdf_link": "https://arxiv.org/pdf/2310.03328v2.pdf"
    },
    {
        "title": "Investigating the Limitation of CLIP Models: The Worst-Performing Categories",
        "authors": [
            "Jie-Jing Shao",
            "Jiang-Xin Shi",
            "Xiao-Wen Yang",
            "Lan-Zhe Guo",
            "Yu-Feng Li"
        ],
        "published": "2023-10-05T05:37:33Z",
        "summary": "Contrastive Language-Image Pre-training (CLIP) provides a foundation model by\nintegrating natural language into visual concepts, enabling zero-shot\nrecognition on downstream tasks. It is usually expected that satisfactory\noverall accuracy can be achieved across numerous domains through well-designed\ntextual prompts. However, we found that their performance in the worst\ncategories is significantly inferior to the overall performance. For example,\non ImageNet, there are a total of 10 categories with class-wise accuracy as low\nas 0\\%, even though the overall performance has achieved 64.1\\%. This\nphenomenon reveals the potential risks associated with using CLIP models,\nparticularly in risk-sensitive applications where specific categories hold\nsignificant importance. To address this issue, we investigate the alignment\nbetween the two modalities in the CLIP model and propose the Class-wise\nMatching Margin (\\cmm) to measure the inference confusion. \\cmm\\ can\neffectively identify the worst-performing categories and estimate the potential\nperformance of the candidate prompts. We further query large language models to\nenrich descriptions of worst-performing categories and build a weighted\nensemble to highlight the efficient prompts. Experimental results clearly\nverify the effectiveness of our proposal, where the accuracy on the worst-10\ncategories on ImageNet is boosted to 5.2\\%, without manual prompt engineering,\nlaborious optimization, or access to labeled validation data.",
        "pdf_link": "https://arxiv.org/pdf/2310.03324v1.pdf"
    },
    {
        "title": "Learning Personalized Story Evaluation",
        "authors": [
            "Danqing Wang",
            "Kevin Yang",
            "Hanlin Zhu",
            "Xiaomeng Yang",
            "Andrew Cohen",
            "Lei Li",
            "Yuandong Tian"
        ],
        "published": "2023-10-05T04:15:48Z",
        "summary": "While large language models (LLMs) have shown impressive results for more\nobjective tasks such as QA and retrieval, it remains nontrivial to evaluate\ntheir performance on open-ended text generation for reasons including (1) data\ncontamination; (2) multi-dimensional evaluation criteria; and (3)\nsubjectiveness stemming from reviewers' personal preferences. To address such\nissues, we propose to model personalization in an uncontaminated open-ended\ngeneration assessment. We create two new datasets Per-MPST and Per-DOC for\npersonalized story evaluation, by re-purposing existing datasets with proper\nanonymization and new personalized labels. We further develop a personalized\nstory evaluation model PERSE to infer reviewer preferences and provide a\npersonalized evaluation. Specifically, given a few exemplary reviews from a\nparticular reviewer, PERSE predicts either a detailed review or fine-grained\ncomparison in several aspects (such as interestingness and surprise) for that\nreviewer on a new text input. Experimental results show that PERSE outperforms\nGPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on\npairwise preference prediction accuracy. Both datasets and code will be\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2310.03304v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models As AI Research Agents",
        "authors": [
            "Qian Huang",
            "Jian Vora",
            "Percy Liang",
            "Jure Leskovec"
        ],
        "published": "2023-10-05T04:06:12Z",
        "summary": "Scientific experimentation involves an iterative process of creating\nhypotheses, designing experiments, running experiments, and analyzing the\nresults. Can we build AI research agents to perform these long-horizon tasks?\nTo take a step towards building and evaluating research agents on such\nopen-ended decision-making tasks, we focus on the problem of machine learning\nengineering: given a task description and a dataset, build a high-performing\nmodel. In this paper, we propose MLAgentBench, a suite of ML tasks for\nbenchmarking AI research agents. Agents can perform actions like\nreading/writing files, executing code, and inspecting outputs. With these\nactions, agents could run experiments, analyze the results, and modify the code\nof entire machine learning pipelines, such as data processing, architecture,\ntraining processes, etc. The benchmark then automatically evaluates the agent's\nperformance objectively over various metrics related to performance and\nefficiency. We also design an LLM-based research agent to automatically perform\nexperimentation loops in such an environment. Empirically, we find that a\nGPT-4-based research agent can feasibly build compelling ML models over many\ntasks in MLAgentBench, displaying highly interpretable plans and actions.\nHowever, the success rates vary considerably; they span from almost 90\\% on\nwell-established older datasets to as low as 10\\% on recent Kaggle Challenges\n-- unavailable during the LLM model's pretraining -- and even 0\\% on newer\nresearch challenges like BabyLM. Finally, we identify several key challenges\nfor LLM-based research agents such as long-term planning and hallucination. Our\ncode is released at https://github.com/snap-stanford/MLAgentBench.",
        "pdf_link": "https://arxiv.org/pdf/2310.03302v1.pdf"
    },
    {
        "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions",
        "authors": [
            "Siwei Wu",
            "Xiangqing Shen",
            "Rui Xia"
        ],
        "published": "2023-10-05T03:45:54Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2310.03293v1.pdf"
    },
    {
        "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
        "authors": [
            "Ke Shen",
            "Mayank Kejriwal"
        ],
        "published": "2023-10-05T03:20:41Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive\nmilestones in natural language processing (NLP). Despite their impressive\nperformance, the models are known to pose important risks. As these models are\ndeployed in real-world applications, a systematic understanding of different\nrisks posed by these models on tasks such as natural language inference (NLI),\nis much needed. In this paper, we define and formalize two distinct types of\nrisk: decision risk and composite risk. We also propose a risk-centric\nevaluation framework, and four novel metrics, for assessing LLMs on these risks\nin both in-domain and out-of-domain settings. Finally, we propose a\nrisk-adjusted calibration method called DwD for helping LLMs minimize these\nrisks in an overall NLI architecture. Detailed experiments, using four NLI\nbenchmarks, three baselines and two LLMs, including ChatGPT, show both the\npractical utility of the evaluation framework, and the efficacy of DwD in\nreducing decision and composite risk. For instance, when using DwD, an\nunderlying LLM is able to address an extra 20.1% of low-risk inference tasks\n(but which the LLM erroneously deems high-risk without risk adjustment) and\nskip a further 19.8% of high-risk tasks, which would have been answered\nincorrectly.",
        "pdf_link": "https://arxiv.org/pdf/2310.03283v1.pdf"
    },
    {
        "title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction",
        "authors": [
            "Zeyuan Wang",
            "Qiang Zhang",
            "Keyan Ding",
            "Ming Qin",
            "Xiang Zhuang",
            "Xiaotong Li",
            "Huajun Chen"
        ],
        "published": "2023-10-05T02:45:39Z",
        "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.",
        "pdf_link": "https://arxiv.org/pdf/2310.03269v1.pdf"
    },
    {
        "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
        "authors": [
            "Shengding Hu",
            "Xin Liu",
            "Xu Han",
            "Xinrong Zhang",
            "Chaoqun He",
            "Weilin Zhao",
            "Yankai Lin",
            "Ning Ding",
            "Zebin Ou",
            "Guoyang Zeng",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-10-05T02:35:00Z",
        "summary": "The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.",
        "pdf_link": "https://arxiv.org/pdf/2310.03262v2.pdf"
    },
    {
        "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning",
        "authors": [
            "Mohamed Aghzal",
            "Erion Plaku",
            "Ziyu Yao"
        ],
        "published": "2023-10-05T01:42:16Z",
        "summary": "Large language models (LLMs) have achieved remarkable success across a wide\nspectrum of tasks; however, they still face limitations in scenarios that\ndemand long-term planning and spatial reasoning. To facilitate this line of\nresearch, in this work, we propose a new benchmark, termed $\\textbf{P}$ath\n$\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage\n($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by\nformulating ''path planning'' tasks that require an LLM to navigate to target\nlocations while avoiding obstacles and adhering to constraints. Leveraging this\nbenchmark, we systematically investigate LLMs including GPT-4 via different\nfew-shot prompting methodologies as well as BART and T5 of various sizes via\nfine-tuning. Our experimental results show the promise of few-shot GPT-4 in\nspatial reasoning, when it is prompted to reason and act interleavedly,\nalthough it still fails to perform long-term temporal reasoning. In contrast,\nwhile fine-tuned LLMs achieved impressive results on in-distribution reasoning\ntasks, they struggled to generalize to larger environments or environments with\nmore obstacles.",
        "pdf_link": "https://arxiv.org/pdf/2310.03249v2.pdf"
    },
    {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "authors": [
            "Tu Vu",
            "Mohit Iyyer",
            "Xuezhi Wang",
            "Noah Constant",
            "Jerry Wei",
            "Jason Wei",
            "Chris Tar",
            "Yun-Hsuan Sung",
            "Denny Zhou",
            "Quoc Le",
            "Thang Luong"
        ],
        "published": "2023-10-05T00:04:12Z",
        "summary": "Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.",
        "pdf_link": "https://arxiv.org/pdf/2310.03214v2.pdf"
    },
    {
        "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
        "authors": [
            "Xiaohan Fu",
            "Zihan Wang",
            "Shuheng Li",
            "Rajesh K. Gupta",
            "Niloofar Mireshghallah",
            "Taylor Berg-Kirkpatrick",
            "Earlence Fernandes"
        ],
        "published": "2023-10-04T22:10:01Z",
        "summary": "Large Language Models (LLMs) are being enhanced with the ability to use tools\nand to process multiple modalities. These new capabilities bring new benefits\nand also new security risks. In this work, we show that an attacker can use\nvisual adversarial examples to cause attacker-desired tool usage. For example,\nthe attacker could cause a victim LLM to delete calendar events, leak private\nconversations and book hotels. Different from prior work, our attacks can\naffect the confidentiality and integrity of user resources connected to the LLM\nwhile being stealthy and generalizable to multiple input prompts. We construct\nthese attacks using gradient-based adversarial training and characterize\nperformance along multiple dimensions. We find that our adversarial images can\nmanipulate the LLM to invoke tools following real-world syntax almost always\n(~98%) while maintaining high similarity to clean images (~0.9 SSIM).\nFurthermore, using human scoring and automated metrics, we find that the\nattacks do not noticeably affect the conversation (and its semantics) between\nthe user and the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.03185v1.pdf"
    },
    {
        "title": "Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference",
        "authors": [
            "Zachary Levonian",
            "Chenglu Li",
            "Wangda Zhu",
            "Anoushka Gade",
            "Owen Henkel",
            "Millie-Ellen Postle",
            "Wanli Xing"
        ],
        "published": "2023-10-04T22:09:28Z",
        "summary": "For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.",
        "pdf_link": "https://arxiv.org/pdf/2310.03184v2.pdf"
    },
    {
        "title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis",
        "authors": [
            "Zishun Yu",
            "Yunzhe Tao",
            "Liyu Chen",
            "Tao Sun",
            "Hongxia Yang"
        ],
        "published": "2023-10-04T21:40:36Z",
        "summary": "Program synthesis aims to create accurate, executable programs from problem\nspecifications, specifically from natural language descriptions in our context.\nRecent studies have leveraged the power of reinforcement learning (RL) in\nconjunction with large language models (LLMs), significantly enhancing code\ngeneration capabilities. The application of RL focuses on directly optimizing\nfor functional correctness, offering an advantage over conventional supervised\nmethods. Despite policy-based RL methods dominating the literature on RL for\nprogram synthesis, the nature of program synthesis tasks hints at a natural\nalignment with value-based methods. This stems from the rich collection of\noff-policy programs, including those developed by human programmers and also\nhistorical samples, coupled with the straightforward verification of generated\nprograms through automated unit testing, meaning rewards are easy to obtain.\nDiverging from the dominant use of policy-based algorithms, our work explores\nthe feasibility of value-based approaches, leading to the development of our\n$\\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based\nmethods presents challenges due to the enormous search space inherent to\nprogram synthesis. To this end, we introduce an initialization protocol for RL\nagents utilizing pre-trained LMs and a conservative Bellman operator to reduce\ntraining complexities. Moreover, we demonstrate how to leverage the learned\nvalue functions as a dual strategy to post-process generated programs. Our\nempirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in\nachieving state-of-the-art performance when compared to policy-based methods.\nRemarkably, this achievement is reached with minimal reward engineering effort,\nhighlighting the effectiveness of value-based RL, independent of reward\ndesigns.",
        "pdf_link": "https://arxiv.org/pdf/2310.03173v2.pdf"
    },
    {
        "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
        "authors": [
            "Satwik Bhattamishra",
            "Arkil Patel",
            "Phil Blunsom",
            "Varun Kanade"
        ],
        "published": "2023-10-04T17:57:33Z",
        "summary": "In order to understand the in-context learning phenomenon, recent works have\nadopted a stylized experimental framework and demonstrated that Transformers\ncan learn gradient-based learning algorithms for various classes of real-valued\nfunctions. However, the limitations of Transformers in implementing learning\nalgorithms, and their ability to learn other forms of algorithms are not well\nunderstood. Additionally, the degree to which these capabilities are confined\nto attention-based models is unclear. Furthermore, it remains to be seen\nwhether the insights derived from these stylized settings can be extrapolated\nto pretrained Large Language Models (LLMs). In this work, we take a step\ntowards answering these questions by demonstrating the following: (a) On a\ntest-bed with a variety of Boolean function classes, we find that Transformers\ncan nearly match the optimal learning algorithm for 'simpler' tasks, while\ntheir performance deteriorates on more 'complex' tasks. Additionally, we find\nthat certain attention-free models perform (almost) identically to Transformers\non a range of tasks. (b) When provided a teaching sequence, i.e. a set of\nexamples that uniquely identifies a function in a class, we show that\nTransformers learn more sample-efficiently. Interestingly, our results show\nthat Transformers can learn to implement two distinct algorithms to solve a\nsingle task, and can adaptively select the more sample-efficient algorithm\ndepending on the sequence of in-context examples. (c) Lastly, we show that\nextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines\non prediction tasks that are guaranteed to not be in their training set.",
        "pdf_link": "https://arxiv.org/pdf/2310.03016v1.pdf"
    },
    {
        "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "authors": [
            "Siddharth Samsi",
            "Dan Zhao",
            "Joseph McDonald",
            "Baolin Li",
            "Adam Michaleas",
            "Michael Jones",
            "William Bergeron",
            "Jeremy Kepner",
            "Devesh Tiwari",
            "Vijay Gadepally"
        ],
        "published": "2023-10-04T17:41:59Z",
        "summary": "Large language models (LLMs) have exploded in popularity due to their new\ngenerative capabilities that go far beyond prior state-of-the-art. These\ntechnologies are increasingly being leveraged in various domains such as law,\nfinance, and medicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for inference.\nInference energy costs already receive less attention than the energy costs of\ntraining LLMs -- despite how often these large models are called on to conduct\ninference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see\nincreasing usage and deployment in various domains, a better understanding of\ntheir resource utilization is crucial for cost-savings, scaling performance,\nefficient hardware usage, and optimal inference strategies.\n  In this paper, we describe experiments conducted to study the computational\nand energy utilization of inference with LLMs. We benchmark and conduct a\npreliminary analysis of the inference performance and inference energy costs of\ndifferent sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta\nAI on two generations of popular GPUs (NVIDIA V100 \\& A100) and two datasets\n(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in\nresearch and practice. We present the results of multi-node, multi-GPU\ninference using model sharding across up to 32 GPUs. To our knowledge, our work\nis the one of the first to study LLM inference performance from the perspective\nof computational and energy resources at this scale.",
        "pdf_link": "https://arxiv.org/pdf/2310.03003v1.pdf"
    },
    {
        "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning",
        "authors": [
            "Chang Gao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Wai Lam"
        ],
        "published": "2023-10-04T16:44:23Z",
        "summary": "Instruction tuning has emerged as a crucial process for harnessing the\ncapabilities of large language models (LLMs) by providing explicit task\ninstructions, leading to improved performance in various tasks. However,\nprevalent text-to-text instruction tuning (TextTuning) methods suffer from\nlimitations in generalization, robustness, and controllability due to the\nambiguity and lack of explicit structure in tasks. In this paper, we propose\nJsonTuning, a novel structure-to-structure approach for instruction tuning. By\nleveraging the versatility and structured nature of JSON to represent tasks,\nJsonTuning enhances generalization by helping the model understand essential\ntask elements and their relations, improves robustness by minimizing ambiguity,\nand increases controllability by providing explicit control over the output. We\nconduct a comprehensive comparative study with diverse language models and\nevaluation benchmarks. Experimental results show that JsonTuning outperforms\nTextTuning in various applications, showcasing improved performance,\nadaptability, robustness, and controllability. By overcoming the limitations of\nTextTuning, JsonTuning demonstrates significant potential for more effective\nand reliable LLMs capable of handling diverse scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2310.02953v2.pdf"
    },
    {
        "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
        "authors": [
            "Xianjun Yang",
            "Xiao Wang",
            "Qi Zhang",
            "Linda Petzold",
            "William Yang Wang",
            "Xun Zhao",
            "Dahua Lin"
        ],
        "published": "2023-10-04T16:39:31Z",
        "summary": "Warning: This paper contains examples of harmful language, and reader\ndiscretion is recommended. The increasing open release of powerful large\nlanguage models (LLMs) has facilitated the development of downstream\napplications by reducing the essential cost of data annotation and computation.\nTo ensure AI safety, extensive safety-alignment measures have been conducted to\narmor these models against malicious use (primarily hard prompt attack).\nHowever, beneath the seemingly resilient facade of the armor, there might lurk\na shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these\nsafely aligned LLMs can be easily subverted to generate harmful content.\nFormally, we term a new attack as Shadow Alignment: utilizing a tiny amount of\ndata can elicit safely-aligned models to adapt to harmful tasks without\nsacrificing model helpfulness. Remarkably, the subverted models retain their\ncapability to respond appropriately to regular inquiries. Experiments across 8\nmodels released by 5 different organizations (LLaMa-2, Falcon, InternLM,\nBaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.\nBesides, the single-turn English-only attack successfully transfers to\nmulti-turn dialogue and other languages. This study serves as a clarion call\nfor a collective effort to overhaul and fortify the safety of open-source LLMs\nagainst malicious attackers.",
        "pdf_link": "https://arxiv.org/pdf/2310.02949v1.pdf"
    },
    {
        "title": "Assessing Large Language Models on Climate Information",
        "authors": [
            "Jannis Bulian",
            "Mike S. Sch\u00e4fer",
            "Afra Amini",
            "Heidi Lam",
            "Massimiliano Ciaramita",
            "Ben Gaiarin",
            "Michelle Chen Huebscher",
            "Christian Buck",
            "Niels Mede",
            "Markus Leippold",
            "Nadine Strauss"
        ],
        "published": "2023-10-04T16:09:48Z",
        "summary": "Understanding how climate change affects us and learning about available\nsolutions are key steps toward empowering individuals and communities to\nmitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,\nit is necessary to assess their capability in this domain. In this study, we\npresent a comprehensive evaluation framework, grounded in science communication\nprinciples, to analyze LLM responses to climate change topics. Our framework\nemphasizes both the presentational and epistemological adequacy of answers,\noffering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our\nframework discerns up to 30 distinct issues in model outputs. The task is a\nreal-world example of a growing number of challenging problems where AI can\ncomplement and lift human performance. We introduce a novel and practical\nprotocol for scalable oversight that uses AI Assistance and relies on raters\nwith relevant educational backgrounds. We evaluate several recent LLMs and\nconduct a comprehensive analysis of the results, shedding light on both the\npotential and the limitations of LLMs in the realm of climate communication.",
        "pdf_link": "https://arxiv.org/pdf/2310.02932v1.pdf"
    },
    {
        "title": "Large language models in textual analysis for gesture selection",
        "authors": [
            "Laura B. Hensel",
            "Nutchanon Yongsatianchot",
            "Parisa Torshizi",
            "Elena Minucci",
            "Stacy Marsella"
        ],
        "published": "2023-10-04T14:46:37Z",
        "summary": "Gestures perform a variety of communicative functions that powerfully\ninfluence human face-to-face interaction. How this communicative function is\nachieved varies greatly between individuals and depends on the role of the\nspeaker and the context of the interaction. Approaches to automatic gesture\ngeneration vary not only in the degree to which they rely on data-driven\ntechniques but also the degree to which they can produce context and speaker\nspecific gestures. However, these approaches face two major challenges: The\nfirst is obtaining sufficient training data that is appropriate for the context\nand the goal of the application. The second is related to designer control to\nrealize their specific intent for the application. Here, we approach these\nchallenges by using large language models (LLMs) to show that these powerful\nmodels of large amounts of data can be adapted for gesture analysis and\ngeneration. Specifically, we used ChatGPT as a tool for suggesting\ncontext-specific gestures that can realize designer intent based on minimal\nprompts. We also find that ChatGPT can suggests novel yet appropriate gestures\nnot present in the minimal training data. The use of LLMs is a promising avenue\nfor gesture generation that reduce the need for laborious annotations and has\nthe potential to flexibly and quickly adapt to different designer intents.",
        "pdf_link": "https://arxiv.org/pdf/2310.13705v1.pdf"
    },
    {
        "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
        "authors": [
            "Pei Zhou",
            "Aman Madaan",
            "Srividya Pranavi Potharaju",
            "Aditya Gupta",
            "Kevin R. McKee",
            "Ari Holtzman",
            "Jay Pujara",
            "Xiang Ren",
            "Swaroop Mishra",
            "Aida Nematzadeh",
            "Shyam Upadhyay",
            "Manaal Faruqui"
        ],
        "published": "2023-10-04T06:47:58Z",
        "summary": "\"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2310.03051v1.pdf"
    },
    {
        "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
        "authors": [
            "Soroush Abbasi Koohpayegani",
            "KL Navaneet",
            "Parsa Nooralinejad",
            "Soheil Kolouri",
            "Hamed Pirsiavash"
        ],
        "published": "2023-10-04T03:30:24Z",
        "summary": "Large Language Models (LLMs) have recently gained popularity due to their\nimpressive few-shot performance across various downstream tasks. However,\nfine-tuning all parameters and storing a unique model for each downstream task\nor domain becomes impractical because of the massive size of checkpoints (e.g.,\n350GB in GPT-3). Current literature, such as LoRA, showcases the potential of\nlow-rank modifications to the original weights of an LLM, enabling efficient\nadaptation and storage for task-specific models. These methods can reduce the\nnumber of parameters needed to fine-tune an LLM by several orders of magnitude.\nYet, these methods face two primary limitations: 1) the parameter reduction is\nlower-bounded by the rank one decomposition, and 2) the extent of reduction is\nheavily influenced by both the model architecture and the chosen rank. For\ninstance, in larger models, even a rank one decomposition might exceed the\nnumber of parameters truly needed for adaptation. In this paper, we introduce\nNOLA, which overcomes the rank one lower bound present in LoRA. It achieves\nthis by re-parameterizing the low-rank matrices in LoRA using linear\ncombinations of randomly generated matrices (basis) and optimizing the linear\nmixture coefficients only. This approach allows us to decouple the number of\ntrainable parameters from both the choice of rank and the network architecture.\nWe present adaptation results using GPT-2 and ViT in natural language and\ncomputer vision tasks. NOLA performs as well as, or better than models with\nequivalent parameter counts. Furthermore, we demonstrate that we can halve the\nparameters in larger models compared to LoRA with rank one, without sacrificing\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2310.02556v1.pdf"
    },
    {
        "title": "Low-Resource Languages Jailbreak GPT-4",
        "authors": [
            "Zheng-Xin Yong",
            "Cristina Menghini",
            "Stephen H. Bach"
        ],
        "published": "2023-10-03T21:30:56Z",
        "summary": "AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.",
        "pdf_link": "https://arxiv.org/pdf/2310.02446v2.pdf"
    },
    {
        "title": "Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions",
        "authors": [
            "Naiming Liu",
            "Shashank Sonkar",
            "Zichao Wang",
            "Simon Woodhead",
            "Richard G. Baraniuk"
        ],
        "published": "2023-10-03T21:19:50Z",
        "summary": "We propose novel evaluations for mathematical reasoning capabilities of Large\nLanguage Models (LLMs) based on mathematical misconceptions. Our primary\napproach is to simulate LLMs as a novice learner and an expert tutor, aiming to\nidentify the incorrect answer to math question resulted from a specific\nmisconception and to recognize the misconception(s) behind an incorrect answer,\nrespectively. Contrary to traditional LLMs-based mathematical evaluations that\nfocus on answering math questions correctly, our approach takes inspirations\nfrom principles in educational learning sciences. We explicitly ask LLMs to\nmimic a novice learner by answering questions in a specific incorrect manner\nbased on incomplete knowledge; and to mimic an expert tutor by identifying\nmisconception(s) corresponding to an incorrect answer to a question. Using\nsimple grade-school math problems, our experiments reveal that, while LLMs can\neasily answer these questions correctly, they struggle to identify 1) the\nincorrect answer corresponding to specific incomplete knowledge\n(misconceptions); 2) the misconceptions that explain particular incorrect\nanswers. Our study indicates new opportunities for enhancing LLMs' math\nreasoning capabilities, especially on developing robust student simulation and\nexpert tutoring models in the educational applications such as intelligent\ntutoring systems.",
        "pdf_link": "https://arxiv.org/pdf/2310.02439v1.pdf"
    },
    {
        "title": "Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions",
        "authors": [
            "Yufan Chen",
            "Arjun Arunasalam",
            "Z. Berkay Celik"
        ],
        "published": "2023-10-03T20:54:29Z",
        "summary": "Users seek security & privacy (S&P) advice from online resources, including\ntrusted websites and content-sharing platforms. These resources help users\nunderstand S&P technologies and tools and suggest actionable strategies. Large\nLanguage Models (LLMs) have recently emerged as trusted information sources.\nHowever, their accuracy and correctness have been called into question. Prior\nresearch has outlined the shortcomings of LLMs in answering multiple-choice\nquestions and user ability to inadvertently circumvent model restrictions\n(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable\nS&P advice is not well-explored. In this paper, we measure their ability to\nrefute popular S&P misconceptions that the general public holds. We first study\nrecent academic literature to curate a dataset of over a hundred S&P-related\nmisconceptions across six different topics. We then query two popular LLMs\n(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to\nthese misconceptions. To comprehensively evaluate their responses, we further\napply three strategies: query each misconception multiple times, generate and\nquery their paraphrases, and solicit source URLs of the responses. Both models\ndemonstrate, on average, a 21.3% non-negligible error rate, incorrectly\nsupporting popular S&P misconceptions. The error rate increases to 32.6% when\nwe repeatedly query LLMs with the same or paraphrased misconceptions. We also\nexpose that models may partially support a misconception or remain\nnoncommittal, refusing a firm stance on misconceptions. Our exploration of\ninformation sources for responses revealed that LLMs are susceptible to\nproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to\nunrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).",
        "pdf_link": "https://arxiv.org/pdf/2310.02431v1.pdf"
    },
    {
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
        "authors": [
            "Xiaogeng Liu",
            "Nan Xu",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2023-10-03T19:44:37Z",
        "summary": "The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.",
        "pdf_link": "https://arxiv.org/pdf/2310.04451v2.pdf"
    },
    {
        "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework",
        "authors": [
            "Mahyar Abbasian",
            "Iman Azimi",
            "Amir M. Rahmani",
            "Ramesh Jain"
        ],
        "published": "2023-10-03T18:54:10Z",
        "summary": "Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via three demonstrations. Moreover, we release openCHA as open\nsource available to the community via GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2310.02374v4.pdf"
    },
    {
        "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation",
        "authors": [
            "Benjamin Steenhoek",
            "Michele Tufano",
            "Neel Sundaresan",
            "Alexey Svyatkovskiy"
        ],
        "published": "2023-10-03T18:48:31Z",
        "summary": "Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at this link: https://figshare.com/s/ded476c8d4c221222849.",
        "pdf_link": "https://arxiv.org/pdf/2310.02368v1.pdf"
    },
    {
        "title": "Investigating Large Language Models' Perception of Emotion Using Appraisal Theory",
        "authors": [
            "Nutchanon Yongsatianchot",
            "Parisa Ghanad Torshizi",
            "Stacy Marsella"
        ],
        "published": "2023-10-03T16:34:47Z",
        "summary": "Large Language Models (LLM) like ChatGPT have significantly advanced in\nrecent years and are now being used by the general public. As more people\ninteract with these systems, improving our understanding of these black box\nmodels is crucial, especially regarding their understanding of human\npsychological aspects. In this work, we investigate their emotion perception\nthrough the lens of appraisal and coping theory using the Stress and Coping\nProcess Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting\nof multiple stories that evolve over time and differ in key appraisal variables\nsuch as controllability and changeability. We applied SCPQ to three recent LLMs\nfrom OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with\npredictions from the appraisal theory and human data. The results show that\nLLMs' responses are similar to humans in terms of dynamics of appraisal and\ncoping, but their responses did not differ along key appraisal dimensions as\npredicted by the theory and data. The magnitude of their responses is also\nquite different from humans in several variables. We also found that GPTs can\nbe quite sensitive to instruction and how questions are asked. This work adds\nto the growing literature evaluating the psychological aspects of LLMs and\nhelps enrich our understanding of the current models.",
        "pdf_link": "https://arxiv.org/pdf/2310.04450v1.pdf"
    },
    {
        "title": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization",
        "authors": [
            "Zijun Liu",
            "Yanzhe Zhang",
            "Peng Li",
            "Yang Liu",
            "Diyi Yang"
        ],
        "published": "2023-10-03T16:05:48Z",
        "summary": "Large language model (LLM) agents have been shown effective on a wide range\nof tasks, and by ensembling multiple LLM agents, their performances could be\nfurther improved. Existing approaches employ a fixed set of agents to interact\nwith each other in a static architecture, which limits their generalizability\nto various tasks and requires strong human prior in designing these agents. In\nthis work, we propose to construct a strategic team of agents communicating in\na dynamic interaction architecture based on the task query. Specifically, we\nbuild a framework named Dynamic LLM-Agent Network ($\\textbf{DyLAN}$) for\nLLM-agent collaboration on complicated tasks like reasoning and code\ngeneration. DyLAN enables agents to interact for multiple rounds in a dynamic\narchitecture with inference-time agent selection and an early-stopping\nmechanism to improve performance and efficiency. We further design an automatic\nagent team optimization algorithm based on an unsupervised metric termed\n$\\textit{Agent Importance Score}$, enabling the selection of best agents based\non the contribution each agent makes. Empirically, we demonstrate that DyLAN\nperforms well in both reasoning and code generation tasks with reasonable\ncomputational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and\nHumanEval, respectively, compared to a single execution on GPT-35-turbo. On\nspecific subjects of MMLU, agent team optimization in DyLAN increases accuracy\nby up to 25.0%.",
        "pdf_link": "https://arxiv.org/pdf/2310.02170v1.pdf"
    },
    {
        "title": "Editing Personality for Large Language Models",
        "authors": [
            "Shengyu Mao",
            "Xiaohan Wang",
            "Mengru Wang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Ningyu Zhang"
        ],
        "published": "2023-10-03T16:02:36Z",
        "summary": "This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct a new benchmark dataset PersonalityEdit to address this task. Drawing\non the theory in Social Psychology, we isolate three representative traits,\nnamely Neuroticism, Extraversion, and Agreeableness, as the foundation for our\nbenchmark. We then gather data using GPT-4, generating responses that not only\nalign with a specified topic but also embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our intriguing findings uncover\npotential challenges of the proposed task, illustrating several remaining\nissues. We anticipate that our work can provide the NLP community with\ninsights. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.",
        "pdf_link": "https://arxiv.org/pdf/2310.02168v3.pdf"
    },
    {
        "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
        "authors": [
            "Zhoubo Li",
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Mengru Wang",
            "Xi Chen",
            "Huajun Chen"
        ],
        "published": "2023-10-03T15:10:46Z",
        "summary": "As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.",
        "pdf_link": "https://arxiv.org/pdf/2310.02129v4.pdf"
    },
    {
        "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
        "authors": [
            "Zhen Bi",
            "Ningyu Zhang",
            "Yida Xue",
            "Yixin Ou",
            "Daxiong Ji",
            "Guozhou Zheng",
            "Huajun Chen"
        ],
        "published": "2023-10-03T13:17:35Z",
        "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reason may be the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\na novel framework to automatically obtain a large volume of ocean domain\ninstruction data, which generates instructions based on multi-agent\ncollaboration. Additionally, we construct the first oceanography benchmark,\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\nexpertise for oceans science tasks but also gains preliminary embodied\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\nsoon be available at https://github.com/zjunlp/KnowLM.",
        "pdf_link": "https://arxiv.org/pdf/2310.02031v6.pdf"
    },
    {
        "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
        "authors": [
            "Aniruddha Deb",
            "Neeva Oza",
            "Sarthak Singla",
            "Dinesh Khandelwal",
            "Dinesh Garg",
            "Parag Singla"
        ],
        "published": "2023-10-03T12:03:06Z",
        "summary": "While forward reasoning (i.e. find the answer given the question) has been\nexplored extensively in the recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation?\n  In this paper, we formally define the backward reasoning task on math word\nproblems and modify three datasets to evaluate this task: GSM8k, SVAMP and\nMultiArith. Our findings show a significant drop in the accuracy of models on\nbackward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,\nGPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we\npropose three novel techniques that improve performance: Rephrase reformulates\nthe given problem into a forward reasoning problem, PAL-Tools combines the idea\nof Program-Aided LLMs to produce a set of equations that can be solved by an\nexternal solver, and Check your Work exploits the availability of natural\nverifier of high accuracy in the forward direction, interleaving solving and\nverification steps. Finally, realizing that each of our base methods correctly\nsolves a different set of problems, we propose a novel Bayesian formulation for\ncreating an ensemble over these base methods aided by a verifier to further\nboost the accuracy by a significant margin. Extensive experimentation\ndemonstrates that our techniques successively improve the performance of LLMs\non the backward reasoning task, with the final ensemble-based method resulting\nin a substantial performance gain compared to the raw LLMs with standard\nprompting techniques such as chain-of-thought.",
        "pdf_link": "https://arxiv.org/pdf/2310.01991v1.pdf"
    },
    {
        "title": "Formalizing Natural Language Intent into Program Specifications via Large Language Models",
        "authors": [
            "Madeline Endres",
            "Sarah Fakhoury",
            "Saikat Chakraborty",
            "Shuvendu K. Lahiri"
        ],
        "published": "2023-10-03T06:55:45Z",
        "summary": "Informal natural language that describes code functionality, such as code\ncomments or function documentation, may contain substantial information about a\nprograms intent. However, there is typically no guarantee that a programs\nimplementation and natural language documentation are aligned. In the case of a\nconflict, leveraging information in code-adjacent natural language has the\npotential to enhance fault localization, debugging, and code trustworthiness.\nIn practice, however, this information is often underutilized due to the\ninherent ambiguity of natural language which makes natural language intent\nchallenging to check programmatically. The \"emergent abilities\" of Large\nLanguage Models (LLMs) have the potential to facilitate the translation of\nnatural language intent to programmatically checkable assertions. However, it\nis unclear if LLMs can correctly translate informal natural language\nspecifications into formal specifications that match programmer intent.\nAdditionally, it is unclear if such translation could be useful in practice. In\nthis paper, we describe LLM4nl2post, the problem leveraging LLMs for\ntransforming informal natural language to formal method postconditions,\nexpressed as program assertions. We introduce and validate metrics to measure\nand compare different LLM4nl2post approaches, using the correctness and\ndiscriminative power of generated postconditions. We then perform qualitative\nand quantitative methods to assess the quality of LLM4nl2post postconditions,\nfinding that they are generally correct and able to discriminate incorrect\ncode. Finally, we find that LLM4nl2post via LLMs has the potential to be\nhelpful in practice; specifications generated from natural language were able\nto catch 70 real-world historical bugs from Defects4J.",
        "pdf_link": "https://arxiv.org/pdf/2310.01831v1.pdf"
    }
]