Title,Talks about LLMs,Rate,Evidence,Year ,Month 
Arabic Dialect Identification with a Few Labeled Examples Using Generative Adversarial Networks,Yes.,2,"""However, to fine-tune these models, a large corpus is required. Getting a large number high quality labeled examples for some Dialect Arabic classes is challenging and time-consuming.""",2022,
Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts,,,,2022,
VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models,Yes.,4,"""Experiments on six representative pre-trained vision-language models demonstrate that stereotypical biases clearly exist in most of these models and across all four bias categories, with gender bias slightly more evident.""",2022,November 
Is Encoder-Decoder Redundant for Neural Machine Translation?,,,,2022,
SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features,,,,2022,
Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,Yes.,3,"""SPLADE still struggles with exact matching of low-frequency words in training data. In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE.""",2022,
Cross-lingual Few-Shot Learning on Unseen Languages,Yes.,3,"""However, this was mostly studied for relatively resource-rich languages, where at least enough unlabeled data is available to be included in pre-training a multilingual language model."" and ""we explore the problem of cross-lingual transfer in unseen languages, where no unlabeled data is available for pre-training a model.""",2022,
Enhancing Financial Table and Text Question Answering with Tabular Graph and Numerical Reasoning,Yes.,3,"""their performances fall significantly when data and computational resources are limited.""",2022,
A Simple Yet Effective Hybrid Pre-trained Language Model for Unsupervised Sentence Acceptability Prediction,Yes.,3,"""first, low-frequency words would have a significant negative impact on the sentence likelihood derived from the language model; second, when it comes to multiple domains, the language model needs to be trained on domain-specific text for domain adaptation.""",2022,
Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,3,"""texts rarely report on common facts, instead focusing on the unusual aspects of a situation. If LMs are only trained on text corpora and naively memorise local co-occurrence statistics, they thus naturally would learn a biased view of the physical world.""",2022,
Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique,Yes.,4,"""BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data.""",2022,November 
MiQA: A Benchmark for Inference on Metaphorical Questions,Yes.,3,"""We examine the performance of state-of-the-art pre-trained models on binary-choice tasks and find a large discrepancy between the performance of small and very large models, going from chance to near-human level."" and ""We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.""",2022,
CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing,Yes.,2,"""LLMs are unsuitable for runtime systems which require low latency.""",2022,
Toward Building a Language Model for Understanding Temporal Commonsense,Yes.,3,"""pre-trained language models such as BERT, which have recently achieved great success in a wide range of natural language processing tasks, are still considered to have poor performance in temporal reasoning.""",2022,
C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code,Yes.,3,"""While large language models (LLMs) have been proposed to translate natural language pseudocode to PL code, they are costly in terms of data and compute.""",2022,
