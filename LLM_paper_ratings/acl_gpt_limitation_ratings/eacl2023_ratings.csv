Title,Talks about LLMs,Rate,Evidence
WinoDict: Probing language models for in-context word acquisition,Yes.,5.,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time."""
Nationality Bias in Text Generation,Yes.,5.,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms"" and ""GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same."""
Do we need Label Regularization to Fine-tune Pre-trained Language Models?,Yes.,1.,"""Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs."""
A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,3.,"""LLMs must overcome frequency biases in order to master such constructions."""
"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility",Yes.,5.,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer"
Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,Yes.,1.,"""PGen randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style."""
Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers,Yes.,3.,"""Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant."""
On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5.,"""The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs,"" and ""the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios,"" and ""the large language model of code is vulnerable to carefully crafted adversarial examples."""
MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers,Yes.,5.,"""the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as overparameterisation."""
A Systematic Search for Compound Semantics in Pretrained BERT Architectures,Yes.,3.,"""To date, transformer-based models such as BERT have been less successful in predicting compositionality of noun compounds than static word embeddings. This is likely related to a suboptimal use of the encoded information, reflecting an incomplete grasp of how the models represent the meanings of complex linguistic structures."""
SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4.,"""A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-specified by the designer of the test."" and ""We also test SODAPOP on debiased models and show the limitations of"
Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding,Yes.,5.,"""However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data,"" and ""the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task."""
LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation,Yes.,3.,"""these large-scale models can suffer from inference speed and computation overhead."""
Extracting Victim Counts from Text,Yes.,2.,"""Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate out-of-distribution and few-shot performance."""
Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5.,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" and ""Our human quality evaluation shows that both models and ground-truth annotations exhibit low performance in terms of equitable tutoring, which measures learning opportunities for"
Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5.,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts."" and ""given a few target-domain examples and a set of models with similar training performance, can we understand how these models will perform on OOD test data?"""
Bootstrapping Multilingual Semantic Parsers using Large Language Models,Yes.,2.,"""Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models."""
Towards preserving word order importance through Forced Invalidation,Yes.,5.,"""However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed."""
Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models,Yes.,4.,"""However, safely deploying them in real world applications is challenging because they generate toxic content."""
Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow,Yes.,3.,"""Recent research has shown that language models exploit ‘artifacts’ in benchmarks to solve tasks, rather than truly learning them, leading to inflated model performance."""
Unsupervised Improvement of Factual Knowledge in Language Models,Yes.,1.,"""Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge."""
Learning to Ignore Adversarial Attacks,No.,1.,The abstract does not mention language models (LLMs) or their limitations.
Should You Mask 15% in Masked Language Modeling?,Yes.,1.,"""Masked language models (MLMs) conventionally mask 15% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies."""
When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization,Yes.,5.,"""Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood."" and ""We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries."""
BERT Shows Garden Path Effects,Yes.,3.,"""We find that the models have relatively low performance in certain instances of question answering based on garden path contexts, and the model incorrectly assigns semantic roles."""
DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,Yes.,3.,"""While LoRA blocks are parameter-efficient, they suffer from two major problems"
Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4.,"""Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models."""
Social Commonsense for Explanation and Cultural Bias Discovery,Yes.,2.,"""We identify influential social commonsense knowledge to explain model behavior in the following ways. First, we augment large-scale language models with social knowledge and show improvements for the tasks, indicating the implicit assumptions a model requires to be successful on each dataset."""
"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",Yes.,2.,"""manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models."""
DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence,No.,1.,The abstract discusses BERT-based evaluation metrics and their limitations but does not address language models (LLMs or LLMs) specifically.
CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,3.,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation."""
