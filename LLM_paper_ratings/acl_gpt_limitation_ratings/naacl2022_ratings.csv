Title,Talks about LLMs,Rate,Evidence
Title,Talks about LLMs,Rate,Evidence
Title,Talks about LLMs,Rate,Evidence
Learning Natural Language Generation with Truncated Reinforcement Learning,Yes.,1.,"""TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model."""
Language Model Augmented Monotonic Attention for Simultaneous Translation,Yes.,1.,"""we propose a framework to aid monotonic attention with an external language model to improve its decisions."""
Enhancing Self-Attention with Knowledge-Assisted Attention Maps,Yes.,3.,"""the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge."""
Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation,Yes.,1.,"""Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources."""
Reframing Human-AI Collaboration for Generating Free-Text Explanations,Yes.,3.,"""while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label."""
Provably Confidential Language Modelling,Yes.,5.,"""Large language models are shown to memorize privacy information such as social security numbers in training data."""
"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",Yes.,5.,"""We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities."""
Towards a Progression-Aware Autonomous Dialogue Agent,Yes.,3.,"""While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of task success inherent therein."""
Cross-Domain Detection of GPT-2-Generated Technical Text,Yes.,1.,"""we examine the problem of detecting GPT-2-generated technical research text."""
Context-Aware Abbreviation Expansion Using Large Language Models,,,
Sort by Structure: Language Model Ranking as Dependency Probing,Yes.,1.,"""Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored."""
SKILL: Structured Knowledge Infusion for Large Language Models,Yes.,2.,"""However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text."""
MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation,,,
Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models,Yes.,4.,"""These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective."""
ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model,Yes.,1.,"""Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms."""
KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation,Yes.,2.,"""While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices."""
Data Augmentation with Dual Training for Offensive Span Detection,Yes.,1.,"""the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD."""
Robust Conversational Agents against Imperceptible Toxicity Triggers,No.,1.,The abstract does not mention LLMs or their limitations.
PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding,Yes.,1.,"""Large language models (LM) based on Transformers allow to generate plausible long texts."""
Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,Yes.,2.,"""recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text."""
Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Yes.,5.,"""current evaluation methods show some significant shortcomings,"" ""they fail to effectively map out the aspects of language understanding that remain challenging to existing models,"" and ""our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models."""
Exposing the Limits of Video-Text Models through Contrast Sets,Yes.,5.,"""We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods."""
Knowledge Inheritance for Pre-trained Language Models,Yes.,3.,"""However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable."""
"Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",Yes.,1.,"""we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks."""
Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge,Yes.,3.,"""We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning."""
Symbolic Knowledge Distillation: from General Language Models to Commonsense Models,Yes.,1.,"""general language models author these commonsense knowledge graphs to train commonsense models."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,3.,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks."""
A Study of the Attention Abnormality in Trojaned BERTs,Yes.,1.,"""In this paper, we investigate the underlying mechanism of Trojaned BERT models."""
Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora,Yes.,3.,"""a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on."""
KALA: Knowledge-Augmented Language Model Adaptation,Yes.,5.,"""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general"
Building a Personalized Dialogue System with Prompt-Tuning,Yes.,1.,"""Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models."""
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,,,
You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas,Yes.,4.,"""privacy concerns have arisen recently"
Methods for Estimating and Improving Robustness of Language Models,Yes.,5.,"""large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem"" and ""weak ability to generalise outside of the training domain."""
Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Yes.,1.,"""The second approach utilizes a domain-specific pretrained language model, MentalBERT."""
Building a Personalized Dialogue System with Prompt-Tuning,Yes.,1.,"""Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models."""
Multimodal large language models for inclusive collaboration learning tasks,Yes.,1.,"""We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop."""
Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition,Yes.,5.,"""We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions."""
An End-to-End Dialogue Summarization System for Sales Calls,Yes.,1.,"""We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting."""
Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning,Yes.,2.,"""automating this task is challenging due to a large number of LLT codes (as of writing over 80\,000), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain."""
Knowledge extraction from aeronautical messages (NOTAMs) with self-supervised language models for aircraft pilots,Yes.,1.,"""In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots."""
