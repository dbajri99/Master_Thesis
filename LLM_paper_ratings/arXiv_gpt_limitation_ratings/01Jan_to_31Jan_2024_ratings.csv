Title,Talks about LLMs,Rate,Evidence
Knowledge-Centric Templatic Views of Documents,Yes.,1.,"""we consider each of these documents to be templatic views of the same underlying knowledge, and we aim to unify the generation and evaluation of these templatic views of documents. We begin by introducing an LLM-powered method to extract the most important information from an input document and represent this information in a structured format."""
Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions,Yes.,2.,"""The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale, aligned image and text datasets in specific domains."""
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads,Yes.,1.,"""The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators."""
A Study on Large Language Models' Limitations in Multiple-Choice Question Answering,Yes.,5.,"""Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations."" and ""We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent."""
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview,Yes.,1.,"""Most of the runs leveraged Large Language Models (LLMs) in their pipelines, with a few focusing on a generate-then-retrieve approach."""
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization,Yes.,1.,"""Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks."""
Fast and Optimal Weight Update for Pruned Large Language Models,Yes.,1.,"""Pruning large language models (LLMs) is a challenging task due to their enormous size."""
CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling,Yes.,1.,"""Traditional rule-based labeling methods fall short of capturing the nuances of diverse free-text patterns. Moreover, models using expert-annotated data are limited by data scarcity and pre-defined classes, impacting their performance, flexibility and scalability."""
A Survey of Resource-efficient LLM and Multimodal Foundation Models,Yes.,3.,"""the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources."""
E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models,Yes.,3.,"""almost all models perform poorly in complex subjects such as mathematics"" and ""most Chinese-dominant LLMs did not achieve higher scores at the primary school level compared to the middle school level"" and ""the mastery of higher-order knowledge by the model does not necessarily imply the mastery of lower-order knowledge as well."""
Monte Carlo Tree Search for Recipe Generation using GPT-2,Yes.,3.,"""Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes."""
The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,Yes.,5.,"""hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications"" and ""To tackle the LLM hallucination, three key questions should be well studied"
The Impact of Reasoning Step Length on Large Language Models,Yes.,3.,"""However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown."""
"A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",Yes.,1.,"""The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models."""
XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model,Yes.,1.,"""With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities."""
Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models,Yes.,3.,"""Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security."""
"Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",Yes.,2.,"""We begin (Part I) with a sustained defense of bibliotechnism against this challenge showing how even entirely novel text may be meaningful only in a derivative sense, and arguing that, in particular, much novel text generated by LLMs is only derivatively meaningful."""
Assessing and Understanding Creativity in Large Language Models,Yes.,3.,"""We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration."""
LLM on FHIR -- Demystifying Health Records,Yes.,4.,"""However, challenges included variability in LLM responses and the need for precise filtering of health data."" and ""While promising, the implementation and pilot also highlight risks such as inconsistent responses and the importance of replicable output."""
Large Language Model Evaluation via Matrix Entropy,Yes.,1.,"""Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains."""
CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM,Yes.,3.,"""Large language models (LLMs) improve content control by allowing unrestricted user instructions, but the token-by-token generation process frequently makes format errors."""
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning,Yes.,5.,"""Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world."" and ""the performance of GPT-4 even drops significantly from 80.00 to 58"
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,Yes.,1.,"""one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world."""
TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data,Yes.,2.,"""utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk."""
Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models,Yes.,2.,"""However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting."""
A Preliminary Study on Using Large Language Models in Software Pentesting,Yes.,1.,"""Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs)."""
"The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",Yes.,5.,"""However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation."""
LLMs for Relational Reasoning: How Far are We?,Yes.,5.,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."" and ""Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and"
Large Language Model Adaptation for Financial Sentiment Analysis,Yes.,3.,"""Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities."""
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning,Yes.,4.,"""Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content."" and ""prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100%"
FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference,Yes.,5.,"""The large number of parameters in Pretrained Language Models enhance their performance, but also make them resource-intensive, making it challenging to deploy them on commodity hardware like a single GPU."" and ""Due to the memory and power limitations of these devices, model compression techniques are often used to decrease both the model's size and its inference latency. This usually results in a trade-off between model accuracy"
IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification,Yes.,2.,"""However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem."""
VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model,Yes.,1.,"""By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM)."""
MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models,Yes.,2.,"""there is a large gap between the performance of LLMs on English and other languages"" and ""finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages."""
Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting,,,
LLMs for Test Input Generation for Semantic Caches,Yes.,4.,"""However, these models are computationally expensive. At scale, the cost of serving thousands of users increases massively affecting also user experience."" and ""Due to the nature of these semantic cache techniques that rely on query embeddings, there is a high chance of errors impacting user confidence in the system."""
Knowledge Distillation for Closed-Source Language Models,Yes.,3.,"""due to the incapability to directly access the weights, hidden states, and output distributions of these closed-source models, the distillation can only be performed by fine-tuning smaller models with data samples generated by closed-source language models, which constrains the effectiveness of knowledge distillation."""
DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure,Yes.,1.,"""This paper introduces DataFrame question answering (QA), a novel task that utilizes large language models (LLMs) to generate Pandas queries for information retrieval and data analysis on dataframes, emphasizing safe and non-revealing data handling."""
GRATH: Gradual Self-Truthifying for Large Language Models,Yes.,5.,"""existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA."""
Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment,Yes.,1.,"""we present the first comprehensive cross-supervision alignment experiment in the role-play domain, revealing that the intrinsic capabilities of LLMs confine the knowledge within role-play."""
Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling,Yes.,5.,"""This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web."""
Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,Yes.,5.,"""Despite being widely applied, in-context learning is vulnerable to malicious attacks."" and ""Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model."""
AugSumm: towards generalizable speech summarization using synthetic labels from large language model,Yes.,1.,"""We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation."""
Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models,Yes.,1.,"""Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks."""
WARM: On the Benefits of Weight Averaged Reward Models,Yes.,3.,"""Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives."" and ""We identify two primary challenges when designing RMs to mitigate reward hacking"
"A Fast, Performant, Secure Distributed Training Framework For Large Language Model",Yes.,3.,"""maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved."""
Transfer Learning for Text Diffusion Models,Yes.,1.,"""We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call 'AR2Diff'."""
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks,Yes.,3.,"""Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks."""
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems,Yes.,2.,"""Large Language Models (LLMs) has shown exceptional capabilities in many natural language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system."""
Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning,,,
LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs,Yes.,3.,"""we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain."""
Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately,Yes.,5.,"""Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions."""
Cheetah: Natural Language Generation for 517 African Languages,Yes.,1.,"""In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages."""
TrustLLM: Trustworthiness in Large Language Models,Yes.,5.,"""Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. ... discussion of open challenges and future directions. ... our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. ... some LLMs may be overly calibrated towards exhibiting trustworthiness"
A Comprehensive Study of Knowledge Editing for Large Language Models,Yes.,3.,"""a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization"" and ""necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge."""
Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers,No.,1.,"The paper discusses adversarial examples and text classifiers, but does not mention language models (LLMs or LMs)."
Contextualization Distillation from Large Language Model for Knowledge Graph Completion,Yes.,2.,"""the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models."""
Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering,Yes.,5.,"""Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community."""
Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning,Yes.,3.,"""Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks."""
A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models,No.,1.,"The abstract focuses on text-to-image generation techniques and copyright protection, without mentioning language models."
Few-Shot Detection of Machine-Generated Text using Style Representations,Yes.,3.,"""model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of newer language models producing still more fluent text than the models used to train the detectors."""
Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education,Yes.,2.,"""Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct."""
Reinforcement Learning for Optimizing RAG for Domain Chatbots,Yes.,1.,"""With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases."""
TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models,Yes.,3.,"""The experimental results show that although some models perform well in some tasks, there is still much room for improvement overall."""
Parameter-Efficient Detoxification with Contrastive Decoding,Yes.,1.,"""We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators."""
Zero Resource Cross-Lingual Part Of Speech Tagging,Yes.,1.,"""Existing systems use two main techniques for POS tagging i.e. pretrained multilingual large language models(LLM) or project the source language labels into the zero resource target language and train a sequence labeling model on it."""
EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction,Yes.,3.,"""it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools."""
Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control,Yes.,3.,"""it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size."""
Language Detection for Transliterated Content,Yes.,1.,"""Emphasizing the pivotal role of comprehensive datasets for training Large Language Models LLMs like BERT."""
CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities,Yes.,5.,"""Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems."" and ""Using this corpus, we find that models often arrive at the correct final answer"
Comparing Template-based and Template-free Language Model Probing,Yes.,1.,"""The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked."""
Multi-Candidate Speculative Decoding,Yes.,3.,"""the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup."""
Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection,Yes.,1.,"""we propose AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets."""
Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models,Yes.,3.,"""existing research typically focuses on limited tasks and often omits key multi-view and temporal information which is crucial for robust autonomous driving."""
MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning,Yes.,3.,"""However, the current LLMs' perceiving tool-use ability is limited to a single text query, which may result in ambiguity in understanding the users' real intentions."""
Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series,Yes.,3.,"""Large pre-trained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pre-training data."" and ""However, these models are typically very slow and large (~billion parameters) and do not consider cross-channel correlations."""
A Computational Framework for Behavioral Assessment of LLM Therapists,Yes.,5.,"""Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences."" and ""Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care"
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese,Yes.,3.,"""While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes."""
T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives,Yes.,1.,"""Large Language Models increasingly rely on distributed techniques for their training and inference."""
Towards Language-Driven Video Inpainting via Multimodal Large Language Models,Yes.,1.,"""This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive."""
"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",Yes.,2.,"""Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain."" and ""Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG."""
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data,,,
Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions,Yes.,1.,"""This work presents an unsupervised method for automatically constructing and expanding topic taxonomies using instruction-based fine-tuned LLMs (Large Language Models)."""
Training microrobots to swim by a large language model,Yes.,1.,"""We discuss the nuanced aspects of prompt design, particularly emphasizing the reduction of monetary expenses of using GPT-4."""
Navigating the OverKill in Large Language Models,Yes.,5.,"""Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries."""
E^2-LLM: Efficient and Extreme Length Extension of Large Language Models,Yes.,5.,"""Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources."""
LoMA: Lossless Compressed Memory Attention,Yes.,5.,"""Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts."""
"Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models",Yes.,1.,"""In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes."""
Can AI Assistants Know What They Don't Know?,Yes.,5.,"""Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications."""
CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs,Yes.,5.,"""Large Multimodal Models (LMMs) encounter two issues in such scenarios"
"When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",Yes.,2.,"""Some challenges still remain such as ensuring physical interpretation, nefarious use cases, and trustworthiness."""
CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning,Yes.,2.,"""existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability."""
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters,Yes.,5.,"""achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context."""
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption,Yes.,3.,"""However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries."" and ""This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance."""
Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis,Yes.,3.,"""Most if not all of the queries fall into these categories and it is insightful to understanding where the faults still lie with LLM program synthesis and where they can be improved."""
TeleChat Technical Report,Yes.,1.,"""It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences."""
Physio: An LLM-Based Physiotherapy Advisor,Yes.,5.,"""However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being."""
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,Yes.,3.,"""we also observe irregularities in the machine--$\infty$-gram agreement level with respect to the suffix length, which indicates deficiencies in neural LLM pretraining and the positional embeddings of Transformers."""
Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning,Yes.,1.,"""However, the reasoning abilities of MLLMs have not been systematically investigated."""
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?,Yes.,3.,"""We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not during the final step which relies on the problem's arithmetic expressions (solution execution)."""
Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping,Yes.,1.,"""Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks."""
"Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",Yes.,5.,"""In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs."""
Cross-target Stance Detection by Exploiting Target Analytical Perspectives,Yes.,1.,"""formulating instructions based on large language model (LLM)."""
Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk,Yes.,3.,"""specializing them towards fulfilling a specific function can be challenging"" and ""requires a number of data samples that a) might not be available or b) costly to generate."""
ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios,Yes.,5.,"""Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning."""
Hierarchical Continual Reinforcement Learning via Large Language Model,Yes.,1.,"""Hierarchical Continual reinforcement learning via large language model (Hi-Core), designed to facilitate the transfer of high-level knowledge."""
ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study,Yes.,2.,"""The transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models."""
F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods,Yes.,3.,"""Previous subjective evaluation methods mainly rely on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences."""
"Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",,,
Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models,,,
ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation,Yes.,1.,"""Only 33% of LLM-generated raw assertions had errors."""
DeepEdit: Knowledge Editing as Decoding with Constraints,Yes.,1.,"""We propose a new perspective of knowledge editing (KE) for large language models (LLMs) that treats it as a constrained decoding problem."""
Reinforcement learning for question answering in programming domain using public community scoring as a human feedback,Yes.,3.,"""demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain."""
From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning,Yes.,2.,"""Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data."""
Can Large Language Models Replace Economic Choice Prediction Labs?,Yes.,1.,"""The AI community has recently contributed to that effort in two ways"
Dynamic Q&A of Clinical Documents with Large Language Models,Yes.,4.,"""Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain."""
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,Yes.,5.,"""language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior."""
TQCompressor: improving tensor decomposition methods in neural networks via permutations,Yes.,1.,"""We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks."""
Security Code Review by LLMs: A Deep Dive into Responses,Yes.,5.,"""Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection."""
Evolving Code with A Large Language Model,Yes.,1.,"""We cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming."""
Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models,Yes.,1.,"""Large Language Models (LLMs) have upended decades of pedagogy in computing education."""
Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance,,,
AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media,Yes.,2.,"""Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost."""
ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning,Yes.,1.,"""The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design."""
EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge,Yes.,4.,"""However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English."""
Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis,Yes.,4.,"""However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount."""
How well can large language models explain business processes?,Yes.,5.,"""Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason."""
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization,Yes.,1.,"""LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows."""
QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners,Yes.,2.,"""highlighting the potential limitations of general LLMs as intelligent teaching assistants in computer programming courses."""
Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models,Yes.,5.,"""a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level."""
Large Language Models for Mathematical Reasoning: Progresses and Challenges,Yes.,4.,"""an overview of factors and concerns affecting LLMs in solving math"" and ""an elucidation of the persisting challenges within this domain."""
Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering,Yes.,5.,"""While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers."""
Scientific Large Language Models: A Survey on Biological & Chemical Domains,Yes.,2.,"""Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs."""
Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System,Yes.,3.,"""prevailing models, both commercial and open-source, confront notable challenges"
Seven Failure Points When Engineering a Retrieval Augmented Generation System,Yes.,4.,"""RAG systems aim to"
Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task,Yes.,3.,"""LLMs trained on multilingual datasets normally struggle to respond to prompts in Portuguese satisfactorily, presenting, for example, code switching in their responses."""
Fine-tuning and Utilization Methods of Domain-specific LLMs,Yes.,2.,"""The study explores the potential of LLMs in the financial domain, identifies limitations, and proposes directions for improvement."""
Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation,Yes.,1.,"""This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions."""
LLMs for Robotic Object Disambiguation,Yes.,5.,"""Despite multiple query attempts with zero-shot prompt engineering (details can be found in the Appendix), the LLM struggled to inquire about features not explicitly provided in the scene description."""
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks,Yes.,5.,"""Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents."""
Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models,Yes.,1.,"""Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning."""
DocFinQA: A Long-Context Financial Reasoning Dataset,Yes.,5.,"""DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents."""
Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation,Yes.,5.,"""existing approaches struggle with hallucinations and overconfident predictions."""
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark,Yes.,1.,"""We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement."""
Prompting open-source and commercial language models for grammatical error correction of English learner text,Yes.,3.,"""Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts"" and ""We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting."""
FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis,Yes.,1.,"""propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis."""
Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education,Yes.,2.,"""While AI's promise in education, assessment, and tutoring is clear, challenges remain."""
Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation,Yes.,1.,"""Despite significant advancements in text-to-image models for generating high-quality images, these methods still struggle to ensure the controllability of text prompts over images in the context of complex text prompts, especially when it comes to retaining object attributes and relationships."""
Are self-explanations from Large Language Models faithful?,Yes.,5.,"""convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk"" and ""showing self-explanations should not be trusted in general."""
MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,Yes.,4.,"""However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence."""
Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models,Yes.,1.,"""This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design."""
Tuning Language Models by Proxy,Yes.,2.,"""tuning these models has become increasingly resource-intensive, or impossible when model weights are private."""
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning,Yes.,4.,"""However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge."" and ""The results demonstrate that CMMU poses a significant challenge to the recent MLLMs."""
JumpCoder: Go Beyond Autoregressive Coder via Online Modification,Yes.,5.,"""While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance."""
"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",Yes.,2.,"""the potential misuse of this intelligence for malicious purposes presents significant risks."""
"Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",Yes.,3.,"""Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges."""
General Flow as Foundation Affordance for Scalable Robot Learning,Yes.,1.,"""Inspired by the success of large-scale auto-regressive prediction in Large Language Models (LLMs), we hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning."""
L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks,Yes.,1.,"""This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks."""
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues,Yes.,1.,"""To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents."""
Pre-trained Large Language Models for Financial Sentiment Analysis,Yes.,1.,"""In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem."""
LightHouse: A Survey of AGI Hallucination,Yes.,4.,"""numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research"" and ""Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models)."""
A Vision Check-up for Language Models,Yes.,1.,"""Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world."""
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences,Yes.,5.,"""we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors."""
Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data,Yes.,2.,"""Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect."""
Improving Domain Adaptation through Extended-Text Reading Comprehension,Yes.,3.,"""regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context."""
Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access,Yes.,3.,"""Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs)."""
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models,Yes.,5.,"""Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance."" and ""This research critically examines these biases and quantifies the effects on a representative list selection task."""
"Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",Yes.,3.,"""While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory."""
A match made in consistency heaven: when large language models meet evolutionary algorithms,Yes.,1.,"""Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way."""
Towards Goal-oriented Large Language Model Prompting: A Survey,Yes.,2.,"""highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans."""
Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study,Yes.,2.,"""However, the problems of LLMs and RL model collaboration still need to be solved."""
Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis,Yes.,2.,"""While highlighting the limitations and the need for real-world testing, this study opens new avenues for AI applications in physical system control and sets a foundation for future advancements in AI-driven automation technologies."""
AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents,Yes.,3.,"""However, their capability in handling complex, multi-character social interactions has yet to be fully explored, primarily due to the absence of robust, quantitative evaluation methods."""
Adaptive Text Watermark for Large Language Models,Yes.,2.,"""it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model."""
EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty,Yes.,3.,"""the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance."""
Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation,Yes.,3.,"""even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4."" and ""we first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data,"
MouSi: Poly-Visual-Expert Vision-Language Models,Yes.,4.,"""Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information."""
Identifying and Analyzing Task-Encoding Tokens in Large Language Models,Yes.,3.,"""unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour."""
Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM,Yes.,3.,"""LLM-generated testsuites still suffer from low coverage."""
ReGAL: Refactoring Programs to Discover Generalizable Abstractions,Yes.,3.,"""While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality."""
Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet,Yes.,5.,"""We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis."""
Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do,Yes.,5.,"""We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception,"" and ""Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others?"""
Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs,Yes.,5.,"""Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings,"" and ""MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations."""
"Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",Yes.,3.,"""the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance"" and ""The paper identifies potential gaps and shortcomings in the legislative framework."""
ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain,Yes.,1.,"""Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis."""
Detecting mental disorder on social media: a ChatGPT-augmented explainable approach,Yes.,1.,"""This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT."""
"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",Yes.,1.,"""This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image inpainting techniques."""
APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference,Yes.,2.,"""Fine-tuning and inference with large Language Models (LM) are generally known to be expensive."""
PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models,Yes.,1.,"""This paper proposes an image pragmatic communication framework based on a Pragmatic Agent for Communication Efficiency (PACE) using Large Language Models (LLM)."""
AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters,Yes.,3.,"""decisions around what data is retained or removed during this initial stage is under-scrutinized"" and ""we conduct the first study investigating how ten 'quality' and English language identification (langID) filters affect webpages that vary along these social dimensions."""
Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior,Yes.,3.,"""Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization."""
"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",Yes.,3.,"""existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships."""
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time,Yes.,1.,"""The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention."""
-CAUSAL: Exploring Defeasibility in Causal Reasoning,Yes.,2.,"""We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by {\delta}-CAUSAL."""
Arrows of Time for Large Language Models,Yes.,3.,"""We empirically find a time asymmetry exhibited by such models in their ability to model natural language"
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,5.,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."""
Leveraging Print Debugging to Improve Code Generation in Large Language Models,Yes.,3.,"""Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal."""
Under the Surface: Tracking the Artifactuality of LLM-Generated Data,Yes.,5.,"""This paper reveals significant hidden disparities, especially in complex tasks where LLMs often miss the nuanced understanding of intrinsic human-generated content,"" and ""It highlights the LLMs' shortcomings in replicating human traits and behaviors, underscoring the importance of addressing biases and artifacts produced in LLM-generated content for future research and development."""
Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes,Yes.,2.,"""However, their performance in actual clinical applications has been underexplored."" and ""This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings."""
"Improving Classification Performance With Human Feedback: Label a few, we label the rest",Yes.,1.,"""By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy."""
Entity Recognition from Colloquial Text,Yes.,3.,"""Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges."""
Generalist embedding models are better at short-context clinical semantic search than specialized embedding models,Yes.,4.,"""Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs."" and ""The highlighted problem of specialized models may be due to the fact that they have not been trained on sufficient data, and in particular on datasets that are not diverse enough to have a reliable global language understanding,"
"Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",Yes.,1.,"""Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications."""
Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs,Yes.,1.,"""Recent prompting techniques, such as chain of thought, have consistently improved LLMs' performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage."""
LongAlign: A Recipe for Long Context Alignment of Large Language Models,Yes.,5.,"""Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length."""
CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs,Yes.,3.,"""LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement."""
A Study on Training and Developing Large Language Models for Behavior Tree Generation,Yes.,1.,"""we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs."""
Benchmarking Large Language Models on Controllable Generation under Diversified Instructions,Yes.,5.,"""revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs."""
An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search,Yes.,1.,"""AEL combines the power of a large language model and the paradigm of evolutionary computation to design, combine, and modify algorithms automatically."""
Gender Bias in Machine Translation and The Era of Large Language Models,Yes.,4.,"""The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies."""
Pheme: Efficient and Conversational Speech Generation,,,
Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications,Yes.,4.,"""The critical challenge of prompt injection attacks in Large Language Models (LLMs) integrated applications, a growing concern in the Artificial Intelligence (AI) field. Such attacks, which manipulate LLMs through natural language inputs, pose a significant threat to the security of these applications. Traditional defense strategies, including output and input filtering, as well as delimiter use, have proven inadequate."""
Multilingual Instruction Tuning With Just a Pinch of Multilinguality,Yes.,1.,"""As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial."""
Leveraging Large Language Models for NLG Evaluation: A Survey,Yes.,4.,"""Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques."""
Prompting Large Vision-Language Models for Compositional Reasoning,Yes.,5.,"""However, these embedding-based models still face challenges in effectively matching images and texts with similar visio-linguistic compositionality, as evidenced by their performance on the recent Winoground dataset."" and ""this limitation stems from two factors"
Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example,Yes.,3.,"""The primary objective is to assess the capabilities and constraints of ChatGPT, a prominent representative of large language models, within the context of automated essay scoring."""
DiffusionGPT: LLM-Driven Text-to-Image Generation System,Yes.,1.,"""However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results."""
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges,Yes.,3.,"""These results underscore INACIA's potential in complex legal task handling while also acknowledging the current limitations."""
Cross-lingual Editing in Multilingual Language Models,Yes.,3.,"""The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families."""
MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models,Yes.,5.,"""We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models' fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance."""
Scaling Sparse Fine-Tuning to Large Language Models,Yes.,3.,"""Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters."" and ""their memory requirements increase proportionally to the size of the LLMs."""
Wordflow: Social Prompt Engineering for Large Language Models,Yes.,1.,"""Large language models (LLMs) require well-crafted prompts for effective use."""
OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models,Yes.,5.,"""The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in this field."""
Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability,Yes.,5.,"""LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps."""
De-identification is not always enough,Yes.,3.,"""We observed that when synthetically generated notes closely match the performance of real data, they also exhibit similar privacy concerns to the real data."""
Transformers are Multi-State RNNs,Yes.,3.,"""They also lay out the option of mitigating one of their most painful computational bottlenecks - the size of their cache memory."""
DevEval: Evaluating Code Generation in Practical Software Projects,Yes.,3.,"""Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts."""
Question Translation Training for Better Multilingual Reasoning,Yes.,3.,"""Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions."""
ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT,Yes.,1.,"""To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM)."""
From Prompt Engineering to Prompt Science With Human in the Loop,Yes.,5.,"""we need to be concerned about how it may affect that research, its findings, or any future works based on that research,"" and ""they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor."""
When Large Language Models Meet Vector Databases: A Survey,Yes.,5.,"""With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues."""
"When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",Yes.,2.,"""Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions."""
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models,Yes.,5.,"""This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped."""
Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches,Yes.,5.,"""although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation."""
Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization,Yes.,1.,"""Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field."""
InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification,Yes.,5.,"""our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."""
ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers,Yes.,5.,"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models."""
Agent Alignment in Evolving Social Norms,Yes.,2.,"""The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate."""
Security and Privacy Challenges of Large Language Models: A Survey,Yes.,5.,"""While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks."""
Hallucination Benchmark in Medical Visual Question Answering,Yes.,5.,"""these models are not extensively tested on the hallucination phenomenon in clinical settings"" and ""The study provides an in-depth analysis of current models' limitations."""
Enhancing Recommendation Diversity by Re-ranking with Large Language Models,Yes.,2.,"""We find that LLM-based re-ranking outperforms random re-ranking across all the metrics that we use but does not perform as well as the traditional re-ranking methods."""
Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs,Yes.,2.,"""Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability."""
Knowledge Verification to Nip Hallucination in the Bud,Yes.,5.,"""they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as hallucination."""
"Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages",Yes.,2.,"""One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages."" and ""MTInstruct is limited by weak cross-lingual signals inherent in the second challenge."""
LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning,Yes.,3.,"""recent attempts to use LLMs for vulnerability detection are still preliminary, as they lack an in-depth understanding of a subject LLM's vulnerability reasoning capability -- whether it originates from the model itself or from external assistance, such as invoking tool support and retrieving vulnerability knowledge."""
DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models,Yes.,3.,"""Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical."""
Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness,Yes.,5.,"""However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective."" and ""Our results shed light on the limitations of the LLM chatbots when compared to specialized models."""
Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values,Yes.,4.,"""troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies."""
Towards Optimizing the Costs of LLM Usage,Yes.,2.,"""enterprises are already incurring huge costs of operating or using LLMs for their respective use cases."""
Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding,,,
Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets,Yes.,3.,"""We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance."""
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications,Yes.,1.,"""Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains."""
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine,Yes.,5.,"""we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our findings emphasize the necessity for further in-depth evaluations of its rationales before integrating such models into clinical workflows."""
ChatGraph: Chat with Your Graphs,Yes.,1.,"""To address the limitations, we propose a large language model (LLM)-based framework called ChatGraph."""
Do We Need Language-Specific Fact-Checking Models? The Case of Chinese,Yes.,3.,"""We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems."""
On Prompt-Driven Safeguarding for Large Language Models,Yes.,5.,"""We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by safety prompts in similar directions where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless."""
Large language model empowered participatory urban planning,Yes.,1.,"""This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process."""
MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds,,,
CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer,Yes.,1.,"""CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style."""
APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding,Yes.,1.,"""the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving."""
Large Language Models Are Neurosymbolic Reasoners,Yes.,1.,"""This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners."""
Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation,Yes.,5.,"""Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations."""
Detection of Machine-Generated Text: Literature Survey,Yes.,4.,"""Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes."" and ""To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented."""
Extreme Compression of Large Language Models via Additive Quantization,Yes.,1.,"""The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices."""
The Neglected Tails of Vision-Language Models,,,
TOFU: A Task of Fictitious Unlearning for LLMs,Yes.,5.,"""Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns."" and ""Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."""
An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks,Yes.,3.,"""Though ChatGPT is the most popular large language model, we do not recommend using it to identify assumptions in DL framework development because of its low performance on the task."""
InFoBench: Evaluating Instruction Following Ability in Large Language Models,Yes.,3.,"""The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following."""
AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment,Yes.,1.,"""Despite recent advances, field experiments applying LLM-based tools in realistic settings are limited."""
Towards Conversational Diagnostic AI,Yes.,2.,"""Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice."""
Extending LLMs' Context Window with 100 Samples,Yes.,5.,"""Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs."""
MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline,Yes.,5.,"""there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints."""
"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Yes.,5.,"""the safety and security issues of LLM systems have become the major obstacle to their widespread application"" and ""potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies."""
SliceGPT: Compress Large Language Models by Deleting Rows and Columns,Yes.,3.,"""Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources."""
"Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",Yes.,1.,"""existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization."""
Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection,Yes.,1.,"""we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering."""
Video Anomaly Detection and Explanation via Large Language Models,Yes.,3.,"""We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling."""
Finetuning Large Language Models for Vulnerability Detection,Yes.,1.,"""This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks."""
Efficient Tool Use with Chain-of-Abstraction Reasoning,Yes.,2.,"""there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning."""
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models,Yes.,3.,"""the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive"" and ""its high computational cost remains a barrier to its widespread applicability in the context of LLMs."""
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models,Yes.,3.,"""To address challenges that still cannot be handled with the encoded knowledge of LLMs,"" and ""poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments."""
Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding,Yes.,1.,"""The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses."""
xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning,Yes.,3.,"""CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization."""
Generative Large Language Models are autonomous practitioners of evidence-based medicine,Yes.,3.,"""Limitations were observed in terms of model ability to handle complex guidelines and diagnostic nuances."""
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis,Yes.,3.,"""To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes."""
YODA: Teacher-Student Progressive Learning for Language Models,Yes.,3.,"""Although large language models (LLMs) have demonstrated adeptness in a range of tasks, they still lag behind human learning efficiency."""
ChemDFM: Dialogue Foundation Model for Chemistry,Yes.,3.,"""the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry."""
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,Yes.,5.,"""Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs."""
Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study,Yes.,1.,"""The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models."""
Large Language Models Can Learn Temporal Reasoning,,,
Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?,Yes.,5.,"""The paper discusses what is needed to address the limitations of current LLM-centered AI systems."""
"""You tell me"": A Dataset of GPT-4-Based Behaviour Change Support Conversations",Yes.,1.,"""Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts."""
Automated Fact-Checking of Climate Change Claims with Large Language Models,Yes.,1.,"""While our research is subject to certain limitations and necessitates careful interpretation, our approach holds significant potential."""
LLsM: Generative Linguistic Steganography with Large Language Model,,,
Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing,Yes.,3.,"""However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture."""
Small Language Model Can Self-correct,Yes.,5.,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone"" and ""large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans."""
Conditional and Modal Reasoning in Large Language Models,Yes.,5.,"""Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."""
PRE: A Peer Review Based Large Language Model Evaluator,Yes.,3.,"""Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term."""
Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?,Yes.,5.,"""The models showed significantly reduced accuracy on tasks with suspected hierarchical bias."""
LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging,Yes.,2.,"""Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain specific data."""
Misconfidence-based Demonstration Selection for LLM In-Context Learning,Yes.,3.,"""However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice."""
Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring,Yes.,4.,"""The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM)."" and ""highlight the potential requirements and limitations of utilizing chatbots in conversational explainability."""
On Detecting Cherry-picking in News Coverage Using Large Language Models,Yes.,1.,"""Our approach relies on language models that consider contextual information from other news sources to classify statements based on their importance to the event covered in the target news story."""
Learning Shortcuts: On the Misleading Promise of NLU in Language Models,Yes.,5.,"""LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules."""
SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition,Yes.,3.,"""Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task."""
Generating Zero-shot Abstractive Explanations for Rumour Verification,Yes.,1.,"""To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM)."""
Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia,Yes.,2.,"""Overall, three LLM chatbots identify AD vs CN surpassing chance-levels but do not currently satisfy clinical application."""
Detecting Multimedia Generated by Large AI Models: A Survey,Yes.,4.,"""this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns."" and ""we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs."""
SocraSynth: Multi-LLM Reasoning with Conditional Statistics,Yes.,4.,"""Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability."""
Critical Data Size of Language Models from a Grokking Perspective,Yes.,1.,"""Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."""
Veagle: Advancements in Multimodal Representation Learning,Yes.,3.,"""While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios."""
Enhanced Automated Code Vulnerability Repair using Large Language Models,Yes.,2.,"""The research also offers a critical assessment of current evaluation metrics, such as perfect predictions, and their limitations in reflecting the true capabilities of automated repair models in real-world scenarios."""
Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation,Yes.,5.,"""Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated."""
The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance,Yes.,5.,"""We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs."""
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems,Yes.,2.,"""However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications."""
MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance,Yes.,5.,"""This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning."""
TP-Aware Dequantization,Yes.,1.,"""Our contribution is an optimized inference deployment scheme that address the current limitations of state-of-the-art quantization kernels when used in conjunction with Tensor Parallel (TP)."""
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review,Yes.,4.,"""the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations."""
Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging,Yes.,5.,"""The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations."""
A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,Yes.,5.,"""a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded,"" and ""The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations,"" and ""we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of L"
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries,Yes.,1.,"""utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries."""
Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code,,,
"""Which LLM should I use?"": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students",Yes.,2.,"""Evaluation for these tasks was carried out by pre-final year and final year undergraduate computer science students and provides insights into the models' strengths and limitations."""
The Reasoning Under Uncertainty Trap: A Structural AI Risk,Yes.,5.,"""we 1) do not currently sufficiently understand LLM capabilities in this regard, and 2) have no guarantees of performance given fundamental computational explosiveness and deep uncertainty constraints on accuracy."""
