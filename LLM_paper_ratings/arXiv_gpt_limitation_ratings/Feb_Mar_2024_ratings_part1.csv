Title,Talks about LLMs,Rate,Evidence,Published
Multi-hop Question Answering under Temporal Knowledge Editing,Yes.,3.,"""existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts.""",2024-03-30T23:22:51Z
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression,Yes.,2.,"""While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts.""",2024-03-30T23:07:58Z
Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App,Yes.,1.,"""MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being.""",2024-03-30T23:01:34Z
Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs,Yes.,5.,"""they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications.""",2024-03-30T22:41:05Z
Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4,Yes.,2.,"""However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency.""",2024-03-30T22:27:21Z
Linguistic Calibration of Language Models,Yes.,5.,"""Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate.""",2024-03-30T20:47:55Z
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Yes.,5.,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation.""",2024-03-30T19:46:59Z
MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks,Yes.,3.,"""prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets.""",2024-03-30T19:43:45Z
Do Vision-Language Models Understand Compound Nouns?,Yes.,3.,"""Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs.""",2024-03-30T16:54:45Z
CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP,Yes.,1.,"""Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints.""",2024-03-30T16:47:06Z
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order,Yes.,4.,"""However, such existing models face challenges",2024-03-30T15:38:54Z
Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks,Yes.,3.,"""While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field."" and ""Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems.""",2024-03-30T14:09:00Z
Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation,Yes.,3.,"""Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues.""",2024-03-30T13:28:51Z
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,5.,"""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving.""",2024-03-30T12:48:31Z
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,Yes.,2.,"""addresses the issue of class imbalance encountered in LLM-based annotations.""",2024-03-30T12:13:57Z
Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation,Yes.,3.,"""A challenging task in using LLMs to generate high level sub-goals is to efficiently represent the environment around the robot."" and ""But providing the LLM with a mass of contextual information (rich 3D scene semantic representation), can lead to redundant and inefficient plans.""",2024-03-30T10:54:59Z
ST-LLM: Large Language Models Are Effective Temporal Learners,Yes.,2.,"""to address the overhead and stability issues introduced by uncompressed video tokens within LLMs.""",2024-03-30T10:11:26Z
"A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",,,,2024-03-30T09:55:58Z
Instruction-Driven Game Engines on Large Language Models,Yes.,1.,"""The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes.""",2024-03-30T08:02:16Z
Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits,Yes.,3.,"""We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process.""",2024-03-30T06:49:17Z
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference,Yes.,3.,"""current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference.""",2024-03-30T04:34:54Z
A Survey of using Large Language Models for Generating Infrastructure as Code,Yes.,2.,"""Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research.""",2024-03-30T02:57:55Z
Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark,Yes.,5.,"""All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%."" and ""This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing.""",2024-03-30T02:08:28Z
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,4.,"""adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date.""",2024-03-30T01:56:07Z
Multi-Conditional Ranking with Large Language Models,Yes.,3.,"""Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow.""",2024-03-30T01:26:05Z
EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs,Yes.,3.,"""Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability.""",2024-03-30T01:16:37Z
Conceptual and Unbiased Reasoning in Language Models,Yes.,5.,"""limited study has been done on large language models' capability to perform conceptual reasoning"" and ""existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods.""",2024-03-30T00:53:53Z
GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs,Yes.,2.,"""By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training.""",2024-03-29T23:04:04Z
"Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value",Yes.,5.,"""the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations."" and ""This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation."" and ""underscore the need for a more nuanced",2024-03-29T22:49:43Z
Uncovering Bias in Large Vision-Language Models with Counterfactuals,Yes.,4.,"""While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs.""",2024-03-29T21:45:53Z
On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,3.,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data.""",2024-03-29T20:59:27Z
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models,Yes.,5.,"""extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements.""",2024-03-29T17:59:53Z
Are We on the Right Way for Evaluating Large Vision-Language Models?,Yes.,5.,"""we dig into current evaluation works and identify two primary issues",2024-03-29T17:59:34Z
ReALM: Reference Resolution As Language Modeling,Yes.,1.,"""While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized.""",2024-03-29T17:59:06Z
Gecko: Versatile Text Embeddings Distilled from Large Language Models,Yes.,1.,"""distilling knowledge from large language models (LLMs) into a retriever.""",2024-03-29T17:56:40Z
Convolutional Prompting meets Language Models for Continual Learning,Yes.,2.,"""We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned.""",2024-03-29T17:40:37Z
Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference,Yes.,3.,"""Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models.""",2024-03-29T17:22:48Z
"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",Yes.,3.,"""Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.""",2024-03-29T16:59:13Z
LayerNorm: A key component in parameter-efficient fine-tuning,No.,1.,"The abstract does not mention LLMs or any limitations associated with them. It focuses on the fine-tuning of BERT, which is a general NLP model.",2024-03-29T16:53:11Z
LUQ: Long-text Uncertainty Quantification for LLMs,Yes.,5.,"""Our study first highlights the limitations of current UQ methods in handling long text generation."" and ""We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about.""",2024-03-29T16:49:24Z
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models,Yes.,4.,"""Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation."" and ""Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability",2024-03-29T16:13:31Z
Using LLMs to Model the Beliefs and Preferences of Targeted Populations,Yes.,3.,"""Existing work has had mixed success using LLMs to accurately model human behavior in different contexts.""",2024-03-29T15:58:46Z
Shallow Cross-Encoders for Low-Latency Retrieval,Yes.,3.,"""Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window.""",2024-03-29T15:07:21Z
Distributed agency in second language learning and teaching through generative AI,Yes.,4.,"""it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations.""",2024-03-29T14:55:40Z
H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model,Yes.,4.,"""still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs"" and ""to address the inevitable 'hallucination' problem in RSVLM.""",2024-03-29T14:50:43Z
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science,Yes.,3.,"""Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training.""",2024-03-29T14:41:21Z
"The Future of Combating Rumors? Retrieval, Discrimination, and Generation",Yes.,1.,"""By using prompt engineering techniques, we feed results and knowledge into a LLM (Large Language Model), achieving satisfactory discrimination and explanatory effects while eliminating the need for fine-tuning, saving computational costs, and contributing to debunking efforts.""",2024-03-29T14:32:41Z
Measuring Taiwanese Mandarin Language Understanding,Yes.,3.,"""The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts.""",2024-03-29T13:56:21Z
ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models,Yes.,3.,"""ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.""",2024-03-29T13:12:09Z
IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context,Yes.,4.,"""The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs)."" and ""We observed that the language models exhibit more bias across a majority of the intersectional groups.""",2024-03-29T12:32:06Z
Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries,Yes.,1.,"""In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations.""",2024-03-29T12:25:37Z
Accurate Block Quantization in LLMs with Outliers,Yes.,4.,"""The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block.""",2024-03-29T12:15:06Z
User Modeling Challenges in Interactive AI Assistant Systems,Yes.,3.,"""One of the remaining challenges is to understand user's mental states during the task for more personalized guidance."" and ""investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance.""",2024-03-29T11:54:13Z
The Impact of Prompts on Zero-Shot Detection of AI-Generated Text,Yes.,2.,"""While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns.""",2024-03-29T11:33:34Z
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4.,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.""",2024-03-29T10:23:18Z
Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning,Yes.,5.,"""We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome.""",2024-03-29T08:30:34Z
PURPLE: Making a Large Language Model a Better SQL Writer,Yes.,3.,"""However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition.""",2024-03-29T07:01:29Z
On Large Language Models' Hallucination with Regard to Known Facts,Yes.,5.,"""Large language models are successful in answering factoid questions but are also prone to hallucination."" and ""Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.""",2024-03-29T06:48:30Z
Large Language Model based Situational Dialogues for Second Language Learning,Yes.,2.,"""Additionally, research in the field of dialogue systems still lacks reliable automatic evaluation metrics, leading to human evaluation as the gold standard (Smith et al., 2022), which is typically expensive.""",2024-03-29T06:43:55Z
Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning,Yes.,4.,"""However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4."" and ""As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance.""",2024-03-29T03:48:12Z
DiJiang: Efficient Large Language Models through Compact Kernelization,Yes.,2.,"""the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters.""",2024-03-29T02:32:15Z
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models,Yes.,5.,"""Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them.""",2024-03-29T01:53:24Z
Towards a Robust Retrieval-Based Summarization System,Yes.,3.,"""While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored.""",2024-03-29T00:14:46Z
LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces,,,,2024-03-28T22:06:04Z
Localizing Paragraph Memorization in Language Models,Yes.,3.,"""memorized continuations are not only harder to unlearn, but also to corrupt than non-memorized ones.""",2024-03-28T21:53:24Z
"Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",Yes.,5.,"""current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary.""",2024-03-28T21:18:33Z
Target Span Detection for Implicit Harmful Content,Yes.,1.,"""The collection is achieved using an innovative pooling method with matching scores based on human annotations and Large Language Models (LLMs).""",2024-03-28T21:15:15Z
Developing Healthcare Language Model Embedding Spaces,Yes.,3.,"""Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text.""",2024-03-28T19:31:32Z
Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care,Yes.,1.,"""We present and evaluate three different approaches for LLM-based, end-to-end ingestion of variable-length clinical EHR data to assist clinicians when triaging referrals.""",2024-03-28T19:17:07Z
GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation,Yes.,3.,"""We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability).""",2024-03-28T18:08:22Z
InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction,,,,2024-03-28T17:59:30Z
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions,Yes.,1.,"""we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs).""",2024-03-28T17:59:20Z
Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis,Yes.,1.,"""The Change-Agent integrates a multi-level change interpretation (MCI) model as the eyes and a large language model (LLM) as the brain.""",2024-03-28T17:55:42Z
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,Yes.,3.,"""Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses.""",2024-03-28T17:47:19Z
TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes,Yes.,1.,"""integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects.""",2024-03-28T17:12:55Z
WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models,Yes.,3.,"""there has been little work in analyzing the impact that these perturbations have on the quality of generated texts.""",2024-03-28T16:28:38Z
JDocQA: Japanese Document Question Answering Dataset for Generative Language Models,Yes.,1.,"""We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models.""",2024-03-28T14:22:54Z
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,Yes.,3.,"""LLMs can inherit harmful biases and produce outputs that are not aligned with human values.""",2024-03-28T14:15:10Z
BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation,Yes.,3.,"""While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation.""",2024-03-28T13:38:13Z
Checkpoint Merging via Bayesian Optimization in LLM Pretraining,Yes.,2.,"""The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs.""",2024-03-28T13:01:18Z
Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors,Yes.,5.,"""However, we argue that a critical obstacle remains in deploying LLMs for practical use",2024-03-28T12:05:15Z
Large Language Models Are Unconscious of Unreasonability in Math Problems,Yes.,5.,"""Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors."" and ""Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content.""",2024-03-28T12:04:28Z
IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation,Yes.,3.,"""However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object.""",2024-03-28T11:52:42Z
Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models,Yes.,4.,"""However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images.""",2024-03-28T11:26:30Z
MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation,Yes.,5.,"""the quality of the text generated by these models often reveals persistent issues"" and ""it is filled with significant uncertainty and instability"" and ""addressing the uncertainties and instabilities in evaluating LLMs-generated text.""",2024-03-28T10:41:47Z
Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation,Yes.,2.,"""How to select informative examples remains an open issue.""",2024-03-28T10:13:34Z
Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction,Yes.,3.,"""However, applying LLMs to grammatical error correction (GEC) is still a challenging task.""",2024-03-28T10:05:57Z
Fine-Tuning Language Models with Reward Learning on Policy,Yes.,4.,"""Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.""",2024-03-28T10:02:10Z
sDPO: Don't Use Your Data All at Once,Yes.,1.,"""As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important.""",2024-03-28T09:56:04Z
Dual-Personalizing Adapter for Federated Foundation Models,Yes.,3.,"""However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications.""",2024-03-28T08:19:33Z
Text Data-Centric Image Captioning with Interactive Prompts,Yes.,3.,"""However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage.""",2024-03-28T07:43:49Z
MUGC: Machine Generated versus User Generated Content Detection,Yes.,2.,"""While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Language Models), may contribute to this high detection accuracy.""",2024-03-28T07:33:53Z
Make Large Language Model a Better Ranker,Yes.,3.,"""These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models"" and ""This shortfall is attributed to the misalignment between the objectives of ranking and language generation.""",2024-03-28T07:22:16Z
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering,Yes.,3.,"""Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models.""",2024-03-28T06:28:35Z
Disentangling Length from Quality in Direct Preference Optimization,Yes.,3.,"""RLHF is known to exploit biases in human preferences, such as verbosity.""",2024-03-28T06:03:47Z
Compressing Large Language Models by Streamlining the Unimportant Layer,Yes.,3.,"""Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models.""",2024-03-28T04:12:13Z
Code Comparison Tuning for Code Large Language Models,Yes.,1.,"""We present Code Comparison Tuning (CCT), a simple and effective tuning method for code large language models (Code LLMs) to better handle subtle code errors.""",2024-03-28T03:25:23Z
MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering,Yes.,1.,"""Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts.""",2024-03-28T03:14:18Z
"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",Yes.,5.,"""prior benchmarks contain only a very limited set of problems, both in quantity and variety,"" ""many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data,"" ""there is a significant drop in performance (on average 39.4%) when using EvoEval,"" ""the brittleness of instruction-following models when encountering reword",2024-03-28T03:10:39Z
FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5.,"""However, hallucination is a significant concern."" and ""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted.""",2024-03-28T03:09:42Z
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,Yes.,5.,"""Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content."" and ""Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address.""",2024-03-28T02:44:02Z
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation,Yes.,1.,"""Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images.""",2024-03-28T02:35:53Z
Learning From Correctness Without Prompting Makes LLM Efficient Reasoner,Yes.,5.,"""Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content.""",2024-03-28T02:12:49Z
CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems,Yes.,1.,"""In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection.""",2024-03-27T23:45:31Z
LITA: Language Instructed Temporal-Localization Assistant,Yes.,5.,"""However, an important missing piece is temporal localization. These models cannot accurately answer the 'When?' questions. We identify three key aspects that limit their temporal localization capabilities",2024-03-27T22:50:48Z
Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data,Yes.,2.,"""LLM-annotated data without human guidance for training light-weight supervised classification models is an ineffective strategy.""",2024-03-27T22:05:10Z
Towards LLM-RecSys Alignment with Textual ID Learning,Yes.,3.,"""current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations.""",2024-03-27T21:22:37Z
TextCraftor: Your Text Encoder Can be Image Quality Controller,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on diffusion-based text-to-image generative models and the text encoder in Stable Diffusion.,2024-03-27T19:52:55Z
"""Sorry, Come Again?"" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",Yes.,5.,"""Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs)."" and ""Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans."" and ""Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle.""",2024-03-27T19:45:09Z
A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products,Yes.,3.,"""Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations.""",2024-03-27T19:02:56Z
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said,Yes.,4.,"""However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications.""",2024-03-27T18:22:48Z
Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment,No.,1.,The abstract focuses on a GPT-based model for dance accompaniment and does not discuss language models or their limitations.,2024-03-27T17:57:02Z
Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation,Yes.,3.,"""This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve.""",2024-03-27T17:50:00Z
Projective Methods for Mitigating Gender Bias in Pre-trained Language Models,Yes.,4.,"""We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but that the two outcomes are not necessarily correlated. This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing",2024-03-27T17:49:31Z
Long-form factuality in large language models,Yes.,5.,"""Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics.""",2024-03-27T17:48:55Z
CheckEval: Robust Evaluation Framework using Large Language Model via Checklist,Yes.,2.,"""addressing the challenges of ambiguity and inconsistency in current evaluation methods"" and ""CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.""",2024-03-27T17:20:39Z
Understanding the Learning Dynamics of Alignment with Human Feedback,Yes.,3.,"""theoretically understanding how these methods affect model behavior remains an open question"" and ""the optimization is prone to prioritizing certain behaviors with higher preference distinguishability.""",2024-03-27T16:39:28Z
The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian,Yes.,3.,"""We show that this is a challenging benchmark where current language models are bound by 60% accuracy.""",2024-03-27T15:46:25Z
NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method,Yes.,5.,"""Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field.""",2024-03-27T15:22:16Z
SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens,Yes.,1.,"""We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT).""",2024-03-27T14:54:27Z
Vulnerability Detection with Code Language Models: How Far Are We?,Yes.,5.,"""Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models."" and ""Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings.""",2024-03-27T14:34:29Z
"A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks",Yes.,1.,"""In this paper, we sketch a proof of principle for such a method using large language models (LLMs), expert legal systems known as legal decision paths, and Bayesian networks.""",2024-03-27T13:12:57Z
SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks,,,,2024-03-27T10:24:25Z
BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text,Yes.,5.,"""However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources.""",2024-03-27T10:18:21Z
Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval,Yes.,2.,"""the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored.""",2024-03-27T09:46:56Z
Improving Attributed Text Generation of Large Language Models via Preference Learning,Yes.,3.,"""Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content.""",2024-03-27T09:19:13Z
BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models,Yes.,4.,"""However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc.""",2024-03-27T08:57:21Z
Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback,Yes.,5.,"""Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope."" and ""These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby",2024-03-27T08:39:56Z
Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective,Yes.,5.,"""MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks.""",2024-03-27T08:38:49Z
LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models,Yes.,1.,"""we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs).""",2024-03-27T08:34:55Z
IterAlign: Iterative Constitutional Alignment of Large Language Models,Yes.,3.,"""However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming.""",2024-03-27T08:32:19Z
Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications,Yes.,5.,"""We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.""",2024-03-27T08:08:00Z
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4.,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions.""",2024-03-27T06:43:58Z
Toward Interactive Regional Understanding in Vision-Large Language Models,Yes.,3.,"""these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability.""",2024-03-27T05:22:06Z
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models,Yes.,3.,"""the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning.""",2024-03-27T04:49:23Z
Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges,Yes.,3.,"""Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency.""",2024-03-27T04:39:18Z
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,Yes.,1.,"""Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge.""",2024-03-27T04:20:18Z
Leveraging Large Language Models for Fuzzy String Matching in Political Science,Yes.,1.,"""we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner.""",2024-03-27T03:04:21Z
Exploring the Privacy Protection Capabilities of Chinese Large Language Models,Yes.,5.,"""Our observations indicate that existing Chinese large language models universally show privacy protection shortcomings. It seems that at the moment this widespread issue is unavoidable and may pose corresponding privacy risks in applications based on these models.""",2024-03-27T02:31:54Z
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models,Yes.,3.,"""However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices.""",2024-03-26T23:51:44Z
Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency,Yes.,2.,"""We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers."" and ""Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.""",2024-03-26T23:32:52Z
Large Language Models Produce Responses Perceived to be Empathic,Yes.,1.,"""Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy.""",2024-03-26T23:14:34Z
Juru: Legal Brazilian Large Language Model from Reputable Sources,Yes.,3.,"""However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language.""",2024-03-26T22:54:12Z
For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers,Yes.,3.,"""it is not well understood how unclear or nonstandard language queries affect the model outputs.""",2024-03-26T22:08:33Z
Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,Yes.,5.,"""However, they still make unjustified logical and computational errors in their reasoning steps and answers.""",2024-03-26T22:01:13Z
Large Language Models for Education: A Survey and Outlook,Yes.,3.,"""identify the risks and challenges associated with deploying LLMs in education.""",2024-03-26T21:04:29Z
Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models,Yes.,2.,"""error analysis reveals several existing issues in the retrieval system that still need resolution.""",2024-03-26T20:25:53Z
PerOS: Personalized Self-Adapting Operating Systems in the Cloud,Yes.,1.,"""the rise of large language models (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and software development paradigms.""",2024-03-26T20:10:31Z
COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning,Yes.,3.,"""However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks.""",2024-03-26T19:24:18Z
Supervisory Prompt Training,Yes.,2.,"""The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable.""",2024-03-26T19:08:20Z
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER,Yes.,3.,"""Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains.""",2024-03-26T18:23:16Z
MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution,Yes.,4.,"""Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level."" and ""To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.""",2024-03-26T17:57:57Z
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning,Yes.,2.,"""their huge memory consumption has become a major roadblock to large-scale training.""",2024-03-26T17:55:02Z
The Unreasonable Ineffectiveness of the Deeper Layers,Yes.,5.,"""the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.""",2024-03-26T17:20:04Z
Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach,,,,2024-03-26T17:02:42Z
Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications,Yes.,2.,"""we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks.""",2024-03-26T16:49:25Z
ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages,Yes.,1.,"""Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models.""",2024-03-26T16:48:13Z
Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs,Yes.,3.,"""little work has been done to establish the degree to which language models capture this type of generalization"" and ""We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural",2024-03-26T16:45:27Z
ArabicaQA: A Comprehensive Dataset for Arabic Question Answering,Yes.,1.,"""our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context.""",2024-03-26T16:37:54Z
Assessment of Multimodal Large Language Models in Alignment with Human Values,Yes.,3.,"""despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations.""",2024-03-26T16:10:21Z
Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs),Yes.,2.,"""We explore various roles that LLMs can play in this context while identifying some of the challenges to address.""",2024-03-26T15:54:48Z
Are Compressed Language Models Less Subgroup Robust?,Yes.,3.,"""To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset.""",2024-03-26T15:50:37Z
Improving Text-to-Image Consistency via Automatic Prompt Optimization,Yes.,1.,"""introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models.""",2024-03-26T15:42:01Z
Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications,Yes.,4.,"""Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models"" and ""This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications.""",2024-03-26T15:20:49Z
Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons,Yes.,5.,"""show that GPT-4 and Llama 2 fail it with strong bias"" and ""we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.""",2024-03-26T14:51:12Z
Can multiple-choice questions really be useful in detecting the abilities of LLMs?,Yes.,5.,"""There are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required."" and ""LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position."" and ""Our results reveal a relatively low correlation between answers from MCQs and L",2024-03-26T14:43:48Z
Optimization-based Prompt Injection Attack to LLM-as-a-Judge,Yes.,4.,"""However, the robustness of these systems against prompt injection attacks remains an open question."" and ""highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.""",2024-03-26T13:58:00Z
Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement,Yes.,1.,"""we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed 'Topic Refinement'.""",2024-03-26T13:50:34Z
ExpressEdit: Video Editing with Natural Language and Sketching,Yes.,1.,"""Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching.""",2024-03-26T13:34:21Z
Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes,Yes.,1.,"""The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains.""",2024-03-26T13:32:32Z
Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games,Yes.,1.,"""Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations.""",2024-03-26T13:02:46Z
Targeted Visualization of the Backbone of Encoder LLMs,Yes.,4.,"""they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues.""",2024-03-26T12:51:02Z
Language Models for Text Classification: Is In-Context Learning Enough?,Yes.,3.,"""In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.""",2024-03-26T12:47:39Z
"""You are an expert annotator"": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",Yes.,2.,"""Regression is considered more challenging than classification",2024-03-26T11:45:22Z
RuBia: A Russian Language Bias Detection Dataset,Yes.,4.,"""Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data"" and ""we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.""",2024-03-26T10:01:01Z
Naive Bayes-based Context Extension for Large Language Models,,,,2024-03-26T09:59:45Z
Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction,Yes.,1.,"""Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC).""",2024-03-26T09:43:15Z
ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler,Yes.,1.,"""Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks.""",2024-03-26T09:41:21Z
KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion,Yes.,5.,"""it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission.""",2024-03-26T09:36:59Z
DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation,Yes.,3.,"""However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs.""",2024-03-26T08:47:23Z
Robust and Scalable Model Editing for Large Language Models,Yes.,5.,"""Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context.""",2024-03-26T06:57:23Z
LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction,Yes.,1.,"""we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).""",2024-03-26T06:12:21Z
Disambiguate Entity Matching through Relation Discovery with Large Language Models,Yes.,1.,"""Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT.""",2024-03-26T03:07:32Z
Residual-based Language Models are Free Boosters for Biomedical Imaging,,,,2024-03-26T03:05:20Z
The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge,Yes.,2.,"""we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to assess the quality of generated captions.""",2024-03-26T03:03:50Z
Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models,Yes.,4.,"""there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers"" and ""jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.""",2024-03-26T02:47:42Z
JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset,Yes.,3.,"""we identified limitations in the task completion capabilities of LLMs in Japanese.""",2024-03-26T02:01:18Z
ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching,Yes.,3.,"""Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature.""",2024-03-26T01:46:34Z
"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",Yes.,5.,"""In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs).""",2024-03-26T01:28:42Z
Automate Knowledge Concept Tagging on Math Questions with LLMs,Yes.,1.,"""In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications.""",2024-03-26T00:09:38Z
A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning,,,,2024-03-25T23:02:33Z
TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models,Yes.,3.,"""large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences.""",2024-03-25T22:47:13Z
SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies,Yes.,3.,"""We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies.""",2024-03-25T21:48:22Z
A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection,Yes.,5.,"""LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that",2024-03-25T21:47:36Z
Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis,Yes.,3.,"""We also find that the task is highly challenging for Large Language Models, even after fine-tuning.""",2024-03-25T21:46:35Z
Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation,Yes.,3.,"""Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code.""",2024-03-25T21:41:31Z
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node,Yes.,1.,"""a system powered by large language models is designed and implemented to process 'semantic node' and generate AAS instance models from textual technical data.""",2024-03-25T21:37:30Z
Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model,Yes.,5.,"""Unexpectedly, the RBS outperformed the LLMs across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and LLM.""",2024-03-25T21:19:50Z
Outcome-Constrained Large Language Models for Countering Hate Speech,Yes.,2.,"""However, it remains unclear what impact the counterspeech might have in an online environment.""",2024-03-25T19:44:06Z
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models,Yes.,3.,"""existing methods are parameter-adherent to the policy model, leading to two key limitations",2024-03-25T19:28:10Z
"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",Yes.,1.,"""This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM).""",2024-03-25T19:17:43Z
The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition,Yes.,5.,"""However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition,"" and ""We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that",2024-03-25T19:07:32Z
"Attribute First, then Generate: Locally-attributable Grounded Text Generation",Yes.,3.,"""Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections.""",2024-03-25T18:41:47Z
DreamLIP: Language-Image Pre-training with Long Captions,Yes.,1.,"""we first re-caption 30M images with detailed descriptions using a pre-trained Multi-modality Large Language Model (MLLM)""",2024-03-25T17:59:42Z
Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows,No.,1.,The abstract does not mention LLMs or their limitations. It focuses on diffusion language models and a proposed method called Language Rectified Flow.,2024-03-25T17:58:22Z
Comp4D: LLM-Guided Compositional 4D Scene Generation,Yes.,1.,"""Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories.""",2024-03-25T17:55:52Z
AIOS: LLM Agent Operating System,Yes.,3.,"""Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations.""",2024-03-25T17:32:23Z
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators,Yes.,5.,"""LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments.""",2024-03-25T17:11:28Z
PropTest: Automatic Property Testing for Improved Visual Programming,Yes.,1.,"""This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data.""",2024-03-25T16:39:15Z
Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data,Yes.,3.,"""we conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of LLMs for synthetic data generation for depression data.""",2024-03-25T16:21:25Z
Do LLM Agents Have Regret? A Case Study in Online Learning and Games,Yes.,4.,"""Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret.""",2024-03-25T15:04:11Z
Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making,Yes.,1.,"""To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision.""",2024-03-25T14:34:06Z
An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems,Yes.,1.,"""We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization.""",2024-03-25T14:32:28Z
Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback,Yes.,5.,"""As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.""",2024-03-25T14:07:27Z
"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",Yes.,4.,"""It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code."" and ""Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks.""",2024-03-25T13:23:24Z
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography,Yes.,5.,"""students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test"" and ""ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students' knowledge application and creativity were insignificant.""",2024-03-25T12:23:12Z
CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment,Yes.,2.,"""a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training.""",2024-03-25T11:37:15Z
Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units,Yes.,3.,"""Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities."" and ""substantial progress, especially in the realm of Large Language Models, remains lacking.""",2024-03-25T10:39:18Z
Can Large Language Models (or Humans) Distill Text?,,,,2024-03-25T09:51:54Z
NSINA: A News Corpus for Sinhala,Yes.,2.,"""their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges",2024-03-25T09:36:51Z
Elysium: Exploring Object-level Perception in Videos via MLLM,Yes.,5.,"""extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships"" and ""processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden.""",2024-03-25T09:17:15Z
Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art,Yes.,4.,"""foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor."" and ""discuss existing approaches to hallucination detection and mitigation with a focus on decision problems.""",2024-03-25T08:11:02Z
Harnessing the power of LLMs for normative reasoning in MASs,Yes.,2.,"""We also highlight challenges in this emerging field.""",2024-03-25T08:09:01Z
LLMs Are Few-Shot In-Context Low-Resource Language Learners,,,,2024-03-25T07:55:29Z
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,Yes.,1.,"""Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks.""",2024-03-25T07:38:40Z
"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm",Yes.,3.,"""Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations.""",2024-03-25T06:17:54Z
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,Yes.,1.,"""The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development.""",2024-03-25T06:09:55Z
Evaluating Large Language Models with Runtime Behavior of Program Execution,Yes.,5.,"""most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3).""",2024-03-25T05:37:16Z
InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models,Yes.,1.,"""This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs).""",2024-03-25T05:31:22Z
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models,Yes.,2.,"""raising concerns about the adversarial vulnerability of this paradigm"" and ""Extensive results demonstrate the effectiveness of $\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo.""",2024-03-25T05:27:35Z
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation,Yes.,3.,"""However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations. Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as",2024-03-25T05:12:18Z
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations,Yes.,3.,"""The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness.""",2024-03-25T05:04:52Z
How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation,Yes.,5.,"""While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation",2024-03-25T04:21:06Z
Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases,Yes.,3.,"""discover the limitations of unified information extraction and large language models in solving definition bias.""",2024-03-25T03:19:20Z
Concurrent Linguistic Error Detection (CLED) for Large Language Models,Yes.,3.,"""Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue.""",2024-03-25T03:17:27Z
Dia-LLaMA: Towards Large Language Model-driven CT Report Generation,Yes.,1.,"""Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges.""",2024-03-25T03:02:51Z
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",Yes.,1.,"""We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images.""",2024-03-25T03:02:27Z
ChatDBG: An AI-Powered Debugging Assistant,Yes.,1.,"""ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers.""",2024-03-25T01:12:57Z
Enhanced Facet Generation with LLM Editing,Yes.,1.,"""The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model.""",2024-03-25T00:43:44Z
Is Watermarking LLM-Generated Code Robust?,Yes.,3.,"""we show that it is easy to remove these watermarks on code by semantic-preserving transformations.""",2024-03-24T21:41:29Z
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review,Yes.,3.,"""Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations.""",2024-03-24T21:29:39Z
Engineering Safety Requirements for Autonomous Driving with Large Language Models,Yes.,1.,"""Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update.""",2024-03-24T20:40:51Z
AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue,Yes.,1.,"""While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited.""",2024-03-24T19:50:49Z
Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling,Yes.,2.,"""Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing LLMs in topic extraction.""",2024-03-24T17:39:51Z
CoverUp: Coverage-Guided LLM-Based Test Generation,Yes.,1.,"""This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs).""",2024-03-24T16:18:27Z
ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models,Yes.,2.,"""Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks.""",2024-03-24T15:09:55Z
A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish,Yes.,4.,"""This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance.""",2024-03-24T13:21:58Z
Opportunities and challenges in the application of large artificial intelligence models in radiology,Yes.,2.,"""Finally, this paper also summarizes some of the challenges of large AI models in radiology.""",2024-03-24T12:05:23Z
Can Language Models Pretend Solvers? Logic Code Simulation with LLMs,Yes.,3.,"""What strength arises along with logic code simulation? And what pitfalls?""",2024-03-24T11:27:16Z
LLMs as Compiler for Arabic Programming Language,Yes.,1.,"""In this paper we introduce APL (Arabic Programming Language) that uses Large language models (LLM) as semi-compiler to covert Arabic text code to python code then run the code.""",2024-03-24T10:57:08Z
Argument Quality Assessment in the Age of Instruction-Following Large Language Models,Yes.,2.,"""We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems.""",2024-03-24T10:43:21Z
Qibo: A Large Language Model for Traditional Chinese Medicine,Yes.,3.,"""the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources.""",2024-03-24T07:48:05Z
Monotonic Paraphrasing Improves Generalization of Language Model Prompting,Yes.,2.,"""Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity.""",2024-03-24T06:49:07Z
CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Yes.,3.,"""While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques.""",2024-03-24T04:34:34Z
LlamBERT: Large-scale low-cost data annotation in NLP,Yes.,2.,"""Despite their effectiveness, the high costs associated with their use pose a challenge.""",2024-03-23T21:54:34Z
Leveraging Zero-Shot Prompting for Efficient Language Model Distillation,Yes.,1.,"""This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor.""",2024-03-23T16:51:52Z
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions,Yes.,5.,"""little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones.""",2024-03-23T16:12:52Z
Using Large Language Models for OntoClean-based Ontology Refinement,Yes.,1.,"""This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology.""",2024-03-23T15:09:50Z
When LLM-based Code Generation Meets the Software Development Process,Yes.,1.,"""This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum.""",2024-03-23T14:04:48Z
ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning,,,,2024-03-23T13:21:09Z
Computational Sentence-level Metrics Predicting Human Sentence Comprehension,Yes.,1.,"""This study introduces innovative methods for computing sentence-level metrics using multilingual large language models.""",2024-03-23T12:19:49Z
AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving,Yes.,3.,"""existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs.""",2024-03-23T10:42:49Z
The Frontier of Data Erasure: Machine Unlearning for Large Language Models,Yes.,5.,"""Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets.""",2024-03-23T09:26:15Z
Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study,Yes.,1.,"""A large language model can quickly summarise information in less time than a human.""",2024-03-23T07:59:30Z
Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models,Yes.,2.,"""How can everyday web users confirm if LLMs misuse their data without permission?"" and ""To explore the effectiveness and usage of this copyrighting tool, we define the user training data identification task with ghost sentences.""",2024-03-23T06:36:32Z
LLMs Instruct LLMs:An Extraction and Editing Method,Yes.,3.,"""The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges. This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLM",2024-03-23T06:03:36Z
Towards a RAG-based Summarization Agent for the Electron-Ion Collider,Yes.,1.,"""second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data.""",2024-03-23T05:32:46Z
Contact-aware Human Motion Generation from Textual Descriptions,No.,1.,The abstract does not mention LLMs or any limitations related to them.,2024-03-23T04:08:39Z
FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models,,,,2024-03-23T03:32:26Z
SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models,Yes.,1.,"""To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling.""",2024-03-23T03:23:29Z
MixRED: A Mix-lingual Relation Extraction Dataset,Yes.,3.,"""we evaluate both state-of-the-art supervised models and large language models (LLMs) on MixRED, revealing their respective advantages and limitations in the mix-lingual scenario.""",2024-03-23T03:18:14Z
EAGLE: A Domain Generalization Framework for AI-generated Text Detection,Yes.,2.,"""With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models."" and ""building supervised detectors for identifying text from such new models would require new labeled training data, which",2024-03-23T02:44:20Z
AI for Biomedicine in the Era of Large Language Models,Yes.,4.,"""we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation.""",2024-03-23T01:40:22Z
SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning,,,,2024-03-22T23:12:28Z
Differentially Private Next-Token Prediction of Large Language Models,Yes.,3.,"""DP-SGD overestimates an adversary's capabilities in having white box access to the model and, as a result, causes longer training times and larger memory usage than SGD.""",2024-03-22T22:27:44Z
Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers,Yes.,2.,"""Concerns about the security implications of this trend have been raised"" and ""Our findings suggest developers are under-educated on insecure code propagation from both platforms.""",2024-03-22T20:06:41Z
"Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",Yes.,4.,"""Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies.""",2024-03-22T19:21:44Z
"Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",Yes.,1.,"""This study delves into university instructors' experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators' perspectives on AI's role in the classroom and its potential impacts on teaching and learning.""",2024-03-22T19:21:29Z
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis,Yes.,3.,"""Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance.""",2024-03-22T19:19:51Z
Long-CLIP: Unlocking the Long-Text Capability of CLIP,Yes.,5.,"""a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and text-to-image generation with extensive prerequisites.""",2024-03-22T17:58:16Z
Can large language models explore in-context?,,,,2024-03-22T17:50:43Z
LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers,Yes.,2.,"""We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities.""",2024-03-22T17:31:43Z
Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs,Yes.,5.,"""suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.""",2024-03-22T17:27:18Z
CoLLEGe: Concept Embedding Generation for Large Language Models,Yes.,4.,"""Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts.""",2024-03-22T17:26:05Z
Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization,Yes.,1.,"""SID is generated utilizing multimodal GPT-4 and can be seamlessly integrated into optimization-based models.""",2024-03-22T16:35:38Z
Sphere Neural-Networks for Rational Reasoning,Yes.,5.,"""This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational reasoning and cause LLMs to be trapped in the swamp of hallucination.""",2024-03-22T15:44:59Z
Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,Yes.,3.,"""Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.""",2024-03-22T15:16:23Z
Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs,,,,2024-03-22T15:16:10Z
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models,Yes.,3.,"""Retrieval-Augmented-Generation and Generation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results",2024-03-22T15:06:45Z
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach,Yes.,3.,"""targeting the inadequacies in current evaluation methods"" and ""challenge assumptions about emergent abilities and the influence of given training types and architectures in LLMs.""",2024-03-22T14:47:35Z
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions,Yes.,3.,"""Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information.""",2024-03-22T14:42:29Z
An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets,Yes.,4.,"""our study...highlights the pervasive issue of license inconsistencies in large language models trained on code.""",2024-03-22T14:23:21Z
Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models,Yes.,1.,"""we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs),"" and ""The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed.""",2024-03-22T14:20:34Z
InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection,Yes.,3.,"""Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media."" and ""Our investigations show that the objectives of fidelity and utility may conflict and that prompt engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity",2024-03-22T13:58:42Z
MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection,Yes.,1.,"""we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates Large Language Models (LLMs) to understand the complementary information at the semantic level and further enhance the fusion process.""",2024-03-22T13:50:27Z
Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study,Yes.,5.,"""knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages,"" and ""CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions.""",2024-03-22T13:13:13Z
CACA Agent: Capability Collaboration based AI Agent,Yes.,3.,"""Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality.""",2024-03-22T11:42:47Z
Text clustering with LLM embeddings,Yes.,2.,"""we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models.""",2024-03-22T11:08:48Z
Construction of a Japanese Financial Benchmark for Large Language Models,Yes.,1.,"""With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity.""",2024-03-22T09:40:27Z
LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement,Yes.,2.,"""While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging.""",2024-03-22T08:57:07Z
Magic for the Age of Quantized DNNs,,,,2024-03-22T07:21:09Z
Risk and Response in Large Language Models: Evaluating Key Threat Categories,Yes.,5.,"""Our findings indicate that LLMs tend to consider Information Hazards less harmful,"" and ""The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in LLM risk assessment and emphasizing the need for improved AI safety measures.""",2024-03-22T06:46:40Z
MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts,Yes.,1.,"""We employ large language models (LLMs) to solve this task through several prompting techniques.""",2024-03-22T06:31:49Z
Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation,Yes.,2.,"""However, our study acknowledges that there are limitations to the proposed approach.""",2024-03-22T05:37:52Z
Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices,No.,1.,The abstract does not mention LLMs or any specific limitations of language models.,2024-03-22T05:23:31Z
Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,Yes.,3.,"""Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses.""",2024-03-22T05:05:45Z
On Zero-Shot Counterspeech Generation by LLMs,Yes.,4.,"""Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size."" and ""GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT",2024-03-22T04:13:10Z
AutoRE: Document-Level Relation Extraction with Large Language Models,Yes.,3.,"""Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple",2024-03-21T23:48:21Z
Evaluating the Performance of LLMs on Technical Language Processing tasks,,,,2024-03-21T23:40:42Z
VidLA: Video-Language Alignment at Scale,Yes.,1.,"""we leverage recent LLMs to curate the largest video-language dataset to date with better visual grounding.""",2024-03-21T22:36:24Z
Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models,Yes.,3.,"""Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-",2024-03-21T22:08:44Z
The opportunities and risks of large language models in mental health,Yes.,4.,"""We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks.""",2024-03-21T19:59:52Z
Can 3D Vision-Language Models Truly Understand Natural Language?,Yes.,5.,"""existing 3D-VL models exhibit sensitivity to the styles of language input, struggling to understand sentences with the same semantic meaning but written in different variants"" and ""Even the state-of-the-art 3D-LLM fails to understand some variants of the same sentences"" and ""our comprehensive evaluation uncovers a significant drop in the performance of all existing models across various 3",2024-03-21T18:02:20Z
VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding,Yes.,1.,"""Recent studies have demonstrated the effectiveness of Large Language Models (LLMs) as reasoning modules that can deconstruct complex tasks into more manageable sub-tasks, particularly when applied to visual reasoning tasks for images.""",2024-03-21T18:00:00Z
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?,Yes.,3.,"""However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood.""",2024-03-21T17:59:50Z
Language Repository for Long Video Understanding,Yes.,5.,"""Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length.""",2024-03-21T17:59:35Z
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey,Yes.,3.,"""However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities.""",2024-03-21T17:55:50Z
RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain,Yes.,4.,"""yet their reliability in realistic use cases is under-researched"" and ""We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.""",2024-03-21T17:30:59Z
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,Yes.,2.,"""A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.""",2024-03-21T17:09:08Z
The Era of Semantic Decoding,Yes.,2.,"""Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs.""",2024-03-21T17:06:17Z
EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling,Yes.,2.,"""However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity.""",2024-03-21T16:41:12Z
Open Source Conversational LLMs do not know most Spanish words,Yes.,4.,"""The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.""",2024-03-21T15:41:02Z
Detoxifying Large Language Models via Knowledge Editing,Yes.,4.,"""This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs)."" and ""We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.""",2024-03-21T15:18:30Z
ChatGPT Alternative Solutions: Large Language Models Survey,Yes.,3.,"""our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.""",2024-03-21T15:16:50Z
gTBLS: Generating Tables from Text by Conditional Question Answering,Yes.,1.,"""Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible.""",2024-03-21T15:04:32Z
Locating and Mitigating Gender Bias in Large Language Models,Yes.,4.,"""this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society"" and ""we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns.""",2024-03-21T13:57:43Z
Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination,Yes.,5.,"""However, they suffer from visual hallucination, where the generated responses diverge from the provided image.""",2024-03-21T13:49:42Z
Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning,Yes.,3.,"""off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.""",2024-03-21T13:47:40Z
From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision,Yes.,3.,"""However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight.""",2024-03-21T13:29:54Z
FIT-RAG: Black-Box RAG with Factual Information and Token Reduction,Yes.,5.,"""Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications."" and ""Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues",2024-03-21T13:05:18Z
"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",Yes.,5.,"""The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are 'unknown' to them.""",2024-03-21T12:45:12Z
Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics,Yes.,3.,"""Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs).""",2024-03-21T12:45:01Z
Exploring the Potential of Large Language Models in Graph Generation,Yes.,3.,"""We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance.""",2024-03-21T12:37:54Z
Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives,Yes.,3.,"""We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts.""",2024-03-21T12:17:59Z
ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting,Yes.,3.,"""Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts."" and ""To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method.""",2024-03-21T11:34:26Z
From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora,Yes.,3.,"""users' reliance on AI-generated content, such as the ones produced by Large Language Models, which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures.""",2024-03-21T11:04:41Z
Multi-role Consensus through LLMs Discussions for Vulnerability Detection,Yes.,2.,"""Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.""",2024-03-21T10:28:18Z
LLM-based Extraction of Contradictions from Patents,Yes.,3.,"""While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models.""",2024-03-21T09:36:36Z
ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification,Yes.,3.,"""it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.""",2024-03-21T09:28:38Z
LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding,Yes.,2.,"""Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate.""",2024-03-21T09:25:24Z
Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology,Yes.,1.,"""The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools.""",2024-03-21T09:02:17Z
Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection,Yes.,4.,"""Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models.""",2024-03-21T08:57:27Z
PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning,Yes.,3.,"""We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback.""",2024-03-21T08:37:15Z
Improving the Robustness of Large Language Models via Consistency Alignment,Yes.,5.,"""their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.""",2024-03-21T08:21:12Z
Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering,Yes.,2.,"""Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality.""",2024-03-21T07:47:57Z
MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation,Yes.,2.,"""However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored."" and ""how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.""",2024-03-21T06:47:28Z
Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond,Yes.,1.,"""Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing.""",2024-03-21T05:57:27Z
Empowering Segmentation Ability to Multi-modal Large Language Models,Yes.,3.,"""Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a large margin compared to the original MLLMs.""",2024-03-21T05:36:25Z
AI and Memory Wall,Yes.,5.,"""the main performance bottleneck is increasingly shifting to memory bandwidth"" and ""we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models.""",2024-03-21T04:31:59Z
Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors,Yes.,2.,"""We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.""",2024-03-21T04:23:56Z
From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation,Yes.,3.,"""The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches.""",2024-03-21T04:07:40Z
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations,Yes.,4.,"""We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability,"" and ""We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.""",2024-03-21T03:52:01Z
"Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",Yes.,4.,"""Independent analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect.""",2024-03-21T02:12:03Z
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics,Yes.,3.,"""We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.""",2024-03-21T01:57:30Z
Protected group bias and stereotypes in Large Language Models,Yes.,5.,"""We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations."" and ""The model not only reflects societal biases, but appears to amplify them."" and ""This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should",2024-03-21T00:21:38Z
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning,Yes.,2.,"""using writing assistants introduces a mental dilemma, as some content isn't directly our creation.""",2024-03-20T21:06:42Z
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs,Yes.,4.,"""our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time.""",2024-03-20T21:02:16Z
Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification,Yes.,4.,"""Despite the growing capabilities of large language models, there exists concerns about the biases they develop."" and ""bias occurs due to both intrinsic model architecture and dataset.""",2024-03-20T18:59:18Z
Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases,Yes.,1.,"""We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs)""",2024-03-20T18:13:17Z
RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition,Yes.,3.,"""the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size.""",2024-03-20T17:59:55Z
Reverse Training to Nurse the Reversal Curse,Yes.,5.,"""Large language models (LLMs) have a surprising failure",2024-03-20T17:55:35Z
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection,Yes.,3.,"""we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection",2024-03-20T17:54:58Z
Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Yes.,1.,"""To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions.""",2024-03-20T17:47:49Z
Information-Theoretic Distillation for Reference-less Summarization,Yes.,3.,"""While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer.""",2024-03-20T17:42:08Z
EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation,Yes.,3.,"""However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs.""",2024-03-20T16:43:42Z
Large Language Models meet Network Slicing Management and Orchestration,Yes.,2.,"""We also discuss the challenges associated with implementing this framework and potential solutions to mitigate them.""",2024-03-20T16:29:52Z
RoleInteract: Evaluating the Social Interaction of Role-Playing Agents,Yes.,2.,"""While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence.""",2024-03-20T15:38:36Z
Defending Against Indirect Prompt Injection Attacks With Spotlighting,Yes.,5.,"""Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources.""",2024-03-20T15:26:23Z
No more optimization rules: LLM-enabled policy-based multi-modal query optimizer,Yes.,3.,"""to prevent LLM from making mistakes or negative optimization.""",2024-03-20T13:44:30Z
Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models,Yes.,3.,"""Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options.""",2024-03-20T13:38:07Z
CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing,Yes.,4.,"""Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""",2024-03-20T13:33:55Z
A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation,Yes.,1.,"""Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender.""",2024-03-20T13:14:29Z
Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach,Yes.,1.,"""The primary contribution of this research is in the demonstration of how Large Language Models can be integrated into machine learning workflows for incident management, thereby simplifying feature extraction from unstructured text and enhancing or matching the precision of severity predictions using conventional machine learning pipeline.""",2024-03-20T12:33:51Z
Motion Generation from Fine-grained Textual Descriptions,Yes.,1.,"""by feeding GPT-3.5-turbo with step-by-step instructions with pseudo-code compulsory checks.""",2024-03-20T11:38:30Z
FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs,Yes.,3.,"""Despite the remarkable performance of video-based large language models (LLMs), their adversarial threat remains unexplored.""",2024-03-20T11:05:07Z
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis,Yes.,2.,"""automatic generation of a video synopsis based on the original single prompt leveraging LLMs,"" and ""open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content.""",2024-03-20T10:58:58Z
An Entropy-based Text Watermarking Detection Method,Yes.,2.,"""Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved.""",2024-03-20T10:40:01Z
Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training,Yes.,1.,"""The advancement of Large Language Models (LLMs) has significantly transformed the field of natural language processing, although the focus on English-centric models has created a noticeable research gap for specific languages, including Vietnamese.""",2024-03-20T10:14:13Z
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models,Yes.,1.,"""Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks.""",2024-03-20T08:08:54Z
ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics,Yes.,1.,"""A baseline for POM, leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities.""",2024-03-20T07:48:32Z
BadEdit: Backdooring large language models by model editing,Yes.,2.,"""Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs).""",2024-03-20T07:34:18Z
Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection,Yes.,3.,"""single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset.""",2024-03-20T06:38:13Z
Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model,Yes.,1.,"""a novel method called ODPC is proposed, in which specific prompts to generate OOD peer classes of ID semantics are designed by a large language model as an auxiliary modality to facilitate detection.""",2024-03-20T06:04:05Z
PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns,Yes.,5.,"""we find that they are not able to generalize well to simple abstract patterns. Notably, even GPT-4V cannot solve more than half of the puzzles"" and ""we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future.""",2024-03-20T05:37:24Z
Polaris: A Safety-focused LLM Constellation Architecture for Healthcare,Yes.,1.,"""Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations.""",2024-03-20T05:34:03Z
LeanReasoner: Boosting Complex Logical Reasoning with Lean,Yes.,4.,"""Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning.""",2024-03-20T05:29:06Z
Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal,Yes.,4.,"""However, this technological advancement is accompanied by significant risks and vulnerabilities. Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs.""",2024-03-20T05:17:22Z
Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference,Yes.,1.,"""This paper investigates the use of Large Language Models (LLMs) for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs (FPNs).""",2024-03-20T04:57:32Z
Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models,Yes.,1.,"""We propose utilizing knowledge distillation of large language models to annotate the dataset.""",2024-03-20T02:29:09Z
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model,Yes.,1.,"""Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'.""",2024-03-20T02:15:55Z
SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization,Yes.,3.,"""the scarcity of fine-tuning samples makes this approach challenging in some cases.""",2024-03-20T02:04:42Z
Technical Report: Competition Solution For BetterMixture,Yes.,1.,"""the challenge of selecting and optimizing datasets from the vast and complex sea of data, to enhance the performance of large language models within the constraints of limited computational resources.""",2024-03-20T01:46:06Z
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards,Yes.,5.,"""However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations."" and ""multiple concerns regarding the safety and ingrained biases in these models remain."" and ""a clear trade-off between the helpfulness and safety of these models has been documented in the literature.""",2024-03-20T00:22:38Z
A Study of Vulnerability Repair in JavaScript Programs with Large Language Models,Yes.,3.,"""while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.""",2024-03-19T23:04:03Z
VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning,Yes.,4.,"""The broader capabilities and limitations of multimodal ICL remain under-explored,"" and ""revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging.""",2024-03-19T21:31:56Z
Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning,Yes.,1.,"""This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs).""",2024-03-19T18:37:05Z
LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction,Yes.,1.,"""We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies.""",2024-03-19T18:10:13Z
LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression,Yes.,3.,"""The challenge is that information entropy may be a suboptimal compression metric",2024-03-19T17:59:56Z
Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models,Yes.,3.,"""it is challenging for the visual encoder in Large Vision-Language Models (LVLMs) to extract useful features tailored to questions that aid the language model's response"" and ""a common practice among existing LVLMs is to utilize lower-resolution images, which restricts the ability for visual recognition.""",2024-03-19T17:59:52Z
Dated Data: Tracing Knowledge Cutoffs in Large Language Models,Yes.,5.,"""To understand the root cause of this observation, we conduct a direct large-scale analysis on open pre-training datasets. Our analysis reveals two reasons for these inconsistencies",2024-03-19T17:57:58Z
Bypassing LLM Watermarks with Color-Aware Substitutions,Yes.,2.,"""Existing attack methods fail to evade detection for longer text segments.""",2024-03-19T17:54:39Z
Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models,Yes.,1.,"""This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases.""",2024-03-19T17:43:08Z
Supporting Energy Policy Research with Large Language Models,Yes.,1.,"""This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape.""",2024-03-19T17:28:51Z
Semantic Layering in Room Segmentation via LLMs,Yes.,1.,"""By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation.""",2024-03-19T17:23:44Z
Yell At Your Robot: Improving On-the-Fly from Language Corrections,Yes.,2.,"""for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.""",2024-03-19T17:08:24Z
Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference,Yes.,1.,"""This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services.""",2024-03-19T16:53:53Z
HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning,Yes.,3.,"""Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors",2024-03-19T16:31:30Z
Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models,Yes.,3.,"""the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data;"" and ""current approaches have side-effects when improving agent abilities by introducing hallucinations.""",2024-03-19T16:26:10Z
Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation,Yes.,1.,"""large language model is utilized to explicitly aggregate the global graph features with local relationship features.""",2024-03-19T15:54:48Z
MELTing point: Mobile Evaluation of Language Transformers,Yes.,5.,"""Their runtime requirements have prevented them from being broadly deployed on mobile,"" ""LLM inference is largely memory-bound,"" ""Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost,"" ""the continuous execution of LLMs remains elusive, as both factors negatively affect user experience,"" and ""the ecosystem is still in its infancy, and algorithmic",2024-03-19T15:51:21Z
Contextual Moral Value Alignment Through Context-Based Aggregation,Yes.,1.,"""Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.""",2024-03-19T15:06:53Z
RelationVLM: Making Large Vision-Language Models Understand Visual Relations,Yes.,3.,"""current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data.""",2024-03-19T15:01:19Z
Investigating Text Shortening Strategy in BERT: Truncation vs Summarization,Yes.,3.,"""The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative.""",2024-03-19T15:01:14Z
Automated Data Curation for Robust Language Model Fine-Tuning,Yes.,3.,"""for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses.""",2024-03-19T14:44:45Z
Instructing Large Language Models to Identify and Ignore Irrelevant Conditions,Yes.,3.,"""Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.""",2024-03-19T14:07:28Z
Pragmatic Competence Evaluation of Large Language Models for Korean,Yes.,3.,"""Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.""",2024-03-19T12:21:20Z
Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code,Yes.,3.,"""Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).""",2024-03-19T10:53:40Z
LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models,Yes.,3.,"""However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture."" and ""Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types.""",2024-03-19T10:11:14Z
Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs,Yes.,1.,"""reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements.""",2024-03-19T10:03:07Z
AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework,Yes.,5.,"""LLMs still suffer from hallucinations and are unable to keep up with the latest information.""",2024-03-19T09:45:33Z
Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation,Yes.,3.,"""Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve.""",2024-03-19T09:00:23Z
AffineQuant: Affine Transformation Quantization for Large Language Models,Yes.,1.,"""Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights.""",2024-03-19T08:40:21Z
To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions,Yes.,1.,"""It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs).""",2024-03-19T08:09:44Z
UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All,Yes.,1.,"""UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts.""",2024-03-19T08:09:27Z
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,Yes.,4.,"""the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges.""",2024-03-19T07:25:02Z
"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",Yes.,4.,"""These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities."" and ""mitigation strategies to address these challenges while identifying limitations of current strategies.""",2024-03-19T07:10:58Z
DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM,Yes.,1.,"""We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini.""",2024-03-19T06:54:33Z
Embodied LLM Agents Learn to Cooperate in Organized Teams,Yes.,3.,"""However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation.""",2024-03-19T06:39:47Z
WoLF: Wide-scope Large Language Model Framework for CXR Understanding,Yes.,3.,"""existing CXR understanding frameworks still possess several procedural caveats... While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness.""",2024-03-19T06:39:23Z
CrossTune: Black-Box Few-Shot Classification with Label Enhancement,Yes.,2.,"""Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks."" and ""Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task",2024-03-19T05:52:56Z
VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation,,,,2024-03-19T03:55:39Z
Third-Party Language Model Performance Prediction from Instruction,Yes.,5.,"""such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task."" and ""Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations",2024-03-19T03:53:47Z
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales,Yes.,1.,"""we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.""",2024-03-19T03:22:35Z
Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering,Yes.,4.,"""LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.""",2024-03-19T03:00:03Z
Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models,Yes.,1.,"""Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret.""",2024-03-19T02:57:07Z
Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning,Yes.,3.,"""However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications.""",2024-03-19T02:34:33Z
RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners,Yes.,3.,"""even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes.""",2024-03-19T02:34:18Z
Advancing Time Series Classification with Multimodal Language Modeling,Yes.,1.,"""Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task.""",2024-03-19T02:32:24Z
Characteristic AI Agents via Large Language Models,Yes.,1.,"""The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems.""",2024-03-19T02:25:29Z
"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",Yes.,4.,"""many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues"" and ""Evaluation results indicate that while Chinese LLMs have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as commonsense reasoning, alignment, and safety.""",2024-03-18T23:21:37Z
Improving LoRA in Privacy-preserving Federated Learning,Yes.,3.,"""LoRA may become unstable due to the following facts",2024-03-18T23:20:08Z
Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach,Yes.,1.,"""Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns.""",2024-03-18T22:39:03Z
FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications,Yes.,3.,"""Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources.""",2024-03-18T22:11:00Z
Zero-Shot Multi-task Hallucination Detection,Yes.,4.,"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""",2024-03-18T20:50:26Z
Reference-based Metrics Disprove Themselves in Question Generation,Yes.,1.,"""utilizing large language models.""",2024-03-18T20:47:10Z
Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models,Yes.,1.,"""The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.""",2024-03-18T19:10:12Z
TnT-LLM: Text Mining at Scale with Large Language Models,Yes.,2.,"""We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.""",2024-03-18T18:45:28Z
EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models,Yes.,5.,"""Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs)."" and ""Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks.""",2024-03-18T18:39:53Z
Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets,Yes.,4.,"""Recent work has shown that false assumptions in naturally occurring questions pose challenges to current models, with low performance on both generative QA and simple detection tasks"" and ""Our findings from evaluating a range of large language models are threefold",2024-03-18T18:01:26Z
MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control,Yes.,1.,"""Specifically, MineDreamer is developed on top of recent advances in Multimodal Large Language Models (MLLMs) and diffusion models.""",2024-03-18T17:59:42Z
RouterBench: A Benchmark for Multi-LLM Routing System,Yes.,3.,"""no single model can optimally address all tasks and applications, particularly when balancing performance with cost."" and ""highlighting their potentials and limitations within our evaluation framework.""",2024-03-18T17:59:04Z
A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models,Yes.,4.,"""Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities"" and ""Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment",2024-03-18T17:56:37Z
Supervised Fine-Tuning as Inverse Reinforcement Learning,Yes.,3.,"""Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.""",2024-03-18T17:52:57Z
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents,Yes.,2.,"""Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.""",2024-03-18T17:51:16Z
NovelQA: A Benchmark for Long-Range Novel Question Answering,Yes.,5.,"""Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens.""",2024-03-18T17:32:32Z
Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching,Yes.,1.,"""In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs).""",2024-03-18T17:21:35Z
Towards Enabling FAIR Dataspaces Using Large Language Models,Yes.,1.,"""The advent of Large Language Models (LLMs) raises the question of how these models can support the adoption of FAIR dataspaces.""",2024-03-18T16:46:00Z
A Closer Look at Claim Decomposition,Yes.,3.,"""We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used.""",2024-03-18T16:03:45Z
Investigating Markers and Drivers of Gender Bias in Machine Translations,Yes.,4.,"""Implicit gender bias in Large Language Models (LLMs) is a well-documented problem,"" and ""These results show that the back-translation method can provide further insights into bias in language models.""",2024-03-18T15:54:46Z
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?,Yes.,2.,"""The major challenges identified are that most XIAI do not explore 'global' modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks.""",2024-03-18T15:53:33Z
QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction,Yes.,3.,"""we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered.""",2024-03-18T15:39:14Z
GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture,Yes.,2.,"""Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge.""",2024-03-18T15:08:01Z
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,Yes.,4.,"""Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues."" and ""challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training.""",2024-03-18T14:48:29Z
Agent3D-Zero: An Agent for Zero-shot 3D Understanding,Yes.,3.,"""Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data.""",2024-03-18T14:47:03Z
"SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",Yes.,1.,"""two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics.""",2024-03-18T14:45:20Z
Metaphor Understanding Challenge Dataset for LLMs,Yes.,3.,"""Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs.""",2024-03-18T14:08:59Z
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments,Yes.,3.,"""Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited.""",2024-03-18T14:04:47Z
"Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",Yes.,3.,"""relatively little is known about how well the long-context capability and performance of leading LLMs (e.g., GPT-4 Turbo and Kimi Chat).""",2024-03-18T14:01:45Z
Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus,Yes.,5.,"""Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity.""",2024-03-18T13:50:50Z
Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models,Yes.,2.,"""Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.""",2024-03-18T13:44:48Z
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs,Yes.,2.,"""However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest.""",2024-03-18T13:03:24Z
Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems,Yes.,1.,"""Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications.""",2024-03-18T13:02:02Z
LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images,Yes.,3.,"""we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy.""",2024-03-18T12:04:11Z
HDLdebugger: Streamlining HDL debugging with Large Language Models,Yes.,3.,"""Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results.""",2024-03-18T11:19:37Z
Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model,Yes.,2.,"""Traditional fine-tuning methods engage all parameters of LLMs, which is computationally expensive and may not be necessary.""",2024-03-18T09:55:01Z
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines,Yes.,1.,"""The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task.""",2024-03-18T08:58:47Z
Reinforcement Learning with Token-level Feedback for Controllable Text Generation,Yes.,3.,"""Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods).""",2024-03-18T08:18:37Z
LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning,Yes.,1.,"""we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning.""",2024-03-18T08:03:47Z
DEE: Dual-stage Explainable Evaluation Method for Text Generation,Yes.,2.,"""Recent advancements have sought to mitigate this limitation by incorporating large language models (LLMs) to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount.""",2024-03-18T06:30:41Z
Do CLIPs Always Generalize Better than ImageNet Models?,Yes.,4.,"""We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different",2024-03-18T06:04:02Z
VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding,Yes.,1.,"""We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos.""",2024-03-18T05:07:59Z
Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V,Yes.,2.,"""Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use.""",2024-03-18T04:41:38Z
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models,Yes.,2.,"""While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how 'offensive content' is conceptualized, and the resulting differences in how these datasets are labeled.""",2024-03-18T04:12:35Z
LLM Guided Evolution -- The Automation of Models Advancing Models,Yes.,1.,"""GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers.""",2024-03-18T03:44:55Z
StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation,Yes.,3.,"""However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks.""",2024-03-18T03:26:18Z
InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions,Yes.,3.,"""LLMs necessitate continual task-specific adaptation without catastrophic forgetting"" and ""traditional replay-based methods do not fully utilize instructions to customize the replay strategy.""",2024-03-18T03:10:36Z
A Novel Paradigm Boosting Translation Capabilities of Large Language Models,Yes.,1.,"""Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited.""",2024-03-18T02:53:49Z
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression,Yes.,5.,"""the potential risks of compression in terms of safety and trustworthiness have been largely neglected"" and ""our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns.""",2024-03-18T01:38:19Z
Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning,Yes.,1.,"""This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs).""",2024-03-18T01:18:48Z
X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment,Yes.,3.,"""However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity.""",2024-03-18T01:14:47Z
"Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot",Yes.,3.,"""Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures.""",2024-03-18T00:13:43Z
What Makes Math Word Problems Challenging for LLMs?,Yes.,5.,"""This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs).""",2024-03-17T23:18:40Z
JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning,Yes.,3.,"""The scaling of Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval Augmented Generation (RAG), faces significant memory constraints, especially when fine-tuning extensive prompt sequences.""",2024-03-17T23:02:04Z
ConvSDG: Session Data Generation for Conversational Search,Yes.,1.,"""Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation.""",2024-03-17T20:34:40Z
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback,Yes.,1.,"""We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals.""",2024-03-17T20:21:26Z
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows,Yes.,1.,"""It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and external environments.""",2024-03-17T19:54:16Z
Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches,Yes.,2.,"""Our findings indicate that for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to the LLM embedding space does not guarantee improved performance over using image captions.""",2024-03-17T19:44:05Z
Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts,Yes.,5.,"""a transformer model may easily learn spurious patterns in the data, short-circuiting actual reasoning"" and ""we then identify a few remaining reasoning errors, not previously described in the literature, arising from using a pre-trained language model.""",2024-03-17T19:32:12Z
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant,Yes.,3.,"""bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck.""",2024-03-17T18:42:38Z
Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,Yes.,5.,"""Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps.""",2024-03-17T17:01:45Z
Cheap Ways of Extracting Clinical Markers from Texts,Yes.,1.,"""Our work focuses on evaluating Large Language Models (LLM) as opposed to an alternative method that is much more memory and resource efficient.""",2024-03-17T14:21:42Z
Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework,Yes.,3.,"""However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers.""",2024-03-17T13:01:03Z
Correcting misinformation on social media with a large language model,Yes.,5.,"""LLMs also have versatile capabilities that could accelerate misinformation correction--however, they struggle due to a lack of recent information, a tendency to produce false content, and limitations in addressing multimodal information.""",2024-03-17T10:59:09Z
Evaluation Ethics of LLMs in Legal Domain,Yes.,3.,"""However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny."" and ""The incorporation of legal ethics into the model has been overlooked by researchers.""",2024-03-17T09:05:13Z
Training A Small Emotional Vision Language Model for Visual Art Comprehension,Yes.,3.,"""While small models are computationally efficient, their capacity is much limited compared with large models.""",2024-03-17T09:01:02Z
Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering,Yes.,3.,"""these methods are prone to the errors of sequential generation due to multiple events in a document.""",2024-03-17T07:41:58Z
Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities,Yes.,3.,"""Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases.""",2024-03-17T07:34:12Z
Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment,Yes.,2.,"""Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback.""",2024-03-17T07:08:55Z
