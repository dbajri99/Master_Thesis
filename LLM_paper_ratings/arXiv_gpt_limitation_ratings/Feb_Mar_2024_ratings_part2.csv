Title,Talks about LLMs,Rate,Evidence,Published
PhD: A Prompted Visual Hallucination Evaluation Dataset,Yes.,4.,"""The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs."" and ""Extensive experiments on five SOTA LVLMs reveal their inability to effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IV",2024-03-17T06:53:44Z
ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models,Yes.,4.,"""Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER).""",2024-03-17T06:12:43Z
m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks,Yes.,3.,"""the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions.""",2024-03-17T04:36:18Z
GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment,Yes.,5.,"""Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in the social and physical context.""",2024-03-17T03:52:52Z
Large Language Models Powered Context-aware Motion Prediction,Yes.,2.,"""considering the cost associated with LLMs, we propose a cost-effective deployment strategy.""",2024-03-17T02:06:49Z
Pre-Trained Language Models Represent Some Geographic Populations Better Than Others,Yes.,4.,"""Results show that these models perform much better for some populations than others. In particular, populations across the US and the UK are represented quite well while those in South and Southeast Asia are poorly represented.""",2024-03-16T22:01:39Z
SelfIE: Self-Interpretation of Large Language Model Embeddings,Yes.,3.,"""The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments.""",2024-03-16T15:30:34Z
Human Centered AI for Indian Legal Text Analytics,Yes.,3.,"""Recent boom in generative AI has not translated to proportionate rise in impactful legal applications, because of low trustworthiness and the scarcity of specialized datasets for training Large Language Models (LLMs).""",2024-03-16T15:17:13Z
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations,Yes.,5.,"""Notably, large language models exhibit a significant performance gap compared to humans, highlighting the limitations of machine learning methods in the cognitive intent understanding task.""",2024-03-16T15:14:15Z
Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization,No.,1.,The abstract does not mention LLMs or their limitations.,2024-03-16T11:09:27Z
Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean,Yes.,2.,"""their expansion requires significant computing resources"" and ""overlooking less-resourced languages (LRLs).""",2024-03-16T10:26:38Z
A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment,Yes.,5.,"""Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.""",2024-03-16T08:30:45Z
Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning,,,,2024-03-16T07:18:29Z
Do Large Language Models understand Medical Codes?,Yes.,5.,"""However, these models are also prone to producing 'hallucinations' or incorrect responses when faced with queries they cannot adequately address,"" and ""Our results indicate that these models as they currently stand do not comprehend the meaning of the medical codes, highlighting the need for better representation of these alphanumeric codes extensively used in healthcare.""",2024-03-16T06:18:15Z
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion,Yes.,3.,"""Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices.""",2024-03-16T04:12:50Z
From Words to Routes: Applying Large Language Models to Vehicle Routing,Yes.,3.,"""We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency."" and ""our proposed framework achieves a 16% increase in feasibility, a 7",2024-03-16T03:54:38Z
LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices,Yes.,3.,"""we experiment and microbenchmark various LLMs' performance in tasks along CaiTI's conversation flow and discuss their strengths and weaknesses.""",2024-03-16T02:48:50Z
Detecting Bias in Large Language Models: Fine-tuned KcBERT,Yes.,4.,"""they have the potential to generate subjective and normative language, leading to discriminatory treatment or outcomes among social groups, especially due to online offensive language.""",2024-03-16T02:27:19Z
Depression Detection on Social Media with Large Language Models,Yes.,1.,"""combining medical knowledge and the recent advances in large language models (LLMs)""",2024-03-16T01:01:16Z
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns,Yes.,1.,"""To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs).""",2024-03-15T21:54:00Z
PERL: Parameter Efficient Reinforcement Learning from Human Feedback,Yes.,3.,"""training models with RLHF is computationally expensive, and an overall complex process"" and ""reducing the computational burden that limits its adoption as an alignment technique for Large Language Models.""",2024-03-15T21:43:46Z
Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery,Yes.,3.,"""Modern LLMs / DNNs are, by contrast, opaque to us in significant ways, and this creates obstacles in obtaining mathematical knowledge from them.""",2024-03-15T21:38:26Z
Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems,Yes.,3.,"""This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance.""",2024-03-15T18:00:00Z
Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?,,,,2024-03-15T17:12:57Z
"S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document",Yes.,1.,"""The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes.""",2024-03-15T17:04:27Z
Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases,Yes.,5.,"""Addressing the challenge of LLM hallucinations,"" and ""The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets.""",2024-03-15T16:30:14Z
Using an LLM to Turn Sign Spottings into Spoken Language Sentences,Yes.,1.,"""In this paper, we introduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and a pretrained large language model to improve SLT performance.""",2024-03-15T16:14:34Z
SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores,Yes.,2.,"""Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address.""",2024-03-15T15:43:02Z
TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale,Yes.,3.,"""However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings.""",2024-03-15T14:36:38Z
CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model,Yes.,1.,"""In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation.""",2024-03-15T14:14:26Z
Uni-SMART: Universal Science Multimodal Analysis and Research Transformer,Yes.,4.,"""existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze.""",2024-03-15T13:43:47Z
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction,,,,2024-03-15T13:25:09Z
A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption,Yes.,5.,"""The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity."" and ""We highlight that, in a typical case study where word-level univariate explanations are analyzed with",2024-03-15T13:15:23Z
Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models,Yes.,4.,"""they are mostly English-centric due to the imbalanced training corpora"" and ""even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios.""",2024-03-15T12:47:39Z
HawkEye: Training Video-Text LLMs for Grounding Text in Videos,Yes.,5.,"""they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images.""",2024-03-15T11:58:18Z
Read between the lines -- Functionality Extraction From READMEs,Yes.,2.,"""making existing text2text generation systems not very useful"" and ""small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard.""",2024-03-15T11:11:57Z
Generative Region-Language Pretraining for Open-Ended Object Detection,Yes.,1.,"""we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names.""",2024-03-15T10:52:39Z
ChatPattern: Layout Pattern Customization via Natural Language,Yes.,1.,"""In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization.""",2024-03-15T09:15:22Z
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning,Yes.,1.,"""we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs).""",2024-03-15T08:51:15Z
Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF,Yes.,3.,"""These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts.""",2024-03-15T08:03:49Z
Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties,Yes.,1.,"""This paper proposes Large Language Models (LLMs) to generate test programs.""",2024-03-15T08:01:02Z
CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner,Yes.,1.,"""Most existing one-shot skeleton-based action recognition focuses on raw low-level information (e.g., joint location), and may suffer from local information loss and low generalization ability. To alleviate these, we propose to leverage text description generated from large language models (LL",2024-03-15T07:51:35Z
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models,Yes.,3.,"""current dynamic RAG methods fall short in both aspects"" and ""the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context.""",2024-03-15T07:45:37Z
Are LLMs Good Cryptic Crossword Solvers?,Yes.,5.,"""showing that their performance on this task is still far from that of humans.""",2024-03-15T06:57:08Z
Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning,Yes.,3.,"""the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded.""",2024-03-15T06:54:20Z
TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model,Yes.,1.,"""Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of PLMs, even Large Language Models (LLMs).""",2024-03-15T06:38:25Z
Knowledge Condensation and Reasoning for Knowledge-based VQA,Yes.,3.,"""However, these retrieved knowledge passages often contain irrelevant or noisy information, which limits the performance of the model.""",2024-03-15T06:06:06Z
Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain,Yes.,2.,"""cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies.""",2024-03-15T05:35:02Z
Lost in Overlap: Exploring Watermark Collision in LLMs,Yes.,5.,"""However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing.""",2024-03-15T05:06:21Z
Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals,Yes.,5.,"""One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes."" and ""The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the",2024-03-15T04:04:45Z
Whose Side Are You On? Investigating the Political Stance of Large Language Models,Yes.,4.,"""it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias.""",2024-03-15T04:02:24Z
Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers,Yes.,3.,"""Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM.""",2024-03-15T02:38:26Z
ViTCN: Vision Transformer Contrastive Network For Reasoning,Yes.,2.,"""However, abstract reasoning remains a challenge for these models, Can AI really thinking like a human? still be a question yet to be answered.""",2024-03-15T02:01:14Z
"Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals",Yes.,1.,"""We present a novel approach to tackle the ObjectNav task for non-stationary and potentially occluded targets in an indoor environment. We refer to this task Portable ObjectNav (or P-ObjectNav), and in this work, present its formulation, feasibility, and a navigation benchmark using a",2024-03-14T22:33:22Z
Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,Yes.,5.,"""Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways."" and ""we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon.""",2024-03-14T19:39:10Z
Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention,Yes.,5.,"""We find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models.""",2024-03-14T18:27:43Z
Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models,Yes.,5.,"""we conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs.""",2024-03-14T18:24:55Z
Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference,,,,2024-03-14T17:59:26Z
3D-VLA: A 3D Vision-Language-Action Generative World Model,Yes.,1.,"""Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM).""",2024-03-14T17:58:41Z
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking,Yes.,3.,"""We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens.""",2024-03-14T17:58:16Z
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey,Yes.,4.,"""addressing fairness and safety issues in LLMs"" and ""understanding and improving the LLMs' reasoning capacity.""",2024-03-14T17:47:20Z
"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",Yes.,4.,"""MLLMs... are also more vulnerable to jailbreak attacks than their LLM predecessors,"" and ""safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features.""",2024-03-14T17:03:04Z
Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models,Yes.,1.,"""a component in charge of generating natural language explanations by harnessing the capabilities of Large Language Models (LLMs) over the data contained within the previously mentioned black box.""",2024-03-14T16:57:18Z
Less is More: Data Value Estimation for Visual Instruction Tuning,Yes.,2.,"""However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy.""",2024-03-14T16:47:25Z
Logits of API-Protected LLMs Leak Proprietary Information,Yes.,5.,"""most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.""",2024-03-14T16:27:49Z
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding,Yes.,2.,"""However, the mismatching between the algorithms with the problem could lead to undesired results.""",2024-03-14T16:13:00Z
MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation,Yes.,3.,"""Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency.""",2024-03-14T16:07:39Z
AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,Yes.,4.,"""with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks""",2024-03-14T15:57:13Z
LLM-based agents for automating the enhancement of user story quality: An early report,Yes.,1.,"""This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams.""",2024-03-14T14:35:53Z
"""Like a Nesting Doll"": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",Yes.,1.,"""We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand.""",2024-03-14T14:01:26Z
GiT: Towards Generalist Vision Transformer through Universal Language Interface,Yes.,1.,"""Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs),""",2024-03-14T13:47:41Z
Komodo: A Linguistic Expedition into Indonesia's Regional Languages,Yes.,1.,"""The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English.""",2024-03-14T13:12:21Z
BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences,Yes.,3.,"""the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences.""",2024-03-14T12:51:58Z
AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions,Yes.,4.,"""These instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks."" and ""Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V.""",2024-03-14T12:51:07Z
Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring,Yes.,4.,"""the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios"" and ""Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc.""",2024-03-14T12:21:37Z
What Was Your Prompt? A Remote Keylogging Attack on AI Assistants,Yes.,2.,"""we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences,"" and ""we were able to accurately reconstruct 29% of an AI assistant's responses and successfully infer the topic from 55% of them.""",2024-03-14T09:38:12Z
Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse,Yes.,3.,"""A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities.""",2024-03-14T08:27:32Z
Caveat Lector: Large Language Models in Legal Practice,Yes.,5.,"""Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire",2024-03-14T08:19:41Z
Unveiling the Generalization Power of Fine-Tuned Large Language Models,Yes.,3.,"""the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.""",2024-03-14T08:18:59Z
Evaluating LLMs for Gender Disparities in Notable Persons,Yes.,4.,"""addressing concerns over their propensity to produce factually incorrect 'hallucinated' responses or to altogether decline to even answer prompt at all"" and ""investigates the presence of gender-based biases in LLMs' responses to factual inquiries.""",2024-03-14T07:58:27Z
USimAgent: Large Language Models for Simulating Search Users,Yes.,1.,"""Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks.""",2024-03-14T07:40:54Z
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text,Yes.,2.,"""Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored.""",2024-03-14T06:49:16Z
Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models,Yes.,3.,"""However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment.""",2024-03-14T05:34:35Z
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,Yes.,5.,"""Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities.""",2024-03-14T04:06:13Z
Large Language Models are Parallel Multilingual Learners,Yes.,1.,The abstract discusses enhancing comprehension abilities of multilingual LLMs using Parallel Input in Multiple Languages (PiM) but does not mention any explicit limitations of the models.,2024-03-14T03:33:46Z
UniCode: Learning a Unified Codebook for Multimodal Large Language Models,Yes.,3.,"""This innovation addresses a critical limitation in existing MLLMs",2024-03-14T03:29:58Z
Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate,Yes.,3.,"""Two primary barriers impede the straightforward application of LLMs to circuits",2024-03-14T03:24:14Z
LAMP: A Language Model on the Map,Yes.,3.,"""nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them.""",2024-03-14T02:56:38Z
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference,Yes.,3.,"""This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.""",2024-03-14T02:42:42Z
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences,Yes.,2.,"""By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment.""",2024-03-14T01:51:35Z
ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning,Yes.,1.,"""We then present two distinct systems for instruction tuning on such datasets",2024-03-14T01:40:23Z
VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework,Yes.,1.,"""With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question.""",2024-03-14T01:39:40Z
AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic,Yes.,3.,"""there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic.""",2024-03-14T00:45:24Z
Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors,Yes.,5.,"""LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints.""",2024-03-14T00:35:39Z
Evaluating the Application of Large Language Models to Generate Feedback in Programming Education,Yes.,3.,"""However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.""",2024-03-13T23:14:35Z
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents,Yes.,5.,"""The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge.""",2024-03-13T22:06:03Z
The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions,Yes.,5.,"""However, these models are susceptible to errors - 'hallucinations' and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.""",2024-03-13T21:39:39Z
Exploring Prompt Engineering Practices in the Enterprise,Yes.,3.,"""Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal.""",2024-03-13T20:32:32Z
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era,Yes.,3.,"""First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities."" and ""We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges.""",2024-03-13T20:25:27Z
LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots,Yes.,1.,"""This paper aims to address this issue by proposing the LMStyle Benchmark, a novel evaluation framework applicable to chat-style text style transfer (C-TST), that can measure the quality of style transfer for LLMs in an automated and scalable manner.""",2024-03-13T20:19:30Z
Bugs in Large Language Models Generated Code: An Empirical Study,Yes.,5.,"""Similar to human-written code, LLM-generated code is prone to bugs,"" and ""examines a sample of 333 bugs collected from code generated using three leading LLMs"" and ""identifies the following 10 distinctive bug patterns.""",2024-03-13T20:12:01Z
Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models,Yes.,3.,"""various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored.""",2024-03-13T18:57:30Z
Teaching Machines to Code: Smart Contract Translation with LLMs,Yes.,1.,"""Despite the advancements in utilizing LLMs for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the LLM, remains largely unexplored.""",2024-03-13T18:55:20Z
Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics,Yes.,3.,"""We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors.""",2024-03-13T18:47:00Z
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation,Yes.,3.,"""Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.""",2024-03-13T18:16:21Z
Cultural evolution in populations of Large Language Models,Yes.,1.,"""We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap.""",2024-03-13T18:11:17Z
DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation,Yes.,3.,"""However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper.""",2024-03-13T18:00:01Z
Simple and Scalable Strategies to Continually Pre-train Large Language Models,Yes.,3.,"""the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data.""",2024-03-13T17:58:57Z
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,Yes.,4.,"""Large language models (LLMs) can easily generate biased and discriminative responses."" and ""it is of crucial importance to develop strategies to mitigate these biases.""",2024-03-13T17:46:28Z
Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,Yes.,5.,"""they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.""",2024-03-13T17:29:45Z
SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents,Yes.,3.,"""We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence",2024-03-13T17:17:48Z
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,Yes.,3.,"""The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm.""",2024-03-13T16:57:57Z
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records,Yes.,1.,"""The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge.""",2024-03-13T16:17:09Z
MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models,Yes.,3.,"""Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital.""",2024-03-13T15:20:30Z
DevBench: A Comprehensive Benchmark for Software Development,Yes.,5.,"""Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.""",2024-03-13T15:13:44Z
Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments,Yes.,1.,"""We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments.""",2024-03-13T14:59:07Z
Non-discrimination Criteria for Generative Language Models,Yes.,4.,"""concerns arise about perpetuating and amplifying harmful biases in applications"" and ""this paper studies how to uncover and quantify the presence of gender biases in generative language models.""",2024-03-13T14:19:08Z
Language models scale reliably with over-training and on downstream tasks,Yes.,1.,"""Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated.""",2024-03-13T13:54:00Z
Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator,Yes.,3.,"""Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks.""",2024-03-13T13:04:58Z
Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking,Yes.,3.,"""limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications.""",2024-03-13T12:55:43Z
Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning,Yes.,3.,"""In view of the huge number of parameters of Large language models (LLMs), tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible.""",2024-03-13T12:50:23Z
SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,Yes.,4.,"""We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies.""",2024-03-13T12:46:51Z
Software Vulnerability and Functionality Assessment using LLMs,Yes.,1.,"""In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews.""",2024-03-13T11:29:13Z
Tastle: Distract Large Language Models for Automatic Jailbreak Attack,Yes.,5.,"""even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors"" and ""highlight the crucial need to develop more effective and practical defense strategies.""",2024-03-13T11:16:43Z
Do Large Language Models Solve ARC Visual Analogies Like People Do?,Yes.,5.,"""Results show that both children and adults outperform most LLMs on these tasks."" and ""Error analysis revealed a similar 'fallback' solution strategy in LLMs and young children, where part of the analogy is simply copied."" and ""On the whole, 'concept' errors were more common in humans, and 'matrix' errors were more common in LLMs.""",2024-03-13T09:48:13Z
SMART: Submodular Data Mixture Strategy for Instruction Tuning,Yes.,1.,"""Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks.""",2024-03-13T09:31:50Z
CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model,Yes.,5.,"""Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated."" and ""Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and",2024-03-13T08:54:31Z
From human experts to machines: An LLM supported approach to ontology and knowledge graph construction,Yes.,2.,"""Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs.""",2024-03-13T08:50:15Z
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments,Yes.,1.,"""This work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties.""",2024-03-13T08:41:55Z
Knowledge Conflicts for LLMs: A Survey,Yes.,5.,"""highlighting the complex challenges they encounter when blending contextual and parametric knowledge"" and ""These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common.""",2024-03-13T08:02:23Z
OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models,Yes.,2.,"""To achieve the above goal, there are three challenges",2024-03-13T07:52:31Z
Is Context Helpful for Chat Translation Evaluation?,Yes.,1.,"""Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.""",2024-03-13T07:49:50Z
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses,,,,2024-03-13T07:44:14Z
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,Yes.,3.,"""highlighting practical limitations of basic RLAIF"" and ""the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness.""",2024-03-13T07:38:20Z
Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform,Yes.,3.,"""Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models."" and ""Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated.""",2024-03-13T07:31:20Z
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale,Yes.,1.,"""GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training.""",2024-03-13T06:54:47Z
CleanAgent: Automating Data Standardization with LLM-based Agents,Yes.,3.,"""Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement.""",2024-03-13T06:54:15Z
Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation,Yes.,1.,"""To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete navigation tasks.""",2024-03-13T06:22:17Z
"Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",Yes.,3.,"""Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously.""",2024-03-13T06:18:48Z
RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education,Yes.,1.,"""We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks.""",2024-03-13T05:51:57Z
Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification,Yes.,3.,"""Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features.""",2024-03-13T05:48:58Z
A Moral Imperative: The Need for Continual Superalignment of Large Language Models,Yes.,5.,"""achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios"" and ""highlighting the discrepancies between static AI models and the dynamic nature of human societies"" and ""LLMs, constrained by their training data, fail to align with contemporary human values and scenarios.""",2024-03-13T05:44:50Z
"TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",Yes.,3.,"""Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities."" and ""To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework.""",2024-03-13T05:22:39Z
Boosting Disfluency Detection with Large Language Model as Disfluency Generator,Yes.,1.,"""We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM.""",2024-03-13T04:14:33Z
Learning to Watermark LLM-generated Text via Reinforcement Learning,Yes.,1.,"""We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse.""",2024-03-13T03:43:39Z
Can Large Language Models Identify Authorship?,Yes.,3.,"""These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability.""",2024-03-13T03:22:02Z
Large Language Models are Contrastive Reasoners,Yes.,1.,"""Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs).""",2024-03-13T03:15:05Z
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models,Yes.,1.,"""This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology.""",2024-03-13T02:53:36Z
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency,Yes.,3.,"""Nevertheless, they face challenges when dealing with verbose database information and complex user intentions.""",2024-03-13T02:32:41Z
MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension,,,,2024-03-13T02:26:16Z
TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,Yes.,3.,"""Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases.""",2024-03-12T22:33:02Z
Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems,Yes.,3.,"""connecting FN models with RAG can cause a decrease in performance"" and ""This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.""",2024-03-12T21:06:31Z
CHAI: Clustered Head Attention for Efficient LLM Inference,Yes.,2.,"""However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory.""",2024-03-12T20:10:04Z
Generating Clarification Questions for Disambiguating Contracts,No.,1.,The abstract does not mention LLMs or their limitations.,2024-03-12T19:57:39Z
Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models,Yes.,5.,"""LLMs have known biases, commonly derived from their training data."" and ""we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented.""",2024-03-12T19:40:18Z
Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection,Yes.,3.,"""Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content.""",2024-03-12T19:12:28Z
LG-Traj: LLM Guided Pedestrian Trajectory Prediction,Yes.,1.,"""This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues.""",2024-03-12T19:06:23Z
Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM,Yes.,1.,"""In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences.""",2024-03-12T18:19:47Z
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging,Yes.,5.,"""Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.""",2024-03-12T18:12:02Z
Beyond Text: Frozen Large Language Models in Visual Signal Comprehension,Yes.,1.,"""In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multi-modal datasets.""",2024-03-12T17:59:51Z
Rethinking Generative Large Language Model Evaluation for Semantic Comprehension,Yes.,3.,"""we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios.""",2024-03-12T17:59:48Z
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,Yes.,2.,"""However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities.""",2024-03-12T17:58:04Z
Exploring Safety Generalization Challenges of Large Language Models via Code,Yes.,5.,"""Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input",2024-03-12T17:55:38Z
The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,Yes.,4.,"""the ripple effect in the hidden space is a significant issue in all current model editing methods.""",2024-03-12T17:04:28Z
Beyond Memorization: The Challenge of Random Memory Access in Language Models,Yes.,5.,"""we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.""",2024-03-12T16:42:44Z
Fine-tuning Large Language Models with Sequential Instructions,Yes.,5.,"""Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks.""",2024-03-12T16:33:30Z
Duwak: Dual Watermarks in Large Language Models,Yes.,1.,"""As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms.""",2024-03-12T16:25:38Z
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations,Yes.,3.,"""Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving.""",2024-03-12T15:56:10Z
Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings,Yes.,1.,"""We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training.""",2024-03-12T15:36:42Z
FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models,,,,2024-03-12T15:32:39Z
Multi-modal Auto-regressive Modeling via Visual Words,Yes.,3.,"""there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification.""",2024-03-12T14:58:52Z
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?,Yes.,3.,"""Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs.""",2024-03-12T14:58:45Z
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models,Yes.,2.,"""Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status.""",2024-03-12T14:57:40Z
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,Yes.,1.,"""In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation.""",2024-03-12T14:56:34Z
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,Yes.,3.,"""existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile.""",2024-03-12T14:51:57Z
"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",Yes.,3.,"""However, data augmentation based on large language models faces two disadvantages",2024-03-12T14:37:03Z
Characterization of Large Language Model Development in the Datacenter,Yes.,4.,"""However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.""",2024-03-12T13:31:14Z
Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework,Yes.,1.,"""This is achieved by consulting a large language model and medical experts.""",2024-03-12T13:18:22Z
generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation,Yes.,3.,"""the considered output candidates of the underlying search algorithm are under-explored and under-explained.""",2024-03-12T13:09:15Z
Couler: Unified Machine Learning Workflow Optimization in Cloud,Yes.,1.,"""We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines.""",2024-03-12T12:47:32Z
LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,Yes.,3.,"""Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance."" and ""we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even",2024-03-12T12:10:18Z
SIFiD: Reassess Summary Factual Inconsistency Detection with LLM,Yes.,5.,"""However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology.""",2024-03-12T11:41:51Z
Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts,Yes.,5.,"""Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge augmentation tools, thereby producing hallucinations.""",2024-03-12T11:40:44Z
MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki,Yes.,5.,"""NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.""",2024-03-12T11:32:30Z
MoAI: Mixture of All Intelligence for Large Language and Vision Models,Yes.,3.,"""current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR).""",2024-03-12T10:44:13Z
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models,Yes.,1.,"""Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due",2024-03-12T07:45:33Z
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,Yes.,5.,"""state-of-the-art SVD-based LLM compression methods have two key limitations",2024-03-12T07:31:18Z
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning,Yes.,3.,"""their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus.""",2024-03-12T07:27:02Z
Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking,Yes.,2.,"""these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues"" and ""no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience.""",2024-03-12T07:17:01Z
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation,Yes.,3.,"""Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data.""",2024-03-12T04:04:38Z
A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism,Yes.,2.,"""there are still concerns regarding cost and trade-offs between privacy issues and accuracy.""",2024-03-12T03:30:04Z
CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation,Yes.,2.,"""However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information.""",2024-03-12T02:37:11Z
AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production,Yes.,1.,"""optimizing the LLM prompts and utilities usage.""",2024-03-12T02:30:50Z
Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits,Yes.,2.,"""Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs.""",2024-03-11T23:52:46Z
Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions,Yes.,1.,"""First, an LLM is utilized initially to obtain a plausible answer for the instruction or question.""",2024-03-11T22:46:46Z
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews,Yes.,2.,"""We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review.""",2024-03-11T21:51:39Z
Narrating Causal Graphs with Large Language Models,Yes.,3.,"""Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings.""",2024-03-11T19:19:59Z
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation,Yes.,5.,"""LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.""",2024-03-11T18:26:02Z
MRL Parsing Without Tears: The Case of Hebrew,No.,1.,"""Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking.""",2024-03-11T17:54:33Z
Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena,Yes.,5.,"""We find that all models struggle with understanding the motion component that the CMC adds to a sentence.""",2024-03-11T17:47:47Z
SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees,Yes.,3.,"""the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance.""",2024-03-11T17:45:47Z
SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data,Yes.,2.,"""SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills"" and ""Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect",2024-03-11T17:35:33Z
"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",Yes.,3.,"""Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level reasoning such as assigning quantifiers.""",2024-03-11T17:20:12Z
ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis,Yes.,3.,"""However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning.""",2024-03-11T17:18:53Z
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning,Yes.,3.,"""Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism."" and ""Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance.""",2024-03-11T17:03:04Z
Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents,Yes.,1.,"""We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts.""",2024-03-11T16:24:08Z
Development of a Reliable and Accessible Caregiving Language Model (CaLM),Yes.,1.,"""Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care.""",2024-03-11T16:12:34Z
DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation,Yes.,1.,"""DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos.""",2024-03-11T16:03:35Z
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,Yes.,5.,"""However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection.""",2024-03-11T15:48:56Z
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework,Yes.,3.,"""This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations.""",2024-03-11T15:48:43Z
ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model,Yes.,1.,"""However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions).""",2024-03-11T14:35:45Z
ALaRM: Align Language Models via Hierarchical Rewards Modeling,Yes.,3.,"""The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals.""",2024-03-11T14:28:40Z
ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation,Yes.,4.,"""However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.""",2024-03-11T14:10:57Z
Real-Time Multimodal Cognitive Assistant for Emergency Medical Services,Yes.,1.,"""augmented with synthetic data generated by large language models (LLMs).""",2024-03-11T13:56:57Z
Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning,Yes.,1.,"""This paper introduces a novel RRG method, LM-RRG, that integrates large models (LMs) with clinical quality reinforcement learning to generate accurate and comprehensive chest X-ray radiology reports.""",2024-03-11T13:47:11Z
Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code,Yes.,4.,"""since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.""",2024-03-11T12:47:04Z
Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,Yes.,2.,"""substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity"" and ""storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories.""",2024-03-11T12:32:14Z
FashionReGen: LLM-Empowered Fashion Report Generation,Yes.,1.,"""In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based on the advanced Large Language Models (LLMs), debbed as GPT-FAR.""",2024-03-11T12:29:35Z
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement,Yes.,2.,"""generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification.""",2024-03-11T12:28:55Z
Elephants Never Forget: Testing Language Models for Memorization of Tabular Data,Yes.,5.,"""the critical issues of data contamination and memorization are often glossed over,"" ""Our investigation reveals that LLMs are pre-trained on many popular tabular datasets,"" ""This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to",2024-03-11T12:07:13Z
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation,Yes.,3.,"""However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination.""",2024-03-11T12:04:20Z
MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding,Yes.,4.,"""This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.""",2024-03-11T10:57:45Z
Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds,Yes.,4.,"""However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.""",2024-03-11T10:53:20Z
Academically intelligent LLMs are not necessarily socially intelligent,Yes.,5.,"""The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors.""",2024-03-11T10:35:53Z
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models,Yes.,1.,"""Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities.""",2024-03-11T10:32:23Z
AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models,Yes.,2.,"""Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancient text comprehension. By highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research.""",2024-03-11T10:24:37Z
Unraveling the Mystery of Scaling Laws: Part I,Yes.,3.,"""However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters."" and ""they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for",2024-03-11T10:05:29Z
From English to ASIC: Hardware Implementation with Large Language Model,Yes.,5.,"""challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities",2024-03-11T09:57:16Z
On the Consideration of AI Openness: Can Good Intent Be Abused?,Yes.,3.,"""We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities.""",2024-03-11T09:24:06Z
QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning,No.,1.,"The paper discusses transformer-based models in general and focuses on model quantization and fine-tuning methods, but does not specifically address LLMs or their limitations.",2024-03-11T08:09:30Z
Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach,Yes.,1.,"""We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation.""",2024-03-11T07:48:35Z
Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models,Yes.,5.,"""Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs.""",2024-03-11T05:51:03Z
CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation,Yes.,3.,"""However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset.""",2024-03-11T05:49:34Z
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models,Yes.,1.,"""In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations.""",2024-03-11T04:13:26Z
Evolving Knowledge Distillation with Large Language Models and Active Learning,Yes.,3.,"""Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high.""",2024-03-11T03:55:24Z
CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean,Yes.,4.,"""Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge."" and ""Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension.""",2024-03-11T03:54:33Z
What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation,Yes.,3.,"""Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization.""",2024-03-11T03:42:51Z
A Knowledge-Injected Curriculum Pretraining Framework for Question Answering,Yes.,3.,"""However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning.""",2024-03-11T03:42:03Z
Can LLMs' Tuning Methods Work in Medical Multimodal Domain?,Yes.,2.,"""Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization.""",2024-03-11T03:38:48Z
DivCon: Divide and Conquer for Progressive Text-to-Image Generation,Yes.,2.,"""However, these methods still struggle with generating images from textural prompts with multiple objects and complicated spatial relationships.""",2024-03-11T03:24:44Z
Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages,Yes.,3.,"""Despite their success, LLMs often struggle to perform well on low-resource languages because there is so little training data available. This shortcoming is especially prevalent with open source models.""",2024-03-11T01:04:36Z
From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification,Yes.,1.,"""User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints.""",2024-03-10T22:14:54Z
LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition,Yes.,3.,"""We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities.""",2024-03-10T20:20:16Z
ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes,Yes.,5.,"""Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust.""",2024-03-10T19:47:00Z
Editing Conceptual Knowledge for Large Language Models,Yes.,4.,"""The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance.""",2024-03-10T16:57:10Z
TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision,Yes.,3.,"""However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context.""",2024-03-10T13:58:38Z
Personalized LoRA for Human-Centered Text Understanding,Yes.,1.,"""Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics.""",2024-03-10T13:04:54Z
Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models,Yes.,2.,"""Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities.""",2024-03-10T12:43:27Z
Can Large Language Models Automatically Score Proficiency of Written Essays?,Yes.,2.,"""Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students.""",2024-03-10T09:39:00Z
Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity,Yes.,1.,"""Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles.""",2024-03-10T08:59:04Z
FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning,Yes.,2.,"""Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks.""",2024-03-10T08:41:22Z
Low-dose CT Denoising with Language-engaged Dual-space Alignment,Yes.,1.,"""While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models.""",2024-03-10T08:21:50Z
FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language,Yes.,1.,"""we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal",2024-03-10T07:21:31Z
Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning,No.,1.,The abstract does not mention LLMs or any limitations related to them.,2024-03-10T06:30:54Z
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery,Yes.,1.,"""The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and Large Language Model annotation.""",2024-03-10T05:12:16Z
RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion,,,,2024-03-10T05:10:34Z
Reframe Anything: LLM Agent for Open World Video Reframing,Yes.,1.,"""The advent of powerful large language models (LLMs) open new avenues for AI capabilities.""",2024-03-10T03:29:56Z
A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation,Yes.,1.,"""Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs).""",2024-03-09T23:22:56Z
Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages,Yes.,2.,"""Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind.""",2024-03-09T21:36:13Z
"Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",Yes.,4.,"""Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model.""",2024-03-09T21:07:16Z
Calibrating Large Language Models Using Their Generations Only,Yes.,3.,"""finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge.""",2024-03-09T17:46:24Z
Thread Detection and Response Generation using Transformers with Prompt Optimisation,Yes.,1.,"""Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM).""",2024-03-09T14:50:20Z
GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing,Yes.,3.,"""We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT for handling advanced tasks in emotion recognition and related fields.""",2024-03-09T13:56:25Z
KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques,Yes.,3.,"""Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases.""",2024-03-09T11:23:38Z
LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content,Yes.,1.,"""inspired by the rich implicit knowledge in large-scale models (e.g., large language models, LLMs), LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content.""",2024-03-09T09:52:15Z
Optimizing LLM Queries in Relational Workloads,Yes.,3.,"""However, LLM inference is highly expensive in both computational and economic terms",2024-03-09T07:01:44Z
MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs,Yes.,5.,"""we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively.""",2024-03-09T06:28:48Z
$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting,Yes.,2.,"""the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting.""",2024-03-09T05:20:48Z
ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes,Yes.,3.,"""most earlier clinical language models were pretrained with a context length limited to roughly one clinical document.""",2024-03-09T04:58:25Z
ItD: Large Language Models Can Teach Themselves Induction through Deduction,Yes.,3.,"""researchers have found that they still have limited ability to conduct induction."" and ""their performance is still constrained by the inherent inductive capability of the LLMs.""",2024-03-09T04:20:46Z
FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs,Yes.,3.,"""the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs.""",2024-03-09T02:27:45Z
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text,Yes.,4.,"""their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices"" and ""propose novel research directions to address the current limitations in this domain.""",2024-03-09T01:13:54Z
A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health,Yes.,2.,"""Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question.""",2024-03-08T23:46:37Z
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries,Yes.,1.,"""While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown.""",2024-03-08T23:17:55Z
Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?,Yes.,3.,"""Moreover, we show that vision models fail to capture the essence of video stimuli and that LLMs tend to rate different communicative acts and behavior desirability higher than people.""",2024-03-08T22:23:23Z
Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations,Yes.,2.,"""The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts.""",2024-03-08T21:26:49Z
DP-TabICL: In-Context Learning with Differentially Private Tabular Data,Yes.,3.,"""it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research.""",2024-03-08T21:19:01Z
Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4,Yes.,3.,"""Although GPT-4V outperformed other models in our evaluation, it still requires overall improvement.""",2024-03-08T21:16:28Z
PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,Yes.,3.,"""retrievals from large databases can constitute a substantial portion of the overall generation time,"" and ""to reduce generation latency and enhance generation quality.""",2024-03-08T21:09:20Z
Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach,Yes.,5.,"""While convenient, this modus operandi aggravates 'hallucination' concerns, particularly given the enigmatic 'black-box' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences.""",2024-03-08T19:18:53Z
Can Large Language Models Play Games? A Case Study of A Self-Play Approach,Yes.,4.,"""their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on.""",2024-03-08T19:16:29Z
GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM,Yes.,3.,"""the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput.""",2024-03-08T18:48:30Z
Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation,Yes.,1.,"""We introduce a novel approach to domain extrapolation that leverages reasoning ability and the extensive knowledge encapsulated within large language models (LLMs) to synthesize entirely new domains.""",2024-03-08T18:44:23Z
Unfamiliar Finetuning Examples Control How Language Models Hallucinate,Yes.,5.,"""Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts.""",2024-03-08T18:28:13Z
Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs,Yes.,4.,"""LLMs exhibit impressive zero/few-shot inference and generation quality for high-resource languages (HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The",2024-03-08T16:37:36Z
VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model,Yes.,1.,"""To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL).""",2024-03-08T14:23:00Z
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues,Yes.,3.,"""limiting the ASU performance"" and ""alleviate the LLMs-intrinsic factual hallucination problem in TSA.""",2024-03-08T14:05:36Z
Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents,Yes.,2.,"""The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task.""",2024-03-08T13:34:20Z
LLM4Decompile: Decompiling Binary Code with Large Language Models,,,,2024-03-08T13:10:59Z
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,Yes.,4.,"""We observe that better LLMs like GPT-4 can handle a larger variety of question types, but are by no means perfect. Also, correct answers do not necessarily imply correct rationales, which is an important evaluation that ERBench does better than other benchmarks for various",2024-03-08T12:42:36Z
Debiasing Multimodal Large Language Models,Yes.,4.,"""our investigation reveals a noteworthy bias in the generated content, where the output is primarily influenced by the underlying Large Language Models (LLMs) prior rather than the input image,"" and ""our investigation sheds light on the instability of LVLMs across various decoding configurations.""",2024-03-08T12:35:07Z
Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering,Yes.,1.,"""Existing methods follow two main paradigms to collect evidence",2024-03-08T11:09:13Z
"Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",Yes.,3.,"""Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge"" and ""highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.""",2024-03-08T10:09:57Z
Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation,Yes.,2.,"""We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs).""",2024-03-08T09:20:12Z
Towards a Psychology of Machines: Large Language Models Predict Human Memory,Yes.,1.,"""Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition.""",2024-03-08T08:41:14Z
Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem,No.,1.,"The abstract discusses the use of reinforcement learning and Transformer architecture for the inverse design of PCSEL, but it does not mention LLMs or their limitations.",2024-03-08T08:38:50Z
Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models,Yes.,2.,"""Recent advances in multi-modal large language models (MLLMs) provide a new and promising way to understand images with the help of text descriptions. However, most current MLLMs are designed for 2D natural images.""",2024-03-08T08:15:53Z
ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment,Yes.,3.,"""However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc.""",2024-03-08T08:08:10Z
ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models,Yes.,4.,"""Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations.""",2024-03-08T07:59:19Z
Benchmarking Large Language Models for Molecule Prediction Tasks,Yes.,5.,"""Notably, LLMs struggle with structured data, such as graphs, and often falter when tasked with answering domain-specific questions requiring deep expertise, such as those in biology and chemistry."" and ""Our investigation reveals several key insights",2024-03-08T05:59:56Z
Can we obtain significant success in RST discourse parsing by using Large Language Models?,Yes.,1.,"""While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question.""",2024-03-08T05:34:29Z
Are Human Conversations Special? A Large Language Model Perspective,Yes.,4.,"""there is a significant gap in their ability to specialize in human conversations"" and ""highlight the unique challenges posed by conversational data.""",2024-03-08T04:44:25Z
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs,Yes.,5.,"""Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.""",2024-03-08T03:49:17Z
Can't Remember Details in Long Documents? You Need Some R&R,,,,2024-03-08T03:03:20Z
DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation,Yes.,1.,"""We present DiffChat, a novel method to align Large Language Models (LLMs) to 'chat' with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation.""",2024-03-08T02:24:27Z
Tell me the truth: A system to measure the trustworthiness of Large Language Models,Yes.,5.,"""one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems."" and ""ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites."" and ""ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases.""",2024-03-08T00:27:57Z
An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment,Yes.,3.,"""However, LLMs have their limitations, as seen in GPT-4's struggles with lexical paraphrasing.""",2024-03-08T00:19:24Z
SecGPT: An Execution Isolation Architecture for LLM-Based Systems,Yes.,4.,"""Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users.""",2024-03-08T00:02:30Z
Automatic and Universal Prompt Injection Attacks against Large Language Models,Yes.,5.,"""These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests.""",2024-03-07T23:46:20Z
Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering,Yes.,1.,"""Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support.""",2024-03-07T20:48:40Z
Evaluating Biases in Context-Dependent Health Questions,Yes.,4.,"""We study how large language model biases are exhibited through these contextual questions in the healthcare domain."" and ""Our experiments reveal biases in each of these attributes, where young adult female users are favored.""",2024-03-07T19:15:40Z
iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries,Yes.,3.,"""their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform.""",2024-03-07T18:56:39Z
LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error,Yes.,5.,"""Existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.""",2024-03-07T18:50:51Z
SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM,Yes.,4.,"""Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses.""",2024-03-07T18:38:17Z
How Far Are We from Intelligent Visual Deductive Reasoning?,Yes.,4.,"""we are still far from achieving comparable proficiency in visual deductive reasoning,"" and ""certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks,"" and ""VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.""",2024-03-07T18:35:54Z
Common 7B Language Models Already Possess Strong Math Capabilities,Yes.,5.,"""The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.""",2024-03-07T18:00:40Z
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes,No.,1.,The abstract does not mention LLMs or any related limitations. It focuses on evaluating the robustness of vision-based models against object-to-background compositional changes.,2024-03-07T17:48:48Z
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification,,,,2024-03-07T17:44:17Z
Telecom Language Models: Must They Be Large?,Yes.,5.,"""the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments"" and ""highlighting its potential and limitations.""",2024-03-07T17:13:12Z
QAQ: Quality Adaptive Quantization for LLM KV Cache,Yes.,5.,"""a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length"" and ""heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance.""",2024-03-07T16:42:37Z
Teaching Large Language Models to Reason with Reinforcement Learning,Yes.,3.,"""during RL training models fail to explore significantly beyond solutions already produced by SFT models.""",2024-03-07T16:36:29Z
CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios,Yes.,3.,"""Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events.""",2024-03-07T16:31:02Z
Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition,Yes.,3.,"""Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.""",2024-03-07T15:22:07Z
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability,Yes.,1.,"""Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic.""",2024-03-07T13:36:08Z
Do Large Language Model Understand Multi-Intent Spoken Language ?,Yes.,1.,"""Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models.""",2024-03-07T13:30:52Z
Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset,Yes.,2.,"""Despite the progress, the field has many aspects left to explore.""",2024-03-07T12:57:16Z
Low-Resource Court Judgment Summarization for Common Law Systems,Yes.,1.,"""Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation.""",2024-03-07T12:47:42Z
Membership Inference Attacks and Privacy in Topic Modeling,No.,1.,The abstract discusses privacy attacks against topic models and does not mention large language models (LLMs) or their limitations.,2024-03-07T12:43:42Z
Feedback-Generation for Programming Exercises With GPT-4,Yes.,3.,"""At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed.""",2024-03-07T12:37:52Z
Acceleron: A Tool to Accelerate Research Ideation,,,,2024-03-07T10:20:06Z
Discriminative Probing and Tuning for Text-to-Image Generation,Yes.,3.,"""Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning."" and ""However, the inherent alignment capabilities of T2I models are still inadequate.""",2024-03-07T08:37:33Z
Online Adaptation of Language Models with a Memory of Amortized Contexts,Yes.,3.,"""large language models (LLMs) quickly run out of date despite enormous development costs"" and ""given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential.""",2024-03-07T08:34:57Z
Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders,Yes.,3.,"""We observe that current embedding models fare poorly in semantic understanding of these concepts.""",2024-03-07T08:32:17Z
HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild,Yes.,5.,"""Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains.""",2024-03-07T08:25:46Z
Effectiveness Assessment of Recent Large Vision-Language Models,Yes.,5.,"""Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems.""",2024-03-07T08:25:27Z
Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy,Yes.,3.,"""existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously.""",2024-03-07T07:31:00Z
Advancing Biomedical Text Mining with Community Challenges,Yes.,2.,"""Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.""",2024-03-07T06:52:51Z
Can Small Language Models be Good Reasoners for Sequential Recommendation?,Yes.,5.,"""However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high",2024-03-07T06:49:37Z
Federated Recommendation via Hybrid Retrieval Augmented Generation,Yes.,3.,"""LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios.""",2024-03-07T06:38:41Z
UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities,Yes.,3.,"""Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE.""",2024-03-07T06:10:02Z
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning,Yes.,3.,"""We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning."" and ""overcoming pretraining sequence length limitations.""",2024-03-07T05:26:41Z
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks,Yes.,1.,"""This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination.""",2024-03-07T05:05:56Z
Aligners: Decoupling LLMs and Alignment,Yes.,3.,"""Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion.""",2024-03-07T04:54:56Z
Self-Evaluation of Large Language Model based on Glass-box Features,Yes.,1.,"""The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods.""",2024-03-07T04:50:38Z
Large Language Models are In-Context Molecule Learners,Yes.,3.,"""previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs.""",2024-03-07T03:58:28Z
"Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",Yes.,3.,"""discuss the current limitations, and suggest potential pathways for future research.""",2024-03-07T03:38:44Z
Metric-aware LLM inference for regression and scoring,Yes.,3.,"""Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. Building on prior work on Minimum Bayes Risk Decoding, we show that this inference strategy can be suboptimal for a range of regression and scoring tasks, and associated evaluation metrics.""",2024-03-07T03:24:34Z
Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy,Yes.,3.,"""However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents.""",2024-03-07T02:34:54Z
Privacy-preserving Fine-tuning of Large Language Models through Flatness,Yes.,3.,"""The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation.""",2024-03-07T00:44:11Z
Exploring LLM-based Agents for Root Cause Analysis,Yes.,5.,"""However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes.""",2024-03-07T00:44:01Z
Can Large Language Models Reason and Plan?,Yes.,4.,"""While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.""",2024-03-07T00:36:32Z
Artificial Intelligence Exploring the Patent Field,Yes.,4.,"""However, patents entail a number of difficulties with which existing models struggle."" and ""Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections",2024-03-06T23:17:16Z
Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models,Yes.,4.,"""there have been growing concerns regarding potential contamination of these benchmarks as they may be leaked into pretraining and finetuning data"" and ""models perform significantly better on the subset of the benchmarks where similar solutions are seen during training.""",2024-03-06T21:45:35Z
Can Large Language Models do Analytical Reasoning?,Yes.,5.,"""we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores."" and ""we conclude that task complexity depends on the length of context, the information density, and the presence of related information.""",2024-03-06T20:22:08Z
Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification,Yes.,1.,"""a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports.""",2024-03-06T20:10:41Z
Guiding Enumerative Program Synthesis with Large Language Models,Yes.,3.,"""We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms.""",2024-03-06T19:13:53Z
FaaF: Facts as a Function for the evaluation of generated text,Yes.,3.,"""this method of prompting is unreliable when faced with incomplete or inaccurate reference information.""",2024-03-06T17:48:06Z
SaulLM-7B: A pioneering Large Language Model for Law,Yes.,1.,"""In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain.""",2024-03-06T17:42:16Z
KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions,Yes.,5.,"""we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement.""",2024-03-06T17:16:44Z
Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,Yes.,5.,"""Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles.""",2024-03-06T17:15:04Z
ShortGPT: Layers in Large Language Models are More Redundant Than You Expect,Yes.,5.,"""However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality.""",2024-03-06T17:04:18Z
The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported,Yes.,3.,"""Recent work arXiv.2310.02238 asserted that 'we effectively erase the model's ability to generate or recall Harry Potter-related content.' This claim is shown to be overbroad.""",2024-03-06T16:39:50Z
Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,Yes.,4.,"""However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen)."" and ""there is a long tail of languages where models are neither accurate nor faithful.""",2024-03-06T16:01:44Z
Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery,Yes.,1.,"""considering that the large language models (LLMs) emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery.""",2024-03-06T15:35:53Z
PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion,,,,2024-03-06T15:33:32Z
German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset,Yes.,5.,"""Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document.""",2024-03-06T14:37:30Z
Towards Safe and Aligned Large Language Models for Medicine,Yes.,4.,"""While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses,"" and ""the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights.""",2024-03-06T14:34:07Z
Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese,Yes.,1.,"""The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation.""",2024-03-06T13:17:07Z
General2Specialized LLMs Translation for E-commerce,Yes.,1.,"""The paradigm can be used for the NMT models based on Large language models (LLMs).""",2024-03-06T13:15:21Z
K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data,Yes.,1.,"""leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias.""",2024-03-06T12:08:14Z
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models,Yes.,3.,"""Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements).""",2024-03-06T11:48:08Z
GPTopic: Dynamic and Interactive Topic Representations,Yes.,1.,"""we introduce GPTopic, a software package that leverages Large Language Models (LLMs) to create dynamic, interactive topic representations.""",2024-03-06T11:34:20Z
Multimodal Large Language Models to Support Real-World Fact-Checking,Yes.,3.,"""While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied."" and ""existing open-source models exhibit strong biases and are highly sensitive to the prompt.""",2024-03-06T11:32:41Z
WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off,Yes.,2.,"""Watermarking is a technical means to dissuade malfeasant usage of Large Language Models."" and ""WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.""",2024-03-06T10:55:30Z
Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem,Yes.,5.,"""However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination.""",2024-03-06T09:06:34Z
Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models,Yes.,4.,"""Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation.""",2024-03-06T08:50:25Z
Prompt Mining for Language-based Human Mobility Forecasting,Yes.,3.,"""using fixed templates for prompting may limit the forecasting capability of language models.""",2024-03-06T08:43:30Z
Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,Yes.,4.,"""recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process.""",2024-03-06T08:31:35Z
CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models,Yes.,5.,"""We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings.""",2024-03-06T07:43:43Z
GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection,Yes.,3.,"""Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states.""",2024-03-06T07:29:57Z
A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation,Yes.,2.,"""Even in the era of large language models, response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach.""",2024-03-06T06:54:02Z
TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs,No.,1.,"The abstract discusses Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) and their application in model order reduction of partial differential equations, but it does not mention language models (LLMs or LMs).",2024-03-06T04:49:18Z
Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models,Yes.,3.,"""However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks.""",2024-03-06T03:33:48Z
Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,Yes.,3.,"""existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data.""",2024-03-06T03:02:38Z
Human vs. Machine: Language Models and Wargames,Yes.,3.,"""We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.""",2024-03-06T02:23:32Z
Japanese-English Sentence Translation Exercises Dataset for Automatic Grading,Yes.,3.,"""the GPT models with few-shot learning show poorer results than finetuned BERT, indicating that our newly proposed task presents a challenging issue, even for the state-of-the-art large language models.""",2024-03-06T01:37:03Z
Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation,Yes.,2.,"""We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements.""",2024-03-05T22:12:01Z
Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse,Yes.,3.,"""We then perform thorough LLM model diagnostics, identifying the role of claim type (i.e. implicit vs explicit claims) and comment length as sources of model error.""",2024-03-05T21:38:19Z
Guardrail Baselines for Unlearning in LLMs,Yes.,3.,"""fine-tuning can be expensive, as it requires both generating a set of examples and running iterations of fine-tuning to update the model.""",2024-03-05T21:19:06Z
Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data,,,,2024-03-05T20:07:42Z
Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,Yes.,5.,"""Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities,""",2024-03-05T19:40:53Z
Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs,Yes.,5.,"""Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other L",2024-03-05T19:32:01Z
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,Yes.,4.,"""The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons."" and ""WMDP serves two roles",2024-03-05T18:59:35Z
CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments,,,,2024-03-05T18:41:37Z
MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets,Yes.,1.,"""Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs.""",2024-03-05T18:31:28Z
Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement,Yes.,1.,"""a customized AI Assistant powered by the GPT-4 Large Language Model.""",2024-03-05T18:24:52Z
SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection,Yes.,3.,"""While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences.""",2024-03-05T18:04:59Z
PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset,Yes.,3.,"""most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities."" and ""Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance.""",2024-03-05T18:01:59Z
Language Guided Exploration for RL Agents in Text Environments,Yes.,1.,"""Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts.""",2024-03-05T17:26:41Z
"Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",Yes.,4.,"""We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes."" and ""The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.""",2024-03-05T17:04:05Z
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents,Yes.,5.,"""Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories",2024-03-05T16:39:12Z
Learning to Use Tools via Cooperative and Interactive Agents,Yes.,3.,"""they still suffer from potential performance degradation when addressing complex tasks due to",2024-03-05T15:08:16Z
Word Importance Explains How Prompts Affect Language Model Outputs,Yes.,3.,"""However, their 'black box' nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use.""",2024-03-05T15:04:18Z
OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,Yes.,3.,"""identify visual perception and low-level action execution as critical bottlenecks.""",2024-03-05T14:53:53Z
Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations,Yes.,4.,"""However, their precision is still far away from acceptable in a sensitive field like education."" and ""reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.""",2024-03-05T14:41:12Z
Localized Zeroth-Order Prompt Optimization,Yes.,3.,"""Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.""",2024-03-05T14:18:15Z
"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",Yes.,3.,"""this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation.""",2024-03-05T14:11:54Z
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering,Yes.,3.,"""Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence.""",2024-03-05T13:43:58Z
Multi-Scale Protein Language Model for Unified Molecular Modeling,Yes.,3.,"""However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules.""",2024-03-05T13:35:41Z
WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction,Yes.,3.,"""Existing research mainly focuses on regular-shaped tables... editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code.""",2024-03-05T13:33:12Z
Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation,Yes.,3.,"""Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.""",2024-03-05T13:23:48Z
ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,Yes.,4.,"""However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored.""",2024-03-05T12:21:57Z
A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods,Yes.,1.,"""the advent of Large Language Models (LLMs) has altered conventional ATS methods.""",2024-03-05T12:11:07Z
In Search of Truth: An Interrogation Approach to Hallucination Detection,Yes.,5.,"""One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.""",2024-03-05T11:50:01Z
MathScale: Scaling Instruction Tuning for Mathematical Reasoning,Yes.,3.,"""However, their proficiency in solving mathematical problems remains inadequate.""",2024-03-05T11:42:59Z
An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers,Yes.,4.,"""their generalizability and fairness severely underperform GPT4.""",2024-03-05T10:20:52Z
DPPA: Pruning Method for Large Language Model to Model Merging,No.,1.,The abstract does not mention LLMs or any limitations related to them.,2024-03-05T09:12:49Z
Evaluating and Optimizing Educational Content with Large Language Model Judgments,Yes.,3.,"""We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.""",2024-03-05T09:09:15Z
EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs,Yes.,5.,"""However, their expensive computations and high memory requirements are prohibitive for deployment."" and ""the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.""",2024-03-05T08:45:30Z
Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations,Yes.,1.,"""the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities.""",2024-03-05T08:31:00Z
"Towards Measuring and Modeling ""Culture"" in LLMs: A Survey",Yes.,4.,"""Our analysis indicates that only certain aspects of 'culture,' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situated",2024-03-05T08:29:36Z
Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models,Yes.,5.,"""when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.""",2024-03-05T08:22:41Z
CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models,Yes.,3.,"""This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets.""",2024-03-05T07:58:12Z
Towards Training A Chinese Large Language Model for Anesthesiology,Yes.,2.,"""The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies.""",2024-03-05T07:53:49Z
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,Yes.,4.,"""they still face challenges of various biases"" and ""Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs.""",2024-03-05T07:47:34Z
Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models,Yes.,4.,"""available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese,"" and ""models with more parameters can introduce more biases and uncalibrated outputs.""",2024-03-05T07:13:28Z
Android in the Zoo: Chain-of-Action-Thought for GUI Agents,Yes.,1.,"""Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API.""",2024-03-05T07:09:35Z
Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment,Yes.,1.,"""an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model.""",2024-03-05T06:28:02Z
Privacy-Aware Semantic Cache for Large Language Models,Yes.,3.,"""these models incur exceptionally high computational costs"" and ""Existing caching solutions for LLMs raise privacy and scalability concerns and perform wasteful query requests.""",2024-03-05T06:23:50Z
InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents,Yes.,5.,"""Our findings raise questions about the widespread deployment of LLM Agents.""",2024-03-05T06:21:45Z
Revisiting Meta-evaluation for Grammatical Error Correction,Yes.,3.,"""These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques."" and ""covering 12 state-of-the-art systems including large language models (LLMs).""",2024-03-05T05:53:09Z
Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding,Yes.,5.,"""the persistent difficulty faced by most LLMs in identifying relevant information situated in the middle of the context has not been adequately tackled.""",2024-03-05T04:58:37Z
Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use,Yes.,1.,"""Our framework leverages recent advances in foundation models, both large language models and vision-language models, to carve out the concept space through conversation and by automatically labeling training data points.""",2024-03-05T03:34:11Z
Exploring the Limitations of Large Language Models in Compositional Relation Reasoning,Yes.,5.,"""We present a comprehensive evaluation of large language models (LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations.""",2024-03-05T03:07:10Z
ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary,Yes.,3.,"""simple CoT method often lacks the ability to provide extensive comparative summary.""",2024-03-05T01:13:56Z
Eliciting Better Multilingual Structured Reasoning from LLMs through Code,Yes.,3.,"""studies have been limited to English or simple reasoning tasks,"" and ""xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.""",2024-03-05T00:48:56Z
Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research,Yes.,2.,"""scaling and evaluating their usage presents new challenges not addressed in previous frameworks.""",2024-03-05T00:27:43Z
Wukong: Towards a Scaling Law for Large-Scale Recommendation,No.,1.,"The abstract primarily discusses recommendation models and their scaling laws, mentioning LLMs only in a comparative context to highlight differences in scalability.",2024-03-04T23:40:20Z
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,Yes.,1.,"""We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique.""",2024-03-04T22:47:58Z
"Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",Yes.,5.,"""a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted.""",2024-03-04T22:02:12Z
SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models,Yes.,5.,"""However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs.""",2024-03-04T21:55:22Z
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents,Yes.,1.,"""In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents.""",2024-03-04T21:50:29Z
Enhancing LLM Safety via Constrained Direct Preference Optimization,Yes.,3.,"""This approach, however, is computationally expensive and often unstable.""",2024-03-04T20:39:24Z
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering,Yes.,2.,"""Despite limitations in generating offensive texts using ChatGPT due to ethical constraints.""",2024-03-04T20:34:58Z
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems,Yes.,5.,"""We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls.""",2024-03-04T19:12:48Z
RegionGPT: Towards Region Understanding Vision Language Model,Yes.,3.,"""they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions.""",2024-03-04T18:58:08Z
Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve,Yes.,1.,"""Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time.""",2024-03-04T18:47:08Z
FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction,Yes.,4.,"""a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations"" and ""these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles),",2024-03-04T17:57:18Z
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection,Yes.,2.,"""To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text.""",2024-03-04T17:38:32Z
Non-autoregressive Sequence-to-Sequence Vision-Language Models,Yes.,3.,"""their applicability is limited by their inference latency due to their autoregressive way of generating predictions.""",2024-03-04T17:34:59Z
Birbal: An efficient 7B instruct-model fine-tuned with curated datasets,Yes.,4.,"""LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible.""",2024-03-04T17:34:46Z
PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models,Yes.,4.,"""Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at."" and ""our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner.""",2024-03-04T17:34:34Z
Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks,Yes.,1.,"""This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network intelligence.""",2024-03-04T17:29:57Z
TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models,Yes.,1.,"""It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning.""",2024-03-04T17:08:57Z
Not all Layers of LLMs are Necessary during Inference,Yes.,3.,"""The inference phase of Large Language Models (LLMs) is very expensive.""",2024-03-04T16:23:58Z
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models,Yes.,3.,"""In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains.""",2024-03-04T16:21:54Z
Cognition is All You Need -- The Next Layer of AI Above Large Language Models,Yes.,5.,"""Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving."" and ""The failure of these systems to address complex knowledge work is due to the fact that they",2024-03-04T16:11:57Z
Using LLMs for the Extraction and Normalization of Product Attribute Values,Yes.,1.,"""This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions.""",2024-03-04T15:39:59Z
Large language models surpass human experts in predicting neuroscience results,Yes.,1.,"""Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts.""",2024-03-04T15:27:59Z
Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models,,,,2024-03-04T15:27:49Z
adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds,Yes.,2.,"""Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored.""",2024-03-04T14:49:18Z
Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5,Yes.,3.,"""Post-hoc qualitative analysis reveals several points for improvement in future work including cross-referencing part-of-speech tagging, better sentence validation, and improving GPT prompts.""",2024-03-04T14:24:47Z
Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?,Yes.,4.,"""inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss.""",2024-03-04T14:01:11Z
Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism,Yes.,4.,"""While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.""",2024-03-04T13:57:37Z
Unveiling Hidden Links Between Unseen Security Entities,Yes.,1.,"""Leveraging ULTRA, a knowledge graph foundation model, combined with a Large Language Model (LLM), VulnScopper effectively handles unseen entities, overcoming the limitations of previous KG approaches.""",2024-03-04T13:14:39Z
Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?,,,,2024-03-04T13:10:08Z
LLM-Oriented Retrieval Tuner,Yes.,3.,"""However, due to the paradigm discrepancy between text generation of LLM and DR, it is still an open challenge to integrate the retrieval and generation tasks in a shared LLM.""",2024-03-04T12:50:25Z
Vanilla Transformers are Transfer Capability Teachers,No.,1.,"The abstract discusses Mixture of Experts (MoE) Transformers and vanilla Transformers, but does not mention language models (LLMs or LMs) or their limitations.",2024-03-04T12:40:28Z
SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis,Yes.,2.,"""Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in scientific literature analysis, especially in scenarios involving complex comprehension and multimodal data.""",2024-03-04T12:19:28Z
Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models,Yes.,3.,"""However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results.""",2024-03-04T12:16:15Z
AS-ES Learning: Towards Efficient CoT Learning in Small Models,Yes.,3.,"""existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data,"" and ""we explore the reason behind the inefficiency of small models in learning CoT.""",2024-03-04T12:13:59Z
Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?,Yes.,4.,"""we analyse the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve and remains limited, with low scores especially for low-resource languages.""",2024-03-04T10:48:13Z
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering,Yes.,1.,"""an alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting.""",2024-03-04T10:41:52Z
LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset,Yes.,1.,"""an innovative application of the Claude 2 large language model to classify cases based on content-specific prompts.""",2024-03-04T10:13:30Z
Online Training of Large Language Models: Learn while chatting,Yes.,3.,"""existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning.""",2024-03-04T10:00:55Z
Predicting Learning Performance with Large Language Models: A Study in Adult Literacy,Yes.,2.,"""While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using",2024-03-04T08:14:07Z
CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text,Yes.,2.,"""current evaluation methods are either limited in task coverage or lack standardization.""",2024-03-04T07:26:07Z
NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models,,,,2024-03-04T07:10:31Z
WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations,Yes.,5.,"""existing datasets and evaluation methods in this domain still exhibit notable limitations"" and ""highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement.""",2024-03-04T07:06:41Z
How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems,Yes.,5.,"""a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems.""",2024-03-04T06:24:21Z
Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models,Yes.,1.,"""Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters.""",2024-03-04T06:20:31Z
Differentially Private Synthetic Data via Foundation Model APIs 2: Text,Yes.,2.,"""existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs.""",2024-03-04T05:57:50Z
Decode Neural signal as Speech,Yes.,2.,"""However, the exploration is not adequate in three aspects",2024-03-04T05:55:01Z
Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study,Yes.,3.,"""state-of-the-art models such as GPT-4 generate relevant and accurate Design Decisions, although they fall short of human-level performance.""",2024-03-04T03:56:14Z
Improving LLM Code Generation with Grammar Augmentation,Yes.,1.,"""We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs).""",2024-03-03T22:38:35Z
Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models,Yes.,1.,"""This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).""",2024-03-03T21:24:35Z
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos,Yes.,1.,"""we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting.""",2024-03-03T19:53:06Z
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction,Yes.,4.,"""LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge.""",2024-03-03T17:35:52Z
ReMatch: Retrieval Enhanced Schema Matching with LLMs,Yes.,1.,"""In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced Large Language Models (LLMs).""",2024-03-03T17:14:40Z
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,Yes.,5.,"""Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited.""",2024-03-03T15:53:41Z
Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics,Yes.,3.,"""the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures.""",2024-03-03T13:14:47Z
Infusing Knowledge into Large Language Models with Contextual Prompts,Yes.,1.,"""Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch.""",2024-03-03T11:19:26Z
Logic Rules as Explanations for Legal Case Retrieval,Yes.,1.,"""introducing a novel explainability metric using Large Language Models (LLMs).""",2024-03-03T09:22:21Z
GuardT2I: Defending Text-to-Image Models from Adversarial Prompts,Yes.,1.,"""GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance.""",2024-03-03T09:04:34Z
Ever-Evolving Memory by Blending and Refining the Past,Yes.,5.,"""current large language models often lack this capability, leading to instances of missing important user information or redundantly asking for the same information, thereby diminishing conversation quality.""",2024-03-03T08:12:59Z
Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge,Yes.,5.,"""However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications.""",2024-03-03T08:07:55Z
MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies,Yes.,3.,"""These models have shown promise in analyzing short video clips. However, when it comes to longer formats like movies, they often fall short.""",2024-03-03T07:43:39Z
The Implicit Bias of Heterogeneity towards Invariance and Causality,Yes.,3.,"""It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations.""",2024-03-03T07:38:24Z
CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge,Yes.,5.,"""existing KGQA datasets focus on popular entities for which large language models (LLMs) can directly answer without hallucinating and without leveraging the KG"" and ""baseline evaluation of LLMs on CR-LT KGQA demonstrate a high rate of",2024-03-03T04:47:01Z
Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models,,,,2024-03-03T04:46:21Z
Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering,Yes.,4.,"""existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable.""",2024-03-03T04:22:13Z
On the Compressibility of Quantized Large Language Models,Yes.,3.,"""However, it also faces critical challenges due to the substantial memory requirement of LLMs"" and ""even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference.""",2024-03-03T03:27:07Z
Automatic Question-Answer Generation for Long-Tail Knowledge,Yes.,3.,"""While they exhibit high accuracy in answering questions related to common knowledge, LLMs encounter difficulties in learning about uncommon long-tail knowledge (tail entities).""",2024-03-03T03:06:31Z
LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems,Yes.,3.,"""However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts.""",2024-03-02T23:32:33Z
Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data,Yes.,,,2024-03-02T22:33:17Z
A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition,Yes.,1.,"""Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves recognition accuracy.""",2024-03-02T21:15:24Z
Improving the Validity of Automatically Generated Feedback via Reinforcement Learning,Yes.,2.,"""However, both feedback generation and evaluation are challenging",2024-03-02T20:25:50Z
Analysis of Privacy Leakage in Federated Large Language Models,Yes.,5.,"""revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.""",2024-03-02T20:25:38Z
