Title,Talks about LLMs,Rate,Evidence,Published
NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention,Yes.,2.,"""Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations.""",2024-03-02T17:29:22Z
Dissecting Language Models: Machine Unlearning via Selective Pruning,Yes.,1.,"""This paper introduces a machine unlearning method specifically designed for LLMs.""",2024-03-02T17:10:44Z
AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks,Yes.,3.,"""Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks.""",2024-03-02T16:52:22Z
Accelerating Greedy Coordinate Gradient via Probe Sampling,Yes.,3.,"""Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications."" and ""Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming",2024-03-02T16:23:44Z
SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code,Yes.,1.,"""This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets.""",2024-03-02T16:16:26Z
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal,Yes.,5.,"""Large language models (LLMs) suffer from catastrophic forgetting during continual learning.""",2024-03-02T16:11:23Z
IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact,Yes.,3.,"""Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance.""",2024-03-02T16:05:26Z
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access,Yes.,3.,"""This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access."" and ""However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance.""",2024-03-02T14:14:45Z
Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning,Yes.,1.,"""This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification.""",2024-03-02T13:43:32Z
The Case for Animal-Friendly AI,Yes.,2.,"""Preliminary results suggest that the outcomes of the tested models can be benchmarked regarding the consideration they give to animals, and that generated positions and biases might be addressed and mitigated with more developed and validated systems.""",2024-03-02T12:41:11Z
DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling,Yes.,2.,"""There remain two challenges in RM training",2024-03-02T12:31:22Z
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots,Yes.,5.,"""However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge."" and ""These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications.""",2024-03-02T12:19:04Z
Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding,Yes.,3.,"""Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial.""",2024-03-02T11:54:55Z
STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models,,,,2024-03-02T10:38:10Z
"A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",Yes.,4.,"""While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale.""",2024-03-02T09:39:13Z
MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining,Yes.,1.,"""Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored.""",2024-03-02T09:27:32Z
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies,Yes.,3.,"""We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (~13% gap) after a light supervision. ... Lastly, we show challenging distractors confuse LLMs, but not humans.""",2024-03-02T08:53:40Z
LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization,Yes.,3.,"""The immense sizes of LLMs have led to very high resource demand and cost for running the models.""",2024-03-02T08:40:07Z
Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data,Yes.,3.,"""In the first phase, we investigate the challenges an LLM like GPT-4 faces in comprehending raw sensor data.""",2024-03-02T08:29:08Z
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation,Yes.,3.,"""However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge.""",2024-03-02T08:21:59Z
OpenGraph: Towards Open Graph Foundation Models,Yes.,2.,"""we introduce a data augmentation mechanism enhanced by a LLM to alleviate the limitations of data scarcity in real-world scenarios.""",2024-03-02T08:05:03Z
Towards Accurate Lip-to-Speech Synthesis in-the-Wild,No.,1.,The abstract does not mention LLMs or their limitations.,2024-03-02T04:07:24Z
LAB: Large-Scale Alignment for ChatBots,,,,2024-03-02T03:48:37Z
LLMCRIT: Teaching Large Language Models to Use Criteria,Yes.,2.,"""However, existing research in this field tends to consider only a limited set of criteria or quality assessment aspects.""",2024-03-02T02:25:55Z
FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis,Yes.,3.,"""it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy.""",2024-03-02T02:00:51Z
Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers,Yes.,5.,"""We find that all three models make faithfulness mistakes in over 50% of summaries and struggle to interpret difficult subtext.""",2024-03-02T01:52:14Z
Towards Full Authorship with AI: Supporting Revision with AI-Generated Views,Yes.,3.,"""This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process."" and ""However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management.""",2024-03-02T01:11:35Z
AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks,Yes.,2.,"""However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or 'hands-on-keyboard' attacks, under various attack techniques and environments.""",2024-03-02T00:10:45Z
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,Yes.,2.,"""However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic.""",2024-03-01T23:38:02Z
Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries,Yes.,4.,"""Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health.""",2024-03-01T21:59:03Z
Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods,Yes.,5.,"""we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs.""",2024-03-01T21:48:08Z
MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection,Yes.,5.,"""contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting 'hallucinations'.""",2024-03-01T20:31:10Z
ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys,Yes.,1.,"""Our evaluations on various models (e.g., BERT and Llama) demonstrate that ATP achieves comparable accuracy with much lower computation and memory complexity than the standard attention mechanism.""",2024-03-01T19:24:37Z
Differentially Private Knowledge Distillation via Synthetic Text Generation,Yes.,3.,"""Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss.""",2024-03-01T19:22:24Z
An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce,Yes.,3.,"""Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare",2024-03-01T19:08:25Z
Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training,Yes.,5.,"""recent studies showcase that causal LLMs suffer from the 'reversal curse'. It is a typical example that the model knows 'A's father is B', but is unable to reason 'B's child is A'. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional",2024-03-01T18:55:20Z
AtP*: An efficient and scalable method for localizing LLM behaviour to components,Yes.,3.,"""We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives.""",2024-03-01T18:43:51Z
Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents,Yes.,5.,"""However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions."" and ""While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack's mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback.""",2024-03-01T17:22:16Z
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models,Yes.,4.,"""Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge,"" and ""many merely focus on the factuality hallucination while ignoring the faithfulness hallucination.""",2024-03-01T15:38:55Z
Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction,Yes.,1.,"""A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories."" and ""We fine-tune two BERT LMs as baselines, as well as eight open-source",2024-03-01T13:36:04Z
"ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models",,,,2024-03-01T13:15:30Z
Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition,Yes.,1.,"""Recent advances in LLMs have sparked a debate on whether they understand text.""",2024-03-01T12:42:47Z
TempCompass: Do Video LLMs Really Understand Videos?,Yes.,5.,"""reveal the discerning fact that these models exhibit notably poor temporal perception ability.""",2024-03-01T12:02:19Z
LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues,Yes.,2.,"""Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality and linguistically sophisticated data.""",2024-03-01T11:33:53Z
Hierarchical Indexing for Retrieval-Augmented Opinion Summarization,Yes.,1.,"""We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs).""",2024-03-01T10:38:07Z
LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness,Yes.,3.,"""their performance is heavily influenced by prompt design"" and ""Recognizing the subjective nature of TSA, we evaluate the ability of LLMs to quantify predictive uncertainty via calibration error and correlation to human inter-annotator agreement.""",2024-03-01T10:10:34Z
Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models,Yes.,3.,"""Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs).""",2024-03-01T10:03:07Z
Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment,Yes.,1.,"""We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs)",2024-03-01T10:01:36Z
Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs,Yes.,5.,"""the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks.""",2024-03-01T09:28:38Z
Invariant Test-Time Adaptation for Vision-Language Model Generalization,Yes.,4.,"""However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of 'decision shortcuts' that hinders their generalization capabilities."" and ""the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in",2024-03-01T09:01:53Z
Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models,Yes.,1.,"""We further enhance LLMs' reasoning abilities through a novel Retrieval-Aware Training (RAT) process and its refined iteration, RAT-R.""",2024-03-01T08:43:43Z
Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models,,,,2024-03-01T08:05:44Z
Never-Ending Embodied Robot Learning,Yes.,3.,"""However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks.""",2024-03-01T07:51:29Z
Teach LLMs to Phish: Stealing Private Information from Language Models,Yes.,5.,"""When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information.""",2024-03-01T06:15:07Z
DPP-Based Adversarial Prompt Searching for Lanugage Models,Yes.,4.,"""Language models risk generating mindless and offensive content, which hinders their safe deployment."" and ""Experimental results on six different pre-trained language models demonstrate the efficacy of ASRA for eliciting toxic content.""",2024-03-01T05:28:06Z
Gender Bias in Large Language Models across Multiple Languages,Yes.,5.,"""assessing the influence of gender biases embedded in LLMs becomes crucial"" and ""Our findings revealed significant gender biases across all the languages we examined.""",2024-03-01T04:47:16Z
SoftTiger: A Clinical Foundation Model for Healthcare Workflows,Yes.,3.,"""Moreover, we address several modeling challenges in the healthcare context, e.g., extra long context window.""",2024-03-01T04:39:16Z
Extracting Polymer Nanocomposite Samples from Full-Length Documents,Yes.,5.,"""Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.""",2024-03-01T03:51:56Z
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,Yes.,4.,"""recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails.""",2024-03-01T03:29:54Z
Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,Yes.,5.,"""However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains."" and ""Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.""",2024-03-01T02:21:30Z
Improving Socratic Question Generation using Data Augmentation and Preference Optimization,Yes.,3.,"""existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions.""",2024-03-01T00:08:20Z
AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs,Yes.,4.,"""Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications.""",2024-03-01T00:02:37Z
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction,Yes.,2.,"""Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters.""",2024-02-29T23:03:19Z
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision,Yes.,3.,"""Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt.""",2024-02-29T22:26:07Z
LLMs in Political Science: Heralding a New Era of Visual Analysis,Yes.,1.,"""This landscape could potentially change thanks to the rise of large language models (LLMs).""",2024-02-29T22:11:20Z
UniTS: Building a Unified Time Series Model,Yes.,3.,"""current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models.""",2024-02-29T21:25:58Z
FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition,Yes.,5.,"""such a paradigm fails to comprehensively differentiate the fine-grained language and cognitive skills, rendering the lack of sufficient interpretation to LLMs' capabilities"" and ""we identify a common shortfall in knowledge utilization among models.""",2024-02-29T21:05:37Z
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications,Yes.,4.,"""highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks.""",2024-02-29T21:05:14Z
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,Yes.,3.,"""such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes.""",2024-02-29T20:25:16Z
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs,Yes.,5.,"""Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates.""",2024-02-29T19:55:06Z
PROC2PDDL: Open-Domain Planning Representations from Texts,Yes.,5.,"""We show that Proc2PDDL is highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around 35%. Our analysis shows both syntactic and semantic errors, indicating LMs' deficiency in both generating domain-specific programs and reasoning about events.""",2024-02-29T19:40:25Z
Resonance RoPE: Improving Context Length Generalization of Large Language Models,Yes.,5.,"""This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences.""",2024-02-29T19:02:03Z
Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization,Yes.,4.,"""However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases."" and ""almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format.""",2024-02-29T19:00:47Z
The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?,Yes.,5.,"""we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit",2024-02-29T18:59:25Z
The All-Seeing Project V2: Towards General Relation Comprehension of the Open World,Yes.,3.,"""diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs).""",2024-02-29T18:59:17Z
Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling,Yes.,3.,"""In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines.""",2024-02-29T18:58:15Z
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,Yes.,4.,"""Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness."" and ""we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions",2024-02-29T18:55:06Z
Curiosity-driven Red-teaming for Large Language Models,Yes.,5.,"""Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content.""",2024-02-29T18:55:03Z
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models,Yes.,3.,"""We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics.""",2024-02-29T18:47:52Z
ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL,Yes.,3.,"""current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks.""",2024-02-29T18:45:56Z
Compositional API Recommendation for Library-Oriented Code Generation,Yes.,3.,"""However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs.""",2024-02-29T18:27:27Z
Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines,Yes.,3.,"""the intricate nature of LLMs renders their 'cognitive' processes opaque, challenging even their designers' understanding.""",2024-02-29T18:20:37Z
On the Scaling Laws of Geographical Representation in Language Models,Yes.,4.,"""we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.""",2024-02-29T18:04:11Z
Entity-Aware Multimodal Alignment Framework for News Image Captioning,Yes.,4.,"""Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-t",2024-02-29T18:03:00Z
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy,Yes.,2.,"""Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate.""",2024-02-29T17:27:59Z
OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models,Yes.,2.,"""Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups.""",2024-02-29T17:19:39Z
SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency,Yes.,3.,"""The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs.""",2024-02-29T17:13:44Z
Watermark Stealing in Large Language Models,Yes.,5.,"""We dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes."" and ""Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.""",2024-02-29T17:12:39Z
"Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",Yes.,1.,"""Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field.""",2024-02-29T16:56:23Z
Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction,Yes.,2.,"""existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks.""",2024-02-29T16:29:53Z
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation,Yes.,4.,"""Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance.""",2024-02-29T16:09:02Z
RL-GPT: Integrating Reinforcement Learning and Code-as-policy,Yes.,3.,"""Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control.""",2024-02-29T16:07:22Z
PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval,Yes.,3.,"""general-purpose large language models often struggle to meet the specific needs of planners.""",2024-02-29T15:41:20Z
GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers,Yes.,5.,"""One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly."" and ""Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.""",2024-02-29T15:26:14Z
Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark,Yes.,3.,"""Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well.""",2024-02-29T15:22:13Z
RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,Yes.,1.,"""Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA.""",2024-02-29T14:50:58Z
Memory-Augmented Generative Adversarial Transformers,Yes.,5.,"""Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy.""",2024-02-29T14:47:24Z
PeLLE: Encoder-based language models for Brazilian Portuguese based on open data,Yes.,1.,"""We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks.""",2024-02-29T14:34:03Z
PRSA: Prompt Reverse Stealing Attacks against Large Language Models,Yes.,2.,"""this problem still has not been comprehensively explored yet"" and ""PRSA poses a severe threat in real world scenarios.""",2024-02-29T14:30:28Z
Teaching Large Language Models an Unseen Language on the Fly,Yes.,5.,"""Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating.""",2024-02-29T13:50:47Z
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning,Yes.,2.,"""the model's pre-training may not naturally encompass these principles, necessitating the need to inspire or teach the model.""",2024-02-29T13:49:56Z
Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model,Yes.,5.,"""However, the Typographic Attack, which disrupts vision-language models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), has also been expected to be a security threat to LVLMs."" and ""Based on the evaluation results, we investigate the causes why typographic attacks may impact VLMs and LVLMs, leading to three highly insightful",2024-02-29T13:31:56Z
Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models,Yes.,5.,"""Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted",2024-02-29T12:35:45Z
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment,,,,2024-02-29T12:12:30Z
Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials,Yes.,5.,"""investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples"" and ""the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent",2024-02-29T12:01:46Z
Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study,Yes.,3.,"""However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.""",2024-02-29T11:29:47Z
"FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use",Yes.,2.,"""However, the use of free public services poses a risk of data leakage, as service providers may exploit user input for additional training and optimization without clear boundaries.""",2024-02-29T09:43:50Z
EyeGPT: Ophthalmic Assistant with Large Language Models,Yes.,3.,"""large language models (LLM) trained with general world knowledge might not possess the capability to tackle medical-related tasks at an expert level.""",2024-02-29T09:35:41Z
Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning,Yes.,5.,"""However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.""",2024-02-29T05:27:45Z
How do Large Language Models Handle Multilingualism?,Yes.,1.,"""Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages.""",2024-02-29T02:55:26Z
On the Decision-Making Abilities in Role-Playing using Large Language Models,Yes.,1.,"""Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks.""",2024-02-29T02:22:23Z
ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph,Yes.,5.,"""large language models (LLMs) remain significantly limited in properly using massive external tools"" and ""Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools.""",2024-02-29T02:04:00Z
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning,Yes.,1.,"""Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks.""",2024-02-29T01:33:08Z
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models,Yes.,3.,"""expensive training as well as inference remains a significant impediment to their widespread applicability.""",2024-02-28T22:21:47Z
Commonsense Ontology Micropatterns,Yes.,1.,"""Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions.""",2024-02-28T21:23:54Z
Learning to Compress Prompt in Natural Language Formats,Yes.,5.,"""Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results.""",2024-02-28T20:41:21Z
CLLMs: Consistency Large Language Models,Yes.,3.,"""Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step.""",2024-02-28T20:17:04Z
Data Interpreter: An LLM Agent For Data Science,Yes.,3.,"""However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning.""",2024-02-28T19:49:55Z
Simple linear attention language models balance the recall-throughput tradeoff,Yes.,3.,"""the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption"" and ""efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall.""",2024-02-28T19:28:27Z
FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability,Yes.,5.,"""Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately.""",2024-02-28T19:23:27Z
Large Language Models and Games: A Survey and Roadmap,Yes.,2.,"""we reconcile the potential and limitations of LLMs within the games domain.""",2024-02-28T19:09:08Z
A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems,Yes.,4.,"""there are also increasing concerns over the security of such probabilistic intelligent systems"" and ""Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components.""",2024-02-28T19:00:12Z
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,Yes.,3.,"""Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications.""",2024-02-28T18:58:25Z
Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates,Yes.,4.,"""even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models.""",2024-02-28T18:23:49Z
Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification,Yes.,2.,"""it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness.""",2024-02-28T17:29:27Z
Language Models Represent Beliefs of Self and Others,Yes.,1.,"""While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive.""",2024-02-28T17:25:59Z
The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA,Yes.,1.,"""which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs).""",2024-02-28T15:05:43Z
Large Language Models As Evolution Strategies,Yes.,1.,"""Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms.""",2024-02-28T15:02:17Z
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,Yes.,5.,"""we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem."" and ""we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers.""",2024-02-28T14:09:02Z
How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning,Yes.,2.,"""a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation.""",2024-02-28T13:14:20Z
Retrieval-based Full-length Wikipedia Generation for Emergent Events,Yes.,2.,"""To ensure that Large Language Models (LLMs) are not trained on corpora related to recently occurred events"" and ""evaluate the capability of LLMs in generating factual full-length Wikipedia documents.""",2024-02-28T11:51:56Z
Towards Generalist Prompting for Large Language Models by Mental Models,Yes.,3.,"""However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks.""",2024-02-28T11:29:09Z
Learning or Self-aligning? Rethinking Instruction Fine-tuning,Yes.,5.,"""attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects.""",2024-02-28T11:16:00Z
Prospect Personalized Recommendation on Large Language Model-based Agent Platform,Yes.,2.,"""Lastly, we discuss potential issues and promising directions for future research.""",2024-02-28T11:12:17Z
CogBench: a large language model walks into a psychology lab,Yes.,2.,"""evaluating them comprehensively remains challenging"" and ""Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior.""",2024-02-28T10:43:54Z
LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History,Yes.,5.,"""we find that performance can in fact also be negatively impacted, if there is a task-switch"" and ""many of the task-switches can lead to significant performance degradation.""",2024-02-28T10:19:05Z
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging,Yes.,1.,"""to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs).""",2024-02-28T09:51:55Z
MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery,Yes.,1.,"""we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions.""",2024-02-28T08:57:42Z
From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs,Yes.,1.,"""Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence.""",2024-02-28T08:42:23Z
MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices,Yes.,3.,"""However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges.""",2024-02-28T08:30:49Z
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,Yes.,5.,"""studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it.""",2024-02-28T08:24:38Z
Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information,,,,2024-02-28T08:09:14Z
Cause and Effect: Can Large Language Models Truly Understand Causality?,Yes.,3.,"""With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails.""",2024-02-28T08:02:14Z
"Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",Yes.,4.,"""cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts.""",2024-02-28T07:18:39Z
Small But Funny: A Feedback-Driven Approach to Humor Distillation,Yes.,3.,"""While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation.""",2024-02-28T07:02:38Z
Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,Yes.,5.,"""One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs.""",2024-02-28T06:50:14Z
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models,Yes.,3.,"""current model editing methods struggle with the specialization and complexity of medical knowledge.""",2024-02-28T06:40:57Z
No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization,Yes.,5.,"""the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself.""",2024-02-28T06:34:54Z
Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions,Yes.,4.,"""However, medical board exam questions or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions.""",2024-02-28T05:44:41Z
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models,Yes.,3.,"""Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other.""",2024-02-28T05:43:22Z
MEGAnno+: A Human-LLM Collaborative Annotation System,Yes.,5.,"""Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations.""",2024-02-28T04:58:07Z
Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension,Yes.,3.,"""these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs.""",2024-02-28T04:56:21Z
Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore,Yes.,4.,"""Large Language Models (LLMs) are prone to factuality hallucination, generating text that contradicts established knowledge,"" and ""multilingual models demonstrate a bias towards factual information from Western continents.""",2024-02-28T04:43:46Z
Datasets for Large Language Models: A Comprehensive Survey,Yes.,3.,"""The survey sheds light on the prevailing challenges and points out potential avenues for future investigation.""",2024-02-28T04:35:51Z
Automated Discovery of Integral with Deep Learning,Yes.,4.,"""Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.""",2024-02-28T04:34:15Z
ResLoRA: Identity Residual Mapping in Low-Rank Adaption,Yes.,1.,"""updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model.""",2024-02-28T04:33:20Z
Corpus-Steered Query Expansion with Large Language Models,Yes.,5.,"""challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.""",2024-02-28T03:58:58Z
Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions,Yes.,3.,"""Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages.""",2024-02-28T03:44:01Z
TroubleLLM: Align to Red Team Expert,Yes.,4.,"""However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content.""",2024-02-28T03:40:46Z
Merino: Entropy-driven Design for Generative Language Models on IoT Devices,Yes.,3.,"""However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost.""",2024-02-28T03:20:27Z
A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems,Yes.,2.,"""discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.""",2024-02-28T03:16:44Z
Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization,Yes.,3.,"""Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information.""",2024-02-28T02:40:09Z
Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars,Yes.,3.,"""Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct.""",2024-02-28T02:12:47Z
FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization,Yes.,5.,"""However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance.""",2024-02-28T02:00:34Z
Collaborative decoding of critical tokens for boosting factuality of large language models,Yes.,5.,"""their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination.""",2024-02-28T01:53:37Z
Gradient-Free Adaptive Global Pruning for Pre-trained Language Models,Yes.,5.,"""The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands.""",2024-02-28T00:09:07Z
EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models,Yes.,1.,"""This paper introduces EmMark, a novel watermarking framework for protecting the intellectual property (IP) of embedded large language models deployed on resource-constrained edge devices.""",2024-02-27T23:30:17Z
Acquiring Linguistic Knowledge from Multimodal Input,,,,2024-02-27T23:29:10Z
Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures,Yes.,2.,"""Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive.""",2024-02-27T23:12:45Z
Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning,Yes.,1.,"""using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan.""",2024-02-27T23:06:53Z
LLM-Resistant Math Word Problem Generation via Adversarial Attacks,Yes.,5.,"""We identify shared vulnerabilities among LLMs and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure, offering a nuanced view into model's limitation.""",2024-02-27T22:07:52Z
A Language Model based Framework for New Concept Placement in Ontologies,Yes.,3.,"""Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance.""",2024-02-27T21:27:35Z
"Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents",Yes.,3.,"""Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs)."" and ""good performance on these benchmarks provides a false sense of security.""",2024-02-27T21:27:16Z
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability,Yes.,1.,"""This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks.""",2024-02-27T21:01:41Z
BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra,Yes.,5.,"""due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting.""",2024-02-27T20:48:24Z
Automated Statistical Model Discovery with Language Models,,,,2024-02-27T20:33:22Z
Deep Learning Detection Method for Large Language Models-Generated Scientific Content,Yes.,2.,"""LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications.""",2024-02-27T19:16:39Z
Self-Refinement of Language Models from External Proxy Metrics Feedback,Yes.,1.,"""In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response.""",2024-02-27T19:13:01Z
Prediction-Powered Ranking of Large Language Models,,,,2024-02-27T19:00:01Z
The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,Yes.,1.,"""Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs).""",2024-02-27T18:56:19Z
Massive Activations in Large Language Models,Yes.,3.,"""We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others... these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output.""",2024-02-27T18:55:17Z
Evaluating Very Long-Term Conversational Memory of LLM Agents,Yes.,5.,"""Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.""",2024-02-27T18:42:31Z
Tower: An Open Multilingual Large Language Model for Translation-Related Tasks,Yes.,1.,"""While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task.""",2024-02-27T18:09:36Z
AmbigNLG: Addressing Task Ambiguity in Instruction for NLG,Yes.,5.,"""their performance is significantly hindered by the ambiguity present in real-world instructions.""",2024-02-27T17:52:33Z
Case-Based or Rule-Based: How Do Transformers Do the Math?,Yes.,5.,"""modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition.""",2024-02-27T17:41:58Z
NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents,Yes.,5.,"""they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism.""",2024-02-27T16:56:30Z
Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs,Yes.,3.,"""Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning.""",2024-02-27T16:19:37Z
SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation,Yes.,1.,"""We present SongComposer, an innovative LLM designed for song composition.""",2024-02-27T16:15:28Z
Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data,Yes.,5.,"""Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously.""",2024-02-27T16:15:03Z
Variational Learning is Effective for Large Deep Networks,Yes.,1.,"""We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data.""",2024-02-27T16:11:05Z
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization,Yes.,3.,"""Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games.""",2024-02-27T15:09:20Z
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation,Yes.,1.,"""DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropagation.""",2024-02-27T14:51:11Z
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,Yes.,5.,"""The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents.""",2024-02-27T14:47:53Z
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space,Yes.,5.,"""Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge.""",2024-02-27T14:45:04Z
Retrieval is Accurate Generation,Yes.,1.,"""Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation.""",2024-02-27T14:16:19Z
Predict the Next Word: Humans exhibit uncertainty in this task and language models _____,Yes.,5.,"""We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.""",2024-02-27T14:11:32Z
BASES: Large-scale Web Search User Simulation with Large Language Model based Agents,Yes.,1.,"""Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation.""",2024-02-27T13:44:09Z
Intensive Care as One Big Sequence Modeling Problem,Yes.,1.,"""previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning.""",2024-02-27T13:36:55Z
REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering,Yes.,5.,"""LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).""",2024-02-27T13:22:51Z
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?,Yes.,1.,"""Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.""",2024-02-27T13:18:00Z
Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles,Yes.,5.,"""Results showed that GPT-4's performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models fine-tuned on the dataset for propaganda detection at different classification granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4",2024-02-27T13:02:19Z
Training-Free Long-Context Scaling of Large Language Models,Yes.,5.,"""The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.""",2024-02-27T12:39:23Z
DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning,Yes.,3.,"""existing LLM agents are hindered by generating unreasonable experiment plans within this scenario.""",2024-02-27T12:26:07Z
Deep Learning Based Named Entity Recognition Models for Recipes,,,,2024-02-27T12:03:56Z
Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder,Yes.,2.,"""EEG-based language decoding is still in its nascent stages, facing several technical issues such as",2024-02-27T11:45:21Z
Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective,Yes.,5.,"""there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive",2024-02-27T11:02:12Z
Investigating Continual Pretraining in Large Language Models: Insights and Implications,Yes.,3.,"""Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting,"" and ""smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning.""",2024-02-27T10:47:24Z
Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies,Yes.,3.,"""At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution.""",2024-02-27T10:44:52Z
Determinants of LLM-assisted Decision-Making,Yes.,2.,"""understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions.""",2024-02-27T10:24:50Z
LLMGuard: Guarding Against Unsafe LLM Behavior,Yes.,5.,"""it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns.""",2024-02-27T10:22:45Z
SoFA: Shielded On-the-fly Alignment via Priority Rule Following,Yes.,5.,"""even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.""",2024-02-27T09:52:27Z
RECOST: External Knowledge Guided Data-efficient Instruction Tuning,Yes.,3.,"""Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples.""",2024-02-27T09:47:36Z
Probing Multimodal Large Language Models for Global and Local Semantic Representations,Yes.,3.,"""We find that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information.""",2024-02-27T08:27:15Z
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese,Yes.,4.,"""Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages.""",2024-02-27T08:24:32Z
Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning,Yes.,3.,"""However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning.""",2024-02-27T07:14:12Z
Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue,Yes.,5.,"""Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.""",2024-02-27T07:11:59Z
Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection,Yes.,5.,"""We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including injecting domain knowledge, strengthening knowledge transfer from IND(In-domain) to OOD,",2024-02-27T07:02:10Z
MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning,Yes.,3.,"""We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH).""",2024-02-27T05:50:35Z
Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models,,,,2024-02-27T05:37:10Z
Measuring Vision-Language STEM Skills of Neural Models,Yes.,5.,"""Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance.""",2024-02-27T04:55:03Z
"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",Yes.,3.,"""our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited.""",2024-02-27T04:18:49Z
"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",No.,1.,"The abstract focuses on Sora, a text-to-video generative AI model, and does not discuss language models (LLMs or LMs).",2024-02-27T03:30:58Z
Benchmarking Data Science Agents,Yes.,3.,"""Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process.""",2024-02-27T03:03:06Z
Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation,Yes.,3.,"""While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations.""",2024-02-27T02:16:07Z
Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models,Yes.,3.,"""we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances.""",2024-02-27T01:37:23Z
Creating Suspenseful Stories: Iterative Planning with Large Language Models,Yes.,3.,"""state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.""",2024-02-27T01:25:52Z
Sinkhorn Distance Minimization for Knowledge Distillation,Yes.,3.,"""However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student."" and ""we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and",2024-02-27T01:13:58Z
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses,Yes.,5.,"""Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios.""",2024-02-27T00:22:18Z
Adapting to Teammates in a Cooperative Language Game,Yes.,2.,"""Previous approaches to designing agents for this game have utilized a single internal language model to determine action choices. This often leads to good performance with some teammates and inferior performance with other teammates, as the agent cannot adapt to any specific teammate.""",2024-02-26T23:15:07Z
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling,Yes.,1.,"""In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts.""",2024-02-26T20:56:06Z
Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset,No.,1.,The abstract does not mention LLMs or any kind of language models.,2024-02-26T20:42:40Z
Pandora's White-Box: Increased Training Data Leakage in Open LLMs,Yes.,5.,"""these findings show that highly effective MIAs are available in almost all LLM training settings, and highlight that great care must be taken before LLMs are fine-tuned on highly sensitive data and then deployed.""",2024-02-26T20:41:50Z
Can Large Language Models Recall Reference Location Like Humans?,Yes.,1.,"""This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position.""",2024-02-26T20:35:32Z
Benchmarking LLMs on the Semantic Overlap Summarization Task,Yes.,3.,"""We conclude the paper by analyzing the strengths and limitations of various LLMs in terms of their capabilities in capturing overlapping information.""",2024-02-26T20:33:50Z
Algorithmic Arbitrariness in Content Moderation,Yes.,4.,"""We analyze (i) the extent of predictive multiplicity among state-of-the-art LLMs used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications.""",2024-02-26T19:27:00Z
A Survey of Large Language Models in Cybersecurity,Yes.,5.,"""This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.""",2024-02-26T19:06:02Z
WIPI: A New Web Threat for LLM-Driven Web Agents,Yes.,2.,"""an essential and pressing question arises",2024-02-26T19:01:54Z
"Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",Yes.,5.,"""However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications.""",2024-02-26T18:59:28Z
MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,Yes.,5.,"""However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency.""",2024-02-26T18:59:03Z
Do Large Language Models Latently Perform Multi-Hop Reasoning?,Yes.,3.,"""However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop.""",2024-02-26T18:57:54Z
Eight Methods to Evaluate Robust Unlearning in LLMs,Yes.,4.,"""we first survey techniques and limitations of existing unlearning evaluations"" and ""Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.""",2024-02-26T18:57:37Z
Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections,Yes.,2.,"""off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications.""",2024-02-26T18:56:48Z
A Survey on Data Selection for Language Models,Yes.,2.,"""naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary.""",2024-02-26T18:54:35Z
Language Agents as Optimizable Graphs,Yes.,1.,"""Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases.""",2024-02-26T18:48:27Z
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts,Yes.,3.,"""Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations.""",2024-02-26T18:47:27Z
A Surprising Failure? Multimodal LLMs and the NLVR Challenge,Yes.,5.,"""Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.""",2024-02-26T18:37:18Z
OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA),Yes.,1.,"""However, there is limited research on LLMs specifically addressing oncology-related queries.""",2024-02-26T18:33:13Z
"If in a Crowdsourced Data Annotation Pipeline, a GPT-4",Yes.,1.,"""Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy.""",2024-02-26T18:08:52Z
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models,Yes.,5.,"""most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness.""",2024-02-26T18:00:49Z
A Comprehensive Evaluation of Quantization Strategies for Large Language Models,Yes.,4.,"""Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings."" and ""Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs.""",2024-02-26T17:45:36Z
CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models,Yes.,2.,"""Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs).""",2024-02-26T16:35:59Z
SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection,Yes.,1.,"""Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions.""",2024-02-26T16:21:53Z
Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models,Yes.,3.,"""the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets.""",2024-02-26T16:11:03Z
HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization,Yes.,2.,"""These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs.""",2024-02-26T16:09:00Z
Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study,Yes.,5.,"""they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes.""",2024-02-26T16:05:33Z
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding,Yes.,4.,"""Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%.""",2024-02-26T15:47:01Z
ESG Sentiment Analysis: comparing human and language model performance including GPT,Yes.,1.,"""Our study seeks to compare human performance with the cutting edge in machine performance in the measurement of ESG related sentiment.""",2024-02-26T15:22:30Z
LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language,Yes.,3.,"""Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts.""",2024-02-26T15:05:16Z
Long-Context Language Modeling with Parallel Context Encoding,Yes.,5.,"""the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window"" and ""while existing long-context models degenerate with retrieved contexts.""",2024-02-26T14:47:35Z
Multi-Bit Distortion-Free Watermarking for Large Language Models,Yes.,1.,"""Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection.""",2024-02-26T14:01:34Z
Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models,Yes.,1.,"""Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area.""",2024-02-26T13:47:09Z
Aligning Large Language Models to a Domain-specific Graph Database,Yes.,3.,"""Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB.""",2024-02-26T13:46:51Z
"Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup",Yes.,1.,"""Retrieval Augmented Generation (RAG) systems have seen huge popularity in augmenting Large-Language Model (LLM) outputs with domain specific and time sensitive data.""",2024-02-26T12:56:17Z
Integrating Large Language Models with Graphical Session-Based Recommendation,Yes.,3.,"""The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs.""",2024-02-26T12:55:51Z
LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification,Yes.,2.,"""Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models.""",2024-02-26T11:52:55Z
LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments,Yes.,4.,"""showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration.""",2024-02-26T11:31:48Z
On Languaging a Simulation Engine,Yes.,1.,"""Depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request.""",2024-02-26T11:01:54Z
Defending LLMs against Jailbreaking Attacks via Backtranslation,Yes.,4.,"""Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent.""",2024-02-26T10:03:33Z
RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering,Yes.,3.,"""The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly.""",2024-02-26T09:59:04Z
"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",Yes.,4.,"""The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner.""",2024-02-26T09:43:02Z
Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models,Yes.,1.,"""Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.""",2024-02-26T09:36:05Z
RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions,Yes.,3.,"""recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples.""",2024-02-26T09:30:55Z
DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models,Yes.,3.,"""Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture.""",2024-02-26T09:21:59Z
Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models,,,,2024-02-26T09:19:46Z
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy,Yes.,3.,"""Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives.""",2024-02-26T09:10:34Z
From RAGs to riches: Using large language models to write documents for clinical trials,Yes.,5.,"""however there are concerns about the quality of their output"" and ""deficiencies remain",2024-02-26T08:59:05Z
MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property,Yes.,3.,"""Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level.""",2024-02-26T08:27:50Z
Immunization against harmful fine-tuning attacks,Yes.,3.,"""However, this focus overlooks another source of misalignment",2024-02-26T08:08:03Z
Improving LLM-based Machine Translation with Systematic Self-Correction,Yes.,4.,"""However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors.""",2024-02-26T07:58:12Z
Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models,Yes.,1.,"""Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic.""",2024-02-26T07:44:56Z
LLM Inference Unveiled: Survey and Roofline Model Insights,Yes.,3.,"""This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware.""",2024-02-26T07:33:05Z
Language-guided Skill Learning with Temporal Variational Inference,Yes.,1.,"""The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories.""",2024-02-26T07:19:23Z
CodeS: Towards Building Open-source Language Models for Text-to-SQL,,,,2024-02-26T07:00:58Z
Personalized Federated Instruction Tuning via Neural Architecture Search,Yes.,2.,"""The evaluation with multiple LLMs non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in perplexity.""",2024-02-26T06:29:05Z
Data-freeWeight Compress and Denoise for Large Language Models,Yes.,3.,"""Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed.""",2024-02-26T05:51:47Z
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models,Yes.,5.,"""our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings"" and ""instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the LLMs.""",2024-02-26T05:43:51Z
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering,Yes.,3.,"""open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis.""",2024-02-26T05:31:34Z
"PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",Yes.,1.,"""Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task.""",2024-02-26T04:09:53Z
From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto,Yes.,3.,"""We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed.""",2024-02-26T03:10:11Z
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering,Yes.,3.,"""the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer.""",2024-02-26T01:17:50Z
HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs,Yes.,5.,"""Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs.""",2024-02-25T22:23:37Z
Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing,Yes.,3.,"""Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content.""",2024-02-25T20:36:03Z
Attacking LLM Watermarks by Exploiting Their Strengths,Yes.,3.,"""However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks.""",2024-02-25T20:24:07Z
How Can LLM Guide RL? A Value-Based Approach,Yes.,3.,"""recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback.""",2024-02-25T20:07:13Z
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers,Yes.,5.,"""The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks,"" and ""current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned L",2024-02-25T17:43:29Z
ChatMusician: Understanding and Generating Music Intrinsically with LLM,Yes.,3.,"""While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language."" and ""but there remains significant territory to be conquered.""",2024-02-25T17:19:41Z
PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization,Yes.,3.,"""Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices.""",2024-02-25T16:43:41Z
What Generative Artificial Intelligence Means for Terminological Definitions,No.,1.,The abstract discusses Generative Artificial Intelligence (GenAI) tools like ChatGPT in the context of terminological definitions but does not specifically mention LLMs or their limitations.,2024-02-25T16:36:51Z
AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation,Yes.,1.,"""This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces.""",2024-02-25T15:51:05Z
FuseChat: Knowledge Fusion of Chat Models,,,,2024-02-25T15:11:58Z
NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification,Yes.,3.,"""We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation.""",2024-02-25T13:20:13Z
UrbanGPT: Spatio-Temporal Large Language Models,Yes.,1.,"""Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks.""",2024-02-25T12:37:29Z
Citation-Enhanced Generation for LLM-based Chatbots,Yes.,,,2024-02-25T11:24:41Z
Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions,Yes.,4.,"""generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate"" and ""retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval.""",2024-02-25T11:22:19Z
How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study,Yes.,3.,"""only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms."" and ""gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence.""",2024-02-25T11:15:42Z
Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression,Yes.,3.,"""Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference.""",2024-02-25T11:07:08Z
LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding,Yes.,1.,"""Empirical evaluations across two challenging tasks--video question answering and temporal question grounding in videos--using a variety of video-language pretrainings (VLPs) and large language models (LLMs) demonstrate the superior performance, speed, and versatility of our proposed",2024-02-25T10:27:46Z
LLMs with Chain-of-Thought Are Non-Causal Reasoners,Yes.,3.,"""Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa."" and ""highlight discrepancies between LLM and human reasoning processes.""",2024-02-25T10:13:04Z
Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy,Yes.,3.,"""machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues.""",2024-02-25T09:44:56Z
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings,Yes.,1.,"""This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments.""",2024-02-25T09:41:50Z
Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations,Yes.,3.,"""In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy.""",2024-02-25T09:19:11Z
Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration,Yes.,3.,"""recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives.""",2024-02-25T08:45:10Z
GraphWiz: An Instruction-Following Language Model for Graph Problems,Yes.,2.,"""Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored."" and ""highlighting the potential for overfitting with increased data.""",2024-02-25T08:41:32Z
LoRA Meets Dropout under a Unified Framework,Yes.,2.,"""a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked.""",2024-02-25T07:09:10Z
From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings,,,,2024-02-25T06:46:27Z
Likelihood-based Mitigation of Evaluation Bias in Large Language Models,Yes.,4.,"""However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation",2024-02-25T04:52:02Z
Cognitive Bias in High-Stakes Decision-Making with LLMs,Yes.,4.,"""LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance.""",2024-02-25T02:35:56Z
Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale,Yes.,2.,"""Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators.""",2024-02-25T02:22:34Z
GreenLLaMA: A Framework for Detoxification with Explanations,Yes.,2.,"""Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified",2024-02-25T01:56:47Z
Bootstrapping Cognitive Agents with a Large Language Model,Yes.,3.,"""Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune.""",2024-02-25T01:40:30Z
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step,Yes.,3.,"""However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations.""",2024-02-25T00:56:27Z
Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware,Yes.,5.,"""The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges.""",2024-02-25T00:53:16Z
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models,Yes.,5.,"""Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination."" and ""detecting and mitigating data contamination for LLMs faces significant challenges.""",2024-02-24T23:54:41Z
Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency,Yes.,4.,"""this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency"" and ""Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures.""",2024-02-24T23:17:56Z
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs,Yes.,2.,"""However, traditional studies do not provide formal guarantees on the performance of LLMs.""",2024-02-24T23:16:57Z
Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis,Yes.,5.,"""The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm.""",2024-02-24T21:36:26Z
PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails,Yes.,5.,"""Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content."" and ""Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective.""",2024-02-24T21:27:13Z
Multimodal Instruction Tuning with Conditional Mixture of LoRA,Yes.,3.,"""However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks.""",2024-02-24T20:15:31Z
SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection,Yes.,1.,"""Detection of machine-generated text is becoming an increasingly important task, with the advent of large language models (LLMs).""",2024-02-24T17:44:56Z
SportQA: A Benchmark for Sports Understanding in Large Language Models,Yes.,3.,"""Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise.""",2024-02-24T17:12:10Z
MATHWELL: Generating Educational Math Word Problems at Scale,Yes.,1.,"""We suggest that language models can support K-8 math education by automatically generating problems at scale.""",2024-02-24T17:08:45Z
NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation,Yes.,1.,"""we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap.""",2024-02-24T16:39:16Z
Prompt Perturbation Consistency Learning for Robust Language Models,Yes.,5.,"""their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts.""",2024-02-24T15:00:58Z
Linguistic Intelligence in Large Language Models for Telecommunications,Yes.,2.,"""We also observe that no single LLM consistently outperforms others, and the performance of different LLMs can fluctuate. Although their performance lags behind fine-tuned models, our findings underscore the potential of LLMs as a valuable resource for understanding various aspects of this field that lack large annotated data.""",2024-02-24T14:01:07Z
PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA,Yes.,1.,"""With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods.""",2024-02-24T13:39:05Z
Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method,Yes.,3.,"""We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance.""",2024-02-24T13:36:58Z
OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining,Yes.,3.,"""Extensive experiments reveal that even advanced algorithms like large language models (LLMs) encounter difficulties in addressing key challenges in certain tasks, such as paper source tracing and scholar profiling.""",2024-02-24T13:15:54Z
Empowering Large Language Model Agents through Action Learning,Yes.,5.,"""Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior.""",2024-02-24T13:13:04Z
Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models,Yes.,2.,"""LLMs promise to revolutionize society, yet training these foundational models poses immense challenges.""",2024-02-24T12:31:22Z
"From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",Yes.,4.,"""Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks."" and ""our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs.""",2024-02-24T09:06:25Z
Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models,Yes.,4.,"""Large language models (LLMs) still grapple with complex tasks like mathematical reasoning.""",2024-02-24T08:40:30Z
Stepwise Self-Consistent Mathematical Reasoning with Large Language Models,Yes.,4.,"""Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions.""",2024-02-24T08:22:39Z
Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation,Yes.,1.,"""TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images.""",2024-02-24T08:10:54Z
Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens,Yes.,3.,"""their widespread application is hindered by the resource-intensive decoding process"" and ""the accuracy of these decoding heads falls short of the auto-regressive decoding approach.""",2024-02-24T08:10:39Z
HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition,Yes.,4.,"""the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""",2024-02-24T08:01:32Z
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning,Yes.,3.,"""However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks.""",2024-02-24T07:22:04Z
MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation,Yes.,1.,"""we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements.""",2024-02-24T06:14:34Z
How Do Humans Write Code? Large Models Do It the Same Way Too,Yes.,3.,"""Large Language Models (LLMs) often make errors when performing numerical calculations"" and ""we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language.""",2024-02-24T05:40:01Z
LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper,Yes.,2.,"""the defensive side has been relatively less explored.""",2024-02-24T05:34:43Z
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models,Yes.,3.,"""Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions.""",2024-02-24T05:14:52Z
Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors,Yes.,3.,"""The primary challenges are catastrophic forgetting and overfitting.""",2024-02-24T04:32:44Z
Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology,Yes.,5.,"""Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks.""",2024-02-24T02:27:55Z
Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study,Yes.,4.,"""The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels",2024-02-24T00:38:29Z
Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics,Yes.,5.,"""demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task.""",2024-02-24T00:01:01Z
On Trojan Signatures in Large Language Models of Code,Yes.,3.,"""Our results suggest that trojan signatures could not generalize to LLMs of code.""",2024-02-23T22:48:29Z
Fine-Grained Self-Endorsement Improves Factuality and Reasoning,Yes.,2.,"""This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations.""",2024-02-23T22:24:40Z
"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",Yes.,2.,"""Training LLMs at this scale brings unprecedented challenges to training efficiency and stability.""",2024-02-23T22:10:59Z
Towards Efficient Active Learning in NLP via Pretrained Representations,Yes.,1.,"""Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications.""",2024-02-23T21:28:59Z
"Selective ""Selective Prediction"": Reducing Unnecessary Abstention in Vision-Language Reasoning",Yes.,2.,"""Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain."" and ""ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if",2024-02-23T21:16:52Z
Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis,Yes.,1.,"""Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers.""",2024-02-23T21:07:20Z
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts,Yes.,1.,"""Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting.""",2024-02-23T20:14:16Z
DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures,Yes.,4.,"""Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web",2024-02-23T20:10:18Z
CI w/o TN: Context Injection without Task Name for Procedure Planning,Yes.,5.,"""we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information.""",2024-02-23T19:34:47Z
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning,Yes.,3.,"""However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories.""",2024-02-23T18:56:26Z
Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts,No.,1.,The abstract does not mention LLMs or any specific language models.,2024-02-23T18:56:11Z
Self-Retrieval: Building an Information Retrieval System with One Large Language Model,Yes.,1.,"""Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models.""",2024-02-23T18:45:35Z
The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),Yes.,4.,"""Whereas extensive research has demonstrated the privacy risks of large language models (LLMs),"" and ""posing new privacy issues that are currently under-explored.""",2024-02-23T18:35:15Z
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs,Yes.,1.,"""There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks.""",2024-02-23T18:30:49Z
Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models,Yes.,4.,"""The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability."" and ""we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently",2024-02-23T18:15:56Z
Repetition Improves Language Model Embeddings,Yes.,3.,"""we address an architectural limitation of autoregressive models",2024-02-23T17:25:10Z
PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning,Yes.,1.,"""we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans.""",2024-02-23T16:30:05Z
Explorations of Self-Repair in Language Models,Yes.,4.,"""We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect).""",2024-02-23T15:42:12Z
Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction,Yes.,3.,"""However, these approaches lack mission performance and safety guarantees.""",2024-02-23T15:02:44Z
Farsight: Fostering Responsible AI Awareness During AI Application Prototyping,Yes.,3.,"""However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping.""",2024-02-23T14:38:05Z
Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies,Yes.,3.,"""Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare.""",2024-02-23T14:17:01Z
GPTVQ: The Blessing of Dimensionality for LLM Quantization,Yes.,1.,"""GPTVQ establishes a new state-of-the-art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral.""",2024-02-23T13:39:16Z
ArabianGPT: Native Arabic GPT-based Large Language Model,Yes.,3.,"""The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax.""",2024-02-23T13:32:47Z
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries,Yes.,5.,"""Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation.""",2024-02-23T13:03:12Z
Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models,Yes.,1.,"""The advance of large language models (LLMs) provides opportunities to address these problems.""",2024-02-23T13:02:10Z
CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora,Yes.,3.,"""Although Multimodal Large Language Models (MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in handling large-scale, diverse, and ambiguous real-world needs of retrieval, due to the computation cost and the injective embeddings they produce.""",2024-02-23T11:47:16Z
"CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models",Yes.,3.,"""However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs.""",2024-02-23T11:25:17Z
DEEM: Dynamic Experienced Expert Modeling for Stance Detection,Yes.,3.,"""considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis.""",2024-02-23T11:24:00Z
Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues,Yes.,2.,"""We assess the impact of this addition by testing two models",2024-02-23T10:27:42Z
GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?,Yes.,2.,"""Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training."" and ""To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs).""",2024-02-23T10:02:01Z
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition,Yes.,2.,"""Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences.""",2024-02-23T09:29:19Z
Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing,Yes.,2.,"""However, realizing this vision involves addressing several socio-technical and practical research challenges.""",2024-02-23T09:06:25Z
Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models,Yes.,4.,"""these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility.""",2024-02-23T09:04:48Z
DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators,Yes.,3.,"""This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts.""",2024-02-23T09:01:00Z
GraphEdit: Large Language Models for Graph Structure Learning,Yes.,1.,"""By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning.""",2024-02-23T08:29:42Z
Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer,Yes.,3.,"""Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process.""",2024-02-23T08:11:55Z
Machine Unlearning of Pre-trained Large Language Models,Yes.,2.,"""We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area.""",2024-02-23T07:43:26Z
Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,,,,2024-02-23T07:21:32Z
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models,Yes.,3.,"""fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge.""",2024-02-23T06:32:18Z
AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System,Yes.,2.,"""However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents.""",2024-02-23T06:25:20Z
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,Yes.,3.,"""However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications.""",2024-02-23T06:11:50Z
Large Multimodal Agents: A Survey,Yes.,2.,"""One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs.""",2024-02-23T06:04:23Z
Executing Natural Language-Described Algorithms with Large Language Models: An Investigation,Yes.,3.,"""Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved.""",2024-02-23T05:31:36Z
A First Look at GPT Apps: Landscape and Vulnerability,Yes.,4.,"""LLMs' susceptibility to attacks raises concerns over safety and plagiarism.""",2024-02-23T05:30:32Z
Studying LLM Performance on Closed- and Open-source Data,Yes.,5.,"""These models are extremely data-hungry,"" ""do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds?"" and ""We find that performance for C# changes little from",2024-02-23T05:17:28Z
Evaluating the Performance of ChatGPT for Spam Email Detection,Yes.,3.,"""Though extensive experiments, the performance of ChatGPT is significantly worse than deep supervised learning methods in the large English dataset, while it presents superior performance on the low-resourced Chinese dataset, even outperforming BERT in this case.""",2024-02-23T04:52:08Z
AttributionBench: How Hard is Automatic Attribution Evaluation?,Yes.,5.,"""our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.""",2024-02-23T04:23:33Z
Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models,Yes.,1.,"""We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection.""",2024-02-23T02:58:12Z
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions,Yes.,4.,"""they often display a considerable level of overconfidence even when the question does not have a definitive answer"" and ""avoid providing hallucinated answers to these unknown questions.""",2024-02-23T02:24:36Z
Fine-tuning Large Language Models for Domain-specific Machine Translation,Yes.,5.,"""Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain",2024-02-23T02:24:15Z
On the Multi-turn Instruction Following for Conversational Web Agents,Yes.,3.,"""To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks.""",2024-02-23T02:18:12Z
ToMBench: Benchmarking Theory of Mind in Large Language Models,Yes.,5.,"""We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicating that LLMs have not achieved a human-level theory of mind yet.""",2024-02-23T02:05:46Z
Unlocking the Power of Large Language Models for Entity Alignment,Yes.,1.,"""To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy.""",2024-02-23T01:55:35Z
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models,Yes.,5.,"""Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness."" and ""We also reveal that data contamination brings no contribution or even negative effect to models' real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.""",2024-02-23T01:30:39Z
CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models,Yes.,3.,"""Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways.""",2024-02-22T23:42:25Z
Unintended Impacts of LLM Alignment on Global Representation,Yes.,4.,"""We explore how alignment impacts performance along three axes of global representation",2024-02-22T23:31:22Z
Divide-or-Conquer? Which Part Should You Distill Your LLM?,Yes.,3.,"""However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization.""",2024-02-22T22:28:46Z
tinyBenchmarks: evaluating LLMs with fewer examples,Yes.,1.,"""The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities.""",2024-02-22T22:05:23Z
Optimizing Language Models for Human Preferences is a Causal Inference Problem,Yes.,1.,"""As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences.""",2024-02-22T21:36:07Z
AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation,Yes.,1.,"""This paper explores twofold aspects of integrating LLMs into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas.""",2024-02-22T21:34:52Z
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data,Yes.,2.,"""These benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation.""",2024-02-22T21:22:04Z
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment,Yes.,3.,"""these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases"" and ""this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack).""",2024-02-22T21:05:18Z
Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning,Yes.,4.,"""recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback.""",2024-02-22T20:57:17Z
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning,Yes.,3.,"""Our findings reveal",2024-02-22T18:59:02Z
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Yes.,1.,"""This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns.""",2024-02-22T18:58:55Z
RelayAttention for Efficient Large Language Model Serving with Long System Prompts,Yes.,5.,"""However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length.""",2024-02-22T18:58:28Z
Identifying Multiple Personalities in Large Language Models with External Evaluation,Yes.,4.,"""Yet many critiques question the applicability and reliability of these self-assessment tests when applied to LLMs."" and ""This shows that LLMs can exhibit different personalities based on different scenarios, thus highlighting a fundamental difference between personality in LLMs and humans.""",2024-02-22T18:57:20Z
Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,Yes.,1.,"""Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes.""",2024-02-22T18:56:07Z
Watermarking Makes Language Models Radioactive,Yes.,1.,"This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data.",2024-02-22T18:55:22Z
Zero-shot cross-lingual transfer in instruction tuning of large language model,Yes.,3.,"""English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.""",2024-02-22T18:37:33Z
DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models,Yes.,3.,"""Current MLLMs typically singularly focus on inputs at a predefined resolution, resulting in deficiencies in detailed questions involving local regions.""",2024-02-22T18:26:02Z
MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues,Yes.,4.,"""comprehensively evaluating the dialogue abilities of LLMs remains a challenge"" and ""neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs.""",2024-02-22T18:21:59Z
Generalizing Reward Modeling for Out-of-Distribution Preference Learning,Yes.,3.,"""Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging.""",2024-02-22T18:20:33Z
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs,Yes.,3.,"""Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases."" and ""We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias.""",2024-02-22T18:14:09Z
Scaling Efficient LLMs,Yes.,1.,"""Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency.""",2024-02-22T18:06:19Z
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation,Yes.,1.,"""LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks.""",2024-02-22T18:03:14Z
Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs,Yes.,3.,"""Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning.""",2024-02-22T17:52:34Z
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images,Yes.,5.,"""they are still vulnerable to adversarial images"" and ""lack of study regarding MLLMs' adversarial robustness with CoT"" and ""finding that CoT marginally improves adversarial robustness against existing attack methods"" and ""introduce a novel",2024-02-22T17:36:34Z
Chain-of-Thought Unfaithfulness as Disguised Accuracy,Yes.,5.,"""We discover that simply changing the order of answer choices in the prompt can reduce the metric by 73 percentage points. The faithfulness metric is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about its validity as a construct for evaluating faithfulness.""",2024-02-22T17:23:53Z
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus,Yes.,3.,"""Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE).""",2024-02-22T17:11:38Z
An LLM-Enhanced Adversarial Editing System for Lexical Simplification,Yes.,1.,"""Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system.""",2024-02-22T17:04:30Z
Unveiling Linguistic Regions in Large Language Models,Yes.,3.,"""freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs.""",2024-02-22T16:56:13Z
UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models,Yes.,5.,"""Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination."" and ""Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources",2024-02-22T16:45:32Z
Visual Hallucinations of Multi-modal Large Language Models,Yes.,5.,"""Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances."" and ""We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 halluc",2024-02-22T16:40:33Z
Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality,Yes.,3.,"""Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual 'behavior', examining the extent to which these models can emulate human-like personality patterns.""",2024-02-22T16:32:08Z
ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models,Yes.,5.,"""we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones.""",2024-02-22T16:06:49Z
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement,,,,2024-02-22T16:06:23Z
LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition,Yes.,3.,"""their performance on information extraction tasks is still not entirely satisfactory"" and ""to overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text.""",2024-02-22T14:19:56Z
LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey,Yes.,2.,"""our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context.""",2024-02-22T13:52:02Z
"Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard",Yes.,1.,"""Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality.""",2024-02-22T13:25:17Z
Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance,Yes.,1.,"""We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs).""",2024-02-22T13:24:10Z
Balanced Data Sampling for Language Model Training with Clustering,Yes.,2.,"""Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal.""",2024-02-22T13:20:53Z
Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond,Yes.,3.,"""However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner.""",2024-02-22T13:13:31Z
"""My Answer is C"": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",Yes.,5.,"""Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%.""",2024-02-22T12:47:33Z
LLMBind: A Unified Modality-Task Integration Framework,Yes.,3.,"""they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field.""",2024-02-22T12:36:31Z
Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis,Yes.,5.,"""Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance",2024-02-22T12:19:04Z
Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?,Yes.,1.,"""This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text.""",2024-02-22T11:16:23Z
COBIAS: Contextual Reliability in Bias Assessment,Yes.,4.,"""Large Language Models (LLMs) are trained on inherently biased data"" and ""highlighting a critical need for contextual exploration.""",2024-02-22T10:46:11Z
Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph,Yes.,1.,"""Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses.""",2024-02-22T10:12:16Z
Uncertainty-Aware Evaluation for Vision-Language Models,Yes.,3.,"""Our empirical findings also reveal a correlation between model uncertainty and its language model part.""",2024-02-22T10:04:17Z
Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching,Yes.,2.,"""a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models.""",2024-02-22T09:49:26Z
Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning,Yes.,5.,"""However, the existing LLM-based model exhibits three shortcomings",2024-02-22T08:51:39Z
Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction,Yes.,2.,"""An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches.""",2024-02-22T08:26:56Z
OpenTab: Advancing Large Language Models as Open-domain Table Reasoners,Yes.,3.,"""Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously."" and ""existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes.""",2024-02-22T08:01:01Z
Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark,Yes.,3.,"""LLMs have limitations in learning from in-context information in scientific domains.""",2024-02-22T07:58:29Z
"Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",Yes.,3.,"""Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning.""",2024-02-22T07:55:26Z
INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models,,,,2024-02-22T06:59:50Z
Understanding and Patching Compositional Reasoning in LLMs,Yes.,5.,"""LLMs have marked a revolutionary shift, yet they falter when faced with compositional reasoning tasks.""",2024-02-22T06:47:56Z
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering,Yes.,2.,"""However, their use in answering questions from knowledge bases remains largely unexplored.""",2024-02-22T06:23:37Z
Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge,Yes.,3.,"""Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes.""",2024-02-22T05:58:03Z
Mitigating Biases of Large Language Models in Stance Detection with Calibration,Yes.,4.,"""LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance.""",2024-02-22T05:17:49Z
Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education,Yes.,1.,"""This study investigates LLMs' capabilities in educational scenarios, focusing on concept graph recovery and question-answering (QA).""",2024-02-22T05:15:27Z
CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations,Yes.,1.,"""Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text.""",2024-02-22T05:07:31Z
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,Yes.,2.,"""However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners.""",2024-02-22T04:55:14Z
Can Language Models Act as Knowledge Bases at Scale?,Yes.,5.,"""However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable.""",2024-02-22T04:20:14Z
Qsnail: A Questionnaire Dataset for Sequential Question Generation,Yes.,5.,"""Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires.""",2024-02-22T04:14:10Z
Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization,Yes.,2.,"""a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data.""",2024-02-22T04:10:57Z
Can Large Language Models Detect Misinformation in Scientific News Reporting?,Yes.,1.,"""The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting.""",2024-02-22T04:07:00Z
Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,Yes.,3.,"""utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance.""",2024-02-22T03:51:34Z
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond,Yes.,1.,"""we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers.""",2024-02-22T03:46:08Z
Eagle: Ethical Dataset Given from Real Interactions,Yes.,5.,"""Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content.""",2024-02-22T03:46:02Z
Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models,Yes.,3.,"""However, aligning the natural language text instructions generated by LLMs with the vectorized operations required for execution presents a significant challenge, often necessitating task-specific details.""",2024-02-22T03:14:03Z
COPR: Continual Human Preference Learning via Optimal Policy Regularization,Yes.,2.,"""Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs.""",2024-02-22T02:20:08Z
Content Conditional Debiasing for Fair Text Embedding,Yes.,2.,"""we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups.""",2024-02-22T01:20:51Z
Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models,Yes.,3.,"""The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.""",2024-02-22T01:20:17Z
Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Yes.,5.,"""state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.""",2024-02-22T01:02:37Z
Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models,Yes.,5.,"""existing work shows that it is challenging for LLMs to integrate structured data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand long text data or select the most relevant evidence prior to inference, and both approaches are not trivial.""",2024-02-22T00:41:23Z
Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization,Yes.,5.,"""Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability."" and ""Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general",2024-02-22T00:01:02Z
Understanding the Dataset Practitioners Behind Large Language Model Development,Yes.,2.,"""We find that although data quality is a top priority, there is little consensus around what data quality is and how to evaluate it.""",2024-02-21T23:50:37Z
Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media,Yes.,2.,"""Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of LLM and MMT in news translation procedures.""",2024-02-21T23:43:04Z
Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement,Yes.,1.,"""Speculative decoding is an inference-acceleration method for large language models (LLMs) where a small language model generates a draft-token sequence which is further verified by the target LLM in parallel.""",2024-02-21T22:57:49Z
TOOLVERIFIER: Generalization to New Tools via Self-Verification,Yes.,3.,"""language models still struggle with learning how to robustly use new tools from only a few demonstrations.""",2024-02-21T22:41:38Z
Automatic Histograms: Leveraging Language Models for Text Dataset Exploration,Yes.,1.,"""We present AutoHistograms, a visualization tool leveraging LLMs.""",2024-02-21T22:29:16Z
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms,Yes.,4.,"""MLLMs have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation."" and ""Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks.""",2024-02-21T22:27:40Z
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives,Yes.,3.,"""No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.""",2024-02-21T22:22:30Z
Driving Generative Agents With Their Personality,Yes.,1.,"""The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters.""",2024-02-21T21:29:57Z
"FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models",,,,2024-02-21T20:30:45Z
EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy,Yes.,1.,"""the extracted information from EHRs using a pre-trained large language model (LLM),""",2024-02-21T19:49:12Z
Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns,Yes.,1.,"""we exploit an LLM by utilizing the knowledge obtained from the graph learning model for interpretation.""",2024-02-21T19:36:24Z
Coercing LLMs to do and reveal (almost) anything,Yes.,5.,"""adversarial attacks on large language models (LLMs) can 'jailbreak' the model into making harmful statements"" and ""we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.""",2024-02-21T18:59:13Z
Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment,Yes.,5.,"""no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs,"" ""both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks,"" and ""Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before",2024-02-21T18:55:20Z
OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,Yes.,5.,"""Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies.""",2024-02-21T18:49:26Z
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models,Yes.,5.,"""Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.""",2024-02-21T18:48:38Z
Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models,,,,2024-02-21T18:40:24Z
What's in a Name? Auditing Large Language Models for Race and Gender Bias,Yes.,5.,"""We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4."" and ""We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women."" and ""Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for harm against marginalized communities.""",2024-02-21T18:25:25Z
Analysing The Impact of Sequence Composition on Language Model Pre-Training,Yes.,3.,"""applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks.""",2024-02-21T18:23:16Z
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,,,,2024-02-21T17:23:59Z
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation,Yes.,2.,"""This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands.""",2024-02-21T17:20:38Z
Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content,Yes.,5.,"""The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.""",2024-02-21T16:46:36Z
Exploring ChatGPT and its Impact on Society,Yes.,2.,"""However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology.""",2024-02-21T16:44:35Z
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization,Yes.,4.,"""Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.""",2024-02-21T16:33:22Z
Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?,Yes.,3.,"""However, the impact of factors beyond training data size on translation performance remains a topic of debate, especially concerning languages not directly encountered during training.""",2024-02-21T16:32:38Z
Calibrating Large Language Models with Sample Consistency,Yes.,3.,"""However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale.""",2024-02-21T16:15:20Z
Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models,Yes.,5.,"""This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations."" and ""We illustrate that these probability-based approaches do not effectively correspond with generative predictions.""",2024-02-21T15:58:37Z
