Title,Talks about LLMs,Rate,Evidence,Published
Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model,Yes.,5.,"""the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development."" and ""We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities.""",2024-02-14T06:24:52Z
Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision,Yes.,1.,"""we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities.""",2024-02-14T06:01:44Z
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data,Yes.,3.,"""As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges.""",2024-02-14T05:57:58Z
Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models,Yes.,5.,"""This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.""",2024-02-14T05:52:23Z
Premise Order Matters in Reasoning with Large Language Models,Yes.,5.,"""we discover a frailty",2024-02-14T04:50:18Z
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences,Yes.,3.,"""we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences.""",2024-02-14T03:56:27Z
Tree-Based Hard Attention with Self-Motivation for Large Language Models,Yes.,3.,"""While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps.""",2024-02-14T00:40:51Z
Large Language Model with Graph Convolution for Recommendation,Yes.,3.,"""existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation.""",2024-02-14T00:04:33Z
GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency,Yes.,4.,"""However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering.""",2024-02-13T23:48:59Z
"eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",Yes.,1.,"""large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields.""",2024-02-13T22:26:24Z
Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy,Yes.,3.,"""However, even LLMs specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications.""",2024-02-13T21:24:21Z
"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions",Yes.,4.,"""identify and understand why LLMs fails"" and ""ChatGPT and LLaMA challenge human expertise, yet do not outperform it for some domains.""",2024-02-13T21:15:33Z
Rethinking Machine Unlearning for Large Language Models,Yes.,3.,"""We highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment.""",2024-02-13T20:51:58Z
"GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",Yes.,5.,"""However, recent work demonstrates that even the best models struggle to identify when and where to refine without access to external feedback."" and ""But they are expensive to train, requiring extensive human annotations.""",2024-02-13T20:16:29Z
Measuring and Controlling Instruction (In)Stability in Language Model Dialogs,Yes.,5.,"""Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal a significant instruction drift within eight rounds of conversations.""",2024-02-13T20:10:29Z
JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models,Yes.,2.,"""Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement.""",2024-02-13T19:54:29Z
LLM-driven Imitation of Subrational Behavior : Illusion or Reality?,Yes.,3.,"""We conclude by discussing the potential benefits, challenges and limitations of our framework.""",2024-02-13T19:46:39Z
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance,Yes.,4.,"""highlighted the critical issue of their tendency to hallucinate non-existing objects in the images"" and ""these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.""",2024-02-13T18:59:05Z
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability,Yes.,3.,"""Jailbreaks on Large language models (LLMs) have recently received increasing attention... Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability.""",2024-02-13T18:58:48Z
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks,Yes.,1.,"""Here we show that this same tradeoff spontaneously emerges with 'in-context learning' (ICL) both in neural networks trained with metalearning and in large language models (LLMs).""",2024-02-13T18:55:27Z
Improving Generalization in Semantic Parsing by Increasing Natural Language Variation,Yes.,3.,"""However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions.""",2024-02-13T18:48:23Z
The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting,Yes.,1.,"""We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health.""",2024-02-13T18:39:36Z
PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs,Yes.,3.,"""Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding.""",2024-02-13T18:39:18Z
Tandem Transformers for Inference Efficient LLMs,Yes.,3.,"""The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations",2024-02-13T18:24:08Z
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages,Yes.,1.,"""offering insights into the capabilities and performance of Large Language Models (LLMs).""",2024-02-13T18:04:53Z
Knowledge Editing on Black-box Large Language Models,Yes.,3.,"""To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses.""",2024-02-13T17:59:34Z
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment,Yes.,3.,"""realistic tasks for agents are multi-step and introduce new challenges",2024-02-13T16:38:01Z
Test-Time Backdoor Attacks on Multimodal Large Language Models,Yes.,1.,"""In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images.""",2024-02-13T16:28:28Z
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,Yes.,5.,"""Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors."" and ""It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost",2024-02-13T16:06:17Z
Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style,Yes.,1.,"""We audited counter-arguments generated by large language models (LLMs), focusing on their ability to generate evidence-based and stylistic counter-arguments to posts from the Reddit ChangeMyView dataset.""",2024-02-13T14:53:12Z
The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale,Yes.,5.,"""ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.""",2024-02-13T14:38:12Z
Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale,Yes.,4.,"""This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff,"" and ""We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.""",2024-02-13T13:50:08Z
Large Language Models as Minecraft Agents,Yes.,2.,"""examining the challenges and opportunities for improvement.""",2024-02-13T11:37:30Z
Punctuation Restoration Improves Structure Understanding without Supervision,Yes.,3.,"""despite impressive generative capabilities of recent large language models, their abilities to capture syntactic or semantic structure within text lag behind.""",2024-02-13T11:22:52Z
Unsupervised Evaluation of Code LLMs with Round-Trip Correctness,Yes.,2.,"""To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains.""",2024-02-13T11:08:08Z
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries,Yes.,2.,"""a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing"" and ""we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time.""",2024-02-13T10:28:57Z
Visually Dehallucinative Instruction Generation,Yes.,4.,"""challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents.""",2024-02-13T10:25:45Z
Eliciting Personality Traits in Large Language Models,Yes.,2.,"""However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these 'black-box' models.""",2024-02-13T10:09:00Z
Prompted Contextual Vectors for Spear-Phishing Detection,Yes.,1.,"""Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance.""",2024-02-13T09:12:55Z
ChatCell: Facilitating Single-Cell Analysis with Natural Language,Yes.,3.,"""High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration.""",2024-02-13T09:06:14Z
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering,Yes.,4.,"""Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability).""",2024-02-13T08:12:48Z
A Survey of Table Reasoning with Large Language Models,Yes.,3.,"""Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research.""",2024-02-13T07:17:52Z
BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT,Yes.,1.,"""To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction.""",2024-02-13T06:02:05Z
Privacy-Preserving Language Model Inference with Instance Obfuscation,Yes.,3.,"""Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page.""",2024-02-13T05:36:54Z
Improving Black-box Robustness with In-Context Rewriting,Yes.,1.,"""We propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function.""",2024-02-13T05:33:35Z
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models,Yes.,3.,"""Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost.""",2024-02-13T05:15:46Z
LLaGA: Large Language and Graph Assistant,Yes.,2.,"""However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language.""",2024-02-13T02:03:26Z
On Limitations of the Transformer Architecture,,,,2024-02-13T01:52:15Z
Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search,Yes.,1.,"""We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.""",2024-02-13T00:55:14Z
On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era,,,,2024-02-12T23:55:55Z
On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks,Yes.,5.,"""While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion."" and ""We observe significant performance collapse with self-critique, significant performance gains with sound external verification,",2024-02-12T23:11:01Z
Addressing cognitive bias in medical language models,Yes.,4.,"""Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.""",2024-02-12T23:08:37Z
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts,Yes.,3.,"""However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions.""",2024-02-12T22:47:57Z
Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,Yes.,5.,"""However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination."" and ""Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to",2024-02-12T22:35:40Z
Grounding Data Science Code Generation with Input-Output Specifications,Yes.,3.,"""However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification.""",2024-02-12T21:32:49Z
Beyond LLMs: Advancing the Landscape of Complex Reasoning,Yes.,5.,"""However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems.""",2024-02-12T21:14:45Z
Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,Yes.,5.,"""Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a",2024-02-12T19:49:58Z
Lumos : Empowering Multimodal LLMs with Scene Text Recognition,Yes.,2.,"""While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference.""",2024-02-12T19:27:26Z
Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs,Yes.,1.,"""In this paper, we introduce refined Direct Preference Optimization (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data.""",2024-02-12T19:10:13Z
Suppressing Pink Elephants with Direct Principle Feedback,,,,2024-02-12T18:57:46Z
WildfireGPT: Tailored Large Language Model for Wildfire Analysis,Yes.,3.,"""LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change.""",2024-02-12T18:41:55Z
Policy Improvement using Language Feedback Models,Yes.,1.,"""To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.""",2024-02-12T18:41:34Z
PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,Yes.,5.,"""Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.""",2024-02-12T18:28:36Z
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,Yes.,1.,"""This study explores the potential of LLMs to augment judgement in forecasting tasks.""",2024-02-12T18:14:43Z
Lissard: Long and Simple Sequential Reasoning Datasets,Yes.,5.,"""Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.""",2024-02-12T18:10:17Z
Mercury: An Efficiency Benchmark for LLM Code Synthesis,Yes.,5.,"""Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for LLM research and development.""",2024-02-12T17:53:22Z
Do Membership Inference Attacks Work on Large Language Models?,Yes.,5.,"""We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.""",2024-02-12T17:52:05Z
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning,Yes.,3.,"""DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD.""",2024-02-12T17:24:15Z
Retrieval-Augmented Thought Process as Sequential Decision Making,Yes.,5.,"""However, several open challenges hinder their wider application",2024-02-12T17:17:50Z
"TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection",Yes.,3.,"""their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs).""",2024-02-12T16:41:54Z
Quantitative knowledge retrieval from large language models,Yes.,2.,"""Implications and challenges of using LLMs as 'experts' are discussed.""",2024-02-12T16:32:37Z
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension,Yes.,4.,"""By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.""",2024-02-12T15:41:22Z
CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity,Yes.,1.,"""Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics.""",2024-02-12T14:53:28Z
"Large Language Models ""Ad Referendum"": How Good Are They at Machine Translation in the Legal Domain?",Yes.,2.,"""The results indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations.""",2024-02-12T14:40:54Z
The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models,Yes.,1.,"""This study explores the potential of Large Language Models (LLMs) to enhance the accuracy of Automatic Speech Recognition (ASR) systems in medical transcription.""",2024-02-12T14:01:12Z
Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models,Yes.,1.,"""We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD.""",2024-02-12T13:34:33Z
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,Yes.,3.,"""To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem.""",2024-02-12T13:13:04Z
Anchor-based Large Language Models,Yes.,3.,"""However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing.""",2024-02-12T12:48:02Z
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping,Yes.,1.,"""our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models.""",2024-02-12T12:30:42Z
BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection,Yes.,3.,"""Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection.""",2024-02-12T10:04:07Z
Secret Collusion Among Generative AI Agents,Yes.,3.,"""While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities.""",2024-02-12T09:31:21Z
T-RAG: Lessons from the LLM Trenches,Yes.,2.,"""making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain.""",2024-02-12T08:45:08Z
Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm,Yes.,2.,"""However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations.""",2024-02-12T08:32:29Z
Pushing The Limit of LLM Capacity for Text Classification,,,,2024-02-12T08:14:03Z
Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch,Yes.,1.,"""The proposed system uses a large language model (LLM) for code generation to interpret and transform natural language commands into behavior branch, a proposed knowledge expression based on behavior trees, which facilitates execution by the game agent.""",2024-02-12T06:49:48Z
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT,Yes.,5.,"""Developing long-context retrieval encoders suitable for these domains raises three challenges",2024-02-12T06:43:52Z
Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples,Yes.,1.,"""we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models.""",2024-02-12T04:59:58Z
DÃ³lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English,,,,2024-02-12T04:50:31Z
Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate,Yes.,4.,"""While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness.""",2024-02-12T04:32:33Z
Exploring Perceptual Limitation of Multimodal Large Language Models,,,,2024-02-12T03:04:42Z
Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning,Yes.,4.,"""the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others"" and ""The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs.""",2024-02-12T01:55:51Z
Differentially Private Training of Mixture of Experts Models,Yes.,2.,"""However, this growth raises significant computational and privacy concerns.""",2024-02-11T23:57:09Z
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,Yes.,1.,"""We find that the mechanistic story behind factual recall is more complex than previously thought.""",2024-02-11T22:58:49Z
ODIN: Disentangled Reward Mitigates Hacking in RLHF,Yes.,3.,"""A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores.""",2024-02-11T22:40:12Z
A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference,Yes.,3.,"""the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.""",2024-02-11T21:44:21Z
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?,Yes.,3.,"""We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty.""",2024-02-11T19:13:26Z
TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation,Yes.,1.,"""This paper presents TransGPT, a novel (multi-modal) large language model for the transportation domain, which consists of two independent variants",2024-02-11T15:50:35Z
Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models,Yes.,2.,"""This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether.""",2024-02-11T13:41:17Z
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning,Yes.,5.,"""However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP.""",2024-02-11T13:30:53Z
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,Yes.,1.,"""Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor.""",2024-02-11T13:24:13Z
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,Yes.,5.,"""We find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers.""",2024-02-11T12:25:41Z
Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy,Yes.,1.,"""facilitated by a large-language model (LLM) to enhance the planning quality"" and ""the LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.""",2024-02-11T11:24:09Z
Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias,Yes.,5.,"""We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic.""",2024-02-11T11:23:28Z
Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces,Yes.,2.,"""inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts.""",2024-02-11T11:03:08Z
Natural Language Reinforcement Learning,Yes.,1.,"""We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4.""",2024-02-11T11:03:04Z
Graph Descriptive Order Improves Reasoning with Large Language Model,Yes.,3.,"""However, the progress in the field of graph reasoning with LLM remains limited."" and ""We discover that the graph reasoning performance of LLMs does not monotonically decrease with the increase in graph size.""",2024-02-11T09:46:24Z
Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education,Yes.,1.,"""In this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge.""",2024-02-11T01:37:48Z
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review,Yes.,4.,"""Despite their transformative potential, challenges persist, including sensitivity to input prompts, occasional misinterpretations, and unforeseen recommendations, necessitating continuous refinement and evolution in LLM-driven recommender systems.""",2024-02-11T00:24:17Z
A Tale of Tails: Model Collapse as a Change of Scaling Laws,Yes.,5.,"""We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning"" of skills, and grokking when mixing human and synthesized data.""",2024-02-10T21:06:34Z
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models,Yes.,2.,"""running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.""",2024-02-10T19:54:08Z
REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models,Yes.,2.,"""address these limitations"" and ""eliminates hallucinations and ensures consistency.""",2024-02-10T18:27:28Z
DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting,Yes.,3.,"""Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports.""",2024-02-10T16:48:45Z
A Thorough Examination of Decoding Methods in the Era of LLMs,Yes.,2.,"""Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.""",2024-02-10T11:14:53Z
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought,,,,2024-02-10T09:51:03Z
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric,Yes.,4.,"""The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset.""",2024-02-10T07:55:27Z
UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction,Yes.,1.,"""UrbanKGent-13B not only can significantly outperform 21 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost.""",2024-02-10T01:50:19Z
"History, Development, and Principles of Large Language Models-An Introductory Survey",Yes.,3.,"""The survey also highlights the limitations of existing work and points out promising future directions.""",2024-02-10T01:18:15Z
ChemLLM: A Chemical Large Language Model,Yes.,3.,"""the direct use of these structured data compromises the model's ability to maintain coherent dialogue.""",2024-02-10T01:11:59Z
Forecasting Events in Soccer Matches Through Language,Yes.,1.,"""a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs).""",2024-02-09T23:02:57Z
The Unreasonable Effectiveness of Eccentric Automatic Prompts,Yes.,3.,"""Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt.""",2024-02-09T22:48:45Z
Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models,No.,1.,The abstract discusses Large Event Models (LEMs) in the context of soccer analytics and does not mention language models (LLMs).,2024-02-09T22:47:25Z
Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing,Yes.,1.,"""This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques.""",2024-02-09T21:37:13Z
Debating with More Persuasive LLMs Leads to More Truthful Answers,Yes.,2.,"""Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data.""",2024-02-09T21:05:01Z
GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding,Yes.,5.,"""their ability to reason over domain-specialized graphs of interconnected entities remains limited"" and ""The answer is no--such capabilities lie beyond current methods.""",2024-02-09T19:53:29Z
EntGPT: Linking Generative Large Language Models with Knowledge Bases,Yes.,3.,"""The ability of Large Language Models (LLMs) to generate factually correct output remains relatively unexplored due to the lack of fact-checking and knowledge grounding during training and inference.""",2024-02-09T19:16:27Z
NICE: To Optimize In-Context Examples or Not?,Yes.,3.,"""We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are tasks for which it yields diminishing returns.""",2024-02-09T19:09:19Z
Feedback Loops With Language Models Drive In-Context Reward Hacking,Yes.,5.,"""we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process.""",2024-02-09T18:59:29Z
Understanding the Effects of Iterative Prompting on Truthfulness,Yes.,5.,"""Yet, the reliability and truthfulness of these models remain pressing concerns."" and ""naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors.""",2024-02-09T18:57:08Z
If Turing played piano with an artificial partner,No.,1.,The abstract discusses neural network architectures and generative models for producing musical scores but does not specifically mention language models (LLMs or LMs).,2024-02-09T18:43:48Z
TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations,Yes.,3.,"""LLMs excel at natural language processing but do not perform well on planning.""",2024-02-09T18:39:13Z
On the Out-Of-Distribution Generalization of Multimodal Large Language Models,Yes.,5.,"""Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation."" and ""We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples",2024-02-09T18:21:51Z
Understanding the Weakness of Large Language Model Agents within a Complex Android Environment,Yes.,5.,"""LLM agents face three primary challenges,"" ""even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints,"" and ""a lack of four key capabilities, i.e., understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents.""",2024-02-09T18:19:25Z
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German,Yes.,1.,"""Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles.""",2024-02-09T18:05:03Z
The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model,Yes.,5.,"""The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates.""",2024-02-09T17:15:45Z
Calibrating Long-form Generations from Large Language Models,Yes.,3.,"""larger models don't necessarily guarantee better calibration,"" and ""calibration performance is found to be metric-dependent.""",2024-02-09T17:00:32Z
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty,Yes.,5.,"""However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist.""",2024-02-09T16:40:59Z
Large Language Models for Captioning and Retrieving Remote Sensing Images,Yes.,1.,"""the development and application of vision and language models to the remote sensing domain have been hindered by the relatively small size of the available datasets and models used in previous studies.""",2024-02-09T15:31:01Z
V-STaR: Training Verifiers for Self-Taught Reasoners,Yes.,3.,"""These approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions.""",2024-02-09T15:02:56Z
CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models,Yes.,1.,"""In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents.""",2024-02-09T12:10:00Z
RareBench: Can LLMs Serve as Rare Diseases Specialists?,Yes.,1.,"""Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis."" and ""Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases.""",2024-02-09T11:34:16Z
ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs,Yes.,2.,"""However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy.""",2024-02-09T11:23:14Z
InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning,Yes.,1.,"""The math abilities of large language models can represent their abstract reasoning ability.""",2024-02-09T11:22:08Z
Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales,Yes.,3.,"""The generative approaches, such as those based on large language models (LLMs), have the potential to get rid of heavy annotations and provide explanations but their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is",2024-02-09T09:44:06Z
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education,,,,2024-02-09T09:25:18Z
On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference,Yes.,4.,"""Despite the recent success associated with Large Language Models (LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands.""",2024-02-09T09:20:59Z
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,Yes.,4.,"""they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak.""",2024-02-09T09:09:39Z
Entropy-Regularized Token-Level Policy Optimization for Large Language Models,Yes.,3.,"""Nonetheless, it faces significant hurdles",2024-02-09T07:45:26Z
Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants,Yes.,3.,"""LLMs often leap to action without sufficient context, giving rise to implicit assumptions and inaccurate responses.""",2024-02-09T07:44:27Z
ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement,Yes.,1.,"""We propose ResumeFlow",2024-02-09T07:13:44Z
"The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",Yes.,5.,"""Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks."" and ""underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators.""",2024-02-09T06:16:08Z
Large Language Models: A Survey,Yes.,3.,"""We review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations.""",2024-02-09T05:37:09Z
CultureLLM: Incorporating Cultural Differences into Large Language Models,Yes.,3.,"""Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora."" and ""Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources.""",2024-02-09T04:02:43Z
Learn To be Efficient: Build Structured Sparsity in Large Language Models,Yes.,3.,"""Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads.""",2024-02-09T01:18:16Z
Exploring Group and Symmetry Principles in Large Language Models,,,,2024-02-09T01:10:25Z
ContPhy: Continuum Physical Concept Learning and Reasoning from Videos,Yes.,1.,"""We also introduce an oracle model (ContPRO) that marries the particle-based physical dynamic models with the recent large language models, which enjoy the advantages of both models, precise dynamic predictions, and interpretable reasoning.""",2024-02-09T01:09:21Z
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling,Yes.,5.,"""the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects.""",2024-02-09T01:00:14Z
LLMs for Coding and Robotics Education,Yes.,2.,"""Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images.""",2024-02-09T00:58:57Z
SubGen: Token Generation in Sublinear Time and Memory,Yes.,5.,"""Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation.""",2024-02-08T22:17:40Z
Large Language Model Augmented Exercise Retrieval for Personalized Language Learning,Yes.,3.,"""vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO.""",2024-02-08T20:35:31Z
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models,Yes.,5.,"""Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.""",2024-02-08T20:35:06Z
A Prompt Response to the Demand for Automatic Gender-Neutral Translation,Yes.,3.,"""Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.""",2024-02-08T20:24:44Z
Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing,Yes.,3.,"""Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili.""",2024-02-08T19:25:40Z
LLMs Among Us: Generative AI Participating in Digital Discourse,Yes.,4.,"""While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors.""",2024-02-08T19:21:33Z
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,Yes.,5.,"""Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time."" and ""However, all finetuned models struggle to generalize to unseen websites.""",2024-02-08T18:58:02Z
On the Convergence of Zeroth-Order Federated Tuning for Large Language Models,Yes.,3.,"""the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources.""",2024-02-08T18:56:40Z
Efficient Stagewise Pretraining via Progressive Subnetworks,Yes.,3.,"""it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages.""",2024-02-08T18:49:09Z
FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs,Yes.,1.,"""FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims.""",2024-02-08T18:43:05Z
Large Language Model Meets Graph Neural Network in Knowledge Distillation,Yes.,4.,"""the deployment of LLMs for production is hindered by its high computational and storage requirements, as well as long latencies during model inference.""",2024-02-08T18:33:21Z
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion,Yes.,1.,"""It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation.""",2024-02-08T18:27:22Z
Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking,Yes.,5.,"""little is known about such a risk of LLM-powered conversational search"" and ""participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias.""",2024-02-08T18:14:33Z
EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models,Yes.,4.,"""they also introduce significant privacy concerns",2024-02-08T17:57:11Z
How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis,Yes.,3.,"""We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans.""",2024-02-08T17:51:48Z
Is it Possible to Edit Large Language Models Robustly?,Yes.,5.,"""However, the robustness of model editing remains an open question."" and ""Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs."" and ""On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline.""",2024-02-08T17:06:45Z
Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning,Yes.,3.,"""The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation.""",2024-02-08T16:46:26Z
Limits of Transformer Language Models on Learning Algorithmic Compositions,Yes.,5.,"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition.""",2024-02-08T16:23:29Z
Text-to-Code Generation with Modality-relative Pre-training,Yes.,3.,"""programming language keywords (e.g. 'while') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa.""",2024-02-08T16:17:24Z
Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images,Yes.,4.,"""we examine potential gender and racial biases in such systems,"" and ""we observe significant differences in the responses according to the perceived gender or race of the person depicted.""",2024-02-08T16:11:23Z
TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation,Yes.,5.,"""Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.""",2024-02-08T15:08:57Z
In-Context Learning Can Re-learn Forbidden Tasks,Yes.,5.,"""Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities."" and ""we investigate whether ICL can undo safety training, which could represent a major security risk.""",2024-02-08T14:54:17Z
Unified Speech-Text Pretraining for Spoken Dialog Modeling,Yes.,1.,"""While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation.""",2024-02-08T14:35:09Z
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation,Yes.,2.,"""Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse.""",2024-02-08T14:21:03Z
Comprehensive Assessment of Jailbreak Attacks Against LLMs,Yes.,5.,"""safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks.""",2024-02-08T13:42:50Z
"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks",Yes.,5.,"""We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good.""",2024-02-08T13:07:31Z
Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design,Yes.,2.,"""We also address a critical limitation of non-autoregressive models -- namely, that they tend to generate unrealistic sequences with overly repeating tokens.""",2024-02-08T13:02:05Z
The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment,Yes.,1.,"""The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering.""",2024-02-08T12:47:57Z
"Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",Yes.,5.,"""We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity."" and ""We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.""",2024-02-08T12:36:29Z
"Efficient Models for the Detection of Hate, Abuse and Profanity",Yes.,5.,"""Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content.""",2024-02-08T12:28:18Z
AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers,Yes.,4.,"""Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process.""",2024-02-08T12:01:24Z
Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset,Yes.,1.,"""To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks.""",2024-02-08T10:32:06Z
Can ChatGPT evaluate research quality?,Yes.,5.,"""ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks.""",2024-02-08T10:00:40Z
Question Aware Vision Transformer for Multimodal Reasoning,Yes.,3.,"""Despite their success, a critical limitation persists",2024-02-08T08:03:39Z
Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia,Yes.,4.,"""Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle.""",2024-02-08T07:56:49Z
It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition,Yes.,3.,"""GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal.""",2024-02-08T07:21:45Z
Large Language Models for Psycholinguistic Plausibility Pretesting,Yes.,3.,"""We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.""",2024-02-08T07:20:02Z
Accurate LoRA-Finetuning Quantization of LLMs via Information Retention,Yes.,3.,"""However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA.""",2024-02-08T06:53:31Z
Do Large Code Models Understand Programming Concepts? A Black-box Approach,Yes.,5.,"""Our findings suggest that current models lack understanding of concepts such as data flow and control flow.""",2024-02-08T06:48:01Z
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study,Yes.,3.,"""All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid.""",2024-02-08T06:20:01Z
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes,Yes.,5.,"""LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target.""",2024-02-08T04:48:26Z
In-Context Principle Learning from Mistakes,Yes.,3.,"""Nonetheless, all ICL-based approaches only learn from correct input-output pairs.""",2024-02-08T04:42:29Z
Enhancing Zero-shot Counting via Language-guided Exemplar Learning,Yes.,1.,"""inheriting rich semantic priors from the prevailing pre-trained Large Language Models (LLMs)""",2024-02-08T04:07:38Z
CIC: A framework for Culturally-aware Image Captioning,Yes.,3.,"""However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups.""",2024-02-08T03:12:25Z
Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception,Yes.,5.,"""existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination.""",2024-02-08T02:37:30Z
Scaling Up LLM Reviews for Google Ads Content Moderation,Yes.,3.,"""Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository.""",2024-02-07T23:47:02Z
Using text embedding models and vector databases as text classifiers with the example of medical data,Yes.,3.,"""Using various LLMs to generate the medical data, we also understand the limitations of the medical knowledge of these models and encourage further expert medical review of our testing data.""",2024-02-07T22:15:15Z
Are LLMs Ready for Real-World Materials Discovery?,Yes.,5.,"""While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge.""",2024-02-07T19:10:36Z
$Î»$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space,Yes.,2.,"""Predominantly, contemporary approaches, involving the training of Hypernetworks and Multimodal Large Language Models (MLLMs), require heavy computing resources that range from 600 to 12300 GPU hours of training.""",2024-02-07T19:07:10Z
InCoRo: In-Context Learning for Robotics Control with Feedback Loops,Yes.,1.,"""Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units.""",2024-02-07T19:01:11Z
Opening the AI black box: program synthesis via mechanistic interpretability,Yes.,3.,"""As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub.""",2024-02-07T18:59:12Z
Hydragen: High-Throughput LLM Inference with Shared Prefixes,Yes.,1.,"""Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users.""",2024-02-07T18:53:01Z
Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation,No.,1.,The abstract does not mention LLMs or any specific limitations related to them.,2024-02-07T18:44:27Z
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications,Yes.,5.,"""Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning."" and ""These findings underscore the urgent need for more robust safety strategies in LLMs.""",2024-02-07T18:34:38Z
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,Yes.,3.,"""Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics.""",2024-02-07T17:33:54Z
A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?,Yes.,3.,"""However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs.""",2024-02-07T16:32:58Z
Pedagogical Alignment of Large Language Models,Yes.,1.,"""In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts.""",2024-02-07T16:15:59Z
An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,Yes.,4.,"""they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.""",2024-02-07T15:56:17Z
ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12,Yes.,1.,"""leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance.""",2024-02-07T15:55:51Z
Reconfidencing LLMs from the Grouping Loss Perspective,Yes.,5.,"""Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone."" and ""Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query.""",2024-02-07T15:40:22Z
Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems,Yes.,2.,"""However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial.""",2024-02-07T15:39:07Z
Prompting Implicit Discourse Relation Annotation,Yes.,5.,"""Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.""",2024-02-07T14:44:42Z
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ,Yes.,3.,"""Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs)."" and ""However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-",2024-02-07T14:35:05Z
Detecting Generated Native Ads in Conversational Search,Yes.,3.,"""In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task.""",2024-02-07T14:22:51Z
"Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",Yes.,3.,"""not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints.""",2024-02-07T13:36:02Z
Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning,Yes.,1.,"""We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement.""",2024-02-07T13:32:11Z
Direct Language Model Alignment from Online AI Feedback,Yes.,3.,"""the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy.""",2024-02-07T12:31:13Z
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,Yes.,5.,"""MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V.""",2024-02-07T12:28:32Z
A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,Yes.,4.,"""how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further"" and ""The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work.""",2024-02-07T12:26:12Z
ApiQ: Finetuning of 2-Bit Quantized Large Language Model,Yes.,3.,"""current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of",2024-02-07T09:36:54Z
Large Language Models As Faithful Explainers,Yes.,4.,"""natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs.""",2024-02-07T09:09:14Z
LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views,Yes.,3.,"""potential limitations in the pre-training data and models are often ignored.""",2024-02-07T08:16:40Z
The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends,Yes.,2.,"""Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion.""",2024-02-07T07:28:34Z
SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph,Yes.,2.,"""one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs.""",2024-02-07T07:24:01Z
MEMORYLLM: Towards Self-Updatable Large Language Models,Yes.,4.,"""Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model.""",2024-02-07T07:14:11Z
CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients,Yes.,1.,"""To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs).""",2024-02-07T07:07:02Z
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory,Yes.,5.,"""existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues.""",2024-02-07T06:50:42Z
TinyLLM: Learning a Small Student from Multiple Large Language Models,Yes.,3.,"""However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information.""",2024-02-07T06:48:24Z
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,Yes.,5.,"""However, there is little to no understanding of their faithfulness,"" ""we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs,"" ""these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness,"" and ""improving faithfulness is an open challenge.""",2024-02-07T06:32:50Z
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,Yes.,3.,"""However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains.""",2024-02-07T06:13:14Z
Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector,Yes.,4.,"""Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).""",2024-02-07T05:56:54Z
"The Role of LLMs in Sustainable Smart Cities: Applications, Challenges, and Future Directions",Yes.,3.,"""Our discourse culminates with an exploration of the formidable challenges that DL, FL, IoT, Blockchain, NLP, and LLMs face within these contexts, and we offer insights into potential future directions.""",2024-02-07T05:22:10Z
Can Large Language Model Agents Simulate Human Trust Behaviors?,Yes.,3.,"""We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations.""",2024-02-07T03:37:19Z
An Artificial Intelligence (AI) workflow for catalyst design and optimization,Yes.,1.,"""this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization.""",2024-02-07T03:25:08Z
RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation,Yes.,3.,"""Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness.""",2024-02-07T02:14:58Z
Online Cascade Learning for Efficient Inference over Streams,Yes.,5.,"""Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.""",2024-02-07T01:46:50Z
The Fine-Grained Complexity of Gradient Computation for Training Large Language Models,Yes.,2.,"""there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false.""",2024-02-07T00:45:31Z
Grandmaster-Level Chess Without Search,No.,1.,The paper focuses on a transformer model trained for chess and does not discuss language models.,2024-02-07T00:36:24Z
De-amplifying Bias from Differential Privacy in Language Model Fine-tuning,Yes.,5.,"""We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP.""",2024-02-07T00:30:58Z
Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,Yes.,5.,"""concentrating on deceptive behaviours of Large Language Models (LLMs)"" and ""emphasising multidimensional biases that underlie their deceptive behaviours"" and ""the literature review covers four types of deception categorised",2024-02-07T00:21:46Z
Detecting Mode Collapse in Language Models via Narration,Yes.,5.,"""we show successive versions of GPT-3 suffer from increasing degrees of 'mode collapse' whereby overfitting the model during alignment constrains it from generalizing over authorship",2024-02-06T23:52:58Z
Structured Entity Extraction Using Large Language Models,Yes.,3.,"""This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues.""",2024-02-06T22:15:09Z
Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton,Yes.,3.,"""Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service.""",2024-02-06T21:14:45Z
Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning,Yes.,3.,"""However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic.""",2024-02-06T21:03:52Z
Fine-Tuned Language Models Generate Stable Inorganic Materials as Text,Yes.,1.,"""We propose fine-tuning large language models for generation of stable materials.""",2024-02-06T20:35:28Z
Monitoring the evolution of antisemitic discourse on extremist social media using BERT,Yes.,1.,"""we created an unsupervised online machine learning approach that uses large language models to assess the contextual similarity of posts.""",2024-02-06T20:34:49Z
The World of Generative AI: Deepfakes and Large Language Models,Yes.,3.,"""LLMs are powerful language models that generate general-purpose language. However due to its generative aspect, it can also be a risk for people if used with ill intentions.""",2024-02-06T20:18:32Z
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains,Yes.,3.,"""However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences.""",2024-02-06T20:11:54Z
The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry,Yes.,3.,"""However, linear attentions often underperform standard softmax attention in quality.""",2024-02-06T19:31:26Z
LESS: Selecting Influential Data for Targeted Instruction Tuning,Yes.,3.,"""The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning.""",2024-02-06T19:18:04Z
Training Language Models to Generate Text with Citations via Fine-grained Rewards,Yes.,5.,"""While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources.""",2024-02-06T19:00:40Z
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,Yes.,3.,"""Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs),"" and ""We identify several desirable properties previously unaccounted for in red teaming evaluations.""",2024-02-06T18:59:08Z
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science,Yes.,5.,"""While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety."" and ""We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents,"" and ""Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents.""",2024-02-06T18:54:07Z
Can Generative Agents Predict Emotion?,Yes.,3.,"""The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.""",2024-02-06T18:39:43Z
Scaling Laws for Downstream Task Performance of Large Language Models,Yes.,2.,"""However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves.""",2024-02-06T17:31:20Z
Harnessing the Plug-and-Play Controller by Prompting,Yes.,3.,"""Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation.""",2024-02-06T17:18:25Z
Multi-line AI-assisted Code Authoring,Yes.,3.,"""First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction."" and ""multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived",2024-02-06T16:48:50Z
Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs),Yes.,1.,"""By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs)",2024-02-06T16:47:34Z
Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science,Yes.,3.,"""two key issues remain",2024-02-06T16:12:36Z
Measuring Implicit Bias in Explicitly Unbiased Large Language Models,Yes.,5.,"""Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases,"" and ""Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).""",2024-02-06T15:59:23Z
The Use of a Large Language Model for Cyberbullying Detection,Yes.,2.,"""However, their performances are not consistent due to high class imbalance and generalisation issues.""",2024-02-06T15:46:31Z
Provably learning a multi-head attention layer,Yes.,1.,"""We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian.""",2024-02-06T15:39:09Z
Systematic Biases in LLM Simulations of Debates,Yes.,5.,"""In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives.""",2024-02-06T14:51:55Z
LLM Agents can Autonomously Hack Websites,Yes.,2.,"""Our findings raise questions about the widespread deployment of LLMs.""",2024-02-06T14:46:08Z
Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought,Yes.,5.,"""We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.""",2024-02-06T13:59:56Z
Enhancing Retrieval Processes for Language Generation with Augmented Queries,Yes.,3.,"""These models sometimes face difficulties, like providing inaccurate information, commonly known as 'hallucination.'""",2024-02-06T13:19:53Z
LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K,Yes.,5.,"""Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation,"" and ""LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of 'needle in a haystack'.""",2024-02-06T13:11:19Z
Discovery of the Hidden World with Large Language Models,Yes.,1.,"""The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data.""",2024-02-06T12:18:54Z
"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",Yes.,5.,"""The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers."" and ""we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.""",2024-02-06T11:54:23Z
Large Language Models to Enhance Bayesian Optimization,Yes.,1.,"""we present LLAMBO, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO.""",2024-02-06T11:44:06Z
Can Large Language Models Detect Rumors on Social Media?,Yes.,5.,"""it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.""",2024-02-06T11:33:57Z
"Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy",Yes.,3.,"""Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated.""",2024-02-06T11:19:40Z
DistiLLM: Towards Streamlined Distillation for Large Language Models,Yes.,3.,"""current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs.""",2024-02-06T11:10:35Z
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models,Yes.,5.,"""Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.""",2024-02-06T10:37:21Z
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models,Yes.,3.,"""However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs.""",2024-02-06T09:50:08Z
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs,Yes.,3.,"""Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources."" and ""existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths.""",2024-02-06T09:26:34Z
Rethinking Skill Extraction in the Job Market Domain using Large Language Models,Yes.,3.,"""However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions.""",2024-02-06T09:23:26Z
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models,,,,2024-02-06T09:10:35Z
ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs,Yes.,1.,"""Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons.""",2024-02-06T08:45:51Z
Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning,Yes.,2.,"""existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians.""",2024-02-06T07:53:23Z
MolTC: Towards Molecular Relational Modeling In Language Models,Yes.,3.,"""Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs.""",2024-02-06T07:51:56Z
Large Language Models As MOOCs Graders,Yes.,3.,"""However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses.""",2024-02-06T07:43:07Z
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs,Yes.,5.,"""those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs."" and ""illustrating that they universally suffer from this instinctive bias to varying degrees.""",2024-02-06T06:48:46Z
INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection,Yes.,3.,"""Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs.""",2024-02-06T06:23:12Z
Similarity-based Neighbor Selection for Graph LLMs,Yes.,3.,"""Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs.""",2024-02-06T05:29:05Z
Automatic Robotic Development through Collaborative Framework by Large Language Models,Yes.,3.,"""Despite the remarkable code generation abilities of large language models LLMs, they still face challenges in complex task handling.""",2024-02-06T04:40:27Z
Personalized Language Modeling from Personalized Human Feedback,Yes.,3.,"""However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse."" and ""explain why vanilla RLHF can be problematic in this context.""",2024-02-06T04:18:58Z
Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning,Yes.,3.,"""previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR.""",2024-02-06T03:41:12Z
Limits of Large Language Models in Debating Humans,Yes.,5.,"""We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.""",2024-02-06T03:24:27Z
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models,Yes.,3.,"""the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others.""",2024-02-06T03:18:58Z
Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue,Yes.,1.,"""Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation.""",2024-02-06T03:14:46Z
Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context,Yes.,5.,"""Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs",2024-02-06T01:59:41Z
Partially Recentralization Softmax Loss for Vision-Language Models Robustness,Yes.,3.,"""it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input.""",2024-02-06T01:44:38Z
Self-Discover: Large Language Models Self-Compose Reasoning Structures,Yes.,1.,"""We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods.""",2024-02-06T01:13:53Z
Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning,Yes.,3.,"""large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships.""",2024-02-06T00:51:27Z
Distinguishing the Knowable from the Unknowable with Language Models,Yes.,2.,"""We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text.""",2024-02-05T22:22:49Z
Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains,Yes.,5.,"""We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.""",2024-02-05T20:51:11Z
Neural networks for abstraction and reasoning: Towards broad generalization in machines,Yes.,3.,"""LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches.""",2024-02-05T20:48:57Z
Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues,Yes.,5.,"""This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and",2024-02-05T20:11:56Z
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,Yes.,4.,"""We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique.""",2024-02-05T19:49:13Z
Arabic Synonym BERT-based Adversarial Examples for Text Classification,Yes.,2.,"""We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained.""",2024-02-05T19:39:07Z
Nevermind: Instruction Override and Moderation in Large Language Models,Yes.,5.,"""Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.""",2024-02-05T18:58:19Z
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,Yes.,3.,"""Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.""",2024-02-05T18:55:32Z
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models,Yes.,4.,"""The discovery of 'jailbreaks' to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures."" and ""Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.""",2024-02-05T18:54:43Z
Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS,Yes.,5.,"""Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.""",2024-02-05T18:47:04Z
"Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",Yes.,3.,"""How well can language models represent inherent uncertainty in conversations?"" and ""Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies... can calibrate smaller open-source models to compete with pre-trained models 10x their size.""",2024-02-05T18:39:47Z
A Framework for Partially Observed Reward-States in RLHF,Yes.,3.,"""Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment.""",2024-02-05T18:38:55Z
MobilityGPT: Enhanced Human Mobility Modeling with a GPT model,Yes.,1.,"""Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits.""",2024-02-05T18:22:21Z
English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts,Yes.,1.,"""Our experiments with natural language inference-based language models show that it is consistently better to use English prompts even if the data is in a different language.""",2024-02-05T17:36:19Z
Unified Hallucination Detection for Multimodal Large Language Models,Yes.,5.,"""Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.""",2024-02-05T16:56:11Z
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System,Yes.,1.,"""One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods.""",2024-02-05T16:47:17Z
Empowering Time Series Analysis with Large Language Models: A Survey,Yes.,3.,"""completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.""",2024-02-05T16:46:35Z
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,Yes.,5.,"""Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.""",2024-02-05T16:46:16Z
CIDAR: Culturally Relevant Instruction Dataset For Arabic,Yes.,3.,"""current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture.""",2024-02-05T16:44:17Z
The Matrix: A Bayesian learning model for LLMs,Yes.,1.,"""We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle.""",2024-02-05T16:42:10Z
MULTI: Multimodal Understanding Leaderboard with Text and Images,Yes.,3.,"""existing benchmarks primarily focus on understanding simple natural images and short context"" and ""Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on MULTI, in contrast to other MLLMs scoring between 28.5% and",2024-02-05T16:41:02Z
Homograph Attacks on Maghreb Sentiment Analyzers,No.,1.,The abstract does not mention language models (LLMs) explicitly.,2024-02-05T16:39:15Z
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization,Yes.,3.,"""In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.""",2024-02-05T16:30:49Z
Constrained Decoding for Cross-lingual Label Projection,Yes.,2.,"""However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods.""",2024-02-05T15:57:32Z
Evaluation of ChatGPT Usability as A Code Generation Tool,Yes.,3.,"""Our experiments demonstrated that ChatGPT is highly useful for generating R program code although it may fail on hard programming tasks."" and ""Our experiment also shows that it is hard for human developers to learn from experiences to improve the skill of using ChatGPT to generate code.""",2024-02-05T15:56:19Z
Best Practices for Text Annotation with Large Language Models,Yes.,4.,"""Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results.""",2024-02-05T15:43:50Z
Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases,Yes.,2.,"""Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction.""",2024-02-05T15:28:43Z
Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations,Yes.,5.,"""Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions.""",2024-02-05T15:08:19Z
UniMem: Towards a Unified View of Long-Context Large Language Models,,,,2024-02-05T13:47:53Z
Conversation Reconstruction Attack Against GPT Models,,,,2024-02-05T13:18:42Z
Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing,Yes.,1.,"""a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images.""",2024-02-05T13:16:12Z
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation,Yes.,5.,"""there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context,"" and ""directly applying LLMs often leads to inaccurate answers.""",2024-02-05T11:58:56Z
LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models,Yes.,2.,"""highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.""",2024-02-05T11:05:20Z
Shortened LLaMA: A Simple Depth Pruning for Large Language Models,Yes.,1.,"""Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs.""",2024-02-05T09:44:49Z
Evading Data Contamination Detection for Language Models is (too) Easy,Yes.,5.,"""However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements."" and ""we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.""",2024-02-05T09:10:32Z
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning,Yes.,5.,"""We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."" and ""LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices.""",2024-02-05T08:26:33Z
Large Language Model Distilling Medication Recommendation Model,,,,2024-02-05T08:25:22Z
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models,Yes.,1.,The abstract discusses a method for fine-tuning LLMs and finding effective subsets of parameters but does not mention any explicit limitations of the models.,2024-02-05T08:19:56Z
Rethinking Optimization and Architecture for Tiny Language Models,Yes.,3.,"""However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required.""",2024-02-05T07:59:38Z
List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation,Yes.,1.,"""retrieval-augmented generation for large language models (LLMs).""",2024-02-05T06:52:53Z
DeAL: Decoding-time Alignment for Large Language Models,Yes.,4.,"""First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training).""",2024-02-05T06:12:29Z
Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering,Yes.,1.,"""These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression.""",2024-02-05T06:08:06Z
KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache,Yes.,3.,"""the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage.""",2024-02-05T06:06:47Z
Understanding the planning of LLM agents: A survey,Yes.,2.,"""further challenges for the field of research are discussed.""",2024-02-05T04:25:24Z
Adversarial Text Purification: A Large Language Model Approach for Defense,Yes.,1.,"""We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations.""",2024-02-05T02:36:41Z
Large Language Models are Geographically Biased,Yes.,5.,"""Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm."" and ""We show various problematic geographic biases, which we define as systemic errors in geospatial predictions."" and ""LLMs exhibit common biases across a range of objective and subjective topics"" and ""LLMs are clearly biased against locations with lower",2024-02-05T02:32:09Z
Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases,Yes.,1.,"""Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them.""",2024-02-05T01:59:31Z
RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews,Yes.,3.,"""Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using LLMs to improve research efficiency.""",2024-02-05T00:56:30Z
Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting,Yes.,5.,"""Large Language Models (LLMs) frequently struggle with complex reasoning tasks, failing to construct logically sound steps towards the solution."" and ""repeated meaningless feedback gradually decreases the quality of the responses, eventually leading to a larger deviation from the intended outcome.""",2024-02-05T00:44:28Z
Zero-Shot Clinical Trial Patient Matching with LLMs,Yes.,1.,"""Large language models (LLMs) offer a promising solution.""",2024-02-05T00:06:08Z
LLM-Enhanced Data Management,Yes.,5.,"""existing LLMs have several limitations",2024-02-04T23:42:02Z
Can Large Language Models Learn Independent Causal Mechanisms?,Yes.,5.,"""Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability.""",2024-02-04T23:04:02Z
Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity,Yes.,3.,"""Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs).""",2024-02-04T22:56:56Z
UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing,Yes.,3.,"""existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code.""",2024-02-04T22:48:05Z
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?,Yes.,5.,"""We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset.""",2024-02-04T20:56:09Z
A Truly Joint Neural Architecture for Segmentation and Parsing,Yes.,1.,"""This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.""",2024-02-04T16:56:08Z
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models,Yes.,3.,"""Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems."" and ""Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT) have predomin",2024-02-04T16:45:01Z
Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials,Yes.,5.,"""Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context.""",2024-02-04T16:18:01Z
Are Large Language Models Table-based Fact-Checkers?,Yes.,2.,"""Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability."" and ""We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples.""",2024-02-04T15:52:59Z
Knowledge Generation for Zero-shot Knowledge-based VQA,Yes.,2.,"""However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.""",2024-02-04T15:41:35Z
CompeteSMoE -- Effective Training of Sparse Mixture of Experts via Competition,Yes.,1.,"""In this work, we propose a competition mechanism to address this fundamental challenge of representation collapse.""",2024-02-04T15:17:09Z
Conversational Crowdsensing: A Parallel Intelligence Powered Novel Sensing Approach,Yes.,1.,"""new requirements and opportunities to current sensing approaches, especially in light of recent progress in Chatbots and Large Language Models (LLMs).""",2024-02-04T15:10:11Z
GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering,Yes.,3.,"""However, such conversion may introduce irrelevant information, which causes the LLM to misinterpret images and ignore visual details crucial for accurate knowledge.""",2024-02-04T14:28:23Z
Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation,Yes.,5.,"""The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine.""",2024-02-04T13:21:19Z
BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback,Yes.,2.,"""However, they leave behind important features of the PPO approach. Methods such as SLiC or RRHF make use of the Reward Model (RM) only for ranking/preference, losing fine-grained information and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce), while methods such as DPO do not use even a separate reward",2024-02-04T13:16:29Z
A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer,No.,1.,The abstract does not mention LLMs or their limitations.,2024-02-04T12:29:40Z
FoldToken: Learning Protein Language via Vector Quantization and Beyond,No.,1.,The abstract does not mention language models or their limitations.,2024-02-04T12:18:51Z
Breaking MLPerf Training: A Case Study on Optimizing BERT,Yes.,1.,"""We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance.""",2024-02-04T11:12:17Z
LQER: Low-Rank Quantization Error Reconstruction for LLMs,Yes.,3.,"""Post-training quantization of Large Language Models (LLMs) is challenging.""",2024-02-04T10:59:52Z
Factuality of Large Language Models in the Year 2024,Yes.,5.,"""Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios.""",2024-02-04T09:36:31Z
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction,Yes.,3.,"""RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters.""",2024-02-04T09:24:51Z
GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model,Yes.,3.,"""Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design.""",2024-02-04T08:57:54Z
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models,Yes.,5.,"""we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases.""",2024-02-04T08:11:45Z
KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion,Yes.,3.,"""Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency.""",2024-02-04T08:01:07Z
Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning,Yes.,5.,"""However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process.""",2024-02-04T07:59:06Z
Evaluating Large Language Models in Analysing Classroom Dialogue,Yes.,2.,"""Results indicate substantial time savings with GPT-4, and a high degree of consistency in coding between the model and human coders, with some discrepancies in specific codes.""",2024-02-04T07:39:06Z
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models,Yes.,1.,"""increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series"" and ""we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters.""",2024-02-04T06:59:21Z
Timer: Transformers for Time Series Analysis at Scale,No.,1.,"The abstract discusses the development of large time series models (LTSM) and their applications in time series analysis, but it does not mention language models (LLMs or LLMs) specifically.",2024-02-04T06:55:55Z
Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques,Yes.,1.,"""Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking.""",2024-02-04T05:51:14Z
Large Language Model Adaptation for Networking,Yes.,1.,"""Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy.""",2024-02-04T04:21:34Z
Enhance Reasoning for Large Language Models in the Game Werewolf,Yes.,1.,"""This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents.""",2024-02-04T03:47:10Z
A Survey of Large Language Models in Finance (FinLLMs),Yes.,4.,"""Finally, we discuss the opportunities and the challenges facing FinLLMs, such as hallucination, privacy, and efficiency.""",2024-02-04T02:06:57Z
Selecting Large Language Model to Fine-tune via Rectified Scaling Law,Yes.,2.,"""We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically.""",2024-02-04T01:55:00Z
Jailbreaking Attack against Multimodal Large Language Model,Yes.,3.,"""seeking to elicit MLLMs to generate objectionable responses to harmful user queries,"" and ""we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks.""",2024-02-04T01:29:24Z
Large Language Model for Table Processing: A Survey,Yes.,2.,"""Finally, we highlight several challenges, ranging from private deployment and efficient inference to the development of extensive benchmarks for table manipulation and advanced data analysis.""",2024-02-04T00:47:53Z
SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking,Yes.,2.,"""However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible.""",2024-02-03T22:49:00Z
"Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times",Yes.,5.,"""Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades.""",2024-02-03T20:22:54Z
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models,,,,2024-02-03T19:20:02Z
"Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",Yes.,4.,"""These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings.""",2024-02-03T19:19:34Z
Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,Yes.,4.,"""Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks."" and ""Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning L",2024-02-03T16:43:42Z
GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events,Yes.,5.,"""Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios.""",2024-02-03T16:38:25Z
Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations,Yes.,4.,"""At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the",2024-02-03T14:28:55Z
Analyzing Sentiment Polarity Reduction in News Presentation through Contextual Perturbation and Large Language Models,Yes.,1.,"""a context-aware masked language model""",2024-02-03T13:27:32Z
Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test,Yes.,5.,"""Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages.""",2024-02-03T12:52:36Z
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models,Yes.,1.,"""Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning.""",2024-02-03T12:19:47Z
Are Large Language Models Good Prompt Optimizers?,,,,2024-02-03T09:48:54Z
Break the Sequential Dependency of LLM Inference Using Lookahead Decoding,Yes.,3.,"""Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators.""",2024-02-03T06:37:50Z
Affordable Generative Agents,Yes.,4.,"""the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents,"" and ""demonstrating that agents can only generate finite behaviors in fixed environments.""",2024-02-03T06:16:28Z
Panacea: Pareto Alignment via Preference Adaptation for LLMs,Yes.,3.,"""Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment.""",2024-02-03T05:01:04Z
A Closer Look at the Limitations of Instruction Tuning,Yes.,5.,"""While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT.""",2024-02-03T04:45:25Z
How well do LLMs cite relevant medical references? An evaluation framework and analyses,Yes.,5.,"""Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide."" and ""Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.""",2024-02-03T03:44:57Z
"PresAIse, A Prescriptive AI Solution for Enterprises",Yes.,1.,"""the integration of large language models (LLMs) to bridge communication gaps via a conversation agent.""",2024-02-03T03:23:08Z
Human-Centered Privacy Research in the Age of Large Language Models,Yes.,4.,"""The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns."" and ""To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems.""",2024-02-03T02:32:45Z
Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes,Yes.,4.,"""Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases.""",2024-02-03T01:40:11Z
SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks,Yes.,2.,"""However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured.""",2024-02-03T01:33:16Z
"A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",Yes.,2.,"""However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments.""",2024-02-03T00:27:22Z
MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles,Yes.,1.,"""We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B.""",2024-02-03T00:23:36Z
Large Language Model Agent for Hyper-Parameter Optimization,Yes.,1.,"""To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks.""",2024-02-02T20:12:05Z
The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,Yes.,3.,"""We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.""",2024-02-02T20:01:15Z
Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision,Yes.,1.,"""We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision",2024-02-02T19:45:39Z
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,Yes.,5.,"""Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase.""",2024-02-02T19:43:15Z
"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",Yes.,5.,"""Beyond known issues like hallucinations, experts revealed novel legal problems, including that users' conversations with LLMs are not protected by attorney-client confidentiality or bound to professional ethics that guard against conflicted counsel or poor quality advice.""",2024-02-02T19:35:34Z
Cross-modality debiasing: using language to mitigate sub-population shifts in imaging,Yes.,2.,"""Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning.""",2024-02-02T18:54:48Z
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment,Yes.,1.,"""Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations.""",2024-02-02T18:49:26Z
TravelPlanner: A Benchmark for Real-World Planning with Language Agents,,,,2024-02-02T18:39:51Z
Stochastic Two Points Method for Deep Model Zeroth-order Optimization,Yes.,2.,"""Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation.""",2024-02-02T18:39:40Z
MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models,Yes.,3.,"""However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference.""",2024-02-02T18:35:14Z
KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases,Yes.,2.,"""PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data.""",2024-02-02T18:32:24Z
Style Vectors for Steering Generative Large Language Model,Yes.,1.,"""This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation.""",2024-02-02T18:31:15Z
Leveraging Large Language Models for Analyzing Blood Pressure Variations Across Biological Sex from Scientific Literature,Yes.,1.,"""In this work, we employed GPT-35-turbo, a large language model (LLM), to automatically extract the mean and standard deviation values of BP for both males and females from a dataset comprising 25 million abstracts sourced from PubMed.""",2024-02-02T18:15:51Z
Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning,Yes.,5.,"""However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability.""",2024-02-02T18:00:35Z
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution,Yes.,4.,"""their trustworthiness remains an under-explored area"" and ""improving the safety dimension of trustworthiness in LLM-based agents.""",2024-02-02T17:26:23Z
Building Guardrails for Large Language Models,,,,2024-02-02T16:35:00Z
Ecologically rational meta-learned inference explains human category learning,Yes.,1.,"""In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge.""",2024-02-02T16:32:04Z
Homogenization Effects of Large Language Models on Human Creative Ideation,Yes.,4.,"""different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST"" and ""ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated.""",2024-02-02T16:27:11Z
An Empirical Analysis of Diversity in Argument Summarization,Yes.,3.,"""We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths.""",2024-02-02T16:26:52Z
Decoding Speculative Decoding,Yes.,2.,"""The speedup provided by speculative decoding heavily depends on the choice of the draft model."" and ""our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases.""",2024-02-02T16:15:24Z
K-Level Reasoning with Large Language Models,Yes.,3.,"""existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works.""",2024-02-02T16:07:05Z
A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation,Yes.,3.,"""large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information.""",2024-02-02T15:26:39Z
AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback,Yes.,1.,"""The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks.""",2024-02-02T14:56:48Z
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach,Yes.,2.,"""The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.""",2024-02-02T14:43:19Z
"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",Yes.,5.,"""We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature.""",2024-02-02T14:43:18Z
Distilling LLMs' Decomposition Abilities into Compact Language Models,Yes.,4.,"""Large Language Models (LLMs) have demonstrated proficiency in their reasoning abilities, yet their large size presents scalability challenges and limits any further customization.""",2024-02-02T13:23:15Z
StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback,Yes.,4.,"""the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective.""",2024-02-02T13:14:31Z
LLM-based NLG Evaluation: Current Status and Challenges,Yes.,3.,"""we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively.""",2024-02-02T13:06:35Z
Continual Learning for Large Language Models: A Survey,Yes.,4.,"""Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale."" and ""Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.""",2024-02-02T12:34:09Z
A Survey on Large Language Model Hallucination via a Creativity Perspective,Yes.,4.,"""This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications.""",2024-02-02T12:21:04Z
Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models,Yes.,5.,"""our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of L",2024-02-02T12:07:00Z
Preference-free Alignment Learning with Regularized Relevance Reward,Yes.,3.,"""our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones"" and ""the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts.""",2024-02-02T11:58:08Z
Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion,Yes.,1.,"""However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices.""",2024-02-02T11:57:50Z
KTO: Model Alignment as Prospect Theoretic Optimization,Yes.,1.,"""We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs).""",2024-02-02T10:53:36Z
Can MLLMs Perform Text-to-Image In-Context Learning?,Yes.,5.,"""we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation.""",2024-02-02T10:30:05Z
Exploring the Limitations of Graph Reasoning in Large Language Models,Yes.,5.,"""We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution.""",2024-02-02T09:45:33Z
"The Human and the Mechanical: logos, truthfulness, and ChatGPT",Yes.,5.,"""âMechanical mindsâ lack these two components",2024-02-02T09:41:51Z
Efficient Causal Graph Discovery Using Large Language Models,Yes.,1.,"""While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs.""",2024-02-02T08:25:32Z
Large Language Models for Time Series: A Survey,Yes.,3.,"""We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis.""",2024-02-02T07:24:35Z
Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus,Yes.,4.,"""yet they often hallucinate, especially in knowledge-intensive tasks that require external knowledge sources.""",2024-02-02T06:44:22Z
Efficient Prompt Caching via Embedding Similarity,Yes.,3.,"""However, it faces the challenge of significant resource consumption during inference.""",2024-02-02T06:34:11Z
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward,Yes.,3.,"""Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference.""",2024-02-02T06:29:34Z
LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning,Yes.,3.,"""Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance.""",2024-02-02T05:54:12Z
CABINET: Content Relevance based Noise Reduction for Table Question Answering,Yes.,3.,"""The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise.""",2024-02-02T05:48:39Z
ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution,Yes.,1.,"""This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces.""",2024-02-02T05:04:51Z
A Multi-Agent Conversational Recommender System,Yes.,4.,"""Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address",2024-02-02T04:20:13Z
PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models,Yes.,1.,"""The design of PokeLLMon incorporates three key strategies",2024-02-02T03:22:12Z
DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models,Yes.,2.,"""Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy.""",2024-02-02T03:21:00Z
Vaccine: Perturbation-aware Alignment for Large Language Model,Yes.,3.,"""The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs)",2024-02-02T02:56:50Z
"Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",Yes.,3.,"""Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings."" and ""Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time.""",2024-02-02T02:53:11Z
The Political Preferences of LLMs,Yes.,4.,"""The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints."" and ""base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests.""",2024-02-02T02:43:10Z
LitLLM: A Toolkit for Scientific Literature Review,Yes.,5.,"""Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on.""",2024-02-02T02:41:28Z
Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs,Yes.,1.,"""we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences.""",2024-02-02T02:11:31Z
Specialized Language Models with Cheap Inference from Limited Domain Data,Yes.,3.,"""Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets.""",2024-02-02T01:45:18Z
Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities,,,,2024-02-02T00:16:45Z
Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer,Yes.,1.,"""With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models.""",2024-02-01T23:46:05Z
Plan-Grounded Large Language Models for Dual Goal Conversational Settings,Yes.,3.,"""Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another.""",2024-02-01T22:56:39Z
"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Yes.,3.,"""Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership.""",2024-02-01T22:54:31Z
IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition,Yes.,1.,"""With the emergence of generative AI models such as large language models (LLMs) and text-driven motion synthesis models, language has become a promising source data modality as well as shown in proof of concepts such as IMUGPT.""",2024-02-01T22:37:33Z
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations,Yes.,1.,"""This study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs.""",2024-02-01T21:51:09Z
Getting the most out of your tokenizer for pre-training and domain adaptation,Yes.,2.,"""Tokenization is an understudied and often neglected component of modern LLMs.""",2024-02-01T21:49:34Z
Repeat After Me: Transformers are Better than State Space Models at Copying,Yes.,3.,"""Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context.""",2024-02-01T21:44:11Z
Executable Code Actions Elicit Better LLM Agents,Yes.,3.,"""LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools).""",2024-02-01T21:38:58Z
HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent,Yes.,2.,"""However, the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset.""",2024-02-01T21:10:44Z
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,Yes.,5.,"""Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details.""",2024-02-01T19:12:25Z
Evaluating Large Language Models for Generalization and Robustness via Data Compression,Yes.,5.,"""Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation."" and ""We find that the compression rate of many models reduces significantly after their cutoff date,"" and ""Results also suggest that models struggle to generalize on news and code",2024-02-01T18:56:18Z
Can Large Language Models Understand Context?,Yes.,5.,"""Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models"" and ""we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.""",2024-02-01T18:55:29Z
Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?,Yes.,3.,"""However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources."" and ""we observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets.""",2024-02-01T18:31:34Z
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents,Yes.,5.,"""However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.""",2024-02-01T17:30:50Z
"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",Yes.,1.,"""Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting.""",2024-02-01T17:28:10Z
Dense Reward for Free in Reinforcement Learning from Human Feedback,,,,2024-02-01T17:10:35Z
Unlearnable Algorithms for In-context Learning,Yes.,1.,"""In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).""",2024-02-01T16:43:04Z
Health-LLM: Personalized Retrieval-Augmented Disease Prediction System,Yes.,2.,"""However, there are conspicuous challenges such as vast data volumes and inconsistent symptom characterization standards, preventing full integration of healthcare AI systems with individual patients' needs.""",2024-02-01T16:40:32Z
Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement,Yes.,4.,"""Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.""",2024-02-01T16:39:51Z
Transforming and Combining Rewards for Aligning Large Language Models,Yes.,2.,"""This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).""",2024-02-01T16:39:28Z
Intent Assurance using LLMs guided by Intent Drift,Yes.,1.,"""we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.""",2024-02-01T16:09:19Z
Ocassionally Secure: A Comparative Analysis of Code Generation Assistants,Yes.,3.,"""These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.""",2024-02-01T15:49:47Z
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,Yes.,5.,"""However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process... the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.""",2024-02-01T15:18:33Z
Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks,Yes.,5.,"""We uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33%.""",2024-02-01T14:41:20Z
Actor Identification in Discourse: A Challenge for LLMs?,Yes.,5.,"""Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output.""",2024-02-01T14:30:39Z
Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning,Yes.,2.,"""Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data."" and ""But it also leads to extra cost and computation due to the involvement of LLMs in this process.""",2024-02-01T11:57:53Z
EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models,Yes.,1.,"""This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs).""",2024-02-01T11:39:04Z
SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models,Yes.,3.,"""However, their effective application in the medical domain is hampered by a lack of medical domain knowledge.""",2024-02-01T10:26:27Z
From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models,Yes.,1.,"""To bridge this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS (LE-PARIS).""",2024-02-01T08:37:13Z
Prompt-Time Symbolic Knowledge Capture with Large Language Models,Yes.,2.,"""LLMs inherently lack mechanisms for prompt-driven knowledge capture.""",2024-02-01T08:15:28Z
Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection,Yes.,4.,"""Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.""",2024-02-01T08:11:56Z
Investigating Bias Representations in Llama 2 Chat via Activation Steering,Yes.,4.,"""We address the challenge of societal bias in Large Language Models (LLMs),"" and ""Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF).""",2024-02-01T07:48:50Z
What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection,Yes.,3.,"""we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection"" and ""LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.""",2024-02-01T06:21:19Z
"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",Yes.,5.,"""knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge"" and ""Motivated by their failures in self-reflection and over-reliance on held-out sets"".",2024-02-01T06:11:49Z
Safety of Multimodal Large Language Models on Images and Text,Yes.,4.,"""the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios.""",2024-02-01T05:57:10Z
Large Language Models Based Fuzzing Techniques: A Survey,Yes.,1.,"""Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models.""",2024-02-01T05:34:03Z
IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators,Yes.,1.,"""we introduce a general bias detection framework, IndiVec, built upon large language models.""",2024-02-01T05:20:07Z
"Redefining ""Hallucination"" in LLMs: Towards a psychology-informed framework for mitigating misinformation",Yes.,5.,"""a notable challenge surfaces in the form of 'hallucinations.' This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base.""",2024-02-01T03:01:11Z
Multimodal Embodied Interactive Agent for Cafe Scene,,,,2024-02-01T02:43:20Z
PAP-REC: Personalized Automatic Prompt for Recommendation Language Model,Yes.,3.,"""handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes.""",2024-02-01T02:29:16Z
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA,Yes.,3.,"""However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application.""",2024-02-01T02:24:15Z
Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better,,,,2024-02-01T01:23:07Z
Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,Yes.,4.,"""the absence of explicit explainability in LLMs significantly hinders their application in the social sciences.""",2024-02-01T01:17:46Z
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs,Yes.,1.,"""This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline.""",2024-02-01T01:09:00Z
Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning,Yes.,3.,"""This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models.""",2024-02-01T00:23:31Z
