Title,Talks about LLMs,Rate,Evidence,Published
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Yes.,5.,"""We provide empirical results that show that these methods fail to generalize in very basic ways."" and ""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons.""",2023-06-30T23:44:51Z
Large Language Models (GPT) for automating feedback on programming assignments,Yes.,3.,"""This suggests potential over-reliance on GPT-generated feedback.""",2023-06-30T21:57:40Z
Meta-training with Demonstration Retrieval for Efficient Few-shot Learning,Yes.,3.,"""Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive.""",2023-06-30T20:16:22Z
Ticket-BERT: Labeling Incident Management Tickets with Language Models,Yes.,1.,"""To handle these issues, we introduce Ticket-BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets.""",2023-06-30T19:48:25Z
Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models,Yes.,4.,"""Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community.""",2023-06-30T19:39:01Z
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,,,,2023-06-30T17:38:10Z
Biomedical Language Models are Robust to Sub-optimal Tokenization,Yes.,3.,"""Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.""",2023-06-30T13:35:24Z
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,Yes.,3.,"""we argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations.""",2023-06-30T11:32:25Z
Preference Ranking Optimization for Human Alignment,Yes.,4.,"""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks",2023-06-30T09:07:37Z
Provable Robust Watermarking for AI-Generated Text,Yes.,1.,"""We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage.""",2023-06-30T07:24:32Z
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection,Yes.,1.,"""we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment.""",2023-06-30T05:50:26Z
SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization,Yes.,2.,"""Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches.""",2023-06-30T03:14:04Z
Modeling Parallel Programs using Large Language Models,Yes.,1.,"""In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes.""",2023-06-29T19:44:55Z
DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios,Yes.,1.,"""Large Language Models (LLMs) offer a powerful solution to expedite this process through in-context learning.""",2023-06-29T19:24:19Z
Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations,Yes.,3.,"""this naive approach heavily relies on the strong in-context learning ability emerged from large language models, which could suffer from significant latency for online recommendations.""",2023-06-29T18:50:12Z
"Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",Yes.,3.,"""These results also highlight settings where GPT-4 still struggles, providing exciting future directions on developing techniques to improve the performance of these models.""",2023-06-29T17:57:40Z
Concept-Oriented Deep Learning with Large Language Models,Yes.,3.,"""Text-only LLMs, however, can represent only symbolic (conceptual) knowledge.""",2023-06-29T16:47:11Z
UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?,Yes.,1.,"""We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets.""",2023-06-29T13:30:41Z
From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data,Yes.,3.,"""We demonstrate the significant enhancement of LLM expertise on the quality of recovered causal structures from data, while also identifying critical challenges and issues, along with potential approaches to address them.""",2023-06-29T12:48:00Z
Benchmarking Large Language Model Capabilities for Conditional Generation,Yes.,3.,"""Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced."" and ""provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language.""",2023-06-29T08:59:40Z
Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game,Yes.,4.,"""Our results showed that ChatGPT can respond well to conceptual questions, but struggled with decimal place values and number line problems."" and ""We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning.""",2023-06-29T02:28:09Z
A negation detection assessment of GPTs: analysis with the xNot360 dataset,Yes.,5.,"""Our findings expose a considerable performance disparity among the GPT models, with GPT-4 surpassing its counterparts and GPT-3.5 displaying a marked performance reduction. The overall proficiency of the GPT models in negation detection remains relatively modest, indicating that this task pushes the boundaries of their natural language understanding capabilities. We not only highlight the constraints of GPT models in handling negation but also",2023-06-29T02:27:48Z
CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?,Yes.,3.,"""We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabilities, and promote their ongoing development and advancement.""",2023-06-29T02:19:50Z
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,Yes.,5.,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification.""",2023-06-28T21:11:15Z
Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023,Yes.,1.,"""Large language models have demonstrated remarkable commonsense-based reasoning ability.""",2023-06-28T20:33:52Z
Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language,Yes.,1.,"""We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs).""",2023-06-28T17:57:10Z
On the Exploitability of Instruction Tuning,Yes.,3.,"""We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.""",2023-06-28T17:54:04Z
Towards Measuring the Representation of Subjective Global Opinions in Language Models,Yes.,4.,"""By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases."" and ""When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily",2023-06-28T17:31:53Z
Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting,Yes.,2.,"""its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear.""",2023-06-28T14:55:13Z
CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,Yes.,4.,"""Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories.""",2023-06-28T14:14:44Z
Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition,Yes.,3.,"""However, these approaches usually require a significant amount of target domain text data for the training of LMs.""",2023-06-28T08:29:00Z
Query Understanding in the Age of Large Language Models,Yes.,1.,"""Querying, conversing, and controlling search and information-seeking interfaces using natural language are fast becoming ubiquitous with the rise and adoption of large-language models (LLM).""",2023-06-28T08:24:14Z
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,Yes.,3.,"""they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM.""",2023-06-28T03:31:31Z
"Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",,,,2023-06-28T03:03:51Z
HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,Yes.,3.,"""Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA.""",2023-06-27T20:46:34Z
Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese,Yes.,3.,"""while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection.""",2023-06-27T20:37:54Z
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,Yes.,1.,"""we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models.""",2023-06-27T19:29:55Z
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,Yes.,1.,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs.""",2023-06-27T18:03:15Z
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,Yes.,3.,"""existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving.""",2023-06-27T17:05:32Z
"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",Yes.,2.,"""While AI-powered tools can automatically analyze the data, they are prone to inaccuracies as they lack domain-specific expertise.""",2023-06-27T14:46:47Z
Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task,Yes.,4.,"""harmful biases are likely increasingly intertwined with those models"" and ""Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.""",2023-06-27T08:36:35Z
WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models,Yes.,5.,"""We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias.""",2023-06-26T22:07:33Z
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback,Yes.,3.,"""current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment.""",2023-06-26T17:59:50Z
Are aligned neural networks adversarially aligned?,Yes.,5.,"""However, adversarial users can construct inputs which circumvent attempts at alignment."" and ""We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image.""",2023-06-26T17:18:44Z
Exploring the Robustness of Large Language Models for Solving Programming Problems,Yes.,5.,"""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.""",2023-06-26T10:48:50Z
Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference,Yes.,3.,"""Deploying pre-trained transformer models like BERT on downstream tasks in resource-constrained scenarios is challenging due to their high inference cost, which grows rapidly with input sequence length.""",2023-06-26T03:06:57Z
RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,Yes.,3.,"""Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets.""",2023-06-25T19:23:21Z
Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning,Yes.,5.,"""Language models still struggle on moral reasoning, despite their impressive performance in many other tasks."" and ""Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero",2023-06-25T18:40:43Z
Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction,Yes.,1.,"""In this study, we explore distilling the reasoning ability of large language models (LLMs) into a more compact student model by generating a \textit{chain of thought} (CoT) -- a sequence of intermediate reasoning steps.""",2023-06-25T04:33:56Z
Language models are weak learners,Yes.,1.,"""we illustrate that prompt-based large language models can operate effectively as said weak learners.""",2023-06-25T02:39:19Z
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models,Yes.,1.,"""Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching.""",2023-06-25T02:24:30Z
Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles,Yes.,1.,"""we accommodate our logic engine to fit the natural language reasoning patterns LLMs have been trained on.""",2023-06-24T23:33:00Z
On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions,Yes.,4.,"""When treating more general cases, despite the power of LLMs, inherent ambiguity exists and limits their predictive power. We then summarize the challenges and recommend research directions on LLMs to treat the inherent ambiguity of TTP descriptions used in various cyber operations.""",2023-06-24T21:08:15Z
"Symbolic Chain-of-Thought Distillation: Small Models Can Also ""Think"" Step-by-Step",Yes.,3.,"""benefits appear to emerge only for sufficiently large models (beyond 50B parameters).""",2023-06-24T20:15:07Z
H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models,Yes.,3.,"""Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing."" and ""a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size",2023-06-24T20:11:14Z
"My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",Yes.,1.,"""The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models.""",2023-06-24T18:17:38Z
LLM-assisted Generation of Hardware Assertions,Yes.,1.,"""In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions.""",2023-06-24T17:44:36Z
Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?,Yes.,5.,"""However, employing chain-of-thought prompting did not lead to noticeably improved performance on this task. Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model. Finally, we observed that the model is quite brittle, as small formatting related changes in the prompt had a high impact on the predictions.""",2023-06-24T08:48:24Z
IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations,,,,2023-06-24T05:02:34Z
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data,Yes.,1.,"""Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size.""",2023-06-24T02:25:56Z
Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models,Yes.,2.,"""Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure.""",2023-06-23T21:25:38Z
LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding,Yes.,2.,"""Additionally, we demonstrate that LACA can help refine prompts for deductive coding, identify codes for which an LLM is randomly guessing, and help assess when to use LLMs vs. human coders for deductive coding.""",2023-06-23T20:57:32Z
Bring Your Own Data! Self-Supervised Evaluation for Large Language Models,Yes.,3.,"""Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations.""",2023-06-23T17:59:09Z
Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation,No.,1.,"The abstract discusses few-shot imitation learning, control problems, and reinforcement learning but does not mention language models or their limitations.",2023-06-23T15:29:15Z
Knowledge-Infused Self Attention Transformers,Yes.,5.,"""These limitations include hallucinations, where they produce incorrect outputs with high confidence, and alignment issues, where they generate unhelpful and unsafe outputs for human users.""",2023-06-23T13:55:01Z
Efficient Online Processing with Deep Neural Networks,Yes.,3.,"""The economic cost and negative environmental externalities of training and serving models is in evident disharmony with financial viability and climate action goals.""",2023-06-23T12:29:44Z
Product Information Extraction using ChatGPT,Yes.,2.,"""The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data.""",2023-06-23T09:30:01Z
ToolQA: A Dataset for LLM Question Answering with External Tools,Yes.,5.,"""Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning.""",2023-06-23T05:43:28Z
Correcting discount-factor mismatch in on-policy policy gradient methods,No.,1.,N/A (The paper does not mention language models).,2023-06-23T04:10:58Z
Visual Adversarial Examples Jailbreak Aligned Large Language Models,Yes.,5.,"""First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs."" and ""we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification.""",2023-06-22T22:13:03Z
Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation,Yes.,3.,"""However, these models still have limitations when it comes to complex tasks that require an understanding of the user, such as mastering human comedy writing strategies.""",2023-06-22T20:38:52Z
Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs,Yes.,5.,"""LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence."" and ""all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement.""",2023-06-22T17:31:44Z
Towards Explainable Evaluation Metrics for Machine Translation,Yes.,2.,"""recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent.""",2023-06-22T17:07:57Z
Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US,,,,2023-06-22T15:56:50Z
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing,Yes.,3.,"""Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize.""",2023-06-22T14:39:04Z
Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT,Yes.,1.,"""This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology.""",2023-06-22T13:21:20Z
Generative Multimodal Entity Linking,Yes.,2.,"""Existing MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs).""",2023-06-22T07:57:19Z
Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,Yes.,5.,"""Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment.""",2023-06-22T03:56:38Z
Identifying and Extracting Rare Disease Phenotypes with Large Language Models,Yes.,4.,"""While the proliferation of large language models may provide opportunities for supporting RD diagnosis and treatment, researchers and clinicians should critically evaluate model outputs and be well-informed of their limitations.""",2023-06-22T03:52:12Z
FLAG: Finding Line Anomalies (in code) with Generative AI,Yes.,1.,"""In this work, we explore the features that help LLMs in this classification and evaluate the performance of FLAG on known bugs.""",2023-06-22T03:04:56Z
ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,Yes.,5.,"""We find that models struggle even to identify the edits that correspond to a comment, especially in cases where the comment is phrased in an indirect way or where the edit addresses the spirit of a comment but not the precise request. When tasked with generating edits, GPT-4 often succeeds in addressing comments on a surface level, but it rigidly follows the wording of the feedback rather than",2023-06-21T22:00:03Z
Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases,Yes.,5.,"""Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.""",2023-06-21T21:04:11Z
FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair,,,,2023-06-21T19:34:16Z
Joint Prompt Optimization of Stacked LLMs using Variational Inference,Yes.,1.,"""We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned.""",2023-06-21T18:45:56Z
Understanding Social Reasoning in Language Models with Language Models,Yes.,4.,"""understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges",2023-06-21T16:42:15Z
Testing of Detection Tools for AI-Generated Text,Yes.,2.,"""Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content.""",2023-06-21T16:29:44Z
GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks,Yes.,3.,"""each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved.""",2023-06-21T15:42:36Z
Solving and Generating NPR Sunday Puzzles with Large Language Models,Yes.,5.,"""We find no evidence that models can generate puzzles",2023-06-21T13:23:48Z
Limits for Learning with Language Models,,,,2023-06-21T12:11:31Z
Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks,Yes.,3.,"""Lastly, by utilizing a straightforward task in which the model predicts the winner of a Tic Tac Toe game, we identify limitations in attention analysis, particularly its inability to capture 2D patterns.""",2023-06-21T11:48:07Z
OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue,Yes.,2.,"""due to the significant differences between medical images and text and general web content, the performance of LMMs in medical scenarios is limited.""",2023-06-21T11:09:48Z
Interactive Molecular Discovery with Natural Language,Yes.,2.,"""Several typical solutions including large language models (e.g., ChatGPT) are evaluated, proving the challenge of conversational molecular design and the effectiveness of our knowledge enhancement method.""",2023-06-21T02:05:48Z
Opportunities and Risks of LLMs for Scalable Deliberation with Polis,Yes.,5.,"""LLM context limitations have a significant impact on insight and quality of these results.""",2023-06-20T22:52:51Z
Open-Domain Text Evaluation via Meta Distribution Modeling,Yes.,3.,"""evaluating and controlling these models for desired attributes remains a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and METEOR are insufficient for open-ended generation tasks.""",2023-06-20T20:37:54Z
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,Yes.,5.,"""we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history.""",2023-06-20T17:24:23Z
A Simple and Effective Pruning Approach for Large Language Models,Yes.,3.,"""Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive.""",2023-06-20T17:18:20Z
Towards Environmentally Equitable AI via Geographical Load Balancing,Yes.,1.,"""Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny.""",2023-06-20T17:13:33Z
Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion,Yes.,1.,"""Subsequently, the top two captions are fused using a Large Language Model (LLM).""",2023-06-20T15:13:02Z
Hallucination is the last thing you need,Yes.,5.,"""The present offering with generative AI presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures."" and ""this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination.""",2023-06-20T13:14:15Z
TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models,Yes.,4.,"""It is crucial to prioritize human-centered principles when utilizing these models. Safeguarding the ethical and moral compliance of LLMs is of utmost importance. However, individual ethical issues have not been well studied on the latest LLMs.""",2023-06-20T12:53:39Z
Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,Yes.,5.,"""they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.""",2023-06-20T12:21:06Z
"Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Motivations and Formal Specifications",Yes.,3.,"""It is conjectured that the shortcomings of current LLMs are due to a lack of ability to generalize.""",2023-06-20T10:45:56Z
Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts,Yes.,4.,"""competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance.""",2023-06-20T08:27:47Z
ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis,Yes.,3.,"""This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.""",2023-06-20T05:20:29Z
Evaluating the Zero-shot Robustness of Instruction-tuned Language Models,Yes.,5.,"""We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re",2023-06-20T03:48:51Z
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,Yes.,5.,"""Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm."" and ""we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected",2023-06-19T21:14:57Z
Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting,Yes.,1.,"""We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model.""",2023-06-19T15:42:02Z
RepoFusion: Training Code Models to Understand Your Repository,Yes.,5.,"""Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions.""",2023-06-19T15:05:31Z
BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models,Yes.,3.,"""the existing LLMs are usually focused on English, leading to inferior performance in non-English languages.""",2023-06-19T14:30:52Z
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis,No.,1.,"The abstract focuses on Generative Adversarial Networks (GAN) and their application to text synthesis, without mentioning Large Language Models (LLMs).",2023-06-19T10:22:12Z
Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost,Yes.,3.,"""training cross-domain LLMs in the medical field poses significant challenges primarily attributed to the requirement of collecting data from diverse domains. This task becomes particularly difficult due to privacy restrictions and the scarcity of publicly available medical datasets.""",2023-06-19T08:15:14Z
Fine-tuning Large Enterprise Language Models via Ontological Reasoning,Yes.,3.,"""However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience.""",2023-06-19T06:48:45Z
Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix),,,,2023-06-18T22:23:18Z
The Importance of Human-Labeled Data in the Era of LLMs,Yes.,1.,"""The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning.""",2023-06-18T12:12:03Z
Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources,Yes.,1.,"""Large language models (LLMs) appear to offer a promising solution to the rapid creation of learning materials at scale, reducing the burden on instructors.""",2023-06-18T09:49:21Z
"News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",Yes.,4.,"""the AI models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information.""",2023-06-18T04:30:29Z
CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents,Yes.,1.,"""we first present an uncertainty estimation method for LLMs to classify whether the command is certain (i.e., clear) or not (i.e., ambiguous or infeasible).""",2023-06-17T15:24:54Z
LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning,Yes.,1.,"""We utilize a pretrained LLM for generating human-like captions with high quality.""",2023-06-17T13:55:54Z
Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values,Yes.,3.,"""dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models.""",2023-06-16T20:07:38Z
Evaluating Superhuman Models with Consistency Checks,Yes.,3.,"""GPT-4 forecasting that sports records will evolve non-monotonically over time.""",2023-06-16T17:26:38Z
ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation,Yes.,5.,"""Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience.""",2023-06-16T16:56:32Z
Friend or Foe? Exploring the Implications of Large Language Models on the Science System,Yes.,4.,"""The study focused on applications and limitations of LLMs,"" and ""risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education.""",2023-06-16T15:50:17Z
Is Self-Repair a Silver Bullet for Code Generation?,,,,2023-06-16T15:13:17Z
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond,Yes.,5.,"""However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered."" and ""Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process.""",2023-06-16T13:39:35Z
Process Knowledge-infused Learning for Clinician-friendly Explanations,,,,2023-06-16T13:08:17Z
Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System,Yes.,3.,"""However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models.""",2023-06-16T13:04:56Z
Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody,Yes.,5.,"""We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner.""",2023-06-16T12:49:44Z
Full Parameter Fine-tuning for Large Language Models with Limited Resources,Yes.,3.,"""Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training.""",2023-06-16T11:37:15Z
Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models,Yes.,4.,"""We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models.""",2023-06-16T10:36:18Z
Pushing the Limits of ChatGPT on NLP Tasks,Yes.,5.,"""its subpar performance was caused by the following factors",2023-06-16T09:40:05Z
Clickbait Detection via Large Language Models,Yes.,5.,"""Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods"" and ""the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.""",2023-06-16T02:49:20Z
Schema-learning and rebinding as mechanisms of in-context learning and emergence,Yes.,3.,"""Yet the mechanisms that underlie it are poorly understood."" and ""a key property of CSCGs is that, unlike transformer-based LLMs, they are interpretable, which considerably simplifies the task of explaining how ICL works.""",2023-06-16T00:29:19Z
Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses,Yes.,3.,"""Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader.""",2023-06-15T22:12:34Z
Explaining Legal Concepts with Augmented Large Language Models (GPT-4),Yes.,4.,"""we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation"" and ""detailed analysis uncovered limitations in terms of the factual accuracy of the explanations.""",2023-06-15T21:58:18Z
Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models,Yes.,3.,"""Existing VideoQA methods typically take two paradigms",2023-06-15T20:56:20Z
Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health,Yes.,4.,"""We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data.""",2023-06-15T20:19:08Z
Inverse Scaling: When Bigger Isn't Better,Yes.,5.,"""Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data.""",2023-06-15T20:11:23Z
"Explore, Establish, Exploit: Red Teaming Language Models from Scratch",Yes.,5.,"""Deploying large language models (LMs) can pose hazards from harmful outputs such as toxic or false text."" and ""We use this approach to red-team GPT-3 to discover classes of inputs that elicit false statements.""",2023-06-15T18:49:50Z
SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts,Yes.,1.,"""To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale.""",2023-06-15T17:59:47Z
Language-Guided Music Recommendation for Video via Prompt Analogies,Yes.,1.,"""we propose a text-synthesis approach that relies on an analogy-based prompting procedure to generate natural language music descriptions from a large-scale language model (BLOOM-176B).""",2023-06-15T17:58:01Z
Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models,Yes.,4.,"""However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains.""",2023-06-15T17:42:48Z
Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization,Yes.,2.,"""it is unclear whether they also make good teachers for weaker agents"" and ""communication is expensive.""",2023-06-15T17:27:20Z
KoLA: Carefully Benchmarking World Knowledge of Large Language Models,Yes.,2.,"""we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination.""",2023-06-15T17:20:46Z
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation,Yes.,5.,"""Despite recent advances, efficiently processing long documents for intense review/analysis tasks remains an open challenge for language models."" and ""existing publicly available models struggle with most tasks, even after in-domain pretraining.""",2023-06-15T16:19:15Z
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",Yes.,3.,"""while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance.""",2023-06-15T16:01:30Z
CMMLU: Measuring massive multitask language understanding in Chinese,Yes.,5.,"""The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs.""",2023-06-15T15:49:51Z
"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",Yes.,1.,"""Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied.""",2023-06-15T12:45:25Z
DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning,Yes.,5.,"""large language models (LLMs) exhibit poor performance in tackling this subjective domain"" and ""current models defect in the application of pragmatic reasoning.""",2023-06-15T10:41:23Z
Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation,Yes.,3.,"""PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment.""",2023-06-15T06:50:51Z
Toward Grounded Commonsense Reasoning,Yes.,3.,"""Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging.""",2023-06-14T17:30:57Z
Language to Rewards for Robotic Skill Synthesis,Yes.,3.,"""since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot.""",2023-06-14T17:27:10Z
Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models,Yes.,1.,"""Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear.""",2023-06-14T17:15:01Z
"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",Yes.,3.,"""Despite this progress, complex visual-based tasks still remain challenging due to the diverse nature of visual tasks.""",2023-06-14T17:12:56Z
MiniLLM: Knowledge Distillation of Large Language Models,Yes.,2.,"""Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs)."" and ""How to effectively distill the knowledge of white-box LLMs into small models is still under-explored.""",2023-06-14T14:44:03Z
Unifying Large Language Models and Knowledge Graphs: A Roadmap,Yes.,3.,"""However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge.""",2023-06-14T07:15:26Z
Language models are not naysayers: An analysis of language models on negation benchmarks,Yes.,5.,"""we show that LLMs have several limitations including insensitivity to the presence of negation, an inability to capture the lexical semantics of negation, and a failure to reason under negation.""",2023-06-14T01:16:37Z
INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation,Yes.,2.,"""rectifies quantization errors in quantized Large Language Models"" and ""ameliorate the gap between the quantized model and its float point counterpart.""",2023-06-13T22:25:35Z
Large-scale Language Model Rescoring on Long-form Data,Yes.,1.,"""In this work, we study the impact of Large-scale Language Models (LLM) on Automated Speech Recognition (ASR) of YouTube videos, which we use as a source for long-form ASR.""",2023-06-13T20:54:12Z
AVIS: Autonomous Visual Information Seeking with Large Language Model Agent,Yes.,1.,"""Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions.""",2023-06-13T20:50:22Z
Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level,Yes.,2.,"""Existing AI-generated text classifiers have limited accuracy and often produce false positives.""",2023-06-13T20:34:55Z
"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",Yes.,2.,"""In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs.""",2023-06-13T19:51:22Z
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning,Yes.,2.,"""We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 136% compared to solely beginner capabilities, with some of them even outperforming experts. However, ChatGPT does not provide consistent improvements across all scenarios.""",2023-06-13T19:27:18Z
FLamE: Few-shot Learning from Natural Language Explanations,Yes.,4.,"""recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification"" and ""human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.""",2023-06-13T18:01:46Z
XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models,Yes.,3.,"""their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images.""",2023-06-13T17:59:59Z
arXiVeri: Automatic table verification with GPT,Yes.,2.,"""Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4.""",2023-06-13T17:59:57Z
Questioning the Survey Responses of Large Language Models,Yes.,5.,"""models' responses are governed by ordering and labeling biases,"" and ""models' responses do not contain the entropy variations and statistical signals typically found in human populations.""",2023-06-13T17:48:27Z
WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,Yes.,2.,"""we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages.""",2023-06-13T16:57:53Z
Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks,Yes.,2.,"""With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results."" and ""our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human.""",2023-06-13T16:46:24Z
Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control,Yes.,4.,"""First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exempl",2023-06-13T15:49:41Z
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,Yes.,3.,"""However, their proficiency within specialized domains such as biomolecular studies remains limited.""",2023-06-13T14:35:34Z
NoCoLA: The Norwegian Corpus of Linguistic Acceptability,Yes.,2.,"""we lack any tool to evaluate their understanding of grammaticality"" and ""conduct a comparative study of the existing Norwegian language models.""",2023-06-13T14:11:19Z
SqueezeLLM: Dense-and-Sparse Quantization,Yes.,5.,"""However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements.""",2023-06-13T08:57:54Z
Large Language Models Sometimes Generate Purely Negatively-Reinforced Text,Yes.,5.,"""One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong",2023-06-13T06:40:37Z
TART: A plug-and-play Transformer module for task-agnostic reasoning,Yes.,4.,"""In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples."" and ""this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.""",2023-06-13T04:37:00Z
"Assigning AI: Seven Approaches for Students, with Prompts",Yes.,3.,"""despite their inherent risks and limitations"" and ""mitigate risks such as complacency about the AI's output, errors, and biases.""",2023-06-13T03:36:36Z
Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment,Yes.,2.,"""GEMBA, the first MT quality assessment metric based on Large Language Models (LLMs), employs one-step prompting to achieve state-of-the-art (SOTA) in system-level MT quality estimation; however, it lacks segment-level analysis.""",2023-06-13T01:18:32Z
Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling,Yes.,5.,"""LLMs fail at simple linguistic tests for negation or quantifier understanding"" and ""suggests that LLMs do not do as well as expected with quantifiers.""",2023-06-12T19:20:18Z
Lost in Translation: Large Language Models in Non-English Content Analysis,Yes.,5.,"""the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages."" and ""Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular.""",2023-06-12T19:10:47Z
Waffling around for Performance: Visual Classification with Random Words and Broad Concepts,Yes.,2.,"""We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors.""",2023-06-12T17:59:48Z
Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow,Yes.,1.,"""Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner.""",2023-06-12T16:12:56Z
Mitigating Prior Errors in Causal Structure Learning: Towards LLM driven Prior Knowledge,Yes.,3.,"""to tackle erroneous prior causal statements from LLM, which is seldom considered in the current context of expert dominating prior resources.""",2023-06-12T11:24:48Z
Weakly supervised information extraction from inscrutable handwritten document images,No.,1.,The abstract does not mention LLMs or their limitations.,2023-06-12T02:22:30Z
Multimodal Audio-textual Architecture for Robust Spoken Language Understanding,Yes.,3.,"""we investigate impacts of this ASR error propagation on state-of-the-art NLU systems based on pre-trained language models (PLM), such as BERT and RoBERTa.""",2023-06-12T01:55:53Z
TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models,Yes.,4.,"""the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined."" and ""Our work sheds light on the potential security risks in current models and offers a potential defensive approach.""",2023-06-12T01:22:39Z
A blind spot for large language models: Supradiegetic linguistic information,Yes.,5.,"""its deficits can be reframed as ignorance of extradiegetic information, including supradiegetic linguistic information"" and ""We use these concepts to investigate why LLMs like ChatGPT have trouble handling palindromes, the visual characteristics of symbols, translating Sumerian cuneiform, and continuing integer sequences.""",2023-06-11T22:15:01Z
Augmenting Greybox Fuzzing with Generative AI,Yes.,2.,"""The experiment results show that our approach improves the edge coverage by 12.77\% over the SOTA greybox fuzzer (AFL++) on 12 target programs from three well-tested benchmarks. As for vulnerability detection, \sys is able to perform similar to or better than AFL++ for programs",2023-06-11T21:44:47Z
Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis,Yes.,2.,"""Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks.""",2023-06-11T20:50:14Z
The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases,Yes.,1.,"""This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging.""",2023-06-11T20:39:13Z
CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution,Yes.,3.,"""Current LLM-based code translation methods lack a training approach to ensure that the translated code reliably compiles or bears substantial functional equivalence to the input code.""",2023-06-11T19:47:52Z
GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model,Yes.,3.,"""the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods.""",2023-06-11T09:17:21Z
Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method,Yes.,2.,"""The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation.""",2023-06-11T08:53:27Z
RestGPT: Connecting Large Language Models with Real-World RESTful APIs,Yes.,3.,"""existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios.""",2023-06-11T08:53:12Z
Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective,Yes.,1.,"""Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their great powerful capabilities in natural language understanding, generalization, and reasoning, which provides unprecedented opportunities to advance molecule discovery.""",2023-06-11T08:16:25Z
Inductive reasoning in humans and large language models,Yes.,3.,"""Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful",2023-06-11T00:23:25Z
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers,Yes.,3.,"""However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal.""",2023-06-10T21:58:29Z
Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification,Yes.,1.,"""This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs.""",2023-06-10T20:55:21Z
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,Yes.,3.,"""However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains.""",2023-06-10T12:42:36Z
Human-in-the-Loop through Chain-of-Thought,Yes.,5.,"""it sometimes demonstrates its weakness in long-term or multi-step logical reasoning.""",2023-06-10T04:31:57Z
Measuring and Modifying Factual Knowledge in Large Language Models,Yes.,4.,"""existing approaches for knowledge measurement have certain limitations,"" and ""LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods.""",2023-06-09T21:25:48Z
Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording,Yes.,5.,"""LLMs are still not reliable,"" ""no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available,"" and ""The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability",2023-06-09T19:07:31Z
Trapping LLM Hallucinations Using Tagged Context Prompts,Yes.,5.,"""However, these models suffer from 'hallucinations,' where the model generates false or fabricated information.""",2023-06-09T17:48:54Z
Mind2Web: Towards a Generalist Agent for the Web,Yes.,3.,"""While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs."" and ""there is still a substantial room to improve towards truly generalizable agents.""",2023-06-09T17:44:31Z
FinGPT: Open-Source Financial Large Language Models,Yes.,2.,"""Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs).""",2023-06-09T16:52:00Z
S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput,Yes.,5.,"""Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself.""",2023-06-09T16:13:43Z
Language Models Can Learn Exceptions to Syntactic Rules,Yes.,3.,"""At the same time, this hypothesis fails to explain the magnitude of unpassivizability demonstrated by certain individual verbs, suggesting that other cues to exceptionality are available in the linguistic input.""",2023-06-09T15:35:11Z
Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?,Yes.,5.,"""vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text.""",2023-06-09T13:03:53Z
Can Large Language Models Infer Causation from Correlation?,Yes.,5.,"""Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but",2023-06-09T12:09:15Z
Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives,Yes.,1.,"""The development of recent large language models (LLMs), particularly ChatGPT, has also introduced a revolutionary contribution to the way that legal texts can be processed and comprehended.""",2023-06-09T11:57:57Z
How Can Recommender Systems Benefit from Large Language Models: A Survey,Yes.,3.,"""we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics.""",2023-06-09T11:31:50Z
Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation,Yes.,2.,"""Results indicate that LLMs exceed average performance of humans in science, engineering, agronomy, medicine, and art, but fall short in economics, jurisprudence, pedagogy, literature, history, and management.""",2023-06-09T09:52:05Z
Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests,Yes.,4.,"""At the same time, the results highlight the unreliability of LLMs",2023-06-09T07:19:43Z
Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,,,,2023-06-09T05:55:52Z
Customizing General-Purpose Foundation Models for Medical Report Generation,Yes.,2.,"""the scarcity of labelled medical image-report pairs presents great challenges in the development of deep and large-scale neural networks capable of harnessing the potential artificial general intelligence power like large language models (LLMs).""",2023-06-09T03:02:36Z
Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding,Yes.,3.,"""Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations.""",2023-06-08T22:33:22Z
Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,Yes.,1.,"""This work investigates the effectiveness of different pseudonymization techniques, ranging from rule-based substitutions to using pre-trained Large Language Models (LLMs), on a variety of datasets and models used for two widely used NLP tasks",2023-06-08T21:06:19Z
Prompt Injection attack against LLM-integrated Applications,Yes.,5.,"""highlighting the constraints of current attack strategies in practice"" and ""We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection.""",2023-06-08T18:43:11Z
Multi-Modal Classifiers for Open-Vocabulary Object Detection,Yes.,1.,"""we prompt a large language model (LLM) to generate informative language descriptions for object classes.""",2023-06-08T18:31:56Z
Artificial General Intelligence for Medical Imaging,Yes.,2.,"""we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field.""",2023-06-08T18:04:13Z
Grounded Text-to-Image Synthesis with Attention Refocusing,Yes.,3.,"""However, these models still fail to precisely follow the text prompt involving multiple objects, attributes, or spatial compositions.""",2023-06-08T17:59:59Z
MIMIC-IT: Multi-Modal In-Context Instruction Tuning,Yes.,1.,"""Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs.""",2023-06-08T17:59:56Z
ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases,Yes.,2.,"""it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training.""",2023-06-08T15:46:32Z
"PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",Yes.,2.,"""uncovering their strengths and weaknesses in handling critical financial tasks.""",2023-06-08T14:20:29Z
"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",Yes.,5.,"""We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions.""",2023-06-08T13:21:29Z
Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures,Yes.,4.,"""However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of parts and the precise location of assembly.""",2023-06-08T13:10:00Z
Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming,Yes.,2.,"""We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive).""",2023-06-08T12:22:56Z
Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet,No.,1.,"The abstract discusses generative AI tools like DALL-E, MidJourney, and ChatGPT but does not specifically focus on language models or their limitations.",2023-06-08T11:14:51Z
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization,Yes.,2.,"""Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models.""",2023-06-08T10:41:56Z
Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models,Yes.,2.,"""large language models (LLMs) which excel at textual tasks, are probably not the best tool for modeling tabular data.""",2023-06-08T09:12:28Z
Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models,Yes.,1.,"""The experiments show that with the PLMs, the dependence on labeled training data has been greatly reduced, and the performance has improved. Meanwhile, we verify that ChatGPT, a renowned LLM, has potential for further advancement in this area.""",2023-06-08T07:10:39Z
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs,Yes.,1.,"""We further demonstrate FedSecurity's utility and adaptability through federated training of Large Language Models (LLMs), showcasing its potential to impact a wide range of complex applications.""",2023-06-08T06:21:35Z
covLLM: Large Language Models for COVID-19 Biomedical Literature,Yes.,1.,"""A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing.""",2023-06-08T04:08:32Z
Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning,Yes.,1.,"""pre-trained large language models such as GPT-3 (low paid).""",2023-06-08T04:04:47Z
Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts,Yes.,3.,"""there is a large performance gap between supernet and training from scratch"" and ""supernet cannot be directly used and retraining is necessary after finding the optimal architectures.""",2023-06-08T00:35:36Z
"Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",Yes.,3.,"""Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance of a dedicated and task-aligned datasets dataset for the target task.""",2023-06-07T22:56:53Z
Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation,Yes.,2.,"""Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging.""",2023-06-07T22:20:51Z
"A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",Yes.,1.,"""The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs.""",2023-06-07T21:51:56Z
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models,Yes.,3.,"""Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies.""",2023-06-07T20:12:29Z
Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models,Yes.,4.,"""However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased."" and ""We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of",2023-06-07T19:49:41Z
ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems,Yes.,3.,"""complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.""",2023-06-07T19:37:55Z
Soft-prompt Tuning for Large Language Models to Evaluate Bias,Yes.,4.,"""Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues.""",2023-06-07T19:11:25Z
ModuleFormer: Modularity Emerges from Mixture-of-Experts,Yes.,5.,"""existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge.""",2023-06-07T17:59:57Z
On the Reliability of Watermarks for Large Language Models,,,,2023-06-07T17:58:48Z
"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",Yes.,3.,"""We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks.""",2023-06-07T17:47:03Z
The Two Word Test: A Semantic Benchmark for Large Language Models,Yes.,5.,"""Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the",2023-06-07T17:22:03Z
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions,Yes.,4.,"""Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.""",2023-06-07T16:50:03Z
"ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",Yes.,5.,"""ChatGPT has not solved computational humor yet but it can be a big leap toward 'funny' machines.""",2023-06-07T16:10:21Z
StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code,Yes.,3.,"""We also find that nondeterministic LLM sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code LLMs.""",2023-06-07T16:03:55Z
Long-form analogies generated by chatGPT lack human-like psycholinguistic properties,Yes.,5.,"""These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text.""",2023-06-07T15:42:31Z
PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,Yes.,5.,"""Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts.""",2023-06-07T15:37:00Z
Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs,Yes.,3.,"""We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language.""",2023-06-07T15:33:07Z
Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering,Yes.,2.,"""it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering.""",2023-06-07T15:20:24Z
STEPS: A Benchmark for Order Reasoning in Sequential Tasks,Yes.,3.,"""The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs.""",2023-06-07T13:58:55Z
"On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",Yes.,2.,"""We start by examining the unsatisfactory performance of existing ChatGPT detecting tools and the challenges faced by human evaluators.""",2023-06-07T12:33:24Z
Multilingual Clinical NER: Translation or Cross-lingual Transfer?,Yes.,2.,"""Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language."" and ""While they can take advantage of monoling",2023-06-07T12:31:07Z
Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks,Yes.,1.,"""Finally, we scale up mPLUG-video based on the frozen Bloomz with only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate impressive instruction and video understanding ability.""",2023-06-07T11:52:36Z
GPT Self-Supervision for a Better Data Annotator,Yes.,3.,"""significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist.""",2023-06-07T11:33:14Z
"Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",Yes.,3.,"""The findings revealed varying levels of agreement in chatbot's responses over time, with some scales displaying excellent agreement while others demonstrated poor agreement.""",2023-06-07T10:14:17Z
Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions,,,,2023-06-07T04:27:09Z
Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,Yes.,5.,"""However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.""",2023-06-07T04:15:21Z
An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,Yes.,4.,"""The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases."" and ""are less effective when it comes to racial and religious bias, which may be attributed to",2023-06-06T23:56:18Z
Certified Deductive Reasoning with Language Models,Yes.,5.,"""Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, even when arriving at a correct final answer, their rationales are often logically unsound or inconsistent.""",2023-06-06T21:49:00Z
Büyük dil modellerinin Türkçe verisetleri ile eğitilmesi ve ince ayarlanması,Yes.,3.,"""When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets.""",2023-06-06T19:31:08Z
Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction,Yes.,2.,"""their widespread deployment is limited by the substantial quantities of task-specific data required for training.""",2023-06-06T18:42:08Z
MISGENDERED: Limits of Large Language Models in Understanding Pronouns,Yes.,5.,"""We comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns."" and ""When prompted out-of-the-box, language models perform poorly at correctly predicting neo",2023-06-06T18:27:52Z
ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory,Yes.,5.,"""mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning.""",2023-06-06T17:58:24Z
Deductive Verification of Chain-of-Thought Reasoning,Yes.,4.,"""its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks.""",2023-06-06T17:18:56Z
Can large language models democratize access to dual-use biotechnology?,Yes.,5.,"""However, these models may also confer easy access to dual-use technologies capable of inflicting great harm."" and ""Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.""",2023-06-06T15:52:05Z
The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter,Yes.,2.,"""With exploding parameter counts, Lottery Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in sparsifying them due to high computation and memory bottleneck of repetitive train-prune-retrain routine of iterative magnitude pruning (IMP) which worsens with increasing model size.""",2023-06-06T15:49:09Z
Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models,Yes.,3.,"""However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts.""",2023-06-06T15:43:16Z
Towards End-to-end Speech-to-text Summarization,Yes.,2.,"""the performance of the E2E model still lies behind the cascade one, which is object of an extensive analysis that includes future directions to close that gap.""",2023-06-06T15:22:16Z
Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach,Yes.,5.,"""interactions with LLMs can be time-consuming. In many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency.""",2023-06-06T11:49:09Z
Language acquisition: do children and language models follow similar learning stages?,Yes.,1.,"""we here compare the learning trajectories of deep language models to those of children.""",2023-06-06T11:08:20Z
An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge,Yes.,1.,"""We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task.""",2023-06-06T10:08:12Z
Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models,Yes.,4.,"""The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation.""",2023-06-06T08:47:42Z
Natural Language Commanding via Program Synthesis,Yes.,3.,"""While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations.""",2023-06-06T07:28:49Z
Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5.,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs.""",2023-06-06T06:35:27Z
On the Role of Attention in Prompt-tuning,Yes.,2.,"""Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting.""",2023-06-06T06:23:38Z
Prompting Large Language Models to Reformulate Queries for Moment Localization,Yes.,1.,"""Inspired by the recent success of large language models, especially their ability of understanding and generating complex natural language contents, in this extended abstract, we make early attempts at reformulating the moment queries into a set of instructions using large language models and making them more friendly to the localization models.""",2023-06-06T05:48:09Z
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model,Yes.,3.,"""Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.""",2023-06-06T01:26:53Z
Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents,Yes.,3.,"""Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations.""",2023-06-05T23:55:37Z
shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation,Yes.,3.,"""Instruction-tuned generative Large language models (LLMs) like ChatGPT and Bloomz possess excellent generalization abilities, but they face limitations in understanding radiology reports, particularly in the task of generating the IMPRESSIONS section from",2023-06-05T21:33:04Z
Early Weight Averaging meets High Learning Rates for LLM Pre-training,Yes.,1.,"""Training Large Language Models (LLMs) incurs significant cost; hence, any strategy that accelerates model convergence is helpful.""",2023-06-05T20:51:44Z
NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks,Yes.,2.,"""Finetuning large language models inflates the costs of NLU applications and remains the bottleneck of development cycles.""",2023-06-05T19:30:41Z
ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs,Yes.,1.,"""Results demonstrate two ways to effectively increase the accuracy of mapping suggestions without modifying the underlying AI models.""",2023-06-05T19:26:21Z
A Static Evaluation of Code Completion by Large Language Models,Yes.,4.,"""Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.""",2023-06-05T19:23:34Z
InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models,Yes.,1.,"""it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden.""",2023-06-05T17:55:22Z
Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs,Yes.,5.,"""Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone.""",2023-06-05T17:55:05Z
SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,Yes.,3.,"""quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments.""",2023-06-05T17:53:28Z
Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion,Yes.,3.,"""It remains unclear whether pre-trained large language models (LLMs) can flexibly handle Japanese honorifics like humans.""",2023-06-05T17:27:48Z
Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset,Yes.,5.,"""The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could",2023-06-05T16:48:41Z
SelfEvolve: A Code Evolution Framework via Large Language Models,Yes.,3.,"""the performance of these retrieval-based methods is limited by the strength of the retrievers used"" and ""while LLMs show great emergent ability, they still struggle to produce the correct code in one turn.""",2023-06-05T14:12:46Z
Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs,Yes.,1.,"""Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents.""",2023-06-05T13:35:01Z
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,Yes.,1.,"""Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video.""",2023-06-05T13:17:27Z
Cheap-fake Detection with LLM using Prompt Engineering,Yes.,1.,"""We enhance the baseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a feature extractor.""",2023-06-05T11:01:00Z
Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia,Yes.,3.,"""The findings highlight the promising potential of LLMs across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity.""",2023-06-05T06:01:00Z
User Behavior Simulation with Large Language Model based Agents,Yes.,1.,"""large language models (LLMs) can achieve human-like intelligence"" and ""we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors.""",2023-06-05T02:58:35Z
Evaluation of AI Chatbots for Patient-Specific EHR Questions,Yes.,1.,"""This paper investigates the use of artificial intelligence chatbots for patient-specific question answering (QA) from clinical notes using several large language model (LLM) based systems",2023-06-05T02:52:54Z
Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning,Yes.,3.,"""we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers.""",2023-06-04T17:02:59Z
Commonsense Knowledge Transfer for Pre-trained Language Models,Yes.,3.,"""pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text.""",2023-06-04T15:44:51Z
Exposing Bias in Online Communities through Large-Scale Language Models,Yes.,4.,"""While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes."" and ""This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets",2023-06-04T08:09:26Z
OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models,Yes.,2.,"""Large language models (LLMs) with hundreds of billions of parameters require powerful server-grade GPUs for inference, limiting their practical deployment.""",2023-06-04T06:33:13Z
Probing Physical Reasoning with Counter-Commonsense Context,Yes.,5.,"""The results show that while large language models can use prepositions such as 'in' and 'into' in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.""",2023-06-04T04:24:43Z
Large Language Model Augmented Narrative Driven Recommendations,Yes.,1.,"""In this work, we explore using large language models (LLMs) for data augmentation to train NDR models.""",2023-06-04T03:46:45Z
Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model,Yes.,3.,"""Despite its success, an embedded vector (Sen2Vec) representing a point estimate does not naturally express uncertainty in a taskagnostic way.""",2023-06-04T03:26:43Z
Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions,Yes.,3.,"""Its limited capability for real-world engagement and the absence of benchmarks contribute to these uncertainties.""",2023-06-04T01:07:20Z
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts,Yes.,3.,"""the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation.""",2023-06-03T22:35:27Z
Towards Coding Social Science Datasets with Language Models,Yes.,1.,"""Recent advances in a specific kind of artificial intelligence tool - language models (LMs) - provide a solution to this problem.""",2023-06-03T19:11:34Z
Unsupervised Human Activity Recognition through Two-stage Prompting with ChatGPT,Yes.,3.,"""previous prompt engineering for ChatGPT exhibits limited generalization ability when dealing with a list of words (i.e., sequence of objects) due to the similar weighting assigned to each word in the list.""",2023-06-03T15:41:59Z
Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions,Yes.,1.,"""we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task.""",2023-06-03T14:57:47Z
MultiLegalPile: A 689GB Multilingual Legal Corpus,Yes.,1.,"""Large, high-quality datasets are crucial for training Large Language Models (LLMs).""",2023-06-03T10:10:38Z
LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas,Yes.,1.,"""Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-based techniques in an integer list manipulation domain.""",2023-06-03T08:24:53Z
On Optimal Caching and Model Multiplexing for Large Model Inference,Yes.,3.,"""Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference.""",2023-06-03T05:01:51Z
Can Contextual Biasing Remain Effective with Whisper and GPT-2?,Yes.,3.,"""Despite the large amount of training data, infrequent content words that occur in a particular task may still exhibit poor ASR performance, with contextual biasing a possible remedy.""",2023-06-02T22:56:01Z
AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,Yes.,4.,"""a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs"" and ""We reflect on the unique challenges that arise in providing transparency for LLMs.""",2023-06-02T22:51:26Z
Revisiting the Role of Language Priors in Vision-Language Models,Yes.,4.,"""some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions"" and ""even a 'blind' language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.""",2023-06-02T19:19:43Z
Knowledge of cultural moral norms in large language models,Yes.,4.,"""We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms.""",2023-06-02T18:23:35Z
Evaluating Language Models for Mathematics through Interactions,Yes.,4.,"""Static assessment fails to account for the essential interactive element in LLM deployment, and therefore limits how we understand language model capabilities."" and ""humans should be aware of language models' algebraic fallibility and discern where they are appropriate to use.""",2023-06-02T17:12:25Z
Fine-Grained Human Feedback Gives Better Rewards for Language Model Training,Yes.,5.,"""Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs.""",2023-06-02T17:11:37Z
MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates,Yes.,3.,"""Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes.""",2023-06-02T17:00:19Z
Harnessing large-language models to generate private synthetic text,Yes.,3.,"""However, generating private synthetic data is much harder than training a private model."" and ""executing it has proven problematic. Previous approaches either show significant performance loss, or have, as we show, critical design flaws.""",2023-06-02T16:59:36Z
Log Parsing: How Far Can ChatGPT Go?,Yes.,3.,"""However, its performance in automated log parsing remains unclear."" and ""Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.""",2023-06-02T14:58:43Z
EmoUS: Simulating User Emotions in Task-Oriented Dialogues,Yes.,1.,"""Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.""",2023-06-02T14:48:19Z
PassGPT: Password Modeling and (Guided) Generation with Large Language Models,Yes.,1.,"""Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision.""",2023-06-02T13:49:53Z
"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",Yes.,5.,"""we discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.""",2023-06-02T12:47:45Z
Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations,Yes.,1.,"""we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task.""",2023-06-02T12:00:03Z
ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?,Yes.,3.,"""Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems.""",2023-06-02T09:15:01Z
An Empirical Study on Challenging Math Problem Solving with GPT-4,Yes.,1.,"""Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields.""",2023-06-02T08:02:15Z
MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models,Yes.,1.,"""Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct",2023-06-02T07:21:03Z
Egocentric Planning for Scalable Embodied Task Achievement,Yes.,3.,"""This work offers a solid baseline for studying end-to-end and hybrid methods that aim to generalize to new tasks, including recent approaches relying on LLMs, but often struggle to scale to long sequences of actions or produce robust plans for novel tasks.""",2023-06-02T06:41:24Z
ChatGPT is a Remarkable Tool -- For Experts,Yes.,5.,"""These limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation.""",2023-06-02T06:28:21Z
KL-Divergence Guided Temperature Sampling,Yes.,3.,"""As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual.""",2023-06-02T06:11:26Z
How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?,Yes.,5.,"""we check for inconsistencies and hallucinations in the summaries"" and ""we often find inconsistent or hallucinated information in the generated abstractive summaries"" and ""our investigation indicates that the pre-trained abstractive summarization models and LLMs are not yet ready for fully automatic deployment for case judgement summarization; rather a human-in-the-loop approach including manual checks for inconsistencies is more",2023-06-02T03:16:19Z
Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators,Yes.,3.,"""The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion."" and ""We further propose and compare two paradigms for implementing the first two capabilities. One is to leverage the generic knowledge of LLMs themselves via prompt engineering while the other is to adopt domain",2023-06-02T02:42:58Z
Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation,,,,2023-06-02T00:57:03Z
Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation,Yes.,5.,"""However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline."" and ""illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.""",2023-06-01T22:43:37Z
Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study,Yes.,4.,"""a closer examination of the texts generated by ChatGPT through human evaluations has shown that there are still critical issues in terms of text coherence, faithfulness, and style.""",2023-06-01T21:58:33Z
Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data,,,,2023-06-01T20:56:34Z
LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization,Yes.,1.,"""Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks.""",2023-06-01T19:33:21Z
Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes,Yes.,5.,"""Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions for the same action (e.g., marinate the meat vs. marinate the meat and leave overnight) and infrequently occurring patterns (e.g., add salt occurs far more frequently than",2023-06-01T18:49:47Z
AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,Yes.,3.,"""Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth).""",2023-06-01T17:59:10Z
Exposing Attention Glitches with Flip-Flop Language Modeling,Yes.,5.,"""The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought."" and ""We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors,",2023-06-01T17:44:35Z
Vocabulary-free Image Classification,Yes.,2.,"""Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving.""",2023-06-01T17:19:43Z
The feasibility of artificial consciousness through the lens of neuroscience,Yes.,4.,"""the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us,"" and ""the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals.""",2023-06-01T17:18:15Z
LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,Yes.,1.,"""Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text.""",2023-06-01T16:50:07Z
"Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",Yes.,1.,"""utilizing the powerful capabilities of large language models for choosing the suitable trigger and text-guided image editing techniques for generating the poisoned image with the trigger.""",2023-06-01T15:42:06Z
GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?,Yes.,2.,"""However, considering the prohibitively high memory and computational cost for implementing such a large model, the conventional models (such as CNN and ViT), are still essential for many visual perception tasks.""",2023-06-01T14:02:45Z
Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,Yes.,3.,"""Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs.""",2023-06-01T13:20:22Z
ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing,Yes.,3.,"""The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs.""",2023-06-01T12:45:53Z
Analysis of ChatGPT on Source Code,Yes.,3.,"""While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely.""",2023-06-01T12:12:59Z
Chain-Of-Thought Prompting Under Streaming Batch: A Case Study,Yes.,3.,"""However, developing effective prompts can be a challenging and labor-intensive task."" and ""Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption.""",2023-06-01T11:11:39Z
CapText: Large Language Model-based Caption Generation From Image Context and Description,Yes.,1.,"""We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly.""",2023-06-01T02:40:44Z
Rethinking Model Evaluation as Narrowing the Socio-Technical Gap,Yes.,4.,"""The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with."" and ""we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods with an acknowledgment of trade-offs between realism to socio-requirements and pragmatic costs to conduct the evaluation.""",2023-06-01T00:01:43Z
An Invariant Learning Characterization of Controlled Text Generation,Yes.,3.,"""researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text"" and ""the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on.""",2023-05-31T21:35:08Z
Automated Annotation with Generative AI Requires Validation,Yes.,4.,"""their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans.""",2023-05-31T20:50:45Z
Measuring the Robustness of NLP Models to Domain Shifts,Yes.,3.,"""both model types suffer from drops upon domain shifts"" and ""few-shot LLMs often surpass them cross-domain, showing better robustness.""",2023-05-31T20:25:08Z
"Better patching using LLM prompting, via Self-Consistency",Yes.,2.,"""Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations.""",2023-05-31T18:28:46Z
Improving CLIP Training with Language Rewrites,Yes.,3.,"""Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image."" and ""However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image.""",2023-05-31T17:59:04Z
Scaling Evidence-based Instructional Design Expertise through Large Language Models,Yes.,3.,"""We discuss the benefits and limitations of AI-driven content generation, emphasizing the necessity of human oversight in ensuring the quality of educational materials.""",2023-05-31T17:54:07Z
Decision-Oriented Dialogue for Human-AI Collaboration,Yes.,3.,"""we highlight a number of challenges models face in decision-oriented dialogues, ranging from efficient communication to reasoning and optimization.""",2023-05-31T17:50:02Z
Let's Verify Step by Step,Yes.,5.,"""However, even state-of-the-art models still regularly produce logical mistakes.""",2023-05-31T17:24:00Z
Supplementary Features of BiLSTM for Enhanced Sequence Labeling,,,,2023-05-31T15:05:25Z
Revisiting the Reliability of Psychological Scales on Large Language Models,Yes.,2.,"""However, the suitability of employing psychological scales, initially devised for humans, on LLMs is a matter of ongoing debate.""",2023-05-31T15:03:28Z
Neuron to Graph: Interpreting Language Model Neurons at Scale,Yes.,2.,"""Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown.""",2023-05-31T14:44:33Z
A Survey on Large Language Models for Recommendation,,,,2023-05-31T13:51:26Z
Red Teaming Language Model Detectors with Language Models,Yes.,4.,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.""",2023-05-31T10:08:37Z
CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,Yes.,2.,"""the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption.""",2023-05-31T05:24:48Z
Large Language Models Are Not Strong Abstract Reasoners,Yes.,5.,"""However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed."" and ""We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks.""",2023-05-31T04:50:29Z
Catalysis distillation neural network for the few shot open catalyst challenge,No.,1.,The abstract focuses on the integration of artificial intelligence in computational chemistry methods and does not discuss language models.,2023-05-31T04:23:56Z
Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning,Yes.,1.,"""Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources.""",2023-05-31T03:18:03Z
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning,Yes.,3.,"""While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues.""",2023-05-31T00:55:40Z
Self-Verification Improves Few-Shot Clinical Information Extraction,Yes.,4.,"""However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health.""",2023-05-30T22:05:11Z
Stable Anisotropic Regularization,Yes.,2.,"""Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space).""",2023-05-30T18:57:45Z
GPT4GEO: How a Language Model Sees the World's Geography,Yes.,3.,"""We provide a broad characterisation of what GPT-4 (without plugins or Internet access) knows about the world, highlighting both potentially surprising capabilities but also limitations.""",2023-05-30T18:28:04Z
SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models,Yes.,1.,"""With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal.""",2023-05-30T17:59:30Z
Grammar Prompting for Domain-Specific Language Generation with Large Language Models,Yes.,3.,"""However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars.""",2023-05-30T17:26:01Z
Controlled Text Generation with Hidden Representation Transformations,Yes.,1.,"""We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity).""",2023-05-30T17:21:17Z
The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,Yes.,4.,"""it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.""",2023-05-30T17:02:58Z
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models,Yes.,3.,"""gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG.""",2023-05-30T16:31:26Z
LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images,Yes.,1.,"""Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights.""",2023-05-30T16:09:16Z
Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate,Yes.,5.,"""Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks,"" and ""our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem",2023-05-30T15:25:45Z
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale,Yes.,5.,"""there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the underlying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will",2023-05-30T15:15:40Z
Does Conceptual Representation Require Embodiment? Insights From Large Language Models,Yes.,3.,"""These results highlight the limitations of language in isolation, and that the integration of diverse modalities of inputs leads to a more human-like conceptual representation.""",2023-05-30T15:06:28Z
"GPT Models in Construction Industry: Opportunities, Limitations, and a Use Case Validation",,,,2023-05-30T12:50:51Z
"Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",Yes.,5.,"""However, for more complex mathematical problems or advanced logic tasks, their answers, although written in a usually 'convincing' way, may not be reliable. Consistency is also an issue, as many times a chatbot will provide conflicting answers when given the same question more than once.""",2023-05-30T11:18:05Z
Multitask learning for recognizing stress and depression in social media,No.,1.,No mention of language models in the abstract or title.,2023-05-30T10:04:01Z
AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,Yes.,1.,"""To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs).""",2023-05-30T09:54:20Z
Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge,Yes.,4.,"""these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result.""",2023-05-30T08:34:13Z
Universality and Limitations of Prompt Tuning,Yes.,5.,"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""",2023-05-30T06:47:07Z
GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction,Yes.,1.,"""This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools.""",2023-05-30T05:27:21Z
AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity Matching using Adapter-tuning,Yes.,3.,"""sequential fine-tuning of overparameterized PrLMs can lead to catastrophic forgetting, especially in low-resource scenarios.""",2023-05-30T04:03:23Z
Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,Yes.,4.,"""However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications).""",2023-05-30T03:00:30Z
Faith and Fate: Limits of Transformers on Compositionality,Yes.,5.,"""Yet, these models simultaneously show failures on surprisingly trivial problems."" and ""Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.""",2023-05-29T23:24:14Z
How Effective Are Neural Networks for Fixing Security Vulnerabilities,Yes.,5.,"""Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to",2023-05-29T20:50:27Z
Controllable Text-to-Image Generation with GPT-4,Yes.,1.,"""Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning.""",2023-05-29T19:56:47Z
Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy,,,,2023-05-29T19:48:37Z
Direct Preference Optimization: Your Language Model is Secretly a Reward Model,Yes.,3.,"""achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training."" and ""RLHF is a complex and often unstable procedure.""",2023-05-29T17:57:46Z
LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections,Yes.,3.,"""However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning.""",2023-05-29T17:56:35Z
Contextual Object Detection with Multimodal Large Language Models,Yes.,3.,"""Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection.""",2023-05-29T17:50:33Z
Beyond Confidence: Reliable Models Should Also Consider Atypicality,Yes.,3.,"""we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models.""",2023-05-29T17:37:09Z
Do Language Models Know When They're Hallucinating References?,Yes.,5.,"""State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.""",2023-05-29T17:12:03Z
Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models,Yes.,5.,"""To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs."" and ""We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts.""",2023-05-29T16:29:22Z
Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning,Yes.,2.,"""designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem (MWP) solving.""",2023-05-29T16:01:40Z
LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning,Yes.,2.,"""However, these models often struggle when fine-tuned on small datasets.""",2023-05-29T15:59:51Z
Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods,Yes.,2.,"""However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored.""",2023-05-29T15:31:29Z
Do Large Language Models Know What They Don't Know?,Yes.,5.,"""Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend."" and ""Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.""",2023-05-29T15:30:13Z
Multiscale Positive-Unlabeled Detection of AI-Generated Texts,Yes.,3.,"""mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews.""",2023-05-29T15:25:00Z
Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models,Yes.,3.,"""current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion.""",2023-05-29T15:14:09Z
ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback,Yes.,1.,"""Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery.""",2023-05-29T14:43:24Z
VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset,Yes.,1.,"""we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions.""",2023-05-29T14:34:50Z
ANPL: Towards Natural Programming with Interactive Decomposition,Yes.,3.,"""Though LLMs are capable of generating plausible programs, it's challenging to interact with the LLMs further to revise the program, especially if the user's specific requirements are different from the initial proposal.""",2023-05-29T14:19:40Z
BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages,Yes.,3.,"""many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored.""",2023-05-29T14:07:52Z
Game of Tones: Faculty detection of GPT-4 generated content in university assessments,Yes.,2.,"""This suggests that the use of adversarial techniques regarding prompt engineering is an effective method in evading AI detection tools and highlights that improvements to AI detection software are needed.""",2023-05-29T13:31:58Z
Image Captioning with Multi-Context Synthetic Data,Yes.,3.,"""However, existing methods struggle to attain satisfactory performance solely through synthetic data.""",2023-05-29T13:18:59Z
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets,Yes.,3.,"""even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks.""",2023-05-29T12:37:21Z
"Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",Yes.,4.,"""Subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and LOL",2023-05-29T12:26:44Z
Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation,Yes.,3.,"""Large diffusion models have been successful in text-to-audio (T2A) synthesis tasks, but they often suffer from common issues such as semantic misalignment and poor temporal consistency due to limited natural language understanding and data scarcity.""",2023-05-29T10:41:28Z
Large Language Models are not Fair Evaluators,Yes.,5.,"""we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models.""",2023-05-29T07:41:03Z
LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,Yes.,3.,"""We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further.""",2023-05-29T05:22:11Z
Baselines for Identifying Watermarked Large Language Models,Yes.,1.,"""We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs).""",2023-05-29T04:26:16Z
Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models,Yes.,2.,"""Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful.""",2023-05-29T04:19:35Z
Taming AI Bots: Controllability of Neural States in Large Language Models,Yes.,3.,"""The fact that AI bots are controllable means that an adversary could steer them towards any state.""",2023-05-29T03:58:33Z
"Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",,,,2023-05-28T22:46:21Z
Transfer Learning for Power Outage Detection Task with Limited Training Data,Yes.,3.,"""Results show that while classical models outperform zero-shot Language Models, few-shot fine-tuning significantly improves their performance."" and ""Our evaluation provides insights into the potential of few-shot fine-tuning with Language Models for power outage detection, highlighting their strengths and limitations.""",2023-05-28T22:36:35Z
Targeted Data Generation: Finding and Fixing Model Weaknesses,Yes.,3.,"""Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust.""",2023-05-28T19:36:50Z
Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR,Yes.,1.,"""We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text.""",2023-05-28T19:31:45Z
Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data,Yes.,1.,"""the results are validated through language models trained on biomedical literature, such as BlueBERT and other large language models trained on medical corpora.""",2023-05-28T17:07:46Z
Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective,Yes.,4.,"""We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework.""",2023-05-28T16:04:48Z
Mitigating Label Biases for In-context Learning,Yes.,5.,"""domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples.""",2023-05-28T15:37:39Z
Conformal Prediction with Large Language Models for Multi-Choice Question Answering,Yes.,2.,"""We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications.""",2023-05-28T15:26:10Z
LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning,Yes.,2.,"""their deployment is still hindered by the vast model scale and computational costs.""",2023-05-28T15:15:48Z
Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs,Yes.,3.,"""their inclusivity and effectiveness remain limited for non-Latin scripts and low-resource languages.""",2023-05-28T14:48:38Z
FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions,Yes.,2.,"""However, these models frequently produce generic captions and may omit semantically important image details.""",2023-05-28T13:16:03Z
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers,Yes.,3.,"""these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs.""",2023-05-28T13:08:13Z
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks,Yes.,3.,"""However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy."" and ""these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required.""",2023-05-28T13:00:00Z
KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application,Yes.,4.,"""Large language models (LLMs) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications.""",2023-05-28T12:07:16Z
SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration,Yes.,4.,"""The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising.""",2023-05-28T11:51:20Z
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation,Yes.,5.,"""A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators."" and ""this persuasiveness may result in incorrect judgments about the hatefulness of the content.""",2023-05-28T10:05:13Z
Reward Collapse in Aligning Large Language Models,Yes.,5.,"""we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training.""",2023-05-28T02:12:00Z
Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds,Yes.,1.,"""Could we leverage the recent advances on pre-trained Large Language Models (LLMs) to enable classical planning systems to deal with novel situations?""",2023-05-27T22:30:15Z
Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,Yes.,5.,"""Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.""",2023-05-27T19:08:04Z
The Curse of Recursion: Training on Generated Data Makes Models Forget,Yes.,5.,"""We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs.""",2023-05-27T15:10:41Z
FERMAT: An Alternative to Accuracy for Numerical Reasoning,Yes.,5.,"""While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning.""",2023-05-27T15:00:45Z
What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks,Yes.,3.,"""In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks.""",2023-05-27T14:17:33Z
Query-Efficient Black-Box Red Teaming via Bayesian Optimization,Yes.,3.,"""The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways."" and ""Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations,",2023-05-27T11:00:15Z
Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making,Yes.,3.,"""However, the adversarial attack task has found that PLMs are vulnerable to small perturbations.""",2023-05-27T10:33:53Z
SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks,Yes.,1.,"""SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance.""",2023-05-27T07:04:15Z
Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques,Yes.,3.,"""Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs).""",2023-05-27T06:09:03Z
Augmenting Large Language Model Translators via Translation Memories,Yes.,1.,"""We find that the ability of LLMs to ``understand'' prompts is indeed helpful for making better use of TMs.""",2023-05-27T04:47:09Z
DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text,Yes.,3.,"""Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.""",2023-05-27T03:58:29Z
Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,Yes.,3.,"""This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models."" and ""Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.""",2023-05-26T23:46:42Z
Improved Instruction Ordering in Recipe-Grounded Conversation,Yes.,3.,"""Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order."" and ""we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order",2023-05-26T21:57:11Z
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended),Yes.,3.,"""Through comprehensive ablations and error analyses, we shed light on the strengths and weaknesses of our framework, offering valuable insights into Text-to-SQL's future work.""",2023-05-26T21:39:05Z
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning,Yes.,5.,"""Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited."" and ""Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are 'lazy learners' that tend to exploit shortcuts in prompts for downstream tasks.""",2023-05-26T20:56:30Z
Large language models improve Alzheimer's disease diagnosis using multi-modality data,Yes.,1.,"""We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.""",2023-05-26T18:42:19Z
From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models,,,,2023-05-26T18:00:57Z
Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time,Yes.,3.,"""Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is",2023-05-26T17:39:58Z
Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,Yes.,5.,"""Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data.""",2023-05-26T17:33:05Z
Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models,Yes.,3.,"""recent studies, have found that their ability to plan remains questionable"" and ""the performance of a finetuned baseline remains poor because it violates pre-conditions of actions in the plans that it generates.""",2023-05-26T16:36:55Z
"LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",Yes.,5.,"""GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly",2023-05-26T16:32:17Z
Mindstorms in Natural Language-Based Societies of Mind,Yes.,1.,"""Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning.""",2023-05-26T16:21:25Z
A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks,Yes.,3.,"""Our empirical findings validate the challenge of segmentation.""",2023-05-26T15:49:43Z
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,Yes.,2.,"""Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models.""",2023-05-26T14:41:06Z
On Evaluating Adversarial Robustness of Large Vision-Language Models,Yes.,4.,"""multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision)"" and ""Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice.""",2023-05-26T13:49:44Z
Large Language Models Are Partially Primed in Pronoun Interpretation,Yes.,3.,"""though in a limited fashion",2023-05-26T13:30:48Z
Playing repeated games with Large Language Models,Yes.,5.,"""we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination.""",2023-05-26T12:17:59Z
HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis,Yes.,2.,"""concerns arise over their potential to compromise academic integrity"" and ""emphasizes the critical need for effective strategies to uphold academic integrity amidst the growing influence of LLMs.""",2023-05-26T11:07:25Z
Do GPTs Produce Less Literal Translations?,Yes.,2.,"""However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.""",2023-05-26T10:38:31Z
Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media,Yes.,3.,"""Despite the fact that transformer-based models achieve noticeable improvements, they cannot often capture rich factual knowledge.""",2023-05-26T10:19:04Z
Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning,Yes.,2.,"""Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well.""",2023-05-26T09:27:43Z
Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification,Yes.,4.,"""approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations.""",2023-05-26T09:15:05Z
Can large language models generate salient negative statements?,Yes.,5.,"""LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.""",2023-05-26T09:13:59Z
A Closer Look at In-Context Learning under Distribution Shifts,Yes.,5.,"""The key question we aim to address is",2023-05-26T07:47:21Z
AdaPlanner: Adaptive Planning from Feedback with Language Models,Yes.,5.,"""most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase.""",2023-05-26T05:52:27Z
TADA: Task-Agnostic Dialect Adapters for English,Yes.,5.,"""Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE).""",2023-05-26T05:45:03Z
Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing,,,,2023-05-26T05:19:24Z
Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks,Yes.,3.,"""Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it."" and ""Our research also highlights how annotating with generative models can be time-intensive.""",2023-05-26T05:13:01Z
Evaluation of Question Generation Needs More References,,,,2023-05-26T04:40:56Z
Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model,Yes.,3.,"""Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance."" and ""DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues.""",2023-05-26T04:23:10Z
AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization,Yes.,2.,"""However, LLMs require massive amounts of data and resources and are challenging to implement as offline applications.""",2023-05-26T03:44:35Z
Heterogeneous Value Alignment Evaluation for Large Language Models,Yes.,3.,"""current methodologies typically attempt to assign value as an attribute to LLMs, yet lack attention to the ability to pursue value and the importance of transferring heterogeneous values in specific practical applications.""",2023-05-26T02:34:20Z
Neural Task Synthesis for Visual Programming,No.,1.,The abstract focuses on generative neural models and does not mention LLMs or their limitations.,2023-05-26T01:08:18Z
CONA: A novel CONtext-Aware instruction paradigm for communication using large language model,Yes.,1.,"""We introduce CONA, a novel context-aware instruction paradigm for effective knowledge dissemination using generative pre-trained transformer (GPT) models.""",2023-05-26T00:53:18Z
The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering,Yes.,5.,"""Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. 'unfaithful' with respect to a rationale as retrieved from a knowledge base.""",2023-05-25T22:54:13Z
On the Tool Manipulation Capability of Open-source Large Language Models,Yes.,3.,"""By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures.""",2023-05-25T22:10:20Z
Coarse-Tuning Models of Code with Reinforcement Learning Feedback,Yes.,3.,"""However, these models are trained using next-token prediction, which ignores the syntax and semantics of code.""",2023-05-25T22:09:08Z
Type Prediction With Program Decomposition and Fill-in-the-Type Training,Yes.,5.,"""Large language models (LLMs) are promising for type prediction, but there are challenges",2023-05-25T21:16:09Z
Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models,Yes.,5.,"""We find that despite capturing some aspects of logical meaning, the models fall far short of human performance.""",2023-05-25T18:56:26Z
Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech,Yes.,1.,"""Next, we pass each transcript and image through BERT and DeiT models respectively.""",2023-05-25T18:18:09Z
Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory,Yes.,1.,"""This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments.""",2023-05-25T17:59:49Z
Landmark Attention: Random-Access Infinite Context Length for Transformers,Yes.,5.,"""their attention mechanism's large memory requirements have limited their ability to handle longer contexts.""",2023-05-25T17:53:42Z
Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots,Yes.,5.,"""there are clear drawbacks in its use, such as the possibility of producing inaccurate or false data and circumventing duplicate content (plagiarism) detectors where originality is essential,"" ""The often reported hallucinations within Generative AI in general, and also relevant for ChatGPT, can render its use of limited benefit where accuracy is essential,"" and ""What ChatGPT lacks is a stochastic measure",2023-05-25T17:35:57Z
UFO: Unified Fact Obtaining for Commonsense Question Answering,Yes.,1.,"""Recently, large-scale language models (LLMs) have dramatically improved the intelligence in capturing and leveraging knowledge, which opens up a new way to address the issue of eliciting knowledge from language models.""",2023-05-25T13:25:49Z
ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs,Yes.,5.,"""However, current works in this field are plagued by limitations, specifically a restricted scope of applicable image domains and the provision of unreliable medical advice. This restricts their overall processing capabilities. Furthermore, the mismatch in writing style between LLMs and radiologists undermines their practical usefulness.""",2023-05-25T12:03:31Z
Linguistic Properties of Truthful Response,Yes.,3.,"""We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features."" and ""the limited scope of our experiments must be taken into account in interpreting the results.""",2023-05-25T09:17:39Z
"Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",Yes.,5.,"""Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context.""",2023-05-25T08:43:46Z
ChatGPT for PLC/DCS Control Logic Generation,Yes.,1.,"""It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks.""",2023-05-25T07:46:53Z
Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback,Yes.,1.,"""Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning, conversational, and zero-shot generation abilities across various domains.""",2023-05-25T07:43:39Z
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,Yes.,5.,"""Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost.""",2023-05-25T07:39:41Z
On the Planning Abilities of Large Language Models : A Critical Investigation,Yes.,5.,"""Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains.""",2023-05-25T06:32:23Z
The False Promise of Imitating Proprietary LLMs,Yes.,5.,"""we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data"" and ""Overall, we conclude that model imitation is a false promise",2023-05-25T05:00:12Z
Asking Before Action: Gather Information in Embodied Decision Making with Language Models,Yes.,5.,"""However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance.""",2023-05-25T04:05:08Z
RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting,Yes.,3.,"""However, as LLMs are primarily trained on final text results rather than intermediate revisions, it might be challenging for them to perform text rewriting tasks.""",2023-05-25T03:26:26Z
Undetectable Watermarks for Language Models,Yes.,1.,"""Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.""",2023-05-25T02:57:16Z
Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models,Yes.,4.,"""the sensitivity of data contained in prompts raises privacy concerns"" and ""we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs.""",2023-05-24T22:06:08Z
Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation,Yes.,1.,"""This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU.""",2023-05-24T19:59:51Z
Large Language Models are Few-Shot Health Learners,Yes.,3.,"""However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus.""",2023-05-24T19:25:16Z
"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",Yes.,5.,"""LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.""",2023-05-24T18:54:39Z
Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective,Yes.,3.,"""bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length.""",2023-05-24T17:59:21Z
LayoutGPT: Compositional Visual Planning and Generation with Large Language Models,Yes.,1.,"""To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models.""",2023-05-24T17:56:16Z
Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing,Yes.,5.,"""However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content."" and ""we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs.""",2023-05-24T16:50:36Z
Gorilla: Large Language Model Connected with Massive APIs,Yes.,5.,"""However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.""",2023-05-24T16:48:11Z
Visual Programming for Text-to-Image Generation and Evaluation,Yes.,2.,"""Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes.""",2023-05-24T16:42:17Z
"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",Yes.,4.,"""This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI.""",2023-05-24T16:23:46Z
Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,Yes.,5.,"""Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world.""",2023-05-24T16:17:36Z
A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification,Yes.,4.,"""However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification."" and ""We observe LLMs are more prone to failure in these cases.""",2023-05-24T16:04:26Z
Revisiting Token Dropping Strategy in Efficient BERT Pretraining,Yes.,3.,"""we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks.""",2023-05-24T15:59:44Z
Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples,Yes.,5.,"""Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.""",2023-05-24T15:55:51Z
EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models,Yes.,3.,"""Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain.""",2023-05-24T15:55:40Z
Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,Yes.,3.,"""With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage.""",2023-05-24T15:52:08Z
Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,Yes.,5.,"""We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models,"" and ""PCW would present unexpected deterioration regarding question miscomprehension and false inference.""",2023-05-24T15:48:29Z
Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM,Yes.,1.,"""We present a novel approach to adapting pre-trained large language models (LLMs) to perform question answering (QA) and speech continuation.""",2023-05-24T15:39:43Z
"Machine Unlearning: its nature, scope, and importance for a ""delete culture""",Yes.,3.,"""The article explores the cultural shift from recording to deleting information in the digital age and its implications on privacy, intellectual property (IP), and Large Language Models like ChatGPT."" and ""However, potential ethical risks, such as misuse, overuse, and underuse of MU, should be",2023-05-24T15:27:04Z
SAIL: Search-Augmented Instruction Learning,Yes.,3.,"""Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information.""",2023-05-24T15:07:30Z
SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation,Yes.,2.,"""Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information.""",2023-05-24T14:26:30Z
STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models,Yes.,1.,"""We propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance.""",2023-05-24T12:15:19Z
Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models,Yes.,3.,"""While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement.""",2023-05-24T11:59:13Z
Contrastive Learning of Sentence Embeddings from Scratch,Yes.,1.,"""Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch).""",2023-05-24T11:56:21Z
"HuatuoGPT, towards Taming Language Model to Be a Doctor",Yes.,3.,"""The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis.""",2023-05-24T11:56:01Z
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models,Yes.,5.,"""The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts.""",2023-05-24T11:55:59Z
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind,Yes.,5.,"""there is a heated debate about whether they are able to perform ToM tasks,"" and ""our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs.""",2023-05-24T11:54:07Z
Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References,Yes.,1.,"""We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible.""",2023-05-24T11:53:29Z
GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,Yes.,5.,"""we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities.""",2023-05-24T11:53:19Z
Lawyer LLaMA Technical Report,Yes.,4.,"""the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems"" and ""to alleviate the hallucination problem during the model's generation"".",2023-05-24T11:52:07Z
Who Wrote this Code? Watermarking for Code Generation,Yes.,2.,"""ethical and legal concerns about using them have been raised, such as plagiarism and copyright issues"" and ""we discover that the previous methods fail to function appropriately with code generation tasks because of the syntactic and semantic characteristics of code.""",2023-05-24T11:49:52Z
A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction,Yes.,4.,"""Unfortunately, we find that current zero-shot EE methods perform poorly for the task, with issues including word sense ambiguity, modality mismatch, and efficiency. Straightforward application of large language model prompting typically performs even worse.""",2023-05-24T11:41:33Z
Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science,Yes.,3.,"""its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is unfaithful).""",2023-05-24T11:27:59Z
Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations,Yes.,3.,"""However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools.""",2023-05-24T11:22:34Z
ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,Yes.,3.,"""However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge.""",2023-05-24T11:14:31Z
ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification,Yes.,4.,"""Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including",2023-05-24T11:06:23Z
