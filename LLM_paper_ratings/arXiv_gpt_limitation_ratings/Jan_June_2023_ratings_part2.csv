Title,Talks about LLMs,Rate,Evidence,Published
Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models,Yes.,3.,"""existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning.""",2023-05-24T11:06:15Z
Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems,Yes.,5.,"""Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.""",2023-05-24T10:58:20Z
Estimating Class Separability of Datasets Using Persistent Homology with Application to LLM Fine-Tuning,Yes.,1.,"""Finally, we empirically show that the proposed method can be part of a stopping criterion for fine-tuning language-model classifiers.""",2023-05-24T10:58:09Z
Unlocking Temporal Question Answering for Large Language Models Using Code Execution,Yes.,3.,"""However, we notice the challenge that LLMs face when it comes to temporal reasoning.""",2023-05-24T10:57:53Z
Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization,,,,2023-05-24T10:48:05Z
Sentiment Analysis in the Era of Large Language Models: A Reality Check,,,,2023-05-24T10:45:25Z
LLMDet: A Third Party Large Language Models Generated Text Detection Tool,Yes.,3.,"""Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct.""",2023-05-24T10:45:16Z
A RelEntLess Benchmark for Modelling Graded Relations between Named Entities,Yes.,3.,"""Overall, we find a strong correlation between model size and performance, with smaller Language Models struggling to outperform a naive baseline.""",2023-05-24T10:41:24Z
Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs,Yes.,3.,"""However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored.""",2023-05-24T10:35:58Z
"RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",Yes.,5.,"""they all suffer from generating untruthful dialogues because of the model hallucination.""",2023-05-24T10:30:42Z
Controlling Pre-trained Language Models for Grade-Specific Text Simplification,Yes.,1.,"""Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output simplicity.""",2023-05-24T10:29:45Z
Reasoning with Language Model is Planning with World Model,Yes.,5.,"""However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g.,",2023-05-24T10:28:28Z
Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios,Yes.,3.,"""However, the adoption of LLMs in real-world applications for table information seeking remains underexplored."" and ""a significant performance gap still exists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4 models.""",2023-05-24T10:22:30Z
IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models,Yes.,3.,"""However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing."" and ""we argue that previous efforts have several inherent shortcomings",2023-05-24T10:19:57Z
GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP,Yes.,5.,"""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."" and ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."" and ""our work adds to a growing body of",2023-05-24T10:12:39Z
Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback,Yes.,4.,"""some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.""",2023-05-24T10:12:33Z
OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning,Yes.,3.,"""However, the prohibitive costs of tokens and time have hindered their adoption in applications.""",2023-05-24T10:08:04Z
"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",Yes.,5.,"""non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies.""",2023-05-24T09:57:37Z
Editing Common Sense in Transformers,Yes.,3.,"""However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness.""",2023-05-24T09:50:54Z
Adversarial Demonstration Attacks on Large Language Models,Yes.,5.,"""We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.""",2023-05-24T09:40:56Z
How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench,Yes.,1.,"""We investigate the predictability of large language model (LLM) capabilities",2023-05-24T09:35:34Z
Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,Yes.,4.,"""we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks,"" and ""pretrained models already possess some innate but limited capabilities of social language understanding.""",2023-05-24T09:21:06Z
GRACE: Discriminator-Guided Chain-of-Thought Reasoning,Yes.,3.,"""In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps.""",2023-05-24T09:16:51Z
In-Context Impersonation Reveals Large Language Models' Strengths and Biases,Yes.,4.,"""However, impersonation can also uncover LLMs' biases",2023-05-24T09:13:15Z
"Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",Yes.,2.,"""We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research.""",2023-05-24T09:10:20Z
Universal Self-Adaptive Prompting,Yes.,3.,"""zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable.""",2023-05-24T09:09:48Z
Frugal Prompting for Dialog Models,Yes.,1.,"""The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing, leading to changes in how researchers approach problems in the field.""",2023-05-24T09:06:49Z
Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,Yes.,5.,"""methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.""",2023-05-24T08:59:15Z
PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions,Yes.,5.,"""The remarkable capabilities of large language models have been accompanied by a persistent drawback",2023-05-24T08:59:00Z
Coverage-based Example Selection for In-Context Learning,Yes.,1.,"""On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points",2023-05-24T08:58:28Z
"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",Yes.,4.,"""Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.""",2023-05-24T08:55:11Z
PIVOINE: Instruction Tuning for Open-world Information Extraction,Yes.,1.,"""we seek to develop a large language model (LLM) that is able to perform Open-world IE to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions.""",2023-05-24T08:52:08Z
Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory,Yes.,2.,"""identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.""",2023-05-24T08:38:23Z
Leveraging GPT-4 for Automatic Translation Post-Editing,Yes.,3.,"""However, we also show that GPT-4 could produce hallucinated edits, thereby urging caution in its use as an expert translation post-editor.""",2023-05-24T08:30:05Z
ClusterLLM: Large Language Models as a Guide for Text Clustering,,,,2023-05-24T08:24:25Z
CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering,Yes.,2.,"""two bottlenecks limit these approaches",2023-05-24T08:21:31Z
How To Train Your (Compressed) Large Language Model,Yes.,5.,"""Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression.""",2023-05-24T08:18:35Z
BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer,Yes.,5.,"""Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples.""",2023-05-24T08:06:33Z
SummIt: Iterative Text Summarization via ChatGPT,Yes.,4.,"""the one-shot summarization setting is sometimes inadequate, as the generated summary may contain hallucinations or overlook essential details related to the reader's interests."" and ""identify a potential issue of over-correction.""",2023-05-24T07:40:06Z
PromptNER: Prompting For Named Entity Recognition,Yes.,3.,"""However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora.""",2023-05-24T07:38:24Z
Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation,,,,2023-05-24T07:34:33Z
Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners,Yes.,3.,"""LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense reasoning tasks by leveraging in-context new knowledge.""",2023-05-24T07:33:34Z
Mitigating Temporal Misalignment by Discarding Outdated Facts,Yes.,5.,"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having",2023-05-24T07:30:08Z
Estimating Large Language Model Capabilities without Labeled Test Data,Yes.,3.,"""the success of ICL varies widely from task to task"" and ""no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.""",2023-05-24T06:55:09Z
MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,Yes.,5.,"""The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. ... While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.""",2023-05-24T06:48:41Z
Prompting Large Language Models for Counterfactual Generation: An Empirical Study,Yes.,5.,"""The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias."" and ""we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals.""",2023-05-24T06:44:32Z
Adapting Language Models to Compress Contexts,Yes.,5.,"""Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.""",2023-05-24T06:42:44Z
ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds,Yes.,5.,"""This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model."" and ""Our analyses emphasize the need for further research into the linguistic comprehension and reasoning capabilities of LLMs, in order to improve their reliability, and establish their trustworthiness for real-world applications.""",2023-05-24T06:41:09Z
Anthropomorphization of AI: Opportunities and Risks,Yes.,4.,"""We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction,",2023-05-24T06:39:45Z
Using Natural Language Explanations to Rescale Human Judgments,Yes.,1.,"""The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation.""",2023-05-24T06:19:14Z
Allies: Prompting Large Language Model with Beam Search,Yes.,3.,"""However, this kind of methods face two limitations",2023-05-24T06:16:44Z
Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models,Yes.,5.,"""We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust To",2023-05-24T06:14:31Z
A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification,Yes.,1.,"""In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities.""",2023-05-24T05:54:10Z
Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,Yes.,3.,"""large language models (LLMs) may produce answers that do not satisfy all criteria of the question"" and ""can give insights into the errors and knowledge gaps of the model.""",2023-05-24T05:53:11Z
ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation,Yes.,1.,"""Furthermore, we develop an interactive system named ChatFace, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latent space.""",2023-05-24T05:28:37Z
In-Context Demonstration Selection with Cross Entropy Difference,Yes.,3.,"""However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples.""",2023-05-24T05:04:00Z
I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors,Yes.,1.,"""We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models.""",2023-05-24T05:01:10Z
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,Yes.,4.,"""These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.""",2023-05-24T04:27:21Z
SciFix: Outperforming GPT3 on Scientific Factual Error Correction,Yes.,3.,"""Our method outperforms the very LLM that was used to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5 achieving 58%, 61%, and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times",2023-05-24T04:24:16Z
Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models,Yes.,3.,"""In the first scenario, MoE models overall underperform dense models of identical computational capacity.""",2023-05-24T04:22:26Z
DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4,Yes.,2.,"""Despite their significance, however, there has been limited research probing these pairwise or $k$-wise comparisons."" and ""It is also unclear if there are other hidden factors influencing human judgments.""",2023-05-24T04:13:15Z
A Causal View of Entity Bias in (Large) Language Models,Yes.,4.,"""Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions."" and ""The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits.""",2023-05-24T03:59:18Z
Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs,Yes.,5.,"""We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable.""",2023-05-24T03:53:43Z
ExpertPrompting: Instructing Large Language Models to be Distinguished Experts,Yes.,1.,"""The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts.""",2023-05-24T03:51:31Z
Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response,Yes.,5.,"""Although reference-free evaluators based on LLMs show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on LLMs."" and ""Empirical results show that the ability of LLMs to identify unreasonable responses is insufficient. There are risks in using reference-free evaluators based on LLMs to evaluate the quality of",2023-05-24T02:52:48Z
Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering,Yes.,1.,"""They are initialized by pre-trained models such as ViT/DiT and GPT-2/LLaMA but aligning the two modalities requires end-to-end finetuning, which aims to minimize the visual discrepancy between the code-rendered webpage and the original screenshot.""",2023-05-24T02:17:32Z
Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,Yes.,3.,"""fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages.""",2023-05-24T02:05:03Z
Getting MoRE out of Mixture of Language Model Reasoning Experts,Yes.,3.,"""state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt.""",2023-05-24T02:00:51Z
Enabling Large Language Models to Generate Text with Citations,Yes.,5.,"""Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination."" and ""current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.""",2023-05-24T01:53:49Z
Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models,Yes.,2.,"""there is still significant room for improvement compared to SOTA fine-tuned models, which suggests that LLM adoption could be a promising approach for future fact-checking research.""",2023-05-24T01:46:07Z
Think Before You Act: Decision Transformers with Internal Working Memory,Yes.,5.,"""However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks.""",2023-05-24T01:20:22Z
"This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models",,,,2023-05-24T01:16:17Z
ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers,Yes.,4.,"""Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification.""",2023-05-24T00:10:15Z
Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person,No.,1.,"The abstract focuses on evaluating the performance of OpenAI's Whisper ASR for punctuation prediction and topic modeling, which is an automatic speech recognition system, not a language model.",2023-05-23T23:37:29Z
PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,Yes.,3.,"""However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain.""",2023-05-23T23:06:04Z
Sources of Hallucination by Large Language Models on Inference Tasks,Yes.,5.,"""We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs.""",2023-05-23T22:24:44Z
LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond,Yes.,5.,"""a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision"" and ""Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4,",2023-05-23T21:50:06Z
MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems,Yes.,5.,"""While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.""",2023-05-23T21:44:56Z
Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models,Yes.,5.,"""Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset.""",2023-05-23T20:26:03Z
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,Yes.,3.,"""they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly.""",2023-05-23T19:58:30Z
Language Model Self-improvement by Reinforcement Learning Contemplation,Yes.,2.,"""fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain.""",2023-05-23T19:25:52Z
Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA,Yes.,3.,"""GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors.""",2023-05-23T18:30:49Z
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models,Yes.,5.,"""we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture,"" ""we find concerning cases of stereotyping and cultural unfairness,"" and ""revealing the incapability of appropriate adaptation to Arab cultural contexts.""",2023-05-23T18:27:51Z
On Robustness of Finetuned Transformer-based NLP Models,Yes.,5.,"""Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned?"" and ""Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models, which should be kept in mind when passing inputs",2023-05-23T18:25:18Z
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding,Yes.,1.,"""we further investigate the utilization of a Large Language Model (LLM) to enhance the FIG for user-entity link prediction in the Video/Music domains.""",2023-05-23T18:15:29Z
Prompting Language-Informed Distribution for Compositional Zero-Shot Learning,Yes.,3.,"""the key aspects that impact the generalization to unseen compositions, including the diversity and informativeness of class context, and the entanglement between visual primitives, i.e., state and object, are not properly addressed in existing CLIP-based CZSL literature.""",2023-05-23T18:00:22Z
Schema-Driven Information Extraction from Heterogeneous Tables,Yes.,2.,"""To assess various LLM's capabilities on this task"" and ""Moreover, through detailed ablation studies and analyses, we investigate the factors contributing to model success and validate the practicality of distilling compact models to reduce API reliance.""",2023-05-23T17:58:10Z
DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation,Yes.,3.,"""Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling shifts in scene composition or object placement from a single abstract user prompt.""",2023-05-23T17:57:09Z
Benchmarking LLM-based Machine Translation on Cultural Awareness,Yes.,2.,"""However, many MT systems still struggle to translate sentences containing cultural-specific entities accurately and understandably."" and ""Nevertheless, the effectiveness of this approach in enhancing machine translation with cultural awareness remains uncertain.""",2023-05-23T17:56:33Z
Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,Yes.,2.,"""Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets.""",2023-05-23T17:56:26Z
Improving Factuality and Reasoning in Language Models through Multiagent Debate,Yes.,3.,"""reducing fallacious answers and hallucinations that contemporary models are prone to.""",2023-05-23T17:55:11Z
ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models,Yes.,4.,"""Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.""",2023-05-23T17:54:33Z
RET-LLM: Towards a General Read-Write Memory for Large Language Models,Yes.,3.,"""existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks.""",2023-05-23T17:53:38Z
CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models,Yes.,3.,"""Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved.""",2023-05-23T17:51:52Z
Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science,Yes.,5.,"""The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large).""",2023-05-23T17:48:21Z
QTSumm: Query-Focused Summarization over Tabular Data,Yes.,1.,"""We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models.""",2023-05-23T17:43:51Z
Evaluation of African American Language Bias in Natural Language Generation,Yes.,4.,"""documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features.""",2023-05-23T17:34:37Z
LLM-powered Data Augmentation for Enhanced Cross-lingual Performance,Yes.,3.,"""LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset.""",2023-05-23T17:33:27Z
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback,Yes.,2.,"""Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text.""",2023-05-23T17:27:22Z
Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs,Yes.,5.,"""We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.""",2023-05-23T17:25:59Z
SciMON: Scientific Inspiration Machines Optimized for Novelty,Yes.,3.,"""Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue.""",2023-05-23T17:12:08Z
Hierarchical Prompting Assists Large Language Model on Web Navigation,Yes.,4.,"""Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks.""",2023-05-23T17:10:39Z
FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation,,,,2023-05-23T17:06:00Z
Language Models with Rationality,Yes.,4.,"""This lack of interpretability is a growing impediment to widespread use of LLMs."" and ""to resolve inconsistencies that may exist.""",2023-05-23T17:04:25Z
On Learning to Summarize with Large Language Models as References,Yes.,3.,"""However, we found that the smaller models can not yet reach LLM-level performance under human evaluation despite promising improvements brought by our proposed training methods."" and ""reveals a discrepancy between human and LLM-based evaluation, highlighting the benefits and risks of this LLM-as-reference setting we investigated.""",2023-05-23T16:56:04Z
Multilingual Large Language Models Are Not (Yet) Code-Switchers,Yes.,5.,"""Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales."" and ""We argue that current 'multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.""",2023-05-23T16:50:48Z
ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media,Yes.,3.,"""Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance.""",2023-05-23T16:40:07Z
Question Answering as Programming for Solving Time-Sensitive Questions,Yes.,5.,"""our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics.""",2023-05-23T16:35:16Z
Exploring Chain-of-Thought Style Prompting for Text-to-SQL,Yes.,3.,"""its performance on text-to-SQL parsing still has much room for improvement"" and ""using detailed reasoning steps tends to have more error propagation issues.""",2023-05-23T16:32:36Z
CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models,Yes.,3.,"""We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization.""",2023-05-23T16:32:27Z
Domain Private Transformers for Multi-Domain Dialog Systems,Yes.,3.,"""While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt.""",2023-05-23T16:27:12Z
"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",Yes.,3.,"""While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers.""",2023-05-23T16:20:43Z
HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations,Yes.,4.,"""GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.""",2023-05-23T16:15:24Z
In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models,Yes.,5.,"""the performance on a downstream task can vary considerably, depending on the instruction. Importantly, such dependency on the context can surface in unpredictable ways, e.g., a seemingly more informative instruction might lead to a worse performance.""",2023-05-23T15:43:04Z
DetGPT: Detect What You Need via Reasoning,Yes.,1.,"""In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs).""",2023-05-23T15:37:28Z
Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization,Yes.,3.,"""Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs.""",2023-05-23T15:20:01Z
GrACE: Generation using Associated Code Edits,Yes.,1.,"""In this work, we address these challenges by endowing pre-trained large language models (LLMs) of code with the knowledge of prior, relevant edits.""",2023-05-23T14:55:44Z
Dr.ICL: Demonstration-Retrieved In-context Learning,Yes.,1.,"""In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs.""",2023-05-23T14:55:25Z
Better Zero-Shot Reasoning with Self-Adaptive Prompting,Yes.,3.,"""some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the",2023-05-23T14:27:16Z
Revisiting Acceptability Judgements,Yes.,5.,"""Our experiments show that even the largest InstructGPT model performs only at chance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much below supervised models (59.03 MCC) and human (65.11 MCC).""",2023-05-23T14:16:22Z
Evaluating Factual Consistency of Summaries with Large Language Models,Yes.,1.,"""Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs.""",2023-05-23T13:48:32Z
The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning,Yes.,3.,"""Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.""",2023-05-23T13:14:59Z
ChipGPT: How far are we from natural language hardware design,Yes.,1.,"""As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction.""",2023-05-23T12:54:02Z
IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,,,,2023-05-23T12:43:19Z
Improving Language Models via Plug-and-Play Retrieval Feedback,Yes.,5.,"""Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can",2023-05-23T12:29:44Z
Make a Choice! Knowledge Base Question Answering with In-Context Learning,Yes.,1.,"""Recently, LLMs have shown strong few-shot performance in many NLP tasks."" and ""We expect LLM can help existing methods improve their generalization ability, especially in low-resource situations.""",2023-05-23T11:56:03Z
Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning,Yes.,3.,"""Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly.""",2023-05-23T11:54:37Z
Robust Prompt Optimization for Large Language Models Against Distribution Shifts,Yes.,5.,"""We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.""",2023-05-23T11:30:43Z
DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text,Yes.,2.,"""Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement.""",2023-05-23T11:18:30Z
Generating Data for Symbolic Language with Large Language Models,,,,2023-05-23T10:44:00Z
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,Yes.,3.,"""their huge size and the inaccessibility of parameters present challenges for practical deployment"" and ""synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities.""",2023-05-23T10:11:56Z
A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,Yes.,4.,"""a little or a large bias in CtB-LLMs may cause huge harm,"" and ""we performed a large investigation of the bias of three families of CtB-LLMs,"" and ""we discovered that bias depends not on the number of parameters but on the perplexity.""",2023-05-23T09:35:37Z
Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,Yes.,5.,"""Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse.""",2023-05-23T09:33:38Z
Learning from Mistakes via Cooperative Study Assistant for Large Language Models,Yes.,3.,"""the feedback from LLM itself is often inaccurate, thereby limiting its benefits.""",2023-05-23T08:51:08Z
"""Is the Pope Catholic?"" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",Yes.,5.,"""recent research indicates that large language models struggle to comprehend these implicatures as effectively as the average human.""",2023-05-23T08:49:50Z
Can Large Language Models Capture Dissenting Human Voices?,Yes.,5.,"""we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.""",2023-05-23T07:55:34Z
Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation,Yes.,1.,"""Training or finetuning large-scale language models (LLMs) such as GPT-3 requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks.""",2023-05-23T07:54:34Z
Aligning Large Language Models through Synthetic Feedback,Yes.,2.,"""Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.""",2023-05-23T06:41:16Z
Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting,Yes.,5.,"""we uncovered a universal vulnerability among LLMs in processing inductive instructions,"" and ""different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.""",2023-05-23T06:38:20Z
Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker,Yes.,1.,"""While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.""",2023-05-23T06:35:33Z
Exploring Self-supervised Logic-enhanced Training for Large Language Models,Yes.,5.,"""Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines.""",2023-05-23T06:13:10Z
Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models,Yes.,3.,"""We argue this is an important feature for mitigating hallucinations."" and ""we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly.""",2023-05-23T05:59:21Z
Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering,Yes.,2.,"""While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time.""",2023-05-23T04:57:31Z
Error Detection for Text-to-SQL Semantic Parsing,No.,1.,The abstract does not mention LLMs or any specific limitations related to them. It focuses on text-to-SQL semantic parsing and error detection.,2023-05-23T04:44:22Z
Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models,Yes.,5.,"""an analysis of LLaMA's errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects.""",2023-05-23T04:31:39Z
The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models,Yes.,5.,"""they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings.""",2023-05-23T04:22:50Z
On the Risk of Misinformation Pollution with Large Language Models,Yes.,5.,"""Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems.""",2023-05-23T04:10:26Z
Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning,Yes.,1.,"""GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search.""",2023-05-23T04:07:03Z
ChatGPT as your Personal Data Scientist,Yes.,5.,"""Interestingly, its development spotlighted several critical weaknesses in the current LLMs (ChatGPT) and highlighted substantial opportunities for improvement.""",2023-05-23T04:00:16Z
LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models,Yes.,3.,"""However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning.""",2023-05-23T03:59:06Z
InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning,Yes.,4.,"""their ability to generalize to underrepresented languages is limited due to the scarcity of available data. Additionally, directly adapting new languages to instruction-tuned LLMs can result in catastrophic forgetting, which leads to the loss of multitasking ability.""",2023-05-23T02:51:34Z
"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",Yes.,5.,"""However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity.""",2023-05-23T02:49:35Z
Understanding Programs by Exploiting (Fuzzing) Test Cases,Yes.,3.,"""However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict.""",2023-05-23T01:51:46Z
"Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",Yes.,1.,"""We discuss zero-shot, few-shot, and fine-tuning approaches on state of the art pre-trained Large Language Models (LLMs).""",2023-05-23T01:21:55Z
ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,Yes.,3.,"""Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution.""",2023-05-23T00:16:48Z
How Language Model Hallucinations Can Snowball,Yes.,5.,"""A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements."" and ""We refer to this phenomenon as hallucination snowballing",2023-05-22T23:14:28Z
A Study of Generative Large Language Model for Medical Research and Healthcare,Yes.,2.,"""This study provides insights on the opportunities and challenges of LLMs for medical research and healthcare.""",2023-05-22T22:37:24Z
Small Language Models Improve Giants by Rewriting Their Outputs,Yes.,3.,"""Despite the impressive performance of large language models (LLMs), they often lag behind specialized models in various tasks.""",2023-05-22T22:07:50Z
Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,Yes.,5.,"""We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU.""",2023-05-22T21:59:26Z
Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents,Yes.,3.,"""showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions"" and ""The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.""",2023-05-22T19:56:10Z
Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method,Yes.,1.,"""we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work.""",2023-05-22T18:54:35Z
DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules,Yes.,4.,"""Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.""",2023-05-22T18:43:31Z
Can LLMs facilitate interpretation of pre-trained language models?,Yes.,3.,"""Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation.""",2023-05-22T18:03:13Z
RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text,Yes.,5.,"""The fixed-size context of Transformer makes GPT models incapable of generating arbitrarily long text.""",2023-05-22T17:58:10Z
Language-Agnostic Bias Detection in Language Models with Bias Probing,Yes.,4.,"""Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases."" and ""We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context.""",2023-05-22T17:58:01Z
Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts,Yes.,5.,"""tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory"" and ""how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?"" and ""Our investigation reveals seemingly contradicting behaviors of LLMs"" and ""These results pose important implications that are worth careful",2023-05-22T17:57:41Z
Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations,Yes.,3.,"""We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases.""",2023-05-22T17:56:31Z
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,Yes.,3.,"""Replicating and understanding this instruction-following requires tackling three major challenges",2023-05-22T17:55:50Z
Fairness of ChatGPT,Yes.,4.,"""there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs,"" and ""This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.""",2023-05-22T17:51:56Z
Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,Yes.,5.,"""Our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold."" and ""Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research",2023-05-22T17:36:58Z
CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles,Yes.,1.,"""We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs).""",2023-05-22T17:35:05Z
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources,Yes.,2.,"""It results in more factual rationales and reduced hallucination in generation.""",2023-05-22T17:34:23Z
Enhance Reasoning Ability of Visual-Language Models via Large Language Models,Yes.,2.,"""Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability.""",2023-05-22T17:33:44Z
Prompting is not a substitute for probability measurements in large language models,Yes.,5.,"""we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities.""",2023-05-22T17:33:17Z
"""According to ..."": Prompting Language Models Improves Quoting from Pre-Training Data",Yes.,5.,"""Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.""",2023-05-22T17:25:24Z
Chip-Chat: Challenges and Opportunities in Conversational Hardware Design,Yes.,2.,"""we thus explore the challenges faced and opportunities presented when leveraging these recent advances in LLMs for hardware design.""",2023-05-22T17:13:33Z
Deepfake Text Detection in the Wild,Yes.,3.,"""Human annotators are only slightly better than random guessing at identifying machine-generated texts. Empirical results on automatic detection methods further showcase the challenges of deepfake text detection in a wild testbed. In addition, out-of-distribution poses a greater challenge for a detector to be employed in realistic application scenarios.""",2023-05-22T17:13:29Z
To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,Yes.,5.,"""LLMs are notoriously token-hungry during pre-training, and high-quality text data on the web is approaching its scaling limit for LLMs,"" ""revealing that the model is susceptible to overfitting, leading to multi-epoch degradation,"" ""significant factors",2023-05-22T17:02:15Z
Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,Yes.,2.,"""Finally, we initiate a discussion regarding the necessity of employing LLMs for only one targeted task, taking into account the efforts required for tuning and the resources consumed during deployment.""",2023-05-22T16:56:44Z
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,Yes.,5.,"""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of",2023-05-22T16:13:50Z
Teaching Probabilistic Logical Reasoning to Transformers,Yes.,5.,"""Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.""",2023-05-22T16:08:20Z
"Editing Large Language Models: Problems, Methods, and Opportunities",Yes.,4.,"""Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive."" and ""we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal.""",2023-05-22T16:00:00Z
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities,Yes.,2.,"""Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors.""",2023-05-22T15:56:44Z
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,Yes.,5.,"""it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way."" and ""LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments.""",2023-05-22T15:47:31Z
Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models,Yes.,2.,"""revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS.""",2023-05-22T15:12:43Z
"Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",Yes.,4.,"""it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes"" and ""Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively.""",2023-05-22T15:06:51Z
Observations on LLMs for Telecom Domain: Capabilities and Limitations,Yes.,3.,"""we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services.""",2023-05-22T15:04:16Z
Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization,Yes.,5.,"""We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance",2023-05-22T14:58:13Z
"InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",Yes.,3.,"""their extensive serving and fine-tuning costs hinder their utilization in various applications"" and ""the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators.""",2023-05-22T14:52:32Z
Making Language Models Better Tool Learners with Execution Feedback,Yes.,3.,"""Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance.""",2023-05-22T14:37:05Z
Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study,Yes.,3.,"""However, there is still much to learn about how well LLMs understand structured data, such as tables."" and ""there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data.""",2023-05-22T14:23:46Z
Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media,Yes.,3.,"""Many of these approaches require annotated training datasets, which limits their applicability for languages where these may not be readily available.""",2023-05-22T13:56:35Z
Iterative Forward Tuning Boosts In-context Learning in Language Models,,,,2023-05-22T13:18:17Z
Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model,Yes.,4.,"""Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research.""",2023-05-22T13:16:07Z
Text-based Person Search without Parallel Image-Text Data,Yes.,1.,"""we propose a fine-grained image captioning strategy to obtain an enriched description of the person image, which firstly utilizes a set of instruction prompts to activate the off-the-shelf pretrained vision-language model to capture and generate fine-grained person attributes, and then converts the extracted attributes into a textual description via the finetuned large language model or the hand-crafted template.""",2023-05-22T12:13:08Z
ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness,Yes.,1.,"""The emergence of generative large language models (LLMs) raises the question",2023-05-22T11:46:32Z
ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,Yes.,4.,"""we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning.""",2023-05-22T11:45:42Z
Album Storytelling with Iterative Story-aware Captioning and Large Language Models,Yes.,3.,"""we find this often results in stories containing hallucinated information that contradicts the images, as each generated caption (""story-agnostic"") is not always about the description related to the whole story or miss some necessary information.""",2023-05-22T11:45:10Z
Lion: Adversarial Distillation of Proprietary Large Language Models,Yes.,1.,"""Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions.""",2023-05-22T09:49:16Z
Automatic Code Summarization via ChatGPT: How Far Are We?,Yes.,5.,"""The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models.""",2023-05-22T09:43:40Z
MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering,Yes.,3.,"""Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table.""",2023-05-22T08:25:15Z
LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space,Yes.,3.,"""efficient adaptation of a language model to diverse conditions remains an open challenge.""",2023-05-22T07:52:04Z
GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs,Yes.,1.,"""Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions.""",2023-05-22T07:35:43Z
Explaining Emergent In-Context Learning as Kernel Regression,Yes.,1.,"""Large language models (LLMs) have initiated a paradigm shift in transfer learning.""",2023-05-22T06:45:02Z
Fact-Checking Complex Claims with Program-Guided Reasoning,Yes.,1.,"""We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process.""",2023-05-22T06:11:15Z
Can We Edit Factual Knowledge by In-Context Learning?,Yes.,3.,"""the stored knowledge could be false or out-dated,"" and ""these gradient-based approaches bring large computation costs.""",2023-05-22T06:07:58Z
llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology,Yes.,3.,"""However, high-performing LLMs are usually mainly for English."" and ""However, we also revealed some difficulties in constructing LLMs in languages other than English.""",2023-05-22T04:59:33Z
Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage,Yes.,4.,"""Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.""",2023-05-22T04:30:35Z
G3Detector: General GPT-Generated Text Detector,Yes.,3.,"""it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas.""",2023-05-22T03:35:00Z
Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,Yes.,5.,"""Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition."" and ""The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT",2023-05-22T03:04:06Z
Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning,Yes.,3.,"""Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from the web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks.""",2023-05-21T23:16:26Z
On the Limitations of Simulating Active Learning,No.,1.,The abstract does not mention LLMs or any other type of language models.,2023-05-21T22:52:13Z
Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models,Yes.,5.,"""We identify fourteen different research areas encompassing 45 research directions that require new research and are not directly solvable by LLMs.""",2023-05-21T19:06:30Z
TheoremQA: A Theorem-driven Question Answering dataset,Yes.,3.,"""However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated.""",2023-05-21T17:51:35Z
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance,Yes.,2.,"""While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data.""",2023-05-21T17:26:16Z
Retrieving Texts based on Abstract Descriptions,Yes.,5.,"""While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval)."" and ""the retrieval task cannot be performed by the LLM directly.""",2023-05-21T17:14:31Z
"GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",Yes.,5.,"""there is a current hot debate regarding their reasoning capacity"" and ""the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks.""",2023-05-21T14:45:17Z
Evaluating the Performance of Large Language Models on GAOKAO Benchmark,Yes.,3.,"""Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects.""",2023-05-21T14:39:28Z
Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification,Yes.,2.,"""Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks.""",2023-05-21T14:03:49Z
Evaluating Open-QA Evaluation,Yes.,3.,"""Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach."" and ""We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators.""",2023-05-21T10:40:55Z
Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition,Yes.,1.,"""LA-GCN proposes a graph convolution network using large-scale language models (LLM) knowledge assistance.""",2023-05-21T08:29:16Z
PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs,Yes.,3.,"""Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation.""",2023-05-21T08:11:24Z
Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models,Yes.,1.,"""Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost.""",2023-05-21T05:28:37Z
Task-agnostic Distillation of Encoder-Decoder Language Models,Yes.,3.,"""Frustratingly, we discover that existing task-agnostic distillation methods can fail to handle the distillation of encoder-decoder LMs.""",2023-05-21T03:35:45Z
Gene Set Summarization using Large Language Models,,,,2023-05-21T02:06:33Z
Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning,Yes.,3.,"""Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems.""",2023-05-20T22:25:38Z
Tweetorial Hooks: Generative AI Tools to Motivate Science on Social Media,Yes.,2.,"""Lastly, we discuss the importance of interactivity with LLMs to preserve the correctness, effectiveness, and authenticity of the writing.""",2023-05-20T18:47:40Z
Collaborative Development of NLP models,Yes.,2.,"""Despite substantial advancements, Natural Language Processing (NLP) models often require post-training adjustments to enforce business rules, rectify undesired behavior, and align with user values"" and ""Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts.""",2023-05-20T15:55:39Z
VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models,Yes.,5.,"""They still have space to grow, though, especially in the areas of mathematics, physics, chemistry, and biology."" and ""especially in resolving LLMs' limits in disciplines involving mathematics and the natural sciences.""",2023-05-20T14:13:08Z
The Case Against Explainability,Yes.,3.,"""we suggest that in some cases the right to explanation of AI systems could bring more harm than good to end users."" and ""considering recent end-user Explainability research trends, Large Language Models' capabilities, and the ability to manipulate end-users by both humans and machines.""",2023-05-20T10:56:19Z
LogiCoT: Logical Chain-of-Thought Instruction-Tuning,Yes.,3.,"""However, they fall short of helping the model handle complex reasoning tasks.""",2023-05-20T09:23:09Z
LMs: Understanding Code Syntax and Semantics for Code Analysis,Yes.,,,2023-05-20T08:43:49Z
Can Public Large Language Models Help Private Cross-device Federated Learning?,Yes.,2.,"""The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users."" and ""Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models.""",2023-05-20T07:55:58Z
Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?,Yes.,5.,"""while doing fairly well on contexts that follow the common assumptions, the models struggle to correctly reason over contexts that break those assumptions. Specifically, the performance gap is as high as 20% absolute points.""",2023-05-20T05:20:37Z
UP5: Unbiased Foundation Model for Fairness-aware Recommendation,Yes.,4.,"""there is unfairness involved in LLMs that lead to unfair recommendation results.""",2023-05-20T04:32:59Z
"MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",Yes.,1.,"""The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema.""",2023-05-20T03:37:09Z
DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining,,,,2023-05-20T03:23:16Z
"AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation",,,,2023-05-20T00:45:15Z
Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding,Yes.,4.,"""Significant challenges concerning reliability, bias, and the potential for outdated knowledge persist.""",2023-05-19T23:07:09Z
OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models,Yes.,3.,"""Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart.""",2023-05-19T20:58:22Z
Deep Learning Approaches to Lexical Simplification: A Survey,Yes.,1.,"""the AI/NLP community has been taken by storm by recent advances in deep learning, particularly with the introduction of large language models (LLM) and prompt learning.""",2023-05-19T20:56:22Z
Reducing Sequence Length by Predicting Edit Operations with Large Language Models,Yes.,3.,"""However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer.""",2023-05-19T17:51:05Z
"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings",Yes.,3.,"""However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research.""",2023-05-19T17:43:58Z
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews,Yes.,5.,"""However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst."" and ""They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews.""",2023-05-19T17:09:19Z
Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs,Yes.,5.,"""However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure",2023-05-19T16:27:43Z
Prompting with Pseudo-Code Instructions,Yes.,1.,"""Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models.""",2023-05-19T16:25:01Z
Cross-Lingual Supervision improves Large Language Models Pre-training,Yes.,1.,"""The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption.""",2023-05-19T16:14:07Z
Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning,Yes.,5.,"""Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk.""",2023-05-19T15:45:29Z
HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,Yes.,5.,"""Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge."" and ""Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.""",2023-05-19T15:36:27Z
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,Yes.,5.,"""these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content.""",2023-05-19T15:19:44Z
S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering,,,,2023-05-19T15:01:48Z
Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses,Yes.,5.,"""We show that its multilingual consistency is still lacking, and that its task and world understanding are thus not language-independent.""",2023-05-19T13:23:51Z
LLM-Pruner: On the Structural Pruning of Large Language Models,Yes.,4.,"""such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages.""",2023-05-19T12:10:53Z
Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate,Yes.,3.,"""Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues.""",2023-05-19T11:15:33Z
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings,Yes.,3.,"""However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from,",2023-05-19T09:54:21Z
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,Yes.,3.,"""its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge.""",2023-05-19T09:23:25Z
InstructIE: A Bilingual Instruction-based Information Extraction Dataset,,,,2023-05-19T08:51:11Z
PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning,Yes.,3.,"""It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field.""",2023-05-19T08:18:24Z
RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought,Yes.,5.,"""LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems.""",2023-05-19T08:02:52Z
LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation,,,,2023-05-19T07:44:39Z
Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models,Yes.,3.,"""current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape.""",2023-05-19T07:31:59Z
Graphologue: Exploring Large Language Model Responses with Interactive Diagrams,Yes.,5.,"""However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure.""",2023-05-19T06:53:25Z
Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs,Yes.,3.,"""Although simple prompting performs well on single-step questions, it cannot permanently activate the correct knowledge path for multi-step reasoning tasks.""",2023-05-19T06:30:17Z
Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions,Yes.,2.,"""However, they typically rely on extensive human-annotated data.""",2023-05-19T06:27:16Z
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,Yes.,3.,"""their potential for performing ill-defined complex tasks is largely under-studied"" and ""large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts.""",2023-05-19T04:59:34Z
Post Hoc Explanations of Language Models Can Improve Language Models,Yes.,3.,"""However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement.""",2023-05-19T04:46:04Z
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models,Yes.,3.,"""optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains.""",2023-05-19T03:51:59Z
A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,Yes.,5.,"""First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs.""",2023-05-19T02:41:12Z
ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery,Yes.,4.,"""data-sensitive domains -- including but not limited to healthcare -- face challenges in using ChatGPT due to privacy and data-ownership concerns.""",2023-05-19T02:09:52Z
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models,Yes.,4.,"""the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically.""",2023-05-19T00:53:45Z
Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs,,,,2023-05-18T22:47:06Z
Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation,,,,2023-05-18T21:53:58Z
CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models,Yes.,4.,"""Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias."" and ""Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases.""",2023-05-18T18:58:30Z
Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses,Yes.,4.,"""We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA's responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require",2023-05-18T18:15:43Z
VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks,Yes.,1.,"""Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications.""",2023-05-18T17:59:42Z
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models,Yes.,3.,"""large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.""",2023-05-18T17:58:35Z
Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors,Yes.,3.,"""even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE),"" and ""instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE's low incidence in instruction-tuning datasets.""",2023-05-18T17:48:03Z
Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement,No.,1.,The abstract focuses on target-oriented opinion word extraction (TOWE) models using BERT-based text encoders and does not mention LLMs or their limitations.,2023-05-18T15:22:00Z
Generalized Planning in PDDL Domains with Pretrained Large Language Models,Yes.,2.,"""We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.""",2023-05-18T14:48:20Z
The Web Can Be Your Oyster for Improving Large Language Models,,,,2023-05-18T14:20:32Z
Large Language Models can be Guided to Evade AI-Generated Text Detection,Yes.,3.,"""the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods.""",2023-05-18T10:03:25Z
X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models,Yes.,1.,"""This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations.""",2023-05-18T09:56:44Z
ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation,Yes.,1.,"""Naturally, these programs can be considered as sequence data, for which large language models (LLM) can help.""",2023-05-18T09:44:18Z
Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants,Yes.,1.,"""we seek to fill this gap through the use of thirteen popular transformer based models, as well as ChatGPT, and we show that discriminative models perform better than GPT-3.5 model with our best performer reporting 92.2349% F1 score in metaphoric flower and plant names identification task.""",2023-05-18T09:22:29Z
Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,Yes.,4.,"""Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.""",2023-05-18T07:56:40Z
Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models,Yes.,3.,"""Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text.""",2023-05-18T07:50:44Z
"Ethical ChatGPT: Concerns, Challenges, and Commandments",Yes.,4.,"""there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications.""",2023-05-18T02:04:13Z
Are Large Language Models Fit For Guided Reading?,Yes.,3.,"""They generate diverse question that cover most topics in the input text even though this ability is significantly degraded as the input text increases,"" and ""They are significantly biased toward low cognitive question.""",2023-05-18T02:03:55Z
Language Models Meet World Models: Embodied Experiences Enhance Language Models,Yes.,5.,"""they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.""",2023-05-18T00:35:38Z
Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning,Yes.,1.,"""We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information.""",2023-05-17T23:50:28Z
Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting,Yes.,3.,"""Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words.""",2023-05-17T23:41:30Z
Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Yes.,5.,"""Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.""",2023-05-17T23:16:17Z
"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",Yes.,4.,"""While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU.""",2023-05-17T20:45:13Z
Statistical Knowledge Assessment for Large Language Models,Yes.,3.,"""Existing LLMs may generate distinct responses for different prompts."" and ""tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.""",2023-05-17T18:54:37Z
ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages,Yes.,5.,"""We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations"" and ""ChatGPT appears to confer a higher respect to men than to women in the same occupation.""",2023-05-17T18:30:05Z
Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families,Yes.,1.,"""This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch.""",2023-05-17T17:52:25Z
BAD: BiAs Detection for Large Language Models in the context of candidate screening,Yes.,4.,"""The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed."" and ""we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could",2023-05-17T17:47:31Z
Predicting Side Effect of Drug Molecules using Recurrent Neural Networks,,,,2023-05-17T16:56:19Z
Evaluating Object Hallucination in Large Vision-Language Models,Yes.,5.,"""we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions."" and ""show that they mostly suffer from severe object hallucination issue.""",2023-05-17T16:34:01Z
Controllable Speaking Styles Using a Large Language Model,Yes.,1.,"""Large generative language models (LLMs) have shown excellent performance in various language-related tasks.""",2023-05-17T16:01:50Z
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models,,,,2023-05-17T15:07:50Z
MemoryBank: Enhancing Large Language Models with Long-Term Memory,,,,2023-05-17T14:40:29Z
Language Model Tokenizers Introduce Unfairness Between Languages,Yes.,4.,"""disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked"" and ""This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided",2023-05-17T14:17:57Z
Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries,Yes.,3.,"""However, these English-centric models encounter challenges in non-English clinical settings, primarily due to limited clinical knowledge in respective languages, a consequence of imbalanced training corpora.""",2023-05-17T12:31:26Z
Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback,Yes.,3.,"""Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement.""",2023-05-17T11:55:32Z
Can Language Models Solve Graph Problems in Natural Language?,Yes.,5.,"""whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored"" and ""the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings",2023-05-17T08:29:21Z
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,Yes.,2.,"""However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive.""",2023-05-17T08:28:54Z
Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models,Yes.,5.,"""By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge.""",2023-05-17T05:25:27Z
"""I'm fully who I am"": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",Yes.,4.,"""We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on",2023-05-17T04:21:45Z
Smaller Language Models are Better Black-box Machine-Generated Text Detectors,Yes.,1.,"""With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures.""",2023-05-17T00:09:08Z
Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs,Yes.,1.,"""In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data.""",2023-05-17T00:08:36Z
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites,Yes.,3.,"""However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize LLMs to mass produce misinformation.""",2023-05-16T21:51:01Z
Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models,Yes.,5.,"""We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness.""",2023-05-16T20:52:04Z
"What In-Context Learning ""Learns"" In-Context: Disentangling Task Recognition and Task Learning",Yes.,3.,"""its mechanisms are not yet well-understood.""",2023-05-16T18:05:19Z
SatLM: Satisfiability-Aided Language Models Using Declarative Prompting,Yes.,3.,"""While such an approach works well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search.""",2023-05-16T17:55:51Z
AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction,Yes.,3.,"""We discuss practical constraints and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction.""",2023-05-16T17:13:07Z
Towards Expert-Level Medical Question Answering with Large Language Models,Yes.,3.,"""significant room for improvement, especially when models' answers were compared to clinicians' answers"" and ""newly introduced datasets of 240 long-form 'adversarial' questions to probe LLM limitations.""",2023-05-16T17:11:29Z
Large Language Models are Built-in Autoregressive Search Engines,Yes.,1.,"""In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.""",2023-05-16T17:04:48Z
Life of PII -- A PII Obfuscation Transformer,Yes.,1.,"""Our Transformer-based approach learns mapping between the original PII and its transformed faux-PII representation, which we call 'obfuscated' data.""",2023-05-16T15:48:36Z
CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling,Yes.,1.,The abstract discusses the use of contextualized word embeddings from BERT but does not mention any limitations of language models.,2023-05-16T10:07:33Z
Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,Yes.,2.,"""training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs.""",2023-05-16T07:52:57Z
Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models,Yes.,3.,"""Vanilla language models are forgetful"" and ""Pre-training leads to retentive language models.""",2023-05-16T03:50:38Z
SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting,Yes.,1.,"""Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks.""",2023-05-15T23:29:56Z
Small Models are Valuable Plug-ins for Large Language Models,Yes.,5.,"""Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of",2023-05-15T17:59:01Z
Large Language Models are Zero-Shot Rankers for Recommender Systems,Yes.,5.,"""We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts.""",2023-05-15T17:57:39Z
RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,Yes.,3.,"""Despite their unprecedented success, even the largest language models make mistakes."" and ""this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network.""",2023-05-15T17:57:16Z
Knowledge Rumination for Pre-trained Language Models,Yes.,3.,"""Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone"" and ""we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks.""",2023-05-15T15:47:09Z
"Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",Yes.,5.,"""However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed."" and ""they are",2023-05-15T15:44:51Z
Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks,Yes.,5.,"""a critical issue has been identified within this domain",2023-05-15T15:19:08Z
Natural Language Decomposition and Interpretation of Complex Utterances,Yes.,1.,"""large language models (LLMs) encode knowledge about goals and plans that can help conversational assistants interpret user requests requiring numerous steps to complete.""",2023-05-15T14:35:00Z
Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering,Yes.,5.,"""PLMs are sensitive to the frequency information of words from their pre-training corpora, resulting in anisotropic embedding space, where the embeddings of high-frequency words are clustered but those of low-frequency words disperse sparsely. This anisotropic phenomenon results in two problems of similarity bias and information bias, lowering the quality of sentence embeddings.""",2023-05-15T13:59:23Z
Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study,Yes.,5.,"""ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations. We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures.""",2023-05-15T07:14:41Z
Text Classification via Large Language Models,,,,2023-05-15T06:24:45Z
C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,Yes.,3.,"""Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs.""",2023-05-15T03:20:19Z
Semantic Composition in Visually Grounded Language Models,Yes.,5.,"""Although large language models display considerable compositional ability, recent work shows that visually-grounded language models drastically fail to represent compositional structure.""",2023-05-15T03:19:42Z
Large Language Model Guided Tree-of-Thought,Yes.,1.,"""In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs).""",2023-05-15T01:18:23Z
$SmartProbe$: A Virtual Moderator for Market Research Surveys,Yes.,1.,"""In this work, we introduce ${\tt SmartProbe}$, an API which leverages the adaptive capabilities of large language models (LLMs), and incorporates domain knowledge from market research, in order to generate effective probing questions in any market research survey.""",2023-05-14T22:36:08Z
Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency,Yes.,5.,"""non-linguistic skill injection typically comes at a cost for LLMs",2023-05-14T20:57:11Z
Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction,Yes.,3.,"""Recently, Large Language Models (LLMs) have exhibited robust reasoning and planning abilities, yet their potential for multi-turn interactions in complex environments remains under-explored.""",2023-05-14T12:31:03Z
Watermarking Text Generated by Black-Box Language Models,Yes.,2.,"""passive detection methods are stuck in domain specificity and limited adversarial robustness"" and ""this method is not applicable in many real-world scenarios where only black-box language models are available.""",2023-05-14T07:37:33Z
ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance,Yes.,3.,"""We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset.""",2023-05-13T21:31:02Z
Beyond the Safeguards: Exploring the Security Risks of ChatGPT,,,,2023-05-13T21:01:14Z
Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics,Yes.,4.,"""Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."" and ""The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilem",2023-05-13T17:23:16Z
Leveraging Large Language Models in Conversational Recommender Systems,,,,2023-05-13T16:40:07Z
CodeT5+: Open Code Large Language Models for Code Understanding and Generation,Yes.,5.,"""existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading",2023-05-13T14:23:07Z
Dual Use Concerns of Generative AI and Large Language Models,Yes.,2.,"""With its demonstrated advantages and drawbacks in biological research, we believe the DURC criteria can be effectively redefined for LLMs, potentially contributing to improved AI governance.""",2023-05-13T10:08:57Z
Improving Small Language Models on PubMedQA via Generative Data Augmentation,Yes.,2.,"""LLMs have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost.""",2023-05-12T23:49:23Z
NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,Yes.,1.,"""In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages.""",2023-05-12T21:22:08Z
Knowledge Authoring for Rules and Actions,No.,1.,"The abstract does not mention language models, LLMs, or any related limitations.",2023-05-12T21:08:35Z
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,Yes.,5.,"""Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English",2023-05-12T20:56:48Z
Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery,Yes.,1.,"""Recently, large language models (LLM) have demonstrated exceptional utility for natural language processing tasks.""",2023-05-12T17:46:06Z
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Yes.,4.,"""it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation."" and ""we conducted an evaluation of ChatGPT and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations.""",2023-05-12T16:54:36Z
Generative AI: Implications and Applications for Education,Yes.,3.,"""In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation.""",2023-05-12T16:52:38Z
LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,Yes.,1.,"""In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs).""",2023-05-12T14:21:38Z
Calibration-Aware Bayesian Learning,Yes.,3.,"""Deep learning models, including modern systems like large language models, are well known to offer unreliable estimates of the uncertainty of their decisions.""",2023-05-12T14:19:15Z
ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter,Yes.,3.,"""a grand challenge of exploiting LLMs for multimodal learning is the size of pre-trained LLMs which are always with billions of parameters"" and ""these models remain limited in their understanding of artistic imagery.""",2023-05-12T14:04:30Z
"Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation",Yes.,2.,"""Our approach is better at detecting gender bias and word-sense-disambiguation errors in translation than supervised QE, indicating its robustness to out-of-domain usage. The performance gap is larger when detecting errors on a nontraditional translation-prompting LLM, indicating that our approach is more generalizable to different MT systems.""",2023-05-12T13:10:57Z
Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation,,,,2023-05-12T11:41:16Z
Surfacing Biases in Large Language Models using Contrastive Input Decoding,Yes.,5.,"""Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour"" and ""We use CID to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturb",2023-05-12T11:09:49Z
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation,Yes.,5.,"""Experiments show that ChatGPT is not a good causal reasoner, but a good causal explainer. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes,",2023-05-12T10:54:13Z
When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust,Yes.,3.,"""While their remarkable performance spans a range of tasks, adapting LLMs for real-world business scenarios still poses challenges warranting further investigation.""",2023-05-12T03:49:59Z
Exploring Zero and Few-shot Techniques for Intent Classification,Yes.,1.,"""zero-shot intent classification using descriptions large language models (LLMs),"" and ""parameter-efficient fine-tuning of instruction-finetuned language models.""",2023-05-11T22:07:27Z
"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",Yes.,3.,"""In this review, we also explored the potential challenges and limitations of a GPT.""",2023-05-11T19:20:38Z
Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales,Yes.,3.,"""We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility.""",2023-05-11T19:01:13Z
Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,Yes.,3.,"""their performance varies substantially across different languages.""",2023-05-11T17:44:17Z
Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach,Yes.,1.,"""Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs.""",2023-05-11T17:39:07Z
Evaluating Open-Domain Question Answering in the Era of Large Language Models,Yes.,5.,"""The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.""",2023-05-11T17:14:33Z
Active Retrieval Augmented Generation,Yes.,5.,"""Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.""",2023-05-11T17:13:40Z
Spear Phishing With Large Language Models,Yes.,3.,"""I demonstrate how basic prompt engineering can circumvent safeguards installed in LLMs, highlighting the need for further research into robust interventions that can help prevent models from being misused.""",2023-05-11T16:55:19Z
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models,Yes.,5.,"""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets."" and ""We find that while existing debiasing methods can mitigate reliance on a chosen spurious feature, the OOD performance gains of these methods can not be explained by mitigated reliance on biased features",2023-05-11T14:35:00Z
Structured Chain-of-Thought Prompting for Code Generation,Yes.,3.,"""However, CoT prompting is designed for natural language generation and has low accuracy in code generation.""",2023-05-11T06:43:37Z
Chain-of-Dictionary Prompting Elicits Translation in Large Language Models,Yes.,5.,"""they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation.""",2023-05-11T05:19:47Z
How Good are Commercial Large Language Models on African Languages?,Yes.,4.,"""However, their performance on African languages is largely unknown."" and ""Our results suggest that commercial language models produce below-par performance on African languages.""",2023-05-11T02:29:53Z
Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,Yes.,3.,"""Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems.""",2023-05-11T01:50:16Z
Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction,Yes.,3.,"""the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question."" and ""we find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data, indicating the importance of user interaction data.""",2023-05-10T21:43:42Z
Bot or Human? Detecting ChatGPT Imposters with A Single Question,Yes.,3.,"""there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks.""",2023-05-10T19:09:24Z
Automatic Evaluation of Attribution by Large Language Models,Yes.,3.,"""However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem.""",2023-05-10T16:58:33Z
Evaluating Embedding APIs for Information Retrieval,Yes.,2.,"""The ever-increasing size of language models curtails their widespread availability to the community, thereby galvanizing many companies into offering access to large language models through APIs.""",2023-05-10T16:40:52Z
"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",Yes.,5.,"""We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents.""",2023-05-10T16:40:37Z
Privacy-Preserving Prompt Tuning for Large Language Model Services,Yes.,2.,"""However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization.""",2023-05-10T14:41:51Z
"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",Yes.,4.,"""examines the errors produced by the LLMs and categorized the errors into three types",2023-05-10T13:40:06Z
Davinci the Dualist: the mind-body divide in large language models and in human learners,Yes.,3.,"""While Davinci's performance is constrained by its syntactic limitations, and it differs from humans, its Dualist bias is robust.""",2023-05-10T12:28:09Z
Enriching language models with graph-based context information to better understand textual data,Yes.,1.,"""The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text.""",2023-05-10T10:57:21Z
Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models,Yes.,1.,"""We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments.""",2023-05-10T08:48:53Z
Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,Yes.,5.,"""Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge,"" and ""statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.""",2023-05-10T08:35:50Z
Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition,Yes.,5.,"""This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.""",2023-05-10T08:16:46Z
Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer,Yes.,2.,"""Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the significant amount of computational resources and labeled data required for the specific task.""",2023-05-10T07:33:36Z
Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment,Yes.,3.,"""We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces.""",2023-05-10T07:24:36Z
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks,Yes.,2.,"""We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models.""",2023-05-10T03:13:54Z
DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text,Yes.,2.,"""Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question.""",2023-05-09T21:31:07Z
TidyBot: Personalized Robot Assistance with Large Language Models,Yes.,1.,"""We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.""",2023-05-09T17:52:59Z
Towards Building the Federated GPT: Federated Instruction Tuning,Yes.,3.,"""Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts.""",2023-05-09T17:42:34Z
Fine-tuning Language Models with Generative Adversarial Reward Modelling,Yes.,3.,"""RLHF is constrained by the expertise and productivity limitations of human evaluators."" and ""while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead.""",2023-05-09T17:06:06Z
The Case Records of ChatGPT: Language Models and Complex Clinical Questions,,,,2023-05-09T16:58:32Z
ChatGPT as a Text Simplification Tool to Remove Bias,Yes.,4.,"""The presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. If the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination.""",2023-05-09T13:10:23Z
Large Language Models Need Holistically Thought in Medical Conversational QA,Yes.,3.,"""Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field.""",2023-05-09T12:57:28Z
A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture,Yes.,2.,"""There is limited understanding about the impact of introducing foundation models in software architecture.""",2023-05-09T11:37:16Z
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,Yes.,3.,"""However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation.""",2023-05-09T05:48:38Z
MoT: Memory-of-Thought Enables ChatGPT to Self-Improve,Yes.,3.,"""However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning.""",2023-05-09T05:25:05Z
Accessible Instruction-Following Agent,Yes.,2.,"""Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages."" and ""the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility.""",2023-05-08T23:57:26Z
Knowledge-enhanced Agents for Interactive Text Games,Yes.,3.,"""Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment.""",2023-05-08T23:31:39Z
Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer,Yes.,3.,"""Large Language Models (LLMs), such as the Generative Pretrained Transformer (GPT), have achieved tremendous success in various language tasks, but their emergent abilities have also raised many questions, concerns, and challenges that need to be addressed.""",2023-05-08T21:35:12Z
ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models,Yes.,3.,"""Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.""",2023-05-08T21:12:20Z
Web Content Filtering through knowledge distillation of Large Language Models,Yes.,1.,"""We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering [...] Our student model matches the performance of the teacher LLM with 175 times less parameters.""",2023-05-08T20:09:27Z
Revisiting Relation Extraction in the era of Large Language Models,Yes.,2.,"""We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching.""",2023-05-08T19:19:07Z
Explanation-based Finetuning Makes Models More Robust to Spurious Cues,Yes.,4.,"""Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data.""",2023-05-08T18:53:45Z
Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust,Yes.,5.,"""The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes."" and ""However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs.""",2023-05-08T18:53:14Z
NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge,Yes.,3.,"""not even the most powerful models are exempt from making errors"" and ""to what extent are models at different scales able to generate valid and diverse comparative knowledge?""",2023-05-08T18:20:36Z
How Do In-Context Examples Affect Compositional Generalization?,Yes.,5.,"""We find that the compositional generalization performance can be easily affected by the selection of in-context examples,"" and ""two strong limitations are observed",2023-05-08T16:32:18Z
Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video,Yes.,3.,"""However, the generally extracted visual features may overlook some summary-worthy visual information, which impedes model performance.""",2023-05-08T16:24:46Z
Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns,Yes.,4.,"""underscores the challenges in developing safe and unbiased LLMs, and emphasizes the importance of understanding the susceptibility of LLMs to external influences.""",2023-05-08T16:10:18Z
Algebra Error Classification with Large Language Models,Yes.,2.,"""Additionally, we analyze common classification errors made by our method and discuss limitations of automated error classification.""",2023-05-08T15:51:38Z
Augmented Large Language Models with Parametric Knowledge Guiding,Yes.,4.,"""However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom",2023-05-08T15:05:16Z
Differentially Private Attention Computation,Yes.,4.,"""one crucial issue concerning the inference results of large language models is security and privacy"" and ""results generated by LLMs could possibly leak many confidential or copyright information.""",2023-05-08T13:32:41Z
Enhancing Knowledge Graph Construction Using Large Language Models,Yes.,3.,"""However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task.""",2023-05-08T12:53:06Z
Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT,Yes.,3.,"""Our findings demonstrate the potential for ChatGPT to serve as an AGR, though several limitations and areas for improvement are identified.""",2023-05-08T07:28:16Z
Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding,Yes.,1.,"""Recent works on form understanding mostly employ multimodal transformers or large-scale pre-trained language models. These models need ample data for pre-training.""",2023-05-08T05:03:07Z
Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts,Yes.,1.,"""to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings.""",2023-05-08T01:55:53Z
Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5,Yes.,2.,"""The fact that an LLM - which lacks these processes - also shows such effects invites consideration of the possibility that language may play a role in generating these effects in humans.""",2023-05-08T01:02:52Z
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,Yes.,5.,"""However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction."" and ""Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety.""",2023-05-07T22:44:25Z
"Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",Yes.,3.,"""current AI-text classifiers cannot reliably detect ChatGPT's use in school work, due to their propensity to classify human-written answers as AI-generated, as well as the ease with which AI-generated text can be edited to evade detection.""",2023-05-07T10:37:51Z
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages,Yes.,1.,"""Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models.""",2023-05-07T02:25:42Z
Controllable Mixed-Initiative Dialogue Generation through Prompting,Yes.,1.,"""We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation.""",2023-05-06T23:11:25Z
Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?,Yes.,4.,"""However, these abilities are quite limited and worse than well-trained humans when the tasks are not known and are not part of the training data.""",2023-05-06T20:53:22Z
Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,Yes.,5.,"""Despite the success of Zero-shot-CoT, it still suffers from three pitfalls",2023-05-06T16:34:37Z
Self-Edit: Fault-Aware Code Editor for Code Generation,Yes.,3.,"""However, with limited sample numbers, LLMs still suffer from poor accuracy.""",2023-05-06T16:12:19Z
Pre-training Language Model as a Multi-perspective Course Learner,Yes.,4.,"""Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning.""",2023-05-06T09:02:10Z
"Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",Yes.,4.,"""However, there are also potential risks associated with the use and development of LLMs, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback.""",2023-05-05T21:20:02Z
On Contrastive Learning of Semantic Similarity forCode to Code Search,Yes.,1.,"""This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training.""",2023-05-05T20:46:56Z
Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation,Yes.,3.,"""End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data.""",2023-05-05T20:35:42Z
Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios,Yes.,3.,"""Although the task-adaptive pre-training approach has the potential to capture domain-specific patterns, it is constrained by the limited task-specific corpus and may be susceptible to overfitting.""",2023-05-05T18:39:07Z
Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,Yes.,2.,"""The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient.""",2023-05-05T17:55:49Z
LMEye: An Interactive Perception Network for Large Language Models,Yes.,3.,"""Such networks project the image feature once yet do not consider the interaction between the image and the human input query. Hence, the obtained visual information without being connected to human intention may be inadequate for LLMs to generate intention-following responses, which we refer to as static visual information.""",2023-05-05T17:27:21Z
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,Yes.,4.,"""Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures.""",2023-05-05T17:15:32Z
"Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs",,,,2023-05-05T16:02:06Z
Now It Sounds Like You: Learning Personalized Vocabulary On Device,No.,1.,"The abstract discusses Federated Learning (FL) and its application to on-device language modeling, but it does not mention large language models (LLMs) or their limitations.",2023-05-05T14:44:20Z
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering,Yes.,3.,"""collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed.""",2023-05-05T11:56:30Z
"Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects",Yes.,3.,"""In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks.""",2023-05-05T11:09:13Z
Using ChatGPT for Entity Matching,Yes.,3.,"""Two major drawbacks of using these models for entity matching are that (i) the models require significant amounts of fine-tuning data for reaching a good performance and (ii) the fine-tuned models are not robust concerning out-of-distribution entities.""",2023-05-05T10:39:32Z
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,Yes.,5.,"""Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality.""",2023-05-05T07:24:46Z
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,Yes.,5.,"""one of its most fatal disadvantages is the lack of factual correctness"" and ""still suffers from factuality concerns in knowledge-intensive tasks.""",2023-05-05T03:49:14Z
VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna,Yes.,2.,"""However, these models can only be accessed via online APIs, which may cause data leak and non-reproducible problems.""",2023-05-05T02:46:22Z
LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics,Yes.,3.,"""we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases.""",2023-05-04T23:54:37Z
Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing,Yes.,3.,"""GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.""",2023-05-04T22:46:43Z
Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,Yes.,5.,"""even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand.""",2023-05-04T19:02:29Z
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision,Yes.,4.,"""However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases.""",2023-05-04T17:59:28Z
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study,Yes.,1.,"""We applied BERT, a powerful Large Language Model (LLM) that enables us to transform code examples into numerical vectors by extracting their semantic information.""",2023-05-04T17:43:19Z
Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence,Yes.,3.,"""Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage.""",2023-05-04T17:31:41Z
"Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search",Yes.,3.,"""Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort.""",2023-05-04T15:15:22Z
An automatically discovered chain-of-thought prompt generalizes to novel models and datasets,Yes.,3.,"""uncertainties remain about how reasoning strategies formulated for previous model generations generalize to new model generations and different datasets.""",2023-05-04T15:07:20Z
"""Oops, Did I Just Say That?"" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",Yes.,4.,"""their subtly unethical suggestions become a serious and real concern"" and ""our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions.""",2023-05-04T08:00:32Z
How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning,Yes.,3.,"""Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships.""",2023-05-04T07:45:49Z
Faithful Question Answering with Monte-Carlo Planning,Yes.,3.,"""Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging.""",2023-05-04T05:21:36Z
Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era,,,,2023-05-04T05:21:09Z
PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits,Yes.,2.,"""there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits"" and ""the accuracy drops significantly when the annotators were informed of AI authorship.""",2023-05-04T04:58:00Z
Can LLMs Capture Human Preferences?,Yes.,5.,"""Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans."" and ""While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings",2023-05-04T03:51:31Z
AutoML-GPT: Automatic Machine Learning with GPT,Yes.,1.,"""Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction.""",2023-05-04T02:09:43Z
Personalized Abstractive Summarization by Tri-agent Generation Pipeline,Yes.,3.,"""Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities.""",2023-05-04T01:12:35Z
Black-box Prompt Tuning with Subspace Learning,Yes.,3.,"""Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces.""",2023-05-04T01:04:25Z
The System Model and the User Model: Exploring AI Dashboard Design,Yes.,2.,"""Recently there has been a surge of attention to chatbots based on large language models, including widely reported unsavory interactions.""",2023-05-04T00:22:49Z
Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs,Yes.,2.,"""However, these models are extremely computationally expensive, even at inference time, raising the natural question",2023-05-03T21:51:42Z
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory,Yes.,1.,"""selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round.""",2023-05-03T21:40:54Z
"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",Yes.,4.,"""the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent",2023-05-03T20:11:22Z
ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs,Yes.,5.,"""two major limitations hinder its potential applications",2023-05-03T19:57:43Z
Entity Tracking in Language Models,Yes.,3.,"""there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities"" and ""pretraining on text corpora alone does not make this capacity surface.""",2023-05-03T18:01:13Z
Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings,Yes.,3.,"""Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks.""",2023-05-03T17:58:29Z
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,Yes.,5.,"""Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications.""",2023-05-03T17:50:56Z
Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections,Yes.,3.,"""The optimal classifier's performance can decline by around 10% to 20% in F-score on the noisy corpora.""",2023-05-03T17:32:16Z
WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models,Yes.,1.,"""the second uses few-shot in-context learning (ICL) with a large language model (LLM).""",2023-05-03T15:58:28Z
Judgments of research co-created by generative AI: experimental evidence,Yes.,2.,"""People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85).""",2023-05-03T15:57:39Z
GPT-RE: In-context Learning for Relation Extraction using Large Language Models,Yes.,5.,"""they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE",2023-05-03T13:28:08Z
Can Large Language Models Be an Alternative to Human Evaluations?,Yes.,3.,"""We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.""",2023-05-03T07:28:50Z
Improving Contrastive Learning of Sentence Embeddings from AI Feedback,Yes.,1.,"""Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning.""",2023-05-03T06:26:13Z
SCOTT: Self-Consistent Chain-of-Thought Distillation,Yes.,3.,"""Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions.""",2023-05-03T03:47:00Z
KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness,Yes.,3.,"""we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics.""",2023-05-02T22:28:26Z
Multimodal Procedural Planning via Dual Text-Image Prompting,Yes.,1.,"""TIP, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models.""",2023-05-02T21:46:44Z
Automated Code generation for Information Technology Tasks in YAML through Large Language Models,Yes.,1.,"""The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages.""",2023-05-02T21:01:01Z
Few-shot In-context Learning for Knowledge Base Question Answering,Yes.,1.,"""KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations.""",2023-05-02T19:31:55Z
Privacy-Preserving In-Context Learning for Large Language Models,Yes.,3.,"""LLM's responses may leak the sensitive private information contained in in-context exemplars.""",2023-05-02T17:52:58Z
Finding Neurons in a Haystack: Case Studies with Sparse Probing,Yes.,3.,"""Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood.""",2023-05-02T17:13:55Z
Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,Yes.,5.,"""LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately."" and ""Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy.""",2023-05-02T15:53:28Z
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information,Yes.,1.,"""Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone.""",2023-05-02T15:36:10Z
"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",Yes.,3.,"""Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era.""",2023-05-02T15:33:01Z
VPGTrans: Transfer Visual Prompt Generator across LLMs,Yes.,1.,"""While developing a new multimodal LLM (MLLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm.""",2023-05-02T09:28:39Z
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,Yes.,5.,"""However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code."" and ""Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k",2023-05-02T05:46:48Z
A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models,Yes.,2.,"""we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies.""",2023-05-02T03:27:27Z
Complex Logical Reasoning over Knowledge Graphs using Large Language Models,Yes.,1.,"""to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively.""",2023-05-02T02:21:49Z
RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models,Yes.,1.,"""We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS).""",2023-05-02T01:33:02Z
Evaluating statistical language models as pragmatic reasoners,Yes.,3.,"""We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with negation.""",2023-05-01T18:22:10Z
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Yes.,5.,"""this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors.""",2023-05-01T17:36:06Z
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs,Yes.,3.,"""We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research.""",2023-05-01T17:09:33Z
Poisoning Language Models During Instruction Tuning,Yes.,5.,"""we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions"" and ""Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.""",2023-05-01T16:57:33Z
Learning to Reason and Memorize with Self-Notes,Yes.,5.,"""Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use.""",2023-05-01T14:02:48Z
Self-Evaluation Guided Beam Search for Reasoning,Yes.,5.,"""the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results.""",2023-05-01T02:37:59Z
Using Large Language Models to Generate JUnit Tests: An Empirical Study,Yes.,3.,"""We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.""",2023-04-30T07:28:06Z
Beyond Classification: Financial Reasoning in State-of-the-Art Language Models,Yes.,2.,"""However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored.""",2023-04-30T04:36:05Z
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality,Yes.,4.,"""The causal capabilities of large language models (LLMs) is a matter of significant debate,"" and ""LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.""",2023-04-28T19:00:43Z
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model,Yes.,3.,"""Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4.""",2023-04-28T17:59:25Z
Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs,Yes.,3.,"""Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.""",2023-04-28T17:39:49Z
"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",Yes.,5.,"""albeit it may not possess the same level of expertise in identifying the temporal order between two events"" and ""the implicit discourse relation remains a formidable challenge"" and ""ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.""",2023-04-28T13:14:36Z
Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?,Yes.,3.,"""Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages.""",2023-04-28T12:11:21Z
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks,Yes.,3.,"""Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM.""",2023-04-28T10:15:25Z
Towards autonomous system: flexible modular production system enhanced with large language model agents,Yes.,2.,"""This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work.""",2023-04-28T09:42:18Z
PMC-LLaMA: Towards Building Open-source Language Models for Medicine,Yes.,3.,"""While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge.""",2023-04-27T18:29:05Z
LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions,Yes.,2.,"""Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive.""",2023-04-27T17:58:49Z
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers,Yes.,3.,"""However, these methods still suffer from limitations in terms of generation quality, diversity, and flexibility.""",2023-04-27T17:58:02Z
We're Afraid Language Models Aren't Modeling Ambiguity,Yes.,5.,"""We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.""",2023-04-27T17:57:58Z
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants,Yes.,2.,"""A major challenge in deploying LLM-based virtual conversational assistants in real world settings is ensuring they operate within what is admissible for the task.""",2023-04-27T17:39:11Z
Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems,Yes.,3.,"""This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively.""",2023-04-27T17:33:49Z
ICE-Score: Instructing Large Language Models to Evaluate Code,Yes.,3.,"""Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement.""",2023-04-27T16:38:17Z
Controlled Text Generation with Natural Language Instructions,Yes.,3.,"""Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications.""",2023-04-27T15:56:34Z
Origin Tracing and Detecting of LLMs,Yes.,3.,"""We provide valuable observations based on the experimental results, such as the difficulty level of AI origin tracing, and the AI origin similarities, and call for ethical concerns of LLM providers.""",2023-04-27T10:05:57Z
Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering,Yes.,3.,"""Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques.""",2023-04-27T01:48:03Z
The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks,,,,2023-04-26T23:09:02Z
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery,Yes.,5.,"""current explorations do not assess the real-world utility and safety of LLMs in clinical settings,"" ""responses contained hallucinated references,"" and ""often do not meet the specific information need of a given question.""",2023-04-26T17:54:28Z
Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,Yes.,4.,"""We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on",2023-04-26T17:52:30Z
Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables,Yes.,1.,"""we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3.""",2023-04-26T13:31:04Z
Enhancing Large Language Model with Self-Controlled Memory Framework,Yes.,5.,"""Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information.""",2023-04-26T07:25:31Z
Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval,Yes.,3.,"""Large language models (LLMs) work well in natural language generation tasks, but they are not specifically pre-trained to understand the syntax and semantics of SQL commands.""",2023-04-26T06:02:01Z
The Closeness of In-Context Learning and Weight Shifting for Softmax Regression,Yes.,1.,"""Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks.""",2023-04-26T04:33:41Z
The Internal State of an LLM Knows When It's Lying,Yes.,5.,"""one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.""",2023-04-26T02:49:38Z
Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks,,,,2023-04-25T23:18:13Z
TABLET: Learning From Instructions For Tabular Data,Yes.,5.,"""We find LLMs often ignore instructions and fail to predict specific instances correctly, even with examples.""",2023-04-25T23:07:20Z
AI-assisted coding: Experiments with GPT-4,Yes.,5.,"""These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance"" and ""we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code.""",2023-04-25T22:59:01Z
The Potential of Visual ChatGPT For Remote Sensing,Yes.,4.,"""we demonstrate the current model's limitations in dealing with remote sensing images, highlighting its challenges and future prospects.""",2023-04-25T17:29:47Z
"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",Yes.,5.,"""Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa).""",2023-04-25T17:05:38Z
What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files,Yes.,3.,"""We also identify key limitations to using LLMs with text data alone, and our findings provide a strong motivation for further work into multi-modal text-geometry models.""",2023-04-25T12:30:01Z
Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting,Yes.,3.,"""Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision).""",2023-04-25T04:09:45Z
Improved Trust in Human-Robot Collaboration with ChatGPT,Yes.,1.,"""The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach.""",2023-04-25T02:48:35Z
Semantic Compression With Large Language Models,Yes.,5.,"""However, in addition to confidently presenting factually inaccurate information at times (known as 'hallucinations'), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information.""",2023-04-25T01:47:05Z
WizardLM: Empowering Large Language Models to Follow Complex Instructions,Yes.,2.,"""Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs.""",2023-04-24T16:31:06Z
Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering,Yes.,5.,"""However, their fixed context length poses challenges when processing long documents or maintaining extended conversations.""",2023-04-24T13:55:47Z
ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain,No.,1.,The abstract discusses a BERT-based focused crawler for cybersecurity and does not mention LLMs or their limitations.,2023-04-24T09:53:33Z
Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology,Yes.,3.,"""ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent"" and ""Because of the risk of hallucination, facts provided by ChatGPT always need to be verified.""",2023-04-24T09:50:39Z
Is ChatGPT the Ultimate Programming Assistant -- How far is it?,Yes.,4.,"""our experiments also reveal limitations in terms of its attention span",2023-04-24T09:20:13Z
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT,Yes.,3.,"""GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored.""",2023-04-24T07:35:38Z
PARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis,No.,1.,The abstract does not mention language models or large language models.,2023-04-24T03:54:48Z
A Lightweight Constrained Generation Alternative for Query-focused Summarization,Yes.,3.,"""Current QFS approaches typically involve injecting additional information, e.g. query-answer relevance or fine-grained token-level interaction between a query and document, into a finetuned large language model. However, these approaches often require extra parameters \& training, and generalize poorly to new dataset distributions.""",2023-04-23T18:43:48Z
Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release,Yes.,1.,"""DomMa targets at testing Large Language Models (LLMs) on their domain knowledge understanding, it features extensive domain coverage, large data volume, and a continually updated data set based on Chinese 112 first-level subject classifications.""",2023-04-23T15:11:49Z
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models,Yes.,3.,"""the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty.""",2023-04-23T13:54:39Z
