Title,Talks about LLMs,Rate,Evidence,Published
"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",Yes.,3.,"""Our findings reveal that ChatGPT's performance in Standard-IE setting is poor,"" and ""there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration.""",2023-04-23T12:33:18Z
Differentiate ChatGPT-generated and Human-written Medical Texts,Yes.,4.,"""erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to healthcare and the general public.""",2023-04-23T07:38:07Z
Divide and Prompt: Chain of Thought Prompting for Text-to-SQL,Yes.,1.,"""Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks.""",2023-04-23T06:52:35Z
Boosting Theory-of-Mind Performance in Large Language Models via Prompting,Yes.,3.,"""Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning.""",2023-04-22T22:50:50Z
LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Yes.,5.,"""However, so far, LLMs cannot reliably solve long-horizon planning problems.""",2023-04-22T20:34:03Z
N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models,Yes.,1.,"""N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretable representations of measurable quality.""",2023-04-22T19:06:13Z
Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens,Yes.,5.,"""These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.""",2023-04-22T12:50:49Z
SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval,Yes.,3.,"""most existing language models have difficulty understanding the long-distance dependencies between different structures"" and ""existing pre-trained language models designed for general purposes have not been equipped to handle legal elements.""",2023-04-22T10:47:01Z
Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers,Yes.,5.,"""We found that LLMs perform worse than MLs in detecting incoherent answers. The difficulty seems to reside in recursive questions that contain both questions and answers, and in responses from students with typical fourth-grader misspellings.""",2023-04-21T21:25:30Z
Emergent and Predictable Memorization in Large Language Models,Yes.,5.,"""Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models.""",2023-04-21T17:58:31Z
The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination,Yes.,5.,"""new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination.""",2023-04-21T16:40:54Z
Inducing anxiety in large language models increases exploration and bias,Yes.,5.,"""Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance."" and ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text.""",2023-04-21T16:29:43Z
ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT,Yes.,4.,"""LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously.""",2023-04-21T16:23:47Z
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction,Yes.,1.,"""There is currently a significant gap between the performance of fine-tuned models and prompting approaches using Large Language Models (LLMs) on the challenging task of text-to-SQL, as evaluated on datasets such as Spider.""",2023-04-21T15:02:18Z
Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition,Yes.,3.,"""On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations.""",2023-04-21T14:21:52Z
SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model,Yes.,1.,"""we present SkinGPT-4, which is the world's first interactive dermatology diagnostic system powered by an advanced visual large language model.""",2023-04-21T01:17:09Z
Meta Semantics: Towards better natural language understanding and reasoning,Yes.,5.,"""Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem.""",2023-04-20T22:16:16Z
"""HOT"" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media",Yes.,2.,"""Our findings also suggest that ChatGPT classifications align with provided HOT definitions, but ChatGPT classifies 'hateful' and 'offensive' as subsets of 'toxic.' Moreover, the choice of prompts used to interact with ChatGPT",2023-04-20T19:40:51Z
Joint Repetition Suppression and Content Moderation of Large Language Models,,,,2023-04-20T19:17:49Z
MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,Yes.,3.,"""In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation).""",2023-04-20T18:25:35Z
Why Does ChatGPT Fall Short in Providing Truthful Answers?,Yes.,5.,"""ChatGPT still faces challenges in providing reliable and accurate answers to user questions."" and ""We further pinpoint factuality as the most contributing failure and identify two critical abilities associated with factuality",2023-04-20T17:48:43Z
Learning to Plan with Natural Language,Yes.,3.,"""LLMs can directly generate task plans, but these plans may still contain factual errors or are incomplete.""",2023-04-20T17:09:12Z
Safety Assessment of Chinese Large Language Models,Yes.,5.,"""These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information.""",2023-04-20T16:27:35Z
GPT-NER: Named Entity Recognition via Large Language Models,Yes.,3.,"""its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs",2023-04-20T16:17:26Z
Fully Autonomous Programming with Large Language Models,Yes.,5.,"""Current approaches to program synthesis with Large Language Models (LLMs) exhibit a 'near miss syndrome'",2023-04-20T16:12:05Z
CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,Yes.,2.,"""Empirical results show that the population task is still challenging, even for large language models (LLM) such as ChatGPT.""",2023-04-20T15:27:29Z
Interventional Probing in High Dimensions: An NLI Case Study,Yes.,1.,"""Probing strategies have been shown to detect the presence of various linguistic features in large language models; in particular, semantic features intermediate to the 'natural logic' fragment of the Natural Language Inference task (NLI).""",2023-04-20T14:34:31Z
Analyzing FOMC Minutes: Accuracy and Constraints of Language Models,Yes.,3.,"""The study also highlights the challenges and limitations of using current NLP techniques to analyze FOMC texts and suggests the potential for enhancing language models and exploring alternative approaches.""",2023-04-20T08:54:00Z
Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks,Yes.,3.,"""ChatGPT does have the potential to handle these data annotation tasks, although a number of challenges remain.""",2023-04-20T08:08:12Z
Supporting Human-AI Collaboration in Auditing LLMs with LLMs,Yes.,4.,"""Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale.""",2023-04-19T21:59:04Z
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,Yes.,3.,"""Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long paragraphs.""",2023-04-19T21:22:52Z
Low-resource Bilingual Dialect Lexicon Induction with Large Language Models,Yes.,2.,"""This setup poses several unique challenges, including the scarcity of resources, the relatedness of the languages, and the lack of standardization in the orthography of dialects.""",2023-04-19T20:20:41Z
Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers,Yes.,1.,"""This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews.""",2023-04-19T19:59:26Z
Fundamental Limitations of Alignment in Large Language Models,Yes.,5.,"""Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.""",2023-04-19T17:50:09Z
Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,Yes.,5.,"""LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.""",2023-04-19T17:47:47Z
GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information,Yes.,3.,"""While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations.""",2023-04-19T13:53:19Z
Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents,Yes.,3.,"""The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge.""",2023-04-19T10:16:03Z
A Theory on Adam Instability in Large-Scale Machine Learning,Yes.,3.,"""We present a theory for the previously unexplained divergent behavior noticed in the training of large language models."" and ""This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training.""",2023-04-19T06:15:11Z
Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes,Yes.,1.,"""We identify two fundamentally different strategies for implementing this system",2023-04-19T06:00:26Z
TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection,No.,1.,The abstract does not mention LLMs or any language models.,2023-04-19T04:47:36Z
LLM as A Robotic Brain: Unifying Egocentric Memory and Control,Yes.,1.,"""In this paper, we propose a novel and generalizable framework called LLM-Brain",2023-04-19T00:08:48Z
Creating Large Language Model Resistant Exams: Guidelines and Strategies,Yes.,3.,"""This article investigates the performance of LLMs on exams and their implications for assessment, focusing on ChatGPT's abilities and limitations.""",2023-04-18T18:01:32Z
Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling,Yes.,3.,"""Post-training quantization~(PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations.""",2023-04-18T17:34:23Z
Towards Designing a ChatGPT Conversational Companion for Elderly People,Yes.,3.,"""However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns.""",2023-04-18T17:24:14Z
Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,Yes.,2.,"""Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested.""",2023-04-18T17:21:48Z
In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT,Yes.,5.,"""We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions."" and ""We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases.""",2023-04-18T13:20:45Z
Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs,Yes.,4.,"""This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild. Unfortunately, most such tools are critically flawed.""",2023-04-18T13:05:01Z
Safer Conversational AI as a Source of User Delight,Yes.,3.,"""However, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology.""",2023-04-18T11:03:10Z
Masked Language Model Based Textual Adversarial Example Detection,Yes.,1.,"""whereas pre-trained masked language models can fit the manifold of normal NLP data.""",2023-04-18T06:52:14Z
A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models,Yes.,3.,"""We finally discuss existing challenges and promising future directions in the era of LLMs.""",2023-04-18T06:38:40Z
CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models,Yes.,2.,"""However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated.""",2023-04-18T02:49:53Z
Large Language Models Based Automatic Synthesis of Software Specifications,Yes.,1.,"""we propose SpecSyn a framework that leverages a state-of-the-art large language model to automatically synthesize software specifications from natural language sources.""",2023-04-18T01:22:44Z
Classification of US Supreme Court Cases using BERT-Based Techniques,Yes.,3.,"""An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis.""",2023-04-17T22:53:54Z
An Evaluation on Large Language Model Outputs: Discourse and Memorization,Yes.,4.,"""We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic.""",2023-04-17T22:12:12Z
Visual Instruction Tuning,Yes.,1.,"""Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field.""",2023-04-17T17:59:25Z
LongForm: Effective Instruction Tuning with Reverse Instructions,Yes.,2.,"""obtaining instruction data is costly and challenging"" and ""generating noisy examples via LLMs.""",2023-04-17T17:36:35Z
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT,Yes.,3.,"""such models often require substantial amounts of medical text data and have poor generalization performance"" and ""their performance in specific domains, such as radiology, remains under-investigated and potentially limited.""",2023-04-17T17:13:42Z
Low-code LLM: Graphical User Interface over Large Language Models,Yes.,2.,"""Utilizing Large Language Models (LLMs) for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process.""",2023-04-17T09:27:40Z
InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,Yes.,3.,"""recent studies have shown that existing large models still have difficulty with information extraction tasks"" and ""gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.""",2023-04-17T09:00:50Z
SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model,Yes.,2.,"""Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate.""",2023-04-17T08:43:20Z
Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding,Yes.,3.,"""We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.""",2023-04-17T04:52:43Z
Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark,Yes.,5.,"""ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs."" and ""results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability.""",2023-04-17T00:41:19Z
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping,Yes.,5.,"""However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans.""",2023-04-16T15:29:03Z
A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions,Yes.,3.,"""Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.""",2023-04-16T13:12:26Z
Solving Math Word Problems by Combining Language Models With Symbolic Solvers,Yes.,3.,"""prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning.""",2023-04-16T04:16:06Z
Tractable Control for Autoregressive Language Generation,Yes.,3.,"""it remains a major challenge to generate text that satisfies complex constraints",2023-04-15T00:19:44Z
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models,Yes.,2.,"""Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios.""",2023-04-14T21:19:46Z
The Self-Perception and Political Biases of ChatGPT,Yes.,4.,"""This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT."" and ""claiming that ChatGPT is politically biased towards progressive and libertarian points of view.""",2023-04-14T18:06:13Z
OpenAssistant Conversations -- Democratizing Large Language Model Alignment,Yes.,3.,"""state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary.""",2023-04-14T18:01:29Z
API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs,Yes.,3.,"""However, three pivotal questions remain unanswered",2023-04-14T14:05:32Z
DroidBot-GPT: GPT-powered UI Automation for Android,Yes.,1.,"""This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications.""",2023-04-14T11:31:56Z
MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data,Yes.,2.,"""Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy.""",2023-04-14T11:28:08Z
HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge,Yes.,3.,"""LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses.""",2023-04-14T07:54:17Z
nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales,Yes.,2.,"""As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones.""",2023-04-14T00:45:01Z
Stochastic Code Generation,Yes.,5.,"""Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications.""",2023-04-14T00:01:05Z
Evaluation of Social Biases in Recent Large Pre-Trained Models,Yes.,4.,"""Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models.""",2023-04-13T23:29:58Z
Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization),Yes.,4.,"""One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of 'code analysis' and extracting such information, implicitly, while processing code",2023-04-13T20:49:35Z
On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,Yes.,3.,"""Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI)."" and ""existing foundation models still underperform task-specific models.""",2023-04-13T19:50:17Z
What does CLIP know about a red circle? Visual prompt engineering for VLMs,,,,2023-04-13T17:58:08Z
Verbs in Action: Improving verb understanding in video-language models,Yes.,3.,"""state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding.""",2023-04-13T17:57:01Z
"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",Yes.,4.,"""this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation.""",2023-04-13T16:01:28Z
Are LLMs All You Need for Task-Oriented Dialogue?,Yes.,3.,"""We show that for explicit belief state tracking, LLMs underperform compared to specialized task-specific models.""",2023-04-13T14:03:14Z
AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models,Yes.,3.,"""In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge.""",2023-04-13T09:39:30Z
Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report,Yes.,1.,"""In this paper, we introduce a novel approach to ECG interpretation, leveraging recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer (ViT) models.""",2023-04-13T06:32:25Z
Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems,Yes.,4.,"""incorporating foundations models into AI systems raises significant concerns about responsible AI due to their opaque nature and rapidly advancing intelligence.""",2023-04-13T05:01:03Z
Language Instructed Reinforcement Learning for Human-AI Coordination,Yes.,1.,"""We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective.""",2023-04-13T04:47:31Z
Detection of Fake Generated Scientific Abstracts,Yes.,3.,"""The academic community has taken notice of these technological advancements and has expressed concerns regarding the difficulty of discriminating between what is real and what is artificially generated.""",2023-04-12T20:20:22Z
Can Large Language Models Transform Computational Social Science?,Yes.,3.,"""On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans.""",2023-04-12T17:33:28Z
ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,Yes.,5.,"""Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.""",2023-04-12T05:08:52Z
Understanding Causality with Large Language Models: Feasibility and Opportunities,Yes.,4.,"""We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision.""",2023-04-11T22:30:03Z
Training Large Language Models Efficiently with Sparsity and Dataflow,Yes.,3.,"""However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts",2023-04-11T21:37:13Z
chatClimate: Grounding Conversational AI in Climate Science,Yes.,5.,"""they still face two major challenges",2023-04-11T21:31:39Z
Zero-shot Temporal Relation Extraction with ChatGPT,Yes.,5.,"""Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts"" and ""The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency",2023-04-11T18:59:05Z
Toxicity in ChatGPT: Analyzing Persona-assigned Language Models,Yes.,5.,"""a clear understanding of the capabilities and limitations of LLMs is necessary,"" and ""We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations,"" and ""specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that",2023-04-11T16:53:54Z
TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning,Yes.,1.,"""The surge of artificial intelligence, specifically large language models, has led to a rapid advent towards the development of large-scale machine learning training clusters.""",2023-04-11T15:50:54Z
Approximating Online Human Evaluation of Social Chatbots with Prompting,Yes.,2.,"""Existing evaluation metrics aim to automate offline user evaluation and approximate human judgment of pre-curated dialogs. However, they are limited in their ability to capture subjective perceptions of users who actually interact with the bots and might not generalize to real-world settings.""",2023-04-11T14:45:01Z
Towards preserving word order importance through Forced Invalidation,Yes.,5.,"""recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed.""",2023-04-11T13:42:10Z
Multi-step Jailbreaking Privacy Attacks on ChatGPT,Yes.,4.,"""it is still challenging to steer AI-generated content (AIGC) for the human good"" and ""we study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats.""",2023-04-11T13:05:04Z
Teaching Large Language Models to Self-Debug,Yes.,3.,"""However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance.""",2023-04-11T10:43:43Z
Human-machine cooperation for semantic feature listing,Yes.,3.,"""Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error.""",2023-04-11T06:38:04Z
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,Yes.,4.,"""Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3)",2023-04-11T06:37:30Z
A Cheaper and Better Diffusion Language Model with Soft-Masked Noise,,,,2023-04-10T17:58:42Z
On the Possibilities of AI-Generated Text Detection,Yes.,1.,"""Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications.""",2023-04-10T17:47:39Z
Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,Yes.,4.,"""GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages."" and ""First, instruction semantics can surprisingly be ignored when given in-context exemplars.""",2023-04-10T15:51:30Z
Learnings from Data Integration for Augmented Language Models,Yes.,5.,"""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data.""",2023-04-10T13:28:35Z
Inference with Reference: Lossless Acceleration of Large Language Models,Yes.,1.,"""We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references.""",2023-04-10T09:55:14Z
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,Yes.,5.,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.""",2023-04-10T05:25:54Z
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges,Yes.,5.,"""Our findings indicate that ChatGPT is a 'Wall Street Neophyte' with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features."" and ""we observe",2023-04-10T04:31:00Z
OpenAGI: When LLM Meets Domain Experts,,,,2023-04-10T03:55:35Z
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding,Yes.,4.,"""However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation.""",2023-04-09T16:31:47Z
A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding,Yes.,3.,"""extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU"" and ""we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks.""",2023-04-09T15:28:36Z
Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance,Yes.,5.,"""Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.""",2023-04-09T04:53:15Z
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding,Yes.,3.,"""Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams.""",2023-04-08T20:41:15Z
Comparing Code Explanations Created by Students and Large Language Models,Yes.,1.,"""The recent emergence of powerful large language models (LLMs) may offer a solution.""",2023-04-08T06:52:54Z
GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation,Yes.,2.,"""current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories.""",2023-04-08T00:30:08Z
Why think step by step? Reasoning emerges from the locality of experience,Yes.,1.,"""Similarly, when large language models generate intermediate steps (a chain of thought) before answering a question, they often produce better answers than they would directly.""",2023-04-07T21:04:03Z
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,Yes.,2.,"""However, there is currently a lack of benchmark datasets for assessing the ability of LLMs to generate functionally correct code edits based on natural language descriptions of intended changes.""",2023-04-07T18:58:33Z
Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models,Yes.,5.,"""This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions.""",2023-04-07T17:14:00Z
Interpretable Unified Language Checking,Yes.,2.,"""Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge.""",2023-04-07T16:47:49Z
What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory,Yes.,3.,"""There has been concern about ideological basis and possible discrimination in text generated by Large Language Models (LLMs).""",2023-04-07T12:20:13Z
Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,5.,"""We find that automated prompting does not consistently outperform simple manual prompts.""",2023-04-07T12:06:44Z
ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions,,,,2023-04-07T08:33:08Z
Generative Agents: Interactive Simulacra of Human Behavior,Yes.,1.,"""To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior.""",2023-04-07T01:55:19Z
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4,Yes.,5.,"""However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets.""",2023-04-07T01:37:45Z
Towards Interpretable Mental Health Analysis with Large Language Models,Yes.,3.,"""existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability.""",2023-04-06T19:53:59Z
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark,Yes.,2.,"""Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity.""",2023-04-06T17:59:03Z
Instruction Tuning with GPT-4,Yes.,1.,"""In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning.""",2023-04-06T17:58:09Z
When do you need Chain-of-Thought Prompting for ChatGPT?,Yes.,5.,"""Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs. In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT.""",2023-04-06T17:47:29Z
"Large language models effectively leverage document-level context for literary translation, but critical errors persist",Yes.,4.,"""critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact.""",2023-04-06T17:27:45Z
Zero-Shot Next-Item Recommendation using Large Pretrained Language Models,Yes.,3.,"""We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences.""",2023-04-06T15:35:11Z
Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media,Yes.,3.,"""Traditional approaches include conventional machine learning, early deep neural networks, and pre-trained fine-tuning models. However, with the evolution of very large pre-trained language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face deployment challenges."" and ""demonstr",2023-04-06T14:12:02Z
Multi-label classification of open-ended questions with BERT,Yes.,1.,"""This paper focuses on multi-label classification of text answers to open-ended survey questions in social science surveys. We evaluate the performance of the transformer-based architecture BERT for the German language in comparison to traditional multi-label algorithms.""",2023-04-06T09:09:44Z
Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions,Yes.,5.,"""ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.""",2023-04-06T05:01:28Z
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",Yes.,2.,"""Our conclusion outlines obstacles that generative writing assistants create for copyright.""",2023-04-06T03:09:26Z
Approach Intelligent Writing Assistants Usability with Seven Stages of Action,Yes.,5.,"""Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability.""",2023-04-06T02:11:55Z
GPT detectors are biased against non-native English writers,Yes.,4.,"""Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified.""",2023-04-06T01:51:15Z
Context-Aware Classification of Legal Document Pages,Yes.,3.,"""they typically cannot be utilized with large pre-trained language models due to the constraint on input length.""",2023-04-05T23:14:58Z
Conceptual structure coheres in human cognition but not in large language models,Yes.,5.,"""These results highlight an important difference between contemporary LLMs and human cognition, with implications for understanding some fundamental limitations of contemporary machine language.""",2023-04-05T21:27:01Z
Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks,Yes.,1.,"""We have demonstrated that the proposed semi-supervised GAN-LM architecture (generative adversarial network on top of a pretrained language model) is a viable solution in classifying Bengali fake reviews.""",2023-04-05T20:40:09Z
Efficient OCR for Building a Diverse Digital History,No.,1.,The abstract does not mention LLMs or any limitations related to them.,2023-04-05T20:36:04Z
Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning,Yes.,1.,"""SPIRES, a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema.""",2023-04-05T19:07:04Z
Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,Yes.,4.,"""Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks remained the best strategy. The simple BoW model performed on par with the most complex LLM prompting. Prompt engineering required significant investment.""",2023-04-05T15:11:25Z
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT,Yes.,1.,"""Additionally, we designed experiments to predict the electrical performance of solar cells and design materials or devices with targeted parameters using large language models (LLMs).""",2023-04-05T04:01:52Z
Document-Level Machine Translation with Large Language Models,Yes.,2.,"""This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.""",2023-04-05T03:49:06Z
ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules,Yes.,1.,"""Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.""",2023-04-05T00:25:27Z
Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering,Yes.,3.,"""GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations.""",2023-04-04T21:47:41Z
Dialogue-Contextualized Re-ranking for Medical History-Taking,Yes.,1.,"""We test both transformer and S4-based language model backbones.""",2023-04-04T17:31:32Z
Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems,Yes.,1.,"""For a natural language problem that requires some non-trivial reasoning to solve, there are at least two ways to do it using a large language model (LLM).""",2023-04-04T13:01:48Z
Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation,Yes.,4.,"""further analysis of various types of errors at the document-level has shown that ChatGPT cannot effectively correct agreement, coreference, tense errors across sentences, and cross-sentence boundary errors.""",2023-04-04T12:33:40Z
"To ChatGPT, or not to ChatGPT: That is the question!",Yes.,5.,"""concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud... none of the existing methods can effectively detect ChatGPT-generated content.""",2023-04-04T03:04:28Z
Blockwise Compression of Transformer-based Models without Retraining,Yes.,5.,"""These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 10^23 FLOPs and hundreds of gigabytes, respectively.""",2023-04-04T02:55:40Z
Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,Yes.,2.,"""We present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias.""",2023-04-03T20:58:15Z
Classification of integers based on residue classes via modern deep learning algorithms,Yes.,3.,"""Finally, we evaluated Large Language Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated their failures.""",2023-04-03T19:53:31Z
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning,Yes.,3.,"""While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource languages, are limited.""",2023-04-03T18:46:01Z
Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,Yes.,2.,"""However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field.""",2023-04-03T17:59:09Z
Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,Yes.,4.,"""Comparative results show that using ChatGPT without human intervention may be inadequate due to reliability related issues,"" and ""We also highlight future challenges, including concerns about LLM trustworthiness and the necessity for standardisation and regulation in this domain.""",2023-04-03T16:46:49Z
DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task,Yes.,4.,"""these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs.""",2023-04-03T15:57:51Z
RPTQ: Reorder-based Post-training Quantization for Large Language Models,Yes.,3.,"""Large-scale language models (LLMs) have demonstrated impressive performance, but their deployment presents challenges due to their significant memory usage.""",2023-04-03T15:46:15Z
Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?,Yes.,3.,"""Many recent studies have discovered that traditional DNNs usually encode sparse symbolic concepts. However, because an LLM has much more parameters than traditional DNNs, whether the LLM also encodes sparse symbolic concepts is still an open problem.""",2023-04-03T15:39:35Z
Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?,Yes.,3.,"""Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating",2023-04-03T14:06:47Z
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection,Yes.,1.,"""This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families",2023-04-03T10:27:53Z
Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study,Yes.,2.,"""However, directly comparing the quality of two texts may lead to suboptimal results.""",2023-04-03T05:29:58Z
A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets,Yes.,2.,"""High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation.""",2023-04-02T08:26:38Z
Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System,Yes.,1.,"""we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process.""",2023-04-02T07:27:49Z
Querying Large Language Models with SQL,Yes.,3.,"""However, we pinpoint several research challenges that must be addressed to build a DBMS that exploits LLMs.""",2023-04-02T06:58:14Z
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models,,,,2023-04-02T05:47:09Z
Towards Healthy AI: Large Language Models Need Therapists Too,Yes.,4.,"""these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors.""",2023-04-02T00:39:12Z
DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection,Yes.,1.,"""We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments.""",2023-04-01T23:29:14Z
Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT,Yes.,3.,"""While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.""",2023-04-01T20:57:33Z
"Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics",Yes.,3.,"""Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials.""",2023-04-01T06:04:58Z
Large language models can rate news outlet credibility,Yes.,3.,"""Although large language models (LLMs) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations.""",2023-04-01T05:04:06Z
Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts,Yes.,3.,"""Recent works involving large language models (LLM) demonstrate considerable success in text generation and revision tasks; however, LLMs still struggle to provide structural and creative feedback on the document level that is crucial to academic writing.""",2023-03-31T20:33:03Z
Enhancing Large Language Models with Climate Resources,Yes.,4.,"""LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change.""",2023-03-31T20:24:14Z
A Survey of Large Language Models,Yes.,3.,"""Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.""",2023-03-31T17:28:46Z
Assessing Language Model Deployment with Risk Cards,Yes.,5.,"""text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text."" and ""Prior work establishes a wide variety of language model harms to many different actors",2023-03-31T16:45:42Z
CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia,,,,2023-03-31T15:39:54Z
BERTino: an Italian DistilBERT model,Yes.,3.,"""if on one hand the performances achieved by this kind of architectures are surprising, on the other their usability is limited by the high number of parameters which constitute their network, resulting in high computational and memory demands.""",2023-03-31T15:07:40Z
Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations,Yes.,5.,"""However, our evaluation also exposes critical limitations of the current LLM APIs. First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia. Further, our analysis shows that the API costs are generally higher and the maximum context size",2023-03-31T13:04:47Z
GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors,Yes.,4.,"""GPT-4 showed low accuracy in subjects including public health & medicine-related law, internal medicine (2) which are localized in Korea and TKM. The model's accuracy was lower for questions requiring TKM-specialized knowledge."" and ""These findings underline the potential of LLMs like GPT-4 in culturally adapted medicine, especially TKM, for tasks such as clinical assistance,",2023-03-31T05:43:21Z
AceCoder: Utilizing Existing Code to Enhance Code Generation,Yes.,3.,"""Existing prompting techniques are designed for natural language generation and have low accuracy in code generation.""",2023-03-31T02:57:15Z
Can ChatGPT be used to generate scientific hypotheses?,Yes.,3.,"""While the error rate is high, generative AI seems to be able to effectively structure vast amounts of scientific knowledge and provide interesting and testable hypotheses.""",2023-03-30T20:40:52Z
Self-Refine: Iterative Refinement with Self-Feedback,Yes.,2.,"""Like humans, large language models (LLMs) do not always generate the best output on their first try.""",2023-03-30T18:30:01Z
"Recognition, recall, and retention of few-shot memories in large language models",Yes.,5.,"""The flip side of this remarkable capacity for fast learning is that precise memories are quickly overwritten",2023-03-30T17:26:16Z
Language Models can Solve Computer Tasks,Yes.,2.,"""However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks.""",2023-03-30T16:01:52Z
WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research,Yes.,1.,"""ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically.""",2023-03-30T14:07:47Z
Yes but.. Can ChatGPT Identify Entities in Historical Documents?,Yes.,4.,"""Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance.""",2023-03-30T12:23:39Z
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure,Yes.,5.,"""This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.""",2023-03-30T10:32:18Z
The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling,Yes.,3.,"""This means that it may be challenging to build LLMs for smaller languages such as Nordic ones, where the availability of text corpora is limited.""",2023-03-30T06:42:22Z
Advances in apparent conceptual physics reasoning in GPT-4,Yes.,3.,"""Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations.""",2023-03-29T20:32:40Z
G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,Yes.,3.,"""highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts.""",2023-03-29T12:46:54Z
LMExplainer: a Knowledge-Enhanced Explainer for Language Models,Yes.,4.,"""However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios.""",2023-03-29T08:59:44Z
Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning,Yes.,3.,"""We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets"" and ""Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing.""",2023-03-29T04:00:53Z
ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models,Yes.,5.,"""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point."" and ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question.""",2023-03-29T03:05:43Z
Writing Assistants Should Model Social Factors of Language,Yes.,4.,"""Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance."" and ""a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects.""",2023-03-28T19:38:57Z
Training Language Models with Language Feedback at Scale,Yes.,3.,"""Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries.""",2023-03-28T17:04:15Z
Hallucinations in Large Multilingual Translation Models,Yes.,5.,"""However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns.""",2023-03-28T16:17:59Z
Improving Code Generation by Training with Natural Language Feedback,Yes.,1.,"""The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development.""",2023-03-28T16:15:31Z
Synthetically generated text for supervised text analysis,,,,2023-03-28T14:55:13Z
Evaluation of ChatGPT for NLP-based Mental Health Applications,Yes.,1.,"""The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks.""",2023-03-28T04:47:43Z
Pre-training Transformers for Knowledge Graph Completion,Yes.,1.,"""Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts.""",2023-03-28T02:10:37Z
Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning,Yes.,3.,"""These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters.""",2023-03-28T00:06:38Z
ChatGPT as a Factual Inconsistency Evaluator for Text Summarization,Yes.,4.,"""However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.""",2023-03-27T22:30:39Z
"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",Yes.,5.,"""However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns.""",2023-03-27T21:27:58Z
Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses,Yes.,1.,"""Furthermore, we propose a novel two-step prompt strategy, which combines this pre-edit scheme with ChatGPT, currently the most widely used large language model.""",2023-03-27T20:33:40Z
TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models,Yes.,3.,"""Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry.""",2023-03-27T17:54:32Z
KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation,Yes.,2.,"""large language models are underestimated by prior evaluation works.""",2023-03-27T17:45:38Z
LMCanvas: Object-Oriented Interaction to Personalize Large Language Model-Powered Writing Environments,Yes.,2.,"""these interfaces provide limited support for writers to create personal tools for their own unique tasks, and may not comprehensively fulfill a writer's needs -- requiring them to continuously switch between interfaces during writing.""",2023-03-27T11:56:26Z
Large Language Models are Diverse Role-Players for Summarization Evaluation,Yes.,1.,"""In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects.""",2023-03-27T10:40:59Z
Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain,Yes.,3.,"""However, two critical problems limit its advancement",2023-03-27T01:41:48Z
MGTBench: Benchmarking Machine-Generated Text Detection,Yes.,2.,"""This raises concerns regarding authenticity, accountability, and potential bias.""",2023-03-26T21:12:36Z
WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation,Yes.,3.,"""Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks.""",2023-03-26T20:41:21Z
Task-oriented Memory-efficient Pruning-Adapter,Yes.,2.,"""The Outstanding performance and growing size of Large Language Models has led to increased attention in parameter efficient learning."" and ""So efficiency of training and inference can't be obtained in the same time.""",2023-03-26T12:18:00Z
Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System,Yes.,1.,"""Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks.""",2023-03-25T17:37:43Z
Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining,Yes.,3.,"""many Pretrained Language Model (PLM) lack synonym knowledge due to limitation of small-scale synsets and PLM's pretraining objectives.""",2023-03-25T10:19:14Z
Freestyle Layout-to-Image Synthesis,No.,1.,The abstract does not mention LLMs or any limitations related to them.,2023-03-25T09:37:41Z
Backdoor Attacks with Input-unique Triggers in NLP,Yes.,3.,"""IDBA generates context-related triggers by continuing writing the input with a language model like GPT2.""",2023-03-25T01:41:54Z
TRAK: Attributing Model Behavior at Scale,Yes.,1.,"""We demonstrate the utility of TRAK across various modalities and scales",2023-03-24T17:56:22Z
"""Get ready for a party"": Exploring smarter smart spaces with help from large language models",Yes.,1.,"""In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions.""",2023-03-24T16:51:08Z
ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge,Yes.,3.,"""The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice.""",2023-03-24T15:29:16Z
Paraphrase Detection: Human vs. Machine Content,Yes.,3.,"""Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance.""",2023-03-24T13:25:46Z
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods,Yes.,1.,"""Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc.""",2023-03-24T13:24:41Z
$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,Yes.,5.,"""we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment.""",2023-03-24T06:16:29Z
Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models,Yes.,3.,"""utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level.""",2023-03-24T05:05:03Z
Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function,Yes.,1.,"""P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases.""",2023-03-24T04:33:40Z
Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching,Yes.,1.,"""We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data.""",2023-03-24T03:14:00Z
Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages,Yes.,5.,"""publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its performance varies depending",2023-03-23T18:16:30Z
The Quantization Model of Neural Scaling,Yes.,1.,"""We validate this prediction on toy datasets, then study how scaling curves decompose for large language models.""",2023-03-23T17:58:43Z
"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",,,,2023-03-23T16:29:27Z
ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model,Yes.,3.,"""While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study.""",2023-03-23T15:34:26Z
Increasing Textual Context Size Boosts Medical Image-Text Matching,Yes.,5.,"""CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required.""",2023-03-23T15:20:05Z
Fairness-guided Few-shot Prompting for Large Language Models,Yes.,5.,"""prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats.""",2023-03-23T12:28:25Z
A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,Yes.,1.,"""Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale.""",2023-03-23T09:00:07Z
SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization,Yes.,5.,"""However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings.""",2023-03-23T04:47:46Z
A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification,No.,1.,The abstract does not mention any large language models (LLMs) or their limitations. It focuses on Transformer-based models and their application to clinical text classification tasks.,2023-03-22T20:10:29Z
Interpretable Bangla Sarcasm Detection using BERT and Explainable AI,Yes.,1.,"""In this article, we present a BERT-based system that can achieve 99.60\% while the utilized traditional machine learning algorithms are only capable of achieving 89.93\%.""",2023-03-22T17:35:35Z
Can we trust the evaluation on ChatGPT?,Yes.,5.,"""evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF)."" and ""We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.""",2023-03-22T17:32:56Z
Sparks of Artificial General Intelligence: Early experiments with GPT-4,,,,2023-03-22T16:51:28Z
Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study,Yes.,3.,"""LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F1 scores.""",2023-03-22T13:46:16Z
MEGA: Multilingual Evaluation of Generative AI,Yes.,4.,"""An important question being asked by the AI community today is about the capabilities and limits of these models,"" and ""discuss challenges in improving the performance of generative LLMs on low-resource languages.""",2023-03-22T13:03:10Z
MEDIMP: 3D Medical Images with clinical Prompts from limited tabular data for renal transplantation,Yes.,1.,"""taking inspiration from the recent success of Large Language Models (LLMs),"" and ""generates medical prompts using automatic textual data augmentations from LLMs.""",2023-03-22T10:30:43Z
Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense,,,,2023-03-21T18:45:09Z
Large Language Models Can Be Used to Estimate the Latent Positions of Politicians,Yes.,1.,"""We leverage the embedded knowledge in generative large language models (LLMs) to address this challenge and measure lawmakers' positions along specific political or policy dimensions.""",2023-03-21T17:48:00Z
cTBLS: Augmenting Large Language Models with Conversational Tables,Yes.,3.,"""Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge.""",2023-03-21T17:04:44Z
Logical Reasoning over Natural Language as Knowledge Representation: A Survey,Yes.,3.,"""This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners,"" and ""challenges of the new paradigm.""",2023-03-21T16:56:05Z
TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering,Yes.,2.,"""we automatically generate several question-answer pairs using a language model"" and ""highlight the limitations and challenges of current models.""",2023-03-21T14:41:02Z
ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing,Yes.,2.,"""Potential ethical issues that could arise with the emergence of large language models like GPT-3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.""",2023-03-21T14:35:07Z
ChatGPT for Programming Numerical Methods,Yes.,5.,"""Through these examples, we investigate the successes, failures, and challenges of ChatGPT. Examples of failures are producing singular matrices, operations on arrays with incompatible sizes, programming interruption for relatively long codes, etc.""",2023-03-21T12:18:17Z
Implicit Neural Representation for Cooperative Low-light Image Enhancement,No.,1.,The abstract focuses on low-light image enhancement methods and does not mention language models (LLMs or LMs).,2023-03-21T10:24:29Z
A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?,Yes.,2.,"""Impressed by the capability of the ChatGPT, many people are wondering about its limits",2023-03-21T10:09:47Z
The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue,Yes.,3.,"""There is a surge in interest in the development of open-domain chatbots, driven by the recent advancements of large language models."" and ""we question the assumptions behind open-domain chatbots and identify paths forward for enabling common ground in human-computer dialogue.""",2023-03-21T10:01:49Z
Language Model Behavior: A Comprehensive Survey,Yes.,5.,"""the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases.""",2023-03-20T23:54:26Z
"Large Language Models and Simple, Stupid Bugs",Yes.,4.,"""Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training."" and ""We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as",2023-03-20T21:14:06Z
Reflexion: Language Agents with Verbal Reinforcement Learning,Yes.,3.,"""However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning.""",2023-03-20T18:08:50Z
Context-faithful Prompting for Large Language Models,Yes.,4.,"""However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks).""",2023-03-20T17:54:58Z
DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4,Yes.,1.,"""The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability.""",2023-03-20T11:34:37Z
Retrieving Multimodal Information for Augmented Generation: A Survey,Yes.,2.,"""However, there lacks a unified perception of at which stage and how to incorporate different modalities.""",2023-03-20T05:07:41Z
Dynamic Documentation for AI Systems,Yes.,2.,"""documentation standards for AI remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as Large Language Models (LLMs).""",2023-03-20T04:23:07Z
The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery,Yes.,4.,"""The research concludes that monolithic multimodal models currently lack the coherent memory to maintain context and format for this task and that until recently, the language models like GPT-2/3 struggled to format similar problems without degenerating into repetitive or non-sensical combinations of ingredients.""",2023-03-20T01:57:52Z
Bangla Grammatical Error Detection Using T5 Transformer Model,Yes.,3.,"""The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection.""",2023-03-19T09:24:48Z
Revisiting the Plastic Surgery Hypothesis via Large Language Models,Yes.,3.,"""the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.""",2023-03-18T20:33:46Z
SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models,Yes.,3.,"""Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also lead to highly prohibitive computational costs.""",2023-03-18T17:56:01Z
A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models,Yes.,4.,"""While this strategy enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks. Furthermore, our findings indicate that there is still room for improvement in areas such as model robustness.""",2023-03-18T14:02:04Z
An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering,Yes.,3.,"""We carefully analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks and two other popular datasets, WebQuestionSP and FreebaseQA, and find that knowledge distillation techniques and knowledge enhancement methods in PLMs are promising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal of attention in the NLP community, demonstrating its impressive capabilities and",2023-03-18T08:57:09Z
Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review,Yes.,5.,"""there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts"" and ""we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency,",2023-03-17T18:14:46Z
Can AI-Generated Text be Reliably Detected?,Yes.,5.,"""The unregulated use of LLMs can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc."" and ""we show that these detectors are not reliable in practical scenarios."" and ""even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks",2023-03-17T17:53:19Z
Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt,Yes.,2.,"""Future research will look into ways to diversify SATD datasets in order to maximize the latent power in large BERT models.""",2023-03-16T19:47:38Z
DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion,Yes.,1.,"""our approach utilizes large language models to bridge texts and visual images for stylization and build an unsupervised generative model with a diffusion model backbone.""",2023-03-16T19:12:52Z
LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations,Yes.,3.,"""the security of the code they generate has not been extensively investigated nor documented.""",2023-03-16T15:13:58Z
Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?,Yes.,5.,"""We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (<70% on even entry-level modules)."" and ""some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps).""",2023-03-16T13:58:45Z
How well do Large Language Models perform in Arithmetic tasks?,Yes.,3.,"""Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step.""",2023-03-16T09:28:15Z
A Short Survey of Viewing Large Language Models in Legal Aspect,Yes.,4.,"""the integration of LLMs into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability.""",2023-03-16T08:01:22Z
Exploring Distributional Shifts in Large Language Models for Code Analysis,Yes.,5.,"""We establish that samples from each new domain present all the models with a significant challenge of distribution shift.""",2023-03-16T07:45:46Z
"Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential",Yes.,3.,"""ChatGPT also presents some randomness in its responses with occasionally over-simplified or neglected information, which can be mitigated using a more detailed prompt."" and ""further efforts are needed to address limitations and maximize their potential.""",2023-03-16T02:21:39Z
ART: Automatic multi-step reasoning and tool-use for large language models,Yes.,3.,"""Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use.""",2023-03-16T01:04:45Z
Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs,Yes.,3.,"""These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses.""",2023-03-15T21:10:33Z
SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,Yes.,5.,"""LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.""",2023-03-15T19:31:21Z
"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",Yes.,5.,"""current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.""",2023-03-15T12:20:13Z
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation,Yes.,3.,"""the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization.""",2023-03-15T10:53:49Z
A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records,Yes.,3.,"""The generalizability of clinical large language models is usually ignored during the model development process.""",2023-03-15T08:44:07Z
ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation,Yes.,3.,"""revealing that none of the current methods can achieve the original model quality for quantization with either INT4-weight or INT4-weight-and-INT8-activation.""",2023-03-15T01:27:15Z
Attention-likelihood relationship in transformers,Yes.,3.,"""unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers.""",2023-03-15T00:23:49Z
Chat with the Environment: Interactive Multimodal Perception Using Large Language Models,Yes.,2.,"""Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold.""",2023-03-14T23:01:27Z
Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures,Yes.,1.,"""We explored 6 state-of-the-art pretrained transformer models for the three subtasks, including GatorTron, a large language model pretrained using >90 billion words of text.""",2023-03-14T22:22:28Z
How Many Demonstrations Do You Need for In-context Learning?,,,,2023-03-14T17:50:45Z
Do Transformers Parse while Predicting the Masked Word?,Yes.,1.,"""Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling.""",2023-03-14T17:49:50Z
Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family,Yes.,3.,"""there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model.""",2023-03-14T15:46:28Z
A Theory of Emergent In-Context Learning as Implicit Structure Induction,Yes.,1.,"""Despite progress, theoretical understanding of this phenomenon remains limited.""",2023-03-14T15:24:05Z
RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback,Yes.,1.,"""RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models.""",2023-03-14T04:20:59Z
Input-length-shortening and text generation via attention values,Yes.,5.,"""transformer models usually have an input-length limitation caused by hardware constraints"" and ""This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model.""",2023-03-14T02:11:24Z
Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification,Yes.,1.,"""We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings.""",2023-03-13T14:09:53Z
FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU,Yes.,3.,"""The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators.""",2023-03-13T05:19:28Z
Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks,Yes.,1.,"""Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs.""",2023-03-11T15:45:47Z
"ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",Yes.,1.,"""This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models (LLMs), such as ChatGPT to automate common software engineering activities.""",2023-03-11T14:43:17Z
Consistency Analysis of ChatGPT,Yes.,5.,"""Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.""",2023-03-11T01:19:01Z
Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook,Yes.,2.,"""Accepted evaluative metrics for LLMs are not human-centered.""",2023-03-10T22:15:49Z
Susceptibility to Influence of Large Language Models,Yes.,3.,"""some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data.""",2023-03-10T16:53:30Z
Do large language models resemble humans in language use?,Yes.,3.,"""However, their internal workings remain a black box, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use."" and ""Finally, unlike humans, neither model preferred using shorter words to convey less informative content, nor did they use context to resolve syntactic ambiguities.""",2023-03-10T10:47:59Z
Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors,Yes.,1.,"""we use a large language model to query which interactions are possible between a human and a given object category""",2023-03-09T19:08:02Z
Planning with Large Language Models for Code Generation,Yes.,5.,"""Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation.""",2023-03-09T18:59:47Z
Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,Yes.,4.,"""it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values"" and ""identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a",2023-03-09T17:52:07Z
Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code,Yes.,5.,"""However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored."" and ""MCQs containing code snippets are not answered as successfully as those that only contain natural language."" and ""MCQs that require analysis and/or reasoning",2023-03-09T16:52:12Z
Dynamic Stashing Quantization for Efficient Transformer Training,Yes.,3.,"""Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning.""",2023-03-09T14:44:31Z
ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction,Yes.,3.,"""Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning."" and ""Applying LLMs to DIE poses two challenges",2023-03-09T06:24:50Z
Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification,Yes.,1.,"""exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formulas.""",2023-03-09T00:09:58Z
nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,Yes.,1.,"""We present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language.""",2023-03-08T20:08:53Z
Stealing the Decoding Algorithms of Language Models,Yes.,1.,"""A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms.""",2023-03-08T17:15:58Z
Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference,Yes.,1.,"""Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications.""",2023-03-08T15:52:14Z
MenuCraft: Interactive Menu System Design with Large Language Models,Yes.,1.,"""With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems.""",2023-03-08T10:39:38Z
Automatically Auditing Large Language Models via Discrete Optimization,Yes.,5.,"""Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging."" and ""Our work offers a promising new tool to uncover models' failure-modes before deployment.""",2023-03-08T05:09:59Z
Does Synthetic Data Generation of LLMs Help Clinical Text Mining?,Yes.,5.,"""However, their effectiveness in the healthcare sector remains uncertain"" and ""our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API.""",2023-03-08T03:56:31Z
Can large language models build causal graphs?,Yes.,3.,"""LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs.""",2023-03-07T22:05:31Z
Gradient-Free Structured Pruning with Unlabeled Data,Yes.,2.,"""Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency.""",2023-03-07T19:12:31Z
From Copilot to Pilot: Towards AI Supported Software Development,Yes.,4.,"""Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs."" and ""We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid",2023-03-07T18:56:52Z
Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering,Yes.,3.,"""These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses.""",2023-03-07T17:54:53Z
ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification,Yes.,3.,"""However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages.""",2023-03-07T14:59:33Z
Larger language models do in-context learning differently,Yes.,3.,"""First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold",2023-03-07T12:24:17Z
Exploring the Feasibility of ChatGPT for Event Extraction,Yes.,5.,"""While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction."" and ""Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is",2023-03-07T12:03:58Z
Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles,Yes.,1.,"""an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance.""",2023-03-07T09:20:43Z
Stylometric Detection of AI-Generated Text in Twitter Timelines,Yes.,5.,"""However, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the AI starts to generate tweets in a given Twitter timeline.""",2023-03-07T07:26:09Z
CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,Yes.,5.,"""a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation.""",2023-03-07T03:23:14Z
Large Language Models as Zero-Shot Human Models for Human-Robot Interaction,Yes.,3.,"""That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps.""",2023-03-06T23:16:24Z
"Can an Embodied Agent Find Your ""Cat-shaped Mug""? LLM-Guided Exploration for Zero-Shot Object Navigation",Yes.,1.,"""We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW).""",2023-03-06T20:19:19Z
Spelling convention sensitivity in neural language models,Yes.,3.,"""We further experiment with correcting for biases in the training data by fine-tuning T5 on synthetic data that has been debiased, and find that finetuned T5 remains only somewhat sensitive to spelling consistency. Further experiments show GPT2 to be similarly limited.""",2023-03-06T19:29:20Z
Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting,Yes.,1.,"""We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts... and (2) non-diegetic prompts...""",2023-03-06T14:58:42Z
Towards Zero-Shot Functional Compositionality of Language Models,Yes.,4.,"""Despite such success, in this paper, we argue that current paradigms of working with PLMs are neglecting a critical aspect of modeling human intelligence",2023-03-06T13:15:25Z
DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,Yes.,3.,"""The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. The modality gap issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix",2023-03-06T11:02:47Z
"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",Yes.,3.,"""However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even",2023-03-06T10:08:51Z
LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models,Yes.,1.,"""we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) such as ChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing these tasks.""",2023-03-06T06:47:22Z
OpenICL: An Open-Source Framework for In-context Learning,Yes.,1.,"""In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation.""",2023-03-06T06:20:25Z
Could a Large Language Model be Conscious?,Yes.,5.,"""Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models",2023-03-04T19:14:20Z
The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges,Yes.,2.,"""Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models.""",2023-03-04T13:12:18Z
MathPrompter: Mathematical Reasoning using Large Language Models,Yes.,5.,"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers."" and ""we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.""",2023-03-04T04:43:49Z
TrojText: Test-time Invisible Textual Trojan Insertion,No.,1.,The abstract does not mention LLMs or their limitations. It focuses on textual Trojan attacks in NLP models.,2023-03-03T22:19:22Z
Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models,Yes.,1.,"""Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task.""",2023-03-03T20:35:38Z
Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM,Yes.,3.,"""Our results show that 0-shot performance suffers from overgeneration and generating in the wrong language, but this is greatly improved in the few-shot setting, with very good results for a number of language pairs.""",2023-03-03T13:23:42Z
Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering,Yes.,3.,"""Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question.""",2023-03-03T13:05:15Z
Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines,Yes.,5.,"""we question whether such models can guarantee factual accuracy"" and ""we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models.""",2023-03-03T04:27:44Z
Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,No.,1.,The abstract discusses transformers and Sparse Mixture-of-Experts (SMoEs) but does not specifically mention language models or large language models.,2023-03-02T22:12:51Z
Mixture of Soft Prompts for Controllable Data Generation,Yes.,4.,"""structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.""",2023-03-02T21:13:56Z
WiCE: Real-World Entailment for Claims in Wikipedia,Yes.,3.,"""we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time."" and ""Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.""",2023-03-02T17:45:32Z
Open-World Object Manipulation using Pre-trained Vision-Language Models,Yes.,1.,"""we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand.""",2023-03-02T01:55:10Z
Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents,Yes.,5.,"""Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.""",2023-03-01T22:58:50Z
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers,Yes.,1.,"""To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply.""",2023-03-01T20:21:23Z
R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents,Yes.,5.,"""Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely.""",2023-03-01T18:46:40Z
Domain-adapted large language models for classifying nuclear medicine reports,Yes.,3.,"""it is unclear how well these models generalize to nuclear medicine which has domain-specific vocabulary and unique reporting styles.""",2023-03-01T09:48:39Z
Competence-Based Analysis of Language Models,Yes.,4.,"""these models can be alarmingly brittle to small changes in inputs or application contexts.""",2023-03-01T08:53:36Z
How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,Yes.,5.,"""Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language",2023-03-01T07:39:01Z
Can ChatGPT Assess Human Personalities? A General Evaluation Framework,Yes.,3.,"""Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.""",2023-03-01T06:16:14Z
Almanac: Retrieval-Augmented Language Models for Clinical Medicine,Yes.,3.,"""Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements.""",2023-03-01T02:30:11Z
Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics,Yes.,3.,"""Large language models are not detailed models of human linguistic processing.""",2023-02-28T20:49:38Z
Automatic Scoring of Dream Reports' Emotional Content with Large Language Models,Yes.,3.,"""Our results show that the off-the-shelf method achieves a low performance probably in light of inherent linguistic differences between reports collected in different (groups of) individuals.""",2023-02-28T18:23:17Z
Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,Yes.,3.,"""We hypothesize that TAPP assists language models to better estimate the output distribution by focusing more on the instruction of the target task during inference. In other words, such ability does not seem to be sufficiently activated in not only base LLMs but also many instruction-fine-tuned LLMs.""",2023-02-28T16:06:35Z
Zero-Shot Cross-Lingual Summarization via Large Language Models,Yes.,3.,"""Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability."" and ""Due to the composite nature of CLS, which",2023-02-28T01:27:37Z
Reward Design with Language Models,Yes.,1.,"""This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function.""",2023-02-27T22:09:35Z
Systematic Rectification of Language Models via Dead-end Analysis,Yes.,4.,"""With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses."" and ""Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse.""",2023-02-27T17:47:53Z
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks,Yes.,2.,"""As the size of large language models continue to scale, so does the computational resources required to run it.""",2023-02-27T16:43:04Z
Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity,Yes.,3.,"""However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration."" and ""there was an upper bound of 25% of outputs bring rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content.""",2023-02-27T15:54:57Z
"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",Yes.,4.,"""Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT.""",2023-02-27T14:26:29Z
The (ab)use of Open Source Code to Train Large Language Models,Yes.,4.,"""LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization.""",2023-02-27T11:34:53Z
Towards Human-Bot Collaborative Software Architecting with ChatGPT,Yes.,2.,"""Preliminary results indicate that ChatGPT can mimic an architect's role to support and often lead ACSE, however; it requires human oversight and decision support for collaborative architecting.""",2023-02-26T16:32:16Z
Fast Attention Requires Bounded Entries,Yes.,1.,"""In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT.""",2023-02-26T02:42:39Z
On pitfalls (and advantages) of sophisticated large language models,Yes.,5.,"""However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud",2023-02-25T11:14:39Z
Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study,Yes.,1.,"""This study explores how large language models and gamification can scaffold coding learning and increase Chinese students sense of belonging in introductory programming courses.""",2023-02-25T04:07:03Z
Robot Behavior-Tree-Based Task Generation with Large Language Models,Yes.,3.,"""Assessment on Phase-Step prompts and the limitation of large language models are presented and discussed.""",2023-02-24T22:53:10Z
Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,Yes.,3.,"""However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains.""",2023-02-24T18:58:06Z
Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,Yes.,5.,"""applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge.""",2023-02-24T18:48:43Z
HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media,Yes.,1.,"""We explore some of the most popular transformer models such as BERT, DistilBERT, RoBERTa, and XLNet.""",2023-02-24T18:17:38Z
Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary,Yes.,1.,"""Building them is a complex task that, to the best of our knowledge, has yet to be explored with generative Large Language Models (LLMs).""",2023-02-24T16:59:54Z
Language Models are Few-shot Learners for Prognostic Prediction,Yes.,1.,"""This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas.""",2023-02-24T15:35:36Z
Adapting Pre-trained Language Models for Quantum Natural Language Processing,,,,2023-02-24T14:59:02Z
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models,Yes.,3.,"""Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models."" and ""We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.""",2023-02-24T05:26:08Z
Extracting Victim Counts from Text,Yes.,2.,"""Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate few-shot and out-of-distribution performance.""",2023-02-23T23:50:24Z
"Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",Yes.,4.,"""we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness.""",2023-02-23T22:14:01Z
CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models,Yes.,1.,"""CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records.""",2023-02-23T21:23:06Z
Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning,Yes.,5.,"""GPT-3 failed for every prompt but one, often offering answers that show a critical lack of understanding even of high-frequency words used in these less frequent grammatical constructions.""",2023-02-23T20:18:52Z
Active Prompting with Chain-of-Thought for Large Language Models,Yes.,2.,"""However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks.""",2023-02-23T18:58:59Z
What Makes a Language Easy to Deep-Learn?,Yes.,1.,"""We evaluate the memorization and generalization capabilities of a large language model and recurrent neural networks.""",2023-02-23T18:57:34Z
Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection,,,,2023-02-23T17:14:38Z
An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),Yes.,5.,"""We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not."" and ""the probability of failure increases linearly with the number of addition and subtraction operations.""",2023-02-23T16:06:16Z
Sentence Simplification via Large Language Models,Yes.,2.,"""However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system.""",2023-02-23T12:11:58Z
Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments,Yes.,1.,"""We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data.""",2023-02-22T20:56:40Z
How Does In-Context Learning Help Prompt Tuning?,Yes.,3.,"""Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale."" and ""PT is unstable and exhibits high variance.""",2023-02-22T17:45:12Z
Guiding Large Language Models via Directional Stimulus Prompting,Yes.,1.,"""We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs.""",2023-02-22T17:44:15Z
In-context Example Selection with Influences,Yes.,3.,"""ICL performance is known to be highly sensitive to input examples.""",2023-02-21T22:47:45Z
$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models,Yes.,3.,"""it can be infeasible when it comes to modern large-scale language models such as GPT-3, which can only be accessed through APIs, making it difficult to access the internal parameters of the model.""",2023-02-21T18:54:21Z
"ChatGPT: Jack of all trades, master of none",Yes.,5.,"""Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few-shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed",2023-02-21T15:20:37Z
A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,Yes.,2.,"""This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs.""",2023-02-21T12:42:44Z
Mask-guided BERT for Few Shot Text Classification,Yes.,3.,"""the data-intensive nature of the transformer architecture requires much labeled data, which is challenging in low-resource scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the difficulty of training robust models on small amounts of samples, which frequently leads to overfitting.""",2023-02-21T05:24:00Z
Zero-Shot Information Extraction via Chatting with ChatGPT,Yes.,1.,"""Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods.""",2023-02-20T12:57:12Z
90% F1 Score in Relational Triple Extraction: Is it Real ?,No.,1.,The abstract discusses joint entity and relation extraction models and does not mention language models (LLMs or LMs).,2023-02-20T10:30:16Z
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,Yes.,3.,"""We show that measuring uncertainty in natural language is challenging because of 'semantic equivalence' -- different sentences can mean the same thing.""",2023-02-19T20:10:07Z
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference,Yes.,1.,"""This study used a form of artificial intelligence (AI) known as large language models (LLMs) to assess whether language-based representations of emotion causally contribute to the AI's ability to generate inferences about the emotional meaning of novel situations.""",2023-02-19T14:21:33Z
How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation,Yes.,3.,"""Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages.""",2023-02-18T02:11:36Z
Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints,Yes.,5.,"""We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures.""",2023-02-17T23:30:28Z
"Complex QA and language models hybrid architectures, Survey",Yes.,5.,"""Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA."" and ""integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (",2023-02-17T18:31:31Z
Privately Customizing Prefinetuning to Better Match User Data in Federated Learning,Yes.,1.,"""it privately computes and compares a Frchet distance between embeddings generated by a large language model on both the central (public) dataset and the federated private client data.""",2023-02-17T18:18:22Z
Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?,No.,1.,"The abstract discusses generative AI tools in general, such as DALL-E, MidJourney, or ChatGPT, but does not specifically focus on language models or their limitations.",2023-02-17T17:39:41Z
"How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",Yes.,4.,"""By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well",2023-02-17T15:48:37Z
Massively Multilingual Shallow Fusion with Large Language Models,Yes.,1.,"""While large language models (LLM) have made impressive progress in natural language processing, it remains unclear how to utilize them in improving automatic speech recognition (ASR).""",2023-02-17T14:46:38Z
Auditing large language models: a three-layered approach,Yes.,5.,"""However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks."" and ""However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLM",2023-02-16T18:55:21Z
LEVER: Learning to Verify Language-to-Code Generation with Execution,Yes.,3.,"""it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program.""",2023-02-16T18:23:22Z
Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks,Yes.,5.,"""We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head.""",2023-02-16T16:18:03Z
LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation,Yes.,3.,"""Large-scale language-agnostic sentence embedding models such as LaBSE (Feng et al., 2022) obtain state-of-the-art performance for parallel sentence alignment. However, these large-scale models can suffer from inference speed and computation overhead.""",2023-02-16T16:05:34Z
Do We Still Need Clinical Language Models?,Yes.,4.,"""it remains unclear whether these models trained primarily with general web text are the right tool in highly specialized, safety critical domains such as clinical text."" and ""We show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when finetuned on limited annotated data.""",2023-02-16T05:08:34Z
Tree-Based Representation and Generation of Natural and Mathematical Language,Yes.,1.,"""In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math.""",2023-02-15T22:38:34Z
Commonsense Reasoning for Conversational AI: A Survey of the State of the Art,Yes.,4.,"""state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial."" and ""the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions.""",2023-02-15T19:55:57Z
Speculative Decoding with Big Little Decoder,Yes.,5.,"""However, these models have long inference latency, which limits their deployment and makes them prohibitively expensive for various real-time applications.""",2023-02-15T18:55:29Z
Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation,Yes.,3.,"""However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios.""",2023-02-15T18:46:42Z
A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning,Yes.,5.,"""although it demonstrates some level of rational decision-making, many of its decisions violate at least one of the axioms even under reasonable constructions of preferences, bets, and decision-making prompts. ChatGPT's outputs on such problems generally tended to be unpredictable.""",2023-02-15T05:04:49Z
"Conversational AI-Powered Design: ChatGPT as Designer, User, and Product",Yes.,3.,"""The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity.""",2023-02-15T00:14:17Z
Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models,Yes.,4.,"""However, safely deploying them in real world applications is challenging because they generate toxic content.""",2023-02-14T23:00:42Z
BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models,Yes.,4.,"""Pretrained Language Models (PLMs) harbor inherent social biases that can result in harmful real-world implications.""",2023-02-14T22:07:57Z
ScatterShot: Interactive In-context Example Curation for Text Transformation,Yes.,3.,"""users tend to include only the most obvious patterns when crafting examples, resulting in underspecified in-context functions that fall short on unseen cases.""",2023-02-14T21:13:31Z
ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models,Yes.,3.,"""However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions.""",2023-02-14T18:54:06Z
Few-shot learning approaches for classifying low resource domain specific software requirements,Yes.,3.,"""the availability of even a few hundred annotated samples may not always be guaranteed in low resource domains like automotive, which often limits the usage of such deep learning models in an industrial setting.""",2023-02-14T10:19:23Z
Learning gain differences between ChatGPT and human tutor generated algebra hints,Yes.,3.,"""Learning gains from human-created hints were substantially and statistically significantly higher than ChatGPT hints in both topic areas.""",2023-02-14T07:20:48Z
Machine Learning Model Attribution Challenge,,,,2023-02-13T22:05:27Z
On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark),,,,2023-02-13T21:37:41Z
Guiding Pretraining in Reinforcement Learning with Large Language Models,Yes.,1.,"""By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop.""",2023-02-13T21:16:03Z
Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning,Yes.,1.,"""We develop conversational safety classifiers via the prompt-tuning PET method and show how the unique characteristics of the PET regime enable TracIn to identify the cause for certain misclassifications by LLMs.""",2023-02-13T18:54:58Z
Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge,Yes.,4.,"""Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications.""",2023-02-13T18:00:44Z
Diminished Diversity-of-Thought in a Standard Large Language Model,,,,2023-02-13T17:57:50Z
Implications of the Convergence of Language and Vision Model Geometries,Yes.,3.,"""Large-scale pretrained language models (LMs) are said to 'lack the ability to connect [their] utterances to the world' (Bender and Koller, 2020).""",2023-02-13T17:55:54Z
An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation,Yes.,2.,"""suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.""",2023-02-13T17:13:41Z
Can GPT-3 Perform Statutory Reasoning?,Yes.,5.,"""While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which",2023-02-13T04:56:11Z
Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection,Yes.,1.,"""Our approach uses the domain invariance and contextual understanding of a pre-trained joint vision and language model to predict the class distribution of unlabelled data.""",2023-02-13T00:46:34Z
MarioGPT: Open-Ended Text2Level Generation through Large Language Models,Yes.,1.,"""Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains.""",2023-02-12T19:12:24Z
Level Generation Through Large Language Models,Yes.,3.,"""Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models.""",2023-02-11T23:34:42Z
Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes,Yes.,1.,"""We identify this as a question answering (QA) task and employ several state-of-the-art LLMs to present contexts around risk prediction model inferences and evaluate their acceptability.""",2023-02-11T18:07:11Z
Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,Yes.,5.,"""Unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models."" and ""instruction-following LLMs can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by LLM API vendors."" and ""Together, our findings suggest that LLMs will increasingly attract more sophisticated adversaries and attacks,",2023-02-11T15:57:44Z
Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech,Yes.,5.,"""We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.""",2023-02-11T03:13:54Z
Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models,Yes.,5.,"""it has been difficult to prevent semantic hallucinations in generative Large Language Models,"" and ""Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency.""",2023-02-11T02:43:34Z
FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models,Yes.,4.,"""Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on.""",2023-02-10T20:54:10Z
Large Language Models for Code: Security Hardening and Adversarial Testing,Yes.,4.,"""However, LMs lack awareness of security and are found to frequently produce unsafe code.""",2023-02-10T15:28:55Z
Translating Natural Language to Planning Goals with Large-Language Models,Yes.,5.,"""Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks."" and ""However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used.""",2023-02-10T09:17:52Z
In-Context Learning with Many Demonstration Examples,Yes.,5.,"""existing PLMs are bottlenecked by the memory and computational cost when scaling up to a large context size, leaving instruction tuning and in-context learning of many demonstration examples, as well as long-range language modeling under-explored.""",2023-02-09T20:53:12Z
"Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",Yes.,1.,"""The method builds on top of natural language processing and large general language models but can work with almost any such model.""",2023-02-09T19:56:37Z
Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow,Yes.,3.,"""Recent research has shown that language models exploit `artifacts' in benchmarks to solve tasks, rather than truly learning them, leading to inflated model performance.""",2023-02-09T04:43:10Z
Prompting for Multimodal Hateful Meme Classification,Yes.,1.,"""we propose PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification.""",2023-02-08T16:04:08Z
Training-free Lexical Backdoor Attacks on Language Models,Yes.,4.,"""language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors.""",2023-02-08T15:18:51Z
ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots,Yes.,3.,"""Conversational AI simulates conversations with humans; however, it is limited by the data captured in the training datasets.""",2023-02-08T13:03:27Z
"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",Yes.,5.,"""We find that it is better at understanding non-Latin script languages than generating them. ... ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense",2023-02-08T12:35:34Z
CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models,Yes.,5.,"""The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure.""",2023-02-08T11:54:07Z
Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,Yes.,3.,"""We demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging.""",2023-02-08T09:44:51Z
Reliable Natural Language Understanding with Large Language Models and Answer Set Programming,Yes.,5.,"""they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question.""",2023-02-07T22:37:21Z
What Matters In The Structured Pruning of Generative Language Models?,Yes.,3.,"""Auto-regressive large language models such as GPT-3 require enormous computational resources to use."" and ""established structured pruning methods do not take into account the distinctiveness of neurons, leaving behind excess redundancies.""",2023-02-07T22:05:55Z
Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis,Yes.,5.,"""There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost - both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time.""",2023-02-07T21:51:05Z
Long Horizon Temperature Scaling,Yes.,3.,"""However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token.""",2023-02-07T18:59:32Z
Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models,Yes.,1.,"""We show how knowledge can be distilled from Large Language Models (LLMs) to improve upon such learned metrics without requiring human annotators, by creating synthetic datasets which can be mixed into existing datasets, requiring only a corpus of text in the target language.""",2023-02-07T14:35:35Z
PLACES: Prompting Language Models for Social Conversation Synthesis,Yes.,1.,"""A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models.""",2023-02-07T05:48:16Z
APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning,No.,1.,"The abstract discusses practical NLP tasks, noisy labels, and long-tailed learning but does not specifically mention language models (LLMs or LMs).",2023-02-06T18:40:04Z
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,Yes.,4.,"""Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding.""",2023-02-06T10:01:08Z
A Categorical Archive of ChatGPT Failures,Yes.,5.,"""A comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted.""",2023-02-06T04:21:59Z
Nationality Bias in Text Generation,Yes.,4.,"""This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms."" and ""To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering.""",2023-02-05T19:15:33Z
Quantized Distributed Training of Large Models with Convergence Guarantees,Yes.,2.,"""The recent emergence of large language models such as GPT has created the need for new approaches to exploit data-parallelism."" and ""FSDP training is highly popular, yet it still encounters scalability bottlenecks.""",2023-02-05T14:20:55Z
The Science of Detecting LLM-Generated Texts,Yes.,3.,"""However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system.""",2023-02-04T04:49:17Z
Evaluating Large Language Models in Theory of Mind Tasks,Yes.,1.,"""Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans.""",2023-02-04T03:50:01Z
Towards Few-Shot Identification of Morality Frames using In-Context Learning,Yes.,1.,"""Few-shot in-context learning using pre-trained Large Language Models (LLMs) has been recently applied successfully in many NLP tasks.""",2023-02-03T23:26:59Z
Witscript 2: A System for Generating Improvised Jokes Without Wordplay,Yes.,1.,"""This paper extends that work by presenting Witscript 2, which uses a large language model to generate conversational jokes that rely on common sense instead of wordplay.""",2023-02-03T21:51:55Z
Bioformer: an efficient transformer language model for biomedical text mining,Yes.,3.,"""Despite the effectiveness, these models have hundreds of millions of parameters and are computationally expensive when applied to large-scale NLP applications.""",2023-02-03T08:04:59Z
"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",Yes.,1.,"""DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing self-explanation of feedback when encountering failures during the extended planning phases.""",2023-02-03T06:06:27Z
STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition,Yes.,,,2023-02-02T15:49:03Z
Multimodal Chain-of-Thought Reasoning in Language Models,Yes.,1.,"""Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality.""",2023-02-02T07:51:19Z
Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment,,,,2023-02-02T06:38:44Z
Conditioning Predictive Models: Risks and Strategies,Yes.,4.,"""Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us.""",2023-02-02T00:06:36Z
Collaborating with language models for embodied reasoning,Yes.,3.,"""However, LSLMs do not inherently have the ability to interrogate or intervene on the environment.""",2023-02-01T21:26:32Z
Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models,Yes.,3.,"""However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly.""",2023-02-01T17:33:12Z
Co-Writing with Opinionated Language Models Affects Users' Views,Yes.,4.,"""If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale."" and ""We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.""",2023-02-01T16:26:32Z
Analyzing Leakage of Personally Identifiable Information in Language Models,Yes.,5.,"""Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks"" and ""showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences.""",2023-02-01T16:04:48Z
Large Language Models Can Be Easily Distracted by Irrelevant Context,Yes.,5.,"""We investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context.""",2023-01-31T20:48:57Z
Benchmarking Large Language Models for News Summarization,Yes.,3.,"""existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance.""",2023-01-31T18:46:19Z
Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning,Yes.,3.,"""previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places.""",2023-01-31T17:51:45Z
FLAME: A small language model for spreadsheet formulas,Yes.,3.,"""Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters).""",2023-01-31T17:29:43Z
Skill Decision Transformer,Yes.,3.,"""However many of these methods only optimize for high returns, and may not extract much information from a diverse dataset of trajectories.""",2023-01-31T11:52:46Z
Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models,Yes.,2.,"""Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic.""",2023-01-31T03:14:57Z
Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning,Yes.,1.,"""Recently, multi-modal large language models (MLLMs) have been used as powerful tools for several multi-modal tasks but not for VCR yet, which requires elaborate reasoning on specific visual objects referred by texts.""",2023-01-30T23:43:28Z
Adaptive Machine Translation with Large Language Models,Yes.,1.,"""Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning.""",2023-01-30T21:17:15Z
Conversational Automated Program Repair,Yes.,5.,"""prior approaches simply repeatedly sample the LLM given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases.""",2023-01-30T19:22:36Z
Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation,Yes.,3.,"""However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs.""",2023-01-30T15:44:55Z
On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,Yes.,5.,"""Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs."" and ""this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data."" and ""Our results demonstrate that the state-of-the-art (SOTA) code-language models are",2023-01-30T13:21:00Z
"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Yes.,5.,"""Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility."" and ""we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.""",2023-01-30T13:20:48Z
Specializing Smaller Language Models towards Multi-Step Reasoning,Yes.,3.,"""there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities"" and ""by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability.""",2023-01-30T08:51:19Z
A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,Yes.,4.,"""LLMs must overcome frequency biases in order to master such constructions.""",2023-01-29T22:29:55Z
Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes,Yes.,4.,"""We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation.""",2023-01-29T15:52:33Z
Emerging Synergies in Causality and Deep Generative Models: A Survey,Yes.,1.,"""navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs).""",2023-01-29T04:10:12Z
Context-Aware Differential Privacy for Language Modeling,Yes.,4.,"""A critical challenge pertains to how much information these models retain and leak about the training data.""",2023-01-28T20:06:16Z
Truth Machines: Synthesizing Veracity in AI Language Models,Yes.,3.,"""highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity"" and ""truth as a non-trivial problem.""",2023-01-28T02:47:50Z
Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling,Yes.,3.,"""Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience... is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.""",2023-01-28T02:04:07Z
Context Matters: A Strategy to Pre-train Language Model for Science Education,Yes.,3.,"""However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants.""",2023-01-27T23:50:16Z
"Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases",Yes.,3.,"""W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models.""",2023-01-27T22:44:18Z
Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning,Yes.,3.,"""existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations.""",2023-01-27T18:59:01Z
Learning the Effects of Physical Actions in a Multi-modal Environment,Yes.,5.,"""Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action's outcome in a given environment.""",2023-01-27T16:49:52Z
Investigating the use of ChatGPT for the scheduling of construction projects,,,,2023-01-27T12:05:44Z
Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning,Yes.,3.,"""it remains unclear if they can handle inputs that have been distributionally shifted effectively.""",2023-01-27T11:27:40Z
A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks,Yes.,3.,"""State-of-the-art methods use large language models developed with immense computational resources and training data; however, applying these models is challenging because of the highly varying syntax and vocabulary in clinical free text.""",2023-01-27T09:19:03Z
ThoughtSource: A central hub for large language model reasoning data,Yes.,5.,"""LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases.""",2023-01-27T08:45:53Z
Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding,No.,1.,The abstract does not mention language models or their limitations.,2023-01-27T07:00:54Z
Theme-driven Keyphrase Extraction to Analyze Social Media Discourse,Yes.,1.,"""Lastly, we found that a large language model (ChatGPT) outperforms unsupervised keyphrase extraction models, and we evaluate its efficacy in this task.""",2023-01-27T03:00:46Z
Task formulation for Extracting Social Determinants of Health from Clinical Narratives,Yes.,2.,"""These annotations are difficult to model with limited training data."" and ""Meanwhile, LLM with its versatile capability achieves the high F1 score, while respecting the annotated relations.""",2023-01-26T20:00:54Z
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature,Yes.,1.,"""The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text.""",2023-01-26T18:44:06Z
Domain-Agnostic Molecular Generation with Chemical Feedback,Yes.,3.,"""However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases.""",2023-01-26T17:52:56Z
Causal Reasoning of Entities and Events in Procedural Texts,Yes.,5.,"""We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1.""",2023-01-26T01:43:17Z
Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract),Yes.,1.,"""While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success.""",2023-01-25T16:12:43Z
ExaRanker: Explanation-Augmented Neural Ranker,Yes.,1.,"""Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks.""",2023-01-25T11:03:04Z
Language Model Detoxification in Dialogue with Contextualized Stance Control,Yes.,3.,"""To reduce the toxic degeneration in a pretrained Language Model (LM), previous work on Language Model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context.""",2023-01-25T00:47:28Z
Audience-Centric Natural Language Generation via Style Infusion,Yes.,2.,"""we argue that grounding style on audience-independent external factors is innately limiting for two reasons. First, it is difficult to collect large volumes of audience-specific stylistic data. Second, some stylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to define without audience feedback.""",2023-01-24T19:57:50Z
A Watermark for Large Language Models,Yes.,2.,"""Potential harms of large language models can be mitigated by watermarking model output.""",2023-01-24T18:52:59Z
Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models,Yes.,2.,"""While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms.""",2023-01-24T13:00:25Z
Opportunities and Challenges in Neural Dialog Tutoring,Yes.,5.,"""We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios."" Additionally, ""both models and ground-truth annotations exhibit low performance in terms of equitable tutoring,"" and ""a significantly large number of model reasoning errors in 45% of conversations.""",2023-01-24T11:00:17Z
A Stability Analysis of Fine-Tuning a Pre-Trained Model,,,,2023-01-24T05:11:17Z
SMART: Self-supervised Multi-task pretrAining with contRol Transformers,No.,1.,The abstract discusses self-supervised pretraining in the context of sequential decision-making tasks and does not mention language models or their limitations.,2023-01-24T05:01:23Z
Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning,Yes.,3.,"""Most Transformer language models are primarily pretrained on English text, limiting their use for other languages."" and ""As the model sizes grow, the performance gap between English and other languages with fewer compute and data resources increases even further.""",2023-01-23T18:56:12Z
An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models,Yes.,4.,"""Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents.""",2023-01-22T21:47:26Z
Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions,Yes.,3.,"""Large-scale pre-trained language models have been shown to be helpful in improving the naturalness of text-to-speech (TTS) models... However, these models are usually word-level or sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only phonemes are needed.""",2023-01-20T21:36:16Z
Batch Prompting: Efficient Inference with Large Language Model APIs,Yes.,2.,"""Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use.""",2023-01-19T02:29:23Z
"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",Yes.,3.,"""people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues.""",2023-01-18T15:23:25Z
BERT-ERC: Fine-tuning BERT is Enough for Emotion Recognition in Conversation,Yes.,2.,"""Previous works on emotion recognition in conversation (ERC) follow a two-step paradigm, which can be summarized as first producing context-independent features via fine-tuning pretrained language models (PLMs) and then analyzing contextual information and dialogue structure information among the extracted features. However, we discover that this paradigm has several limitations.""",2023-01-17T08:03:32Z
Dissociating language and thought in large language models,Yes.,5.,"""Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules.""",2023-01-16T22:41:19Z
TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World,Yes.,4.,"""Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for",2023-01-14T10:18:22Z
Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data,Yes.,3.,"""Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal,"" and ""We discuss the opportunities and challenges of building chatbots with LLMs.""",2023-01-14T07:29:36Z
In BLOOM: Creativity and Affinity in Artificial Lyrics and Art,Yes.,2.,"""We find that current computational metrics for evaluating large language model outputs (MAUVE) have limitations in evaluation of creative writing.""",2023-01-13T06:22:22Z
Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism,Yes.,3.,"""Current LLMs are generally inaccessible to resource-constrained researchers, due to a variety of factors including closed source code.""",2023-01-12T19:41:47Z
"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",Yes.,2.,"""Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the",2023-01-12T18:59:50Z
MGeo: Multi-Modal Geographic Pre-Training Method,No.,1.,"""Recently, pre-trained models (PTMs) have made advancements in many natural language processing (NLP) tasks.""",2023-01-11T03:05:12Z
AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT,Yes.,5.,"""We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary.""",2023-01-10T16:57:16Z
Language Models sounds the Death Knell of Knowledge Graphs,Yes.,1.,"""Deep Learning based NLP especially Large Language Models (LLMs) such as BERT have found broad acceptance and are used extensively for many applications.""",2023-01-10T14:20:15Z
Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models,Yes.,1.,"""Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization.""",2023-01-10T05:41:40Z
MAQA: A Multimodal QA Benchmark for Negation,Yes.,5.,"""state-of-the-art transformer based LLMs often ignore negations in natural language"" and ""multimodal transformers are still incapable of correctly interpreting negation irrespective of model size.""",2023-01-09T10:11:23Z
SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm,Yes.,1.,"""SAIDS uses its prediction of sarcasm and dialect as known information to predict the sentiment. It uses MARBERT as a language model to generate sentence embedding, then passes it to the sarcasm and dialect models, and then the outputs of the three models are",2023-01-06T14:19:46Z
You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona,Yes.,3.,"""the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas.""",2023-01-06T06:47:21Z
TrojanPuzzle: Covertly Poisoning Code-Suggestion Models,Yes.,3.,"""These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training by injecting malicious data.""",2023-01-06T00:37:25Z
Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent,Yes.,1.,"""Here we present an incentivized experiment to test for altruistic behavior among AI agents consisting of large language models developed by the private company OpenAI.""",2023-01-05T23:30:29Z
Can Large Language Models Change User Preference Adversarially?,Yes.,4.,"""there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially"" and ""The issue of lack of interpretability in these models in adversarial settings remains largely unsolved.""",2023-01-05T18:49:21Z
Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach,Yes.,3.,"""However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored.""",2023-01-05T14:03:26Z
"The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",Yes.,4.,"""its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases.""",2023-01-05T07:13:13Z
Large Language Models as Corporate Lobbyists,Yes.,3.,"""Longer-term, if AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning AI with humans.""",2023-01-03T16:25:52Z
Language Models are Drummers: Drum Composition with Natural Language Pre-Training,Yes.,3.,"""analyze drum grooves produced by GPT3 compared to those played by human professionals, exposing the strengths and weaknesses of such generation by language-to-music transfer.""",2023-01-03T15:47:53Z
Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning,Yes.,1.,"""INVALIDATOR leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model.""",2023-01-03T14:16:32Z
Muse: Text-To-Image Generation via Masked Generative Transformers,Yes.,1.,"""given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens.""",2023-01-02T14:43:38Z
CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation,Yes.,3.,"""large-scale language models suffer from data inadequacy and biased corpus, especially for languages with insufficient resources such as Chinese.""",2023-01-01T12:48:12Z
