Title,Talks about LLMs,Rate,Evidence,Published
Rethinking with Retrieval: Faithful Large Language Model Inference,,,,2022-12-31T22:35:34Z
A Survey on In-context Learning,Yes.,4.,"""We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research.""",2022-12-31T15:57:09Z
Inconsistencies in Masked Language Models,Yes.,5.,"""However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together."" and ""This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference.""",2022-12-30T22:53:25Z
Black-box language model explanation by context length probing,Yes.,1.,"""We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies.""",2022-12-30T16:24:10Z
Targeted Phishing Campaigns using Large Scale Language Models,Yes.,2.,"""Our evaluations show that NLMs are capable of generating phishing emails that are difficult to detect and that have a high success rate in tricking individuals, but their effectiveness varies based on the specific NLM and training data used.""",2022-12-30T03:18:05Z
Cramming: Training a Language Model on a Single GPU in One Day,Yes.,3.,"""training language models is out of reach for most researchers and practitioners"" and ""We investigate why scaling down is hard, and which modifications actually improve performance in this scenario.""",2022-12-28T18:59:28Z
Using Large Language Models to Generate Engaging Captions for Data Visualizations,Yes.,1.,"""In this work we explore the opportunities offered by the newly emerging crop of large language models (LLM)"" and ""We report on first experiments using the popular LLM GPT-3 and deliver some promising results.""",2022-12-27T23:56:57Z
TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence,Yes.,1.,"""Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific word embeddings learnt from the given corpus and the general-purpose word embeddings provided by a GPT-2 model pre-trained on massive text data is integrated into the decoder.""",2022-12-27T11:50:14Z
A Survey on Knowledge-Enhanced Pre-trained Language Models,Yes.,4.,"""PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks.""",2022-12-27T09:54:14Z
DeepCuts: Single-Shot Interpretability based Pruning for BERT,Yes.,3.,"""As language models have grown in parameters and layers, it has become much harder to train and infer with them on single GPUs. This is severely restricting the availability of large language models such as GPT-3, BERT-Large, and many others.""",2022-12-27T07:21:41Z
Measuring an artificial intelligence agent's trust in humans using machine incentives,Yes.,1.,"""We present a method for incentivizing machine decisions without altering an AI agent's underlying algorithms or goal orientation.""",2022-12-27T06:05:49Z
Large Language Models Encode Clinical Knowledge,Yes.,5.,"""human evaluation reveals key gaps in Flan-PaLM responses"" and ""Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.""",2022-12-26T14:28:24Z
Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,Yes.,3.,"""We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time."" and ""Finally, we collect error annotations from our participants and use them to show that certain textual genres influence models to make different types of errors and that certain sentence-level features correlate highly with annotator",2022-12-24T06:40:25Z
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5.,"""These results suggest that the propensity of larger Transformer-based models to 'memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.""",2022-12-23T03:57:54Z
Methodological reflections for AI alignment research using human feedback,Yes.,3.,"""AI alignment is particularly relevant for large language models (LLMs), which have the potential to exhibit unintended behavior due to their ability to learn and adapt in ways that are difficult to predict.""",2022-12-22T14:27:33Z
Multi-Lingual DALL-E Storytime,Yes.,3.,"""its decreased performance when given input in a different language, limits its audience and deepens the gap between populations,"" and ""An additional limitation of the current DALL-E model is that it only allows for the creation of a few images in response to a given input prompt, rather than a series of consecutive coherent frames that tell a story or describe a process that changes over time.""",2022-12-22T07:06:35Z
CAMeMBERT: Cascading Assistant-Mediated Multilingual BERT,Yes.,3.,"""Their widespread use and adoption, however, is hindered by the lack of availability and portability of sufficiently large computational resources.""",2022-12-22T02:19:25Z
What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,Yes.,2.,"""Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.""",2022-12-21T19:11:19Z
Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges,Yes.,1.,"""This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges.""",2022-12-21T17:41:16Z
Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,Yes.,1.,"""Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism.""",2022-12-21T16:56:07Z
Parallel Context Windows for Large Language Models,Yes.,5.,"""When applied to processing long text, Large Language Models (LLMs) are limited by their context window.""",2022-12-21T11:38:51Z
Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners,Yes.,5.,"""However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model.""",2022-12-21T09:37:05Z
From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models,Yes.,4.,"""However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task.""",2022-12-21T08:39:36Z
ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models,Yes.,4.,"""Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting."" and ""We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions",2022-12-21T07:06:55Z
KL Regularized Normalization Framework for Low Resource Tasks,Yes.,2.,"""the success of normalization in low resource downstream NLP and speech tasks is limited. One of the reasons is the inability to capture expressiveness by rescaling parameters of normalization.""",2022-12-21T05:59:25Z
"Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",Yes.,5.,"""Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking.""",2022-12-21T04:43:19Z
CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding,Yes.,3.,"""while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have.""",2022-12-21T04:21:35Z
Analyzing Semantic Faithfulness of Language Models via Input Intervention on Question Answering,Yes.,5.,"""While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a significant number of cases (~50% for deletion intervention, and ~20% drop in accuracy for negation intervention)."" and ""But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models'",2022-12-21T00:00:01Z
Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,Yes.,4.,"""Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics."" and ""existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain",2022-12-20T22:41:24Z
Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions,Yes.,4.,"""Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs."" and ""Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.""",2022-12-20T18:59:23Z
Self-Instruct: Aligning Language Models with Self-Generated Instructions,Yes.,3.,"""Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.""",2022-12-20T18:59:19Z
DISCO: Distilling Counterfactuals with Large Language Models,Yes.,2.,"""DISCO engineers prompts to generate phrasal perturbations with a large general language model."" and ""high-quality counterfactual data is scarce for most tasks and not easily generated at scale.""",2022-12-20T18:46:08Z
Evaluating Psychological Safety of Large Language Models,Yes.,3.,"""Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.""",2022-12-20T18:45:07Z
When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,Yes.,5.,"""large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge."" and ""We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail.""",2022-12-20T18:30:15Z
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,,,,2022-12-20T18:26:34Z
DePlot: One-shot visual language reasoning by plot-to-table translation,Yes.,1.,"""The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.""",2022-12-20T18:20:50Z
Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?,Yes.,3.,"""In WebTOD, the dialogue system learns how to understand the web/mobile interface that the human agent interacts with, powered by a large-scale language model.""",2022-12-20T18:18:41Z
ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models,,,,2022-12-20T17:49:49Z
Generic Temporal Reasoning with Differential Analysis and Explanation,Yes.,5.,"""We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions.""",2022-12-20T17:40:03Z
Is GPT-3 a Good Data Annotator?,Yes.,1.,"""GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks.""",2022-12-20T17:28:41Z
Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters,Yes.,3.,"""However, such transferred models suffer from mismatches in the languages of the input text during training and inference."" and ""However, we found that the prior suggestion of replacing the language adapters to match the target language at inference time is suboptimal for dense retrieval models.""",2022-12-20T17:25:04Z
Perplexed by Quality: A Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data,Yes.,1.,"""As demand for large corpora increases with the size of current state-of-the-art language models, using web data as the main part of the pre-training corpus for these models has become a ubiquitous practice.""",2022-12-20T17:14:45Z
Towards Reasoning in Large Language Models: A Survey,,,,2022-12-20T16:29:03Z
Data Curation Alone Can Stabilize In-context Learning,Yes.,3.,"""ICL is very sensitive to the choice of training examples",2022-12-20T15:58:54Z
True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,Yes.,5.,"""GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area.""",2022-12-20T09:34:43Z
Do language models have coherent mental models of everyday things?,Yes.,5.,"""Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent 'parts mental models' (54-59% accurate, 19-43% conditional constraint violation).""",2022-12-20T06:54:04Z
On the Blind Spots of Model-Based Evaluation Metrics for Text Generation,Yes.,3.,"""We examine a range of recently proposed evaluation metrics based on pretrained language models,"" and ""Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics.""",2022-12-20T06:24:25Z
Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,Yes.,3.,"""Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance.""",2022-12-20T05:20:54Z
On Improving Summarization Factual Consistency from Natural Language Feedback,Yes.,3.,"""We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.""",2022-12-20T02:47:37Z
Python Code Generation by Asking Clarification Questions,Yes.,3.,"""While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified.""",2022-12-19T22:08:36Z
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,Yes.,3.,"""performance drops significantly when no demonstrations are available.""",2022-12-19T21:34:26Z
The case for 4-bit precision: k-bit Inference Scaling Laws,Yes.,2.,"""We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float).""",2022-12-19T18:48:33Z
Multilingual Sequence-to-Sequence Models for Hebrew NLP,Yes.,3.,"""current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages"" and ""previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature",2022-12-19T18:10:23Z
Visconde: Multi-document QA with GPT-3 and Neural Reranking,Yes.,1.,"""The system, called Visconde, uses a three-step pipeline to perform the task",2022-12-19T17:39:07Z
Explanation Regeneration via Information Bottleneck,Yes.,3.,"""explanation generated through single-pass prompting often lacks sufficiency and conciseness.""",2022-12-19T16:41:19Z
Large Language Models are Better Reasoners with Self-Verification,Yes.,3.,"""However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation.""",2022-12-19T15:51:52Z
Improving the Generalizability of Text-Based Emotion Detection by Leveraging Transformers with Psycholinguistic Features,Yes.,2.,"""Yet, deployment of such models in real-world sentiment and emotion applications faces challenges, in particular poor out-of-domain generalizability.""",2022-12-19T13:58:48Z
ChatGPT: The End of Online Exam Integrity?,Yes.,3.,"""This capacity raises concerns about the potential use of ChatGPT as a tool for academic misconduct in online exams"" and ""Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools.""",2022-12-19T08:15:16Z
Very Large Language Model as a Unified Methodology of Text Mining,Yes.,3.,"""Finally I discuss the challenges in the design and development of VLLM techniques for text mining.""",2022-12-19T06:52:13Z
TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,No.,1.,The abstract does not mention LLMs or any specific limitations related to them.,2022-12-19T05:55:58Z
Discovering Language Model Behaviors with Model-Written Evaluations,Yes.,5.,"""We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes",2022-12-19T05:13:52Z
Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Yes.,5.,"""Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse.""",2022-12-18T19:27:41Z
"Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",Yes.,3.,"""It brings deeper interaction between mention and types to reach better performance but has to perform N (type set size) forward passes to infer types of a single mention. CE is therefore very slow in inference when the type set is large (e.g., N = 10",2022-12-18T16:42:52Z
Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search,Yes.,1.,"""Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation.""",2022-12-18T05:26:40Z
Language model acceptability judgements are not always robust to context,Yes.,4.,"""we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context"" and ""we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures.""",2022-12-18T00:11:06Z
Graph Learning and Its Advancements on Large Language Models: A Holistic Survey,,,,2022-12-17T22:05:07Z
HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,Yes.,3.,"""However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse.""",2022-12-17T11:56:21Z
Neural Story Planning,Yes.,3.,"""pre-trained neural language models can generate stories with great diversity, while being generally incapable of ending a story in a specified manner and can have trouble maintaining coherence.""",2022-12-16T21:29:41Z
Plansformer: Generating Symbolic Plans using Transformers,Yes.,1.,"""Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation.""",2022-12-16T19:06:49Z
MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation,Yes.,5.,"""these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency.""",2022-12-16T17:36:23Z
Teaching Small Language Models to Reason,Yes.,4.,"""However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters.""",2022-12-16T11:24:42Z
LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,Yes.,3.,"""Many popular language models, such as BERT or RoBERTa, are general-purpose models, which have limitations on processing specialized legal terminology and syntax.""",2022-12-16T00:15:14Z
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,Yes.,2.,"""the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model.""",2022-12-15T21:35:46Z
"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Yes.,5.,"""We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output"" and ""Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.""",2022-12-15T18:59:32Z
Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models,Yes.,2.,"""Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).""",2022-12-15T18:45:29Z
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,Yes.,3.,"""our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.""",2022-12-15T17:26:05Z
Visually-augmented pretrained language models for NLP tasks without images,Yes.,3.,"""they are found lack of visual semantics or commonsense.""",2022-12-15T16:13:25Z
DeepJoin: Joinable Table Discovery with Pre-trained Language Models,Yes.,1.,"""Our solution is an embedding-based retrieval, which employs a pre-trained language model (PLM) and is designed as one framework serving both equi- and semantic joins.""",2022-12-15T02:40:57Z
ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages,Yes.,1.,"""In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs).""",2022-12-13T17:21:44Z
Benchmarking Large Language Models for Automated Verilog RTL Code Generation,Yes.,1.,"""In this paper, we characterize the ability of LLMs to generate useful Verilog.""",2022-12-13T16:34:39Z
On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning,No.,1.,The abstract does not mention language models (LLMs or LMs) at all.,2022-12-13T13:38:04Z
"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",Yes.,5.,"""Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to 'explain their reasoning' often leads",2022-12-13T00:29:45Z
Evaluation of Synthetic Datasets for Conversational Recommender Systems,Yes.,3.,"""The efficiency brought about by LLMs in the data generation phase is impeded during the process of evaluation of the generated data, since it generally requires human-raters to ensure that the data generated is of high quality and has sufficient diversity.""",2022-12-12T18:53:10Z
Prompting Is Programming: A Query Language for Large Language Models,Yes.,2.,"""However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.""",2022-12-12T18:09:09Z
A Study of Slang Representation Methods,Yes.,3.,"""Despite the success of large language models and the spontaneous emergence of slang dictionaries, it is unclear how far their combination goes in terms of slang understanding for downstream social good tasks."" and ""Our error analysis identifies core challenges for slang representation learning, including out-of-vocabulary words, polysemy, variance, and annotation disagreements, which can",2022-12-11T21:56:44Z
Elixir: Train a Large Language Model on a Small GPU Cluster,Yes.,2.,"""However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs.""",2022-12-10T17:26:05Z
Structured information extraction from complex scientific text with fine-tuned large language models,Yes.,1.,"""The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs).""",2022-12-10T07:51:52Z
"The Role of AI in Drug Discovery: Challenges, Opportunities, and Strategies",No.,1.,The abstract does not mention LLMs or any specific type of language models. It focuses on AI in drug discovery in general.,2022-12-08T23:23:39Z
Learning Video Representations from Large Language Models,Yes.,1.,"""We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs).""",2022-12-08T18:59:59Z
Learning Domain Invariant Prompt for Vision-Language Models,Yes.,3.,"""However, although prompt learning achieves excellent performance over in-domain data, it still faces the major challenge of generalizing to unseen classes and domains.""",2022-12-08T11:23:24Z
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,Yes.,3.,"""The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly.""",2022-12-08T05:46:32Z
Talking About Large Language Models,,,,2022-12-07T10:01:44Z
A Generative Approach for Script Event Prediction via Contrastive Fine-tuning,Yes.,3.,"""First, the pretrained language models adopted by current works ignore event-level knowledge, resulting in an inability to capture the correlations between events well.""",2022-12-07T07:32:47Z
CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain,Yes.,3.,"""However, due to the domain-knowledge and many technical terms in cybersecurity general language models might miss the gist of textual information, hence doing more harm than good.""",2022-12-06T13:49:12Z
LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training,Yes.,3.,"""current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors.""",2022-12-06T01:31:37Z
Legal Prompt Engineering for Multilingual Legal Judgement Prediction,Yes.,3.,"""Our results show that zero-shot LPE is better compared to the baselines, but it still falls short compared to current state of the art supervised approaches.""",2022-12-05T12:17:02Z
Human-in-the-Loop Hate Speech Classification in a Multilingual Context,No.,1.,The abstract does not mention language models (LMs or LLMs) explicitly. It focuses on a BERT-based hate speech classification pipeline.,2022-12-05T09:05:40Z
Understanding How Model Size Affects Few-shot Instruction Prompting,Yes.,5.,"""Large Language Models are affected by the phenomena of memorizing and forgetting their training data."" and ""We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes.""",2022-12-04T19:59:52Z
Event knowledge in large language models: the gap between the impossible and the unlikely,Yes.,4.,"""However, LLMs show less consistent preferences for likely vs. unlikely events"" and ""highlight a gap between representations of possible/impossible and likely/unlikely events.""",2022-12-02T23:43:18Z
Nonparametric Masked Language Modeling,Yes.,3.,"""Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases.""",2022-12-02T18:10:42Z
a survey on GPT-3,Yes.,4.,"""We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers.""",2022-12-01T20:24:19Z
Language Model Pre-training on True Negatives,Yes.,3.,"""Existing PLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on pseudo-negative data and leads to less efficiency and less robustness in the resulting PLMs.""",2022-12-01T12:24:19Z
Distilling Reasoning Capabilities into Smaller Language Models,Yes.,3.,"""However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.""",2022-12-01T00:39:56Z
BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?,Yes.,3.,"""Many state-of-the-art Language Models (LMs), however, do not scale well above the threshold of 512 input tokens."" and ""since the pretraining process is extremely costly in general - but even more so as the sequence length increases.""",2022-11-30T16:09:20Z
Quadapter: Adapter for GPT-2 Quantization,Yes.,3.,"""Transformer language models such as GPT-2 are difficult to quantize because of outliers in activations leading to a large quantization error.""",2022-11-30T11:20:33Z
Explicit Knowledge Transfer for Weakly-Supervised Code Generation,Yes.,2.,"""In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain.""",2022-11-30T04:51:26Z
Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models,Yes.,3.,"""existing models are often overly confident on unseen classes.""",2022-11-28T19:03:35Z
Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation,Yes.,1.,"""We introduce Action-GPT, a plug-and-play framework for incorporating Large Language Models (LLMs) into text-based action generation models.""",2022-11-28T17:57:48Z
Automatically Extracting Information in Medical Dialogue: Expert System And Attention for Labelling,No.,1.,The abstract discusses attention-based models and a novel model called ESAL but does not mention LLMs or their limitations.,2022-11-28T16:49:13Z
Scientific and Creative Analogies in Pretrained Language Models,Yes.,5.,"""We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.""",2022-11-28T12:49:44Z
Fine-tuning language models to find agreement among humans with diverse preferences,Yes.,2.,"""Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user... This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a single 'generic' user will confer more general alignment.""",2022-11-28T02:24:14Z
Understanding BLOOM: An empirical study on diverse NLP tasks,Yes.,3.,"""BLOOM performance does not scale with parameter size, unlike other LLMs like GPT and BERT"" and ""Zero-shot cross-lingual and multi-lingual fine-tuning experiments show that BLOOM is at par or worse than monolingual GPT-2 models.""",2022-11-27T15:48:14Z
An Analysis of Social Biases Present in BERT Variants Across Multiple Languages,Yes.,4.,"""Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings.""",2022-11-25T23:38:08Z
GPT-3-driven pedagogical agents for training children's curious question-asking skills,Yes.,1.,"""we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training.""",2022-11-25T16:41:59Z
PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices,Yes.,1.,"""Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters.""",2022-11-25T14:16:35Z
Complementary Explanations for Effective In-Context Learning,Yes.,1.,"""Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective.""",2022-11-25T04:40:47Z
SEAT: Stable and Explainable Attention,No.,1.,The abstract does not mention LLMs or their limitations. It focuses on the stability and explainability of the attention mechanism in general NLP models.,2022-11-23T20:33:30Z
Automatic Generation of Socratic Subquestions for Teaching Math Word Problems,Yes.,1.,"""We explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving.""",2022-11-23T10:40:22Z
OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type,No.,1.,The abstract does not mention language models (LLMs) or their limitations.,2022-11-22T10:42:07Z
Validating Large Language Models with ReLM,Yes.,5.,"""there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language.""",2022-11-21T21:40:35Z
Deanthropomorphising NLP: Can a Language Model Be Conscious?,Yes.,2.,"""This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models."" and ""Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task.""",2022-11-21T14:18:25Z
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text,Yes.,2.,"""propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations.""",2022-11-21T09:41:25Z
Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task,Yes.,3.,"""We analyse the capabilities and limitations of our model to better understand the potential of language-music models.""",2022-11-21T07:19:17Z
L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi,Yes.,2.,"""Sentence representation from vanilla BERT models does not work well on sentence similarity tasks.""",2022-11-21T05:15:48Z
You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model,Yes.,3.,"""The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing.""",2022-11-21T02:32:25Z
Conceptor-Aided Debiasing of Large Language Models,Yes.,5.,"""Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus."" and ""CI-BERT reduces the language model accuracy.""",2022-11-20T21:24:48Z
The Stack: 3 TB of permissively licensed source code,Yes.,2.,"""We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets.""",2022-11-20T18:15:30Z
Leveraging per Image-Token Consistency for Vision-Language Pre-training,Yes.,3.,"""However, we find that CMLM is insufficient for this purpose according to our observations",2022-11-20T12:10:53Z
SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,Yes.,3.,"""Large language models (LLMs) show excellent performance but are compute- and memory-intensive.""",2022-11-18T18:59:33Z
PAL: Program-aided Language Models,Yes.,5.,"""While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly.""",2022-11-18T18:56:13Z
CAPE: Corrective Actions from Precondition Errors using Large Language Models,Yes.,3.,"""Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.""",2022-11-17T23:14:51Z
Ignore Previous Prompt: Attack Techniques For Language Models,,,,2022-11-17T13:43:20Z
Prompting PaLM for Translation: Assessing Strategies and Performance,Yes.,3.,"""find that its performance, while impressive, still lags that of state-of-the-art supervised systems.""",2022-11-16T18:42:37Z
Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT,Yes.,2.,"""However, the large number of parameters and computations also pose challenges for their deployment.""",2022-11-16T11:43:09Z
Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,Yes.,3.,"""few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large scale manual rationale annotation.""",2022-11-15T19:36:06Z
PromptCap: Prompt-Guided Task-Aware Image Captioning,Yes.,3.,"""However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly.""",2022-11-15T19:07:53Z
Evaluating the Factual Consistency of Large Language Models Through News Summarization,Yes.,5.,"""While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information."" and ""However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries.""",2022-11-15T18:50:34Z
Large Language Models Struggle to Learn Long-Tail Knowledge,Yes.,5.,"""However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely."" and ""while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data.""",2022-11-15T18:49:27Z
Introducing Semantics into Speech Encoders,Yes.,1.,"""Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM.""",2022-11-15T18:44:28Z
FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,Yes.,1.,"""Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph.""",2022-11-15T17:20:40Z
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective,Yes.,5.,"""the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods"" and ""significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.""",2022-11-15T11:53:55Z
Teaching Algorithmic Reasoning via In-context Learning,Yes.,3.,"""Despite this progress, LLMs are still unable to solve algorithmic reasoning problems."" and ""Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved.""",2022-11-15T06:12:28Z
UGIF: UI Grounded Instruction Following,Yes.,4.,"""We compare the performance of different LLMs including PaLM and GPT-3 and find that the end-to-end task completion rate is 48% for English UI but the performance drops to 32% for other languages. We analyze the common failure modes of existing models on this task and point out areas for improvement.""",2022-11-14T18:36:19Z
Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,Yes.,3.,"""for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements.""",2022-11-14T16:46:14Z
Does Debiasing Inevitably Degrade the Model Performance,Yes.,4.,"""Gender bias in language models has attracted sufficient attention because it threatens social justice. However, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious.""",2022-11-14T13:46:13Z
Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction,No.,1.,"The abstract does not mention LLMs, LLM limitations, or any specific language models. It focuses on a new evaluation metric for classifiers in general, comparing a language model classifier to XGBoost.",2022-11-13T23:59:11Z
GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost,,,,2022-11-13T18:59:15Z
Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT RoBERTa Approach for Patronizing and Condescending Language Detection,Yes.,1.,"""The experiments compare pre-BERT neural network (NN) based systems against post-BERT pretrained language model RoBERTa.""",2022-11-13T10:59:45Z
Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters,Yes.,3.,"""aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations.""",2022-11-13T10:16:39Z
Textual Data Augmentation for Patient Outcomes Prediction,Yes.,1.,"""we fine-tune the generative language model GPT-2 to synthesize labeled text with the original training data.""",2022-11-13T01:07:23Z
DocuT5: Seq2seq SQL Generation with Table Documentation,Yes.,4.,"""Current SQL generators based on pre-trained language models struggle to answer complex questions requiring domain context or understanding fine-grained table structure.""",2022-11-11T13:31:55Z
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation,Yes.,1.,"""We evaluate fine-tuning and prompt-engineering techniques with pre-trained language models of the BERT family and the generative large language models of the GPT family using persuasive strategies as an additional source of information.""",2022-11-11T03:26:37Z
Measuring Reliability of Large Language Models through Semantic Consistency,Yes.,3.,"""recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers.""",2022-11-10T20:21:07Z
The CRINGE Loss: Learning what language not to model,Yes.,3.,"""Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data.""",2022-11-10T19:30:08Z
Syntax-Guided Domain Adaptation for Aspect-based Sentiment Analysis,No.,1.,"The abstract focuses on aspect-based sentiment analysis (ABSA) and domain adaptation, and does not mention language models (LLMs or LLMs).",2022-11-10T10:09:33Z
EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Yes.,5.,"""Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world.""",2022-11-10T07:48:01Z
Large Language Models with Controllable Working Memory,Yes.,5.,"""We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size.""",2022-11-09T18:58:29Z
Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,Yes.,5.,"""In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags >20% behind, highlighting a notable limitation in existing approaches' ToM capabilities.""",2022-11-09T05:06:12Z
Zero-Label Prompt Selection,Yes.,3.,"""the cross-task performance is highly sensitive to the choice of prompts, while selecting a high-performing prompt is challenging given the scarcity of labels.""",2022-11-09T04:13:31Z
Active Example Selection for In-Context Learning,Yes.,3.,"""We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information.""",2022-11-08T19:00:02Z
Active Learning with Tabular Language Models,Yes.,2.,"""real-world applications are still challenging"" and ""open fundamental questions concerning computational efficiency and the perspective of human annotators.""",2022-11-08T09:50:30Z
Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,Yes.,3.,"""SPLADE still struggles with exact matching of low-frequency words in training data. In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE.""",2022-11-08T03:58:26Z
Retrieval augmentation of large language models for lay language generation,Yes.,3.,"""However, the applicability of these models is constrained by the limited size and topical breadth of available corpora."" and ""Such explanation is challenging for neural models to generate because it goes beyond simplification by adding content absent from the source.""",2022-11-07T19:06:53Z
Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach,Yes.,1.,"""We distill several insights from our analysis on study the peer review process with the help of large LMs.""",2022-11-07T16:19:42Z
AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,Yes.,3.,"""pre-training these large multilingual language models requires a lot of training data, which is not available for African Languages.""",2022-11-07T02:15:25Z
Noisy Channel for Automatic Text Simplification,No.,1.,The abstract does not mention LLMs or any specific limitations of language models.,2022-11-06T15:28:42Z
Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust,Yes.,3.,"""we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination.""",2022-11-06T07:03:31Z
MolE: a molecular foundation model for drug discovery,Yes.,1.,"""Recently, large language models have addressed this problem by using self-supervised pretraining on large unlabeled datasets, followed by fine-tuning on smaller, labeled datasets.""",2022-11-03T21:22:05Z
Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Yes.,5.,"""Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning.""",2022-11-03T18:53:30Z
LMentry: A Language Model Benchmark of Elementary Language Tasks,Yes.,5.,"""Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI's latest 175B-parameter instruction-tuned model, TextDavinci002.""",2022-11-03T18:01:12Z
"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",Yes.,2.,"""We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.""",2022-11-03T17:13:48Z
Large Language Models Are Human-Level Prompt Engineers,,,,2022-11-03T15:43:03Z
Contextual information integration for stance detection via cross-attention,Yes.,2.,"""Most existing stance detection models are limited because they do not consider relevant contextual information which allows for inferring the stance correctly.""",2022-11-03T15:04:29Z
Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer,Yes.,5.,"""They impose, however, limits on the maximum input length (512 sub-words in BERT), which are too restrictive in the legal domain. Even sparse-attention models, such as Longformer and BigBird, which increase the maximum input length to 4,096 sub-",2022-11-02T09:27:01Z
BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder,Yes.,3.,"""One crucial factor that makes this integration challenging lies in the vocabulary mismatch; the vocabulary constructed for a pre-trained LM is generally too large for E2E-ASR training and is likely to have a mismatch against a target ASR domain.""",2022-11-02T00:10:43Z
Two-stage LLM Fine-tuning with Less Specialization and More Generalization,Yes.,5.,"""fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances,"" and ""format specialization, where the model overfits to the format of the fine-tuned task.""",2022-11-01T17:56:57Z
The future is different: Large pre-trained language models fail in prediction tasks,Yes.,5.,"""Yet, it is known that their performance can drastically drop when there is a distribution shift between the data used during training and that used at inference time."" and ""we empirically demonstrate that LPLM can display average performance drops of about 88% (in the best case!) when predicting the",2022-11-01T11:01:36Z
Generating Sequences by Learning to Self-Correct,Yes.,3.,"""Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs.""",2022-10-31T18:09:51Z
Query Refinement Prompts for Closed-Book Long-Form Question Answering,,,,2022-10-31T17:44:42Z
Emergent Linguistic Structures in Neural Networks are Fragile,Yes.,5.,"""Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structures."" and ""Our key observation is that emergent syntactic representations in neural networks are brittle.""",2022-10-31T15:43:57Z
"A Simple, Yet Effective Approach to Finding Biases in Code Generation",Yes.,4.,"""This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.""",2022-10-31T15:06:15Z
GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers,Yes.,3.,"""due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models.""",2022-10-31T13:42:40Z
Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task,Yes.,1.,"""this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model.""",2022-10-31T11:49:49Z
Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,Yes.,5.,"""Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data.""",2022-10-31T08:12:41Z
QuaLA-MiniLM: a Quantized Length Adaptive MiniLM,Yes.,3.,"""Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized,"" and ""the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering.""",2022-10-31T07:42:52Z
A Solvable Model of Neural Scaling Laws,Yes.,3.,"""whether such scaling laws can break down and how they behave when they do.""",2022-10-30T15:13:18Z
Solving Math Word Problems via Cooperative Reasoning induced Language Models,Yes.,3.,"""However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans.""",2022-10-28T16:47:03Z
Probing for targeted syntactic knowledge through grammatical error detection,Yes.,3.,"""we also observe a divergence in performance when probes are trained on different training sets, and when they are evaluated on different syntactic constructions, suggesting the information pertaining to SVA error detection is not robustly encoded.""",2022-10-28T16:01:25Z
QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation,Yes.,2.,"""Search queries though pose a unique challenge, given their short-length and lack of nuance or context."" and ""While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy).""",2022-10-27T18:44:58Z
What Language Model to Train if You Have One Million GPU Hours?,Yes.,3.,"""large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone.""",2022-10-27T13:43:27Z
Truncation Sampling as Language Model Desmoothing,Yes.,3.,"""Long samples of text from neural language models can be of poor quality.""",2022-10-27T05:52:35Z
Contrastive Decoding: Open-ended Text Generation as Optimization,Yes.,5.,"""maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text"" and ""sampling can often produce incoherent text that drifts from the original topics.""",2022-10-27T00:58:21Z
Privately Fine-Tuning Large Language Models with Differential Privacy,Yes.,4.,"""However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs.""",2022-10-26T21:18:31Z
Learning on Large-scale Text-attributed Graphs via Variational Inference,Yes.,3.,"""the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together.""",2022-10-26T13:40:57Z
Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis,Yes.,1.,"""The invention of transformer-based models such as BERT, GPT, and RoBERTa has enabled researchers and financial companies to finetune these powerful models and use them in different downstream tasks to achieve state-of-the-art performance.""",2022-10-26T01:13:41Z
"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",Yes.,3.,"""zero-shot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints.""",2022-10-25T21:39:36Z
MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction,Yes.,1.,"""New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM).""",2022-10-25T12:08:14Z
Linguistic-Enhanced Transformer with CTC Embedding for Speech Recognition,No.,1.,The abstract focuses on automatic speech recognition (ASR) and does not mention language models (LLMs or LMs).,2022-10-25T08:12:59Z
Parameter-Efficient Legal Domain Adaptation,Yes.,3.,"""State-of-the-art language models are growing increasingly large, making parameter-efficient learning increasingly important. Unfortunately, parameter-efficient methods perform poorly with small amounts of data, which are common in the legal domain (where data labelling costs are high).""",2022-10-25T02:14:15Z
Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,Yes.,4.,"""We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally.""",2022-10-25T01:46:00Z
Speeding Up Question Answering Task of Language Models via Inverted Index,Yes.,4.,"""Despite the wide popularity of large language models (LLMs), few real-world conversational agents take advantage of LLMs. Extensive resources consumed by LLMs disable developers from integrating them into end-user applications.""",2022-10-24T19:59:17Z
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Yes.,5.,"""We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box,"" and ""Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively,"" and """,2022-10-24T14:58:58Z
Proficiency assessment of L2 spoken English using wav2vec 2.0,No.,1.,There is no mention of LLMs or their limitations in the abstract or title.,2022-10-24T12:36:49Z
Do Language Models Understand Measurements?,Yes.,5.,"""In this study, we show that PLMs lack the capability required for reasoning over measurements.""",2022-10-23T10:52:52Z
Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning,Yes.,3.,"""With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning.""",2022-10-23T01:33:16Z
Leveraging Large Language Models for Multiple Choice Question Answering,Yes.,3.,"""While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA).""",2022-10-22T05:04:54Z
Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,Yes.,3.,"""However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., 'an orange is orange'.""",2022-10-21T21:33:10Z
WikiWhy: Answering and Explaining Cause-and-Effect Questions,Yes.,4.,"""GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.""",2022-10-21T17:59:03Z
A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Yes.,5.,"""the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.""",2022-10-21T15:12:37Z
LittleBird: Efficient Faster & Longer Transformer for Question Answering,Yes.,5.,"""BERT has shown a lot of success in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism.""",2022-10-21T10:46:41Z
Using Large Language Models to Enhance Programming Error Messages,Yes.,2.,"""We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.""",2022-10-20T23:17:26Z
Large Language Models Can Self-Improve,Yes.,2.,"""However, fine-tuning an LLM requires extensive supervision.""",2022-10-20T21:53:54Z
ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications,Yes.,1.,"""We introduce ObSynth, an interactive system leveraging the domain knowledge embedded in large language models (LLMs) to help users design object models from high level natural language prompts.""",2022-10-20T17:59:19Z
Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing,Yes.,5.,"""I will argue that these models are not suitable as neural models of human language. Firstly, because they fail on fundamental boundary conditions, such as the amount of learning they require. This would in fact imply that the mechanisms of GPT and brain language processing are fundamentally different. Secondly, because they do not possess the logistics of access needed for compositional and productive human language processing.""",2022-10-19T13:31:26Z
Language Detoxification with Attribute-Discriminative Latent Space,Yes.,3.,"""Transformer-based Language Models (LMs) ... can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications."" and ""previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application.""",2022-10-19T06:54:42Z
SafeText: A Benchmark for Exploring Physical Safety in Language Models,Yes.,5.,"""We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice.""",2022-10-18T17:59:31Z
ROSE: Robust Selective Fine-tuning for Pre-trained Language Models,Yes.,3.,"""Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.""",2022-10-18T07:53:15Z
Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models,Yes.,3.,"""Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples.""",2022-10-18T02:44:38Z
Adversarial and Safely Scaled Question Generation,Yes.,2.,"""However, there are critical risks of misinformation with these approaches.""",2022-10-17T22:51:45Z
CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model,Yes.,1.,"""Inspired by the outstanding performance of bidirectional encoder representations from transformers (BERT) for improving many natural language processing tasks, we propose in this paper ``CAN-BERT"", a deep learning based network intrusion detection system, to detect cyber attacks on CAN bus protocol.""",2022-10-17T21:21:37Z
Deep Bidirectional Language-Knowledge Graph Pretraining,,,,2022-10-17T18:02:52Z
Prompting GPT-3 To Be Reliable,Yes.,4.,"""the crucial problem of how to improve the reliability of GPT-3 is still under-explored"" and ""we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important",2022-10-17T14:52:39Z
Exposing Influence Campaigns in the Age of LLMs: A Behavioral-Based AI Approach to Detecting State-Sponsored Trolls,Yes.,2.,"""textual and linguistic properties can be easily mimicked by Large Language Models (LLMs)"" and ""ensuring greater resilience in identifying influence campaigns, especially given the potential increase in the usage of LLMs for generating inauthentic content.""",2022-10-17T07:01:17Z
"RARR: Researching and Revising What Language Models Say, Using Language Models",Yes.,5.,"""However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence.""",2022-10-17T03:44:30Z
A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems,Yes.,3.,"""First, it is unclear whether we can leverage pretrained language models to design, for example, GPT-2 based USs, to catch up and interact with the recently advanced GPT-2 based DSs."" and ""how to flexibly integrate goal state tracking and develop an end-to-end trainable US for multi-domains has remained to be a challenge.""",2022-10-17T01:57:50Z
Temporal Word Meaning Disambiguation using TimeLMs,,,,2022-10-15T06:34:59Z
MiQA: A Benchmark for Inference on Metaphorical Questions,Yes.,3.,"""We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.""",2022-10-14T17:46:05Z
Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,Yes.,4.,"""Several studies have explored these harms and called for their mitigation via development of safer, fairer models."" and ""this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models.""",2022-10-14T10:43:39Z
BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation,Yes.,4.,"""it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern on the fairness of PLMs as metrics"" and ""We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely",2022-10-14T08:24:11Z
DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,Yes.,3.,"""With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry."" and ""While LoRA blocks are parameter-efficient, they suffer from two major problems",2022-10-14T06:29:22Z
"""John is 50 years old, can his son be 65?"" Evaluating NLP Models' Understanding of Feasibility",Yes.,5.,"""Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities."" and ""We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly.""",2022-10-14T02:46:06Z
Bootstrapping Multilingual Semantic Parsers using Large Language Models,Yes.,3.,"""translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models.""",2022-10-13T19:34:14Z
SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,Yes.,4.,"""A common limitation of diagnostic tests for detecting social biases in NLP models"" and ""we are able to uncover the model's stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations",2022-10-13T18:04:48Z
Mass-Editing Memory in a Transformer,Yes.,3.,"""this line of work is predominantly limited to updating single associations.""",2022-10-13T17:55:53Z
Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods,Yes.,2.,"""Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human ratings, but they are not faithfully capturing numeric information and are inconsistent in their interpretation of the task.""",2022-10-13T17:48:15Z
CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing,Yes.,3.,"""Large Language Models (LLMs) excel at SP given only a few examples, however LLMs are unsuitable for runtime systems which require low latency.""",2022-10-13T15:01:03Z
Spontaneous Emerging Preference in Two-tower Language Model,Yes.,2.,"""With the existence of side-effects brought about by the large size of the foundation language model such as deployment cost, availability issues, and environmental cost, there is some interest in exploring other possible directions, such as a divide-and-conquer scheme.""",2022-10-13T13:55:19Z
Sample-Then-Optimize Batch Neural Thompson Sampling,No.,1.,"The abstract discusses Bayesian optimization, Gaussian processes, neural networks, and Thompson sampling but does not mention language models or their limitations.",2022-10-13T09:01:58Z
Assessing Out-of-Domain Language Model Performance from Few Examples,Yes.,5.,"""While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts.""",2022-10-13T04:45:26Z
Language Models are Realistic Tabular Data Generators,Yes.,1.,"""less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature.""",2022-10-12T15:03:28Z
A context-aware knowledge transferring strategy for CTC-based ASR,Yes.,2.,"""The former is designed to distill linguistic information from a pre-trained language model, and the latter is framed to modulate the limitations caused by the conditional independence assumption.""",2022-10-12T14:31:38Z
AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning,Yes.,3.,"""Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available.""",2022-10-12T02:54:41Z
SEAL : Interactive Tool for Systematic Error Analysis and Labeling,Yes.,4.,"""However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation.""",2022-10-11T23:51:44Z
Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,Yes.,4.,"""Here we show another problem with multilingual models",2022-10-11T17:06:38Z
A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models,Yes.,5.,"""Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges",2022-10-11T07:26:34Z
Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval,Yes.,1.,"""The case-view contrastive learning minimizes the hidden space distance between relevant legal case representations produced by a pre-trained language model (PLM) encoder.""",2022-10-11T06:47:23Z
Generating Executable Action Plans with Environmentally-Aware Language Models,Yes.,5.,"""However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints.""",2022-10-10T18:56:57Z
Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks,Yes.,2.,"""the generic corpora used to pretrain the teacher and the corpora associated with the downstream target domain are often significantly different, which raises a natural question",2022-10-10T16:49:52Z
Readability Controllable Biomedical Document Summarization,Yes.,3.,"""Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task.""",2022-10-10T14:03:20Z
DEPTWEET: A Typology for Social Media Texts to Detect Depression Severities,No.,1.,The abstract does not mention LLMs or any specific limitations related to them.,2022-10-10T08:23:57Z
FairGer: Using NLP to Measure Support for Women and Migrants in 155 Years of German Parliamentary Debates,No.,1.,The abstract does not mention LLMs or any other language models.,2022-10-09T22:02:58Z
Quantifying Social Biases Using Templates is Unreliable,Yes.,5.,"""Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases."" and ""Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution.""",2022-10-09T20:05:29Z
Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection,Yes.,1.,"""Pre-training large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks.""",2022-10-09T13:53:06Z
Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,Yes.,3.,"""We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements).""",2022-10-09T06:35:14Z
"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",Yes.,3.,"""Such rich contextualization can be especially beneficial for long document understanding tasks since standard pretrained LMs are typically bounded by the input sequence length.""",2022-10-08T20:51:02Z
Understanding HTML with Large Language Models,Yes.,1.,"""Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks.""",2022-10-08T07:27:17Z
AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models,Yes.,1.,"""There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet.""",2022-10-08T00:36:00Z
How Large Language Models are Transforming Machine-Paraphrased Plagiarism,Yes.,2.,"""However, the role of large autoregressive transformers in generating machine-paraphrased plagiarism and their detection is still developing in the literature.""",2022-10-07T14:08:57Z
Automatic Chain of Thought Prompting in Large Language Models,,,,2022-10-07T12:28:21Z
Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems,Yes.,5.,"""However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization.""",2022-10-07T11:16:45Z
Measuring and Narrowing the Compositionality Gap in Language Models,Yes.,5.,"""we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.""",2022-10-07T06:50:23Z
Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners,Yes.,3.,"""meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training.""",2022-10-06T15:00:47Z
ReAct: Synergizing Reasoning and Acting in Language Models,Yes.,3.,"""ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API.""",2022-10-06T01:00:32Z
Learning to Reason With Relational Abstractions,Yes.,3.,"""the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce.""",2022-10-06T00:27:50Z
Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,Yes.,1.,"""we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection.""",2022-10-05T18:44:35Z
Antibody Representation Learning for Drug Discovery,Yes.,2.,"""Existing works have not yet investigated the value, limitations and opportunities of these methods in application to antibody-based drug discovery.""",2022-10-05T13:48:41Z
Grounding Language with Visual Affordances over Unstructured Data,Yes.,1.,"""Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills.""",2022-10-04T21:16:48Z
Towards Improving Faithfulness in Abstractive Summarization,Yes.,4.,"""Despite the success achieved in neural abstractive summarization based on pre-trained language models, one unresolved issue is that the generated summaries are not always faithful to the input document."" and ""the model over-relies on the language model to generate fluent but inadequate words.""",2022-10-04T19:52:09Z
Explaining Patterns in Data with Language Models via Interpretable Autoprompting,Yes.,1.,"""Large language models (LLMs) have displayed an impressive ability to harness natural language to perform complex tasks.""",2022-10-04T18:32:14Z
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,Yes.,3.,"""We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA.""",2022-10-04T09:04:27Z
Less is More: Task-aware Layer-wise Distillation for Language Model Compression,Yes.,3.,"""However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning.""",2022-10-04T03:36:53Z
ThinkSum: Probabilistic reasoning over sets using large language models,Yes.,4.,"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""",2022-10-04T00:34:01Z
Robot Task Planning and Situation Handling in Open Worlds,Yes.,1.,"""common sense is extracted from Large Language Models based on the current task at hand and robot skills.""",2022-10-04T00:21:00Z
"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",Yes.,3.,"""using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment.""",2022-10-03T21:38:29Z
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Yes.,5.,"""However, they have difficulty with proof planning",2022-10-03T21:34:32Z
The (In)Effectiveness of Intermediate Task Training For Domain Adaptation and Cross-Lingual Transfer Learning,Yes.,3.,"""it remains an open question, if and when transfer learning will work, i.e. leading to positive or negative transfer.""",2022-10-03T17:17:07Z
A Non-monotonic Self-terminating Language Model,Yes.,5.,"""However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.""",2022-10-03T00:28:44Z
Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks,Yes.,3.,"""Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost.""",2022-10-01T04:08:50Z
On the Impossible Safety of Large AI Models,Yes.,5.,"""Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues."" and ""we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.""",2022-09-30T06:36:49Z
Learning by Distilling Context,Yes.,3.,"""However, they do not internalize these performance gains, which disappear when the context tokens are gone.""",2022-09-30T02:30:15Z
Unpacking Large Language Models with Conceptual Consistency,Yes.,5.,"""While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency.""",2022-09-29T20:55:57Z
Compositional Semantic Parsing with Large Language Models,Yes.,3.,"""we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them.""",2022-09-29T17:58:28Z
Repairing Bugs in Python Assignments Using Large Language Models,Yes.,1.,"""We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments.""",2022-09-29T15:41:17Z
Downstream Datasets Make Surprisingly Good Pretraining Corpora,Yes.,1.,"""For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets.""",2022-09-28T19:28:43Z
Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer,Yes.,1.,"""The paper explores the relevance of the Text-To-Text Transfer Transformer language model (T5) for Polish (plT5) to the task of intrinsic and extrinsic keyword extraction from short text passages.""",2022-09-28T11:31:43Z
How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI,Yes.,4.,"""While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society."" and ""We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat",2022-09-27T18:44:41Z
Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors,Yes.,3.,"""However, such systems all succumb to the same problem",2022-09-27T00:44:41Z
Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,Yes.,4.,"""Gordon and Van Durme (2013) point out that LMs can thus suffer from reporting bias",2022-09-26T15:45:23Z
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,Yes.,5.,"""By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions.""",2022-09-26T14:05:10Z
News Summarization and Evaluation in the Era of GPT-3,Yes.,3.,"""Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.""",2022-09-26T01:04:52Z
WinoDict: Probing language models for in-context word acquisition,Yes.,5.,"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs",2022-09-25T05:30:13Z
Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,Yes.,3.,"""Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases.""",2022-09-24T23:55:53Z
Augmenting Interpretable Models with LLMs during Training,Yes.,3.,"""However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency.""",2022-09-23T18:36:01Z
Promptagator: Few-shot Dense Retrieval From 8 Examples,Yes.,1.,"""we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator.""",2022-09-23T17:59:06Z
ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,Yes.,3.,"""However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context.""",2022-09-22T20:29:49Z
"A Case Report On The ""A.I. Locked-In Problem"": social concerns with modern NLP",Yes.,5.,"""However, practical experimentation with GPT-3 shows that there is a recurring problem with these modern NLP systems, namely that they can 'get stuck' in the narrative so that further conversations, prompt executions or commands become futile. This is here referred to as the 'Locked-In Problem' and is exemplified with an experimental case report, followed by practical and social concerns that are accompanied with",2022-09-22T16:39:35Z
Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,Yes.,2.,"""Our method works under the constraints of 1) a black-box (non-modifiable) question generation model and 2) lack of access to human-annotated references -- both of which are realistic limitations for real-world deployment of LLMs.""",2022-09-22T13:33:48Z
Representing Affect Information in Word Embeddings,Yes.,3.,"""Our analyses show that word embedding from the vanilla BERT model did not saliently encode the affect information of English words.""",2022-09-21T18:16:33Z
Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers,Yes.,1.,"""Current applications often use large transformer-based language models to classify input texts.""",2022-09-21T17:05:12Z
Bias at a Second Glance: A Deep Dive into Bias for German Educational Peer-Review Data Modeling,Yes.,4.,"""However, recent research has highlighted a variety of biases in pre-trained language models."" and ""the pre-trained German language models find substantial conceptual, racial, and gender bias and have significant changes in bias across conceptual and racial axes during fine-tuning on the peer-review data",2022-09-21T13:08:16Z
T5QL: Taming language models for SQL generation,Yes.,3.,"""Current SOTA methods for semantic parsing depend on LLMs to achieve high predictive accuracy on benchmark datasets. This reduces their applicability, since LLMs requires expensive GPUs. Furthermore, SOTA methods are ungrounded and thus not guaranteed to always generate valid SQL.""",2022-09-21T10:43:13Z
Generate rather than Retrieve: Large Language Models are Strong Context Generators,Yes.,1.,"""In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators.""",2022-09-21T01:30:59Z
Open-vocabulary Queryable Scene Representations for Real World Planning,Yes.,3.,"""However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene.""",2022-09-20T17:29:56Z
Relaxed Attention for Transformer Models,Yes.,3.,"""The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models.""",2022-09-20T14:10:28Z
EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics,Yes.,1.,"""Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs.""",2022-09-20T10:12:07Z
Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,Yes.,1.,"""This process is normally a black box in the case of deep learning models like large-scale language models.""",2022-09-20T07:04:24Z
Enabling Conversational Interaction with Mobile UI using Large Language Models,Yes.,1.,"""Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming.""",2022-09-18T20:58:39Z
CodeQueries: A Dataset of Semantic Queries over Code,Yes.,3.,"""We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot settings on a subset of CodeQueries. We also evaluate a BERT style model (CuBERT) with fine-tuning. We find that these models achieve limited success on CodeQueries.""",2022-09-17T17:09:30Z
Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models,Yes.,3.,"""large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models.""",2022-09-16T19:23:13Z
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",Yes.,3.,"""Despite impressive results across various tasks, the reasons behind their success have not been explored."" and ""brings into question the conventional wisdom around few-shot prompting.""",2022-09-16T02:54:00Z
Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach,Yes.,3.,"""Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances.""",2022-09-15T01:51:22Z
Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing,No.,1.,"The abstract focuses on using NLP techniques and specific models like BERT, LSTM, and a rule-based algorithm for fidelity assessment in rehabilitation sessions. It does not discuss LLMs or their limitations.",2022-09-14T15:33:30Z
SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus,Yes.,5.,"""However, since BERT is quadratic to the text length, the BERT model is difficult to be used directly on the long-text corpus."" and ""alleviating the time and space overflow problem of basic BERT on long-text data.""",2022-09-13T05:49:10Z
DECK: Behavioral Tests to Improve Interpretability and Generalizability of BERT Models Detecting Depression from Text,Yes.,3.,"""However, these models are known to suffer from performance inconsistencies and poor generalization.""",2022-09-12T14:39:46Z
T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition,Yes.,3.,"""cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine-tuned on a combined dataset.""",2022-09-09T15:00:38Z
Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages,Yes.,5.,"""It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency",2022-09-08T21:40:41Z
Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots,Yes.,5.,"""We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too."" and ""This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users.""",2022-09-07T20:45:41Z
AILAB-Udine@SMM4H 22: Limits of Transformers and BERT Ensembles,Yes.,2.,"""We explored the limits of Transformer based models on text classification, entity extraction and entity normalization.""",2022-09-07T20:17:15Z
The Ethical Need for Watermarks in Machine-Generated Language,Yes.,4.,"""The ethical imperative to not blur this distinction arises from the asemantic nature of large language models and from human projections of emotional and cognitive states on machines, possibly leading to manipulation, spreading falsehoods or emotional distress.""",2022-09-07T13:09:44Z
Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples,,,,2022-09-05T20:29:17Z
Every picture tells a story: Image-grounded controllable stylistic story generation,Yes.,1.,"""we introduce Plug-and-Play Story Teller (PPST) and improve image-to-story generation by",2022-09-04T15:07:53Z
Do Large Language Models know what humans know?,Yes.,3.,"""the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime.""",2022-09-04T01:29:53Z
Neural Approaches to Multilingual Information Retrieval,Yes.,1.,"""This paper investigates whether advances in neural document translation and pretrained multilingual neural language models enable improvements in the state of the art over earlier MLIR techniques.""",2022-09-03T06:02:52Z
Petals: Collaborative Inference and Fine-tuning of Large Models,Yes.,3.,"""However, these techniques have innate limitations",2022-09-02T17:38:03Z
Generating Coherent Drum Accompaniment With Fills And Improvisations,No.,1.,The abstract discusses the use of transformer models for music generation but does not mention LLMs or their limitations.,2022-09-01T08:31:26Z
Enhancing Semantic Understanding with Self-supervised Methods for Abstractive Dialogue Summarization,Yes.,2.,"""We introduce self-supervised methods to compensate shortcomings to train a dialogue summarization model.""",2022-09-01T07:51:46Z
Faithful Reasoning Using Large Language Models,Yes.,3.,"""Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step.""",2022-08-30T13:44:41Z
LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems,Yes.,3.,"""state-of-the-art language models still struggle evaluating precise statements consistently"" and ""CLIP is not able to rerank those generated samples consistently.""",2022-08-29T11:40:36Z
JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents,,,,2022-08-28T18:30:46Z
Target Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization,No.,1.,The abstract does not mention LLMs or their limitations.,2022-08-27T21:11:45Z
Training a T5 Using Lab-sized Resources,Yes.,4.,"""Training large neural language models on large datasets is resource- and time-intensive. These requirements create a barrier to entry, where those with fewer resources cannot build competitive models.""",2022-08-25T13:55:16Z
On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,Yes.,5.,"""their ability to understand the physical world using only language data remains a question"" and ""Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness.""",2022-08-25T10:21:23Z
Shortcut Learning of Large Language Models in Natural Language Understanding,Yes.,5.,"""However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness.""",2022-08-25T03:51:39Z
Repair Is Nearly Generation: Multilingual Program Repair with LLMs,Yes.,1.,"""We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex.""",2022-08-24T16:25:58Z
DPTDR: Deep Prompt Tuning for Dense Passage Retrieval,Yes.,1.,"""applying DPT in dense retrieval largely underperforms FT methods.""",2022-08-24T12:55:00Z
Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models,Yes.,5.,"""Recent work shows that this limitation persists in state-of-the-art Transformer-based models."" and ""our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance.""",2022-08-24T11:25:27Z
"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",Yes.,4.,"""We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types."" and ""We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs.""",2022-08-23T23:37:14Z
Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning,Yes.,3.,"""Current pre-trained language model (PLM)-based reasoning follows the traditional practice using perplexity metric. However, commonsense reasoning is more than existing probability evaluation, which is biased by word frequency.""",2022-08-23T14:42:14Z
Interpreting Embedding Spaces by Conceptualization,Yes.,4.,"""One major drawback of this type of representation is their incomprehensibility to humans."" and ""Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model.""",2022-08-22T15:32:17Z
Selection Collider Bias in Large Language Models,Yes.,4.,"""sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world"" and ""selection collider bias can become amplified in underspecified learning tasks"".",2022-08-22T05:38:15Z
GRETEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization,Yes.,3.,"""However, in these methods, there remain limitations in the way they capture and integrate the global semantic information.""",2022-08-21T23:09:29Z
VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media,Yes.,3.,"""ViLT, importantly, enables efficient training and inference in VL tasks, achieved by encoding images using a linear projection of patches instead of an object detector. However, it is pretrained on captioning datasets, where the language input is simple, literal, and descriptive, therefore",2022-08-18T18:51:13Z
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies,Yes.,5.,"""A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior."" and ""the last TE reveals a 'hyper-accuracy distortion' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.""",2022-08-18T17:54:49Z
MulZDG: Multilingual Code-Switching Framework for Zero-shot Dialogue Generation,Yes.,1.,"""The typical zero-shot approaches in dialogue generation rely heavily on large-scale pre-trained language generation models such as GPT-3 and T5.""",2022-08-18T04:28:20Z
Ask Question First for Enhancing Lifelong Language Learning,No.,1.,"The abstract discusses lifelong language learning, NLP tasks, and catastrophic forgetting, but does not mention large language models (LLMs) or their limitations.",2022-08-17T15:58:33Z
HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models,Yes.,3.,"""Controlling the text generated by language models and customizing the content has been a long-standing challenge."" and ""Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further",2022-08-17T11:20:41Z
Transformer Encoder for Social Science,Yes.,1.,"""We have witnessed the success of pretrained deep neural network models, such as BERT and RoBERTa, in recent social science research.""",2022-08-17T01:01:25Z
LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,Yes.,2.,"""Large language models have been widely adopted but require significant GPU memory for inference.""",2022-08-15T17:08:50Z
Targeted Honeyword Generation with Language Models,Yes.,1.,"""we propose to build a more secure and trustworthy authentication system that employs off-the-shelf pre-trained language models which require no further training on real passwords to produce honeywords.""",2022-08-15T00:06:29Z
What is it like to program with artificial intelligence?,Yes.,4.,"""We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.""",2022-08-12T10:48:46Z
New drugs and stock market: how to predict pharma market reaction to clinical trial announcements,No.,1.,The abstract does not mention language models (LLMs or LMs) at all.,2022-08-11T20:20:21Z
Interactive Code Generation via Test-Driven User-Intent Formalization,Yes.,5.,"""users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.""",2022-08-11T17:41:08Z
Reducing Retraining by Recycling Parameter-Efficient Prompts,Yes.,3.,"""these learned prompts are tightly coupled to a given frozen model -- if the model is updated, corresponding new prompts need to be obtained.""",2022-08-10T22:10:53Z
Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts,Yes.,5.,"""Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.""",2022-08-08T20:59:16Z
Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models,Yes.,1.,"""We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs.""",2022-08-05T17:46:38Z
Atlas: Few-shot Learning with Retrieval Augmented Language Models,Yes.,3.,"""However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed.""",2022-08-05T17:39:22Z
Meaning without reference in large language models,,,,2022-08-05T02:48:26Z
Debiasing Gender Bias in Information Retrieval Models,No.,1.,The abstract does not mention LLMs or their limitations.,2022-08-02T21:12:05Z
Gender bias in (non)-contextual clinical word embeddings for stereotypical medical categories,Yes.,4.,"""clinical embeddings carry a high degree of bias for some medical terms and diseases which is conflicting with medical literature. Having such an ill-founded relationship might cause harm in downstream applications that use clinical embeddings.""",2022-08-02T10:02:21Z
PASTA: A Dataset for Modeling Participant States in Narratives,,,,2022-07-31T01:21:48Z
Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions,Yes.,3.,"""Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics.""",2022-07-28T17:36:24Z
HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative,Yes.,1.,"""HelixFold-Single is proposed to combine a large-scale protein language model with the superior geometric learning capability of AlphaFold2.""",2022-07-28T07:30:33Z
When BERT Fails -- The Limits of EHR Classification,Yes.,5.,"""Although they outperform baselines on readmission prediction, they are not infallible. Here, we look into one such failure case, and report patterns that lead to inferior predictive performance.""",2022-07-26T17:18:24Z
A Hazard Analysis Framework for Code Synthesis Large Language Models,Yes.,5.,"""models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential.""",2022-07-25T20:44:40Z
Robots Enact Malignant Stereotypes,No.,1.,"The abstract does not mention LLMs or any specific language models. It focuses on ML bias in robotics, particularly with CLIP-powered methods.",2022-07-23T18:08:12Z
Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices,Yes.,3.,"""However, the DP-noise introduced to the model increases as the model size grows, which often prevents convergence.""",2022-07-18T23:53:17Z
Selection Bias Induced Spurious Correlations in Large Language Models,Yes.,5.,"""large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias.""",2022-07-18T23:43:52Z
Can large language models reason about medical questions?,Yes.,3.,"""it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge.""",2022-07-17T11:24:44Z
Automatic Context Pattern Generation for Entity Set Expansion,Yes.,1.,"""we devise a context pattern generation module that utilizes autoregressive language models (e.g., GPT-2) to automatically generate high-quality context patterns for entities.""",2022-07-17T06:50:35Z
Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model,Yes.,3.,"""Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations",2022-07-16T13:02:54Z
Confident Adaptive Language Modeling,Yes.,2.,"""These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time.""",2022-07-14T17:00:19Z
Language models show human-like content effects on reasoning tasks,Yes.,3.,"""Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections.""",2022-07-14T16:51:09Z
Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model,Yes.,1.,"""This study discusses the effect of semi-supervised learning in combination with pretrained language models for data-to-text generation.""",2022-07-14T11:53:04Z
BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling,Yes.,2.,"""The pre-training of large language models usually requires massive amounts of resources, both in terms of computation and data.""",2022-07-14T10:48:42Z
A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America,Yes.,4.,"""Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them.""",2022-07-14T01:07:55Z
DocPrompting: Generating Code by Retrieving the Docs,No.,1.,The abstract does not mention LLMs or any specific limitations related to language models.,2022-07-13T06:47:51Z
Exploring Length Generalization in Large Language Models,Yes.,5.,"""We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale."" and ""We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.""",2022-07-11T14:24:38Z
Few-shot training LLMs for project-specific code-summarization,Yes.,1.,"""Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code.""",2022-07-09T09:57:11Z
Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation,Yes.,2.,"""Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance.""",2022-07-07T18:00:22Z
A Large Scale Search Dataset for Unbiased Learning to Rank,,,,2022-07-07T02:37:25Z
The Role of Complex NLP in Transformers for Text Ranking?,,,,2022-07-06T08:54:18Z
Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning,Yes.,4.,"""Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue."" and ""we re-discover a bias-performance trade-off",2022-07-06T06:20:35Z
Machine Learning Model Sizes and the Parameter Gap,Yes.,3.,"""We also identify that, since 2020, there have been many language models below 20B parameters, many models above 70B parameters, but a scarcity of models in the 20-70B parameter range. We refer to that scarcity as the parameter gap.""",2022-07-05T20:55:38Z
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,Yes.,3.,"""free-text rationales aim to use natural language to explain neural language model (LM) behavior"" and ""free-text rationales' unconstrained nature makes them prone to hallucination"" and ""we discuss the limitations of using RLC to evaluate free-text rationales.""",2022-07-02T09:25:29Z
Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,Yes.,3.,"""One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information.""",2022-07-01T06:25:15Z
When Does Differentially Private Learning Not Suffer in High Dimensions?,Yes.,1.,"""Large pretrained models can be privately fine-tuned to achieve performance approaching that of non-private models.""",2022-07-01T02:36:51Z
