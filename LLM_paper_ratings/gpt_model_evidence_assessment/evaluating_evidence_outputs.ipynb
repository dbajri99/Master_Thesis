{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cfaf77",
   "metadata": {},
   "source": [
    "## Evaluating Evidence Outputs\n",
    "\n",
    "\n",
    "This Jupyter Notebook script is designed to analyze a dataset containing text summaries and associated evidence annotations from two human annotators (Deidamea and Aaron) as well as a model. The primary objective is to assess the alignment of annotations between the model and each human annotator, and subsequently, between the two human annotators themselves. This analysis aims to provide valuable insights into the model's consistency and reliability in accurately identifying evidence of LLMs limitations.\n",
    "\n",
    "**Process**\n",
    "\n",
    "1. **Load data:** Import dataset from Excel.\n",
    "2. **Tokenize:** Split text into words.\n",
    "3. **Convert evidence:** Transform annotations into token sets.\n",
    "4. **BIO tagging:** Label tokens based on evidence occurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6213056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              summary  \\\n",
      "0   Large Language Models (LLMs) excel in various ...   \n",
      "1   We introduce Codex, a GPT language model fine-...   \n",
      "2   Recent advancements in the field of natural la...   \n",
      "3   Large language models (LLMs) have exhibited an...   \n",
      "4   Large language models (LLMs) implicitly learn ...   \n",
      "5   Large language models (LLMs) are competitive w...   \n",
      "6   Large language models of code (Code-LLMs) have...   \n",
      "7   The prevalence and strong capability of large ...   \n",
      "8   The rise of algorithmic pricing raises concern...   \n",
      "9   Large Language Models (LLMs) exhibit remarkabl...   \n",
      "10  Businesses and software platforms are increasi...   \n",
      "11  Language models struggle with handling numeric...   \n",
      "12  Large Language Models (LLMs) have demonstrated...   \n",
      "13  Large Language Models (LLMs) still face challe...   \n",
      "14  Large Language Models (LLMs) have shown profic...   \n",
      "15  Large Language Models (LLMs) have become incre...   \n",
      "16  The widespread adoption of Large Language Mode...   \n",
      "17  Large language models (LLMs) have revolutioniz...   \n",
      "18  This research paper presents a comprehensive c...   \n",
      "19  Using large language models (LLMs) for source ...   \n",
      "20  One of the limitations of large language model...   \n",
      "21  In this paper, we aim to develop a large langu...   \n",
      "22  The domain of natural language processing (NLP...   \n",
      "23  This article explores the challenges and limit...   \n",
      "24  This work presents a linguistic analysis into ...   \n",
      "25  Many current NLP systems are built from langua...   \n",
      "26  Language models are typically evaluated on the...   \n",
      "27  Large language models (LLMs) often contain mis...   \n",
      "28  Large language models (LLMs) have been recentl...   \n",
      "\n",
      "    Model vs Aaron - Precision  Model vs Aaron - Recall  \\\n",
      "0                     0.972222                 0.972603   \n",
      "1                     0.851852                 0.566441   \n",
      "2                     0.681818                 0.177970   \n",
      "3                     0.966667                 0.549957   \n",
      "4                     0.893939                 0.722926   \n",
      "5                     1.000000                 0.910700   \n",
      "6                     0.478571                 0.700000   \n",
      "7                     0.341085                 0.345238   \n",
      "8                     0.675824                 0.642857   \n",
      "9                     0.964286                 0.751863   \n",
      "10                    0.839286                 0.481481   \n",
      "11                    0.916667                 0.229487   \n",
      "12                    0.404324                 0.270769   \n",
      "13                    0.933333                 0.816353   \n",
      "14                    0.780998                 0.729870   \n",
      "15                    0.976190                 0.786004   \n",
      "16                    0.812500                 0.126207   \n",
      "17                    0.369597                 0.276414   \n",
      "18                    0.611886                 0.918919   \n",
      "19                    0.808281                 0.826831   \n",
      "20                    1.000000                 0.931613   \n",
      "21                    0.739860                 0.924528   \n",
      "22                    0.781250                 0.521104   \n",
      "23                    0.521445                 0.553360   \n",
      "24                    0.271167                 0.256633   \n",
      "25                    0.484483                 0.551064   \n",
      "26                    1.000000                 1.000000   \n",
      "27                    0.500000                 0.027778   \n",
      "28                    0.906250                 0.609685   \n",
      "\n",
      "    Model vs Aaron - F1 Score  Aaron vs Deidamea - Precision  \\\n",
      "0                    0.971630                       0.965079   \n",
      "1                    0.636990                       0.735119   \n",
      "2                    0.277306                       0.310213   \n",
      "3                    0.700980                       0.855769   \n",
      "4                    0.784028                       0.722926   \n",
      "5                    0.952330                       1.000000   \n",
      "6                    0.546914                       0.833333   \n",
      "7                    0.343137                       0.869344   \n",
      "8                    0.545168                       0.666667   \n",
      "9                    0.844797                       1.000000   \n",
      "10                   0.604334                       0.479971   \n",
      "11                   0.366776                       0.353383   \n",
      "12                   0.320307                       0.675397   \n",
      "13                   0.870642                       0.973333   \n",
      "14                   0.752838                       0.932540   \n",
      "15                   0.868750                       0.473684   \n",
      "16                   0.209209                       0.266667   \n",
      "17                   0.314588                       0.624072   \n",
      "18                   0.726534                       1.000000   \n",
      "19                   0.815730                       0.895349   \n",
      "20                   0.964372                       1.000000   \n",
      "21                   0.814815                       0.983333   \n",
      "22                   0.623525                       0.866667   \n",
      "23                   0.536908                       0.897283   \n",
      "24                   0.263698                       0.317901   \n",
      "25                   0.514286                       0.606732   \n",
      "26                   1.000000                       0.933333   \n",
      "27                   0.052632                       0.094572   \n",
      "28                   0.717471                       0.845791   \n",
      "\n",
      "    Aaron vs Deidamea - Recall  Aaron vs Deidamea - F1 Score  \\\n",
      "0                     0.972222                      0.968625   \n",
      "1                     0.783670                      0.752549   \n",
      "2                     0.818182                      0.448413   \n",
      "3                     1.000000                      0.918768   \n",
      "4                     0.893939                      0.784028   \n",
      "5                     0.983333                      0.991525   \n",
      "6                     0.417857                      0.533566   \n",
      "7                     0.969697                      0.915318   \n",
      "8                     0.648352                      0.595328   \n",
      "9                     1.000000                      1.000000   \n",
      "10                    0.839286                      0.600950   \n",
      "11                    1.000000                      0.518519   \n",
      "12                    0.959459                      0.787768   \n",
      "13                    0.983333                      0.978283   \n",
      "14                    0.849963                      0.889246   \n",
      "15                    0.904762                      0.621469   \n",
      "16                    0.906250                      0.403030   \n",
      "17                    0.900000                      0.729532   \n",
      "18                    1.000000                      1.000000   \n",
      "19                    0.749007                      0.809635   \n",
      "20                    1.000000                      1.000000   \n",
      "21                    0.990909                      0.987092   \n",
      "22                    0.677083                      0.754839   \n",
      "23                    0.934783                      0.915302   \n",
      "24                    0.401602                      0.346670   \n",
      "25                    0.619828                      0.613120   \n",
      "26                    0.927588                      0.929228   \n",
      "27                    1.000000                      0.166667   \n",
      "28                    0.953125                      0.890873   \n",
      "\n",
      "    Model vs Deidamea - Precision  Model vs Deidamea - Recall  \\\n",
      "0                        0.972222                    0.979452   \n",
      "1                        0.833333                    0.557288   \n",
      "2                        0.860000                    0.638528   \n",
      "3                        0.975000                    0.654177   \n",
      "4                        1.000000                    1.000000   \n",
      "5                        1.000000                    0.894571   \n",
      "6                        0.962963                    0.717143   \n",
      "7                        0.427036                    0.485931   \n",
      "8                        0.886792                    0.812698   \n",
      "9                        0.964286                    0.751863   \n",
      "10                       0.982143                    0.986842   \n",
      "11                       0.735589                    0.511538   \n",
      "12                       0.572222                    0.451538   \n",
      "13                       0.933333                    0.824974   \n",
      "14                       0.724868                    0.612987   \n",
      "15                       0.503509                    0.780933   \n",
      "16                       0.897436                    0.467241   \n",
      "17                       0.585189                    0.598586   \n",
      "18                       0.611886                    0.918919   \n",
      "19                       0.728148                    0.627457   \n",
      "20                       1.000000                    0.931613   \n",
      "21                       0.725758                    0.915094   \n",
      "22                       0.700000                    0.347403   \n",
      "23                       0.545380                    0.605072   \n",
      "24                       0.549383                    0.698980   \n",
      "25                       0.619869                    0.707979   \n",
      "26                       0.933333                    0.927588   \n",
      "27                       0.937500                    0.494108   \n",
      "28                       0.813926                    0.615991   \n",
      "\n",
      "    Model vs Deidamea - F1 Score  \n",
      "0                       0.975225  \n",
      "1                       0.648370  \n",
      "2                       0.725186  \n",
      "3                       0.781067  \n",
      "4                       1.000000  \n",
      "5                       0.943860  \n",
      "6                       0.821830  \n",
      "7                       0.453636  \n",
      "8                       0.835034  \n",
      "9                       0.844797  \n",
      "10                      0.984242  \n",
      "11                      0.601389  \n",
      "12                      0.501525  \n",
      "13                      0.875661  \n",
      "14                      0.662043  \n",
      "15                      0.609082  \n",
      "16                      0.605302  \n",
      "17                      0.591449  \n",
      "18                      0.726534  \n",
      "19                      0.672957  \n",
      "20                      0.964372  \n",
      "21                      0.801903  \n",
      "22                      0.456989  \n",
      "23                      0.573304  \n",
      "24                      0.601619  \n",
      "25                      0.659930  \n",
      "26                      0.929228  \n",
      "27                      0.628695  \n",
      "28                      0.699476  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_24352/1969898580.py:61: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.1' currently installed).\n",
      "  results_df.to_excel('evaluation_results.xlsx', index=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "file_path = 'limitation_evidences_evaluation.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\" Tokenize text into words while handling punctuation. \"\"\"\n",
    "    return word_tokenize(text.lower()) \n",
    "\n",
    "def generate_bio_tags(tokens, evidence_tokens_set):\n",
    "    \"\"\" Generate BIO tags based on whether tokens appear in any evidence set. \"\"\"\n",
    "    tags = ['O'] * len(tokens) \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in evidence_tokens_set:\n",
    "            tags[i] = 'B-EVID' if (i == 0 or tags[i-1] == 'O') else 'I-EVID'\n",
    "    return tags\n",
    "\n",
    "def evaluate_annotations(tokens, bio1, bio2):\n",
    "    \"\"\" Evaluate BIO tagged tokens and return precision, recall, F1. \"\"\"\n",
    "    precision = precision_score(bio1, bio2, labels=['B-EVID', 'I-EVID'], average='macro', zero_division=0)\n",
    "    recall = recall_score(bio1, bio2, labels=['B-EVID', 'I-EVID'], average='macro', zero_division=0)\n",
    "    f1 = f1_score(bio1, bio2, labels=['B-EVID', 'I-EVID'], average='macro', zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def convert_highlights_to_set(highlights):\n",
    "    \"\"\" Convert highlights to a set of tokens for comparison. \"\"\"\n",
    "    return set(word_tokenize(highlights.lower())) if pd.notna(highlights) else set()\n",
    "\n",
    "results = []\n",
    "for index, row in data.iterrows():\n",
    "    text = row['summary']\n",
    "    tokens = tokenize_text(text)\n",
    "    highlight_model_set = convert_highlights_to_set(row['Model Evidences'])\n",
    "    highlight_you_set = convert_highlights_to_set(row['Deida Evidences'])\n",
    "    highlight_aaron_set = convert_highlights_to_set(row['Aaron Evidences'])\n",
    "    bio_model = generate_bio_tags(tokens, highlight_model_set)\n",
    "    bio_you = generate_bio_tags(tokens, highlight_you_set)\n",
    "    bio_aaron = generate_bio_tags(tokens, highlight_aaron_set)\n",
    "    precision_ma, recall_ma, f1_ma = evaluate_annotations(tokens, bio_model, bio_aaron)\n",
    "    precision_ay, recall_ay, f1_ay = evaluate_annotations(tokens, bio_aaron, bio_you)\n",
    "    precision_my, recall_my, f1_my = evaluate_annotations(tokens, bio_model, bio_you)\n",
    "\n",
    "    results.append({\n",
    "        'summary': text,\n",
    "        'Model vs Aaron - Precision': precision_ma,\n",
    "        'Model vs Aaron - Recall': recall_ma,\n",
    "        'Model vs Aaron - F1 Score': f1_ma,\n",
    "        'Aaron vs Deidamea - Precision': precision_ay,\n",
    "        'Aaron vs Deidamea - Recall': recall_ay,\n",
    "        'Aaron vs Deidamea - F1 Score': f1_ay,\n",
    "        'Model vs Deidamea - Precision': precision_my,\n",
    "        'Model vs Deidamea - Recall': recall_my,\n",
    "        'Model vs Deidamea - F1 Score': f1_my\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "results_df.to_excel('evaluation_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02e13486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Scores for Annotation Comparisons:\n",
      "------------------------------------------------\n",
      "F1 Score for Model vs Aaron: 0.619\n",
      "F1 Score for Aaron vs Deidamea: 0.753\n",
      "F1 Score for Model vs Deidamea: 0.730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "average_f1_ma = results_df['Model vs Aaron - F1 Score'].mean()\n",
    "average_f1_ay = results_df['Aaron vs Deidamea - F1 Score'].mean()\n",
    "average_f1_my = results_df['Model vs Deidamea - F1 Score'].mean()\n",
    "\n",
    "print(\"Average F1 Scores for Annotation Comparisons:\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"F1 Score for Model vs Aaron: {average_f1_ma:.3f}\")\n",
    "print(f\"F1 Score for Aaron vs Deidamea: {average_f1_ay:.3f}\")\n",
    "print(f\"F1 Score for Model vs Deidamea: {average_f1_my:.3f}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
