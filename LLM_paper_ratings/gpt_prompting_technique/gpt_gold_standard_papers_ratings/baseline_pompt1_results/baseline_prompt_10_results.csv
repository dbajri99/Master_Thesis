Title,LMs,Limitations of LMs,Evidence
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,Yes.,3.,"""existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models"" and ""alleviates the error accumulation problem in autoregressive generation."""
Time-and-Space-Efficient Weighted Deduction,No.,1.,The abstract and title do not mention language models or their limitations. The focus is on weighted deduction strategies in NLP algorithms.
Conditional Generation with a Question-Answering Blueprint,Yes.,4.,"""The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details."""
Collective Human Opinions in Semantic Textual Similarity,No.,1.,"""We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset."""
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,No.,1.,The abstract and title discuss biases in task design for crowdsourcing implicit discourse relations and do not mention language models or their limitations.
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,Yes.,1.,"""We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning."""
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,No.,1.,"""Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders."""
Cross-functional Analysis of Generalization in Behavioral Learning,Yes.,3.,"""there is the risk that the model narrowly captures spurious correlations from the behavioral test suite, leading to overestimation and misrepresentation of model performance—one of the original pitfalls of traditional evaluation."""
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,Yes.,3.,"""Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,Yes.,2.,"""Label scarcity is a bottleneck for improving task performance in specialized domains."""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,No.,1.,The abstract and title focus on a multilingual retrieval dataset and do not discuss language models (LMs or LLMs) or their limitations.
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,No.,1.,The paper focuses on a dataset for dataset mention detection in scientific literature and does not discuss language models or their limitations.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,Yes.,4.,"""Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",yes.,4.,"""We analyze mathematical language processing methods across five strategic sub-areas... highlighting prevailing methodologies, existing limitations, overarching trends, and promising avenues for future research."""
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or their limitations. The focus is on evaluating adjective ordering in cognitive science using natural language processing technologies and datasets.
Improving Multitask Retrieval by Promoting Task Specialization,No.,1.,"The paper focuses on improving multitask retrieval and does not specifically mention language models (LMs or LLMs), nor does it discuss their limitations."
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,Yes.,3.,"""We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,No.,1.,"The abstract and title do not mention language models (e.g., LMs or LLMs) or their limitations. The focus is on intent-calibrated self-training for answer selection in open-domain dialogues."
Benchmarking the Generation of Fact Checking Explanations,Yes.,2.,"""we experiment with several extractive and abstractive strategies"" and ""cross-dataset experiments suffer from performance degradation."""
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,No.,1.,"""In this paper, we propose a two-stage span-based framework with templates, namely, T2-NER, to resolve the unified NER task."""
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,Yes.,4.,"""Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual)."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,No.,1.,The abstract and title focus on Open Relation Extraction (ORE) and do not mention language models (LMs or LLMs) or their limitations. The discussion is centered around contrastive learning and clustering techniques for ORE tasks.
In-Context Retrieval-Augmented Language Models,Yes.,3.,"""they can mitigate the problem of factually inaccurate text generation"" and ""Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment."""
Learning to Paraphrase Sentences to Different Complexity Levels,Yes.,2.,"""Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,Yes.,1.,The abstract does not specifically discuss limitations of language models.
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,yes.,5.,"""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training... This finding points to a limitation with current models and points to a reason for which their training is data-intensive."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",Yes.,2.,"""we first take stock of the current landscape of multilingual ToD datasets, offering a systematic overview of their properties and limitations."""
Can Authorship Representation Learning Capture Stylistic Features?,No.,1.,"The paper discusses ""authorship representation learning"" and ""authorship attribution,"" focusing on stylistic features and writing style, without mentioning language models (e.g., LMs or LLMs) or their limitations."
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,No.,1.,The paper focuses on cross-lingual semantic parsing and optimal transport for minimizing cross-lingual divergence. It does not mention language models or their limitations in the abstract or title.
