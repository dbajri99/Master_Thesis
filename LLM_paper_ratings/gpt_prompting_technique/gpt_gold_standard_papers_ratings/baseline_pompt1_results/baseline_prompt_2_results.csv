Title,Talks about LLMs,Rate,Evidence
Automating Behavioral Testing in Machine Translation,,,"""Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",,,"""ChatGPT for Suicide Risk Assessment on Social Media"
GenIE: Generative Information Extraction,,,"""Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,,,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500...""; ""finding that"
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,,,"""We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models. Our approach could not beat the baseline results."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,,,"""We develop models for predicting veracity handling this ambiguity via soft labels... observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning."""
Language Varieties of Italy: Technology Challenges and Opportunities,,,The paper discusses the linguistic diversity in Italy and the need for a shift from machine-centric to speaker-centric NLP approaches. It does not specifically address language models or their limitations.
Benchmarking Large Language Models for News Summarization,,,"""the reasons behind their successes are poorly understood,"" ""existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance."""
mGPT: Few-Shot Learners Go Multilingual,,,"""The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia."""
Large Language Models of Code Fail at Completing Code with Potential Bugs,,,"""Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion... We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs... Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance."""
Cultural Adaptation of Recipes,,,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,,,The paper focuses on a metric-free method for few-shot Aspect Category Sentiment Analysis and does not mention language models or their limitations.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,,,"The abstract and title focus on calibration assessment and binning problems in computational linguistics models, without specifically mentioning language models (LMs or LLMs) or discussing their limitations."
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,,,The paper discusses an energy-based model for word-level autocompletion in computer-aided translation and does not mention language models (LMs or LLMs) or their limitations. The focus is on improving the use of context hidden vectors and efficiency/effectiveness challenges in their model.
Lost in the Middle: How Language Models Use Long Contexts,,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Red Teaming Language Model Detectors with Language Models,,,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."""
Text Attribute Control via Closed-Loop Disentanglement,,,"""However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,,,"""previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph)."""
Exploring Human-Like Translation Strategy with Large Language Models,,,"""Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,,,"""improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,,,"""Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations."" and ""We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,,,"""When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. ... Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,,,"""Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations."" and ""We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge."""
