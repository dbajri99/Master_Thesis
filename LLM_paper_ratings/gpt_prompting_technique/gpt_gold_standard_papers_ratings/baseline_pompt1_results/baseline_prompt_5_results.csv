Title,LMs,Limitations of LMs,Evidence
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,Yes.,1.,"""Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton."""
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,Yes.,2.,"""The identified challenges and future directions offer guidance for researchers and practitioners aiming to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence."""
Large Language Models Are Neurosymbolic Reasoners,Yes.,1.,The abstract and title discuss the use of Large Language Models (LLMs) as symbolic reasoners and their application in text-based games. There is no mention of limitations of LLMs in the provided text.
LLMs for Relational Reasoning: How Far are We?,yes.,5.,"""it is hard to conclude that the LLMs possess strong reasoning ability,"" ""LLMs are poor at solving sequential decision-making problems,"" ""the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization."""
Large Language Models in Plant Biology,yes.,1.,"""Large Language Models (LLMs), such as ChatGPT, have taken the world by storm and have passed certain forms of the Turing test. However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,Yes.,1.,"""This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. ... This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues."""
GeoGalactica: A Scientific Large Language Model in Geoscience,Yes.,1.,"""Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP)."""
Large Language Models for Generative Information Extraction: A Survey,Yes.,2.,"""We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms, then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies."""
Building Efficient Universal Classifiers with Natural Language Inference,Yes.,4.,"""Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks... while being significantly more efficient than generative LLMs."""
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,Yes.,4.,"""We also outline potential challenges and limitations in adopting LLMs for IS."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,Yes.,3.,"""Furthermore, we summarize the challenges and opportunities introduced by LLMs to provide a complete view of this design pattern for more discussions."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,yes.,3.,"""Despite their successful implementations, there is still a gap in existing literature on their effectiveness in low-resource languages."""
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,Yes.,3.,"""the existing models for ABSA face three main challenges, including domain-specificity, reliance on labeled data, and a lack of exploration into the potential of newer large language models (LLMs) such as GPT, PaLM, and T5."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,Yes.,2.,"""To combine the controllability of VAE latent spaces with the state-of-the-art performance of recent large language models (LLMs)... aiming to provide better text generation control to LLMs."""
A Comparative Analysis of Large Language Models for Code Documentation Generation,yes.,2.,"""Considering the time taken for generation, GPT-4 demonstrated the longest duration, followed by Llama2, Bard, with ChatGPT and Starchat having comparable generation times. Additionally, file level documentation had a considerably worse performance across all parameters (except for time taken) as compared to inline and function level documentation."""
TigerBot: An Open Multilingual Multitask LLM,Yes.,1.,"""We release and introduce the TigerBot family of large language models (LLMs)..."" and ""TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community."" The abstract primarily discusses the development and performance gains of the TigerBot LLMs without mentioning their limitations."
Efficiently Programming Large Language Models using SGLang,Yes.,3.,"""Large language models (LLMs) are increasingly used for complex tasks requiring multiple chained generation calls, advanced prompting techniques, control flow, and interaction with external environments. However, efficient systems for programming and executing these applications are lacking."""
Large Language Models on Graphs: A Comprehensive Survey,Yes.,4.,"""While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs... although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning)."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,Yes.,5.,"""We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons."""
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,Yes.,1.,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
