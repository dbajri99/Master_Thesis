Title,LMs,Limitations of LMs,Evidence
Generative Spoken Dialogue Language Modeling,Yes.,1.,"""We show that our model is able to generate speech, laughter, and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn taking compared to a text-based cascaded model."""
Discontinuous Combinatory Constituency Parsing,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or their limitations. The focus is on combinatory constituency parsing and the development of parsers for discontinuous structures.
Efficient Long-Text Understanding with Short-Text Models,Yes.,5.,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."""
Hate Speech Classifiers Learn Normative Social Stereotypes,Yes.,4.,"""Finally, we demonstrate how normative stereotypes embedded in language resources are associated with systematic prediction errors in a hate-speech classifier. The results demonstrate that hate-speech classifiers reflect social stereotypes against marginalized groups, which can perpetuate social inequalities when propagated at scale."""
Domain-Specific Word Embeddings with Structure Prediction,No.,1.,"The paper focuses on domain-specific word embeddings and structure prediction, and does not discuss language models or their limitations."
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,yes.,5.,"""This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times... These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,yes.,4.,"""it remains unclear whether they are robust to shifts in setting (sometimes to more realistic ones), training data size (e.g., from large to small sets) and domain. ... We find that high benchmark scores do not necessarily translate to strong robustness, and that various methods can perform extremely differently under different settings."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or their limitations. The focus is on semantic parsing and data synthesis for question answering.
Naturalistic Causal Probing for Morpho-Syntax,Yes.,2.,"""However, there is still a lack of understanding of the limitations and weaknesses of various types of probes."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,No.,1.,"""In this paper, we propose a novel dynamic Brand-Topic Model (dBTM) which is able to automatically detect and track brand-associated sentiment scores and polarity-bearing topics from product reviews organized in temporally ordered time intervals."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,No.,1.,"The abstract and title focus on human localization and dubbing practices, with insights for automatic dubbing. There is no mention of language models (LMs or LLMs) or their limitations."
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,Yes.,4.,"""Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not 'structurally ready' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This 'lack of readiness' results from the gap between language model pre-training and DPR fine-tuning."""
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,Yes.,4.,"""We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies."""
Sub-Character Tokenization for Chinese Pretrained Language Models,Yes.,2.,"""Tokenization is fundamental to pretrained language models (PLMs). Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level."""
Erasure of Unaligned Attributes from Neural Representations,No.,1.,"The abstract and title do not mention language models or their limitations. The focus is on the AMSAL algorithm for erasing information from neural representations, tested on various datasets."
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,Yes.,3.,"""The current models indeed suffer from spurious correlations and have a tendency to generate irrelevant and generic responses."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,Yes.,5.,"""Limitations of Log-Precision Transformers"" in the title and ""transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions"" in the abstract."
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,Yes.,4.,"""Neural sequence generation models are known to 'hallucinate', by producing outputs that are unrelated to the source text. These hallucinations are potentially harmful, yet it remains unclear in what conditions they arise and how to mitigate their impact."""
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,No.,1.,"""Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP)."""
