Title,LMs,Limitations of LMs,Evidence
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,Yes.,3.,"""However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs."""
Questions Are All You Need to Train a Dense Passage Retriever,Yes.,1.,"""generic initialization from a pre-trained language model"""
Transparency Helps Reveal When Language Models Learn Meaning,Yes.,5.,"""current language models do not represent natural language semantics well"" and ""this ability degrades"" when denotations are context-dependent."
Visual Spatial Reasoning,Yes.,5.,"""previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information"" and ""We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,Yes.,4.,"""Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions?...models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set...GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-"
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,No.,1.,The abstract and title focus on a benchmark for few-shot region-aware machine translation and do not specifically mention language models or their limitations.
OpenFact: Factuality Enhanced Open Knowledge Extraction,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or discuss their limitations. The focus is on factuality in Open Knowledge Extraction and the creation of the OpenFact corpus.
On Graph-based Reentrancy-free Semantic Parsing,No.,1.,"The abstract and title discuss a novel graph-based approach for semantic parsing and the limitations of seq2seq models and phrase structure parsers, but do not specifically address language models (LMs or LLMs) or their limitations."
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,No.,1.,"""In this paper, we propose a supervised GML approach for ATSA, which can effectively exploit labeled training data to improve knowledge conveyance."""
Chinese Idiom Paraphrasing,Yes.,1.,"The paper mentions ""three sequence-to-sequence methods as the baselines"" and a ""novel infill-based approach based on text infilling,"" which are indicative of the use of language models. However, it does not explicitly discuss their limitations."
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,Yes.,1.,"""In this study, we first collect human speakers’ answers on unknown Character naming tasks and then evaluate a set of transformer models by comparing their performance with human behaviors on an unknown Chinese character naming task."""
Rank-Aware Negative Training for Semi-Supervised Text Classification,No.,1.,The abstract and title focus on semi-supervised text classification and the Rank-aware Negative Training (RNT) framework. There is no mention of language models (LMs or LLMs) or their limitations.
MACSum: Controllable Summarization with Mixed Attributes,Yes.,3.,"""Results and analysis demonstrate that hard prompt models yield the best performance on most metrics and human evaluations. However, mixed-attribute control is still challenging for summarization tasks."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,Yes.,4.,"""Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness."""
Efficient Methods for Natural Language Processing: A Survey,yes.,4.,"""Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed."""
Abstractive Meeting Summarization: A Survey,Yes.,3.,"""In this paper, we provide an overview of the challenges raised by the task of abstractive meeting summarization and of the data sets, models, and evaluation metrics that have been used to tackle the problems."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,Yes.,1.,"""Using neural language models to approximate human predictive distributions..."""
Reasoning over Public and Private Data in Retrieval-Based Systems,No.,1.,"""State-of-the-art systems for these tasks explicitly retrieve information that is relevant to an input question from a background corpus before producing an answer. While today’s retrieval systems assume relevant corpora are fully (e.g., publicly) accessible, users are often unable or unwilling to expose their private data to entities hosting public data."""
Multilingual Coreference Resolution in Multiparty Dialogue,no.,1.,The paper focuses on creating a dataset for multilingual coreference resolution in multiparty dialogue and discusses the performance of models on this dataset. It does not specifically address language models or their limitations.
