Title,Talks about LLMs,Rate,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,Yes.,2.,"""handle the typical constraints of using LLMs, such as context limitations."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,Yes.,3.,"""Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive."" and ""relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains"
Re3val: Reinforced and Reranked Generative Retrieval,No.,N/A.,"The abstract discusses generative retrieval models and their limitations, but it does not specifically mention language models (LMs or LLMs) or their limitations. The focus is on generative retrieval and reranking techniques."
Reward Engineering for Generating Semi-structured Explanation,Yes.,2.,"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL)."""
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,Yes.,4.,"""LLMs excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations."" and ""Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages"
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,Yes.,2.,"""Our approach stands out by eliminating the need for parameter updates in LMs, as required in fine-tuning, and does not impose limitations on the number of training examples faced while building prompts for in-context learning."""
Evaluating Large Language Models Trained on Code,Yes.,3.,"""Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,Yes.,2.,"""current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications."""
ICE-Score: Instructing Large Language Models to Evaluate Code,Yes.,3.,"""Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment."""
Transformer-specific Interpretability,Yes.,2.,"""In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today."" and ""We start by discussing the potential pitfalls and misleading results model-agnostic approaches may produce when interpreting Transformers."""
Can docstring reformulation with an LLM improve code generation?,Yes.,3.,"""Our results show that, when operating on docstrings reformulated by an LLM instead of the original (or worsened) inputs, the performance of a number of open-source LLMs does not change significantly. This finding demonstrates an unexpected robustness of current open-source LLMs to the details of the docstrings."" and ""We conclude by examining a series of questions, accompanied"
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,4.,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount."""
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,Yes.,3.,"""employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size."""
Document-Level Language Models for Machine Translation,Yes.,1.,"""Finally, we explore language model fusion in the light of recent advancements in large language models."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,Yes.,5.,"""GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language{'}s resource level is the most important feature in determining ChatGPT{'}s relative ability to translate it, and suggests that"
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",,,
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",Yes.,1.,"""While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored."""
