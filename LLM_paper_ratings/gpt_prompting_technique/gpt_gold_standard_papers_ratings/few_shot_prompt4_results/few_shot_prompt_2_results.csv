Title,Talks about LLMs,Rate,Evidence
Automating Behavioral Testing in Machine Translation,Yes.,2.,"""To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."" and ""revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying"
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",,,
GenIE: Generative Information Extraction,Yes.,1.,"""GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,2.,"""we evaluate three facets of adaptability"" and ""failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,Yes.,1.,"""We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,No.,1.,The abstract does not mention large language models (LLMs) or any limitations associated with them. It focuses on fact-checking ambiguous claims with evidence and developing models to handle ambiguity.
Language Varieties of Italy: Technology Challenges and Opportunities,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. The focus is on the linguistic diversity of Italy and the challenges and opportunities for NLP in this context.
Benchmarking Large Language Models for News Summarization,,,
mGPT: Few-Shot Learners Go Multilingual,Yes.,1.,"The abstract focuses on introducing and evaluating the mGPT model, detailing its design, pretraining procedure, and performance across various languages. It does not mention any limitations of LLMs."
Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5.,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential"
Cultural Adaptation of Recipes,Yes.,2.,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,,,
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,No.,1.,"The abstract focuses on calibration assessment in computational linguistics models, particularly addressing the binning problem and proposing solutions involving scalar annotations. It does not mention large language models or their limitations."
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,No.,1.,The abstract discusses an energy-based model for Word-level AutoCompletion (WLAC) in Computer-aided Translation. It does not mention large language models (LLMs) or their limitations.
Lost in the Middle: How Language Models Use Long Contexts,,,
Red Teaming Language Model Detectors with Language Models,Yes.,4.,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users"" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."""
Text Attribute Control via Closed-Loop Disentanglement,No.,1.,The abstract focuses on a novel approach for text attribute control via closed-loop disentanglement and does not mention large language models or their limitations.
Unifying Structured Data as Graph for Data-to-Text Pre-Training,No.,1.,The abstract discusses data-to-text generation and the use of structured data in graph format for pre-training but does not mention large language models (LLMs) or their limitations.
Exploring Human-Like Translation Strategy with Large Language Models,Yes.,2.,"""Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,Yes.,1.,"""improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,,,
