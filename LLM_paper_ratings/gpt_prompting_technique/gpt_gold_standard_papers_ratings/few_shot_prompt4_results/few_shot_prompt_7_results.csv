Title,Talks about LLMs,Rate,Evidence
Instruction Tuning with GPT-4,,,
Exploring Language Models: A Comprehensive Survey and Analysis,,,
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",,,
Challenges and Limitations of ChatGPT and Other Large Language Models,Yes.,5.,"""This article explores the challenges and limitations of large language models, focusing on ChatGPT as a representative example. ... we also acknowledge the concerns around these models, including their environmental impact, potential for bias, and lack of interpretability. ... including limitations in their understanding of context, difficulty in handling rare or out-of-vocabulary words, and their tendency to generate nonsensical or offensive"
Document-Level Machine Translation with Large Language Models,Yes.,2.,"""This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs."""
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,Yes.,1.,The abstract focuses on the development and evaluation of adapter-based parameter-efficient fine-tuning (PEFT) methods for LLMs. It discusses the performance and effectiveness of these methods but does not mention any limitations of LLMs.
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,No.,1.,"The abstract discusses Retrieval Augmented Generation (RAG) models and their domain adaptation for Open-Domain Question Answering (ODQA), but it does not mention large language models or their limitations."
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,Yes.,2.,"""our experiments on French agreements show that (i) the agreement task suffers from several confounders that partially question the conclusions drawn so far."""
On the Role of Negative Precedent in Legal Outcome Prediction,No.,1.,The abstract discusses legal outcome prediction and the performance of models in predicting positive and negative outcomes. It does not mention large language models or their limitations.
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,No.,1.,The abstract focuses on cross-lingual semantic parsing and meta-learning algorithms for training semantic parsers. It does not mention large language models or their limitations.
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,Yes.,1.,"""We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases."""
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on a novel architecture for pronouncing geographic features and applying it to different tasks.
Locally Typical Sampling,Yes.,2.,"""Today’s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,,,
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,No.,1.,"The abstract focuses on the creation and evaluation of a multilingual task-oriented dialogue dataset and does not mention large language models (LLMs) or their limitations. The discussion revolves around dataset creation, annotation processes, and benchmarking of dialogue systems."
Modeling Emotion Dynamics in Song Lyrics with State Space Models,No.,1.,The abstract does not mention large language models or their limitations. It focuses on modeling emotion dynamics in song lyrics using State Space Models.
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,No.,1.,"The abstract discusses a multimodal dataset and a task related to the emotional connotation of color in context, but it does not mention large language models (LLMs) or their limitations."
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,,,
Coreference Resolution through a seq2seq Transition-Based System,,,
Transformers for Tabular Data Representation: A Survey of Models and Applications,Yes.,1.,"""In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs)."" The abstract focuses on the application of LMs to tabular data without mentioning any limitations of LLMs."
