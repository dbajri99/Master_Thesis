Title,Talks about LLMs,Rate,Evidence
Generative Spoken Dialogue Language Modeling,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It discusses a model for generating audio samples of spoken dialogues using unsupervised spoken unit discovery and a dual-tower transformer architecture.
Discontinuous Combinatory Constituency Parsing,No.,1.,The abstract focuses on combinatory constituency parsing and does not mention large language models or their limitations.
Efficient Long-Text Understanding with Short-Text Models,Yes.,2.,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."""
Hate Speech Classifiers Learn Normative Social Stereotypes,No.,1.,"The abstract discusses the role of social stereotypes in hate speech classifiers, the impact of stereotypes on annotation behaviors, and systematic prediction errors in hate-speech classifiers. There is no mention of large language models or their limitations."
Domain-Specific Word Embeddings with Structure Prediction,No.,1.,The abstract discusses domain-specific word embeddings and structure prediction methods but does not mention large language models (LLMs) or their limitations. The focus is on dynamic word embeddings and their applications.
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5.,"""This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times."" and ""These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained"
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,No.,1.,The abstract does not mention large language models (LLMs) or any limitations associated with them. It focuses on the robustness of dialogue history representation in Conversational Question Answering (CQA) and the design of a new prompt-based method.
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,No.,1.,The abstract focuses on semantic parsing and data synthesis methods to improve zero-shot learning and generalization to natural questions. It does not mention large language models (LLMs) or their limitations.
Naturalistic Causal Probing for Morpho-Syntax,Yes.,1.,"""Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes."" The abstract focuses on probing methodologies and does not specifically address the limitations of the language models themselves."
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,No.,1.,The abstract does not mention large language models or any limitations related to them. It focuses on a dynamic Brand-Topic Model (dBTM) for tracking brand-associated sentiment scores and polarity-bearing topics in user reviews.
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,No.,1.,The abstract discusses human dubbing and automatic dubbing systems but does not mention large language models (LLMs) or their limitations.
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,Yes.,2.,"""However, recent work has shown that models such as BERT are not 'structurally ready' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This 'lack of readiness' results from the gap between language model pre-training and DPR fine-tuning."""
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,No.,1.,"The abstract does not mention large language models (LLMs) or their limitations. It focuses on a dataset for information-seeking conversations and the performance of conversational models, but does not specify if these models are LLMs or discuss their limitations."
Sub-Character Tokenization for Chinese Pretrained Language Models,Yes.,1.,The abstract discusses the proposal of a new tokenization method for Chinese pretrained language models (PLMs) and mentions their advantages but does not address any limitations of the language models themselves. The focus is on improving tokenization rather than discussing the broader limitations of LLMs.
Erasure of Unaligned Attributes from Neural Representations,No.,1.,"The abstract does not mention large language models (LLMs) or any specific limitations related to them. Instead, it focuses on the AMSAL algorithm for erasing information from neural representations."
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,No.,1.,"The abstract does not mention large language models (LLMs) or any specific limitations related to them. It focuses on open-domain response generation models and their issues with spurious correlations, proposing a novel method to address these issues."
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,Yes.,4.,"""We thus speculatively introduce the idea of a fundamental parallelism tradeoff"
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,No.,1.,The abstract discusses neural sequence generation models in the context of neural machine translation (NMT) and hallucinations but does not specifically mention large language models (LLMs) or their limitations.
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,,,
