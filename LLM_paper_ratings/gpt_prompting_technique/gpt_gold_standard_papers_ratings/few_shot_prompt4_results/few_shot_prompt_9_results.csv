Title,Talks about LLMs,Rate,Evidence
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,No.,1.,The abstract discusses Sequence-to-Sequence (S2S) models and their application to sequence tagging and structure parsing tasks. It does not mention Large Language Models (LLMs) or their limitations.
Questions Are All You Need to Train a Dense Passage Retriever,Yes.,1.,"""Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses."""
Transparency Helps Reveal When Language Models Learn Meaning,Yes.,4.,"""Our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well. We show this failure relates to the context-dependent nature of natural language form-meaning mappings."""
Visual Spatial Reasoning,Yes.,3.,"""previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information"" and ""We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,Yes.,2.,"""For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set,"" and ""models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,No.,1.,The abstract does not mention large language models or their limitations. It focuses on a dataset and evaluation benchmark for Few-shot Region-aware Machine Translation.
OpenFact: Factuality Enhanced Open Knowledge Extraction,No.,1.,The abstract focuses on the factuality property during the extraction of an OpenIE corpus named OpenFact and does not mention large language models (LLMs) or their limitations.
On Graph-based Reentrancy-free Semantic Parsing,No.,1.,The abstract focuses on a novel graph-based approach for semantic parsing and does not mention large language models (LLMs) or their limitations.
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,No.,1.,The abstract does not mention large language models (LLMs) or any related terminology. It focuses on Gradual Machine Learning (GML) for Aspect-Term Sentiment Analysis (ATSA) and does not address any limitations of LLMs.
Chinese Idiom Paraphrasing,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on Chinese Idiom Paraphrasing (CIP) and its application in improving Chinese NLP tasks.
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,,,
Rank-Aware Negative Training for Semi-Supervised Text Classification,No.,1.,"The abstract discusses semi-supervised text classification, focusing on self-training and handling noisy labels. It does not mention large language models or their limitations."
MACSum: Controllable Summarization with Mixed Attributes,No.,1.,The abstract focuses on controllable summarization and the creation of a new dataset for this purpose. It does not mention large language models or their limitations.
MENLI: Robust Evaluation Metrics from Natural Language Inference,No.,1.,"The abstract discusses BERT-based evaluation metrics and their vulnerabilities to adversarial attacks, as well as the development of NLI-based metrics. It does not mention large language models or their limitations."
Efficient Methods for Natural Language Processing: A Survey,No.,1.,The abstract discusses the efficiency of natural language processing methods in general and does not specifically mention large language models (LLMs) or their limitations.
Abstractive Meeting Summarization: A Survey,No.,1.,"The abstract discusses the general advancements in deep learning and encoder-decoder architectures for language generation systems, but it does not specifically mention large language models (LLMs) or their limitations."
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,Yes.,1.,"""Using neural language models to approximate human predictive distributions"" indicates the use of language models but does not discuss any limitations of these models."
Reasoning over Public and Private Data in Retrieval-Based Systems,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. The focus is on retrieval-based systems and the challenges of handling public and private data.
Multilingual Coreference Resolution in Multiparty Dialogue,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It discusses the creation of a dataset for multilingual coreference resolution in multiparty dialogue and the performance of off-the-shelf models on this dataset.
