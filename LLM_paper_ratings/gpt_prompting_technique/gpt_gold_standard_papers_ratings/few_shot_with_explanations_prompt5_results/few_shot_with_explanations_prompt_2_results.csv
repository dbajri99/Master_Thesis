Title,Talks about LLMs,Rate,Evidence
Automating Behavioral Testing in Machine Translation,Yes.,1.,"""we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",Yes.,3.,"""Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance."""
GenIE: Generative Information Extraction,Yes.,1.,"""GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,3.,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500,"" and ""failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,Yes.,1.,"""We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,No.,N/A.,The abstract does not mention language models or their limitations.
Language Varieties of Italy: Technology Challenges and Opportunities,No.,N/A.,The abstract does not mention language models or their limitations.
Benchmarking Large Language Models for News Summarization,Yes.,3.,"""existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance."""
mGPT: Few-Shot Learners Go Multilingual,Yes.,1.,"""This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus."""
Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5.,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs."""
Cultural Adaptation of Recipes,Yes.,3.,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,,,
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,No.,N/A.,"The abstract does not mention language models, large language models, or any specific limitations related to them. It focuses on calibration assessment and binning problems in computational linguistics models."
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,No.,N/A.,The abstract does not mention language models or their limitations.
Lost in the Middle: How Language Models Use Long Contexts,,,
Red Teaming Language Model Detectors with Language Models,Yes.,4.,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""we investigate the robustness and reliability of these LLM detectors under adversarial attacks."""
Text Attribute Control via Closed-Loop Disentanglement,,,
Unifying Structured Data as Graph for Data-to-Text Pre-Training,No.,N/A.,The abstract does not mention language models (LLMs or LMs) or their limitations. It focuses on data-to-text generation and structured data pre-training methods.
Exploring Human-Like Translation Strategy with Large Language Models,Yes.,1.,"""Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,Yes.,1.,"""improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,Yes.,5.,"""However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations."" and ""showing that they fail to introduce consistent changes in the model’s knowledge."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,Yes.,3.,"""Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words."""
