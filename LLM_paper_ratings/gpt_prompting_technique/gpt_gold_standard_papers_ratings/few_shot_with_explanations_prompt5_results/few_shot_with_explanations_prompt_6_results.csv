Title,Talks about LLMs,Rate,Evidence
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,Yes.,3.,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,Yes.,3.,"""researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets"" and ""off-the-shelf LLMs do not fully understand these challenging ranking formulations."""
Preference Ranking Optimization for Human Alignment,Yes.,4.,"""Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks"
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",,,
Concept-Oriented Deep Learning with Large Language Models,Yes.,3.,"""Text-only LLMs, however, can represent only symbolic (conceptual) knowledge."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,Yes.,5.,"""Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area."" and ""Unlike task-specific models, there lack an effective method to calibrate the confidence level of L"
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,Yes.,1.,"""Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning, including ChatGPT, a chat-based model built on top of LLMs such as GPT-3.5 and GPT-4."""
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,Yes.,4.,"""they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM"" and ""synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,Yes.,1.,"""we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,Yes.,5.,"""The observed differences highlight LLM limitations in understanding context and addressing ambiguity."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,Yes.,1.,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs."""
Exploring the Robustness of Large Language Models for Solving Programming Problems,Yes.,5.,"""Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance."" and ""This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high"
Language models are weak learners,Yes.,1.,"""In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners."""
Teaching Large Language Models to Self-Debug,Yes.,3.,"""Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,Yes.,4.,"""effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and"
On the Possibilities of AI-Generated Text Detection,Yes.,3.,"""Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text,"" and ""as machine-generated text approximates human-like quality, the sample size needed for detection increases."""
Learnings from Data Integration for Augmented Language Models,Yes.,5.,"""One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,Yes.,5.,"""existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}."""
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,Yes.,2.,"""This has enabled the research community to identify significant and reproducible advancements in LLM capabilities. However, there is currently a lack of benchmark datasets for assessing the ability of LLMs to generate functionally correct code edits based on natural language descriptions of intended changes."""
Revisiting Automated Prompting: Are We Actually Doing Better?,Yes.,3.,"""We find that automated prompting does not consistently outperform simple manual prompts."""
