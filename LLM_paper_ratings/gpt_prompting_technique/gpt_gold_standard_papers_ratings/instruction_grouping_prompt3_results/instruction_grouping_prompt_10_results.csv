Title,Talks about LLMs,Rate,Evidence
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,Yes.,2.,"""existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models"" and ""Further analysis shows that PreDAT benefits from the unbiased prediction order that alleviates the error accumulation problem in autoregressive generation."""
Time-and-Space-Efficient Weighted Deduction,No.,1.,The abstract does not mention large language models or their limitations. It focuses on weighted deduction strategies in NLP algorithms and their efficiency in terms of time and space.
Conditional Generation with a Question-Answering Blueprint,Yes.,2.,"The abstract mentions that ""neural seq-to-seq models...often reveal hallucinations and fail to correctly cover important details,"" indicating some limitations of these models. However, these limitations are mentioned as secondary points and not discussed in detail."
Collective Human Opinions in Semantic Textual Similarity,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on semantic textual similarity (STS) and the introduction of an uncertainty-aware dataset for STS.
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,No.,1.,"The abstract focuses on biases introduced by task design in crowdsourced linguistic annotations, specifically in the context of implicit discourse relation annotation. It does not discuss large language models or their limitations."
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,No.,1.,"The abstract focuses on neural agents and their simulations of language emergence and change, specifically addressing the word-order/case-marking trade-off. It does not mention large language models or their limitations."
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,No.,1.,The abstract discusses the uniform information density hypothesis and word order patterns in natural languages. There is no mention of large language models or their limitations.
Cross-functional Analysis of Generalization in Behavioral Learning,No.,1.,"The abstract does not mention large language models (LLMs) or their limitations. The focus is on behavioral learning, generalization, and evaluation methods for NLP tasks, but there is no specific mention of LLMs or their limitations."
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,Yes.,2.,"""Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,Yes.,1.,"The abstract mentions the use of a masked language model and discusses a novel compositional transfer learning framework. However, it does not discuss any limitations of large language models. The focus is on the proposed method's effectiveness and performance improvements."
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,No.,1.,The abstract discusses a multilingual retrieval dataset for ad hoc retrieval tasks across various languages and emphasizes the creation and quality control of the dataset. It does not mention large language models (LLMs) or their limitations.
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on dataset mention detection and the introduction of a new dataset for this task.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,Yes.,3.,"""However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",No.,1.,The abstract discusses mathematical language processing methods and their sub-areas but does not mention large language models (LLMs) or their limitations.
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,No.,1.,The abstract focuses on the cognitive science of adjective ordering and evaluates various proposals in this area using natural language processing technologies and datasets. It does not mention large language models or their limitations.
Improving Multitask Retrieval by Promoting Task Specialization,No.,1.,"The abstract discusses multitask retrieval and the methods to improve it, but it does not mention large language models or their limitations."
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,Yes.,2.,"The abstract mentions ""Sequence generation models"" which include pre-trained transformer-based language models. It discusses the calibration of these models and indicates that there are variations in calibration error across models and datasets. However, the limitations of these models are mentioned as secondary points, focusing mainly on the aspect of calibration rather than a broad discussion of limitations."
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,No.,1.,The abstract focuses on answer selection in open-domain dialogues and introduces a self-training paradigm using intent labels. It does not mention large language models (LLMs) or their limitations.
Benchmarking the Generation of Fact Checking Explanations,No.,1.,The abstract does not mention large language models (LLMs) or any limitations related to them. It focuses on the generation of fact-checking explanations and summarization approaches without discussing LLMs or their limitations.
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,No.,1.,The abstract focuses on a two-stage span-based framework for unified Named Entity Recognition (NER) and does not mention large language models or their limitations.
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,Yes.,3.,"""Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual)."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,No.,1.,The abstract does not mention large language models or their limitations. It focuses on a novel framework for Open Relation Extraction using Contrastive Learning and Clustering techniques.
In-Context Retrieval-Augmented Language Models,Yes.,2.,"""In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism."" This mentions a limitation (factually inaccurate text generation) but does not delve deeply into it within the abstract. The primary focus is on the method and its benefits rather than an in-depth discussion of limitations."
Learning to Paraphrase Sentences to Different Complexity Levels,Yes.,1.,"""Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,No.,1.,"The abstract does not mention large language models (LLMs) or their limitations. It focuses on a direct speech translation model for automatic subtitling, discussing its performance and comparison with other systems."
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,yes.,4.,"""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations... This finding points to a limitation with current models and points to a reason for which their training is data-intensive."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",No.,1.,"The abstract does not mention large language models or their limitations. It focuses on the creation and evaluation of a multilingual, multi-domain, multi-parallel task-oriented dialog dataset."
Can Authorship Representation Learning Capture Stylistic Features?,No.,1.,The abstract focuses on authorship representation learning and its ability to capture stylistic features. It does not mention or discuss large language models (LLMs) or their limitations.
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,no.,1.,The abstract focuses on cross-lingual semantic parsing and methods to improve performance using Optimal Transport. It does not mention large language models or their limitations.
