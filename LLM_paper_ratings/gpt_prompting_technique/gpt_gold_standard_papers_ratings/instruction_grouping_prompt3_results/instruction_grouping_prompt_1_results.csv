Title,Talks about LLMs,Rate,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,Yes.,2.,"""The dialogue system is constructed on top of LLMs, focusing on defining specific roles for individual modules. We show that using multiple independent sub-modules working cooperatively on this task can improve performance and handle the typical constraints of using LLMs, such as context limitations."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,Yes.,4.,"""Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains."
Re3val: Reinforced and Reranked Generative Retrieval,**Yes**.,**3**.,"""However, we identify two limitations"
Reward Engineering for Generating Semi-structured Explanation,Yes.,3.,"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge..."""
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,Yes.,4.,"""Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations."" and ""Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and"
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,Yes.,2.,"""Our approach stands out by eliminating the need for parameter updates in LMs, as required in fine-tuning, and does not impose limitations on the number of training examples faced while building prompts for in-context learning."""
Evaluating Large Language Models Trained on Code,Yes.,4.,"""Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,Yes.,2.,"""However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications."""
ICE-Score: Instructing Large Language Models to Evaluate Code,Yes.,3.,"The abstract mentions that ""their applicability in code intelligence tasks remains limited without human involvement"" and discusses the difficulty in developing evaluation metrics that align with human judgment. While the primary focus is on proposing a new evaluation metric, the abstract does touch upon limitations of LLMs in the context of code intelligence tasks."
Transformer-specific Interpretability,Yes.,4.,"""In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today."" and ""We start by discussing the potential pitfalls and misleading results model-agnostic approaches may produce when interpreting Transformers."""
Can docstring reformulation with an LLM improve code generation?,Yes.,3.,"""We conclude by examining a series of questions, accompanied by in-depth analyses, pertaining to the sensitivity of current open-source LLMs to the details in the docstrings, the potential for improvement via docstring reformulation and the limitations of the methods employed in this work."""
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,Yes.,4.,"""Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely"
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,Yes.,4.,"""However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size."" The abstract explicitly discusses the limitations of LLMs, particularly focusing on the immense computational resources required for large-scale inference and domain-specific fine-tuning."
Document-Level Language Models for Machine Translation,Yes.,2.,"The abstract mentions ""Despite the known limitations, most machine translation systems today still operate on the sentence-level,"" and ""we explore language model fusion in the light of recent advancements in large language models."" However, it does not go into detail about the limitations of large language models, only acknowledging their existence and potential."
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,Yes.,4.,"""Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language{'}s resource level is the most important feature in determining ChatGPT{'}s relative ability to translate it"
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",yes.,4.,"""However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. ... critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",Yes.,1.,"The abstract discusses the use of large language models in the context of machine translation and compares different methodologies like zero-shot prompting, few-shot learning, and fine-tuning. However, it does not mention any limitations of large language models."
