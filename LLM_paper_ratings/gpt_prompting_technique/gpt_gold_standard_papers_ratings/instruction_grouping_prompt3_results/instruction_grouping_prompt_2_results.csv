Title,Talks about LLMs,Rate,Evidence
Automating Behavioral Testing in Machine Translation,Yes.,2.,"""Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages."" This suggests a limitation in the current testing methods, but the paper does not focus extensively on the limitations of LLMs themselves. The abstract primarily discusses the application of LLMs for improving behavioral testing in MT rather than delving deeply"
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",Yes.,3.,"""Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance."" This indicates that the paper mentions some limitations of ChatGPT (a large language model) in comparison to fine-tuned transformer-based models, but it is not the primary focus of the abstract."
GenIE: Generative Information Extraction,Yes.,2.,"""Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,2.,"The abstract discusses the ""features and limits of LM adaptability"" and mentions specific limitations such as ""failure to match training label distributions"" and ""mismatches in the intrinsic difficulty of predicting individual labels."" However, these limitations are mentioned as part of a broader study on adaptability rather than being the central focus."
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,Yes.,1.,"""We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,No.,1.,"The abstract does not mention large language models or their limitations. It focuses on fact-checking systems, ambiguity in evidence, and annotator disagreements."
Language Varieties of Italy: Technology Challenges and Opportunities,No.,1.,The abstract does not mention large language models or their limitations. It focuses on the linguistic diversity in Italy and the challenges of applying NLP to endangered languages and dialects.
Benchmarking Large Language Models for News Summarization,Yes.,3.,"""First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance."""
mGPT: Few-Shot Learners Go Multilingual,**Yes**.,**1**.,"The abstract discusses the introduction, design, pretraining, and evaluation of the mGPT model, but it does not mention any limitations of large language models. It focuses on the capabilities and performance of the model across various languages and tasks."
Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5.,"""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential"
Cultural Adaptation of Recipes,Yes.,2.,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,no.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on a metric-free method for few-shot aspect category sentiment analysis and discusses issues related to aspect embeddings and sentiment noise.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,No.,1.,"The abstract does not mention large language models (LLMs) or their limitations. It focuses on calibration assessment and binning problems in computational linguistics models, without specifying LLMs."
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on an energy-based model for word-level autocompletion in computer-aided translation.
Lost in the Middle: How Language Models Use Long Contexts,Yes.,5.,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for"
Red Teaming Language Model Detectors with Language Models,Yes.,4.,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."""
Text Attribute Control via Closed-Loop Disentanglement,No.,1.,The abstract focuses on text attribute control and disentanglement using a semi-supervised contrastive learning method. There is no mention of large language models or their limitations.
Unifying Structured Data as Graph for Data-to-Text Pre-Training,No.,1.,The abstract focuses on data-to-text pre-training and the transformation of structured data into natural language text. It discusses the unification of structured data into a graph format and the design of a structure-enhanced Transformer. There is no mention of large language models (LLMs) or their limitations.
Exploring Human-Like Translation Strategy with Large Language Models,Yes.,2.,"The abstract mentions that ""MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission."" However, these limitations are mentioned as secondary points and not discussed in detail. The primary focus of the paper is on exploring human-like translation strategies with LLMs and proposing the MAPS framework."
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,Yes.,1.,"The abstract mentions large-scale pre-trained language models (e.g., ChatGPT) but does not discuss any limitations of them. The focus is on improving performance via a mutual learning framework."
Evaluating the Ripple Effects of Knowledge Editing in Language Models,Yes.,4.,"""Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations... We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,Yes.,3.,"""We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words."""
