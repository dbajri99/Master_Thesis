Title,Talks about LLMs,Rate,Evidence
Large Language Models Enable Few-Shot Clustering,Yes.,1.,"The abstract discusses the use of large language models (LLMs) in the context of semi-supervised text clustering and how they can improve clustering quality. However, it does not mention any limitations of LLMs. The focus is on the effectiveness and benefits of using LLMs in this application."
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,Yes.,1.,The abstract discusses the application of a language model (JustiLM) for justification generation in fact-checking but does not mention any limitations of large language models. The focus is on the performance and capabilities of the model rather than its limitations.
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,No.,1.,The abstract discusses a comparative analysis of machine translations (MTs) versus human translations (HTs) focusing on morphosyntactic divergence. It does not mention large language models (LLMs) or their limitations.
What Do Self-Supervised Speech Models Know About Words?,No.,1.,"The abstract discusses self-supervised speech models (S3Ms) and their encoding of linguistic properties, but it does not mention large language models (LLMs) or their limitations."
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,Yes.,2.,"The abstract mentions the efficiency tradeoff of byte models and suggests their usage in non-time-critical scenarios, which implies some limitations related to the efficiency of these models. However, the discussion of limitations is not a central focus of the abstract."
Geographic Adaptation of Pretrained Language Models,Yes.,2.,"""the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,Yes.,2.,"""Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks among the automatic controllable TS systems. However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as 'unanswerable' based on simplified content."""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,Yes.,2.,"""We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,Yes.,1.,"""Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM"
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,Yes.,4.,"""We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. ... Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,Yes.,1.,"The abstract primarily discusses the use of Large Language Models (LLMs) for extracting social determinants of health from pediatric patient notes. However, it does not mention any limitations of LLMs. The focus is on the performance and methodology rather than the limitations."
Fairness in Large Language Models: A Taxonomic Survey,Yes.,4.,"""Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs... Additionally, the concept of fairness in LLMs is discussed categorically, summar"
Algorithmic Collusion by Large Language Models,Yes.,2.,"The abstract mentions that ""variation in seemingly innocuous phrases in LLM instructions ('prompts') may increase collusion"" and discusses regulatory challenges unique to LLM-based pricing agents. However, the limitations of LLMs are not the primary focus and are mentioned more as secondary points related to their potential for collusion and regulatory needs."
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,Yes.,2.,"""Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs."""
Can Language Models Recognize Convincing Arguments?,Yes.,2.,"""The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,Yes.,4.,"""However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,Yes.,4.,"""Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,Yes.,4.,"""However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Yes.,4.,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,4.,"""This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning."""
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,Yes.,2.,"The abstract mentions ""ameliorate the noise inherent in manual annotations"" and ""addresses the issue of class imbalance encountered in LLM-based annotations."" These points suggest some limitations of LLMs, but they are mentioned as secondary points and not discussed in detail."
ST-LLM: Large Language Models Are Effective Temporal Learners,Yes.,4.,"""To address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness."""
