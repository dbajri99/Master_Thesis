Title,Talks about LLMs,Rate,Evidence
A Survey of using Large Language Models for Generating Infrastructure as Code,Yes.,2.,"""In this survey, we delve into the details of IaC, usage of IaC in different platforms, their challenges, LLMs in terms of code-generation aspects and the importance of LLMs in IaC along with our own experiments. Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,4.,"""However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date."" This indicates a significant limitation of LLMs, which the paper addresses in detail through the investigation of Supervised Fine-Tuning (SFT) as a method for knowledge injection."
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",yes.,1.,"The abstract discusses the use of OpenAI's GPT-3.5 as a ""Language Data Scientist"" and its ability to answer data science queries effectively. However, it does not mention any limitations of the model. The focus is on the model's potential and success in performing tasks, without addressing any shortcomings or constraints."
On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,4.,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data."" This sentence explicitly mentions the limitations of LLMs in the context of biomedical Named Entity Recognition (NER) tasks. The paper discusses these limitations in the context of improving LLM performance through definition augmentation and careful prompting strategies."
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4.,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior."""
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,Yes.,4.,"""Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,Yes.,4.,"""Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment"
FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5.,"""The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. ... This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. ... To address this, we introduces a new type of TE called ``Factual Entailment (FE).'', aims to detect factual inaccuracies in"
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4.,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,Yes.,2.,"The abstract mentions that Retrieval-Augmented Generation (RAG) aims to ""generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge."" This indicates a recognition of some limitations of LLMs in terms of reliability and accuracy, but these limitations are mentioned as secondary points and not discussed in detail. The primary focus is on"
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,Yes.,1.,"""Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling"
PropTest: Automatic Property Testing for Improved Visual Programming,Yes.,1.,"The abstract mentions the use of Large Language Models (LLMs) and their application in visual programming and reasoning tasks. However, it does not discuss any limitations of LLMs. The focus is on the proposed method (PropTest) and its performance improvements."
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,Yes.,3.,"""Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks... Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism"
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,yes.,1.,"The abstract mentions the ""impressive performance of large language models (LLMs) on code-related tasks"" but does not discuss any limitations of LLMs. The focus is on the introduction of the new task (NL2Repo) and the proposed framework (CodeS), along with its evaluation methods."
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",**Yes**.,**1**.,"""We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images."" The abstract does not mention any limitations of LLMs; instead, it focuses on their strengths and how they are used to enhance chart VQA models."
ChatDBG: An AI-Powered Debugging Assistant,Yes.,1.,"The abstract mentions that ChatDBG ""integrates large language models (LLMs)"" to enhance debugging capabilities but does not discuss any limitations of the LLMs. There is no mention of the limitations or challenges associated with using LLMs in this context."
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,Yes.,2.,"""A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments."""
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,Yes.,2.,"The abstract mentions the complexity behind factual recall in LLMs and the need for multiple mechanisms to retrieve facts accurately. However, it does not focus extensively on the limitations of LLMs but rather on the mechanisms behind their factual recall capabilities. The discussion of limitations is more of a secondary point."
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,Yes.,3.,"""Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,Yes.,3.,"""Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor."""
