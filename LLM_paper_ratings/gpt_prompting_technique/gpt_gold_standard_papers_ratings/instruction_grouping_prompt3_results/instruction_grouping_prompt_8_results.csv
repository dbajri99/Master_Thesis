Title,Talks about LLMs,Rate,Evidence
Generative Spoken Dialogue Language Modeling,Yes.,1.,The abstract does not discuss any limitations of large language models. It focuses on introducing a new “textless” model for generating audio samples of spoken dialogues and highlights its capabilities without mentioning any limitations.
Discontinuous Combinatory Constituency Parsing,No.,1.,The abstract focuses on extending combinatory constituency parsers and does not mention language models or their limitations.
Efficient Long-Text Understanding with Short-Text Models,Yes.,4.,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."" This clearly discusses a significant limitation of current LMs in handling long sequences."
Hate Speech Classifiers Learn Normative Social Stereotypes,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on the impact of social stereotypes on hate speech classifiers and annotation behaviors.
Domain-Specific Word Embeddings with Structure Prediction,No.,1.,The abstract does not mention large language models or their limitations. It focuses on domain-specific word embeddings and structure prediction.
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,Yes.,5.,"""This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. ... These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language"
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,Yes.,3.,"""While existing models show impressive results on CQA leaderboards, it remains unclear whether they are robust to shifts in setting (sometimes to more realistic ones), training data size (e.g., from large to small sets) and domain. ... We find that high benchmark scores do not necessarily translate to strong robustness, and that various methods can perform extremely differently under different settings."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on semantic parsing and data synthesis for zero-shot learning.
Naturalistic Causal Probing for Morpho-Syntax,Yes.,2.,"""However, there is still a lack of understanding of the limitations and weaknesses of various types of probes."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on a novel dynamic Brand-Topic Model (dBTM) for tracking sentiment and topics in user reviews.
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,,,
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,Yes.,4.,"""Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not 'structurally ready' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This 'lack of readiness' results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call"
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,Yes.,2.,"""We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies."""
Sub-Character Tokenization for Chinese Pretrained Language Models,Yes.,2.,"The paper discusses tokenization methods for Chinese pretrained language models and proposes a new sub-character tokenization method. It mentions improvements in computational efficiency and robustness to homophone typos, but it does not focus extensively on the limitations of large language models. Limitations are mentioned indirectly as part of the motivation for proposing a new tokenization method."
Erasure of Unaligned Attributes from Neural Representations,no.,1.,The abstract does not mention large language models or their limitations. It focuses on the AMSAL algorithm for erasing unaligned attributes from neural representations and discusses its application and limitations in that context.
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,Yes.,2.,"""The current models indeed suffer from spurious correlations and have a tendency to generate irrelevant and generic responses."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,Yes.,5.,"""This suggests a potential inherent weakness of the scaling paradigm,"" ""transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions,"" and ""We thus speculatively introduce the idea of a fundamental parallelism tradeoff"
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,Yes.,2.,"The abstract mentions that neural sequence generation models, which can include large language models, are prone to ""hallucinate"" by generating outputs unrelated to the source text. However, the discussion on limitations is relatively brief and focuses on a specific issue (hallucinations) rather than a broad range of limitations. The primary focus is on detecting and mitigating this specific problem."
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,No.,1.,"The abstract does not mention large language models (LLMs) or their limitations. The focus is on image-based story generation and a new dataset called Visual Writing Prompts (VWP). There is no discussion of LLMs, transformer-based models, or their limitations."
