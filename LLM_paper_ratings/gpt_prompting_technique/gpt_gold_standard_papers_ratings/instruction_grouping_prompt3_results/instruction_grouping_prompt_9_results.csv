Title,Talks about LLMs,Rate,Evidence
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,No.,1.,The abstract focuses on Sequence-to-Sequence (S2S) models and their application to sequence tagging and structure parsing. It does not mention large language models (LLMs) or discuss their limitations.
Questions Are All You Need to Train a Dense Passage Retriever,Yes.,1.,"""Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses."""
Transparency Helps Reveal When Language Models Learn Meaning,yes.,4.,"""Our systematic experiments with synthetic data reveal that, with languages where all expressions have context-independent denotations... both autoregressive and masked language models successfully learn to emulate semantic relations between expressions. However, when denotations are changed to be context-dependent... this ability degrades. Turning to natural language, our experiments with a specific phenomenon—referential opacity—add to the growing body"
Visual Spatial Reasoning,Yes.,4.,"""previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information... We demonstrate a large gap between human and model performance"
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,Yes.,4.,"""Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions?... For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set... but models still sometimes copy substantially, in some cases duplicating"
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,No.,1.,The abstract focuses on a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation and does not mention large language models or their limitations.
OpenFact: Factuality Enhanced Open Knowledge Extraction,No.,1.,The abstract focuses on the factuality property in the extraction of an OpenIE corpus named OpenFact and does not mention large language models or their limitations.
On Graph-based Reentrancy-free Semantic Parsing,No.,1.,"The abstract focuses on a graph-based approach for semantic parsing and discusses issues with seq2seq models and phrase structure parsers, but it does not mention large language models or their limitations."
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,No.,1.,The abstract focuses on Gradual Machine Learning (GML) for Aspect-Term Sentiment Analysis and does not mention large language models (LLMs) or their limitations.
Chinese Idiom Paraphrasing,No.,1.,"The abstract focuses on the task of Chinese Idiom Paraphrasing (CIP) and does not mention large language models or their limitations. It discusses the creation of a CIP dataset and the performance of different methods for paraphrase generation, but there is no reference to LLMs or their limitations."
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,Yes.,1.,The abstract discusses the evaluation of transformer models in the context of Chinese character naming and their comparison with human behavior. It does not mention any limitations of these models.
Rank-Aware Negative Training for Semi-Supervised Text Classification,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on semi-supervised text classification and a new Rank-aware Negative Training (RNT) framework. There is no discussion of LLMs or their limitations.
MACSum: Controllable Summarization with Mixed Attributes,No.,1.,"The abstract discusses controllable summarization and the creation of a new dataset for this purpose. It does not mention language models, let alone their limitations."
MENLI: Robust Evaluation Metrics from Natural Language Inference,Yes.,3.,"""Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness."""
Efficient Methods for Natural Language Processing: A Survey,Yes.,3.,"""Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed."""
Abstractive Meeting Summarization: A Survey,Yes.,1.,"The abstract mentions ""recent advances in deep learning"" and ""encoder-decoder architectures"" which are related to language models, but it does not discuss any limitations of these models. The focus is on the challenges of abstractive meeting summarization rather than limitations of the models themselves."
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,Yes.,1.,"The abstract mentions the use of ""neural language models to approximate human predictive distributions"" but does not discuss any limitations of these models. The focus is on explaining scalar inferences and context-driven expectations, not on the limitations of language models."
Reasoning over Public and Private Data in Retrieval-Based Systems,No.,1.,The abstract does not mention large language models (LLMs) or their limitations. It focuses on retrieval-based systems and the challenges of handling public and private data in such systems.
Multilingual Coreference Resolution in Multiparty Dialogue,No.,1.,The abstract does not mention large language models or their limitations. The focus is on creating a dataset for multilingual coreference resolution in multiparty dialogue and evaluating models on this dataset.
