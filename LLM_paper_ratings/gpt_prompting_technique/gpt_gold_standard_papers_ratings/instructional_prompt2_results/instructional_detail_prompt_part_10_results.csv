Title,Talks about LLMs,Rate,Evidence
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,Yes.,3.,"""existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models"" and ""alleviates the error accumulation problem in autoregressive generation."""
Time-and-Space-Efficient Weighted Deduction,No.,1.,"The abstract and title discuss weighted deduction in NLP algorithms, topological sorting, and dynamic programming but do not mention language models or their limitations."
Conditional Generation with a Question-Answering Blueprint,Yes.,3.,"""neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details"" and ""rendering conditional generation less opaque and more grounded""."
Collective Human Opinions in Semantic Textual Similarity,No.,1.,"The abstract focuses on semantic textual similarity (STS) and human annotation disagreements. It does not mention language models (LMs or LLMs) or their limitations. The discussion is centered on the variance in human opinions and the inadequacy of current STS models to capture this variance, without specifying any particular type of language model."
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,no.,1.,"The abstract focuses on biases introduced by task design in crowdsourced linguistic annotations, particularly for implicit discourse relation annotation. It does not mention language models (LMs or LLMs) or their limitations."
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,No.,1.,"The abstract discusses neural agent-based simulations of language emergence and change, focusing on the word-order/case-marking trade-off and the development of a new framework (NeLLCom) for language learning and communication. It does not mention large language models (LLMs) or their limitations."
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,No.,1.,The abstract and title focus on the uniform information density hypothesis and word order patterns in natural languages. There is no mention of language models (LMs or LLMs) or their limitations.
Cross-functional Analysis of Generalization in Behavioral Learning,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or their limitations. The focus is on behavioral learning and generalization in NLP tasks.
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,Yes.,3.,"""We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,Yes.,1.,"The abstract discusses a novel compositional transfer learning framework (DoT5) for zero-shot domain transfer and its effectiveness in the biomedical domain. However, it does not mention any limitations or challenges related to LLMs."
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,No.,1.,"The abstract and title focus on a multilingual retrieval dataset and do not mention language models (LMs or LLMs) or their limitations. The discussion is centered on the dataset's creation, scope, and purpose."
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,No.,1.,"The abstract and title focus on dataset mention detection in scientific literature and do not discuss language models (LMs, LLMs) or their limitations."
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,yes.,3.,"""Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",No.,1.,"The abstract discusses ""mathematical language processing methods"" and specific tasks such as identifier-definition extraction, formula retrieval, and informal theorem proving. It does not mention language models (LMs or LLMs) or their limitations."
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,No.,1.,The abstract focuses on the cognitive science of adjective ordering and the use of natural language processing technologies to evaluate proposals across multiple languages. It does not mention language models (LMs or LLMs) or their limitations.
Improving Multitask Retrieval by Promoting Task Specialization,No.,1.,The abstract does not mention language models (LMs or LLMs) or discuss their limitations. It focuses on multitask retrieval and task specialization.
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,No.,1.,"The abstract focuses on sequence generation models used for semantic parsing, calibration, and confidence estimation. There is no mention of language models (LMs or LLMs) or their limitations."
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,No.,1.,"The abstract does not mention language models (LMs or LLMs) or discuss their limitations. The focus is on answer selection in open-domain dialogues and the introduction of an intent-calibrated self-training paradigm, without any reference to language models."
Benchmarking the Generation of Fact Checking Explanations,No.,1.,The abstract does not mention language models (LMs or LLMs) or their limitations. It focuses on the generation of fact-checking explanations and benchmarking with datasets and summarization approaches.
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,No.,1.,"The abstract and title focus on a two-stage span-based framework for Named Entity Recognition (NER) and do not mention language models (LMs or LLMs) or their limitations. The content is centered on NER tasks and methodologies, with no discussion of LLM-related challenges."
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,Yes.,3.,"""Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual)."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,No.,1.,The abstract and title focus on Open Relation Extraction (ORE) tasks and propose a framework called U-CORE. There is no mention of language models (LMs or LLMs) or their limitations.
In-Context Retrieval-Augmented Language Models,Yes.,3.,"""they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism"" and ""Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment."""
Learning to Paraphrase Sentences to Different Complexity Levels,Yes.,1.,"""Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,No.,1.,"The abstract discusses automatic subtitling and proposes a direct speech translation model, but it does not mention language models (LMs or LLMs) or any limitations related to them."
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,Yes.,4.,"""LLMs fail at generalizations between related contexts that have not been observed during pre-training... LLMs show a bias to generalize based on linear order. This finding points to a limitation with current models and points to a reason for which their training is data-intensive."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",No.,1.,"The abstract discusses the creation and evaluation of a multilingual, multi-domain, and multi-parallel dataset for task-oriented dialog systems. It focuses on the challenges of creating culturally adapted datasets and does not mention language models (LMs or LLMs) or their limitations."
Can Authorship Representation Learning Capture Stylistic Features?,No.,1.,"The abstract discusses authorship representation learning and its ability to capture stylistic features, but it does not mention language models (LMs or LLMs) or any limitations associated with them. The focus is on authorship attribution and writing style."
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,No.,1.,The abstract focuses on cross-lingual semantic parsing and methods to improve it using Optimal Transport. It does not mention language models (LMs or LLMs) or discuss their limitations.
