Title,Talks about LLMs,Rate,Evidence
Automating Behavioral Testing in Machine Translation,Yes.,2.,"""To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",Yes.,3.,"""Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance."" and ""Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task."""
GenIE: Generative Information Extraction,Yes.,2.,"""Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,Yes.,4.,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500... (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the"
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,Yes.,2.,"""We leverage popular code-mixed metrics and embeddings of multilingual large language models (MLLMs) as features and train task-specific MLP regression models. Our approach could not beat the baseline results."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,No.,1.,The abstract and title do not mention language models (LMs or LLMs) or their limitations. The focus is on fact-checking ambiguous claims and the development of a dataset and models for this purpose.
Language Varieties of Italy: Technology Challenges and Opportunities,,,
Benchmarking Large Language Models for News Summarization,Yes.,3.,"""existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance."""
mGPT: Few-Shot Learners Go Multilingual,Yes.,1.,"The abstract focuses on the introduction, design, pretraining procedure, and evaluation of mGPT. It does not mention any limitations or challenges related to LLMs."
Large Language Models of Code Fail at Completing Code with Potential Bugs,Yes.,5.,"""Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion... However, most existing works ignore the possible presence of bugs in the code context for generation... We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs... Finally, we investigate several post-hoc methods for mitigating the adverse effect of"
Cultural Adaptation of Recipes,Yes.,2.,"""While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,No.,1.,The abstract and title focus on few-shot aspect category sentiment analysis (ACSA) and the proposed metric-free method using Dual Relations Propagation (DRP). There is no mention of language models (LMs or LLMs) or their limitations.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,No.,1.,The abstract discusses computational linguistics models in the context of calibration assessment and binning problems but does not mention language models (LMs or LLMs) or their limitations.
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,No.,1.,The abstract focuses on an energy-based model for word-level autocompletion in computer-aided translation. It mentions issues related to the model's performance and efficiency but does not discuss language models (LMs or LLMs) or their limitations.
Lost in the Middle: How Language Models Use Long Contexts,Yes.,4.,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for"
Red Teaming Language Model Detectors with Language Models,Yes.,4.,"""The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users."" and ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems."""
Text Attribute Control via Closed-Loop Disentanglement,No.,1.,There is no mention of language models (LMs or LLMs) or their limitations in the title or abstract. The paper focuses on text attribute control and disentanglement techniques.
Unifying Structured Data as Graph for Data-to-Text Pre-Training,No.,1.,The abstract focuses on data-to-text generation and the unification of structured data types into a graph format for pre-training. It discusses the design of a structure-enhanced Transformer and related methods but does not mention language models (LMs or LLMs) or their limitations.
Exploring Human-Like Translation Strategy with Large Language Models,Yes.,3.,"The abstract mentions ""various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission"" which are limitations of LLMs in the context of translation. However, these limitations are not the main focus of the abstract; they are discussed as part of the broader topic of improving translation quality with the MAPS framework."
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,Yes.,2.,"""improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,Yes.,4.,"""Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations... Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected... we argue that such evaluation is limited, since injecting one fact... introduces a 'ripple effect' in the form of additional facts"
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,Yes.,3.,"""When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. ... Our analysis reveals ... that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words."""
