Title,Talks about LLMs,Rate,Evidence
Large Language Models Enable Few-Shot Clustering,Yes.,1.,The abstract discusses how LLMs can be used to improve clustering and enhance expert guidance but does not mention any limitations or challenges related to LLMs.
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,yes.,1.,"The abstract does not mention any limitations of LLMs. It focuses on the introduction of a new approach, dataset, and model for justification generation and its performance in experiments. There is no discussion of challenges or limitations related to LLMs."
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,No.,1.,The abstract and title focus on machine translations (MTs) and human translations (HTs) from a morphosyntactic perspective. There is no mention of language models (LMs or LLMs) or their limitations.
What Do Self-Supervised Speech Models Know About Words?,No.,1.,The abstract focuses on self-supervised speech models (S3Ms) and their encoding of linguistic properties. It does not mention language models (LMs or LLMs) or discuss their limitations.
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,Yes.,2.,"""We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality."""
Geographic Adaptation of Pretrained Language Models,Yes.,3.,"""While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,No.,1.,"The abstract discusses text simplification systems and their evaluation, focusing on meaning preservation and reading comprehension. It does not mention language models (LMs or LLMs) or their limitations."
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,Yes.,3.,"""enables tool-augmented large language models to access information stored in the OSM database,"" ""detailed evaluation reveals strengths and weaknesses of the considered learning strategies."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,Yes.,2.,"""For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,Yes.,4.,"""We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. ... Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,Yes.,1.,"The abstract discusses the use of Large Language Models (LLMs) for extracting social determinants of health but does not mention any limitations or challenges related to LLMs. The focus is on the novel corpus, annotation scheme, and the performance of the models."
Fairness in Large Language Models: A Taxonomic Survey,Yes.,5.,"""Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. ... Specifically, a brief introduction to LLMs is provided, followed by an"
Algorithmic Collusion by Large Language Models,Yes.,4.,"""We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ('prompts') may increase collusion. ... Our findings underscore the need for antitrust regulation regarding algorithmic pricing"
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,Yes.,3.,"""Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs."""
Can Language Models Recognize Convincing Arguments?,Yes.,2.,"""The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,Yes.,4.,"""However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,Yes.,4.,"""Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. ... existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,Yes.,4.,"""current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,yes.,4.,"""Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,Yes.,5.,"""Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands."" ""Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately."" ""This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving."" ""Through case analysis,"
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,Yes.,2.,"""addresses the issue of class imbalance encountered in LLM-based annotations."""
ST-LLM: Large Language Models Are Effective Temporal Learners,Yes.,4.,"""to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness."""
