Title,Talks about LLMs,Rate,Evidence
A Survey of using Large Language Models for Generating Infrastructure as Code,Yes.,3.,"The paper discusses ""the feasibility of applying Large Language Models (LLM) to address this problem"" and mentions ""presenting the challenges in this area and highlighting the scope for future research."" However, the limitations of LLMs are not the primary focus and are discussed alongside other topics such as the usage and importance of LLMs in IaC."
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,Yes.,3.,"""However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",Yes.,1.,"The abstract focuses on evaluating GPT-3.5's performance as a ""Language Data Scientist"" and its success in answering data science queries. It does not mention any limitations or challenges related to LLMs."
On-the-fly Definition Augmentation of LLMs for Biomedical NER,Yes.,3.,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,4.,"""Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior."""
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,yes.,4.,"""Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,yes.,3.,"""LLMs can inherit harmful biases and produce outputs that are not aligned with human values."" The abstract mentions the limitations of LLMs, specifically biases and misalignment with human values, but these points are secondary to the main focus on proposing and evaluating the new MPO method."
FACTOID: FACtual enTailment fOr hallucInation Detection,Yes.,5.,"""However, hallucination is a significant concern... conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs... current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted... aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality."""
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,Yes.,4.,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,Yes.,2.,"""Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied."""
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,Yes.,1.,The abstract discusses the application and potential effectiveness of LLMs in enhancing psychiatric interviews but does not mention any limitations or challenges related to LLMs. It focuses on the positive outcomes and high performance achieved by LLMs in the tasks of symptom delineation and summarization.
PropTest: Automatic Property Testing for Improved Visual Programming,Yes.,2.,"""This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data."" The abstract mentions the advantage of not requiring finetuning, which can be interpreted as a limitation of LLMs, but it is mentioned very briefly."
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,Yes.,3.,"""Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. ... Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. ... Our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retr"
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,Yes.,2.,"""The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development."" This mentions the potential of LLMs but does not discuss their limitations in detail."
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",Yes.,2.,"""current chart visual question answering (chart VQA) models suffer on complex reasoning questions."" This indicates a limitation but does not delve deeply into it. The rest of the abstract focuses on the benefits and improvements using LLMs rather than their limitations."
ChatDBG: An AI-Powered Debugging Assistant,Yes.,1.,The abstract mentions that ChatDBG integrates large language models (LLMs) to enhance debugging capabilities but does not discuss any limitations or challenges related to LLMs.
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,Yes.,3.,"""A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments."""
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,Yes.,3.,"""We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,Yes.,3.,"""Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,Yes.,3.,"""While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor."""
