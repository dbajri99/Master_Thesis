Title,Talks about LLMs,Rate,Evidence
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,No.,1.,"The abstract discusses Sequence-to-Sequence (S2S) models and their application to tasks like part-of-speech tagging, named entity recognition, constituency, and dependency parsing. It does not mention limitations of large language models (LLMs) or any other kind of language models."
Questions Are All You Need to Train a Dense Passage Retriever,Yes.,1.,"The abstract mentions the use of ""generic initialization from a pre-trained language model"" but does not discuss any limitations or challenges related to LLMs. The focus is on the new autoencoding approach for training dense retrieval models."
Transparency Helps Reveal When Language Models Learn Meaning,Yes.,4.,"""Our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well. We show this failure relates to the context-dependent nature of natural language form-meaning mappings."""
Visual Spatial Reasoning,Yes.,4.,"""previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information,"" ""We demonstrate a large gap between human and model performance,"" ""We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,Yes.,4.,"""For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set. ... models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. ... GPT-2’s novel text is usually well-formed morphologically and synt"
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,No.,1.,"""We present FRMT, a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of professional translations from English into two regional variants each of Portuguese and Mandarin Chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for FR"
OpenFact: Factuality Enhanced Open Knowledge Extraction,No.,1.,"The abstract focuses on the factuality property during the extraction of an OpenIE corpus named OpenFact and discusses aspects like expressiveness and groundedness, but it does not mention language models (LMs or LLMs) or their limitations."
On Graph-based Reentrancy-free Semantic Parsing,No.,1.,"""We propose a novel graph-based approach for semantic parsing that resolves two problems observed in the literature"
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,No.,1.,"The abstract and title focus on ""Aspect-Term Sentiment Analysis (ATSA),"" ""Gradual Machine Learning (GML),"" and ""polarity classification DNN,"" without mentioning language models (LMs or LLMs) or their limitations."
Chinese Idiom Paraphrasing,No.,1.,The abstract does not mention language models (LMs or LLMs) or discuss any limitations related to them. The focus is on Chinese Idiom Paraphrasing and the creation of a dataset for this task.
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,Yes.,1.,The abstract discusses the performance of transformer models in capturing human behavior in Chinese character naming tasks but does not mention any limitations of these models.
Rank-Aware Negative Training for Semi-Supervised Text Classification,No.,1.,"""The abstract discusses semi-supervised text classification-based paradigms (SSTC) and introduces a Rank-aware Negative Training (RNT) framework. It does not mention or discuss language models (LMs or LLMs) or their limitations."""
MACSum: Controllable Summarization with Mixed Attributes,No.,1.,"The abstract discusses controllable summarization, dataset creation, and evaluation methods without mentioning language models (LMs or LLMs) or their limitations."
MENLI: Robust Evaluation Metrics from Natural Language Inference,Yes.,2.,"""Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness."""
Efficient Methods for Natural Language Processing: A Survey,Yes.,3.,"""Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed."""
Abstractive Meeting Summarization: A Survey,Yes.,3.,"""In this paper, we provide an overview of the challenges raised by the task of abstractive meeting summarization and of the data sets, models, and evaluation metrics that have been used to tackle the problems."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,Yes.,1.,"The abstract mentions ""neural language models"" but does not discuss any limitations or challenges related to them. The focus is on how these models are used to approximate human predictive distributions for scalar inferences."
Reasoning over Public and Private Data in Retrieval-Based Systems,No.,1.,The abstract focuses on retrieval-based systems and the challenges of incorporating private data in such systems. It does not mention language models (LMs or LLMs) or their limitations.
Multilingual Coreference Resolution in Multiparty Dialogue,No.,1.,The abstract primarily discusses the creation of a multilingual dataset for coreference resolution in multiparty dialogue and evaluates model performance on this dataset. There is no mention of language models (LMs or LLMs) or their limitations.
