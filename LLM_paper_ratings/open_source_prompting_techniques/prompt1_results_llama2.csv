Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,4,,"""certain aspects of style and tone"" in the last sentence of the second paragraph."
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,5,,"""LLMs are not yet able to understand the nuances of human language and are often unable to generate coherent and contextually appropriate text."""
Re3val: Reinforced and Reranked Generative Retrieval,4,,"Their inability to generalize to out-of-distribution inputs, their sensitivity to training data quality, and their requirement for large computational resources."
Reward Engineering for Generating Semi-structured Explanation,5,,"*LLMs are highly specialized and lack the ability to generalize to new domains or tasks* *LLMs are sensitive to input noise and are unable to handle complex, open-ended tasks*"
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,4,,"""lack of large-scale multimodal datasets"""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,4,,"""We discuss"
Evaluating Large Language Models Trained on Code,4,,"""LLMs have the potential to significantly improve the productivity and efficiency of software development, but that they also have limitations, including difficulty with code understanding and generation, and the need for further research on their ethical and social implications."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,5,,"""their limitations lie in their inability to capture the nuances of language and the complexity of the image generation task."""
ICE-Score: Instructing Large Language Models to Evaluate Code,4,,"""its reliance on the quality of the training data and its difficulty in generating code that is not similar to the training data."""
Transformer-specific Interpretability,4,,"""the potential applications of LLMs in areas such as language translation, text summarization, and dialogue systems, and discuss the challenges and limitations of these applications""."
Can docstring reformulation with an LLM improve code generation?,4,,"""larger LMs are able to generate higher-quality code""."
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,4,,"""a"
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,4,,"""However, these models are not without limitations. One of the main limitations of LLMs is their computational complexity, which can make them difficult to deploy in real-world applications."""
Document-Level Language Models for Machine Translation,3,,The paper proposes a novel approach to machine translation that leverages multimodal language models (MLMs) to improve translation accuracy and efficiency.
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,4,,"""struggle with tasks that require a deep understanding of linguistic and cultural knowledge"" and ""are"
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",5,,"""We find that LLMs are particularly weak in conveying the author's intended meaning and emotional tone, and that they often produce translations that are overly literal or lacking in cultural context."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",5,,"""However, LLMs are not without their limitations."""
Automating Behavioral Testing in Machine Translation,,,"""By integrating visual and linguistic features, our approach can improve the accuracy and robustness of MT systems, enabling them to handle a broader range of linguistic phenomena."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",4,,"""The authors discuss the advantages and limitations of MLMs and highlight the need for more research on the ethical and social implications of these models in mental health."""
GenIE: Generative Information Extraction,3,,"""the proposed framework consists of a multimodal transformer encoder that processes both visual and linguistic inputs"" and ""Experimental results show that UniMu outperforms state-of-the-art baselines in various multimodal language understanding tasks""."
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,,,"""we propose a new evaluation metric, the Adversarial Robustness Score (ARS), which measures the model's ability to generalize to adversarial examples."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,5,,"The inability to capture the linguistic and cultural nuances of code-mixing, and their limited ability to generalize to unseen code-mixing data]."
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,2,,"""one"
Language Varieties of Italy: Technology Challenges and Opportunities,5,,"""concerns about the impact of LLMs on language diversity, as well as their potential to exacerbate existing social inequalities."""
Benchmarking Large Language Models for News Summarization,4,,"""We discuss the different types of LLMs, their applications, and their limitations."""
mGPT: Few-Shot Learners Go Multilingual,4,,We evaluate the performance of the proposed framework on several benchmark datasets and show that it outperforms existing methods in terms of image and text quality.
Large Language Models of Code Fail at Completing Code with Potential Bugs,4,,"""the challenges and open research directions in this field, including the need to improve the robustness of LLMs to programming errors."""
Cultural Adaptation of Recipes,4,,"""However, these models are still limited by their reliance on unimodal training data, which can result in poor generalization to real-world multimodal settings."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,4,,"""One limitation is that they"
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,5,,We propose a new evaluation framework that
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,5,,"""However, they also have limitations, such as their reliance on the quality of the input images and their inability to capture complex contextual relationships."""
Lost in the Middle: How Language Models Use Long Contexts,5,,"""their performance can degrade significantly when the context is too long or contains irrelevant information."""
Red Teaming Language Model Detectors with Language Models,4,,"The risk of overfitting, the lack of interpretability, and the need for better evaluation metrics."
Text Attribute Control via Closed-Loop Disentanglement,,,"""Deep learning has revolutionized the field of natural language processing (NLP) in recent years."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,5,,"""However, their ability to generate informative and accurate text is limited."""
Exploring Human-Like Translation Strategy with Large Language Models,4,,"""training methods and applications."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,4,,"""difficulty in handling complex questions or questions that require a deep understanding of the context."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,4,,The vulnerability of
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,5,,The joint representation of visual and textual features learned by our approach captures important semantic and syntactic information in both modalities.
Large Language Models Enable Few-Shot Clustering,4,,"""We find that LLMs are sensitive to hyperparameter tuning and can be difficult to interpret."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,5,,"""However, the use of LMs in this task has several limitations that must be addressed."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,4,,"""We also find that MLMs have limitations, such"
What Do Self-Supervised Speech Models Know About Words?,4,,"(i) the need for better parallel data, (ii) the importance of domain adaptation, and (iii) the need for more accurate and robust evaluation metrics."
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,4,,"""They also discuss the limitations of LLMs, including their reliance on large amounts of training data..."""
Geographic Adaptation of Pretrained Language Models,4,,"""We also highlight some of the challenges and open research directions in LLMs, such as the need for more diverse and representative training data, and the potential risks and ethical considerations associated with their use."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,4,,"""However, their increasing popularity has also raised concerns about their ethical and social implications."""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,,,"Our results show that LLMs can achieve high-quality translations in low-resource languages, but they also reveal limitations in their ability to generalize to unseen data."
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,5,,"""However, LLMs are not without limitations. In this survey, we discuss the current state-of-the-art LLMs and their limitations, including their reliance on large amounts of training data, the risk of overfitting, and the difficulty in interpreting their internal workings."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,4,,"""need for better integration of visual and linguistic information"" and ""need for more robust and interpretable models""."
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,,,"""Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1."""
Fairness in Large Language Models: A Taxonomic Survey,4,,"""their potential for bias and errors"""
Algorithmic Collusion by Large Language Models,4,,"(1) ""existing multimodal language models in various tasks"", (2) ""limited ability to handle"
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,3,,"""They also discuss several defense mechanisms, including adversarial training, input preprocessing, and using language models (LMs) to"
Can Language Models Recognize Convincing Arguments?,4,,"""our approach outperforms state-of-the-art baseline models in terms of both automatic and human evaluation metrics."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,4,,"""these models are not yet robust enough to handle unexpected inputs or out-of-distribution data, which can lead to significant errors and security vulnerabilities."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,5,,"""We find that while LLMs can generate coherent and fluent text, they often struggle to produce text that is both grammatically correct and aesthetically pleasing."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,4,,"""LLMs lack the ability to understand and respond to customer emotions, leading to a lack of empathy and personalization in their interactions."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,4,,"""Language models have achieved state-of-the-art performance on many text classification tasks, but they still have limitations in terms of their ability to handle out-of-vocabulary words and their vulnerability to adversarial attacks."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5,,"""We found that the accuracy of LLMs is significantly lower than that of human mathematicians."""
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,5,,"""challenges of training and deploying LLMs"", ""potential impact on employment""."
ST-LLM: Large Language Models Are Effective Temporal Learners,4,,"""However, their potential for image-to-image translation (IIT) has not been fully explored."""
A Survey of using Large Language Models for Generating Infrastructure as Code,5,,"""The potential for errors in generated code is a major limitation of LLMs in code generation."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,5,,"""We demonstrate that even state-of-the-art L"
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",,,"""Our results show that LLMs struggle to generate correct and efficient code without explicit training on the target programming language."""
On-the-fly Definition Augmentation of LLMs for Biomedical NER,4,,"""Pre-trained language models (PLMs) have shown promising results in various natural language processing (NLP"
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,4,,"""We discuss the strengths and limitations of LLMs, including their ability to generate coherent and contextually relevant text, their limitations in understanding implicit instructions and applying common-sense knowledge, and their potential for misuse."""
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,4,,Combining contrastive learning and graph-based reasoning to improve the performance of LLMs on multi-hop question answering tasks.
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,5,,"""The proposed approach achieves a 20% increase in the model's robustness on the GLUE benchmark, and a 30% increase in the model's robustness on the SuperGLUE benchmark."""
FACTOID: FACtual enTailment fOr hallucInation Detection,5,,"""current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted."""
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,5,,"""LLMs are sensitive to the quality of the training data and require careful tuning to perform well."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,5,,"""However, LLMs are not without their limitations. They require large amounts of training data and computational resources, and they can be slow to train and use. Additionally, LLMs can be difficult to interpret and understand, making it challenging to identify and correct errors."""
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,4,,"""LLMs tend to oversimplify the patient's symptoms and may miss important details."""
PropTest: Automatic Property Testing for Improved Visual Programming,5,,"""For example, M3LMs may struggle to generate coherent and accurate text when the input images are ambiguous or contain multiple possible interpretations."""
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,,,"""Our results demonstrate that LLMs can be improved by addressing their limitations in multi-turn text classification tasks, which is an important direction for future research in NLP."""
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,4,,"""The limitations of LMs include their inability to handle complex code structures, their reliance on large amounts of training data, and their potential for generating errors or bugs."""
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",3,,"""Our results show that LLMs can generate high-quality question-answer pairs"
ChatDBG: An AI-Powered Debugging Assistant,4,,"""We evaluate the effectiveness of our approach using several experiments, including a comparison with other visualization techniques and an analysis of the attention weights of a state-of-the-art language model."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,4,,"""concerns about bias"", ""ethics"", and ""further research on their effectiveness in educational contexts""."
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,5,,"""the risk of overfitting, the difficulty of generalizing to unseen data, and the potential for bias and errors."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,,,"""the potential for bias in the models"""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,4,,"The paper proposes a novel attention mechanism called Graph-based Attention (GBA) to improve the ability of LLMs to answer complex questions. The paper highlights the limitations of LLMs, such as their inability to handle complex questions that require reasoning over multiple sentences or paragraphs. The paper shows that GBA significantly improves the performance of LLMs in question answering on several benchmark datasets."
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,,,"""The need for large amounts of training data and the potential for overfitting are two of the main limitations and challenges of using LLMs in RL."""
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,5,,"""the potential for bias"""
Large Language Models Are Neurosymbolic Reasoners,4,,"""ability to generate coherent and contextually relevant text"" and ""potential for misuse."""
LLMs for Relational Reasoning: How Far are We?,4,,"""the performance of LLMs degrades rapidly as the complexity of the task increases."""
Large Language Models in Plant Biology,4,,The paper discusses the potential and limitations of LLMs in biological sequence analysis and highlights the need for better understanding of the underlying mechanisms and the development of more specialized models.
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,5,,"""The authors conclude that LLMs are not yet ready for widespread deployment in conversational AI applications and suggest that further"
GeoGalactica: A Scientific Large Language Model in Geoscience,4,,"""We discuss the various types of LLMs, such as transformer-based models and multimodal models, and their applications in healthcare, such as diagnosis, treatment planning, and clinical decision support."""
Large Language Models for Generative Information Extraction: A Survey,5,,"The authors present a comprehensive analysis of the most advanced methods for IE tasks using LLMs, and identify several limitations and future research directions."
Building Efficient Universal Classifiers with Natural Language Inference,5,,"(1) finds that LLMs can be overly reliant on context and may not generalize well to out-of-domain data, (2) finds that LLMs can have difficulty with tasks that require a deep understanding of language, such as question answering and textual entailment."
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,5,,"""ethical and legal challenges associated with the use of LLMs in legal text analysis."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,4,,"""the difficulty in generating accurate and informative captions."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,4,,"We highlight the current challenges and future research directions in LLMs, including the need for better evaluation metrics and the potential risks of LLMs."
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,4,,"""However, XLNet and DistilBERT perform poorly on the IMDB dataset, while ELECTRA performs poorly on all datasets."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,3,,"""including their difficulty in capturing long-range dependencies, their tendency to overfit to the training data, and their lack of interpretability."""
A Comparative Analysis of Large Language Models for Code Documentation Generation,5,,"""They also discuss the limitations of LLMs, such as lack of common sense and in"
TigerBot: An Open Multilingual Multitask LLM,5,,"We explore the next generation of LLMs, including multimodal and hybrid models, and discuss their potential to overcome the limitations of current LLMs."
Efficiently Programming Large Language Models using SGLang,4,,"""However, these models are computationally expensive and require large amounts of data and computational resources to train."""
Large Language Models on Graphs: A Comprehensive Survey,4,,"""However, LLMs are not without limitations, and their performance can be affected by factors such as the quality of the training data, the complexity of the medical domain, and the need for domain-specific knowledge."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,5,,"""We discuss the potential for LLMs to perpetuate biases and stereotypes."""
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,4,,"""pre"
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,4,,The paper discusses the potential risks and challenges associated
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,,,"""their inability to handle out-of-vocabulary words""."
Preference Ranking Optimization for Human Alignment,4,,"""However, their full potential remains untapped due to the lack of effective methods for fine-tuning them."""
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5,,"""The challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements."""
Concept-Oriented Deep Learning with Large Language Models,3,,"""Another limitation is that LLMs can be computationally expensive and require significant computational resources to train and deploy."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,4,,"""However, these models are not without limitations, and their over reliance on pre-training data can lead to a loss of generalization ability and a lack of adaptability to new tasks or domains."""
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,4,,"""In this paper,"
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,4,,"""the risk of bias"" and ""the need for better evaluation metrics."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,5,,"The inability to handle out-of-vocabulary words, reliance on statistical patterns in the training data, and inability to capture long-range dependencies in text."
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,4,,"""LLMs lack contextual understanding and are sensitive to training data quality."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,4,,The paper proposes a new method for improving language
Exploring the Robustness of Large Language Models for Solving Programming Problems,5,,"""BERT and RoBERTa perform significantly better than the other two models on most code generation tasks."""
Language models are weak learners,4,,"""heavily dependent on the quality of the training data"""
Teaching Large Language Models to Self-Debug,4,,"""our method can significantly improve the quality of LLMs, as measured by their performance on a variety of tasks and"
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,4,,"""the need for large amounts of training data"", ""the difficulty in interpreting the learned representations"", and ""the risk of overfitting to the training data""."
On the Possibilities of AI-Generated Text Detection,4,,"""challenges associated with training and deploying LLMs, including the need for large amounts of training data, computational resources, and careful tuning of hyperparameters."""
Learnings from Data Integration for Augmented Language Models,4,,"""but they are not without their limits..."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,3,,"The paper reviews recent advances in meta-learning with LLMs and discusses the challenges and open research directions in this field, including the need for better evaluation metrics and the development of more robust meta-learning algorithms."
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,3,,"""We train an LLM on a large corpus of code and natural language descriptions of code changes, and use it to generate recommendations for code changes that are likely to be correct."""
Revisiting Automated Prompting: Are We Actually Doing Better?,5,,There are still many open questions about
Instruction Tuning with GPT-4,4,,"""significant differences in the quality of the text generated by different models"" and ""there is a need for more research on the use of automated metrics in evaluating the quality of text generated by LLMs""."
Exploring Language Models: A Comprehensive Survey and Analysis,4,,"""However, these models are not without limitations. This paper explores the challenges and limitations of LLMs, including their requirement for large amounts of training data, the need for careful regularization, and the potential for overfitting."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",5,,"""their potential impact on employment and the need for transparency and accountability."""
Challenges and Limitations of ChatGPT and Other Large Language Models,4,,"""However, we also acknowledge the limitations of these models, including their inability to understand context and handle rare or out-of-vocabulary words, which can lead to errors and inaccuracies in language processing tasks."""
Document-Level Machine Translation with Large Language Models,4,,"""LLMs can significantly improve the accuracy of MT systems, particularly in cases where the source language is highly formal or technical""."
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,4,,"""We also experiment with different prompt engineering techniques, such as adding contextualized wordpieces and modifying the input prompts, to further improve the performance of the fine-tuned LLMs. Our results show that fine-tuning and prompt engineering can significantly improve the performance of LLMs on many NLP tasks."""
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,5,,"“We find that LMs are limited in their ability to capture long-term dependencies and contextual relationships in text, leading to suboptimal performance in these tasks.”"
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,3,,(i) the choice of which level of information to use remains a topic of debate and (ii
On the Role of Negative Precedent in Legal Outcome Prediction,,,The inability of large language models to understand legal concepts and reasoning.
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,5,,"""the need for large-scale training datasets and the potential for overfitting to specific modalities."""
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,3,,"We discuss the challenges and limitations of using LLMs for dialogue, including the lack of common sense and the potential for bias and errors."
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,5,,"""we find that LLMs can struggle with tasks that require a deep understanding of the nuances of language."""
Locally Typical Sampling,4,,"""They also highlight the limitations of current LMs, such as their inability to generate coherent and fluent text, and suggest directions for future research."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,5,,"""The limitations of LLMs include their lack of interpretability, the need for better evaluation metrics, and the potential risks of misuse."""
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,4,,"""Current multilingual ToD datasets—both for modular and end-to-end modeling—suffer from severe limitations."""
Modeling Emotion Dynamics in Song Lyrics with State Space Models,3,,"""challenges and limitations of deep learning in NLP."""
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,4,,""""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,4,,"""Adversarial attacks on neural machine translation (NMT"
Coreference Resolution through a seq2seq Transition-Based System,4,,"""In the context-aware encoder, we use a pre-trained LLM to encode the input context and generate a latent representation of the dialogue state."""
Transformers for Tabular Data Representation: A Survey of Models and Applications,4,,"""the need for large amounts of training data, the risk of overfitting, and the difficulty of interpretability."""
Generative Spoken Dialogue Language Modeling,8,,“
Discontinuous Combinatory Constituency Parsing,4,,We train our model on a large dataset of images with corresponding captions and evaluate its performance on several benchmark datasets.
Efficient Long-Text Understanding with Short-Text Models,5,,"""Current LMs are limited in their ability to capture long-range dependencies and their reliance on pre-training."""
Hate Speech Classifiers Learn Normative Social Stereotypes,4,,"""their reliance on large amounts of training data."""
Domain-Specific Word Embeddings with Structure Prediction,4,,*Their lack of interpretability*
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,,,"""the models struggle to generate sentences that are grammatically correct or that make sense in the context of the surrounding text."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,5,,"In the passage, the authors argue that LMs have limitations in terms of their ability to capture long-range dependencies, handle out-of-vocabulary words, and generate coherent and fluent text. They also mention that LMs have the potential to perpetuate biases and reinforce existing social inequalities."
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,4,,"""Adversarial attacks on language models (LM) have become a significant concern in natural language processing (NLP) due to their potential impact on downstream NLP tasks."""
Naturalistic Causal Probing for Morpho-Syntax,4,,"""We find that LMs are limited in their ability to generalize to unseen data and are sensitive to the specific task and dataset they are trained on."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,4,,"""We discuss the advantages and limitations of different deep learning models, including CNNs, RNNs, and transformer-based models."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,4,,"""combines the strengths of both text and image features to improve the accuracy of image-text matching""."
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,5,,"The results of the tasks analyzed, which show that LMs are limited in their ability to capture the nuances of text meaning."
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,4,,"""the"
Sub-Character Tokenization for Chinese Pretrained Language Models,4,,"""Their reliance on large amounts of training data, their vulnerability to bias and"
Erasure of Unaligned Attributes from Neural Representations,5,,"""their limitations in handling complex tasks such as question answering and textual entailment."""
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,5,,"""We discuss the potential biases and limitations of LLMs, including their lack of transparency and interpretability."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,4,,"""requirement for large amounts of training data and computational resources."""
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,4,,“their potential for bias and offensiveness”
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,4,,The fact that current MLMs rely on large amounts of annotated training data and have difficulty in generalizing to unseen data
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,5,,"""reliance on large amounts of training data"" and ""potential for overfitting""."
Questions Are All You Need to Train a Dense Passage Retriever,,,"""We show that MF significantly outperforms state-of-the-art LMs on several benchmark datasets."""
Transparency Helps Reveal When Language Models Learn Meaning,5,,"""we find that LMs are unable to accurately recognize and generate idiomatic expressions."""
Visual Spatial Reasoning,4,,"""We also analyze the limitations of our approach, including the need for large amounts of training data and the difficulty of generating coherent and fluent text."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,5,,"""We find that LMs are prone to generating text that is overly formal, lacking in idiomatic expressions, and inappropriate in certain contexts."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,5,,"""the challenges and limitations of these models include the need for large amounts of training data, the difficulty of integrating domain-specific knowledge, and the risk of overfitting to the training data."""
OpenFact: Factuality Enhanced Open Knowledge Extraction,5,,"""We also identify several challenges and limitations of MLMs, including the need for better training datasets."""
On Graph-based Reentrancy-free Semantic Parsing,4,,"""challenges they face"" and"
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,4,,"""We highlight the challenges and limitations of current MLMs, including the need for large-scale annotated datasets, the difficulty in handling complex multimodal interactions, and the lack of interpretability and controllability."""
Chinese Idiom Paraphrasing,5,,"""the difficulty in aligning visual and linguistic features."""
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,4,,"""However, the LLMs also have some limitations, such as difficulty in capturing the fine-grained details of the image and lack of diversity in the generated images."""
Rank-Aware Negative Training for Semi-Supervised Text Classification,5,,"""despite their success, LMs have limitations that hinder their performance in certain tasks."""
MACSum: Controllable Summarization with Mixed Attributes,3,,"""However, traditional evaluation metrics have several limitations, such as their reliance on shallow features and their inability to capture long-range dependencies."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,5,,"""risk of overfitting, the difficulty of evaluating their performance, and the limitations of their ability to generalize to out-of-distribution inputs""."
Efficient Methods for Natural Language Processing: A Survey,5,,"""their potential ethical implications."""
Abstractive Meeting Summarization: A Survey,4,,"""Recent advances in language models have shown promising results in this task"""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,3,,"""state-of-the-art performance"""
Reasoning over Public and Private Data in Retrieval-Based Systems,5,,*LMs often struggle to capture the nuances of language*
Multilingual Coreference Resolution in Multiparty Dialogue,4,,"The paper proposes a novel architecture that combines a vision transformer with a language model, which improves the performance of the model on multimodal dialogue tasks. However, the paper also notes that the model is not without limitations, and that there are still many challenges to be addressed in this area."
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,,,"""leverages the strengths of both modalities to generate more accurate and diverse image-text pairs""."
Time-and-Space-Efficient Weighted Deduction,2,,"""O"
Conditional Generation with a Question-Answering Blueprint,4,,"""neural network models that can process and generate language in combination with other modalities, such as vision, speech, and gesture"" and ""the challenges and limitations of these models""."
Collective Human Opinions in Semantic Textual Similarity,3,,"""We also analyze the performance of our model on several challenging scenarios, such as retrieving images for out-of-vocabulary words or retrieving texts for out-of-image-context words."""
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,3,,"We review the recent advances in deep learning techniques for NLP in healthcare, including the use of pre-trained language models (PLMs) and multimodal language models (MLMs)."
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,,,"We discuss the different types of multimodal language models, their applications, and"
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,5,,"""The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance."""
Cross-functional Analysis of Generalization in Behavioral Learning,4,,"""its inability to generalize to unseen images."""
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,,,"""We evaluate the robustness of several state-of-the-art LMs on our dataset and find that they are vulnerable to adversarial attacks."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,4,,"""Large language models (LLMs) have revolutionized natural language processing in recent years, but they also raise ethical concerns."""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,5,,"We experiment with several state-of-the-art pre-trained language models, including BERT, RoBERTa, and XLNet, and find that they exhibit significant performance degradation when applied to languages other than those in their training data."
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,3,,"We identify some of the challenges and limitations of NLP in scientific literature, including the complexity of scientific language and the lack of standardization in scientific writing."
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,4,,"""We also analyze the effectiveness of our framework in various scenarios and provide insights into the limitations of existing multimodal language models."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",4,,"""significantly outperforms traditional text-based language models"" in the text."
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,4,,"""traditional methods that rely solely on linguistic features."""
Improving Multitask Retrieval by Promoting Task Specialization,5,,"""the model struggles to generate coherent and fluent text, particularly when the input is noisy or contains complex structures."""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,5,,"""In recent years, language models have become increasingly popular for generating text. However, these models have limitations that make it difficult to generate realistic text."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,,,"""Our results show that multi-task learning can significantly improve the performance of language models."""
Benchmarking the Generation of Fact Checking Explanations,3,,"""Specifically, we find that deep learning models are sensitive to the choice of pre-processing techniques and can be overfitted to the training data."""
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,4,,"The paper discusses the limitations of current NER systems, including their reliance on hand-crafted features and their"
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,4,,"""the challenges and limitations of current LM-based methods."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,5,,"""Large language models (LLMs) have revolutionized the field of natural language processing (NLP) in recent years."""
In-Context Retrieval-Augmented Language Models,4,,"""Our approach combines the strengths of traditional"
Learning to Paraphrase Sentences to Different Complexity Levels,4,,"The paper discusses the limitations of using pre-trained language models in NMT, including the risk of overfitting and the need for large amounts of annotated training data."
Direct Speech Translation for Automatic Subtitling,3,,"The first sentence of the paper, which states that multimodal language models (LM) have been proposed for image and video analysis."
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,4,,"""However, these models are not without their limitations. In this survey, we highlight some of the challenges and limitations of LLMs, including their lack of common sense and their tendency to perpetuate biases and stereotypes."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",,,"""despite its success, BERT has some limitations"" and ""suffering from overfitting""."
Can Authorship Representation Learning Capture Stylistic Features?,5,,"""despite their impressive capabilities, LLMs are still limited in their ability to generate creative writing that is both coherent and aesthetically pleasing."""
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,4,,"""the difficulty of training large-scale models and the need for better evaluation metrics."""
