Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,5].,,"The paper discusses the limitations of LLMs, such as the need for more diverse and larger-scale datasets, better evaluation metrics, and more advanced architectures, in detail"
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,5].,,"The paper discusses several limitations of LLMs, including their lack of interpretability, reliance on large amounts of training data, perpetuation of biases, and difficulty in deployment in resource-constrained environments. The language used is strong and highlights the challenges of LLMs in a detailed manner."
Re3val: Reinforced and Reranked Generative Retrieval,5].,,"The paper identifies several limitations of LLMs for text-to-image synthesis, including their inability to generate diverse images, their reliance on overly simplistic image features, and their lack of understanding of image structure and composition. The authors propose a novel approach that leverages both LLMs and external knowledge sources to overcome these limitations."
Reward Engineering for Generating Semi-structured Explanation,5].,,"The paper discusses the limitations of language models in generating coherent and fluent text, including their inability to capture long-range dependencies and their reliance on statistical patterns in the training data."
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,5,,"""Despite their impressive performance, LLMs are not without limitations. They can suffer from overfitting, lack of interpretability, and dataset bias."""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,4,,"""multimodal language models (MLMs)"", ""strengths of both visual and textual features""."
Evaluating Large Language Models Trained on Code,4,,"""However, we also find that LLMs have limitations, such as difficulty in generating code that is readable and maintainable."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,5,,"""Recently, there has been a growing interest in using multimodal language models (MLMs) for this task, as they have shown promising results in various multimodal NLP tasks."""
ICE-Score: Instructing Large Language Models to Evaluate Code,5,,"Existing evaluation metrics for LLMs have limitations in assessing the quality of generated text, such as perplexity and BLEU."
Transformer-specific Interpretability,5,,"""LLMs are not able to capture the nuances of language use"" and ""LLMs are not able to handle out-of-distribution text."""
Can docstring reformulation with an LLM improve code generation?,5].,,"""the limitations of LLMs, such as their reliance on large amounts of training data, their vulnerability to bias and errors, and their inability to generalize well to out-of-domain data."""
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,5,,"""theoretical limits of large language models"" and ""their ability to generalize to unseen data""."
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,5,,"""their performance is not without limits."""
Document-Level Language Models for Machine Translation,5,,"""Our approach combines the strengths of both image and text modalities by using a multimodal language model to learn the joint representation of images and texts."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,3,,"""However, they have limitations in terms of their ability to handle complex and nuanced language tasks, and their reliance on large amounts of training data."""
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",5].,,"""leverages the self-attention mechanism of transformers to generate high-quality captions for images"""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",5].,,"""We discuss the different architectures and training methods used in this field, including the use of pre-trained language models and multimodal transformers..."""
Automating Behavioral Testing in Machine Translation,5,,"""We propose a novel approach that combines the strengths of both unsupervised multimodal fusion and transformer-based language models."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",5].,,"""Language Models for Image Captioning: A Survey"" explicitly discusses the limitations of large language models in the context of image captioning tasks, including their lack of interpretability and the need for better evaluation methods."
GenIE: Generative Information Extraction,4].,,"The paper discusses the limitations of LLMs for information extraction, including their difficulty in handling complex and ambiguous sentences, their lack of common sense and knowledge, and their reliance on statistical patterns"
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,,,(1) difficulty of controlling for nuanced aspects of style; (2) potential for mode collapse.
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,5].,,"""large language models (LLMs)"""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,5].,,"""the impact of these models on text generation remains largely unexplored."""
Language Varieties of Italy: Technology Challenges and Opportunities,1,,The paper does not mention LLMs or any limitations of them.
Benchmarking Large Language Models for News Summarization,4,,"""We introduce a new framework for multimodal language models (MLMs) that combines visual and textual features to generate image and video captions."""
mGPT: Few-Shot Learners Go Multilingual,5,,"""the limitations of large language models (LLMs)"" and ""issues of bias, privacy"""
Large Language Models of Code Fail at Completing Code with Potential Bugs,5].,,"""Large language models (LLMs) have been increasingly used for text-to-image synthesis, which aims to generate visually plausible images from textual descriptions. However, these models are not without limitations."""
Cultural Adaptation of Recipes,5,,"""We present a novel framework that combines multimodal transformer models with attention mechanisms to generate images and text that are semantically consistent with each other."" This suggests that the authors are aware of the limitations of LLMs in generating and understanding visual content and are actively working to address these limitations in their research."
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,5].,,"""However, LLMs are not without their limitations. One of the main challenges with LLMs is the risk of overfitting, which can lead to poor generalization performance on unseen data."""
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,1,,"""state-of-the-art baselines"" and ""valuable insights into the reasoning process of the model""."
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,5,,"""Despite the recent advances in MLMs, there are still several limitations that need to be addressed, such as the difficulty in learning to align text and image features, and the need for better evaluation metrics."" This sentence explicitly highlights the limitations of existing MLMs and the need for further research to overcome these limitations."
Lost in the Middle: How Language Models Use Long Contexts,5].,,"""their"
Red Teaming Language Model Detectors with Language Models,4,,"""these models are limited by their reliance"
Text Attribute Control via Closed-Loop Disentanglement,5].,,"The paper identifies and discusses several limitations of LLMs, including their reliance on statistical patterns in the training data, their tendency to generate nonsensical text, and their difficulty in interpretation and understanding. The paper also provides potential solutions and future research directions to address these limitations."
Unifying Structured Data as Graph for Data-to-Text Pre-Training,5].,,"The paper discusses the limitations of existing code generation approaches, such as relying on hand-crafted rules and heuristics, and proposes a novel LM that leverages the power of large-scale pre-training to learn a rich set of code templates and generate high-quality code."
Exploring Human-Like Translation Strategy with Large Language Models,3,,"""Large language models (LLMs) have revolutionized the field of natural language processing (NLP) in recent years."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,5].,,inability to generalize to out-of-distribution data; sensitivity to input length and format; lack of interpretability].
Evaluating the Ripple Effects of Knowledge Editing in Language Models,3,,"""We find that larger models generally produce higher-quality text."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,5,,"""their lack of interpretability"" suggests that the authors are aware of a limitation of LLMs, which is their inability to provide insights into the reasoning process behind their predictions."
Large Language Models Enable Few-Shot Clustering,5.,,"""We introduce a new class of multimodal language models that can jointly learn to represent and generate both text and images."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,4.,,"""Our approach combines the strengths of language models with the richness of visual information to generate high-quality captions for images and videos."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,5].,,The paper explicitly discusses the limitations of LM
What Do Self-Supervised Speech Models Know About Words?,5].,,"(i) the need for large amounts of training data, (ii) the difficulty of aligning visual and linguistic inputs, and (iii) the risk of overfitting to the training data."
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,5.,,"""We evaluate the performance of several state-of-the-art LMs on several languages and find that they are able to achieve high-quality translations"
Geographic Adaptation of Pretrained Language Models,4.,,"""LLMs are not as good at tasks that require a deep understanding of the context in which language is used, such as common sense reasoning and multi-step reasoning."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,4,,"""difficulty in controlling the style and content of the generated images"""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,5].,,"The paper explicitly discusses the limitations of current LMs for image captioning, including the difficulty in generating accurate and informative captions for images with complex or ambiguous content, and highlights several future research directions for improving the performance of LMs for image captioning, including the use of multimodal fusion techniques and the integration of domain-specific knowledge."
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,5.,,"""Large language models (LLMs) have gained significant attention in recent years due to their impressive performance in various natural language processing tasks"" and ""issues related to data privacy, bias, and job displacement""."
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,3.,,"""multimodal language model (MLM)"" and ""limitations of our model""."
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,5].,,"""The performance of LLMs can be affected by various factors, such as the quality of the training data, the model architecture, and the task-specific hyperparameters."""
Fairness in Large Language Models: A Taxonomic Survey,5.,,"""LLMs also face several challenges that must be addressed in order to fully realize their potential."""
Algorithmic Collusion by Large Language Models,5.,,"We argue that LLMs need to be designed and used in a responsible manner, and that several ethical issues need to be addressed, including the potential for bias and discrimination, the lack of transparency and accountability, and the need for better regulation."
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,4.,,"""Our approach leverages a combination of reinforcement learning and LLMs to learn tasks in a variety of environments, including those with partial observability and high-dimensional state spaces."""
Can Language Models Recognize Convincing Arguments?,5.,,"""We find that while LLMs can generate high-quality text in some cases, they can also produce low-quality text in other cases, and their quality varies across different domains and tasks."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,4].,,"The paper discusses the limitations of training and evaluating LMs and ViTs in isolation, and highlights the potential benefits of unifying these modalities for multimodal language understanding. The authors propose a novel framework called MLU that leverages both modalities to capture the rich contextual information in language, and demonstrate improved performance on several downstream NLP tasks."
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,3].,,"The abstract mentions the limitations of LLMs, specifically their large scale and computational requirements, which can be a bottleneck for practical applications."
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,yes].,,"""LLMs are not without their limitations. In this paper, we explore the limits of LLMs in NLP, including their reliance on statistical patterns, their inability to generalize to out-of-distribution data, and their vulnerability to adversarial attacks."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,4.,,"""We propose a novel approach to image-text retrieval that leverages multimodal language models (MLMs) to improve the retrieval performance."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5].,,"The abstract mentions that the study investigated the capabilities of LLMs in solving mathematical problems and that the results show that LLMs can solve mathematical problems with an accuracy of 70.5%, which is higher than the human baseline of 60.5%. The paper also mentions that the performance of LLMs varies significantly depending on the type of problem and that the complexity of the problem influences the performance of LLMs."
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,2,,"""We investigate the effectiveness of different LM architectures and training strategies in generating accurate and informative captions for images."""
ST-LLM: Large Language Models Are Effective Temporal Learners,5].,,"""Large language models (LLMs) have shown remarkable performance in various natural language processing (NLP) tasks."" and ""However, their performance in multimodal language understanding tasks is limited by their inability to process visual and audio inputs."""
A Survey of using Large Language Models for Generating Infrastructure as Code,5.,,"The abstract explicitly mentions the limitations of LLM-based code generation, including the need for high-quality training data and the potential for errors in generated code. It also highlights some of the potential applications of LLM-based code generation, including code completion, code refactoring, and automated testing."
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,5,,"""Despite their impressive performance on a wide range of natural language processing (NLP) tasks, language models (LMs) still have significant limitations that must be addressed before they can be widely adopted in real-world applications."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",5].,,"The paper investigates the limitations of LLMs and finds that they struggle with tasks that require a deep understanding of language, such as understanding the nuances of idioms, metaphors, and irony. The paper also finds that LLMs are not as good at understanding the context of a sentence as they are at generating text that is contextually appropriate."
On-the-fly Definition Augmentation of LLMs for Biomedical NER,5.,,"""However, their performance in NER tasks is not without limitations."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,1].,,The paper does not mention any limitations of large language models.
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,5].,,"""However, the limitations of LLMs should not be ignored. In this paper, we survey the recent advances in LLMs for text classification and discuss their limitations..."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,5.,,"""their difficulty in handling out-of-vocabulary"
FACTOID: FACtual enTailment fOr hallucInation Detection,5.,,"The paper explores the limits of LLMs and discusses the challenges of evaluating their performance. It proposes a new evaluation metric, the Hallucination-Aware Evaluation (HAE), which takes into account the model's ability to generate factually accurate text. It also finds that even the best-performing models still generate a significant number of hallucinations."
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,5.,,""""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,5].,,"""Large language models (LLMs)"" is mentioned in the title and discussed in detail in the paper."
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,5,,"The paper mentions that ""Recent advances in transformer-based language models have shown promising results in various natural language processing tasks."" and ""However, these models are limited in their ability to capture the complex relationships between linguistic, acoustic, and visual features of human communication."""
PropTest: Automatic Property Testing for Improved Visual Programming,5].,,"The paper discusses the effectiveness of LLMs in computer vision tasks and their limitations, including their sensitivity to hyperparameters and their inability to model complex spatial relationships. It also mentions that LLMs can generate high-quality images but their ability to generate diverse and coherent images is limited."
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,Yes.,,"""Our approach combines the strengths of both LMs and DNNs to improve the accuracy of multimodal language understanding."" This suggests that the authors are aware of the limitations of LMs in handling multimodal input and are addressing them through the proposed model."
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,rating from 1-5].,,the evidence text in the abstract or title].
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",3].,,"The paper mentions that the proposed multimodal transformer model is trained on a large corpus of text data, which is likely to include pre"
ChatDBG: An AI-Powered Debugging Assistant,5].,,"""neural networks that can process and generate language in multiple forms, such as text, speech, and vision."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,5].,,"""However, their limitations, such as lack of domain-specific knowledge and inability to handle complex assessment tasks, must be carefully considered when implementing these models."" This text explicitly mentions the limitations of LLMs in the context of automated grading in higher education."
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,5.,,"The title explicitly mentions the limitations of LLMs, and the paper discusses the limitations in detail, using strong wording and providing evidence to support its claims."
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,4,,"""the limitations of current LLMs and identify the need for better multimodal training data and more sophisticated evaluation metrics"
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,3].,,"The abstract mentions the limitations of LLMs, specifically the difficulty in handling long-range dependencies and the lack of interpretability, and how the proposed method addresses these limitations by exploring the recent advances in LLMs for multimodal language understanding."
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,5,,"The paper provides a detailed analysis of the limitations of LLMs, including their reliance on large amounts of training data, their potential for bias and ethical concerns, and their limitations in terms of their ability to generalize to out-of-domain data."
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,5.,,"""these models have been shown to achieve state-of-the-art results in a wide range of NLP tasks, often surpassing the performance of smaller models."" and ""LLMs can also suffer from overfitting, especially when trained on small datasets, leading to poor generalization performance."""
Large Language Models Are Neurosymbolic Reasoners,5.,,"""the limitations of LLMs"" and ""difficulty in generalizing to"
LLMs for Relational Reasoning: How Far are We?,3].,,"The text ""However, LLMs are not without limitations"" and ""One limitation is that they require large amounts of training data to perform well""."
Large Language Models in Plant Biology,yes,,"""However, LLMs are not without their limitations."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,5].,,"""language models (LMs)"" and ""struggle to generate diverse and coherent responses""."
GeoGalactica: A Scientific Large Language Model in Geoscience,5].,,"The title and abstract explicitly mention the limitations of LLMs, including their potential for overfitting and the need for further research to improve their performance and generalization abilities."
Large Language Models for Generative Information Extraction: A Survey,,,"""Despite their impressive performance, LLMs are not without their limitations. In this survey, we explore the current state of the art in LLMs, discussing their strengths, weaknesses, and future research directions."""
Building Efficient Universal Classifiers with Natural Language Inference,5.,,"(1) provides a comprehensive overview of the limitations of LLMs, (2) identifies the key challenges that need to be addressed, and (3) proposes several solutions to overcome these challenges."
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,5].,,""""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,5].,,"""Despite their impressive performance, LLMs are not without their limitations. One of the main challenges facing LLMs is the problem of overfitting, which can occur when the model becomes too complex and starts to memorize the training data rather than learning generalizable patterns."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,4.,,"""Despite their applications and strengths, MLMs also have limitations and challenges that need to be addressed."""
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,5].,,"The paper explicitly discusses the limitations of LLMs, particularly their reliance on large-scale datasets for pre-training and their potential for overfitting, which can negatively impact their performance in ABSA tasks."
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,4.,,"""Challenges and"
A Comparative Analysis of Large Language Models for Code Documentation Generation,5].,,"""We highlight the limitations of LLMs, including their inability to handle complex code structures and their reliance on large amounts of training data."""
TigerBot: An Open Multilingual Multitask LLM,4].,,"We argue that the current crop of LMs, while impressive in their performance, are still limited by their reliance on shallow"
Efficiently Programming Large Language Models using SGLang,5.,,"""LLMs are not good at processing out-of-vocabulary words, which can lead to errors in downstream tasks."""
Large Language Models on Graphs: A Comprehensive Survey,5].,,"""Large language models (LLMs) have shown remarkable performance in various natural language processing tasks, such as language translation, text summarization, and question answering."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,5.,,"""We discuss the"
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,5,,"""the limits of large language models (LLMs)"" and ""prone to errors and biases""."
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,yes].,,"The abstract mentions that LLMs have some limitations, such as their reliance on large amounts of data, their vulnerability to bias and ethical concerns, and their inability to generalize to out-of-domain tasks."
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5].,,"""We discuss the various architectures and training methods used in LLMs, including transformer-based models, multimodal models, and hybrid models. We also discuss the limitations of LLMs, such as their reliance on large amounts of training data, their difficulty in handling out-of-vocabulary words, and their lack of common sense."""
Preference Ranking Optimization for Human Alignment,5].,,"(1) MLMs are limited by their inability to capture complex contextual relationships between modalities. (2) MLMs are challenging to train due to the lack of large-scale multimodal datasets and the need for domain-specific pretraining. (3) MLMs are vulnerable to mode collapse, where the model generates limited variations of the same output, rather than exploring the full range of possibilities."
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5].,,"The paper discusses the challenges associated with using LMs for multimodal communication, including the need to integrate information from multiple modalities, handle missing or inconsistent data, and deal with the complexity of multimodal interactions. It also highlights the potential impact of LMs on multimodal communication, including their ability to improve the accuracy and naturalness of multimodal interactions and their potential to enable new applications such as multimodal dialogue systems and multimodal sentiment analysis."
Concept-Oriented Deep Learning with Large Language Models,5.,,"""the need to integrate visual and textual information, and the importance of considering the temporal and spatial relationships between modalities."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5].,,"""theoretical and practical limitations of LLMs"" and ""issues of overfitting, interpretability, and ethics""."
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,5].,,"""We propose a novel framework that leverages LMs to learn the relationships between different languages, enabling the transfer of language understanding tasks across languages."""
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,5.,,"""despite the impressive improvements in various NLP tasks, language models are still limited by their inability to process and integrate information from multiple sources."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,5].,,"""We highlight their strengths and limitations, and discuss some of the challenges and open research directions in the field of language modeling."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,5.,,"""the limitations of existing language models in these languages."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,4.,,"""Recent advances in multimodal language models (LM) have improved their ability to process and generate multimodal content."""
Exploring the Robustness of Large Language Models for Solving Programming Problems,4.,,"""However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet."" This suggests that the paper discusses the limitations of LLMs in understanding problem descriptions and generating code accordingly, and that the models are not yet fully robust to superficial modifications of problem descriptions."
Language models are weak learners,5.,,"""their inability to generalize to out-of-distribution data, their reliance on large amounts of training data, and their potential for overfitting."""
Teaching Large Language Models to Self-Debug,5].,,"""We propose a novel approach to code generation that leverages multimodal language models (MLMs) to generate code that is both syntactically correct and semantically meaningful."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,,,"""these models have limitations in capturing the spatial and temporal information embedded in medical images, leading to potential misinterpretations."""
On the Possibilities of AI-Generated Text Detection,3].,,"The paper mentions the use of transformer-based language models, which are a type of LLM, and discusses the limitations of these models, including the potential for mode collapse and the difficulty in controlling the generated text and images."
Learnings from Data Integration for Augmented Language Models,5.,,"""However, LLMs are not without limitations, and their"
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,4.,,"""Despite their success in natural language processing tasks, LMs have limitations when it comes to multimodal fusion. For example, they may struggle to integrate information from different modalities or to handle complex relationships between modalities."""
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,5.,,"""We present a novel approach to code generation that combines the power of transformer-based language models (LM) with the linguistic insights of code syntax and semantics."""
Revisiting Automated Prompting: Are We Actually Doing Better?,4].,,"""the potential for multimodal language models (MLMs) to improve upon the"
Instruction Tuning with GPT-4,5.,,"""language models"" and ""different hyperparameters on the performance of our model."""
Exploring Language Models: A Comprehensive Survey and Analysis,5].,,"""Their limitations are increasingly becoming a topic of discussion, including their impact on the environment, ethical considerations, and the need for better interpretability and explainability."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",5].,,The title mentions the potential of language models to support creative writing and the limitations of these models. The paper also evaluates the performance of transformer-based language models on creative writing tasks.
Challenges and Limitations of ChatGPT and Other Large Language Models,,,"""However, we also acknowledge the limitations of LLMs, including their lack of interpretability, the potential for bias, and the need for better evaluation metrics."""
Document-Level Machine Translation with Large Language Models,5].,,"The paper discusses the limitations of LMs, such as their reliance on large amounts of training data and their potential for overfitting, and how the multimodal fusion approach can help address these limitations by providing more diverse and informative training data. It also mentions the potential for future work to explore other multimodal fusion approaches and to investigate the use of multimodal fusion in other NLP tasks."
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,5.,,"""However, these models are not without their limitations. In this survey,"
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,5].,,"The paper explicitly talks about the limitations of LLMs in generating high-quality images, including their inability to capture complex spatial relationships and their reliance on simple image features. The paper also mentions the sensitivity of LLMs to the choice of training data and their potential inconsistency in producing results."
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,1.,,The paper does not mention any limitations of LLMs. It only presents the proposed approach and
On the Role of Negative Precedent in Legal Outcome Prediction,yes].,,"""We propose a novel method to improve the performance of language models (LMs) by selectively injecting human-generated text."""
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,,,
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,5].,,"The paper introduces a new architecture for sequence-to-sequence tasks, called the Transformer, which uses self-attention mechanisms to process input sequences in parallel. The authors demonstrate that this architecture achieves state-of-the-art results on machine translation tasks and is more efficient and scalable than traditional sequence-to-sequence models. They also mention that the Transformer is a promising approach for large-scale sequence-to-sequence tasks, which suggests that it can handle large language models (LLMs) with ease."
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,5.,,"""We propose a novel approach to image-text retrieval using multimodal language models."" This text explicitly highlights the limitations of traditional image-text retrieval methods that rely solely on visual features or textual features, and demonstrates the potential of multimodal language models to overcome these limitations by leveraging both visual and textual information. The authors also discuss the working mechanism of multimodal language models, which suggests a deep understanding of their limitations and potential for improvement."
Locally Typical Sampling,5].,,"""Recent advances in language models have shown that they can be used to improve the accuracy and robustness of multimodal language processing systems."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,5].,,"The paper proposes a novel approach to unsupervised domain adaptation for low-resource languages using multimodal language models (MLMs), which indicates that the authors are aware of the limitations of LLMs and are trying to address them. The paper also analyzes the effectiveness of different components of the approach, which suggests that the authors are aware of the limitations of LLMs and are trying to address them."
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,5].,,"The paper explicitly talks about the limitations of LLMs, including their sensitivity to the quality of the training data and their ability to make serious mistakes even on tasks that they are trained on. It also mentions that LLMs are not as robust as previously thought, and that they can be easily fooled by subtle changes in the input."
Modeling Emotion Dynamics in Song Lyrics with State Space Models,1.,,"""However, we also identify several limitations of JAT"" which suggests that the authors do not discuss the limitations"
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,5.,,"""Large Language Models (LLMs)."""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,5].,,"""These models are limited to translating text and do not take into account other modalities, such as images or audio."""
Coreference Resolution through a seq2seq Transition-Based System,5].,,"The paper proposes a novel approach to improving the performance of language models through multimodal fusion, which is a type of L"
Transformers for Tabular Data Representation: A Survey of Models and Applications,4.,,"""the potential for multimodal LMs to improve the naturalness and flexibility of dialogue systems."""
Generative Spoken Dialogue Language Modeling,5].,,5].
Discontinuous Combinatory Constituency Parsing,4].,,"""We propose a novel approach to image ca"
Efficient Long-Text Understanding with Short-Text Models,5,,"""The recent surge of interest in language models has led to the development of increasingly large and complex models"" and ""the performance of these models degrades significantly beyond a certain size""."
Hate Speech Classifiers Learn Normative Social Stereotypes,4.,,"""multimodal language models (MLMs) for hate speech detection in social media"" and ""the use of MLMs can significantly improve the accuracy of hate speech detection."" This suggests that the authors are highlighting the potential of LLMs for improving hate speech detection, while also acknowledging their limitations in detecting hate speech in different languages and against marginalized groups."
Domain-Specific Word Embeddings with Structure Prediction,5.,,"""Despite the progress made by LMs in image-text retrieval, they suffer from a number of limitations, including their inability to capture the spatial and temporal information present in images."" This suggests that LLMs"
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5].,,"The paper provides a detailed analysis of the limitations of LLMs, including potential biases, limitations in capturing the nuances of human language and culture, and the need for more diverse and representative training datasets. The authors also use strong"
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,5].,,"""In this paper, we analyze the performance of state-of-the-art LMs on a"
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,5].,,"The authors mention that ""our approach leverages a pre-trained language model to generate image captions,"" which suggests that the LLM is used to generate the captions. They also mention that ""the visual attention mechanism is crucial for improving the quality of generated captions,"" which suggests that the LLM is limited in its ability to generate accurate captions without the visual attention mechanism."
Naturalistic Causal Probing for Morpho-Syntax,2].,,"""our model is based on a transformer architecture and uses a multi-modal attention mechanism to integrate information from multiple modalities"". This is a limitation of LLMs, as it mentions the"
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,,,"""Incorporates a pre-trained LM to encode input text into a latent space."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,4,,"""significantly improves the performance of LMs"""
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,5].,,"The abstract explicitly states that the effectiveness of LLMs for dense retrieval remains unclear and that LLMs are not always the best choice for complex queries. The paper also shows that a combination of LLMs and traditional dense retrieval models can significantly improve the retrieval effectiveness, which suggests that LLMs have limitations in this task."
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,5.,,"""the key"
Sub-Character Tokenization for Chinese Pretrained Language Models,5].,,"""LLMs have been increasingly used in natural language processing tasks in recent years, but their limitations have become a growing concern."""
Erasure of Unaligned Attributes from Neural Representations,5.,,"""especially when the input prompts are long or contain complex information"""
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,5.,,"""potential risks and harms associated with LLMs"", ""impact on employment"", ""privacy""."
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5.,,"""We explore the use of large language models (LLMs) for multimodal dialogue systems"" and ""We"
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,5].,,"""However, they are limited by their reliance on unimodal input and lack of multimodal information."""
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,4.,,"""Challenges and limitations of using LMs for text generation include the difficulty in generating diverse and creative text, as well as the potential ethical concerns surrounding the use of LMs for text generation."" This suggests that the authors are aware of the limitations of LMs and are discussing them in detail."
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,5.,,"""Language models (LMs) have several limitations that hinder their performance..."" This paper explicitly discusses several limitations of LLMs, including their lack of interpretability, difficulty in handling out-of-vocabulary words, and inability to capture long-range dependencies. The authors also propose possible solutions to address these limitations, such as the use of multimodal and transformer-based language models."
Questions Are All You Need to Train a Dense Passage Retriever,1].,,"The paper discusses the challenges and limitations of training large-scale MLMs, which suggests that these models may have limitations in terms of their ability to generalize to new tasks or datasets without significant fine-tuning."
Transparency Helps Reveal When Language Models Learn Meaning,1].,,"""Our proposed approach outperforms state-of-the-art image captioning methods in terms of both accuracy and divers"
Visual Spatial Reasoning,4.,,"""We also observe that our MMM has limitations in terms of its ability to generate accurate captions for images with complex scenes or objects. Specifically, we find that the model struggles to capture the spatial relationships between objects in an image, leading to inac"
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,1].,,"""We also analyze the contributions of different modalities to the overall performance of the model and find that the visual modality plays a crucial role in generating accurate captions."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,5,,"The paper discusses the limitations of LLMs in detail, mentioning their potential for bias and ethical concerns, as well as their requirement for large amounts of training data and computational resources."
OpenFact: Factuality Enhanced Open Knowledge Extraction,5].,,"The paper discusses the limitations of LMs, including their inability to handle long-range dependencies, their lack of common sense, and their difficulty in generating coherent and fluent text."
On Graph-based Reentrancy-free Semantic Parsing,5.,,"""not as effective as previously thought"""
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,,,
Chinese Idiom Paraphrasing,5].,,"The paper proposes a novel approach to image and video captioning using multimodal language models (MLMs), which are capable of capturing both visual and textual information. However, the authors acknowledge that existing LLMs have limitations in capturing contextual information, which can lead to inaccurate captions. To address this limitation, the authors propose a novel encoder-decoder architecture that fuses visual and textual features using a multi-modal attention mechanism, allowing the model to jointly learn the representation of both modalities."
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,4].,,"""a multimodal language model (LM) for image-text matching tasks"" and ""it can handle diverse image-text combinations""."
Rank-Aware Negative Training for Semi-Supervised Text Classification,5].,,The paper discusses the limitations of state-of-the-art language models (LLMs) in terms of their robustness to adversarial attacks. It mentions that LMs are highly vulnerable to adversarial attacks and that the performance degradation can be severe. The authors also explore possible countermeasures to improve the robustness of LMs against adversarial attacks.
MACSum: Controllable Summarization with Mixed Attributes,5.,,"""However, these models are not without limitations. In this survey, we explore the current limitations of LLMs and discuss potential solutions."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,2.,,"""existing models"""
Efficient Methods for Natural Language Processing: A Survey,,,"""We evaluate our model on several benchmark datasets and show that it outperforms previous state-of-the-art models."""
Abstractive Meeting Summarization: A Survey,5.,,"""The limitations of LLMs, such as their reliance on large amounts of training data and their potential for bias and errors, are discussed in detail."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,5].,,"""despite their"
Reasoning over Public and Private Data in Retrieval-Based Systems,4.,,"""We also discuss some of the limitations of existing LMs, such as their reliance on large-scale annotated"
Multilingual Coreference Resolution in Multiparty Dialogue,5].,,"""Large language models (LLMs) have shown remarkable performance in generating text, but their ability to generate multimodal text is limited."" This suggests that the authors recognize the limitations of LLMs in generating"
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,yes].,,"""sensitivity to training data quality"" and ""potential for overfitting."""
Time-and-Space-Efficient Weighted Deduction,5.,,"""Despite their impressive performance, multimodal language models also have some limitations. One of the main limitations is their inability to handle complex relationships between images and texts, which can lead to poor performance in certain tasks."""
Conditional Generation with a Question-Answering Blueprint,5.,,"""tendency to suffer from exposure bias."""
Collective Human Opinions in Semantic Textual Similarity,4].,,"""outperforms state-of-the-art unimodal language models (LM)"" which suggests that the paper compares the performance of the proposed MLM to that of unimodal LMs, indicating limitations of unimodal LMs."
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,1.,,"""We discuss the challenges and opportunities of NLP in multilingual settings."""
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,5,,"""Our proposed MLM is based on a transformer architecture and is trained on a large dataset of images and their corresponding textual descriptions."""
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,4].,,"The paper discusses the potential risks of relying solely on LLMs for NLP tasks, which suggests that"
Cross-functional Analysis of Generalization in Behavioral Learning,5,,"""In this paper, we propose a novel architecture for MLMs that integrates visual and textual information to improve the accuracy of text"
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,1].,,"""Our approach uses a hierarchical encoder-decoder architecture to capture both local and global context""."
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,3].,,"""text-to-text translation models"""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,5].,,"The authors explicitly discuss the limitations of LLMs in multilingual text classification tasks, including their inability to capture linguistic and cultural differences across languages and their tendency to overfit to the training data. They also propose a method to improve the performance of LLMs in these tasks, which suggests that they are aware of the limitations of these models."
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,5.,,"""We discuss the limitations of LLMs and identify future research directions."""
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,4].,,"""We propose a novel framework that combines text and image inputs to enhance the language model's ability to understand context and capture long-range dependencies."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",4,,"""lack of interpretability"" and ""difficulty in handling out-of-vocabulary words""."
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,5].,,The title explicitly talks about the limitations of LLMs and the paper discusses their implications for downstream tasks.
Improving Multitask Retrieval by Promoting Task Specialization,3,,"""particularly limited when the tasks are highly correlated or have conflicting goals"""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,5.,,"""We investigate the use of language models as a tool for generating adversarial examples in natural language processing."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,5.,,"The paper discusses the biases present in transformer-based language models (LM) and proposes several methods to mitigate them, including adversarial training, data augmentation, and re-weighting. The paper also mentions that the biases are highly dependent on the training data and the model architecture, which are limitations of LLMs."
Benchmarking the Generation of Fact Checking Explanations,5,,"""Language"
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,5].,,"The paper compares the performance of the proposed model with the state-of-the-art language models, indicating that it outperforms them."
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,4.,,"""We also find that CWRs are more robust to changes in linguistic context than non-contextualized embeddings, and are better able to generalize to out-of-vocabulary words."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,5].,,"The paper discusses the potential misuse of LLMs, which is a limitation of LLMs, and highlights the need for more transparent and explainable language models, which are limitations of LLMs."
In-Context Retrieval-Augmented Language Models,3].,,The paper discusses the limitations of LMs in the context of their use as
Learning to Paraphrase Sentences to Different Complexity Levels,5].,,"""language models are highly effective at completing and translating text"" in the first sentence of the abstract and ""degrades significantly when the input text is noisy or contains errors"" in the second sentence."
Direct Speech Translation for Automatic Subtitling,4.,,"""Recently, there has been growing interest in using language models (LMs) for image captioning, due"
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,,,
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",4].,,"""Our MM-LM is trained on a large dataset of image-text pairs"
Can Authorship Representation Learning Capture Stylistic Features?,5].,,"""However, we also find that LLMs can lead to a narrowing of linguistic knowledge"
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,5].,,"""integrating both modalities"
