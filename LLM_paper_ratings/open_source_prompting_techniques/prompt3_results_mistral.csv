Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,2,,"The limitations of LLMs are mentioned as a constraint, but they are not the primary focus of the paper. The paper mainly focuses on the benefits of using LLMs in a modular dialogue system for form filling and the comparison of commercial and open-access models. The limitations of LLMs are briefly mentioned as context limitations, but the paper does not discuss them in detail."
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,5,,The paper discusses the limitations of LLMs in terms of domain generalization and the need for post-editing to enhance their text quality. It also mentions the limitations of current post-editing methods and their suboptimal editing strategies for text generation tasks. The paper proposes a solution to address these limitations using a neural programmer-interpreter approach.
Re3val: Reinforced and Reranked Generative Retrieval,5,,"The paper identifies two limitations of generative retrieval models, which are types of LLMs. The first limitation is the lack of consideration of contextual information, and the second limitation is the inability to be fine-tuned for downstream readers. The paper proposes Re3val to address these limitations by incorporating dense passage retrieval, reinforcement learning, and question generation. These approaches can be considered as improvements or solutions to the identified limitations of LLMs."
Reward Engineering for Generating Semi-structured Explanation,5,,The abstract explicitly discusses the limitations of not-so-large language models (FLAN-T5-XXL) in generating structured explanations for their reasoning capabilities. It also highlights the challenges in producing structured explanations and the need for alternative methods like RL. The abstract also provides a detailed discussion on the limitations of SFT in addressing this challenge.
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,5,,"The abstract discusses the limitations of LLMs in multilingual evaluation and highlights the need for calibration with human judgments to ensure accurate evaluation. It also mentions the bias towards higher scores in LLM-based evaluators, which is a significant limitation."
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,1,,"The paper does mention the use of Language Models, but it does not mention any limitations of them. The focus of the paper is on a novel technique for text classification using LMs, and it does not discuss any limitations of LMs."
Evaluating Large Language Models Trained on Code,5,,"The paper discusses the limitations of the Codex language model, a fine-tuned GPT model, in great detail. The limitations include difficulty with long chains of operations and binding operations to variables in docstrings. The authors also mention that the model solves only 28.8% of the problems on HumanEval compared to 0% for GPT-3 and 11.4% for GPT-J, highlighting its limitations in code generation. The paper also discusses the broader impacts of deploying powerful code generation technologies, covering safety, security, and economics, which can be considered as limitations of LLMs."
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,5,,"The paper mentions that current CLIP models face limitations in dealing with linguistic variations in input queries, which are a common issue for LLMs, and the study aims to enhance the representations of CLIP models for paraphrases by using large language models to generate paraphrases. The limitations are discussed in detail, and the paper uses strong wording to describe the challenges faced by current models in handling linguistic variations in input queries."
ICE-Score: Instructing Large Language Models to Evaluate Code,5,,"The paper discusses the limitations of using LLMs for code intelligence tasks, specifically the difficulty in developing evaluation metrics that align with human judgment and the need for human involvement due to the complexity of programming concepts. The paper also highlights the limitations of token-matching-based metrics, such as BLEU, and the challenges of utilizing human-written test suites for functional correctness evaluation. The paper proposes a new evaluation metric, ICE-Score, to address these limitations and achieve superior correlations with human judgment and functional correctness."
Transformer-specific Interpretability,5,,"The abstract explicitly mentions ""limitations of Transformer-specific interpretability methods"" and ""current limitations"" of these methods."
Can docstring reformulation with an LLM improve code generation?,5,,The paper discusses the limitations of LLMs in the context of code generation and their sensitivity to the details in the docstrings. It also concludes by examining the limitations of the methods employed in this work.
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,5,,"The abstract explicitly mentions the limitations of LLMs like error propagation and hallucination, and acknowledges these challenges in specialised areas like finance. It also proposes a solution to mitigate these limitations by equipping LLMs with external tools."
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,5,,"The paper discusses the limitations of LLMs in terms of their substantial model size and the need for immense computational resources, which make them unsuitable for large-scale inference or domain-specific fine-tuning. It then proposes a method, GenCo, to overcome these limitations by utilizing an LLM to generate multiple augmented texts for each input instance and high-quality training instances conditioned on predicted labels, effectively reducing the need for extensive unlabeled texts and making the training process more efficient. The paper also mentions the remarkable performance of LLMs in zero-shot language understanding and their importance in the self-training loop. The limitations are a primary focus of the paper."
Document-Level Language Models for Machine Translation,5,,"The paper explicitly discusses the limitations of language models in text classification, specifically that they may capture domain-specific features in the source domain that are not present or relevant in the target domain. The paper also proposes a solution to this issue by fine-tuning the language model on the target domain data. The limitations of language models are a central focus of the paper and are discussed in detail."
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,5,,"The paper explicitly states that ChatGPT lags behind traditional MT models for 84.1% of the languages it covers, and is especially disadvantaged for low-resource languages and African languages."
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",3,,"The abstract mentions that ""critical errors still abound"" and ""a human translator's intervention remains necessary to ensure that the author's voice remains intact."" However, the limitations are not the main focus of the abstract, as it primarily discusses the benefits of using LLMs for document-level literary translation."
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",5,,"The title and abstract explicitly discuss the limitations of large language models and provide a detailed analysis of the challenges they face in understanding, explanation, and robustness."
Automating Behavioral Testing in Machine Translation,3,,"The abstract mentions the use of LLMs to generate a diverse set of source sentences for behavioral testing, but the limitations of LLMs are not the main focus and are not discussed in detail. The abstract primarily focuses on the benefits of using LLMs for behavioral testing in MT systems."
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",2,,"The paper discusses the limitations of ChatGPT, but it is mentioned as a secondary point and the limitations are not discussed in detail. The limitations mentioned are that transformer-based models fine-tuned on human-annotated datasets exhibit superior performance."
GenIE: Generative Information Extraction,5,,"The paper explicitly states that it uses a pre-trained transformer to generate relations and entities in textual form, and it discusses the limitations of existing approaches that are only applicable to small numbers of entities and relations, which can be attributed to the lack of language knowledge in those approaches. The paper also mentions the need for a unified end-to-end approach to information extraction, which can be seen as a limitation of current LLMs that require separate fine-tuning for different tasks. The paper proposes a new approach, GenIE, that addresses these limitations by using a pre-trained transformer to generate entities and relations in textual form and by using a new bi-level constrained generation strategy to ensure consistency with a predefined knowledge base schema. The paper also shows that GenIE scales to a previously unmanageable number of entities and relations, which is a significant limitation of current LLMs."
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,5,,"The paper evaluates the limitations of language models in adapting to new tasks and discusses the factors that affect their adaptability. It also presents a new benchmark, TaskBench500, to systematically study these limitations. The paper mentions the intrinsic difficulty of predicting individual labels, which is a significant limitation of LLMs, and discusses it in detail. The paper also concludes with a discussion of additional aspects of adaptability that could be studied, indicating a deeper understanding of the limitations of LLMs."
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,2,,"The limitations of LLMs are mentioned as secondary points, with the focus being on the use of LLMs as features for code-mixing prediction tasks. No explicit discussion of limitations is provided."
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,5,,The paper discusses the limitations of language models when handling ambiguity and the need for soft labels to represent the uncertainty in their predictions. It also shows that models trained on ambiguous instances perform better when dealing with such linguistic phenomena.
Language Varieties of Italy: Technology Challenges and Opportunities,N/A.,,"The abstract does not mention any limitations of large language models. Instead, it focuses on the challenges and opportunities related to endangered languages in Italy and the need for a shift from machine-centric to speaker-centric NLP."
Benchmarking Large Language Models for News Summarization,1,,"The abstract does not mention any limitation of LLMs. Instead, it focuses on the successes of LLMs and the factors contributing to their performance."
mGPT: Few-Shot Learners Go Multilingual,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on the multilingual capabilities and evaluation of mGPT."
Large Language Models of Code Fail at Completing Code with Potential Bugs,5,,"The paper explicitly discusses the limitations of large language models of code (Code-LLMs) in handling potential bugs in the code context, which are inevitable in software development. It finds that the presence of potential bugs significantly degrades the generation performance of high-performing Code-LLMs and investigates several post-hoc methods for mitigating the adverse effect of potential bugs. The paper also provides evidence of the significant performance drop of Code-LLMs when encountering potential bugs."
Cultural Adaptation of Recipes,5,,"The abstract explicitly mentions the use of GPT-4, a specific large language model, and acknowledges its impressive abilities but also points out its limitations in cross-cultural recipe adaptation, particularly when translating English recipes into Chinese. This is a clear indication of discussing the limitations of large language models."
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,N/A.,,The paper does not mention any Large Language Model or its limitations at all. The focus is on few-shot Aspect Category Sentiment Analysis (ACSA) and proposing a metric-free method for it.
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,N/A.,,The paper does not mention language models or their limitations. It focuses on calibration assessment for categorical prediction models and the problem of binning schemes.
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,N/A.,,None of the text in the abstract mentions Large Language Models.
Lost in the Middle: How Language Models Use Long Contexts,5,,"The paper explicitly discusses the limitations of language models in handling long contexts, stating that performance degrades significantly when relevant information is in the middle of long contexts, even for explicitly long-context models. The study provides empirical evidence for this limitation."
Red Teaming Language Model Detectors with Language Models,5,,"The abstract discusses the safety and ethical risks of LLMs and mentions their potential for deceptive usage. It also mentions the need to improve the robustness of LLM-generated text detection systems, which directly addresses a major limitation of LLMs."
Text Attribute Control via Closed-Loop Disentanglement,3,,"The limitations of LLMs are not the main focus of the paper, but the paper discusses the need for better disentanglement of content and attributes in LLMs, which can help improve their performance and reduce the risk of generating inappropriate or misleading responses. The paper proposes a method to improve disentanglement, but it does not extensively discuss the limitations of LLMs or provide a detailed analysis of their limitations."
Unifying Structured Data as Graph for Data-to-Text Pre-Training,2,,"The paper does not focus on the limitations of LLMs, but rather discusses a new approach for data-to-text generation using graph-to-text generation, which is a different problem from LLMs. It briefly mentions that previous pre-training methods for D2T generation either oversimplified structured data or were tailored for specific data structures, but it does not provide a detailed analysis of the limitations of LLMs or discuss any specific limitations beyond oversimplification or tailoring."
Exploring Human-Like Translation Strategy with Large Language Models,5,,"The paper explicitly discusses the limitations of LLMs, such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission, and proposes a framework to mitigate these limitations."
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,3,,The paper mentions the improvement in the zero-shot performance of large-scale pre-trained language models (ChatGPT) but does not discuss any specific limitations of LLMs in detail.
Evaluating the Ripple Effects of Knowledge Editing in Language Models,5,,"The paper explicitly talks about the limitations of language models in terms of their ability to accurately update factual knowledge and introduces the concept of ""ripple effects"" which can result in the model's incorrect generation of related facts. The paper also evaluates the performance of various editing methods in dealing with these limitations and finds that they fail to introduce consistent changes in the model's knowledge."
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,3,,"The paper discusses the quality of representations of OOV words, which is a limitation of LLMs, but it is not the main focus of the paper and is mentioned as a secondary point."
Large Language Models Enable Few-Shot Clustering,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on their ability to improve clustering performance."
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,3,,"The paper mentions the use of a retrieval-augmented Language Model (JustiLM) for justification generation, but it does not extensively discuss any limitations of LLMs. Instead, it focuses on the benefits and performance improvements of using JustiLM for justification generation."
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,1,,"The abstract does not mention any limitations of LLMs. Instead, it discusses the discrepancy between machine translations and human translations, and attributes the difference to the use of beam search in machine translation. It does not discuss any limitations of LLMs in handling morphosyntactic divergences."
What Do Self-Supervised Speech Models Know About Words?,5,,"The title and abstract explicitly mention the limitations of pre-trained language models for text classification, and the paper goes on to discuss these limitations in detail, including data sparsity, fairness, and interpretability. The paper also provides experimental evidence to support these findings."
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,3,,"The abstract discusses the limitations of fine-tuning data in driving model predictions, which can be a limitation for large language models, but it is mentioned as a secondary point in comparison to the effectiveness of character-level modeling for neural machine translation."
Geographic Adaptation of Pretrained Language Models,5,,"The abstract mentions the limitations of pretrained language models (PLMs) in handling extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. It then introduces a new method, geoadaptation, to address this limitation by coupling language modeling with geolocation prediction in a multi-task learning setup. The abstract explicitly states that the geoadapted PLMs consistently outperform PLMs adapted using only language modeling, indicating a significant limitation of PLMs in handling geolinguistic knowledge. Additionally, it shows that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs, implying a limitation of the PLMs' ability to capture geographic information without explicit adaptation."
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,3,,"The paper mentions that supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks, which implies the use of LLMs. However, it only mentions that even the best-performing system struggles with some questions and does not go into detail about the limitations of LLMs."
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,2,,"The limitations of LLMs are mentioned as a weakness of the considered learning strategies. The paper focuses on the strengths and weaknesses of different learning strategies for the Text-to-OverpassQL task. It does not provide a detailed discussion of the limitations of LLMs. However, it does acknowledge that LLMs might not be perfect for this task and that there is room for improvement."
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,4,,"The paper discusses the limitations of LLMs in multilingual translation tasks, specifically their reliance on understanding translation instructions and the alignment among different languages. The authors also find that the performance of LLMs in multilingual translation depends on the similarity of the target language to English and the amount of data used in pretraining. These limitations are explicitly stated and discussed in detail throughout the paper."
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,5,,"The paper explicitly states that transformer models capture MWE semantics inconsistently due to reliance on surface patterns and memorized information. It also highlights that MWE meaning is strongly localized and predominantly in early layers of the architecture, which could be considered a limitation of LLMs. The paper also questions the ability of transformer models to robustly capture fine-grained semantics, which could be another limitation. Additionally, it mentions the need for more directly comparable evaluation setups, which could be a limitation in evaluating LLMs."
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,5,,"The title and abstract of the paper explicitly discuss the limitations of language models, and the paper provides a detailed survey of these limitations, including their lack of understanding of the world, their inability to reason about the physical world, their susceptibility to biases, and their limited ability to generate coherent and contextually appropriate text. The paper also discusses the limitations of current evaluation methods and the need for new approaches to evaluate and improve language models."
Fairness in Large Language Models: A Taxonomic Survey,5,,"The paper explicitly discusses the limitations of LLMs, focusing on their lack of fairness and potential for discriminatory outcomes against marginalized populations. It also provides a detailed analysis of factors contributing to bias in LLMs and summarizes metrics and algorithms for promoting fairness. The paper also mentions the challenges and open questions in the field of fair LLMs."
Algorithmic Collusion by Large Language Models,5,,"The paper explicitly discusses the limitations of Large Language Models in the context of algorithmic pricing, and finds that they can collude autonomously in oligopoly settings to the detriment of consumers. The paper also notes that variations in LLM instructions can increase collusion, highlighting a significant limitation of these models. The paper's findings underscore the need for antitrust regulation regarding algorithmic pricing, which further emphasizes the limitations of LLMs in real-world applications."
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,5,,"The paper explicitly discusses the limitations of LLMs for failure detection and recovery in robotics, stating that they often operate offline and necessitate scene resets, leading to high costs. The authors propose a neuro-symbolic framework, Recover, to address these limitations by integrating ontologies, logical rules, and LLM-based planners to enhance the ability of LLMs to generate recovery plans and decrease associated costs."
Can Language Models Recognize Convincing Arguments?,5,,"The abstract explicitly discusses the capabilities and potential impact of LLMs, raising concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. It also mentions the need for continuous evaluation and monitoring of LLMs' capabilities."
WavLLM: Towards Robust and Adaptive Speech Large Language Model,5,,"The paper explicitly mentions the limitations of LLMs, such as the need for generalizing across varied contexts and executing complex auditory tasks. It proposes a solution to these limitations by introducing a robust and adaptive speech large language model called WavLLM. The paper also validates the proposed model on universal speech benchmarks and applies it to specialized datasets, demonstrating its robust generalization capabilities."
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,5,,"The abstract explicitly mentions the limitations of Large Language Models, specifically their susceptibility to generating inaccurate or hallucinatory responses due to their reliance on vast pretraining datasets and their inability to handle complex queries effectively. It also highlights the need for external knowledge to enhance LLMs' performance and addresses the limitations of existing RAG implementations in this regard."
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,5,,"The paper discusses the limitations of LLMs in customer service scenarios, such as limited integration with customer profiles and lack of operational capabilities, as well as the need to provide accurate and reasonable responses and avoid harmful operations. The authors propose a solution to address these limitations by designing an LLM agent named CHOPS that efficiently utilizes existing databases or systems, provides accurate and reasonable responses, and leverages a combination of small and large LLMs. The paper also mentions the limitations of existing API integrations that emphasize diversity over precision and error avoidance. Overall, the paper explicitly discusses the limitations of LLMs in customer service applications and proposes a solution to address them."
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,5,,"The paper explicitly states that language models struggle with handling numerical data and performing arithmetic operations, which is a known limitation of LLMs. The authors propose a method to address this limitation by adjusting the representation of numbers, which is a significant improvement in the field of LLMs. The paper also shows the effectiveness of the proposed method on arithmetic tasks and language understanding, further emphasizing the importance and relevance of addressing the limitations of LLMs."
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5,,"The paper explicitly states in the abstract that it explores the current limitations of LLMs in mathematics and provides evidence of this in the form of a case analysis, which reveals that while GPT-4 can generate relevant responses, it does not consistently answer all mathematical questions accurately."
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,2,,"The limitations of LLMs are mentioned as a secondary point, specifically addressing the issue of class imbalance encountered in LLM-based annotations. However, it does not discuss other limitations such as hallucinations, biases, or toxicity."
ST-LLM: Large Language Models Are Effective Temporal Learners,1,,"The paper does mention some limitations of LLMs, such as overhead and stability issues, but these are not the primary focus of the paper and are only mentioned in passing. The paper primarily focuses on the effectiveness of LLMs for video understanding and proposes methods to address some of their limitations."
A Survey of using Large Language Models for Generating Infrastructure as Code,2,,"The abstract mentions the limitations of IaC orchestration and the need for automation, but it does not explicitly discuss any limitations of LLMs in detail. Instead, it focuses on their potential benefits for IaC generation."
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,5,,"The paper discusses the limitations of Large Language Models in adapting to new, out-of-domain knowledge, specifically focusing on facts and events that occur after the model's knowledge cutoff date. It also highlights the challenges in adapting these models to incorporate new knowledge and mentions the need for effective methods for knowledge injection. The study then investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs and presents results showing considerable performance improvements in Q&A tasks related to out-of-domain knowledge. The paper also mentions the limitations of token-based scaling, which can lead to improvements in Q&A accuracy but may not provide uniform coverage of new knowledge, and advocates for a more systematic approach using fact-based scaling."
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",1,,"The abstract does not mention any limitations of Large Language Models. Instead, it focuses on their potential benefits for data analysis."
On-the-fly Definition Augmentation of LLMs for Biomedical NER,5,,"The paper explicitly states that LLMs struggle on biomedical NER tasks and that they lack sufficient training data. It also shows that definition augmentation leads to significant performance improvements, indicating limitations of LLMs in handling specialized terminology and lack of training data. The authors conduct experiments to demonstrate these limitations and release their code to facilitate further research in this area."
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,5,,"The abstract explicitly discusses the limitations of LLMs in understanding implicit instructions and applying common-sense knowledge, and highlights their impact on long-term consistency and behavior. It proposes a solution (ITCMA) to address these limitations and provides evidence of its superiority over SOTA in Alfworld and real-world tasks."
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,5,,"The abstract explicitly mentions that Large Language Models struggle with real-time knowledge updates, leading to potentially outdated or inaccurate responses, and specifically mentions this as a problem for multi-hop questions. It proposes a framework to address this problem by retrieving and editing facts to update the LLMs. The abstract also discusses the ""hallucination problem"" which is a known limitation of LLMs. The abstract provides theoretical justification for the proposed framework's ability to improve the fact retrieval efficacy of LLMs and validates its performance through comprehensive evaluation."
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,5,,"The paper discusses the limitations of LLMs extensively, focusing on their potential to inherit harmful biases and produce outputs that are not aligned with human values. It also proposes methods to mitigate these limitations through RLHF and contrastive learning-based methods like DPO."
FACTOID: FACtual enTailment fOr hallucInation Detection,5,,"The paper discusses the limitations of LLMs in terms of generating factually incorrect information, known as hallucinations. It highlights that current textual entailment (TE) methods are inadequate for spotting hallucinations and introduces a new type of TE called Factual Entailment (FE) to address this limitation. The paper also benchmarks several state-of-the-art LLMs on the new FACTOID dataset and ranks them based on their hallucination vulnerability."
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,4,,"The abstract discusses the limitations of large language models, specifically in the context of mathematical reasoning tasks, where they struggle with incorrect, missing, and redundant steps in CoT generation. The authors propose a strategy to address these limitations by introducing new tasks for forward and reverse reasoning, which can improve the understanding and execution of instructions by the LLMs."
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,5,,"The paper explicitly states that it aims to augment LLMs with external knowledge using Retrieval-Augmented Generation (RAG) and discusses the limitations of LLMs in conversational question answering (CQA) settings, where the question is interdependent on the preceding context. The paper also mentions that previous work focuses on single-round question answering, while the complex conversational setting is not well studied, suggesting that LLMs may struggle with conversational context. The paper also uses strong wording, stating that ""most previous work focuses on using RAG for single-round question answering"" and ""however, how to adapt RAG to the complex conversational setting is not well studied,"" implying that there is a significant limitation in applying LLMs to conversational settings without RAG. The paper also proposes a conversation-level RAG approach to address these limitations."
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,1,,"The abstract discusses the use and effectiveness of LLMs for psychiatric interviews, but it does not mention any limitations of LLMs."
PropTest: Automatic Property Testing for Improved Visual Programming,2,,"The paper mentions the limitations of LLMs as a secondary point, stating that the proposed method achieves comparable results to state-of-the-art methods while using smaller and publicly available LLMs. However, it does not discuss any specific limitations in detail."
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,3,,"The limitations of LLMs are mentioned as a secondary point, with the primary focus being on the introduction and benefits of LARA. The limitations mentioned are the challenges of multi-turn intent classification and the need for adaptive retrieval techniques to enhance the cross-lingual capabilities of LLMs without extensive retraining and fine-tuning."
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,1,,"The abstract mentions LLMs in the context of their impressive performance on code-related tasks, but it does not mention any limitation of them."
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",5,,The paper explicitly discusses the limitations of LLMs in reasoning-heavy tasks and argues that they can be used as data annotators to generate synthetic data for reasoning-based chart VQA tasks. It also highlights the need for data augmentation using LLMs to improve the performance of reasoning-heavy models. The authors also mention that the LLMs can suffer from generating incorrect or irrelevant answers and the importance of the quality of the LLM-generated data.
ChatDBG: An AI-Powered Debugging Assistant,1,,The limitations of LLMs are not discussed in the abstract. The abstract only mentions the benefits of integrating LLMs into ChatDBG.
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,4,,"The paper discusses the limitations of LLMs, such as the need for human oversight, the potential for generating incorrect answers, and the difficulty in understanding complex scientific concepts. It also highlights the importance of incorporating human feedback and knowledge to improve LLM performance in grading open-ended science assessments. The authors acknowledge that LLMs, while useful, are not perfect and require careful consideration and implementation."
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,5,,"The paper discusses the limitations of LLMs in terms of their factual recall capabilities and identifies the additive motif as a limitation, where models compute through summing up multiple independent contributions that may not be sufficient alone but constructively interfere on the correct answer. It also introduces the concept of mixed heads, which are a pair of two separate additive updates from different source tokens, as another limitation. The paper explicitly states that these limitations impact the model's ability to store and retrieve knowledge effectively."
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,4,,"The abstract mentions the need to understand the performance strengths and limitations of existing models, indicating an acknowledgement of limitations. The study aims to enhance understanding and provide references for future development, implying a focus on addressing current limitations. The language used to describe the purpose of the study is also strong and explicit."
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,5,,"The paper explicitly discusses the limitations of LLMs in handling pre-defined tasks and their potential for open-ended tasks. It proposes a method to bridge the gap between graph models and LLMs using a translator, GraphTranslator, to effectively handle both pre-defined and open-ended tasks. The paper mentions that existing methods fail to handle both types of tasks effectively with LLMs. It also mentions that LLMs can only offer a limited interface for handling open-ended tasks and that GraphTranslator enables LLMs to make predictions based on language instructions, providing a unified perspective for both types of tasks. The paper also mentions that the extensive results demonstrate the effectiveness of GraphTranslator on zero-shot node classification and graph question answering, revealing its potential across a broad spectrum of open-ended tasks through language instructions."
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,1,,The limitations of LLMs are not mentioned at all in the abstract. The paper focuses on the potential benefits of using LLMs for automating and expediting reinforcement learning.
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,1,,"The abstract does not mention any limitation of LLMs. Instead, it focuses on their benefits and potential applications."
Large Language Models Are Neurosymbolic Reasoners,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on their potential application as symbolic reasoners in text-based games."
LLMs for Relational Reasoning: How Far are We?,5,,The paper evaluates the reasoning ability of LLMs on the inductive logic programming benchmark and compares their performance with neural program induction systems. It concludes that LLMs are much poorer in terms of reasoning ability than smaller neural program induction systems.
Large Language Models in Plant Biology,1,,"The abstract does not mention any limitation of LLMs. Instead, it focuses on their potential applications in plant biology."
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,3,,"The paper mentions the training of LLMs as a phase, but the limitations are not the main focus of the abstract. Instead, it focuses on the memory system enhancements for conversational agents and their benefits. The abstract does not discuss any specific limitations of LLMs, only mentioning their integration into conversational agents."
GeoGalactica: A Scientific Large Language Model in Geoscience,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on their successes and their application to the geoscience domain."
Large Language Models for Generative Information Extraction: A Survey,1,,"The abstract focuses on the potential of LLMs for IE tasks and the recent advancements in this field. However, it does not mention any limitations of LLMs."
Building Efficient Universal Classifiers with Natural Language Inference,1,,"The paper mentions that LLMs are the mainstream choice for fewshot and zeroshot learning, but it does not discuss any limitations of them. Instead, it focuses on how smaller BERT-like models can be more efficient for classification tasks."
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,5,,The abstract explicitly discusses the potential challenges and limitations of adopting LLMs for IS research.
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,5,,"The paper discusses the limitations of LLMs in detail, including the challenges of not being low-code, not dependency-free, and not knowledge-aware, and proposes a new design pattern to address these limitations. It also mentions the need for fine-tuning LLMs with domain-specific data to enhance performance on domain-specific tasks. The paper provides concrete examples to illustrate the advantages of the proposed design pattern and summarizes the challenges and opportunities introduced by LLMs for further discussions."
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,3,,"The abstract mentions limitations in the context of low-resource languages, but it is not the primary focus. Instead, it emphasizes the effectiveness of LLMs in cross-lingual reranking for African languages."
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,4,,"The abstract mentions the lack of exploration into the potential of newer large language models such as GPT, PaLM, and T5 as a limitation for existing models in aspect-based sentiment analysis. It also highlights the need to evaluate the performance of these newer models and compares their performance with that of prominent models such as DeBERTa, ATAE-LSTM, and flan-t5-large-absa. This suggests that the abstract explicitly discusses the limitations of large language models and their potential impact on aspect-based sentiment analysis."
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,5,,The paper explicitly discusses the limitations of LLMs in handling adversarial inputs and proposes
A Comparative Analysis of Large Language Models for Code Documentation Generation,2,,"The paper discusses the limitations of LLMs in terms of time taken for generation, but it is mentioned as a secondary point. The paper does not discuss any other limitations of LLMs in detail."
TigerBot: An Open Multilingual Multitask LLM,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on the benefits and improvements of the TigerBot models."
Efficiently Programming Large Language Models using SGLang,5,,"The paper explicitly discusses the limitations of LLMs in terms of their complexity and the need for efficient systems for programming and executing their applications. It also mentions the lack of efficient programming systems for LLMs and the need for optimization techniques such as parallelism, batching, caching, sharing, and compilation to address these limitations. The paper proposes SGLang, a language designed specifically for the efficient programming of LLMs, and describes how it enables optimization techniques to improve the performance and efficiency of LLMs. The paper also mentions the need for a high-performance runtime for LLMs and proposes a new technique, RadixAttention, for optimizing the use of the Key-Value cache in LLMs. Overall, the paper provides a detailed discussion of the limitations of LLMs and proposes solutions to address these limitations."
Large Language Models on Graphs: A Comprehensive Survey,5,,The abstract explicitly discusses the limitations of LLMs when applied to graphs and mentions the need for future research to address these limitations. The limitations mentioned include the inability of LLMs to directly process graph data and the lack of generalization of LLM's reasoning ability to graphs. The abstract also discusses the advantages and disadvantages of different approaches to addressing these limitations.
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,4,,"The paper discusses the limitations of Large Language Models in understanding beliefs and their measurement. It argues that existing methods for measuring beliefs in LLMs are unlikely to be successful for conceptual reasons, and there is still no lie-detector for LLMs. The paper also mentions recent arguments that LLMs cannot have beliefs and shows that these arguments are misguided. The paper suggests concrete paths for future work on understanding the status of beliefs in LLMs."
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,1,,"The paper does mention LLMs several times, but it does not discuss any limitations of LLMs in the abstract. Instead, it focuses on how to enable LLMs to perform multimodal tasks using the proposed SPAE method."
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,5,,"The paper discusses the limitations of LLMs and proposes a solution to enhance their reasoning abilities by using a neural-symbolic method called Meta-Reasoning. The method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. The paper also compares the proposed method with Chain-of-Thought technique, highlighting the limitations of the latter and the advantages of the proposed method. The limitations of LLMs mentioned in the paper include inability to handle complex reasoning tasks, learning efficiency, out-of-domain generalization, and output stability."
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5,,"The paper argues that LLMs do not fully understand ranking formulations and proposes a new technique, Pairwise Ranking Prompting, to significantly reduce the burden on LLMs. The paper also compares the performance of LLMs with other approaches and highlights the limitations of LLMs in handling ranking tasks. The paper achieves state-of-the-art ranking performance using moderate-sized LLMs, outperforming larger LLMs and commercial solutions, and proposes several variants to improve efficiency while maintaining competitive results. The paper also discusses the limitations of LLMs in terms of model size and complexity."
Preference Ranking Optimization for Human Alignment,5,,"The paper discusses the limitations of LLMs in terms of misleading content and the need for human alignment. It mentions the complexity, instability, and sensitivity to hyperparameters of RLHF, which is used for human alignment, as a limitation. It also mentions the lack of macro-level contrasts in RLHF. The paper proposes an alternative method, Preference Ranking Optimization (PRO), to directly fine-tune LLMs for human alignment and states that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses. This suggests that the limitations of LLMs are significant and that there is a need for more effective methods for human alignment."
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5,,"The paper explicitly discusses the challenges and limitations of LLMs, including ethical considerations, model biases, interpretability, and computational resource requirements. The paper also highlights techniques for enhancing the robustness and controllability of LLMs and addressing bias, fairness, and generation quality issues. The title itself mentions the limitations of LLMs. The paper provides a comprehensive overview of the state-of-the-art knowledge in the field, making it a valuable resource for further advancements in the development and utilization of LLMs."
Concept-Oriented Deep Learning with Large Language Models,5,,The abstract explicitly mentions that LLMs have limitations in representing only symbolic knowledge and that multimodal LLMs are required to represent the full range of human knowledge. It also discusses the limitations of text-only LLMs for concept-oriented deep learning and the importance of conceptual understanding in visual-language LLMs.
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5,,"The paper discusses the limitations of LLMs, such as their potential for generating ungrounded or erroneous responses, and the need for effective methods to calibrate their confidence levels. The authors propose a solution to this problem using a Pareto optimal self-supervision framework, which can systematically calibrate LLM responses and facilitate error correction. The experiments demonstrate that the proposed risk score is highly correlated with the actual LLM error rate, and using a dynamic prompting strategy based on the risk score significantly improves the accuracy of off-the-shelf LLMs. The paper also mentions that LLMs lack an effective method for calibrating their confidence levels and that expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations, can be used for this purpose. The authors also compare the performance of LLMs to that of SOTA weak supervision models and SOTA supervised models."
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,1,,"The abstract does not mention any limitations of LLMs. Instead, it focuses on their impressive performance and capabilities."
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,5,,"The paper discusses the limitations of LLMs extensively, including their biases and high querying cost. It presents empirical evidence that attributed prompts can mitigate these limitations and improve model performance. The study demonstrates that simple class-conditional prompts exhibit significant biases and that attribute diversity plays a crucial role in enhancing model performance. The paper also highlights the high querying cost of LLMs and shows that attributed prompts can achieve similar performance at a significantly lower cost."
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,3,,The paper discuss
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,5,,The paper explicitly discusses the limitations of LLMs in understanding context and addressing ambiguity.
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,2,,"The paper focuses on using LLMs for failure explanation and correction in robotic systems, but it does not mention any limitations of LLMs in the abstract. The limitations are only mentioned in the context of the need for a language-based planner to correct the failures."
Exploring the Robustness of Large Language Models for Solving Programming Problems,5,,"The paper discusses the limitations of LLMs, specifically in the context of code generation tasks. The authors conduct experiments to evaluate the robustness of several popular LLMs and find that they are sensitive to superficial modifications of problem descriptions and rely on variable names. They also compare the performance of these models to SOTA models, which show higher robustness. The paper emphasizes the importance of careful formatting of prompts for high-quality code generation and highlights the limitations of current LLMs in this regard."
Language models are weak learners,1,,"The abstract does not mention any limitation of LLMs, instead, it focuses on their strengths and potential applications in machine learning pipelines."
Teaching Large Language Models to Self-Debug,5,,"The paper discusses limitations of LLMs, such as their lack of understanding of code correctness and error messages, and their inability to perform debugging tasks effectively without human feedback. The paper proposes a solution to these limitations by teaching LLMs to debug their own predictions via few-shot demonstrations, which significantly improves their performance on code generation tasks. The paper also highlights the sample efficiency of the proposed solution and its ability to outperform baseline models that generate more candidates."
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,2,,"The paper mentions that fine-tuning LLMs on code corpora is computationally expensive, but it focuses on proposing a method to reduce the computational cost using transfer learning. It does not discuss any specific limitations of LLMs."
On the Possibilities of AI-Generated Text Detection,5,,"The paper discusses limitations of LLMs in detail, focusing on their indistinguishability from human-generated text as they approach human-like quality, and the need for larger sample sizes for detection. The paper also presents empirical evaluations and findings related to LLMs' limitations."
Learnings from Data Integration for Augmented Language Models,5,,"The paper directly states that one limitation of LLMs is that they lack access to up-to-date, proprietary, or personal data. The paper also compares LLMs to data integration systems, emphasizing that both aim to provide access to large collections of data. The limitations of LLMs are a primary focus of the paper."
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,5,,"The abstract explicitly discusses the limitations of LLMs in handling graph data and performing multi-step logic reasoning, precise mathematical calculation, and perception about spatial and temporal factors."
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,3,,"The abstract mentions the limitations of LLMs by stating that there is a lack of benchmark datasets for assessing their ability to generate functionally correct code edits. It also mentions that there are significant and reproducible advancements in LLM capabilities, but it does not elaborate much on the limitations themselves."
Revisiting Automated Prompting: Are We Actually Doing Better?,1,,"The abstract does not mention any limitations of LLMs. Instead, it discusses the effectiveness of automated prompting and manual prompts for LLMs."
Instruction Tuning with GPT-4,2,,"The abstract mentions the limitations of the instruction-following data generated by previous state-of-the-art models, but it does not explicitly discuss limitations of LLMs themselves. The limitations mentioned are related to the instruction-following data, not the models."
Exploring Language Models: A Comprehensive Survey and Analysis,5,,"The abstract explicitly discusses several limitations of large language models, including model bias, interpretability, data privacy, and environmental impact. It also highlights the challenges that need to be addressed, indicating a detailed discussion of these limitations."
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",1,,"The paper discusses the use of LLMs, but it does not mention any limitations of them. Instead, it focuses on the copyright implications of using open data sets to train LLMs."
Challenges and Limitations of ChatGPT and Other Large Language Models,5,,"The title and abstract both explicitly mention ""Challenges and Limitations of ChatGPT and Other Large Language Models,"" and the text goes on to discuss several specific limitations of these models, including their understanding of context, handling of rare words, and tendency to generate offensive text. The article also acknowledges broader concerns around the environmental impact, bias, and interpretability of large language models."
Document-Level Machine Translation with Large Language Models,5,,"The paper evaluates the limitations of LLMs in the context of document-level machine translation, focusing on their ability to model discourse and compare their performance with commercial MT systems and advanced methods. The study finds that LLMs outperform commercial MT systems and shows that they have stronger abilities for probing linguistic knowledge, but also acknowledges challenges and opportunities for future design and evaluation of LLMs."
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,4,,"The paper discusses the limitations of deep bidirectional RNNs in the context of language modeling, including the inability to capture long-range dependencies effectively and the high computational requirements. These limitations are directly related to the limitations of language models in general."
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,1,,"The abstract does not mention any limitation of LLMs explicitly. The focus is on the domain adaptation of Retrieval Augmented Generation (RAG) models, not on the limitations of LLMs themselves."
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,3,,"The paper discusses the limitations of transformer models in handling agreement in French, which is a limitation of language models in general, but it is not the main focus of the paper and is mentioned as a secondary point. The paper mainly focuses on understanding the internal working of transformers and the difference in how they handle subject-verb and object-past participle agreements in French."
On the Role of Negative Precedent in Legal Outcome Prediction,N/A.,,"The abstract does not mention language models at all. It talks about legal outcome prediction and improving models for that task, but it does not mention any limitations or challenges related to language models."
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,N/A.,,The paper does not mention large language models or their limitations. It focuses on semantic parsing and cross-lingual generalization.
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,5,,"The abstract mentions the problem of inaccessibility to large-scale task-oriented dialogue data with annotated structured dialogue state, which prevents the development of pretrained language models for task-oriented dialogue. It also mentions the need to bridge the gap between the pretraining method and downstream tasks, and uses strong wording such as ""prevents"" and ""alleviate this problem"" to discuss the limitations of LLMs. The abstract also proposes a simple yet effective pretraining method to address these limitations, which shows a clear recognition of the issues with existing LLMs."
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,N/A,,"The paper does not mention any language model or discuss any limitations of language models. Instead, it focuses on using the pronunciations of neighboring names to guess the pronunciation of a given target feature."
Locally Typical Sampling,3,,"The abstract mentions that ""Todays probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity)"". It also states that ""high-probability texts can be dull or repetitive"" and ""humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner"". These limitations are mentioned as secondary points, but they do discuss some limitations of LLMs."
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,N/A.,,The paper focuses on cross-lingual parsing and does not mention language models.
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,5,,"The paper discusses limitations of multilingual task-oriented dialogue datasets, which includes limitations of LLMs used for multilingual dialogue systems. The limitations mentioned are: 1) lack of naturalness and cultural specificity in the target language due to translation-based datasets, and 2) failure to cover many possible dialogue flows and limited scale in current multilingual ToD datasets. The paper also mentions that these limitations are addressed by the proposed outline-based annotation process, which aims to improve the quality of the data for LLMs."
Modeling Emotion Dynamics in Song Lyrics with State Space Models,N/A.,,The title and abstract do not mention any language models or their limitations.
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,1,,"The abstract does not mention any limitation of Large Language Models at all, or even if it talks about LLMs, it does not mention any limitation of them. Instead, it talks about a visual-linguistic model."
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,1,,"The paper does not mention any limitation of LLMs specifically. Instead, it discusses data augmentation methods to improve data efficiency in NLP, which can be used with LLMs but is not limited to them."
Coreference Resolution through a seq2seq Transition-Based System,1,,"The limitations of LLMs are not mentioned at all in the abstract. Instead, the abstract focuses on the advantages and performance improvements of using LLMs for coreference resolution."
Transformers for Tabular Data Representation: A Survey of Models and Applications,1,,"The abstract does not mention any limitations of LLMs. Instead, it discusses the applications and solutions for extending LLMs to tabular data."
Generative Spoken Dialogue Language Modeling,1,,The abstract does not mention any limitations of LLMs. It focuses on introducing a new model called dGSLM and its ability to generate audio samples of naturalistic spoken dialogues using unsupervised spoken unit discovery and a dual-tower transformer architecture. The limitations of LLMs are not discussed in the abstract.
Discontinuous Combinatory Constituency Parsing,N/A.,,The abstract does not mention anything about language models or their limitations. It is about a new parsing algorithm.
Efficient Long-Text Understanding with Short-Text Models,5,,The abstract explicitly mentions the quadratic complexity and inability of transformer-based LMs to process long sequences as a limitation. It also mentions the need for expensive pretraining for efficient transformer variants and the competition with specialized models that are up to 50x larger.
Hate Speech Classifiers Learn Normative Social Stereotypes,5,,"The paper discusses the limitations of hate speech classifiers, which are a type of language model, in reflecting and perpetuating social stereotypes and biases. The paper provides evidence of this limitation through annotation experiments and the analysis of prediction errors in a hate speech classifier. The paper also discusses the potential implications of these limitations for machine learning fairness and social inequalities."
Domain-Specific Word Embeddings with Structure Prediction,1,,"The paper focuses on the benefits of domain-specific word embeddings and improving the performance of word embeddings for specific domains, but it does not mention any limitations of LLMs. Instead, it presents a method to overcome one of the challenges in LLMs, which is the lack of domain-specific information."
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5,,"The abstract discusses the limitations of larger Transformer-based language models, specifically their tendency to 'memorize' sequences during training, which makes their surprisal estimates diverge from humanlike expectations. This is a significant limitation, as it can impact the accuracy and usefulness of these models for studying human language processing. The abstract also presents empirical evidence to support this claim, including regression analyses and analysis of residual errors."
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,2,,"The paper focuses on the robustness of conversational question answering (CQA) systems, which is a problem that can be addressed by using large language models (LLMs) or by designing specific methods for modeling conversational history. However, the paper does not explicitly discuss the limitations of LLMs in this context, but only mentions them as a potential solution for the problem of robustness. The limitations of LLMs, such as their tendency to generate incorrect or biased answers, are not mentioned or discussed in the abstract. Instead, the paper focuses on the design and evaluation of a novel prompt-based history modeling approach that is shown to be effective and robust across various settings."
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,N/A.,,"The paper does not mention language models at all, let alone their limitations. It focuses on semantic parsing and data synthesis."
Naturalistic Causal Probing for Morpho-Syntax,1,,"The abstract mentions pre-trained models (BERT, RoBERTa, GPT-2) in the context of natural language processing, but it does not mention any limitations of these models. The focus of the paper is on probing methods for understanding the behavior of these models and the importance of naturalistic causal probing, not on the limitations of the models themselves."
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,N/A.,,"The paper focuses on a different problem, brand-topic modeling, and does not mention any limitations related to large language models."
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,N/A.,,The paper does not mention anything related to language models or their limitations. It focuses on human dubbing practices and challenges common assumptions in the literature.
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,2,,"The paper mentions that pre-trained language models such as BERT are not ""structurally ready"" for dense passage retrieval and that they need to be adapted to this task. However, the limitations of LLMs are not the main focus of the paper, and the paper instead proposes a solution for adapting LLMs to dense passage retrieval."
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,1,,"The paper mentions the use of state-of-the-art models of conversational knowledge identification and open-domain question answering, but it does not mention any limitation of Large Language Models specifically. Instead, it focuses on the performance of the models on a specific dataset."
Sub-Character Tokenization for Chinese Pretrained Language Models,1,,The abstract does not mention any limitations of Large Language Models at all. The focus is on tokenization and its impact on the efficiency and robustness of language models.
Erasure of Unaligned Attributes from Neural Representations,2,,"The paper discusses the limitations of erasing information from neural representations in the context of language models, specifically in relation to the strong entanglement between the main task and the information to be erased. However, the limitations are mentioned as secondary points and the focus of the paper is on the proposed algorithm for erasing information."
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,5,,"The paper explicitly states that current models (implicitly referring to LLMs) suffer from spurious correlations and generate irrelevant and generic responses, which are major limitations of LLMs. The paper proposes a method to mitigate these limitations."
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5,,The paper discusses the limitations of transformer-based LLMs in terms of their inability to solve certain classes of problems such as linear equalities and membership in arbitrary context-free grammars with empty productions. The limitations arise due to the logarithmic arithmetic precision and logspace-uniform threshold circuits used to simulate these models. The paper also speculates that other parallelizable model architectures may have similar limitations.
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,5,,"The paper explicitly discusses the issue of hallucinations in neural sequence generation models (LLMs) and the limitations of these models to produce accurate and reliable outputs. The authors present a method for detecting hallucinations in LLMs, which shows the importance of understanding and addressing the limitations of these models."
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,N/A,,The abstract does not mention anything about language models. It focuses on the generation of coherent stories from a new image-grounded dataset called Visual Writing Prompts (VWP).
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,N/A.,,"The paper does not mention language models or their limitations at all. It focuses on sequence-to-sequence models and their applications in various text generation tasks, but it does not discuss any limitations of language models in general."
Questions Are All You Need to Train a Dense Passage Retriever,1,,"The limitations of LLMs are not mentioned in the abstract at all. Instead, the abstract focuses on a new approach for training dense retrieval models using unsupervised learning and passage-retrieval autoencoding. The abstract does not even mention the limitations of existing LLMs as a motivation for their work."
Transparency Helps Reveal When Language Models Learn Meaning,5,,"The paper discusses how language models struggle to learn meaning in context-dependent languages and natural language, specifically in relation to referential opacity. It also shows that language models do not represent natural language semantics well due to the context-dependent nature of natural language form-meaning mappings."
Visual Spatial Reasoning,5,,"The abstract explicitly mentions that the state-of-the-art models are ""in general incapable of recognising relations concerning the orientations of objects,"" which is a significant limitation of large language models in understanding spatial relations and context."
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,5,,"The paper explicitly discusses the limitations of LLMs, stating that they can sometimes copy substantially from the training data, even duplicating passages over 1,000 words long. It also performs manual analysis and finds evidence of semantic issues in the generated text."
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,N/A,,The paper does not mention Large Language Models at all. It focuses on few-shot region-aware machine translation and presents a new dataset and evaluation benchmark for this task.
OpenFact: Factuality Enhanced Open Knowledge Extraction,N/A.,,"The paper focuses on the OpenFact dataset, which is a knowledge graph dataset, not on language models. The title and abstract do not mention any limitations of language models."
On Graph-based Reentrancy-free Semantic Parsing,N/A.,,"The title and abstract do not mention language models at all. Instead, they discuss semantic parsing and propose a new approach for it."
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,N/A.,,The title and abstract do not mention language models or their limitations.
Chinese Idiom Paraphrasing,1,,"The abstract does not mention any limitation of LLMs at all. Instead, it mentions using LLMs to preprocess Chinese datasets and improve the performance of Chinese NLP tasks. The limitations of LLMs are not discussed at all."
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,1,,"The abstract does not mention any limitations of LLMs. It only talks about how Mosaic, a multi-modal pre-training framework, enables models to learn rich visual and linguistic features jointly and outperforms previous models on several benchmarks for visual grounding and visual question answering."
Rank-Aware Negative Training for Semi-Supervised Text Classification,N/A,,The abstract does not mention anything about language models or their limitations. The focus is on text classification and semi-supervised learning.
MACSum: Controllable Summarization with Mixed Attributes,N/A.,,The paper does not mention anything about language models in the abstract.
MENLI: Robust Evaluation Metrics from Natural Language Inference,4,,"The paper discusses the limitations of semantic similarity models like BERT, which are a type of large language model. The authors argue that these models are vulnerable to adversarial attacks and that natural language inference (NLI) metrics are a more appropriate modeling for evaluating LLMs. The paper also shows that combining existing metrics with NLI metrics can improve both adversarial robustness and quality metrics for LLMs."
Efficient Methods for Natural Language Processing: A Survey,1,,"The abstract does mention large language models, but it does not mention any limitations of them. Instead, it focuses on the need for efficient methods to overcome the resource consumption challenges associated with training and using them."
Abstractive Meeting Summarization: A Survey,1,,"The abstract does not mention any limitations of language models. Instead, it focuses on the potential of language models to improve abstractive summarization."
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,1,,"The paper uses neural language models to approximate human predictive distributions, but it does not mention any limitation of these models. The limitations of language models are not discussed in the abstract at all."
Reasoning over Public and Private Data in Retrieval-Based Systems,N/A.,,The abstract does not mention language models or their limitations at all. It discusses retrieval systems and their limitations when dealing with private data.
Multilingual Coreference Resolution in Multiparty Dialogue,1,,"The abstract does not mention any limitations of LLMs at all. Instead, it discusses the challenges of creating a large-scale multilingual multiparty dialogue dataset and the limitations of off-the-shelf models on it."
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,1,,"The abstract does not mention any limitations of LLMs at all. Instead, it talks about the advantages of a specific type of LLM, namely non-autoregressive language models, and their pre-training methods."
Time-and-Space-Efficient Weighted Deduction,N/A.,,None of the mentioned concepts are related to language models.
Conditional Generation with a Question-Answering Blueprint,3,,"The paper discusses the limitations of ""neural seq-to-seq models"" which can be considered a type of LLMs, but it mainly focuses on their inability to convey relevant and faithful information and the need for planning to address this issue. It does mention some limitations of LLMs, but they are not the main focus of the paper."
Collective Human Opinions in Semantic Textual Similarity,5,,"The paper highlights that current STS models (presumably including LLMs) cannot capture the variance caused by human disagreement on individual instances. This is a significant limitation, as it means that LLMs may not be able to accurately model the uncertainty and variability in human language use, which is an important aspect of semantic textual similarity. The paper also notes that averaging human ratings, which is a common approach in evaluating LLMs, masks the true distribution of human opinions and prevents models from capturing the semantic vagueness that the individual ratings represent."
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,N/A.,,The paper focuses on analyzing biases in crowdsourced linguistic annotations and comparing the performance of two annotation tasks for implicit discourse relation annotation. It does not mention any limitation of LLMs or discuss them in the abstract.
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,N/A,,The paper focuses on language universals in neural agent-based simulations and does not mention any limitations of large language models.
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,N/A.,,"The paper does not discuss Large Language Models or their limitations. Instead, it focuses on the cross-linguistic statistical patterns of word order in natural languages and the functional pressures that influence them."
Cross-functional Analysis of Generalization in Behavioral Learning,2,,"The paper mentions ""three representative NLP tasks"" and ""models"" without specifying the type, but it is reasonable to assume that they refer to language models given the context. The limitations discussed in the abstract are secondary points, focusing on the risk of overfitting and the need for domain generalization methods. The abstract does not provide a detailed discussion of the limitations of LLMs."
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,2,,"The paper uses LLMs for generating minimally edited questions as challenging contrast sets for evaluating OpenQA models, but it does not mention any specific limitations of LLMs in the abstract. Instead, it focuses on the limitations of the OpenQA model DPR in handling contrast consistency."
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,3,,"The limitations of LLMs are mentioned as a reason for the need for the proposed method, but they are not discussed in detail. The paper focuses on the proposed method for zero-shot domain transfer and improving the transferability of task training."
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,N/A,,"The paper does not mention LLMs or language models in general. Instead, it focuses on a multilingual retrieval dataset, MIRACL, and its importance in enhancing information access capabilities for diverse populations."
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,N/A.,,The paper does not mention language models at all. The focus is on dataset mention detection and the creation of a large dataset for this task.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,3,,"The abstract mentions that the performance of LMs varies significantly across languages and tasks, suggesting that the superposition of language modelling and classification tasks is not always effective. However, this is only mentioned as a secondary point. The main focus of the paper is on the proposed approach to separate the translation and classification stages."
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",N/A.,,"The paper focuses on mathematical language processing, not large language models."
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,N/A.,,The paper focuses on the cognitive science of adjective ordering and does not mention any limitations of language models.
Improving Multitask Retrieval by Promoting Task Specialization,1,,"The abstract mentions pretrained models, but it does not discuss any limitations of language models. Instead, it focuses on improving multitask retrieval by promoting task specialization and using a better choice of pretrained model."
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,1,,"The abstract does not mention any limitations of language models specifically. It talks about sequence generation models and their calibration, but it does not discuss any limitation related to LLMs."
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,1,,The paper does not mention any limitation of large language models at all. It only talks about the difficulties of collecting labeled data for answer selection models.
Benchmarking the Generation of Fact Checking Explanations,1,,The paper does not mention any limitations of LLMs in the abstract. It focuses on summarization approaches over unstructured knowledge for generating justifications and benchmarking their performance.
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,N/A.,,"The abstract does not mention any language models, large or otherwise, and does not discuss their limitations. It talks about named entity recognition and proposes a new method for it."
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,5,,"The abstract explicitly states that the study evaluates the ability of LLMs to reason about states and that there is a large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge."
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,N/A.,,"The paper does not mention large language models or their limitations at all. Instead, it focuses on Open Relation Extraction (ORE) and proposes a unified framework for both Zero-shot and Unsupervised ORE. The proposed method, U-CORE, utilizes Contrastive Learning (CL) and Clustering techniques to enhance the accuracy and efficiency of the model. The paper also discusses the limitations of CL-based Zero-shot ORE methods and the data-shifting problems, but these are not related to large language models."
In-Context Retrieval-Augmented Language Models,3,,"The paper mentions that In-Context RALM can mitigate the problem of factually inaccurate text generation, which is a known limitation of LLMs. However, this is only mentioned as a secondary benefit, and the main focus of the paper is on the performance gains of using retrieved documents as input to the LLM."
Learning to Paraphrase Sentences to Different Complexity Levels,1,,The abstract does not mention any limitations of Large Language Models. It only mentions their performance on the tasks.
Direct Speech Translation for Automatic Subtitling,N/A.,,The paper does not mention any limitation of Large Language Models. It only talks about the problem of automatic subtitling and proposes a solution.
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,5,,"The paper explicitly discusses the limitations of LLMs in their ability to generalize to abstract, unseen contexts, and their bias towards linear order. It also mentions the data-intensive nature of their training as a related limitation."
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",N/A,,"The paper discusses the limitations of multilingual, multidomain, multi-parallel ToD datasets, but it does not mention any limitation related to Language Models."
Can Authorship Representation Learning Capture Stylistic Features?,1,,"The abstract does not mention any limitations of Large Language Models. Instead, it talks about limitations of authorship representations, which is a separate topic."
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,N/A.,,"The abstract does not mention any limitation of large language models. Instead, it discusses cross-lingual semantic parsing, optimal transport, and probabilistic latent variables."
