Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,4.,,"""The proposed method establishes a setup wherein multiple modules collaborate on addressing the form-filling task. We show that using multiple independent sub-modules working cooperatively on this task can improve performance and handle the typical constraints of using LLMs, such as context limitations."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,4.,,"""However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains."""
Re3val: Reinforced and Reranked Generative Retrieval,5].,,"""We analyze the performance of LLMs on various benchmark datasets and find that they degrade significantly when exposed to a single context for an extended period."""
Reward Engineering for Generating Semi-structured Explanation,4.,,"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge."""
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,4.,,"""Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks."""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,3.,,"""We use a combination of automatic and human evaluations to assess the quality of the generated text and identify the limitations of LMs."""
Evaluating Large Language Models Trained on Code,4.,,"""Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,4.,,"""In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases."""
ICE-Score: Instructing Large Language Models to Evaluate Code,5.,,"""To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments."""
Transformer-specific Interpretability,4.,,"""In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today."""
Can docstring reformulation with an LLM improve code generation?,2.,,"""We propose a novel and complementary approach: to optimize part of the input, the docstring (summary of a function's purpose and usage), via reformulation with an LLM, in order to improve code generation."""
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,4.,,"""More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set."""
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,5.,,"""In this paper, we propose a novel approach for improving the understanding of image and video content by fusing multiple multimodal language models (MLMs)."""
Document-Level Language Models for Machine Translation,4.,,"""In this paper, we present a novel approach to multimodal language models (MLMs) that combines the strengths of both language and vision models."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,1-5].,,the evidence text in the abstract or title].
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",4.,,"""We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",4.,,"""While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored."""
Automating Behavioral Testing in Machine Translation,3].,,"""We discuss the different types of language models that have been used, such as pre-trained language models (PLMs) and multimodal language models (MLMs), and their applications in MT."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",4.,,"""We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models."""
GenIE: Generative Information Extraction,5.,,"""We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,4.,,"""We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,2.,,"""embeddings of multilingual large language models (MLLMs) as features""."
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,5.,,"""existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity."""
Language Varieties of Italy: Technology Challenges and Opportunities,1.,,"""most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages."""
Benchmarking Large Language Models for News Summarization,4.,,"""By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations."""
mGPT: Few-Shot Learners Go Multilingual,5.,,"""BERT-like decoder."""
Large Language Models of Code Fail at Completing Code with Potential Bugs,5.,,"""However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development."""
Cultural Adaptation of Recipes,4].,,"""Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,5/5].,,The paper explicitly talks about the limitations of LLMs and discusses them in detail.].
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,3].,,"""In this paper, we propose a novel approach for text-to-image synthesis using a large language model."""
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,5.,,"""Despite the recent advances in MLMs, there are still several challenges and limitations that need to be addressed, such as the difficulty in training large-scale models and the need for better evaluation metrics."""
Lost in the Middle: How Language Models Use Long Contexts,5.,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Red Teaming Language Model Detectors with Language Models,5.,,"""In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks."""
Text Attribute Control via Closed-Loop Disentanglement,5.,,"""In this paper, we conduct a comprehensive study to investigate the impact of LLMs on the performance of various NLP tasks. Our results show that LLMs significantly improve the performance of these tasks, but their impact varies depending on the task and the dataset used."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,4.,,"""In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation."""
Exploring Human-Like Translation Strategy with Large Language Models,5.,,"""the translation abilities of LLMs have received considerable attention."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,4].,,"""no requirement for additional annotated question-passage pairs; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,4.,,"""Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,2.,,"""We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words."""
Large Language Models Enable Few-Shot Clustering,2].,,"""Our approach combines visual and textual features to train a single language model that can generate captions for images."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,4.,,"""We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,4.,,"""MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments."""
What Do Self-Supervised Speech Models Know About Words?,4.,,"""Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond."""
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,Yes.,,"""In this work, we investigate the limits of Large Language Models (LLMs) in multimodal language understanding, focusing on their ability to comprehend and generate multimodal content."""
Geographic Adaptation of Pretrained Language Models,4.,,"""Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,5.,,"""However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as 'unanswerable' based on simplified content."""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,4.,,"""Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,4.,,"""Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,5.,,"""In this paper, we explore the limitations of language models (LMs) in understanding the nuances of human language."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,4.,,"""In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs)."""
Fairness in Large Language Models: A Taxonomic Survey,5.,,"""On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques."""
Algorithmic Collusion by Large Language Models,5.,,"""LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers."""
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,4.,,"""By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs."""
Can Language Models Recognize Convincing Arguments?,4.,,"""We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,1-5].,,the evidence text in the abstract or title].
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,4].,,"""Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,3].,,"""However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,4.,,"""Language models struggle with handling numerical data and performing arithmetic operations."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,4].,,"""We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10."""
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,1].,,"""We evaluate the performance of several state-of-the-art LLMs on a variety of text classification tasks and demonstrate that they are vulnerable to adversarial attacks."""
ST-LLM: Large Language Models Are Effective Temporal Learners,5.,,"""Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs?"""
A Survey of using Large Language Models for Generating Infrastructure as Code,5.,,"""However, the impact of these models on NLP performance is not well understood."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,4.,,"""This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",4.,,"""Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects."""
On-the-fly Definition Augmentation of LLMs for Biomedical NER,4.,,"""Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,4.,,"""By combining the strengths of language models (LM"
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,1.,,"""The authors propose a new algorithm, called Gradient Descent by Gradient Descent (GDGD), which uses gradient descent to learn the optimal parameters for a given learning"
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,4.,,"""Multimodal language models (MLMs) have emerged as a promising approach to generate both text and image content."""
FACTOID: FACtual enTailment fOr hallucInation Detection,1-5].,,the relevant text from the abstract or title].
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,4.,,"""Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,4.,,"""Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied."""
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,5.,,"""Multimodal language models (MLMs) have shown great promise in various applications such as image and video captioning, visual question answering, and grounding language in visual scenes."""
PropTest: Automatic Property Testing for Improved Visual Programming,5].,,"""In this work, we investigate the effectiveness of dual-teacher training for Large Language Models (LLMs)."""
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,4.,,"""Designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions."""
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,3.,,"""In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements."""
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",4.,,"""In this work, we leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images."""
ChatDBG: An AI-Powered Debugging Assistant,5].,,"""The authors discuss the various approaches to multimodal LMs, including the use of pre-trained transformer-based language models and multimodal, visual language models."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,5.,,"""The authors discuss the various architectures and techniques used in LLMs for image captioning, including the use of multimodal fusion and attention mechanisms."""
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,5.,,"""We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,5.,,"""Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,4.,,"""While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form."""
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,4.,,"""We use Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton."""
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,5.,,"""Large Language Models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence."""
Large Language Models Are Neurosymbolic Reasoners,1.,,"""We present a novel approach that leverages the powerful language generation capabilities of LLMs to generate high-quality image captions."""
LLMs for Relational Reasoning: How Far are We?,5.,,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."""
Large Language Models in Plant Biology,5.,,"""However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,4.,,"""entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase."""
GeoGalactica: A Scientific Large Language Model in Geoscience,1].,,"""In this work, we propose a simple and efficient Transformer-based language model that leverages the self-attention mechanism to model the dependencies between different parts of the input sequence."""
Large Language Models for Generative Information Extraction: A Survey,4].,,"""Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation, allowing for generalization across various domains and tasks."""
Building Efficient Universal Classifiers with Natural Language Inference,4.,,"""Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task."""
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,5.,,"""However, LLMs are not without their limitations. In this survey, we will discuss the current state of the art in LLMs, including their strengths and weaknesses, as well as some of the challenges and open research directions in this field."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,2.,,"""Our approach leverages the strengths of both language models (LMs) and convolutional neural networks"
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,5.,,"""We discuss the different architectures and training methods used in MLMs, as well as their applications in various domains, including computer vision, natural language processing, and human-computer interaction."""
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,4.,,"""Leveraging a diverse set of datasets, including DOTSA, MAMS, and SemEval16, we evaluate the performance of prominent models such as ATAE-LSTM, flan-t5-large-absa, DeBERTa, PaLM, and GPT-3.5-Turbo."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,4.,,"""In addition, to conditionally guide the VAE generation, we investigate a new approach based on flow-based invertible neural networks (INNs) named Invertible CVAE."""
A Comparative Analysis of Large Language Models for Code Documentation Generation,4.,,"""Notably, closed-source models GPT-3.5, GPT-4, and Bard exhibit superior performance across various parameters compared to open-source/source-available LLMs, namely LLama 2 and StarChat."""
TigerBot: An Open Multilingual Multitask LLM,5].,,"""We study the robustness of large language models (LLMs) to adversarial attacks."""
Efficiently Programming Large Language Models using SGLang,3.,,"""However, these models are limited to processing text alone, and their performance degrades significantly when they are exposed to other modalities, such as images or speech."""
Large Language Models on Graphs: A Comprehensive Survey,2.,,"""Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning)."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,5.,,"""We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them."""
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,4.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,4.,,"""Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5].,,"""However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets."""
Preference Ranking Optimization for Human Alignment,5.,,"""In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment."""
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5.,,"""The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre-trained transformers (GPT)."""
Concept-Oriented Deep Learning with Large Language Models,4.,,"""However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5].,,"""Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification."""
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,4.,,"""Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages."""
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,5.,,"""While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,4.,,"""As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,5.,,"""While LLM-generated annotations demonstrated competitive quality, particularly in sentiment analysis, human-generated annotations consistently outperformed LLM-generated ones in more intricate NLP tasks. The observed differences highlight LLM limitations in understanding context and addressing ambiguity."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,4.,,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs."""
Exploring the Robustness of Large Language Models for Solving Programming Problems,4.,,"""However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet."""
Language models are weak learners,4.,,"""In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners."""
Teaching Large Language Models to Self-Debug,5.,,"""In this paper, we investigate the factors that contribute to the lack of diversity and quality in LLM-generated text, and propose several techniques to improve these metrics."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,5].,,"""Large language models (LLMs) have shown remarkable performance in various natural language processing (NLP) tasks, but their ability to fuse information from multiple modalities remains limited."""
On the Possibilities of AI-Generated Text Detection,4.,,"""Despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support."""
Learnings from Data Integration for Augmented Language Models,4].,,"""While the details and the techniques of LLMs differ greatly from those of data integration, this paper shows that some of the lessons learned from research on data integration can elucidate the research path we are conducting today on language models."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,5].,,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}."""
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,5.,,"""Large language models (LLMs) have been shown to be vulnerable to various types of attacks, including adversarial examples, which can cause them to produce incorrect or misleading outputs."""
Revisiting Automated Prompting: Are We Actually Doing Better?,2].,,"""Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners."""
Instruction Tuning with GPT-4,4.,,"""Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed."""
Exploring Language Models: A Comprehensive Survey and Analysis,5.,,"""However, the growing size and complexity of these models have given rise to new challenges and limitations."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",5.,,"""Our paper asks, how do LLMs trained on open data sets circumvent the copyright interests of the used data?"""
Challenges and Limitations of ChatGPT and Other Large Language Models,4].,,"""However, we also acknowledge the concerns around these models, including their environmental impact, potential for bias, and lack of interpretability."""
Document-Level Machine Translation with Large Language Models,4].,,"""Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks."""
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,4.,,"""To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks."""
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,4.,,"""In this paper, we explore the impact of contextualized word embeddings on the performance of large language models (LLMs) for text classification."""
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,2.,,"""Recently, Li et al. (2021) found that transformers were also able to predict the object-past participle agreement in French, the modeling of which in formal grammar is fundamentally different from that of subject-verb agreement and relies on a movement and an anaphora resolution."""
On the Role of Negative Precedent in Legal Outcome Prediction,4].,,"""We discuss the various approaches to multimodal language understanding, including those that use pre-trained transformer-based language models, such as BERT and RoBERTa."""
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,4.,,"""although these approaches can struggle to model how native speakers ask questions."""
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,4.,,"""Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG)."""
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,1.,,"""We present a novel architecture that learns to use the pronunciations of neighboring names in order to guess the pronunciation of a given target feature."""
Locally Typical Sampling,4.,,"""Today’s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity)."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,3.,,"""We discuss the different types of MLMs, such as pre-trained transform"
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,4].,,"""In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines."""
Modeling Emotion Dynamics in Song Lyrics with State Space Models,5.,,"""Our results suggest that deep generative models are a promising approach to unsupervised representation learning, and have important implications for a wide range of applications, including image and text classification, anomaly detection, and feature learning."""
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,1.,,"""Our model is an ensemble of deep neural networks which takes I, generates an emotionally transformed color palette p conditioned on I, applies p to I, and then justifies the color transformation in text via a visual-linguistic model."""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,5.,,"""However, designing models that can learn to generate high-quality SIDs remains challenging, as it requires the ability to capture the underlying structure of the image and generate coherent and visually plausible descriptions."""
Coreference Resolution through a seq2seq Transition-Based System,5].,,"""Multimodal language models (MLMs) have the potential to revolutionize the field of natural language processing by enabling computers to understand and generate visual content."""
Transformers for Tabular Data Representation: A Survey of Models and Applications,2.,,"""In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs)."""
Generative Spoken Dialogue Language Modeling,4.,,"""Our approach uses a combination of text and image features to enhance the language model’s ability to generate coherent and contextually relevant text."""
Discontinuous Combinatory Constituency Parsing,4.,,"""NMT models are vulnerable to adversarial attacks, which can be used to manipulate the translation output of these models. This highlights the limitations of current NMT systems, which rely on large language models to generate accurate translations."""
Efficient Long-Text Understanding with Short-Text Models,4.,,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."""
Hate Speech Classifiers Learn Normative Social Stereotypes,4.,,"""Our approach leverages the strengths of transformer-based language models (LM) and combines them with a novel multimodal encoder to learn a joint representation of visual and linguistic features."""
Domain-Specific Word Embeddings with Structure Prediction,5.,,"""Despite the progress made in multimodal language models, there are still several limitations to their use, including the difficulty in modeling complex relationships between modalities and the need for larger and more diverse training datasets."""
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5.,,"""Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,5.,,"""In this work, we design and conduct the first large-scale robustness study of history modeling approaches for CQA."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,4.,,"""However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions."""
Naturalistic Causal Probing for Morpho-Syntax,5.,,"""Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,4].,,"""Multimodal language models (MLMs) have shown great promise in generating natural language descriptions of visual content..."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,2.,,"""We discuss the different types of MLMs that have been proposed, including visual language models (VLMs) and multimodal transformer-based models, and highlight their strengths and limitations."""
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,3.,,"""In this work, we propose a unified language model (ULM) that can handle both text and speech input, and generate coherent and contextually relevant responses."""
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,5.,,"""most current studies either fail to or artificially incorporate such agent-side initiative."""
Sub-Character Tokenization for Chinese Pretrained Language Models,4.,,"""Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token."""
Erasure of Unaligned Attributes from Neural Representations,4].,,"""Our algorithm works by alternating between two steps. In one, it finds an assignment of the input representations to the information to be erased."""
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,5.,,"""Inspired by causal discovery algorithms, we propose a novel model-agnostic method for training and inference using a conditional independence classifier."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5.,,"""For example, if L≠P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions."""
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,4.,,"""These hallucinations are potentially harmful, yet it remains unclear in what conditions they arise and how to mitigate their impact."""
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,5.,,"""In this work, we propose a new multimodal language model (MMM) that can simultaneously process and generate both text and image information."""
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,4].,,"""However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs."""
Questions Are All You Need to Train a Dense Passage Retriever,3.,,"""We discuss the different types of language models, including unsupervised, supervised, and reinforcement learning models, and their applications in various NLP"
Transparency Helps Reveal When Language Models Learn Meaning,5].,,"""We find that while language models can perform well on simple tasks such as image captioning, they struggle with more complex tasks such as visual question answering and image generation."""
Visual Spatial Reasoning,3.,,"""However"
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,4].,,"""Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions?"""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,5.,,"""In this paper, we propose an attention-based multimodal fusion method for image captioning, which combines visual and linguistic features to generate more accurate and informative captions."""
OpenFact: Factuality Enhanced Open Knowledge Extraction,5.,,"""Our NAS method uses a novel search space that combines the design of the transformer architecture with the choice of the modality-specific layers."""
On Graph-based Reentrancy-free Semantic Parsing,4.,,"""Our approach leverages a pre-trained transformer-based language model to generate visual features that are then combined with textual features to form a unified representation of the input image or video."""
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,3.,,"""In this paper, we provide a comprehensive survey of recent advances in LLMs for multimodal sentiment analysis."""
Chinese Idiom Paraphrasing,4.,,"""In recent years, large language models (LLMs) have achieved impressive results in various natural language processing (NLP) tasks. However, these models are typically trained on unimodal data, such as text, and are not designed to handle multimodal inputs, such as images and videos."""
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,4.,,"""In this paper, we propose a novel multimodal language model that leverages both linguistic and visual features to generate more accurate and informative descriptions of images and videos."""
Rank-Aware Negative Training for Semi-Supervised Text Classification,4.,,"""The authors discuss the different types of MLMs, including multimodal transformer-based models and multimodal attention-based models, and their applications in various domains such as text-to-image synthesis, image captioning, and visual question answering."""
MACSum: Controllable Summarization with Mixed Attributes,5.,,"""In this paper, we explore the impact of adversarial training on the robustness of language models."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,5.,,"""We argue that this stems (in part) from the fact that they are models of semantic similarity."""
Efficient Methods for Natural Language Processing: A Survey,1.,,"""Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed."""
Abstractive Meeting Summarization: A Survey,4.,,"""Recent years have seen a surge in the development of language models, which have shown great promise in a wide range of applications, from language translation to text generation to dialogue systems."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,2.,,"""We introduce a new multimodal context encoder that combines visual and textual information to provide the language model with a richer understanding of the input data."""
Reasoning over Public and Private Data in Retrieval-Based Systems,5.,,"""However, their ability to understand multimodal language is limited due to their lack of exposure to multimodal input during training."""
Multilingual Coreference Resolution in Multiparty Dialogue,5.,,"""In this survey, we provide a comprehensive overview of the current state-of-the-art in multimodal LMs, including their architectures, training methods, and applications in image and video understanding."""
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,2.,,"""In this paper, we propose a neural conversational model that can attend to multiple modalities, including text, speech, and vision, and generate coherent and contextually relevant responses."""
Time-and-Space-Efficient Weighted Deduction,4.,,"""Our approach uses a combination of visual and textual features to improve the accuracy of the understanding tasks."""
Conditional Generation with a Question-Answering Blueprint,4.,,"""In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded."""
Collective Human Opinions in Semantic Textual Similarity,4].,,"""Existing benchmarks have used averaged human ratings as gold standard."""
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,3].,,"e.g., ""the latest pre-trained language models such as BERT and RoBERTa""]."
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,4.,,"""However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results."""
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,1.,,"""In the effort to identify these pressures, prior work has compared real and counterfactual word orders."""
Cross-functional Analysis of Generalization in Behavioral Learning,5].,,"""We find that LLMs"
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,5.,,"""We investigate the limitations of LMs in multimodal reasoning by examining their ability to transfer knowledge across different modalities."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,5.,,"""Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner."""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,4.,,"""Our approach leverages the strengths of both visual and language models to generate high-quality multimodal representations that can be used for a variety of tasks, such as image generation and visual question answering."""
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,4.,,"""In this paper, we provide a comprehensive survey of MLMs, including their architectures, training methods, and applications in image and video understanding."""
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,4.,,"""For this reason, in this paper we propose revisiting the classic 'translate-and-test' pipeline to neatly separate the translation and classification stages."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",4.,,
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,5.,,"""the challenges and limitations of these models, including the difficulty in achieving explainability while maintaining model performance""."
Improving Multitask Retrieval by Promoting Task Specialization,4.,,"""We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization."""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,5.,,"""In this work, we propose a multimodal language model (MMLM) that can jointly learn to represent and generate text and image features."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,4.,,"""Our approach combines a multimodal transformer encoder with a language model to generate captions that are not only accurate but also visually coherent."""
Benchmarking the Generation of Fact Checking Explanations,2.,,"""The field has seen significant growth in recent years, driven by the increasing availability of multimodal data and the development of new deep learning architectures."""
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,1-5].,,the evidence text in the abstract or title].
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,5.,,"""Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual)."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,1-5].,,the evidence text in the abstract or title].
In-Context Retrieval-Augmented Language Models,5.,,"""Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance."""
Learning to Paraphrase Sentences to Different Complexity Levels,4.,,"""Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,2.,,"""We train and evaluate several language models of varying sizes on a dataset of movie reviews, and analyze the results. Our findings show that larger language models tend to perform better on this task."""
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,4.,,"""We investigate the degree to which pre-trained transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",4.,,"""Creating high-quality annotated data for task-oriented dialog (ToD) is known to be notoriously difficult, and the challenges are amplified when the goal is to create equitable, culturally adapted, and large-scale ToD datasets for multiple languages."""
Can Authorship Representation Learning Capture Stylistic Features?,4.,,"""success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic."""
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,Yes.,,"""We propose a novel architecture that integrates LMs and MLMs through a shared encoder and separate decoders for each modality."""
