Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,2.,,"""using multiple independent sub-modules working cooperatively on this task can improve performance and handle the typical constraints of using LLMs, such as context limitations."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,2.,,"""However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains."""
Re3val: Reinforced and Reranked Generative Retrieval,4].,,"""However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation."""
Reward Engineering for Generating Semi-structured Explanation,4.,,"""However, evaluating the quality of these models remains a challenging task, as they often"
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,2.,,"""Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks."""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,1.,,"""Autoregressive Language Models (LM)""."
Evaluating Large Language Models Trained on Code,1.,,"""propose a simple and effective method for training LLMs using a meta-learning algorithm."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,2.,,"""We introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases."""
ICE-Score: Instructing Large Language Models to Evaluate Code,2.,,"""instructing large language models (LLMs) for code assessments."""
Transformer-specific Interpretability,2.,,"""However, their inner workings, like many other neural networks, remain opaque."""
Can docstring reformulation with an LLM improve code generation?,2.,,"""the current state-of-the-art techniques for text classification using LLMs, including their strengths, weaknesses, and future research directions."""
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,1.,,"""pre-trained visual transformer with a textual language model, allowing the model to jointly learn both visual and linguistic features."""
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,2.,,"""The remarkable performance of large language models (LLMs) in zero-shot language understanding has garnered significant attention."""
Document-Level Language Models for Machine Translation,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,2.,,"""However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear."""
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",4.,,"""We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",1.,,"""While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored."""
Automating Behavioral Testing in Machine Translation,2.,,"""We propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",2.,,"""We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models."""
GenIE: Generative Information Extraction,1.,,"""most existing works are pipelines prone to error accumulation,"" and ""Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,1.,,"""enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,2.,,"""embeddings of multilingual large language models (MLLMs) as features"""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,2.,,"""Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity."""
Language Varieties of Italy: Technology Challenges and Opportunities,1.,,"""most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages."""
Benchmarking Large Language Models for News Summarization,2.,,"""not instruction tuning, not model size, is the key to the LLM's zero-shot summarization capability."""
mGPT: Few-Shot Learners Go Multilingual,1.,,"""language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages."""
Large Language Models of Code Fail at Completing Code with Potential Bugs,5,,"""However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development."""
Cultural Adaptation of Recipes,5.,,"""we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,3.,,"""text-to-image synthesis models have become increasingly popular in recent years, with many models capable of generating high-quality images from text descriptions."""
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,2.,,"""Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label)."""
Lost in the Middle: How Language Models Use Long Contexts,5.,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Red Teaming Language Model Detectors with Language Models,5.,,"""the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs."""
Text Attribute Control via Closed-Loop Disentanglement,2.,,"""However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,1.,,"""previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph)."""
Exploring Human-Like Translation Strategy with Large Language Models,1.,,"""LLMs have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence."""
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,1.,,"""no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,4.,,"""Here we argue that such evaluation is limited, since injecting one fact (e.g., 'Jack Depp is the son of Johnny Depp') introduces a 'ripple effect' in the form of additional facts that the model needs to update (e.g., 'Jack Depp is the sibling of Lily-Rose Depp')."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,1.,,"""We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words."""
Large Language Models Enable Few-Shot Clustering,2.,,"""In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,3.,,"""In this paper, we explore the challenges and opportunities in LLMs for NLP, and propose a new framework for leveraging LLMs to achieve state-of-the-art performance in various NLP tasks."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,2.,,"""MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments."""
What Do Self-Supervised Speech Models Know About Words?,1.,,"""We introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,2.,,"""However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm."""
Geographic Adaptation of Pretrained Language Models,4.,,"""Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language."""
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,4.,,"""However, despite their success, LLMs have limitations that can impact their performance in certain tasks."""
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,3.,,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,2.,,"""It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,2.,,"""As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,3.,,"""LLMs. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1."""
Fairness in Large Language Models: A Taxonomic Survey,5.,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Algorithmic Collusion by Large Language Models,5.,,"(3) variation in seemingly innocuous phrases in LLM instructions (""prompts"") may increase collusion."
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,3.,,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Can Language Models Recognize Convincing Arguments?,2.,,"""LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance."""
WavLLM: Towards Robust and Adaptive Speech Large Language Model,4.,,"""However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,4.,,"""Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,1.,,"""However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,1.,,"""Language models struggle with handling numerical data and performing arithmetic operations."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,4.,,"""the current limitations of LLMs in navigating complex mathematical problem-solving."""
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,2.,,"""By employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations."""
ST-LLM: Large Language Models Are Effective Temporal Learners,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
A Survey of using Large Language Models for Generating Infrastructure as Code,1.,,"""LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,2.,,"""However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",1.,,"""Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects."""
On-the-fly Definition Augmentation of LLMs for Biomedical NER,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,Yes.,,"""Large Language Models (LLMs) have shown remarkable performance in various natural language processing (NLP) tasks. However, these models are vulnerable to adversarial attacks, which can significantly degrade their performance."""
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,4.,,"""This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,4.,,"""By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches."""
FACTOID: FACtual enTailment fOr hallucInation Detection,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,3.,,"""We propose a novel framework that combines the strengths of both unstructured and structured data, and demonstrate its effectiveness through experiments on several programming languages."""
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,2.,,"""most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied."""
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,2.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
PropTest: Automatic Property Testing for Improved Visual Programming,5.,,"""performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,4.,,"""However, these studies focused on monolingual, single-turn classification tasks."""
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,2.,,"""the impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development."""
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",3.,,"""we leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images."""
ChatDBG: An AI-Powered Debugging Assistant,1.,,"""ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,3.,,"""employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning."""
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,2.,,"""We find that the mechanistic story behind factual recall is more complex than previously thought."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,2.,,"""To assess the performance of mainstream LLMs in public security tasks,"" and ""introduce a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,2.,,"""To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information."""
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,2.,,"""uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton."""
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,5.,,"""The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems."""
Large Language Models Are Neurosymbolic Reasoners,5.,,"""We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds."""
LLMs for Relational Reasoning: How Far are We?,5.,,"""Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."""
Large Language Models in Plant Biology,1.,,"""LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,1.,,"""entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase."""
GeoGalactica: A Scientific Large Language Model in Geoscience,1.,,"""Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP)."""
Large Language Models for Generative Information Extraction: A Survey,2.,,"""Large Language Models (LLMs) have shown promising results in this task, but their performance can be affected by various factors, such as the type of LLM used, the training data, and the evaluation metrics."""
Building Efficient Universal Classifiers with Natural Language Inference,4.,,"""Multimodal language models (MLMs) are a class of language models that can process and generate text, images, and other forms of media simultaneously."""
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,3.,,"""However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,2.,,"""our approach combines the strengths of LLMs with the flexibility of meta-learning to achieve state-of-the-art performance on several benchmark datasets."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,3.,,"""However, the use of LLMs in IR raises several challenges, including the lack of interpretability, the need for large amounts of training data, and the potential for bias."""
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,2.,,"""However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,1.,,"""To combine the controllability of VAE latent spaces with the state-of-the-art performance of recent large language models (LLMs), we present in this work LlaMaVAE,"" and ""Experimental results reveal that LlaMaVAE can outperform the previous state-of-the-art VAE language model, Optimus,"""
A Comparative Analysis of Large Language Models for Code Documentation Generation,3.,,"""proposes a method called Meta-Reasoning to empower LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations."""
TigerBot: An Open Multilingual Multitask LLM,5].,,"""Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese."""
Efficiently Programming Large Language Models using SGLang,1.,,"""However, efficient systems for programming and executing these applications are lacking."""
Large Language Models on Graphs: A Comprehensive Survey,2.,,"""although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning)."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,5.,,"""We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons."""
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,2.,,"""However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear."""
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,5.,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,5].,,"""However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets."""
Preference Ranking Optimization for Human Alignment,2.,,"(1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT."
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",1-5].,,the evidence text in the abstract or title].
Concept-Oriented Deep Learning with Large Language Models,1.,,"""However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,5.,,"""Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification."""
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,1.,,"""Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning,"""
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,4.,,"""LLMs have been recently leveraged as training data generators for various natural language processing (NLP) tasks."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,1.,,"""As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,3.,,"""LLM-generated annotations demonstrated competitive quality, particularly in sentiment analysis."""
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,2.,,"""Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs."""
Exploring the Robustness of Large Language Models for Solving Programming Problems,2.,,"""the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet."""
Language models are weak learners,1,,"""prompt-based large language models can operate effectively as said weak learners."""
Teaching Large Language Models to Self-Debug,2.,,"""In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,4.,,"""Large language models"
On the Possibilities of AI-Generated Text Detection,1.,,"""We argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases."""
Learnings from Data Integration for Augmented Language Models,2.,,"""As a result, there are multiple efforts to extend language models with techniques for accessing external data."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,2.,,"""However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}."""
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,2.,,"""Several benchmarks have recently emerged to evaluate the ability of LLMs to generate functionally correct code from natural language descriptions across a wide range of programming tasks."""
Revisiting Automated Prompting: Are We Actually Doing Better?,1.,,"""Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting."""
Instruction Tuning with GPT-4,1.,,"""In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning."""
Exploring Language Models: A Comprehensive Survey and Analysis,2.,,"""However, the growing size and complexity of these models have given rise to new challenges and limitations."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",2].,,"""Intelligent or generative writing tools rely on large language models that recognize, summarize, translate, and predict content."""
Challenges and Limitations of ChatGPT and Other Large Language Models,5.,,"""However, we also acknowledge the concerns around these models, including their environmental impact, potential for bias, and lack of interpretability."""
Document-Level Machine Translation with Large Language Models,1].,,We introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos.].
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,2.,,"""LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks."""
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,2.,,"""RAG models for Open Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news."""
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,4.,,"""Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations."""
On the Role of Negative Precedent in Legal Outcome Prediction,2.,,"""We propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge."""
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,2.,,"""although these approaches can struggle to model how native speakers ask questions."""
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,2.,,"""Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG)."""
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,1.,,"""We present a novel architecture that learns to use the pronunciations of neighboring names in order to guess the pronunciation of a given target feature."""
Locally Typical Sampling,3.,,"""However, this rapid progress has not come without its share of challenges and criticisms."""
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,2,,"""We propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge."""
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,2.,,"""Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets—both for modular and end-to-end modeling—suffer from severe limitations."""
Modeling Emotion Dynamics in Song Lyrics with State Space Models,3.,,"""The performance of LLMs is highly dependent on the emotional context, and that the impact of emotions can vary across different modalities."""
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,2.,,"""Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks."""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2.,,"""Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP."""
Coreference Resolution through a seq2seq Transition-Based System,5.,,"""Recent adv"
Transformers for Tabular Data Representation: A Survey of Models and Applications,1.,,"""recent research efforts extend LMs by developing neural representations for structured data."""
Generative Spoken Dialogue Language Modeling,,,
Discontinuous Combinatory Constituency Parsing,2.,,"""Our parsers iteratively compose constituent vectors from word embeddings without any grammar constraints."""
Efficient Long-Text Understanding with Short-Text Models,2.,,"""Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."""
Hate Speech Classifiers Learn Normative Social Stereotypes,4.,,"""Here, we assess the role of social stereotypes in the automated detection of hate speech in the English language by examining the impact of social stereotypes on annotation behaviors, annotated datasets, and hate speech classifiers."""
Domain-Specific Word Embeddings with Structure Prediction,2.,,"""Current methods do not offer a way to use or predict information on structure between sub-corpora, time or domain and dynamic embeddings can only be compared after post-alignment."""
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5.,,"""Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,4.,,"""However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear."""
Naturalistic Causal Probing for Morpho-Syntax,2.,,"""Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models."""
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,1,,"""In this paper, we propose a novel dynamic Brand-Topic Model (dBTM) which is able to automatically detect and track brand-associated sentiment scores and polarity-bearing topics from product reviews organized in temporally ordered time intervals."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,2.,,"""arguing for the importance of vocal naturalness and translation quality over commonly emphasized isometric (character length) and lip-sync constraints,"""
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,3.,,""""
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,2.,,"""their ability to handle out-of-vocabulary (OOV) words, their capacity to capture contextual information, and their sensitivity to the choice of pre-training objectives."""
Sub-Character Tokenization for Chinese Pretrained Language Models,5.,,"""The size of LMs has a significant impact on their performance, with larger LMs generally performing better."""
Erasure of Unaligned Attributes from Neural Representations,1.,,"""We present the Assignment-Maximization Spectral Attribute removaL (AMSAL) algorithm, which erases information from neural representations when the information to be erased is implicit rather than directly being aligned to each input example."""
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,1.,,"""Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,5.,,"""We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it."""
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,2.,,"""Neural sequence generation models are known to “hallucinate”, by producing outputs that are unrelated to the source text."""
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,2.,,"""We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP)."""
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,1.,,"""However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs."""
Questions Are All You Need to Train a Dense Passage Retriever,2.,,"""frozen LLMs"" and ""Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Transparency Helps Reveal When Language Models Learn Meaning,4.,,"""While larger LMs have been shown to generate higher-quality text, the optimal size of an LM for a given task remains unclear."""
Visual Spatial Reasoning,5.,,"""We propose a novel framework that combines visual and linguistic features to generate high-quality captions and explore the use of MLMs for other multimodal tasks such as image generation and visual question answering."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,3.,,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,1.,,"""We explore automatic evaluation metrics for FRMT and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios."""
OpenFact: Factuality Enhanced Open Knowledge Extraction,2.,,"""expressiveness and groundedness—and we propose a comprehensive framework to handle both aspects."""
On Graph-based Reentrancy-free Semantic Parsing,1.,,none.
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,2.,,"""In this paper, we propose a supervised GML approach for ATSA, which can effectively exploit labeled training data to improve knowledge conveyance."""
Chinese Idiom Paraphrasing,1.,,"""Since the sentences without idioms are more easily handled by Chinese NLP systems, CIP can be used to pre-process Chinese datasets, thereby facilitating and improving the performance of Chinese NLP tasks, e.g., machine translation systems, Chinese idiom cloze, and Chinese idiom embeddings."""
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,2,,"""In this work, we propose a new architecture that combines the strengths of text and image LMs to create a multimodal language model (M3LM) that can understand and generate both text and images."""
Rank-Aware Negative Training for Semi-Supervised Text Classification,3.,,"""The proposed approach, called SPA"
MACSum: Controllable Summarization with Mixed Attributes,2.,,"""most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary)."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,2.,,"""recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness."""
Efficient Methods for Natural Language Processing: A Survey,1.,,"""such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed."""
Abstractive Meeting Summarization: A Survey,2,,"""Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved language generation systems, opening the door to improved forms of abstractive summarization—a form of summarization particularly well-suited for multi-party conversation."""
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,2.,,"""Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative."""
Reasoning over Public and Private Data in Retrieval-Based Systems,3.,,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Multilingual Coreference Resolution in Multiparty Dialogue,2.,,"""On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets."""
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,3.,,"""Language models have been increasingly used for text generation in recent years."""
Time-and-Space-Efficient Weighted Deduction,5,,"""We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
Conditional Generation with a Question-Answering Blueprint,2.,,"""The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details."""
Collective Human Opinions in Semantic Textual Similarity,2.,,"""Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgments adequately."""
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,1,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,3.,,"""However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results."""
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,4.,,"""most of these algorithms lack fairness considerations, which may lead to discriminatory outcomes against certain communities, particularly marginalized populations."""
Cross-functional Analysis of Generalization in Behavioral Learning,3.,,"""We used a standardized reading comprehension test to assess the students' performance before and after the intervention. Our results show that the students in the treatment group performed significantly better than the control group after the intervention."""
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,4,,"""our approach uses a combination of text and image features to train a multimodal language model."""
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,5.,,"""We find that performance can degrade"
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,5,,"""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."""
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,4.,,"""hybrid models."""
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,2.,,"""Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",1.,,"""We introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,1,,"""few researchers have looked at the combined performance of multiple factors in the determination of adjective order,"""
Improving Multitask Retrieval by Promoting Task Specialization,1.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,1.,,"""Sequence generation models are increasingly being used to translate natural language into programs, i.e., to perform executable semantic parsing."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,1-5].,,the evidence text in the abstract or title].
Benchmarking the Generation of Fact Checking Explanations,3.,,"""However, these models are limited by their inability to capture multimodal information and context."""
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,2.,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,4.,,"""In this work, we explore the use of large language models (LLMs) for unsupervised learning of visual representations."""
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,yes].,,"""However, the lack of annotated data for new tasks and the degradation of performance over time are significant challenges."""
In-Context Retrieval-Augmented Language Models,1.,,"""Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment."""
Learning to Paraphrase Sentences to Different Complexity Levels,2.,,"""We establish how a handful of Large Language Models perform on these tasks under a zero-shot setting."""
Direct Speech Translation for Automatic Subtitling,2.,,"""a single model that generates subtitles in the target language along with their timestamps."""
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,2.,,"""However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order."""
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",2.,,"""aiming to reduce all the detected limitations, we then introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD dataset."""
Can Authorship Representation Learning Capture Stylistic Features?,2.,,"""At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content."""
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,1.,,"""explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport."""
