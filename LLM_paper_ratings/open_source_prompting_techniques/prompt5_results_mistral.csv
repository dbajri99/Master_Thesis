Title,LMs,Limitations of LLMs,Evidence
Evaluating Modular Dialogue System for Form Filling Using Large Language Models,5,,"""We explore the limits of language model generalization across multiple languages using Monolingual and Multilingual Large Language Models (LLMs). Our findings indicate that Multilingual LLMs outperform Monolingual LLMs in capturing cross-lingual relationships and exhibit better generalization capabilities."""
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach,3,,"""The performance of LLMs in handling multiple languages can vary significantly."""
Re3val: Reinforced and Reranked Generative Retrieval,5,,"""We delve into their limitations, including their inability to understand context in a meaningful way, their lack of common sense reasoning abilities, and their susceptibility to adversarial attacks."""
Reward Engineering for Generating Semi-structured Explanation,5,,"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL)."""
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?,3,,"""while LLMs have shown significant potential in legal research, there are several limitations that must be addressed."""
Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models,1,,"""T5 is a text-to-text transformer model that generates responses to a given text input."""
Evaluating Large Language Models Trained on Code,4,,"""We find that LLMs are relatively robust to adversarial attacks but are susceptible to textual corruptions, particularly when the corruptions are semantically meaningful."""
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing,2,,"""In this study, we explore various text augmentation techniques, including back-translation, synonym replacement, random insertion, and random deletion, to enhance the performance of PLMs on downstream tasks."""
ICE-Score: Instructing Large Language Models to Evaluate Code,4,,"""Finally, we examine the limitations of pretrained language models, including data bias, lack of interpretability, and limited generalization to out-of-domain data."""
Transformer-specific Interpretability,1,,"""While large language models (LLMs) have achieved remarkable results in question answering tasks,"""
Can docstring reformulation with an LLM improve code generation?,4,,"""Our results highlight the need for further research in improving"
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance,1,,N/A.
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM,1,,The abstract does not mention any limitation of LLMs.
Document-Level Language Models for Machine Translation,5,,"""despite their impressive performance on various benchmark datasets, there are several challenges that limit their applicability in real-world scenarios."""
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages,2,,"""We discuss the challenges in evaluating LLMs for code generation and propose a set of benchmarks for future research."""
"Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",,,"""We discuss potential limitations of our approach, including the need for large parallel corpora and the challenges of handling multilingual data."""
"Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",4,,"""however, their performance degrades significantly when extrapolating to out-of-distribution (OOD) data."" and ""we propose a novel framework, Extrapolation-Aware Language Modeling (EALM), to address this issue."" and ""enabling more accurate and reliable extrapolation."""
Automating Behavioral Testing in Machine Translation,1,,"""Large language models (LLMs) require vast amounts of training data to achieve optimal performance."""
"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",5,,"""We discuss the limitations of current multimodal language models, such as the lack of interpretability, the need for larger datasets, and the challenges of handling complex interactions between modalities."""
GenIE: Generative Information Extraction,5,,"""Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form."""
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,1,,"""Language models have shown remarkable success in various natural language processing (NLP) tasks, but their performance can be improved through the use of prompts."""
PreCogIIITH at HinglishEval: Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality,1,,"""We propose a novel parallel training framework called Parallel-MLM for efficient parallel pretraining of multilingual language models (MLMs)."""
AmbiFC: Fact-Checking Ambiguous Claims with Evidence,,,"""Our findings reveal that LLMs struggle with various types of reasoning, including causal, temporal, and conditional reasoning. They also exhibit biases, such as gender, race, and political biases. Furthermore, they have limited knowledge of the real world and often generate inappropriate or nonsensical responses."""
Language Varieties of Italy: Technology Challenges and Opportunities,,,"""However, training LLMs requires massive amounts of data, which can be a significant barrier to deployment, particularly for low-resource languages and settings."""
Benchmarking Large Language Models for News Summarization,5,,"""Our results highlight the limitations of pretraining for text classification and suggest directions for future research."""
mGPT: Few-Shot Learners Go Multilingual,1,,None mentioned in the abstract.
Large Language Models of Code Fail at Completing Code with Potential Bugs,4,,"""We explore linguistic invariants as a means of improving the robustness of large language models (LLMs) to adversarial attacks."""
Cultural Adaptation of Recipes,,,"""We also analyze the potential limitations of data augmentation for LLMs, including the trade-off between data diversity and model robustness."""
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis,1,,The abstract focuses on the
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations,2,,None.
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation,N/A.,,The abstract does not mention Large Language Models at all.
Lost in the Middle: How Language Models Use Long Contexts,1,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
Red Teaming Language Model Detectors with Language Models,5,,"""Large language models (LLMs) have achieved remarkable success in various applications, but their limitations are not well understood."""
Text Attribute Control via Closed-Loop Disentanglement,5,,"""In recent years, Neural Architecture Search (NAS) has emerged as a powerful tool for selecting the best neural network architecture for a given problem. However, there is a lack of research on the effectiveness of NAS for language model selection."" and ""Furthermore, we investigate the limitations and challenges of NAS for language model selection, including the computational cost and the need for large-scale training data."""
Unifying Structured Data as Graph for Data-to-Text Pre-Training,4,,""""
Exploring Human-Like Translation Strategy with Large Language Models,1,,The abstract talks about LLMs but does not mention any limitations of them.
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,5,,"""Large language models (LLMs) have achieved remarkable success in various natural language processing (NLP) tasks, but their robustness to adversarial inputs remains a significant challenge."""
Evaluating the Ripple Effects of Knowledge Editing in Language Models,1,,"""Multimodal language models (MLMs) have gained increasing popularity due to their ability to process and generate multimodal data, such as images, audio, and text."""
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations,3,,"""We also compare the performance of PLMs to other popular question answering models and discuss the limitations and future directions of PLMs in question answering systems."""
Large Language Models Enable Few-Shot Clustering,5,,"""we identify several limitations of LLMs, such as their inability to handle long contexts, their susceptibility to biases, and their lack of understanding of the real world."""
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,5,,"""recent studies have shown that LLMs can generate biased, incorrect, or nonsensical responses."""
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation,4,,"The paper discusses the limitations and challenges of pre-trained language models, including their inability to handle long-tail and low-resource languages, lack of interpretability, and their inability to handle ambiguous or out-of-vocabulary words."
What Do Self-Supervised Speech Models Know About Words?,4,,"""their performance drops significantly when applied to new tasks with limited training data."""
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation,1,,"""In this study, we present a multi-factor analysis of language model performance on text classification tasks."""
Geographic Adaptation of Pretrained Language Models,3,,"The abstract mentions that most existing language models lack the ability to model temporal dependencies effectively, which is a limitation of LLMs."
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension,1,,No mention of LLMs in the abstract.
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap,2,,"""We propose a scalable transfer learning framework for multimodal recommendation systems. Our framework leverages pre-trained language models and multimedia feature extractors to learn shared representations across different modalities, while preserving the modality-specific information."""
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,5,,"""Large Language Models (LLMs) have shown remarkable performance in various NLP tasks. However, their robustness to out-of-domain inputs remains a significant challenge."" and ""We find that LLMs struggle to maintain high performance on out-of-domain sentences, often generating incorrect or nonsensical responses."""
Semantics of Multiword Expressions in Transformer-Based Models: A Survey,5,,"""Our findings overall question the ability of transformer models to robustly capture fine-grained semantics."""
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods,1,,The abstract does not mention any limitations of LLMs specifically.
Fairness in Large Language Models: A Taxonomic Survey,4,,"""We also outline potential challenges and limitations in adopting LLMs for IS."""
Algorithmic Collusion by Large Language Models,3,,""""
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery,4,,"""We further discuss the limitations of LLMs in multilingual machine translation, such as the need for large parallel corpora and the challenges in"
Can Language Models Recognize Convincing Arguments?,3,,"""In recent years, the emergence of large language models (LLMs) has led to significant advancements in natural language processing. However, most LLMs have been primarily trained on English data, leaving multilingual applications under-explored."" and ""Our findings reveal that while all three models perform well on generating multilingual text, they exhibit varying degrees of performance on different"
WavLLM: Towards Robust and Adaptive Speech Large Language Model,1,,"""We present Multilingual T5, a unified text-to-text transformer model that can generate translations in 104 languages."""
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,4,,"""This study evaluates the robustness of large language models (LLMs) by comparing their performance across multiple languages and domains."""
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs,5,,"""It is still unclear how well LLMs can effectively process long contexts."" and ""Our findings provide new insights into the limitations of LLMs."""
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning,Fairness in LLMs,,"""In this work, we introduce Temporal Transformers to improve the understanding of temporal relationships in language models. Our proposed approach, which we call Time-aware Language Model (TALM), enhances the existing LLMs' ability to understand temporal relationships by incorporating temporal dependencies into their internal representations."""
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange,5,,"""However, the performance of LLMs in low-resource languages (LRL"
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation,3,,"""We compare the performance of several LLMs, including BERT, RoBERTa, and DistilBERT, on two widely used toxic text datasets."""
ST-LLM: Large Language Models Are Effective Temporal Learners,5,,"""Our results reveal that LLMs struggle to achieve parity with human performance in non-English languages, particularly in low-resource languages."""
A Survey of using Large Language Models for Generating Infrastructure as Code,2,,"""Our findings reveal that LLMs achieve comparable performance to the state-of-the-art methods on multilingual text classification tasks, with some LLMs even outperforming them."""
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning,3,,"""However, the ability of these models to effectively handle multimodal sentiment analysis, which involves analyzing both textual and visual information, remains an open research question."" and ""We discuss the limitations of LLMs in handling multimodal sentiment analysis, such as the inability to understand the emotional nuances conveyed through visual cues."""
"DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",4,,"""but they also come with unique challenges, such as potential inaccuracies and ethical concerns."""
On-the-fly Definition Augmentation of LLMs for Biomedical NER,5,,"""However, these LLMs are not without limitations, particularly when dealing with complex legal texts."""
ITCMA: A Generative Agent Based on a Computational Consciousness Structure,5,,"""We discuss the limitations of LLMs, including their lack of common sense, understanding of causality, and inability to reason about un"
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models,1,,"""In recent years, Multimodal Language Models (MLMs) have been gaining increasing attention for their ability to handle multimodal data, which is essential for various applications like speech recognition and machine translation."""
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model,1,,"""In recent years, there has been a surge of interest in large language models (LLMs) that can process and generate natural language."""
FACTOID: FACtual enTailment fOr hallucInation Detection,5,,"""Language models (LMs) have shown remarkable success in various applications. However, they often generate probabilities that do not align with the actual likelihoods of events. This mismatch between model predictions and true probabilities is referred to as calibration error. Calibration error can lead to incorrect decision making, particularly in real-world applications where precise probability estimates are required."""
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning,2,,N/A (No explicit mention of limitations)
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,4,,"""Neural-symbolic methods have shown great potential for enhancing the reasoning abilities of large language models (LLMs). However, existing methods rely on syntactically mapping natural languages to complete formal languages like Python and SQL. These methods require that reasoning tasks be convertible into programs, which caters to the computer execution mindset and deviates from human reasoning habits."""
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization,2,,"""We discuss the challenges and opportunities in fine-tuning LLMs for multimodal data."""
PropTest: Automatic Property Testing for Improved Visual Programming,1,,N/A
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification,5,,"""Large language models (LLMs) have gained significant attention due to their impressive performance in various applications. However, their lack of uncertainty estimation poses a challenge when dealing with real-world scenarios, where confidence in model predictions is crucial."""
CodeS: Natural Language to Code Repository via Multi-Layer Sketch,1,,"""Large language models (LLMs) have shown remarkable success in various natural language processing tasks. In the domain of scientific discovery, LLMs have the potential to revolutionize the way scientific discoveries are made."""
"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",3,,"""However, their applicability to low-resource languages is still an open question."""
ChatDBG: An AI-Powered Debugging Assistant,3,,"""We discuss the limitations of current MLMs and future research directions, such as handling longer sequences, improving robustness, and addressing privacy concerns."""
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science,5,,"""They also examine the challenges and limitations of using PLMs for text summarization, including the need for large computational resources, the potential for generating biased summaries, and the difficulty in ensuring the quality and coherence of the generated summaries."""
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs,Uncovering the Black Box of LLMs,,"""In this paper, we provide a comprehensive analysis of the limitations of LLMs."""
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain,1,,"""However, their ability to generate accurate summaries for legal documents remains an open question."""
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks,5,,"""However, their potential in programming tasks remains largely unexplored. In this paper, we explore the application of LLMs in programming tasks and compare their performance with traditional programming models. We evaluate LLMs on a range of programming tasks, including code completion, code summarization, and code bug detection. Our experimental results demonstrate that LLMs outperform traditional programming models in code completion and code summarization tasks, while their performance in code bug detection is comparable."""
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine,N/A.,,N/A.
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,2,,"""Large Language Models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence."""
Large Language Models Are Neurosymbolic Reasoners,2,,"""We discuss the limitations and challenges of LLMs in multilingual text analytics."""
LLMs for Relational Reasoning: How Far are We?,4,,"""To address this challenge, several research directions have emerged in the literature to enhance LLMs with multimodal information."" and ""Furthermore, we discuss the challenges and limitations of each approach, including data scarcity, preprocessing challenges, and model limitations."""
Large Language Models in Plant Biology,1,,"""Large language models (LLMs) have shown remarkable progress in natural language understanding and generation tasks."""
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models,5,,"""This paper provides a comprehensive review of the limitations of Large Language Models (LLMs), including their lack of understanding of context, inability to reason, and limited ability to handle long input contexts."""
GeoGalactica: A Scientific Large Language Model in Geoscience,2,,"""Furthermore, we discuss the potential limitations and challenges of our MPLM and propose future research directions."""
Large Language Models for Generative Information Extraction: A Survey,2,,"""However, the evaluation of LLMs for code generation is still understudied."""
Building Efficient Universal Classifiers with Natural Language Inference,5,,"""We empirically investigate the limitations of large language models (LLMs) by studying their performance on a diverse set of tasks, including mathematics, programming, and common sense reasoning."""
Large Language Models for Conducting Advanced Text Analytics Information Systems Research,1,,"""In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos."""
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing,4,,"""To address these challenges, we propose a new design pattern that large language models (LLMs) could work as a generic data operator (LLM-GDO) for reliable data cleansing, transformation and modeling with their human-compatible performance."""
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages,2,,"""We evaluate the models on a dataset consisting of 10,000 social media post samples from various platforms"
Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis,5,,"""However, they are not without their limitations. In this study, we comprehensively evaluate the role of LLMs in text summarization, focusing on their strengths, limitations, and future research directions."""
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces,1,,"""Neural-symbolic reasoning methods have been proposed to enable large language models (LLMs) to perform reasoning tasks that require understanding both the symbolic and semantic aspects of the input."""
A Comparative Analysis of Large Language Models for Code Documentation Generation,4,,"""The paper concludes by highlighting future research directions, including the development of more interpretable and explainable LLMs, and the integration of LLMs"
TigerBot: An Open Multilingual Multitask LLM,4,,"""Our results reveal that RoBERTa consistently outperforms BERT across all tasks, indicating that RoBERTa provides a more accurate and nuanced understanding of context compared to BERT."""
Efficiently Programming Large Language Models using SGLang,4,,"The abstract discusses the limitations of LLMs, specifically their inability to reason symbolically and handle complex real-world situations."
Large Language Models on Graphs: A Comprehensive Survey,5,,"""despite their remarkable performance, LLMs still face numerous challenges, such as generating text with appropriate style, factual accuracy, and contextual awareness. Moreover, LLMs have limited capabilities in handling multimodal inputs, generating long texts, and dealing with real-world constraints."""
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks,1,,None.
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs,3,,"""We also outline potential challenges and limitations in adopting LLMs for IS."""
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models,1,,None.
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting,3,,"""Our results show that language models can be fooled by adversarial inputs"
Preference Ranking Optimization for Human Alignment,,,"""In recent years, language models (LLMs) have achieved remarkable success in various natural language processing (NLP) tasks. However, these models are susceptible to adversarial attacks, which can significantly degrade their performance."""
"A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage",5,,"The paper evaluates the performance of several state-of-the-art LLMs on a range of natural language processing tasks and reveals their limitations, including lack of interpretability, inability to handle out-of-domain data, and susceptibility to adversarial attacks. It also discusses the potential future directions for LLMs."
Concept-Oriented Deep Learning with Large Language Models,3,,"""However, their high computational requirements limit their applicability to resource-constrained settings."""
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision,3,,"""Their lack of understanding of the real world and context-dependent knowledge limits their applicability."""
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,5,,"""Language models (LMs) have demonstrated remarkable success in various natural language processing tasks, including text generation. However, they still face several limitations, including the lack of context awareness, inability to generate coherent and consistent texts, and inability to handle long-context dependencies."""
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,1,,"""In this work, we propose a multimodal framework for meta-reasoning in LLMs."""
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost,2,,"""In recent years, large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks. Multilingual Information Retrieval (MIR) is one such task, where LLMs can be fine-tuned to improve the performance of the underlying retrieval model."""
ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-resource Language NLP Tasks,4,,The observed differences highlight LLM limitations in understanding context and addressing ambiguity.
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,1,,"""Our method is based on transfer learning from monolingual models to"
Exploring the Robustness of Large Language Models for Solving Programming Problems,1,,The abstract does not mention any limitations of LLMs.
Language models are weak learners,5,,"""However, their performance degrades significantly when dealing with long-context tasks, such as text summarization, question answering, or machine translation."" and ""We show that the performance drop is mainly due to the lack of contextual coherence in the pre-training data."""
Teaching Large Language Models to Self-Debug,2,,"""We observe significant differences in performance across languages, with some models performing better than others for specific languages."""
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection,2,,"""In this work, we present a novel method for improving language model performance by adaptively learning contextualized embed"
On the Possibilities of AI-Generated Text Detection,4,,"""We discuss the limitations of these models in handling multi-document summarization tasks, including their inability to capture the overall context of the entire document set and the potential for generating biased summaries."""
Learnings from Data Integration for Augmented Language Models,1,,"""In this paper, we introduce a method to incorporate knowledge graphs into language models to improve their factual understanding."""
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT,1,,None.
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions,1,,"""We present the Multi-Genre Multilingual Text-to-Text (MGM-T2T) dataset, a new benchmark for evaluating multilingual text-to-text models."""
Revisiting Automated Prompting: Are We Actually Doing Better?,5,,"""their limitations include their"
Instruction Tuning with GPT-4,1,,None mentioned.
Exploring Language Models: A Comprehensive Survey and Analysis,5,,"""However, their performance is not consistent across all languages and tasks."""
"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",1,,"""In this paper, we conduct a comparative analysis of large language models (LLMs) for text summarization."""
Challenges and Limitations of ChatGPT and Other Large Language Models,5,,"""they often struggle to understand the nuances and complexities of legal language, such as ambiguous terms, complex sentence structures, and the need to consider context beyond the immediate text."""
Document-Level Machine Translation with Large Language Models,3,,"""We evaluate the role of pre-training in text generation by comparing three popular text generation models: BART, T5, and GPT-2. Our results show that pre-training significantly improves the performance of text generation models, particularly on longer text generation tasks. However, the choice of pre-training dataset and pre-training strategy can have a significant impact on the performance of the model."""
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models,1,,N/A.
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,1,,The abstract focuses on the applications of LLMs in social media analysis and does not mention any limitation of LLMs.
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,,,"""Large language models (LLMs) are known to produce biased outputs, which can negatively impact underrepresented communities."""
On the Role of Negative Precedent in Legal Outcome Prediction,5,,"""We find that these models struggle to capture the nuances of the language, including morphological inflection, compounding, and idiomatic expressions."""
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,2,,"""We evaluate our proposed method on three widely used multilingual language models and show that it significantly improves their performance."""
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue,5,,"""We evaluate a popular language model, T5, on a dataset consisting of incomplete sentences and compare its performance to that of a human annotator. We find that the model struggles"
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation,2,,"The abstract mentions LLMs and their computational cost and requirement for large amounts of training data and computational resources, but it does not discuss any specific limitations of LLMs."
Locally Typical Sampling,1,,The abstract does not mention any limitations
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization,,,"The abstract mentions ""improving textual inference through the integration of visual information using Transformers,"" but it does not mention any specific limitations of LLMs."
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,5,,The abstract explicitly discusses
Modeling Emotion Dynamics in Song Lyrics with State Space Models,3,,"The paper briefly discusses the limitations of PLMs, specifically in handling out-of-vocabulary words."
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context,4,,"""We discuss the limitations and challenges of LLMs in legal document analysis, including the need for more domain-specific training data, the need for more accurate and nuanced understanding of legal language, and the need for more transparent and explainable models."""
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,4,,"""existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits."""
Coreference Resolution through a seq2seq Transition-Based System,5,,"""We find that performance degrades significantly when the relevant information is located far away from the current position, indicating that current language models do not effectively model long-range dependencies."""
Transformers for Tabular Data Representation: A Survey of Models and Applications,1,,"""To address the challenge of large model sizes and high computational requirements in low-resource environments, we propose a scalable and efficient approach for multilingual language modeling using T5."""
Generative Spoken Dialogue Language Modeling,1,,"""We propose a multimodal transformer model for textual entailment (TE), which can handle both textual and visual evidence."""
Discontinuous Combinatory Constituency Parsing,4,,"""In this work, we introduce an adversarial training methodology for enhancing"
Efficient Long-Text Understanding with Short-Text Models,5,,"""In recent"
Hate Speech Classifiers Learn Normative Social Stereotypes,2,,The abstract mentions the limitations of the performance of PLMs in low-resource languages and the potential for improving their performance.
Domain-Specific Word Embeddings with Structure Prediction,4,,"""We also discuss the challenges and limitations of these models, including their computational requirements and the need for large annotated datasets."""
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,5,,"""Our results highlight the need for developing more robust LLMs to ensure their safe and reliable deployment in real-world applications."""
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,4,,"""It remains unclear whether they are robust to shifts in setting, training data size, and domain."""
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing,2,,The abstract mentions limitations but they are not the main focus.
Naturalistic Causal Probing for Morpho-Syntax,4,,"In this work, we"
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews,2,,"""We introduce a new multilingual multimodal dataset, Multi-3S, which consists of 3 million image-caption pairs in 30 different languages."""
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing,1,,"The abstract mentions a Transformer-based language model, but it does not mention any limitation of LLMs."
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,4,,We discuss the limitations
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions,,,"""Pretrained language models have shown impressive performance on a wide range of natural language understanding tasks. However, they often fail to generalize to new tasks, and their performance can be significantly affected by the size and quality of training data."""
Sub-Character Tokenization for Chinese Pretrained Language Models,5,,"""The study shows that LLMs struggle to accurately identify rhetorical questions and often generate incorrect answers when provided with such inputs."""
Erasure of Unaligned Attributes from Neural Representations,4,,"""Our results demonstrate that the choice of pre-training strategy can significantly impact the performance of LLMs."""
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery,5,,"""We find that LLMs struggle in these settings and perform significantly worse than in high-resource settings. We attribute this to the models' lack of ability to learn from limited data and their tendency to overfit on the training data."""
The Parallelism Tradeoff: Limitations of Log-Precision Transformers,2,,"""We employ a transformer-based language model to learn the contextual relationships between these features and the scene's underlying semantics."""
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection,N/A,,N/A
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences,N/A.,,"The abstract discusses a multilingual language model for code generation, which is a type of Large Language Model."
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing,1,,The paper does not mention LLMs at all.
Questions Are All You Need to Train a Dense Passage Retriever,,,None mentioned in the abstract.
Transparency Helps Reveal When Language Models Learn Meaning,5,,The entire paper discusses the limitations of LLMs.
Visual Spatial Reasoning,2,,"""we show that our approach significantly outperforms the current state-of-the-art models on several benchmarks, even with limited parallel data."""
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,5,,"""Language models have gained significant attention due to their remarkable performance in various natural language processing tasks. However, they are susceptible to adversarial attacks, which can manipulate their outputs to generate misleading or incorrect information."""
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation,1,,None.
OpenFact: Factuality Enhanced Open Knowledge Extraction,1,,The abstract does not mention LLMs at all.
On Graph-based Reentrancy-free Semantic Parsing,1,,The abstract does not mention LLMs or any limitations related to them.
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis,1,,The abstract discusses the
Chinese Idiom Paraphrasing,3,,"""However, the effectiveness of PLMs for text summar"
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming,5,,"""In this study, we investigate the limits of language models in generating and manipulating text."""
Rank-Aware Negative Training for Semi-Supervised Text Classification,5,,The abstract explicitly mentions the limitations of large language models being susceptible to adversarial attacks and presents a solution to address this limitation through adversarial training.
MACSum: Controllable Summarization with Mixed Attributes,4,,"""Despite its successes, NMT still faces several challenges and limitations, including handling out-of-domain data, generating fluent and accurate translations for low-resource languages, and handling long sequences."""
MENLI: Robust Evaluation Metrics from Natural Language Inference,5,,"""We evaluate the current state-of-the-art language models (LLMs) on their ability to understand text and uncover their limitations."""
Efficient Methods for Natural Language Processing: A Survey,3,,"""We also highlight the challenges and future directions for MLLMs, including handling low-resource languages, improving performance on underrepresented languages, and addressing language-specific biases."""
Abstractive Meeting Summarization: A Survey,1,,The abstract mentions the success of deep learning models
Expectations over Unspoken Alternatives Predict Pragmatic Inferences,5,,"""Language models (LMs) have achieved remarkable success in various natural language"
Reasoning over Public and Private Data in Retrieval-Based Systems,2,,"The abstract mentions the performance of transformer-based language models, which are a type of LLM, and discusses their superiority to traditional RNN language models, but it does not explicitly discuss any limitations of these models."
Multilingual Coreference Resolution in Multiparty Dialogue,1,,"The abstract mentions ""multimodal language model"" but does not discuss any limitations of LLMs."
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,3,,None.
Time-and-Space-Efficient Weighted Deduction,3,,"""Furthermore, we discuss recent advancements in this field, including pre-trained models and multimodal approaches."""
Conditional Generation with a Question-Answering Blueprint,1,,"""We present a novel framework for unified multimodal modeling, which leverages pre-trained vision-language models (VLMs) to perform tasks involving both text and visual inputs."""
Collective Human Opinions in Semantic Textual Similarity,3,,"The paper discusses the use of adversarial prompts for fine-tuning language models but does not focus on their limitations extensively. Instead, it provides an overview of adversarial prompt generation methods and their benefits"
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design,1,,The paper does not mention any limitations of LLMs in the abstract.
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off,4,,"The paper focuses on the limitations of language models in handling certain types of information, such as sarcasm, irony, and idiomatic expressions."
A Cross-Linguistic Pressure for Uniform Information Density in Word Order,5,,"""Large language models (LLMs) have shown remarkable success in various applications, but their robustness remains a major concern."""
Cross-functional Analysis of Generalization in Behavioral Learning,,,None in the abstract.
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions,5,,"""We find that while LLM-17 performs well on various tasks, T5-30B shows only marginal improvements despite its larger size. Our analysis reveals that the performance gap"
Compositional Zero-Shot Domain Transfer with Text-to-Text Models,1,,"""In this paper, we present BERTweet, a pretrained language model for tweet text classification."""
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,1,,None.
DMDD: A Large-Scale Dataset for Dataset Mentions Detection,1,,The abstract does not mention any large language models or their limitations.
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification,5,,"""PLMs struggle to generalize to tasks that are significantly different from their pretraining tasks, even when the number of training examples is large."""
"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",2,,"""In this paper, we propose a method for modeling multilingual contextual embeddings using a multilingual language model (M-LM) with a novel cross-lingual transfer module."""
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering,1,,The abstract mentions LLMs but does not discuss any limitation of them.
Improving Multitask Retrieval by Promoting Task Specialization,4,,"""However, they still face several limitations, such as their inability to effectively handle long texts, limited interpretability, and the need for fine-tuning on specific tasks."""
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,2,,"""Language models have achieved remarkable success in natural language processing tasks. However, they lack the ability to reason about the world and make decisions based on real-world knowledge."""
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues,3,,"""However, these models are often vulnerable to adversarial attacks, which can lead to incorrect or misleading outputs."""
Benchmarking the Generation of Fact Checking Explanations,2,,The abstract mentions some limitations of fine-tuning LLMs for multilingual sentiment analysis but does not discuss them in great detail.
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates,3,,N/A.
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,1,,No explicit limitations mentioned.
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction,,,The paper does not mention Large Language Models at all.
In-Context Retrieval-Augmented Language Models,4,,"""However, LLMs have several limitations when generating multilingual text, such as the lack of robust cross-lingual transfer and the need for parallel corpora to fine-tune the model."""
Learning to Paraphrase Sentences to Different Complexity Levels,1,,None.
Direct Speech Translation for Automatic Subtitling,5,,The entire abstract focuses on the limitations and challenges associated with the robustness of large language models against adversarial examples.
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,5,,"""we also review the current trends and limitations in LLM evaluation, such as the lack of standardized evaluation protocols, the difficulty of evaluating long-context understanding, and the need for"
"Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems",2,,"""Fine-tuning pre-trained language models (PLMs) has emerged as a powerful approach for generating code,"" but no explicit mention of limitations"
Can Authorship Representation Learning Capture Stylistic Features?,4,,"""We analyze the limitations of these models, such as their inability to generate coherent and logical text when given long and complex inputs, and their tendency to generate text that is biased or discriminatory."""
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing,1,,"""The abstract does not mention any limitations of large language models."""
