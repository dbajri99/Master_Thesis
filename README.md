## Master Thesis on Large Language Model Limitations
### Project Overview
This repository contains the script conducted for my master's thesis, focusing on identifying and analyzing limitations of Large Language Models (LLMs). The study examines approximately 145,800 academic papers from ArXiv and ACL Anthology, ranging from 2022 to 2024, to create a taxonomy of LLM limitations.

In this project, we use methods such as zero-shot and few-shot learning applied to OpenAI GPT-3.5 Turbo, GPT 4.o [gpt-4o-2024-05-13] and two opensource models: LLaMa2 [meta-llama/Llama-2-7b-chat-hf] and Mistral [‚Äùmistralai/Mistral-7B-Instruct-v0.2], LLM-based embeddings using OpenAI's feature [text-embedding-ada-002], BERTopic clustering combined to HDBSCAN, and the semi-supervised clustering using few-shot learning approach by using the OpenAI's GPT-3.5 Turbo to evaluate and categorize these limitations into clusters like reasoning failures, bias propagation, hallucination risks, privacy concerns, and resource constraints.
The taxonomy categorizes LLM challenges into five areas: Cognitive and Logical Limitations, Ethical and Social Limitations, Technical and Operational Limitations, Application-Specific Limitations, and Performance and Reliability. 

### Data Crawling Scripts
This section includes Python scripts for automated data retrieval from interacting with arXiv and ACL Anthology APIs, targeting papers published from the beginning of 2022 to the start of 2024. The scripts, housed in this repository's "data_crawling" folder, facilitate systematic collection and structuring of data into JSON format. This format includes essential details such as titles, authors, publication dates, summary (abstracts), and full-text links.
#### ACL retriever.ipynb
- Dynamic URL Construction: Automatically generates URLs based on specified conference names and years.
- Data Scraping: Extracts key details such as titles, authors, publication dates, summaries, and PDF links.
- Progress Tracking: Utilizes tqdm to provide real-time progress feedback during data retrieval.
- HTML Structure Adaptation: Efficiently handles different HTML structures across multiple data sources.
#### ArXiv retriever.ipynb
- Dynamic Query Building: Constructs search queries based on user-specified fields ("cs.CL", "cs.LG", 'cs.AI', 'cs.CV'), categories, and date ranges.
- Robust Handling: Efficiently processes large volumes of data, with built-in mechanisms for retries and pagination.
#### Data Retrieval Scripts Setup and Usage
To run the data retrieval scripts for ACL Anthology and arXiv, first ensure Python is installed on your system. Install necessary dependencies with pip install requests beautifulsoup4 tqdm arxiv. For the ACL script, navigate to the script's directory and run python "ACL retriever.ipynb" to fetch and save papers in a JSON format (e.g., acl2023.json). Similarly, for the arXiv script, execute python "arXiv retriever.ipynb" to retrieve and store articles (e.g., data_collected_Jan2022-Feb2022.json). Both scripts allow customization of query parameters, such as date ranges and fields, to suit specific research needs.

### Code Description
This repository contains a collection of Jupyter Notebooks that form the experimental pipeline of this thesis project:

- Filter_Papers.ipynb: Located in the "filtering_dataset" folder, this notebook filters scientific papers from a JSON dataset based on specified keywords found in the titles or abstracts. The filtering process uses regular expressions for efficient matching, and the results are exported to a new JSON file for further analysis.
- GPT-3.5_Automatic_Classification_of_LLM_Limitations.ipynb: For the GPT-3.5_Automatic_Classification_of_LLM_Limitations.ipynb in the LLM_paper_classification folder which classifies the papers into two groups ("yes" for papers that mention at least one limitation of LLMs, "no" for papers that are completely irrelevant to limitaitons), ensure Python dependencies and the OpenAI API key are configured. The script processes academic papers, classifying them based on mentions of LLM limitations using GPT-3.5 Turbo. Results are saved to a CSV file, with progress tracked via a progress.txt file to resume interrupted tasks. Adjust the script parameters as needed for specific requirements. Ensure file paths are correctly set up in your environment, and monitor for errors, particularly API rate limits during execution.
- gpt_prompting_techniques.ipynb: This Jupyter notebook, located in the LLM_paper_ratings folder, employs advanced GPT-4.o prompting techniques to evaluate the depth of discussions on LLM limitations in academic papers. Ensure you have the openai and tqdm packages installed (pip install openai tqdm). Before running, update the notebook with your OpenAI API key.
- Open_Source_Model_Prompting_Techniques.ipynb: In the open_source_prompting_techniques folder, the Open_Source_Model_Prompting_Techniques.ipynb uses the "Mistral-7B-Instruct-v0.2" model from HuggingFace to rate academic papers on language model limitations. Ensure dependencies (tqdm, json, csv, langchain_community.llms) are installed and set the HuggingFace API token. Open the notebook, run cells to input a prompt number (1-3), and execute the assessment. Results are saved in a CSV file, summarizing titles, LLM discussions, ratings, and evidence.
- Baseline_Models_TF-IDF_SBERT.ipynb: This notebook is designed to train and evaluate baseline machine learning models using TF-IDF with Logistic Regression and SBERT embeddings to classify academic papers based on their ratings. This process helps in establishing a standard to compare against more complex models. To run the Baseline_Models_TF-IDF_SBERT.ipynb notebook, ensure Python is installed with pandas, sklearn, sentence_transformers, and imblearn. Place the dataset completed_gold_standard.xlsx in the specified path, preprocess it as necessary, and run each cell sequentially to train and evaluate the models. Adjust TF-IDF vectorizer settings and use SMOTE for class imbalance. Monitor for deprecation warnings from sklearn and adjust parameters accordingly. Review model performance metrics from the output and document any significant findings or adjustments for optimization.
- data_analysis.ipynb: For the data_analysis.ipynb notebook, ensure the dataset results_time_series_acl_all_papers.csv is correctly placed in your project directory. Install necessary Python packages like pandas, numpy, matplotlib, and seaborn using pip or conda. Open the notebook, execute all cells to process the data, perform analyses, and generate visualizations showing trends in ACL and Arxiv papers discussing LLM limitations. Modify the analysis period or visualization styles directly within the notebook as needed. Visual outputs can be viewed within the notebook or saved as images for external use.
- The clustering_LLM_limitations_final.ipynb: Applies two clustering algorithms to the analyzed papers to categorize discussed LLM limitations into a coherent taxonomy. This notebook synthesizes the findings into structured groups, aiding in the understanding and visualization of common themes and issues within the field.

##### LLM-Based Embeddings using Bertopic: 
For the clustering_LLM_limitations_final.ipynb GitHub documentation, specify that users must install BERTopic, SpaCy, NLTK, UMAP, HDBSCAN, and necessary libraries for running the notebook. The file should be configured with an openai.api_key and requires the dataset acl_only_4-5_rated_papers_final.xlsx uploaded to the environment. The notebook utilizes LLM embeddings and BERTopic with HDBSCAN for clustering textual data to identify distinct categories of LLM limitations discussed in ACL academic papers. Instructions should guide users on preprocessing texts, generating embeddings, fitting the model, and saving the clustered results to CSV. Ensure users adjust parameters according to their data specifics and check embedding generation and file permissions for successful execution.







