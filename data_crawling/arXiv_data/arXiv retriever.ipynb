{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d1c8c2",
   "metadata": {},
   "source": [
    "### Fetching the papers from arXiv API \n",
    "###### Retrieving the papers in a JSON format including title, published date, abstract, authors and the link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8e6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading papers.\n",
      "Progress: 0\n",
      "(cat:cs.CL OR cat:cs.LG OR cat:cs.AI OR cat:cs.CV) AND submittedDate:[202201010000 TO 202202010000]\n",
      "2022-1-Progress1: 500\n",
      "2022-1-Progress2: 1000\n",
      "2022-1-Progress3: 1500\n",
      "2022-1-Progress4: 2000\n",
      "2022-1-Progress5: 2500\n",
      "2022-1-Progress6: 3000\n",
      "Final Progress: 3235\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import types\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import arxiv\n",
    "import time\n",
    "import seaborn as sb\n",
    "from semanticscholar.Paper import Paper\n",
    "from tqdm import tqdm\n",
    "from semanticscholar import SemanticScholar\n",
    "import numpy as np\n",
    "from os.path import basename\n",
    "import asyncio\n",
    "\n",
    "def search(queries=[], field=\"all\", cats=[\"cs.CL\", \"cs.LG\", 'cs.AI', 'cs.CV'], start='202201010000', end='202202010000'):\n",
    "    # Use the arxiv API for the below categories\n",
    "    query_string, client = \"\", arxiv.Client(num_retries=40, page_size=1000)\n",
    "    if queries:\n",
    "        query_string += \"(\" + \" OR \".join(f\"{field}:{query}\" for query in queries) + \")\"\n",
    "    if cats:\n",
    "        if query_string:\n",
    "            query_string += \" AND \"\n",
    "        query_string += \"(\" + \" OR \".join(f\"cat:{cat}\" for cat in cats) + \")\"\n",
    "    query_string += f\" AND submittedDate:[{start} TO {end}]\"\n",
    "    print(query_string)\n",
    "    return client.results(arxiv.Search(\n",
    "        query=query_string,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    ))\n",
    "\n",
    "def _get_papers(self, paper_ids, fields: list = None):\n",
    "    if not fields:\n",
    "        fields = Paper.SEARCH_FIELDS\n",
    "\n",
    "    url = f'{self._AsyncSemanticScholar.api_url}/graph/v1/paper/batch'\n",
    "    fields = ','.join(fields)\n",
    "    parameters = f'&fields={fields}'\n",
    "\n",
    "    payload = {\"ids\": paper_ids}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            data = asyncio.run(self._AsyncSemanticScholar._requester.get_data_async(\n",
    "                url, parameters, self._AsyncSemanticScholar.auth_header, payload))\n",
    "            break\n",
    "        except:\n",
    "            print('time out; sleep 2 seconds...')\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "    papers = [Paper(item) if item else None for item in data]\n",
    "    return papers\n",
    "\n",
    "def get_papers(file=\"papers.shelf\", cached=False, start='202201010000', end='202404010000'):\n",
    "    papers = defaultdict(list)\n",
    "\n",
    "    if cached:\n",
    "        print(\"Loading cached papers.\")\n",
    "    else:\n",
    "        print(\"Downloading papers.\")\n",
    "        results = []\n",
    "        batch = []\n",
    "        progress = 0\n",
    "        print(\"Progress:\", 0)\n",
    "        count = 1\n",
    "        for result in search(start=start, end=end):\n",
    "            if len(batch) < 500:\n",
    "                results.append(result)\n",
    "                batch.append(\"arxiv:\" + basename(result.entry_id).split(\"v\")[0])\n",
    "                progress += 1\n",
    "            else:\n",
    "                print(f\"{result.published.year}-{result.published.month}-Progress{count}:\", progress)\n",
    "                count += 1\n",
    "                results, batch = [], []\n",
    "\n",
    "        if len(results) > 0:\n",
    "            print(\"Final Progress:\", progress)\n",
    "\n",
    "    return papers\n",
    "\n",
    "sch = SemanticScholar(timeout=100)\n",
    "sch.get_papers = types.MethodType(_get_papers, sch)\n",
    "\n",
    "def get_citations(batch):\n",
    "    papers = sch.get_papers(paper_ids=batch)\n",
    "    citation_counts = []\n",
    "    for p in papers:\n",
    "        try:\n",
    "            citation_counts.append(p.citationCount)\n",
    "        except:\n",
    "            citation_counts.append(0)\n",
    "    return citation_counts\n",
    "class Args:\n",
    "    start = '202201010000'\n",
    "    end = '202202010000'\n",
    "    use_cache = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "sb.set()\n",
    "\n",
    "papers = get_papers(cached=args.use_cache, start=args.start, end=args.end)\n",
    "papers_list = [vars(a) for a in sum(dict(papers).values(), [])]\n",
    "json_path = \"data_collected_Jan2022-Feb2022.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as jsonf:\n",
    "    json.dump(papers_list, jsonf, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33802088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading papers.\n",
      "Progress: 0\n",
      "(cat:cs.CL OR cat:cs.LG OR cat:cs.AI OR cat:cs.CV) AND submittedDate:[202201010000 TO 202202010000]\n",
      "Progress1: 500\n",
      "Progress2: 1000\n",
      "Progress3: 1500\n",
      "Progress4: 2000\n",
      "Progress5: 2500\n",
      "Progress6: 3000\n",
      "Final Progress: 3241\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import collections\n",
    "import types\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import arxiv\n",
    "import time\n",
    "import seaborn as sb\n",
    "from semanticscholar.Paper import Paper\n",
    "from tqdm import tqdm\n",
    "from semanticscholar import SemanticScholar\n",
    "import numpy as np\n",
    "from os.path import basename\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "    The `search_arxiv` function constructs a search query string based on provided parameters such as \n",
    "    queries, fields, categories, start date, and end date. It then uses the arXiv API to execute the \n",
    "    search query and fetches results within the specified date range. The function returns the search \n",
    "    results obtained from the arXiv repository.\n",
    "\"\"\"   \n",
    "def search(queries=[], field=\"all\", cats=[\"cs.CL\", \"cs.LG\", 'cs.AI', 'cs.CV'], start='202201010000', end='202202010000'):\n",
    "    query_string, client = \"\", arxiv.Client(num_retries=40, page_size=1000)\n",
    "    if queries:\n",
    "        query_string += \"(\" + \" OR \".join(f\"{field}:{query}\" for query in queries) + \")\"\n",
    "    if cats:\n",
    "        if query_string:\n",
    "            query_string += \" AND \"\n",
    "        query_string += \"(\" + \" OR \".join(f\"cat:{cat}\" for cat in cats) + \")\"\n",
    "    query_string += f\" AND submittedDate:[{start} TO {end}]\"\n",
    "    print(query_string)\n",
    "    return client.results(arxiv.Search(\n",
    "        query=query_string,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    ))\n",
    "\"\"\"\n",
    "    The `get_papers` function retrieves academic papers from the arXiv repository within a specified \n",
    "    date range. It iterates over the search results obtained using the `search_arxiv` function and \n",
    "    extracts relevant information such as the title, authors, publication date, summary, and PDF link \n",
    "    for each paper. Progress updates are printed every 500 papers processed, and the final number of \n",
    "    papers fetched is displayed before returning the collected data.\n",
    "\"\"\"\n",
    "def get_papers(cached=False, start='202201010000', end='202202010000'):\n",
    "    papers = []\n",
    "\n",
    "    print(\"Downloading papers.\")\n",
    "    progress = 0\n",
    "    print(\"Progress:\", 0)\n",
    "    count = 1\n",
    "    for result in search(start=start, end=end):\n",
    "        published_date = result.published  # Directly use the datetime object\n",
    "        pdf_link = f\"https://arxiv.org/pdf/{basename(result.entry_id)}.pdf\"\n",
    "        paper_info = {\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"published\": published_date.strftime('%Y-%m-%dT%H:%M:%SZ'),  # Format datetime as string if needed\n",
    "            \"summary\": result.summary,\n",
    "            \"pdf_link\": pdf_link\n",
    "        }\n",
    "        papers.append(paper_info)\n",
    "        progress += 1\n",
    "        if progress % 500 == 0:\n",
    "            print(f\"Progress{count}: {progress}\")\n",
    "            count += 1\n",
    "\n",
    "    print(\"Final Progress:\", progress)\n",
    "    return papers\n",
    "\n",
    "\n",
    "sb.set()\n",
    "\n",
    "#adjust dates and parameters to fetch the papers of different periods\n",
    "start_date = '202201010000'\n",
    "end_date = '202202010000'\n",
    "papers = get_papers(cached=False, start=start_date, end=end_date)\n",
    "json_path = \"data_collected_Jan2022-Feb2022.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as jsonf:\n",
    "    json.dump(papers, jsonf, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230caeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
