[
    {
        "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER",
        "authors": [
            "Sreyan Ghosh",
            "Utkarsh Tyagi",
            "Manan Suri",
            "Sonal Kumar",
            "Ramaneswaran S",
            "Dinesh Manocha"
        ],
        "published": "2023",
        "summary": "Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.8.pdf"
    },
    {
        "title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
        "authors": [
            "Pengcheng Yin",
            "Wen-Ding Li",
            "Kefan Xiao",
            "Abhishek Rao",
            "Yeming Wen",
            "Kensen Shi",
            "Joshua Howland",
            "Paige Bailey",
            "Michele Catasta",
            "Henryk Michalewski",
            "Oleksandr Polozov",
            "Charles Sutton"
        ],
        "published": "2023",
        "summary": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.9.pdf"
    },
    {
        "title": "MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning",
        "authors": [
            "Xu Zhang",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.11.pdf"
    },
    {
        "title": "A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces",
        "authors": [
            "Gabriella Chronis",
            "Kyle Mahowald",
            "Katrin Erk"
        ],
        "published": "2023",
        "summary": "We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., \u2018a beautiful three days\u2019). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.14.pdf"
    },
    {
        "title": "Text Adversarial Purification as Defense against Adversarial Attacks",
        "authors": [
            "Linyang Li",
            "Demin Song",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models,using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We test our proposed adversarial purification method on several strong adversarial attack methods including Textfooler and BERT-Attack and experimental results indicate that the purification algorithm can successfully defend against strong word-substitution attacks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.20.pdf"
    },
    {
        "title": "Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation",
        "authors": [
            "Kexin Yang",
            "Dayiheng Liu",
            "Wenqiang Lei",
            "Baosong Yang",
            "Mingfeng Xue",
            "Boxing Chen",
            "Jun Xie"
        ],
        "published": "2023",
        "summary": "Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address these concerns, we explore attribute-based CTG in a parameter-efficient manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector i.e., single-attribute prompt), which guides the generation of a fixed pre-trained language model (PLM) to satisfy a pre-specified attribute. These prompts can be simply concatenated as a whole for multi-attribute CTG without any re-training. Nevertheless, this may raise problems of fluency downgrading and position sensitivity. To solve this, Tailor provides two solutions to enhance the combination. The former contains a multi-attribute prompt mask and a re-indexing position sequence to bridge the gap between the training (one single-attribute prompt for each task) and the testing stage (concatenating two prompts). The latter introduces a trainable prompt connector to further enhance the combinations. Experiments demonstrate that, only requiring 0.08% extra training parameters of the GPT-2, Tailor can achieve effective and general improvements on eleven attribute-specific generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.25.pdf"
    },
    {
        "title": "Knowledge of cultural moral norms in large language models",
        "authors": [
            "Aida Ramezani",
            "Yang Xu"
        ],
        "published": "2023",
        "summary": "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as \u201chomosexuality\u201d and \u201cdivorce\u201d; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.26.pdf"
    },
    {
        "title": "Revealing Single Frame Bias for Video-and-Language Learning",
        "authors": [
            "Jie Lei",
            "Tamara Berg",
            "Mohit Bansal"
        ],
        "published": "2023",
        "summary": "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong \u201cstatic appearance bias\u201d in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https://github.com/jayleicn/singularity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.29.pdf"
    },
    {
        "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models",
        "authors": [
            "Ziqiao Ma",
            "Jiayi Pan",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open-world language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.31.pdf"
    },
    {
        "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
        "authors": [
            "Alessandro Stolfo",
            "Zhijing Jin",
            "Kumar Shridhar",
            "Bernhard Schoelkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.32.pdf"
    },
    {
        "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
        "authors": [
            "John Chung",
            "Ece Kamar",
            "Saleema Amershi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.34.pdf"
    },
    {
        "title": "Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest",
        "authors": [
            "Jack Hessel",
            "Ana Marasovic",
            "Jena D. Hwang",
            "Lillian Lee",
            "Jeff Da",
            "Rowan Zellers",
            "Robert Mankoff",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Large neural networks can now generate jokes, but do they really \u201cunderstand\u201d humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \u201cunderstanding\u201d a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image\u2019s locations/entities, what\u2019s unusual in the scene, and an explanation of the joke.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.41.pdf"
    },
    {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "authors": [
            "Kechi Zhang",
            "Zhuo Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.45.pdf"
    },
    {
        "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling",
        "authors": [
            "Haw-Shiuan Chang",
            "Ruei-Yao Sun",
            "Kathryn Ricci",
            "Andrew McCallum"
        ],
        "published": "2023",
        "summary": "Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model can even outperform the corresponding BERT_Large model. We analyze the behavior of our Multi-CLS BERT, showing that it has many of the same characteristics and behavior as a typical BERT 5-way ensemble, but with nearly 4-times less computation and memory.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.48.pdf"
    },
    {
        "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
        "authors": [
            "Lennart Wachowiak",
            "Dagmar Gromann"
        ],
        "published": "2023",
        "summary": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor\u2019s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT\u2019s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.58.pdf"
    },
    {
        "title": "ALERT: Adapt Language Models to Reasoning Tasks",
        "authors": [
            "Ping Yu",
            "Tianlu Wang",
            "Olga Golovneva",
            "Badr AlKhamissi",
            "Siddharth Verma",
            "Zhijing Jin",
            "Gargi Ghosh",
            "Mona Diab",
            "Asli Celikyilmaz"
        ],
        "published": "2023",
        "summary": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro \u2018OUR\u2019}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro \u2018OUR\u2019}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro \u2018OUR\u2019}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.60.pdf"
    },
    {
        "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
        "authors": [
            "Ayyoob ImaniGooghari",
            "Peiqin Lin",
            "Amir Hossein Kargaran",
            "Silvia Severini",
            "Masoud Jalili Sabet",
            "Nora Kassner",
            "Chunlan Ma",
            "Helmut Schmid",
            "Andr\u00e9 Martins",
            "Fran\u00e7ois Yvon",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023",
        "summary": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world\u2019s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.61.pdf"
    },
    {
        "title": "Pretrained Bidirectional Distillation for Machine Translation",
        "authors": [
            "Yimeng Zhuang",
            "Mei Tu"
        ],
        "published": "2023",
        "summary": "Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained Bidirectional Distillation (PBD) for NMT, which aims to efficiently transfer bidirectional language knowledge from masked language pretraining to NMT models. Its advantages are reflected in efficiency and effectiveness through a globally defined and bidirectional context-aware distillation objective. Bidirectional language knowledge of the entire sequence is transferred to an NMT model concurrently during translation training. Specifically, we propose self-distilled masked language pretraining to obtain the PBD objective. We also design PBD losses to efficiently distill the language knowledge, in the form of token probabilities, to the encoder and decoder of an NMT model using the PBD objective. Extensive experiments reveal that pretrained bidirectional distillation can significantly improve machine translation performance and achieve competitive or even better results than previous pretrain-finetune or unified multilingual translation methods in supervised, unsupervised, and zero-shot scenarios. Empirically, it is concluded that pretrained bidirectional distillation is an effective and efficient method for transferring language knowledge from pretrained language models to NMT models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.63.pdf"
    },
    {
        "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
        "authors": [
            "Batu Ozturkler",
            "Nikolay Malkin",
            "Zhen Wang",
            "Nebojsa Jojic"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think \u2013 retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum \u2013 probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.68.pdf"
    },
    {
        "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
        "authors": [
            "Xiang Yue",
            "Huseyin Inan",
            "Xuechen Li",
            "Girish Kumar",
            "Julia McAnallen",
            "Hoda Shajari",
            "Huan Sun",
            "David Levitan",
            "Robert Sim"
        ],
        "published": "2023",
        "summary": "Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.74.pdf"
    },
    {
        "title": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
        "authors": [
            "Jianfei Yu",
            "Qiankun Zhao",
            "Rui Xia"
        ],
        "published": "2023",
        "summary": "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA2LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA2LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.81.pdf"
    },
    {
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "authors": [
            "Myra Cheng",
            "Esin Durmus",
            "Dan Jurafsky"
        ],
        "published": "2023",
        "summary": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling. Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.84.pdf"
    },
    {
        "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information",
        "authors": [
            "Sunjae Kwon",
            "Rishabh Garodia",
            "Minhwa Lee",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023",
        "summary": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.88.pdf"
    },
    {
        "title": "Elaboration-Generating Commonsense Question Answering at Scale",
        "authors": [
            "Wenya Wang",
            "Vivek Srikumar",
            "Hannaneh Hajishirzi",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models\u2014an elaboration generator and an answer predictor\u2014allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap with GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.90.pdf"
    },
    {
        "title": "DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation",
        "authors": [
            "Menglong Lu",
            "Zhen Huang",
            "Yunxiang Zhao",
            "Zhiliang Tian",
            "Yang Liu",
            "Dongsheng Li"
        ],
        "published": "2023",
        "summary": "Self-training emerges as an important research line on domain adaptation. By taking the model\u2019s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanish- ment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.92.pdf"
    },
    {
        "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels",
        "authors": [
            "Luyu Gao",
            "Xueguang Ma",
            "Jimmy Lin",
            "Jamie Callan"
        ],
        "published": "2023",
        "summary": "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is \u201cfake\u201d and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder\u2019s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.99.pdf"
    },
    {
        "title": "Do language models have coherent mental models of everyday things?",
        "authors": [
            "Yuling Gu",
            "Bhavana Dalvi Mishra",
            "Peter Clark"
        ],
        "published": "2023",
        "summary": "When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that \u201cthe yolk surrounds the shell\u201d is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 \u201cX relation Y?\u201d true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent \u201cparts mental models\u201d (54-59% accurate, 19-43% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM\u2019s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM\u2019s pictures of everyday things can be significantly reduced.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.106.pdf"
    },
    {
        "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
        "authors": [
            "Or Honovich",
            "Uri Shaham",
            "Samuel R. Bowman",
            "Omer Levy"
        ],
        "published": "2023",
        "summary": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.108.pdf"
    },
    {
        "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
        "authors": [
            "Avi Caciularu",
            "Matthew Peters",
            "Jacob Goldberger",
            "Ido Dagan",
            "Arman Cohan"
        ],
        "published": "2023",
        "summary": "The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while \u201cpeeking\u201d into other topically-related documents. In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information. This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization).Following this scheme, we pre-train our model - termed QAmden - and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.110.pdf"
    },
    {
        "title": "ELQA: A Corpus of Metalinguistic Questions and Answers about English",
        "authors": [
            "Shabnam Behzad",
            "Keisuke Sakaguchi",
            "Nathan Schneider",
            "Amir Zeldes"
        ],
        "published": "2023",
        "summary": "We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic\u2014it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.113.pdf"
    },
    {
        "title": "Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues",
        "authors": [
            "Yue Feng",
            "Yunlong Jiao",
            "Animesh Prasad",
            "Nikolaos Aletras",
            "Emine Yilmaz",
            "Gabriella Kazai"
        ],
        "published": "2023",
        "summary": "User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user\u2019s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user\u2019s task goals. Existing studies on USM neglect explicitly modeling the user\u2019s task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user\u2019s preferences regarding the task attributes are fulfilled by the system for predicting the user\u2019s satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task attributes. Finally, it predicts the user satisfaction based on task attribute fulfillment and task attribute importance. Experimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC) show that SG-USM consistently outperforms competitive existing methods. Our extensive analysis demonstrates that SG-USM can improve the interpretability of user satisfaction modeling, has good scalability as it can effectively deal with unseen tasks and can also effectively work in low-resource settings by leveraging unlabeled data. Code is available at https://github.com/amzn/user-satisfaction-modeling.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.116.pdf"
    },
    {
        "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
        "authors": [
            "KiYoon Yoo",
            "Wonhyuk Ahn",
            "Jiho Jang",
            "Nojun Kwak"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios",
        "pdf_link": "https://aclanthology.org/2023.acl-long.117.pdf"
    },
    {
        "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
        "authors": [
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Wenqian Zhang",
            "Zhenyu Lei",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts \u2014 from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.118.pdf"
    },
    {
        "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Binxing Jiao",
            "Linjun Yang",
            "Daxin Jiang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available at https://github.com/microsoft/unilm/tree/master/simlm .",
        "pdf_link": "https://aclanthology.org/2023.acl-long.125.pdf"
    },
    {
        "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "authors": [
            "Xinxi Lyu",
            "Sewon Min",
            "Iz Beltagy",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.129.pdf"
    },
    {
        "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
        "authors": [
            "Yue Yu",
            "Rongzhi Zhang",
            "Ran Xu",
            "Jieyu Zhang",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2023",
        "summary": "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.141.pdf"
    },
    {
        "title": "Training-free Neural Architecture Search for RNNs and Transformers",
        "authors": [
            "Aaron Serianni",
            "Jugal Kalita"
        ],
        "published": "2023",
        "summary": "Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. Our source code is available at https://github.com/aaronserianni/training-free-nas.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.142.pdf"
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "authors": [
            "Lei Wang",
            "Wanyu Xu",
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Yunshi Lan",
            "Roy Ka-Wei Lee",
            "Ee-Peng Lim"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.147.pdf"
    },
    {
        "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step",
        "authors": [
            "Liunian Harold Li",
            "Jack Hessel",
            "Youngjae Yu",
            "Xiang Ren",
            "Kai-Wei Chang",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Chain-of-thought prompting (e.g., \u201cLet\u2019s think step-by-ste\u201d) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.150.pdf"
    },
    {
        "title": "Generating EDU Extracts for Plan-Guided Summary Re-Ranking",
        "authors": [
            "Griffin Adams",
            "Alex Fabbri",
            "Faisal Ladhak",
            "No\u00e9mie Elhadad",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues. We ground each candidate abstract on its own unique content plan and generate distinct plan-guided abstracts using a model\u2019s top beam. More concretely, a standard language model (a BART LM) auto-regressively generates elemental discourse unit (EDU) content plans with an extractive copy mechanism. The top K beams from the content plan generator are then used to guide a separate LM, which produces a single abstractive candidate for each distinct plan. We apply an existing re-ranker (BRIO) to abstractive candidates generated from our method, as well as baseline decoding methods. We show large relevance improvements over previously published methods on widely used single document news article corpora, with ROUGE-2 F1 gains of 0.88, 2.01, and 0.38 on CNN / Dailymail, NYT, and Xsum, respectively. A human evaluation on CNN / DM validates these results. Similarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow EDU plans outperforms sampling-based methods by by 1.05 ROUGE-2 F1 points. Code to generate and realize plans is available at https://github.com/griff4692/edu-sum.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.151.pdf"
    },
    {
        "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "authors": [
            "Boshi Wang",
            "Sewon Min",
            "Xiang Deng",
            "Jiaming Shen",
            "You Wu",
            "Luke Zettlemoyer",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs\u2019 capability to learn to reason in context.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.153.pdf"
    },
    {
        "title": "RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks",
        "authors": [
            "Zhaoyang Wang",
            "Zhiyue Liu",
            "Xiaopeng Zheng",
            "Qinliang Su",
            "Jiahai Wang"
        ],
        "published": "2023",
        "summary": "Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during inference. Towards this overlooked aspect, we propose a defense framework that aims to mitigate attacks by confusing attackers and correcting adversarial contexts that are caused by malicious perturbations. Our framework comprises three components: (1) a synonym-based transformation to randomly corrupt adversarial contexts in the word level, (2) a developed BERT defender to correct abnormal contexts in the representation level, and (3) a simple detection method to filter out adversarial examples, any of which can be flexibly combined. Additionally, our framework helps improve the robustness of the victim model during training. Extensive experiments demonstrate the effectiveness of our framework in defending against word-level adversarial attacks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.155.pdf"
    },
    {
        "title": "DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation",
        "authors": [
            "Guanqun Bi",
            "Lei Shen",
            "Yanan Cao",
            "Meng Chen",
            "Yuqiang Xie",
            "Zheng Lin",
            "Xiaodong He"
        ],
        "published": "2023",
        "summary": "Empathy is a crucial factor in open-domain conversations, which naturally shows one\u2019s caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outperforms competitive baselines in terms of controllability, informativeness, and diversity without the loss of context-relatedness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.158.pdf"
    },
    {
        "title": "Dynamic and Efficient Inference for Text Generation via BERT Family",
        "authors": [
            "Xiaobo Liang",
            "Juntao Li",
            "Lijun Wu",
            "Ziqiang Cao",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 \u2192 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks. Our code will be publicly available at GitHub https://github.com/dropreg/DEER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.162.pdf"
    },
    {
        "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
        "authors": [
            "Fan Yin",
            "Jesse Vig",
            "Philippe Laban",
            "Shafiq Joty",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.172.pdf"
    },
    {
        "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
        "authors": [
            "Mario Giulianelli",
            "Iris Luden",
            "Raquel Fernandez",
            "Andrey Kutuzov"
        ],
        "published": "2023",
        "summary": "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users \u2014 historical linguists, lexicographers, or social scientists \u2014 to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the \u2018definitions as representations\u2019 paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.176.pdf"
    },
    {
        "title": "An Invariant Learning Characterization of Controlled Text Generation",
        "authors": [
            "Carolina Zheng",
            "Claudia Shi",
            "Keyon Vafa",
            "Amir Feder",
            "David Blei"
        ],
        "published": "2023",
        "summary": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments. We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.179.pdf"
    },
    {
        "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Chuanqi Tan",
            "Fei Huang",
            "Songfang Huang"
        ],
        "published": "2023",
        "summary": "Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In addition, HyPe acquires negligible computational overheads, and is better than and compatible with previous state-of-the-art fine-tuning techniques.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.182.pdf"
    },
    {
        "title": "Word sense extension",
        "authors": [
            "Lei Yu",
            "Yang Xu"
        ],
        "published": "2023",
        "summary": "Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.184.pdf"
    },
    {
        "title": "Decoding Symbolism in Language Models",
        "authors": [
            "Meiqi Guo",
            "Rebecca Hwa",
            "Adriana Kovashka"
        ],
        "published": "2023",
        "summary": "This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on different types of symbolism and analyze the outcomes along multiple metrics. Our findings suggest that conventional symbols are more reliably elicited from LMs while situated symbols are more challenging. Results also reveal the negative impact of the bias in pre-trained corpora. We further demonstrate that a simple re-ranking strategy can mitigate the bias and significantly improve model performances to be on par with human performances in some cases.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.186.pdf"
    },
    {
        "title": "A Survey on Zero Pronoun Translation",
        "authors": [
            "Longyue Wang",
            "Siyou Liu",
            "Mingzhou Xu",
            "Linfeng Song",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023",
        "summary": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution so that researchers can recognize the current state and future directions of this field. We provide an organization of the literature based on evolution, dataset, method, and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.187.pdf"
    },
    {
        "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation",
        "authors": [
            "Nuo Chen",
            "Linjun Shou",
            "Jian Pei",
            "Ming Gong",
            "Bowen Cao",
            "Jianhui Chang",
            "Jia Li",
            "Daxin Jiang"
        ],
        "published": "2023",
        "summary": "Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on Semantic Textual Similarity and Transfer datasets",
        "pdf_link": "https://aclanthology.org/2023.acl-long.197.pdf"
    },
    {
        "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
        "authors": [
            "Li Sun",
            "Florian Luisier",
            "Kayhan Batmanghelich",
            "Dinei Florencio",
            "Cha Zhang"
        ],
        "published": "2023",
        "summary": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model\u2019s robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.200.pdf"
    },
    {
        "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
        "authors": [
            "Yu Song",
            "Santiago Miret",
            "Bang Liu"
        ],
        "published": "2023",
        "summary": "We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro \u2018BENCHMARK\u2019} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.201.pdf"
    },
    {
        "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
        "authors": [
            "Xingyao Wang",
            "Sha Li",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12% absolute F1.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.202.pdf"
    },
    {
        "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
        "authors": [
            "Alexander Scarlatos",
            "Andrew Lan"
        ],
        "published": "2023",
        "summary": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.205.pdf"
    },
    {
        "title": "Entity Tracking in Language Models",
        "authors": [
            "Najoung Kim",
            "Sebastian Schuster"
        ],
        "published": "2023",
        "summary": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.213.pdf"
    },
    {
        "title": "Faithful Question Answering with Monte-Carlo Planning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Hong Zhao",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023",
        "summary": "Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves advanced performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.218.pdf"
    },
    {
        "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
        "authors": [
            "Yiduo Guo",
            "Yaobo Liang",
            "Dongyan Zhao",
            "Bing Liu",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.221.pdf"
    },
    {
        "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model",
        "authors": [
            "Ali Omrani",
            "Alireza Salkhordeh Ziabari",
            "Charles Yu",
            "Preni Golazizian",
            "Brendan Kennedy",
            "Mohammad Atari",
            "Heng Ji",
            "Morteza Dehghani"
        ],
        "published": "2023",
        "summary": "Existing bias mitigation methods require social-group-specific word pairs (e.g., \u201cman\u201d \u2013 \u201cwoman\u201d) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) \u2014 a theoretical framework developed in social psychology for understanding the content of stereotyping \u2014 can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: \u201cgenuine\u201d \u2013 \u201cfake\u201d; competence: \u201csmart\u201d \u2013 \u201cstupid\u201d), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.227.pdf"
    },
    {
        "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
        "authors": [
            "Yixin Liu",
            "Alex Fabbri",
            "Pengfei Liu",
            "Yilun Zhao",
            "Linyong Nan",
            "Ruilin Han",
            "Simeng Han",
            "Shafiq Joty",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators\u2019 prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.228.pdf"
    },
    {
        "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "authors": [
            "Andrew Zhu",
            "Karmanya Aggarwal",
            "Alexander Feng",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.229.pdf"
    },
    {
        "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
        "authors": [
            "Siyu Yuan",
            "Jiangjie Chen",
            "Ziquan Fu",
            "Xuyang Ge",
            "Soham Shah",
            "Charles Jankowski",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "published": "2023",
        "summary": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., \u201cmake a cake\u201d), but leaves more specific goals with multi-facet constraints understudied (e.g., \u201cmake a cake for diabetics\u201d). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.236.pdf"
    },
    {
        "title": "CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels",
        "authors": [
            "Hyunsoo Cho",
            "Youna Kim",
            "Sang-goo Lee"
        ],
        "published": "2023",
        "summary": "Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis (CELDA), a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels).Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold:(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.239.pdf"
    },
    {
        "title": "MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction",
        "authors": [
            "Zhibin Gou",
            "Qingyan Guo",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.240.pdf"
    },
    {
        "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
        "authors": [
            "Josh Magnus Ludan",
            "Yixuan Meng",
            "Tai Nguyen",
            "Saurabh Shah",
            "Qing Lyu",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs\u2019 reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). The efficacy generalizes across multiple model families and scales, with greater gains for larger models. Finally, our method also works well with explanations generated by the model, implying its applicability to more datasets without human-written explanations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.242.pdf"
    },
    {
        "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization",
        "authors": [
            "Yang Luo",
            "Xiaozhe Ren",
            "Zangwei Zheng",
            "Zhuo Jiang",
            "Xin Jiang",
            "Yang You"
        ],
        "published": "2023",
        "summary": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.243.pdf"
    },
    {
        "title": "On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
        "authors": [
            "Omar Shaikh",
            "Hongxin Zhang",
            "William Held",
            "Michael Bernstein",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model\u2019s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.244.pdf"
    },
    {
        "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
        "authors": [
            "Xinyu Zhu",
            "Junjie Wang",
            "Lin Zhang",
            "Yuxiang Zhang",
            "Yongfeng Huang",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.245.pdf"
    },
    {
        "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
        "authors": [
            "Zhengfu He",
            "Tianxiang Sun",
            "Qiong Tang",
            "Kuanning Wang",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.248.pdf"
    },
    {
        "title": "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
        "authors": [
            "Chen Tang",
            "Hongbo Zhang",
            "Tyler Loakman",
            "Chenghua Lin",
            "Frank Guerin"
        ],
        "published": "2023",
        "summary": "Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.253.pdf"
    },
    {
        "title": "Unified Demonstration Retriever for In-Context Learning",
        "authors": [
            "Xiaonan Li",
            "Kai Lv",
            "Hang Yan",
            "Tianyang Lin",
            "Wei Zhu",
            "Yuan Ni",
            "Guotong Xie",
            "Xiaoling Wang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks\u2019 training signals into a unified list-wise ranking formulation by language model\u2019s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks\u2019 signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR\u2019s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.256.pdf"
    },
    {
        "title": "Hidden Schema Networks",
        "authors": [
            "Ramses Sanchez",
            "Lukas Conrads",
            "Pascal Welke",
            "Kostadin Cvejoski",
            "Cesar Ojeda Marin"
        ],
        "published": "2023",
        "summary": "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk \u201creasoning\u201d models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.263.pdf"
    },
    {
        "title": "Don\u2019t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
        "authors": [
            "Yu Gu",
            "Xiang Deng",
            "Yu Su"
        ],
        "published": "2023",
        "summary": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.270.pdf"
    },
    {
        "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
        "authors": [
            "Pengcheng He",
            "Baolin Peng",
            "Song Wang",
            "Yang Liu",
            "Ruochen Xu",
            "Hany Hassan",
            "Yu Shi",
            "Chenguang Zhu",
            "Wayne Xiong",
            "Michael Zeng",
            "Jianfeng Gao",
            "Xuedong Huang"
        ],
        "published": "2023",
        "summary": "This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model\u2019s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ createsa new state-of-the-art on 9 of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM540B on XSum, and the finetuned 200x larger GPT3175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.279.pdf"
    },
    {
        "title": "Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model",
        "authors": [
            "Yi Xu",
            "Shuqian Sheng",
            "Jiexing Qi",
            "Luoyi Fu",
            "Zhouhan Lin",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "published": "2023",
        "summary": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language model that does not introduce external annotation tools or additional parallel information. It achieves fully unsupervised graph-text mutual conversion for the first time. Specifically, INFINITY treats both G2T and T2G as a bidirectional sequence generation task by fine-tuning only one pretrained seq2seq model. A novel back-translation-based framework is then designed to generate synthetic parallel data automatically. Besides, we investigate the impact of graph linearization and introduce the structure-aware fine-tuning strategy to alleviate possible performance deterioration via retaining structural information in graph sequences. As a fully unsupervised framework, INFINITY is empirically verified to outperform state-of-the-art baselines for G2T and T2G tasks. Additionally, we also devise a new training setting called cross learning for low-resource unsupervised information extraction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.281.pdf"
    },
    {
        "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
        "authors": [
            "Han Cheol Moon",
            "Shafiq Joty",
            "Ruochen Zhao",
            "Megh Thakkar",
            "Chi Xu"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
        "pdf_link": "https://aclanthology.org/2023.acl-long.282.pdf"
    },
    {
        "title": "MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning",
        "authors": [
            "Zhenrui Yue",
            "Huimin Zeng",
            "Yang Zhang",
            "Lanyu Shang",
            "Dong Wang"
        ],
        "published": "2023",
        "summary": "With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose MetaAdapt, a meta learning based approach for domain adaptive few-shot misinformation detection. MetaAdapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). In particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. Based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. As such, MetaAdapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. To demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.286.pdf"
    },
    {
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "authors": [
            "Yifei Li",
            "Zeqi Lin",
            "Shizhuo Zhang",
            "Qiang Fu",
            "Bei Chen",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2023",
        "summary": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.291.pdf"
    },
    {
        "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
        "authors": [
            "Tamanna Hossain",
            "Sunipa Dev",
            "Sameer Singh"
        ],
        "published": "2023",
        "summary": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce Misgendered, a framework for evaluating large language models\u2019 ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual\u2019s pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.293.pdf"
    },
    {
        "title": "Reasoning with Language Model Prompting: A Survey",
        "authors": [
            "Shuofei Qiao",
            "Yixin Ou",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yunzhi Yao",
            "Shumin Deng",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "published": "2023",
        "summary": "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.294.pdf"
    },
    {
        "title": "DISCO: Distilling Counterfactuals with Large Language Models",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao",
            "Antoine Bosselut",
            "Ashish Sabharwal",
            "Kyle Richardson"
        ],
        "published": "2023",
        "summary": "Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high-quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository are available at: https://github.com/eric11eca/disco",
        "pdf_link": "https://aclanthology.org/2023.acl-long.302.pdf"
    },
    {
        "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
        "authors": [
            "Peifeng Wang",
            "Zhengyang Wang",
            "Zheng Li",
            "Yifan Gao",
            "Bing Yin",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM\u2019s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.304.pdf"
    },
    {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "authors": [
            "Ehsan Kamalloo",
            "Nouha Dziri",
            "Charles Clarke",
            "Davood Rafiei"
        ],
        "published": "2023",
        "summary": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.307.pdf"
    },
    {
        "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
        "authors": [
            "Sha Li",
            "Ruining Zhao",
            "Manling Li",
            "Heng Ji",
            "Chris Callison-Burch",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.312.pdf"
    },
    {
        "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training",
        "authors": [
            "Yan Zeng",
            "Wangchunshu Zhou",
            "Ao Luo",
            "Ziming Cheng",
            "Xinsong Zhang"
        ],
        "published": "2023",
        "summary": "In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval datasets show that while conceptually simpler, CCLM significantly outperforms the prior state-of-the-art with an average absolute improvement of over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained model that surpasses the translate-test performance of representative English vision-language models by zero-shot cross-lingual transfer.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.315.pdf"
    },
    {
        "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Shafiq Joty",
            "Chengwei Qin",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.320.pdf"
    },
    {
        "title": "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
        "authors": [
            "Qingyu Lu",
            "Liang Ding",
            "Liping Xie",
            "Kanjian Zhang",
            "Derek F. Wong",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "The pretrained language model (PLM) based metrics have been successfully used in evaluating language generation tasks. Recent studies of the human evaluation community show that considering both major errors (e.g. mistranslated tokens) and minor errors (e.g. imperfections in fluency) can produce high-quality judgments. This inspires us to approach the final goal of the automatic metrics (human-like evaluations) by fine-grained error analysis. In this paper, we argue that the ability to estimate sentence confidence is the tip of the iceberg for PLM-based metrics. And it can be used to refine the generated sentence toward higher confidence and more reference-grounded, where the costs of refining and approaching reference are used to determine the major and minor errors, respectively. To this end, we take BARTScore as the testbed and present an innovative solution to marry the unexploited sentence refining capacity of BARTScore and human-like error analysis, where the final score consists of both the evaluations of major and minor errors. Experiments show that our solution consistently and significantly improves BARTScore, and outperforms top-scoring metrics in 19/25 test settings. Analyses demonstrate our method robustly and efficiently approaches human-like evaluations, enjoying better interpretability. Our code and scripts will be publicly released in https://github.com/Coldmist-Lu/ErrorAnalysis_NLGEvaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.324.pdf"
    },
    {
        "title": "Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation",
        "authors": [
            "Hongyi Wu",
            "Hao Zhou",
            "Man Lan",
            "Yuanbin Wu",
            "Yadong Zhang"
        ],
        "published": "2023",
        "summary": "Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.325.pdf"
    },
    {
        "title": "What is the best recipe for character-level encoder-only modelling?",
        "authors": [
            "Kris Cao"
        ],
        "published": "2023",
        "summary": "This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations (Clark et al., 2022, Jaegle et al., 2022, Tay et al., 2021) and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level tokeniser during pretraining, and final model performance is highly dependent on tokeniser quality. We believe our results demonstrate the readiness of character-level models for multilingual language representation, and encourage NLP practitioners to try them as drop-in replacements for token-based models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.326.pdf"
    },
    {
        "title": "Language model acceptability judgements are not always robust to context",
        "authors": [
            "Koustuv Sinha",
            "Jon Gauthier",
            "Aaron Mueller",
            "Kanishka Misra",
            "Keren Fuentes",
            "Roger Levy",
            "Adina Williams"
        ],
        "published": "2023",
        "summary": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models\u2019 syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models\u2019 implicit in-context learning abilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.333.pdf"
    },
    {
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
        "authors": [
            "Yilun Zhao",
            "Chen Zhao",
            "Linyong Nan",
            "Zhenting Qi",
            "Wenlin Zhang",
            "Xiangru Tang",
            "Boyu Mi",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "Despite significant progress having been made in question answering on tabular data (Table QA), it\u2019s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.334.pdf"
    },
    {
        "title": "Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks",
        "authors": [
            "Moritz Plenz",
            "Juri Opitz",
            "Philipp Heinisch",
            "Philipp Cimiano",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction heuristics by computing semantic similarity between KG triplets and textual arguments. Using these triplet similarities as weights, we extract contextualized knowledge paths that connect a conclusion to its premise, while maximizing similarity to the argument. We combine multiple paths into a CCKG that we optionally prune to reduce noise and raise precision. Intrinsic evaluation of the quality of our graphs shows that our method is effective for (re)constructing human explanation graphs. Manual evaluations in a large-scale knowledge selection setup verify high recall and precision of implicit CSK in the CCKGs. Finally, we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument quality rating task, outperforming strong baselines and rivaling a GPT-3 based system.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.338.pdf"
    },
    {
        "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2023",
        "summary": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.340.pdf"
    },
    {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "authors": [
            "Eshaan Tanwar",
            "Subhabrata Dutta",
            "Manish Borthakur",
            "Tanmoy Chakraborty"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy \u2014 Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.346.pdf"
    },
    {
        "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
        "authors": [
            "Vaishali Pal",
            "Andrew Yates",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "published": "2023",
        "summary": "Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.348.pdf"
    },
    {
        "title": "Long-Tailed Question Answering in an Open World",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Yinhe Zheng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM).Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing.A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.351.pdf"
    },
    {
        "title": "Parallel Context Windows for Large Language Models",
        "authors": [
            "Nir Ratner",
            "Yoav Levine",
            "Yonatan Belinkov",
            "Ori Ram",
            "Inbal Magar",
            "Omri Abend",
            "Ehud Karpas",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2023",
        "summary": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\u201cwindows\u201d), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.352.pdf"
    },
    {
        "title": "ContraCLM: Contrastive Learning For Causal Language Model",
        "authors": [
            "Nihal Jain",
            "Dejiao Zhang",
            "Wasi Uddin Ahmad",
            "Zijian Wang",
            "Feng Nan",
            "Xiaopeng Li",
            "Ming Tan",
            "Ramesh Nallapati",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.355.pdf"
    },
    {
        "title": "Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model",
        "authors": [
            "Pengwei Zhan",
            "Jing Yang",
            "Xiao Huang",
            "Chunlei Jing",
            "Jingying Li",
            "Liming Wang"
        ],
        "published": "2023",
        "summary": "Neural language models have achieved superior performance. However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions. In this paper, we explain the model pathology from the view of sentence representation and argue that the counter-intuitive bias degree and direction of the out-of-distribution examples\u2019 representation cause the pathology. We propose a Contrastive learning regularization method using Adversarial examples for Alleviating the Pathology (ConAAP), which calibrates the sentence representation of out-of-distribution examples. ConAAP generates positive and negative examples following the attribution results and utilizes adversarial examples to introduce direction information in regularization. Experiments show that ConAAP effectively alleviates the model pathology while slightly impacting the generalization ability on in-distribution examples and thus helps interpretation methods obtain more faithful results.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.358.pdf"
    },
    {
        "title": "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue",
        "authors": [
            "Weihao Zeng",
            "Keqing He",
            "Yejie Wang",
            "Chen Zeng",
            "Jingang Wang",
            "Yunsen Xian",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills future knowledge to the representation of the previous dialogue context using a self-training framework. Our intuition is that a good dialogue representation both learns local context information and predicts future information. Extensive experiments on diverse downstream dialogue tasks demonstrate the effectiveness of our model, especially the generalization, robustness, and learning discriminative dialogue representations capabilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.360.pdf"
    },
    {
        "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
        "authors": [
            "Mehran Kazemi",
            "Najoung Kim",
            "Deepti Bhatia",
            "Xin Xu",
            "Deepak Ramachandran"
        ],
        "published": "2023",
        "summary": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.361.pdf"
    },
    {
        "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Meeyoung Cha",
            "Yejin Choi",
            "Byoungpil Kim",
            "Gunhee Kim",
            "Eun-Ju Lee",
            "Yong Lim",
            "Alice Oh",
            "Sangchul Park",
            "Jung-Woo Ha"
        ],
        "published": "2023",
        "summary": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.370.pdf"
    },
    {
        "title": "FLamE: Few-shot Learning from Natural Language Explanations",
        "authors": [
            "Yangqiaoyu Zhou",
            "Yiming Zhang",
            "Chenhao Tan"
        ],
        "published": "2023",
        "summary": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., \u201cnot know\u201d for the neutral label) in generated explanations.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.372.pdf"
    },
    {
        "title": "What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics",
        "authors": [
            "Julia Watson",
            "Barend Beekhuizen",
            "Suzanne Stevenson"
        ],
        "published": "2023",
        "summary": "Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people\u2019s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT\u2019s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.375.pdf"
    },
    {
        "title": "Interpretable Math Word Problem Solution Generation via Step-by-step Planning",
        "authors": [
            "Mengxue Zhang",
            "Zichao Wang",
            "Zhichao Yang",
            "Weiqi Feng",
            "Andrew Lan"
        ],
        "published": "2023",
        "summary": "Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning approach for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed, given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our approach improves the accuracy and interpretability of the solution on both automatic metrics and human evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.379.pdf"
    },
    {
        "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
        "authors": [
            "Zixiu Wu",
            "Simone Balloccu",
            "Ehud Reiter",
            "Rim Helaoui",
            "Diego Reforgiato Recupero",
            "Daniele Riboni"
        ],
        "published": "2023",
        "summary": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.382.pdf"
    },
    {
        "title": "PairSpanBERT: An Enhanced Language Model for Bridging Resolution",
        "authors": [
            "Hideo Kobayashi",
            "Yufang Hou",
            "Vincent Ng"
        ],
        "published": "2023",
        "summary": "We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing SpanBERT with PairSpanBERT in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.383.pdf"
    },
    {
        "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
        "authors": [
            "Tianle Li",
            "Xueguang Ma",
            "Alex Zhuang",
            "Yu Gu",
            "Yu Su",
            "Wenhu Chen"
        ],
        "published": "2023",
        "summary": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.385.pdf"
    },
    {
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "authors": [
            "Liangming Pan",
            "Xiaobao Wu",
            "Xinyuan Lu",
            "Anh Tuan Luu",
            "William Yang Wang",
            "Min-Yen Kan",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.386.pdf"
    },
    {
        "title": "Patton: Language Model Pretraining on Text-Rich Networks",
        "authors": [
            "Bowen Jin",
            "Wentao Zhang",
            "Yu Zhang",
            "Yu Meng",
            "Xinyang Zhang",
            "Qi Zhu",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.387.pdf"
    },
    {
        "title": "When and how to paraphrase for named entity recognition?",
        "authors": [
            "Saket Sharma",
            "Aviral Joshi",
            "Yiyun Zhao",
            "Namrata Mukhija",
            "Hanoz Bhathena",
            "Prateek Singh",
            "Sashank Santhanam"
        ],
        "published": "2023",
        "summary": "While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, and dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength, while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.390.pdf"
    },
    {
        "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales",
        "authors": [
            "Brihi Joshi",
            "Ziyi Liu",
            "Sahana Ramnath",
            "Aaron Chan",
            "Zhewei Tong",
            "Shaoliang Nie",
            "Qifan Wang",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale\u2019s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs\u2019 ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.392.pdf"
    },
    {
        "title": "Learning Better Masking for Better Language Model Pre-training",
        "authors": [
            "Dongjie Yang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.400.pdf"
    },
    {
        "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
        "authors": [
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Haonan Yin",
            "B\u00f6rje Karlsson",
            "Chin-Yew Lin"
        ],
        "published": "2023",
        "summary": "Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.403.pdf"
    },
    {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "authors": [
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2023",
        "summary": "State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.406.pdf"
    },
    {
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "authors": [
            "Myles Foley",
            "Ambrish Rawat",
            "Taesung Lee",
            "Yufang Hou",
            "Gabriele Picco",
            "Giulio Zizzo"
        ],
        "published": "2023",
        "summary": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.410.pdf"
    },
    {
        "title": "Large Language Models Meet NL2Code: A Survey",
        "authors": [
            "Daoguang Zan",
            "Bei Chen",
            "Fengji Zhang",
            "Dianjie Lu",
            "Bingchao Wu",
            "Bei Guan",
            "Wang Yongji",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \u201cLarge Size, Premium Data, Expert Tuning\u201d. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.411.pdf"
    },
    {
        "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
        "authors": [
            "Youngjin Jin",
            "Eugene Jang",
            "Jian Cui",
            "Jin-Woo Chung",
            "Yongjae Lee",
            "Seungwon Shin"
        ],
        "published": "2023",
        "summary": "Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.415.pdf"
    },
    {
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "authors": [
            "Wenjun Peng",
            "Jingwei Yi",
            "Fangzhao Wu",
            "Shangxi Wu",
            "Bin Bin Zhu",
            "Lingjuan Lyu",
            "Binxing Jiao",
            "Tong Xu",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \u2018METHOD\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\u2019s model for copyright verification while minimizing the adverse impact on the original embeddings\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.423.pdf"
    },
    {
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
        "authors": [
            "Afra Feyza Akyurek",
            "Ekin Akyurek",
            "Ashwin Kalyan",
            "Peter Clark",
            "Derry Tanti Wijaya",
            "Niket Tandon"
        ],
        "published": "2023",
        "summary": "Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.427.pdf"
    },
    {
        "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
        "authors": [
            "CheolWon Na",
            "YunSeok Choi",
            "Jee-Hyong Lee"
        ],
        "published": "2023",
        "summary": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model), a high-performance and effective black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.430.pdf"
    },
    {
        "title": "Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation",
        "authors": [
            "Ke Wang",
            "Xin Ge",
            "Jiayi Wang",
            "Yuqi Zhang",
            "Yu Zhao"
        ],
        "published": "2023",
        "summary": "Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human\u2019s edits. Translation Suggestion (TS), as an interactive mode to assist human translators, requires machines to generate alternatives for specific incorrect words or phrases selected by human translators. In this paper, we utilize the parameterized objective function of neural machine translation (NMT) and propose a novel constrained decoding algorithm, namely Prefix-Suffix Guided Decoding (PSGD), to deal with the TS problem without additional training. Compared to state-of-the-art lexical-constrained decoding method, PSGD improves translation quality by an average of 10.6 BLEU and reduces time overhead by an average of 63.4% on benchmark datasets. Furthermore, on both the WeTS and the WMT 2022 Translation Suggestion datasets, it is superior over other supervised learning systems trained with TS annotated data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.434.pdf"
    },
    {
        "title": "Query Refinement Prompts for Closed-Book Long-Form QA",
        "authors": [
            "Reinald Kim Amplayo",
            "Kellie Webster",
            "Michael Collins",
            "Dipanjan Das",
            "Shashi Narayan"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once \u2013 to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.444.pdf"
    },
    {
        "title": "Data Curation Alone Can Stabilize In-context Learning",
        "authors": [
            "Ting-Yun Chang",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets\u2014both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7% and 6.3%, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.452.pdf"
    },
    {
        "title": "S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction",
        "authors": [
            "Benfeng Xu",
            "Quan Wang",
            "Yajuan Lyu",
            "Dai Dai",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023",
        "summary": "Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.455.pdf"
    },
    {
        "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models",
        "authors": [
            "Xuxi Chen",
            "Tianlong Chen",
            "Weizhu Chen",
            "Ahmed Hassan Awadallah",
            "Zhangyang Wang",
            "Yu Cheng"
        ],
        "published": "2023",
        "summary": "Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-rank updates on top of the pre-trained weights; and (ii) resource-efficient inference - by encouraging a sparse weight structure towards the final fine-tuned model. We leverage sparsity in these two directions by exploiting both unstructured and structured sparse patterns in pre-trained language models viaa unified approach. Extensive experiments and in-depth investigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2) on dozens of datasets, consistently demonstrate impressive parameter-/inference-efficiency, while maintaining competitive downstream performance. For instance, DSEE saves about 25% inference FLOPs while achieving comparable performance, with 0.5% trainable parameters on BERT. Codes are available at https://github.com/VITA-Group/DSEE.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.456.pdf"
    },
    {
        "title": "A New Dataset and Empirical Study for Sentence Simplification in Chinese",
        "authors": [
            "Shiping Yang",
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.462.pdf"
    },
    {
        "title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression",
        "authors": [
            "Siyue Wu",
            "Hongzhan Chen",
            "Xiaojun Quan",
            "Qifan Wang",
            "Rui Wang"
        ],
        "published": "2023",
        "summary": "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher\u2019s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.471.pdf"
    },
    {
        "title": "Targeted Data Generation: Finding and Fixing Model Weaknesses",
        "authors": [
            "Zexue He",
            "Marco Tulio Ribeiro",
            "Fereshte Khani"
        ],
        "published": "2023",
        "summary": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (LLMs) with a human in the loop. TDG estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within-group performance without hurting overall performance. In our experiments, TDG significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.474.pdf"
    },
    {
        "title": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation",
        "authors": [
            "Anchun Gui",
            "Han Xiao"
        ],
        "published": "2023",
        "summary": "To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameter-efficient fine-tuning method HiFi, that is, only the highly informative and strongly correlated attention heads for the specific task are fine-tuned. To search for those significant attention heads, we develop a novel framework to analyze the effectiveness of heads. Specifically, we first model the relationship between heads into a graph from two perspectives of information richness and correlation, and then apply PageRank algorithm to determine the relative importance of each head. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi obtains state-of-the-art performance over the prior baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.475.pdf"
    },
    {
        "title": "On \u201cScientific Debt\u201d in NLP: A Case for More Rigour in Language Model Pre-Training Research",
        "authors": [
            "Made Nindyatama Nityasya",
            "Haryo Wibowo",
            "Alham Fikri Aji",
            "Genta Winata",
            "Radityo Eko Prasojo",
            "Phil Blunsom",
            "Adhiguna Kuncoro"
        ],
        "published": "2023",
        "summary": "This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand: \u201cHow exactly does each factor contribute to the progress that we have today?\u201d We provide a case in point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and demonstrate how \u2014 under comparable conditions where the baselines are tuned to a similar extent \u2014 these baselines (and even-simpler variants thereof) can, in fact, achieve competitive or better performance than BERT. These findings demonstrate how disentangling different factors of model improvements can lead to valuable new insights. We conclude with recommendations for how to encourage and incentivize this line of work, and accelerate progress towards a better and more systematic understanding of what factors drive the progress of our foundation models today.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.477.pdf"
    },
    {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2023",
        "summary": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \u201cLasswell Communication Model\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.482.pdf"
    },
    {
        "title": "Dynamic Regularization in UDA for Transformers in Multimodal Classification",
        "authors": [
            "Ivonne Monter-Aldana",
            "Adrian Pastor Lopez Monroy",
            "Fernando Sanchez-Vega"
        ],
        "published": "2023",
        "summary": "Multimodal machine learning is a cutting-edge field that explores ways to incorporate information from multiple sources into models. As more multimodal data becomes available, this field has become increasingly relevant. This work focuses on two key challenges in multimodal machine learning. The first is finding efficient ways to combine information from different data types. The second is that often, one modality (e.g., text) is stronger and more relevant, making it difficult to identify meaningful patterns in the weaker modality (e.g., image). Our approach focuses on more effectively exploiting the weaker modality while dynamically regularizing the loss function. First, we introduce a new two-stream model called Multimodal BERT-ViT, which features a novel intra-CLS token fusion. Second, we utilize a dynamic adjustment that maintains a balance between specialization and generalization during the training to avoid overfitting, which we devised. We add this dynamic adjustment to the Unsupervised Data Augmentation (UDA) framework. We evaluate the effectiveness of these proposals on the task of multi-label movie genre classification using the Moviescope and MM-IMDb datasets. The evaluation revealed that our proposal offers substantial benefits, while simultaneously enabling us to harness the weaker modality without compromising the information provided by the stronger.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.485.pdf"
    },
    {
        "title": "The CRINGE Loss: Learning what language not to model",
        "authors": [
            "Leonard Adolphs",
            "Tianyu Gao",
            "Jing Xu",
            "Kurt Shuster",
            "Sainbayar Sukhbaatar",
            "Jason Weston"
        ],
        "published": "2023",
        "summary": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data \u2013 examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the \u201cCRINGE\u201d loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.493.pdf"
    },
    {
        "title": "My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave",
        "authors": [
            "Pavan Holur",
            "David Chong",
            "Timothy Tangherlini",
            "Vwani Roychowdhury"
        ],
        "published": "2023",
        "summary": "News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of aligned story actors responsible for sustaining the issue-specific narratives. Discovering aligned actors, and the groups these alignments create, brings us closer to estimating the narrative that each group represents. With the help of Large Language Models (LLM), we address this task by: (i) Introducing a corpus of text segments rich in narrative content associated with six different current issues; (ii) Introducing a novel two-step graph-based framework that (a) identifies alignments between actors (INCANT) and (b) extracts aligned actor groups using the network structure (TAMPA). Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework. Across domains, alignment relationships from INCANT are accurate (macro F1 >= 0.75) and actor groups from TAMPA are preferred over 2 non-trivial baseline models (ACC >= 0.75).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.497.pdf"
    },
    {
        "title": "Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model",
        "authors": [
            "Hongwei Zeng",
            "Bifan Wei",
            "Jun Liu",
            "Weiping Fu"
        ],
        "published": "2023",
        "summary": "Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation (ZeroCQG), which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage knowledge transfer framework, Synthesize, Prompt, and trAnsfer with pRe-Trained lAnguage model (SPARTA) to effectively leverage knowledge from single-turn question generation instances. To validate the zero-shot performance of SPARTA, we conduct extensive experiments on three conversational datasets: CoQA, QuAC, and DoQA by transferring knowledge from three single-turn datasets: MS MARCO, NewsQA, and SQuAD. The experimental results demonstrate the superior performance of our method. Specifically, SPARTA has achieved 14.81 BLEU-4 (88.2% absolute improvement compared to T5) in CoQA with knowledge transferred from SQuAD.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.500.pdf"
    },
    {
        "title": "Backpack Language Models",
        "authors": [
            "John Hewitt",
            "John Thickstun",
            "Christopher Manning",
            "Percy Liang"
        ],
        "published": "2023",
        "summary": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model\u2019s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM\u2019s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.506.pdf"
    },
    {
        "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
        "authors": [
            "Virginia Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2023",
        "summary": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.507.pdf"
    },
    {
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
        "authors": [
            "Joshua Maynez",
            "Priyanka Agrawal",
            "Sebastian Gehrmann"
        ],
        "published": "2023",
        "summary": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like fewshot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks\u2013while they can be used to compare systems at a high level\u2013relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.511.pdf"
    },
    {
        "title": "Span-level Aspect-based Sentiment Analysis via Table Filling",
        "authors": [
            "Mao Zhang",
            "Yongxin Zhu",
            "Zhen Liu",
            "Zhimin Bao",
            "Yunfei Wu",
            "Xing Sun",
            "Linli Xu"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding opinion expressions, in this paper, we propose Table Filling BERT (TF-BERT), which considers the consistency of multi-word opinion expressions at the span-level. Specially, we learn the span representations with a table filling method, by constructing an upper triangular table for each sentiment polarity, of which the elements represent the sentiment intensity of the specific sentiment polarity for all spans in the sentence. Two methods are then proposed, including table-decoding and table-aggregation, to filter out target spans or aggregate each table for sentiment polarity classification. In addition, we design a sentiment consistency regularizer to guarantee the sentiment consistency of each span for different sentiment polarities. Experimental results on three benchmarks demonstrate the effectiveness of our proposed model.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.515.pdf"
    },
    {
        "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
        "authors": [
            "Xiaohang Tang",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2023",
        "summary": "Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two distinct timestamps T1 and T2, we first propose an unsupervised method to select (a) pivot terms related to both C1 and C2, and (b) anchor terms that are associated with a specific pivot term in each individual snapshot.We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms.Moreover, we propose an automatic method to learn time-sensitive templates from C1 and C2, without requiring any human supervision.Next, we use the generated prompts to adapt a pretrained MLM to T2 by fine-tuning using those prompts.Multiple experiments show that our proposed method significantly reduces the perplexity of test sentences in C2, outperforming the current state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.520.pdf"
    },
    {
        "title": "Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM\u2019s Translation Capability",
        "authors": [
            "Eleftheria Briakou",
            "Colin Cherry",
            "George Foster"
        ],
        "published": "2023",
        "summary": "Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism\u2014the unintentional consumption of bilingual signals, including translation examples\u2014in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM\u2019s out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.524.pdf"
    },
    {
        "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation",
        "authors": [
            "Chandra Bhagavatula",
            "Jena D. Hwang",
            "Doug Downey",
            "Ronan Le Bras",
            "Ximing Lu",
            "Lianhui Qin",
            "Keisuke Sakaguchi",
            "Swabha Swayamdipta",
            "Peter West",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model\u2019s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.535.pdf"
    },
    {
        "title": "A Measure-Theoretic Characterization of Tight Language Models",
        "authors": [
            "Li Du",
            "Lucas Torroba Hennigen",
            "Tiago Pimentel",
            "Clara Meister",
            "Jason Eisner",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can \u201cleak\u201d onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.543.pdf"
    },
    {
        "title": "PromptRank: Unsupervised Keyphrase Extraction Using Prompt",
        "authors": [
            "Aobo Kong",
            "Shiwan Zhao",
            "Hao Chen",
            "Qicheng Li",
            "Yong Qin",
            "Ruiqi Sun",
            "Xiaoyan Bai"
        ],
        "published": "2023",
        "summary": "The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to document embeddings. However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning. To this end, in this paper, we propose a simple yet effective unsupervised approach, PromptRank, based on the PLM with an encoder-decoder architecture. Specifically, PromptRank feeds the document into the encoder and calculates the probability of generating the candidate with a designed prompt by the decoder. We extensively evaluate the proposed PromptRank on six widely used benchmarks. PromptRank outperforms the SOTA approach MDERank, improving the F1 score relatively by 34.18%, 24.87%, and 17.57% for 5, 10, and 15 returned results, respectively. This demonstrates the great potential of using prompt for unsupervised keyphrase extraction. We release our code at https://github.com/HLT-NLP/PromptRank.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.545.pdf"
    },
    {
        "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
        "authors": [
            "Alex Mallen",
            "Akari Asai",
            "Victor Zhong",
            "Rajarshi Das",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs\u2019 strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.546.pdf"
    },
    {
        "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
        "authors": [
            "Akshita Jha",
            "Aida Mostafazadeh Davani",
            "Chandan K Reddy",
            "Shachi Dave",
            "Vinodkumar Prabhakaran",
            "Sunipa Dev"
        ],
        "published": "2023",
        "summary": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.548.pdf"
    },
    {
        "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
        "authors": [
            "Jiangjie Chen",
            "Wei Shi",
            "Ziquan Fu",
            "Sijie Cheng",
            "Lei Li",
            "Yanghua Xiao"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \u201clions don\u2019t live in the ocean\u201d, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.550.pdf"
    },
    {
        "title": "Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction",
        "authors": [
            "Ashish Sharma",
            "Kevin Rushton",
            "Inna Lin",
            "David Wadden",
            "Khendra Lucas",
            "Adam Miner",
            "Theresa Nguyen",
            "Tim Althoff"
        ],
        "published": "2023",
        "summary": "A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful \u201creframed thought.\u201d Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people\u2019s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a \u201chigh-quality\u201d reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.555.pdf"
    },
    {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "Harsh Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published": "2023",
        "summary": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.557.pdf"
    },
    {
        "title": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Jens Lehmann",
            "Sung Ju Hwang"
        ],
        "published": "2023",
        "summary": "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.558.pdf"
    },
    {
        "title": "Improved Instruction Ordering in Recipe-Grounded Conversation",
        "authors": [
            "Duong Le",
            "Ruohao Guo",
            "Wei Xu",
            "Alan Ritter"
        ],
        "published": "2023",
        "summary": "In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model\u2019s lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks, namely User Intent Detection and Instruction State Tracking, to support Response Generation with improved instruction grounding. Experimenting with our newly collected dataset, ChattyChef, shows that incorporating user intent and instruction state information helps the response generation model mitigate the incorrect order issue. Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order instructions. We will release ChattyChef to facilitate further research in this area at: https://github.com/octaviaguo/ChattyChef.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.561.pdf"
    },
    {
        "title": "Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023",
        "summary": "While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have made their behavior somewhat opaque. To mitigate this opacity, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to analyze their influence on model probabilities over a sequence of upcoming words with only one forward pass from the model. Using the change in next-word probability as a measure of importance, this work first examines which context words make the biggest contribution to language model predictions. Regression experiments suggest that Transformer-based language models rely primarily on collocational associations, followed by linguistic factors such as syntactic dependencies and coreference relationships in making next-word predictions. Additionally, analyses using these measures to predict syntactic dependencies and coreferent mention spans show that collocational association and repetitions of the same token largely explain the language models\u2019 predictions on these tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.562.pdf"
    },
    {
        "title": "Language Detoxification with Attribute-Discriminative Latent Space",
        "authors": [
            "Jin Myung Kwak",
            "Minseon Kim",
            "Sung Ju Hwang"
        ],
        "published": "2023",
        "summary": "Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.565.pdf"
    },
    {
        "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Xuebo Liu",
            "Min Zhang",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings up to +1.56% average improvement over the vanilla token dropping.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.579.pdf"
    },
    {
        "title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers",
        "authors": [
            "Ariel Gera",
            "Roni Friedman",
            "Ofir Arviv",
            "Chulaka Gunasekara",
            "Benjamin Sznajder",
            "Noam Slonim",
            "Eyal Shnarch"
        ],
        "published": "2023",
        "summary": "Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.580.pdf"
    },
    {
        "title": "FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering",
        "authors": [
            "Anku Rani",
            "S.M Towhidul Islam Tonmoy",
            "Dwip Dalal",
            "Shreya Gautam",
            "Megha Chakraborty",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude claim and conclude whether it\u2019s truthful or a mere masquerade. Popular fact-checking websites follow a common structure for fact categorization such as half true, half false, false, pants on fire, etc. Therefore, it is necessary to have an aspect-based (delineating which part(s) are true and which are false) explainable system that can assist human fact-checkers in asking relevant questions related to a fact, which can then be validated separately to reach a final verdict. In this paper, we propose a 5W framework (who, what, when, where, and why) for question-answer-based fact explainability. To that end, we present a semi-automatically generated dataset called FACTIFY-5WQA, which consists of 391, 041 facts along with relevant 5W QAs \u2013 underscoring our major contribution to this paper. A semantic role labeling system has been utilized to locate 5Ws, which generates QA pairs for claims using a masked language model. Finally, we report a baseline QA system to automatically locate those answers from evidence documents, which can serve as a baseline for future research in the field. Lastly, we propose a robust fact verification system that takes paraphrased claims and automatically validates them. The dataset and the baseline model are available at https: //github.com/ankuranii/acl-5W-QA",
        "pdf_link": "https://aclanthology.org/2023.acl-long.581.pdf"
    },
    {
        "title": "Rethinking Masked Language Modeling for Chinese Spelling Correction",
        "authors": [
            "Hongqiu Wu",
            "Shaohua Zhang",
            "Yuchen Zhang",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy \u2013 randomly masking 20% non-error tokens from the input sequence during fine-tuning \u2013 is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.600.pdf"
    },
    {
        "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Chen Xinyu",
            "Yuxin Ding",
            "Lin Ma",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.601.pdf"
    },
    {
        "title": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships",
        "authors": [
            "David Jurgens",
            "Agrima Seth",
            "Jackson Sargent",
            "Athena Aghighi",
            "Michael Geraci"
        ],
        "published": "2023",
        "summary": "Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.616.pdf"
    },
    {
        "title": "How Do In-Context Examples Affect Compositional Generalization?",
        "authors": [
            "Shengnan An",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ],
        "published": "2023",
        "summary": "Compositional generalization\u2013understanding unseen combinations of seen primitives\u2013is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning\u2013the prevailing few-shot paradigm based on large language models\u2013exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.618.pdf"
    },
    {
        "title": "Is GPT-3 a Good Data Annotator?",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Linlin Liu",
            "Yew Ken Chia",
            "Boyang Li",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.626.pdf"
    },
    {
        "title": "Few-shot Event Detection: An Empirical Study and a Unified View",
        "authors": [
            "Yubo Ma",
            "Zehao Wang",
            "Yixin Cao",
            "Aixin Sun"
        ],
        "published": "2023",
        "summary": "Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we compare 12 representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. Experiments consistently demonstrate that prompt-based methods, including ChatGPT, still significantly trail prototype-based methods in terms of overall performance. To investigate their superior performance, we break down their design elements along several dimensions and build a unified framework on prototype-based methods. Under such unified view, each prototype-method can be viewed a combination of different modules from these design elements. We further combine all advantageous modules and propose a simple yet effective baseline, which outperforms existing methods by a large margin (e.g., 2.7% F1 gains under low-resource setting).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.628.pdf"
    },
    {
        "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
        "authors": [
            "Chenglei Si",
            "Dan Friedman",
            "Nitish Joshi",
            "Shi Feng",
            "Danqi Chen",
            "He He"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases\u2014for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.632.pdf"
    },
    {
        "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
        "authors": [
            "Yuheng Zha",
            "Yichi Yang",
            "Ruichen Li",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.634.pdf"
    },
    {
        "title": "Introducing Semantics into Speech Encoders",
        "authors": [
            "Derek Xu",
            "Shuyan Dong",
            "Changhan Wang",
            "Suyoun Kim",
            "Zhaojiang Lin",
            "Bing Liu",
            "Akshat Shrivastava",
            "Shang-Wen Li",
            "Liang-Hsuan Tseng",
            "Guan-Ting Lin",
            "Alexei Baevski",
            "Hung-yi Lee",
            "Yizhou Sun",
            "Wei Wang"
        ],
        "published": "2023",
        "summary": "Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.639.pdf"
    },
    {
        "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "authors": [
            "Deokjae Lee",
            "JunYeong Lee",
            "Jung-Woo Ha",
            "Jin-Hwa Kim",
            "Sang-Woo Lee",
            "Hwaran Lee",
            "Hyun Oh Song"
        ],
        "published": "2023",
        "summary": "The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient black-box red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods.The source code is available at https://github.com/snu-mllab/Bayesian-Red-Teaming.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.646.pdf"
    },
    {
        "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
        "authors": [
            "Xiaochuang Han",
            "Sachin Kumar",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM\u2014a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.647.pdf"
    },
    {
        "title": "Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
        "authors": [
            "Chengyue Jiang",
            "Wenyang Hui",
            "Yong Jiang",
            "Xiaobin Wang",
            "Pengjun Xie",
            "Kewei Tu"
        ],
        "published": "2023",
        "summary": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g., president, politician) of a given entity mention (e.g., Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance. It brings deeper interaction between the mention and the type to reach better performance but has to perform N (the type set size) forward passes to infer all the types of a single mention. CE is therefore very slow in inference when the type set is large (e.g., N=10k for UFET). % Cross-encoder also ignores the correlation between different types.To this end, we propose to perform entity typing in a recall-expand-filter manner. The recall and expansion stages prune the large type set and generate K (typically much smaller than N) most relevant type candidates for each mention. At the filter stage, we use a novel model called {pasted macro \u2018NAME\u2019} to concurrently encode and score all these K candidates in only one forward pass to obtain the final type prediction. We investigate different model options for each stage and conduct extensive experiments to compare each option, experiments show that our method reaches SOTA performance on UFET and is thousands of times faster than the CE-based architecture. We also found our method is very effective in fine-grained (130 types) and coarse-grained (9 types) entity typing. Our code is available at {pasted macro \u2018CODE\u2019}.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.648.pdf"
    },
    {
        "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
        "authors": [
            "Liyan Tang",
            "Tanya Goyal",
            "Alex Fabbri",
            "Philippe Laban",
            "Jiacheng Xu",
            "Semih Yavuz",
            "Wojciech Kryscinski",
            "Justin Rousseau",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems\u2019 outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.650.pdf"
    },
    {
        "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
        "authors": [
            "Zheng Xin Yong",
            "Hailey Schoelkopf",
            "Niklas Muennighoff",
            "Alham Fikri Aji",
            "David Ifeoluwa Adelani",
            "Khalid Almubarak",
            "M Saiful Bari",
            "Lintang Sutawika",
            "Jungo Kasai",
            "Ahmed Baruwa",
            "Genta Winata",
            "Stella Biderman",
            "Edward Raff",
            "Dragomir Radev",
            "Vassilina Nikoulina"
        ],
        "published": "2023",
        "summary": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.653.pdf"
    },
    {
        "title": "SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT",
        "authors": [
            "Aditya Yadavalli",
            "Alekhya Yadavalli",
            "Vera Tobin"
        ],
        "published": "2023",
        "summary": "Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker\u2019s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES)\u2014a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese\u2014to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.657.pdf"
    },
    {
        "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
        "authors": [
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.658.pdf"
    },
    {
        "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?",
        "authors": [
            "Chengwei Qin",
            "Shafiq Joty",
            "Qian Li",
            "Ruochen Zhao"
        ],
        "published": "2023",
        "summary": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.659.pdf"
    },
    {
        "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
        "authors": [
            "Hritik Bansal",
            "Karthik Gopalakrishnan",
            "Saket Dingliwal",
            "Sravan Bodapati",
            "Katrin Kirchhoff",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: ~70% of the attention heads and ~20% of the feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These induction heads overlap with task-specific important heads, reinforcing arguments by Olsson et al. (2022) regarding induction head generality to more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.660.pdf"
    },
    {
        "title": "ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain",
        "authors": [
            "Mike Zhang",
            "Rob van der Goot",
            "Barbara Plank"
        ],
        "published": "2023",
        "summary": "The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R-large on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.662.pdf"
    },
    {
        "title": "Generalizing Backpropagation for Gradient-Based Interpretability",
        "authors": [
            "Kevin Du",
            "Lucas Torroba Hennigen",
            "Niklas Stoehr",
            "Alex Warstadt",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model\u2019s output with respect to its inputs. While these methods can indicate which input features may be important for the model\u2019s prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT\u2019s behavior on the subject\u2013verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.669.pdf"
    },
    {
        "title": "Generic Temporal Reasoning with Differential Analysis and Explanation",
        "authors": [
            "Yu Feng",
            "Ben Zhou",
            "Haoyu Wang",
            "Helen Jin",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems\u2019 generalizability due to existing datasets\u2019 limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY\u2019s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.671.pdf"
    },
    {
        "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
        "authors": [
            "Tianxing He",
            "Jingyu Zhang",
            "Tianle Wang",
            "Sachin Kumar",
            "Kyunghyun Cho",
            "James Glass",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.674.pdf"
    },
    {
        "title": "WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings",
        "authors": [
            "Wenjie Zhuo",
            "Yifan Sun",
            "Xiaohan Wang",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "published": "2023",
        "summary": "This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \u201cpushing\u201d operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on BERT{pasted macro \u2018BA\u2019}) Spearman correlation on STS tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.677.pdf"
    },
    {
        "title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization",
        "authors": [
            "SongYang Gao",
            "Shihan Dou",
            "Yan Liu",
            "Xiao Wang",
            "Qi Zhang",
            "Zhongyu Wei",
            "Jin Ma",
            "Ying Shan"
        ],
        "published": "2023",
        "summary": "Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data\u2019s probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70% compared to current best-performing adversarial training methods. Experiments demonstrate that DSRM considerably improves BERT\u2019s resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.680.pdf"
    },
    {
        "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
        "authors": [
            "Kundan Krishna",
            "Saurabh Garg",
            "Jeffrey Bigham",
            "Zachary Lipton"
        ],
        "published": "2023",
        "summary": "For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gainsare attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning.In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around 10x\u2013500x less data), outperforming the latter on 7 and 5 datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks,including the GLUE benchmark. Besides classification tasks, self-pretraining also provides benefits on structured output prediction tasks such as span based question answering and commonsense inference, often providing more than 50% of the performance boosts provided by pretraining on the BookWiki corpus. Our results hint that in many scenarios, performance gains attributable to pretraining are driven primarily by the pretraining objective itself and are not always attributable to the use of external pretraining data in massive amounts. These findings are especially relevant in light of concerns about intellectual property and offensive content in web-scale pretraining data.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.682.pdf"
    },
    {
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
        "authors": [
            "Xiang Lisa Li",
            "Ari Holtzman",
            "Daniel Fried",
            "Percy Liang",
            "Jason Eisner",
            "Tatsunori Hashimoto",
            "Luke Zettlemoyer",
            "Mike Lewis"
        ],
        "published": "2023",
        "summary": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.687.pdf"
    },
    {
        "title": "Resolving Indirect Referring Expressions for Entity Selection",
        "authors": [
            "Mohammad Javad Hosseini",
            "Filip Radlinski",
            "Silvia Pareti",
            "Annie Louis"
        ],
        "published": "2023",
        "summary": "Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice \u2018Should we make a Simnel cake or a Pandan cake\u00bf a natural response from a non-expert may be indirect: \u2018let\u2019s make the green one\u2018. Reference resolution has been little studied with natural expressions, thus robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of entity pairs and utterances, and develop models for the disambiguation problem. Consisting of 42K indirect referring expressions across three domains, it enables for the first time the study of how large language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.688.pdf"
    },
    {
        "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
        "authors": [
            "Sumanth Doddapaneni",
            "Rahul Aralikatte",
            "Gowtham Ramesh",
            "Shreya Goyal",
            "Mitesh M. Khapra",
            "Anoop Kunchukuttan",
            "Pratyush Kumar"
        ],
        "published": "2023",
        "summary": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.693.pdf"
    },
    {
        "title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks",
        "authors": [
            "Yun Tang",
            "Anna Sun",
            "Hirofumi Inaguma",
            "Xinyue Chen",
            "Ning Dong",
            "Xutai Ma",
            "Paden Tomasello",
            "Juan Pino"
        ],
        "published": "2023",
        "summary": "Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED\u2019s strength in non-monotonic sequence to sequence learning while retaining Transducer\u2019s streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the MuST-C dataset and the findings demonstrate that TAED performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.695.pdf"
    },
    {
        "title": "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It",
        "authors": [
            "Jisoo Mok",
            "Jaeyoung Do",
            "Sungjin Lee",
            "Tara Taghavi",
            "Seunghak Yu",
            "Sungroh Yoon"
        ],
        "published": "2023",
        "summary": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM\u2019s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.703.pdf"
    },
    {
        "title": "DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
        "authors": [
            "Haiming Wang",
            "Ye Yuan",
            "Zhengying Liu",
            "Jianhao Shen",
            "Yichun Yin",
            "Jing Xiong",
            "Enze Xie",
            "Han Shi",
            "Yujun Li",
            "Lin Li",
            "Jian Yin",
            "Zhenguo Li",
            "Xiaodan Liang"
        ],
        "published": "2023",
        "summary": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).",
        "pdf_link": "https://aclanthology.org/2023.acl-long.706.pdf"
    },
    {
        "title": "Effective Contrastive Weighting for Dense Query Expansion",
        "authors": [
            "Xiao Wang",
            "Sean MacAvaney",
            "Craig Macdonald",
            "Iadh Ounis"
        ],
        "published": "2023",
        "summary": "Verbatim queries submitted to search engines often do not sufficiently describe the user\u2019s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query\u2019srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.710.pdf"
    },
    {
        "title": "Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model",
        "authors": [
            "Jakob Prange",
            "Man Ho Ivy Wong"
        ],
        "published": "2023",
        "summary": "We use both Bayesian and neural models to dissect a data set of Chinese learners\u2019 pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.712.pdf"
    },
    {
        "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
        "authors": [
            "Fangyu Liu",
            "Francesco Piccinno",
            "Syrine Krichene",
            "Chenxi Pang",
            "Kenton Lee",
            "Mandar Joshi",
            "Yasemin Altun",
            "Nigel Collier",
            "Julian Eisenschlos"
        ],
        "published": "2023",
        "summary": "Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models\u2019 capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.714.pdf"
    },
    {
        "title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization",
        "authors": [
            "Yulong Chen",
            "Yang Liu",
            "Ruochen Xu",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Michael Zeng",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UniSumm outperforms strong baselines by a large margin across all sub-tasks in SummZoo under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.718.pdf"
    },
    {
        "title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
        "authors": [
            "Yi Xu",
            "Shuqian Sheng",
            "Bo Xue",
            "Luoyi Fu",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "published": "2023",
        "summary": "Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the emergence of a new idea can be regarded as the fusion of two concepts that co-occur in an academic paper. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.727.pdf"
    },
    {
        "title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language",
        "authors": [
            "Nuwa Xi",
            "Sendong Zhao",
            "Haochun Wang",
            "Chi Liu",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023",
        "summary": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first open-vocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, UniCoRN: the Unified Cognitive Signal ReconstructioN for Brain Decoding. By reconstructing both individual time points and time series, UniCoRN establishes a robust encoder for cognitive signals (fMRI & EEG). Leveraging a pre-trained language model as decoder, UniCoRN proves its efficacy in decoding coherent text from fMRI series across various split settings. Our model achieves a 34.77% BLEU score on fMRI2text, and a 37.04% BLEU when generalized to EEG-to-text decoding, thereby surpassing the former baseline. Experimental results indicate the feasibility of decoding consecutive fMRI volumes, and the effectiveness of decoding different cognitive signals using a unified structure.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.741.pdf"
    },
    {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "authors": [
            "Yizhong Wang",
            "Yeganeh Kordi",
            "Swaroop Mishra",
            "Alisa Liu",
            "Noah A. Smith",
            "Daniel Khashabi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Large \u201cinstruction-tuned\u201d language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.754.pdf"
    },
    {
        "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Alexander Rudnicky",
            "Peter Ramadge"
        ],
        "published": "2023",
        "summary": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.756.pdf"
    },
    {
        "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Zijing Shi",
            "Yitong Li",
            "Ling Chen",
            "Mykola Pechenizkiy"
        ],
        "published": "2023",
        "summary": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models\u2019 conversational capabilities.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.757.pdf"
    },
    {
        "title": "Entailment as Robust Self-Learner",
        "authors": [
            "Jiaxin Ge",
            "Hongyin Luo",
            "Yoon Kim",
            "James Glass"
        ],
        "published": "2023",
        "summary": "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.772.pdf"
    },
    {
        "title": "ReCode: Robustness Evaluation of Code Generation Models",
        "authors": [
            "Shiqi Wang",
            "Zheng Li",
            "Haifeng Qian",
            "Chenghao Yang",
            "Zijian Wang",
            "Mingyue Shang",
            "Varun Kumar",
            "Samson Tan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Ramesh Nallapati",
            "Murali Krishna Ramanathan",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model\u2019s robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.773.pdf"
    },
    {
        "title": "Mitigating Label Biases for In-context Learning",
        "authors": [
            "Yu Fei",
            "Yifan Hou",
            "Zeming Chen",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model\u2019s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model\u2019s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.783.pdf"
    },
    {
        "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
        "authors": [
            "Dongfu Jiang",
            "Xiang Ren",
            "Bill Yuchen Lin"
        ],
        "published": "2023",
        "summary": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.792.pdf"
    },
    {
        "title": "Python Code Generation by Asking Clarification Questions",
        "authors": [
            "Haau-Sing (Xiaocheng) Li",
            "Mohsen Mesgar",
            "Andr\u00e9 Martins",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "Code generation from text requires understanding the user\u2019s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.799.pdf"
    },
    {
        "title": "PAD-Net: An Efficient Framework for Dynamic Networks",
        "authors": [
            "Shwai He",
            "Liang Ding",
            "Daize Dong",
            "Boan Liu",
            "Fuqiang Yu",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model\u2019s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by +0.7% top-1 acc with only 30% dynamic parameters for ResNet-50 and +1.9% average score in language understanding with only 50% dynamic parameters for BERT. Code will be released at: https://github.com/Shwai-He/PAD-Net.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.803.pdf"
    },
    {
        "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
        "authors": [
            "Or Honovich",
            "Thomas Scialom",
            "Omer Levy",
            "Timo Schick"
        ],
        "published": "2023",
        "summary": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.806.pdf"
    },
    {
        "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
        "authors": [
            "Nitay Calderon",
            "Subhabrata Mukherjee",
            "Roi Reichart",
            "Amir Kantor"
        ],
        "published": "2023",
        "summary": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.818.pdf"
    },
    {
        "title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
        "authors": [
            "Qingyu Tan",
            "Hwee Tou Ng",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.828.pdf"
    },
    {
        "title": "Large Language Models Are Reasoning Teachers",
        "authors": [
            "Namgyu Ho",
            "Laura Schmid",
            "Se-Young Yun"
        ],
        "published": "2023",
        "summary": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model\u2019s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.830.pdf"
    },
    {
        "title": "Visually-augmented pretrained language models for NLP tasks without images",
        "authors": [
            "Hangyu Guo",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Qinyu Zhang",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.833.pdf"
    },
    {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "authors": [
            "Jasivan Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2023",
        "summary": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect. The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.838.pdf"
    },
    {
        "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
        "authors": [
            "Yixin Liu",
            "Budhaditya Deb",
            "Milagro Teruel",
            "Aaron Halfaker",
            "Dragomir Radev",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2023",
        "summary": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.844.pdf"
    },
    {
        "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
        "authors": [
            "Julia Mendelsohn",
            "Ronan Le Bras",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023",
        "summary": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \u201ccosmopolitan\u201d in a sentence such as \u201cwe need to end the cosmopolitan experiment\u201d can mean \u201cworldly\u201d to many but also secretly mean \u201cJewish\u201d to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians\u2019 speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3\u2019s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.845.pdf"
    },
    {
        "title": "Exploring Large Language Models for Classical Philology",
        "authors": [
            "Frederick Riemenschneider",
            "Anette Frank"
        ],
        "published": "2023",
        "summary": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5\u2019s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.846.pdf"
    },
    {
        "title": "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media",
        "authors": [
            "Mario Aragon",
            "Adrian Pastor Lopez Monroy",
            "Luis Gonzalez",
            "David E. Losada",
            "Manuel Montes"
        ],
        "published": "2023",
        "summary": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.853.pdf"
    },
    {
        "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
        "authors": [
            "Peng Li",
            "Tianxiang Sun",
            "Qiong Tang",
            "Hang Yan",
            "Yuanbin Wu",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.855.pdf"
    },
    {
        "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
        "authors": [
            "David Vilar",
            "Markus Freitag",
            "Colin Cherry",
            "Jiaming Luo",
            "Viresh Ratnakar",
            "George Foster"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM\u2019s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM\u2019s MT output which reveals some interesting properties and prospects for future work.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.859.pdf"
    },
    {
        "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
        "authors": [
            "Ilias Chalkidis",
            "Nicolas Garneau",
            "Catalina Goanta",
            "Daniel Katz",
            "Anders S\u00f8gaard"
        ],
        "published": "2023",
        "summary": "In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models\u2019 size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model\u2019s size and prior legal knowledge which can be estimated by upstream and probing performance. Based on these findings, we can conclude that both dimensions are important for those seeking the development of domain-specific PLMs.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.865.pdf"
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "authors": [
            "Somin Wadhwa",
            "Silvio Amir",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.868.pdf"
    },
    {
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2023",
        "summary": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.870.pdf"
    },
    {
        "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
        "authors": [
            "Zhongbin Xie",
            "Thomas Lukasiewicz"
        ],
        "published": "2023",
        "summary": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.876.pdf"
    },
    {
        "title": "Few-shot Reranking for Multi-hop QA via Language Model Prompting",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples \u2014 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.885.pdf"
    },
    {
        "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
        "authors": [
            "Yusen Zhang",
            "Jun Wang",
            "Zhiguo Wang",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.887.pdf"
    },
    {
        "title": "Crosslingual Generalization through Multitask Finetuning",
        "authors": [
            "Niklas Muennighoff",
            "Thomas Wang",
            "Lintang Sutawika",
            "Adam Roberts",
            "Stella Biderman",
            "Teven Le Scao",
            "M Saiful Bari",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Hailey Schoelkopf",
            "Xiangru Tang",
            "Dragomir Radev",
            "Alham Fikri Aji",
            "Khalid Almubarak",
            "Samuel Albanie",
            "Zaid Alyafeai",
            "Albert Webson",
            "Edward Raff",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.891.pdf"
    },
    {
        "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
        "authors": [
            "Mounica Maddela",
            "Yao Dou",
            "David Heineman",
            "Wei Xu"
        ],
        "published": "2023",
        "summary": "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank & Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.905.pdf"
    },
    {
        "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
        "authors": [
            "Luyu Gao",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Anthony Chen",
            "Arun Tejasvi Chaganty",
            "Yicheng Fan",
            "Vincent Zhao",
            "Ni Lao",
            "Hongrae Lee",
            "Da-Cheng Juan",
            "Kelvin Guu"
        ],
        "published": "2023",
        "summary": "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.910.pdf"
    },
    {
        "title": "Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models",
        "authors": [
            "Daniel Hardt"
        ],
        "published": "2023",
        "summary": "We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.4.pdf"
    },
    {
        "title": "Tracing Linguistic Markers of Influence in a Large Online Organisation",
        "authors": [
            "Prashant Khare",
            "Ravi Shekhar",
            "Mladen Karan",
            "Stephen McQuistin",
            "Colin Perkins",
            "Ignacio Castro",
            "Gareth Tyson",
            "Patrick Healey",
            "Matthew Purver"
        ],
        "published": "2023",
        "summary": "Social science and psycholinguistic research have shown that power and status affect how people use language in a range of domains. Here, we investigate a similar question in a large, distributed, consensus-driven community with little traditional power hierarchy \u2013 the Internet Engineering Task Force (IETF), a collaborative organisation that designs internet standards. Our analysis based on lexical categories (LIWC) and BERT, shows that participants\u2019 levels of influence can be predicted from their email text, and identify key linguistic differences (e.g., certain LIWC categories, such as \u201cWE\u201d are positively correlated with high-influence). We also identify the differences in language use for the same person before and after becoming influential.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.8.pdf"
    },
    {
        "title": "Dataset Distillation with Attention Labels for Fine-tuning BERT",
        "authors": [
            "Aru Maekawa",
            "Naoki Kobayashi",
            "Kotaro Funakoshi",
            "Manabu Okumura"
        ],
        "published": "2023",
        "summary": "Dataset distillation aims to create a small dataset of informative synthetic samples to rapidly train neural networks that retain the performance of the original dataset. In this paper, we focus on constructing distilled few-shot datasets for natural language processing (NLP) tasks to fine-tune pre-trained transformers. Specifically, we propose to introduce attention labels, which can efficiently distill the knowledge from the original dataset and transfer it to the transformer models via attention probabilities. We evaluated our dataset distillation methods in four various NLP tasks and demonstrated that it is possible to create distilled few-shot datasets with the attention labels, yielding impressive performances for fine-tuning BERT. Specifically, in AGNews, a four-class news classification task, our distilled few-shot dataset achieved up to 93.2% accuracy, which is 98.5% performance of the original dataset even with only one sample per class and only one gradient step.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.12.pdf"
    },
    {
        "title": "Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques",
        "authors": [
            "Daking Rai",
            "Bailin Wang",
            "Yilun Zhou",
            "Ziyu Yao"
        ],
        "published": "2023",
        "summary": "Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM\u2019s generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.15.pdf"
    },
    {
        "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
        "authors": [
            "Xin Liu",
            "Muhammad Khalifa",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model\u2019s output logits. Unlike prior work, BOLT maintains the generator\u2019s autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), BOLT demonstrates significantly improved efficiency and fluency. On sentiment control, BOLT is 7x faster than competitive baselines, and more fluent in 74.4% of the evaluation samples according to human judges.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.18.pdf"
    },
    {
        "title": "Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer",
        "authors": [
            "Xingtai Lv",
            "Ning Ding",
            "Yujia Qin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks. In this paper, we explore 5 parameter-efficient weight ensembling methods to achieve such transferability and verify the effectiveness of them. These methods extract the information of datasets and trained lightweight parameters from different perspectives to obtain the similarity between tasks, and weight the existing lightweight parameters according to the comparability to acquire a suitable module for the initialization of new tasks. We apply them to three parameter-efficient tuning methods and test them on a wide set of downstream tasks. Experimental results show that our methods show an improvement of 5%~8% over baselines and could largely facilitate task-level knowledge transfer.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.24.pdf"
    },
    {
        "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
        "authors": [
            "Himanshu Thakur",
            "Atishay Jain",
            "Praneetha Vaddamanu",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "published": "2023",
        "summary": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.30.pdf"
    },
    {
        "title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English",
        "authors": [
            "Jianfeng Chi",
            "Wasi Uddin Ahmad",
            "Yuan Tian",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at https://github.com/JFChi/PLUE.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.31.pdf"
    },
    {
        "title": "Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications",
        "authors": [
            "Jatin Arora",
            "Youngja Park"
        ],
        "published": "2023",
        "summary": "In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.36.pdf"
    },
    {
        "title": "Credible without Credit: Domain Experts Assess Generative Language Models",
        "authors": [
            "Denis Peskoff",
            "Brandon Stewart"
        ],
        "published": "2023",
        "summary": "Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy, and sourcing of two language models across 100 expert-written questions. While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.37.pdf"
    },
    {
        "title": "Efficient Diagnosis Assignment Using Unstructured Clinical Notes",
        "authors": [
            "Louis Blankemeier",
            "Jason Fries",
            "Robert Tinn",
            "Joseph Preston",
            "Nigam Shah",
            "Akshay Chaudhari"
        ],
        "published": "2023",
        "summary": "Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these challenges, we propose HyDE (hybrid diagnosis extractor). HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a disease-agnostic neural network to assign diagnoses to patients. By training HyDE\u2019s model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85. We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 - 0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average. Furthermore, we demonstrate a speedup of >4x by pruning the length of inputs into our language model to ~2.3% of the full clinical notes, with negligible impact to the AUPRC. HyDE has the potential to improve the efficiency and efficacy of interpreting large-scale unstructured clinical notes for accurate EHR phenotyping.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.42.pdf"
    },
    {
        "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
        "authors": [
            "Masoud Monajatipoor",
            "Liunian Harold Li",
            "Mozhdeh Rouhsedaghat",
            "Lin Yang",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.43.pdf"
    },
    {
        "title": "Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity",
        "authors": [
            "Hongwei Wang",
            "Dong Yu"
        ],
        "published": "2023",
        "summary": "Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability. This paper presents a token-level matching inference algorithm, which can be applied on top of any language model to improve its performance on STS task. Our method calculates pairwise token-level similarity and token matching scores, and then aggregates them with pretrained token weights to produce sentence similarity. Experimental results on seven STS datasets show that our method improves the performance of almost all language models, with up to 12.7% gain in Spearman\u2019s correlation. We also demonstrate that our method is highly explainable and computationally efficient.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.49.pdf"
    },
    {
        "title": "Probing Physical Reasoning with Counter-Commonsense Context",
        "authors": [
            "Kazushi Kondo",
            "Saku Sugawara",
            "Akiko Aizawa"
        ],
        "published": "2023",
        "summary": "In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates. We measure the ability of several masked language models and encoder-decoder models. The results show that while large language models can use prepositions such as \u201cin\u201d and \u201cinto\u201d in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.53.pdf"
    },
    {
        "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
        "authors": [
            "Yahan Yang",
            "Soham Dan",
            "Dan Roth",
            "Insup Lee"
        ],
        "published": "2023",
        "summary": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.58.pdf"
    },
    {
        "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
        "authors": [
            "Amirhossein Abaskohi",
            "Sascha Rothe",
            "Yadollah Yaghoobzadeh"
        ],
        "published": "2023",
        "summary": "In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, especially large language models such as GPT-3 and OPT-175B, for data augmentation. Our experiments on multiple text classification benchmarks show that this augmentation method outperforms other methods, such as easy data augmentation, back translation, and multiple templates.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.59.pdf"
    },
    {
        "title": "Exploring Continual Learning for Code Generation Models",
        "authors": [
            "Prateek Yadav",
            "Qing Sun",
            "Hantian Ding",
            "Xiaopeng Li",
            "Dejiao Zhang",
            "Ming Tan",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Ramesh Nallapati",
            "Murali Krishna Ramanathan",
            "Mohit Bansal",
            "Bing Xiang"
        ],
        "published": "2023",
        "summary": "Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.68.pdf"
    },
    {
        "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
        "authors": [
            "Jiaxuan Li",
            "Lang Yu",
            "Allyson Ettinger"
        ],
        "published": "2023",
        "summary": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.70.pdf"
    },
    {
        "title": "Gradient Ascent Post-training Enhances Language Model Generalization",
        "authors": [
            "Dongkeun Yoon",
            "Joel Jang",
            "Sungdong Kim",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks. We also show that applying GAP on out-of-distribution corpora leads to the most reliable performance improvements. Our findings indicate that GAP can be a promising method for improving the generalization capability of LMs without any task-specific fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.74.pdf"
    },
    {
        "title": "A Better Way to Do Masked Language Model Scoring",
        "authors": [
            "Carina Kauf",
            "Anna Ivanova"
        ],
        "published": "2023",
        "summary": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models. Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.80.pdf"
    },
    {
        "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
        "authors": [
            "Michael Heck",
            "Nurul Lubis",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Shutong Feng",
            "Christian Geishauser",
            "Hsien-chin Lin",
            "Carel van Niekerk",
            "Milica Gasic"
        ],
        "published": "2023",
        "summary": "Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.81.pdf"
    },
    {
        "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
        "authors": [
            "Maximillian Chen",
            "Xiao Yu",
            "Weiyan Shi",
            "Urvi Awasthi",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.82.pdf"
    },
    {
        "title": "Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement",
        "authors": [
            "Samuel Mensah",
            "Kai Sun",
            "Nikolaos Aletras"
        ],
        "published": "2023",
        "summary": "State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to be effective at representing rare words or words with insufficient context information. To address this issue, this work trades syntax trees for BERT wordpieces by entirely removing the GCN component from the methods\u2019 architectures. To enhance TOWE performance, we tackle the issue of aspect representation loss during encoding. Instead of solely utilizing a sentence as the input, we use a sentence-aspect pair. Our relatively simple approach achieves state-of-the-art results on benchmark datasets and should serve as a strong baseline for further research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.86.pdf"
    },
    {
        "title": "Do GPTs Produce Less Literal Translations?",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes",
            "Matt Post",
            "Hany Hassan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.90.pdf"
    },
    {
        "title": "Black-box language model explanation by context length probing",
        "authors": [
            "Ond\u0159ej C\u00edfka",
            "Antoine Liutkus"
        ],
        "published": "2023",
        "summary": "The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present *context length probing*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign *differential importance scores* to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The [source code](https://github.com/cifkao/context-probing/) and an [interactive demo](https://cifkao.github.io/context-probing/) of the method are available.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.92.pdf"
    },
    {
        "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
        "authors": [
            "Jianyi Zhang",
            "Aashiq Muhamed",
            "Aditya Anantharaman",
            "Guoyin Wang",
            "Changyou Chen",
            "Kai Zhong",
            "Qingjun Cui",
            "Yi Xu",
            "Belinda Zeng",
            "Trishul Chilimbi",
            "Yiran Chen"
        ],
        "published": "2023",
        "summary": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher\u2019s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.97.pdf"
    },
    {
        "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Li-Wei Chen",
            "Alexander Rudnicky",
            "Peter Ramadge"
        ],
        "published": "2023",
        "summary": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.102.pdf"
    },
    {
        "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning",
        "authors": [
            "Zhen-Ru Zhang",
            "Chuanqi Tan",
            "Haiyang Xu",
            "Chengyu Wang",
            "Jun Huang",
            "Songfang Huang"
        ],
        "published": "2023",
        "summary": "Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.107.pdf"
    },
    {
        "title": "Evaluating pragmatic abilities of image captioners on A3DS",
        "authors": [
            "Polina Tsvilodub",
            "Michael Franke"
        ],
        "published": "2023",
        "summary": "Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans. To enable such evaluation, we present a novel open source image-text dataset \u201cAnnotated 3D Shapes\u201d (A3DS) comprising over nine million exhaustive natural language annotations and over 12 million variable-granularity captions for the 480,000 images provided by Burgess & Kim (2018).We showcase the evaluation of pragmatic abilities developed by a task-neutral image captioner fine-tuned in a multi-agent communication setting to produce contrastive captions. The evaluation is enabled by the dataset because the exhaustive annotations allow to quantify the presence of contrastive features in the model\u2019s generations. We show that the model develops human-like patterns (informativity, brevity, over-informativity for specific features (e.g., shape, color biases)).",
        "pdf_link": "https://aclanthology.org/2023.acl-short.110.pdf"
    },
    {
        "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",
        "authors": [
            "Chantal Shaib",
            "Millicent Li",
            "Sebastian Joseph",
            "Iain Marshall",
            "Junyi Jessy Li",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Large language models, particularly GPT-3, are able to produce high quality summaries ofgeneral domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine. In this paper we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given no supervision. We consider bothsingle- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in thelatter, we assess the degree to which GPT-3 is able to synthesize evidence reported acrossa collection of articles. We design an annotation scheme for evaluating model outputs, withan emphasis on assessing the factual accuracy of generated summaries. We find that whileGPT-3 is able to summarize and simplify single biomedical articles faithfully, it strugglesto provide accurate aggregations of findings over multiple documents. We release all data,code, and annotations used in this work.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.119.pdf"
    },
    {
        "title": "Hexatagging: Projective Dependency Parsing as Tagging",
        "authors": [
            "Afra Amini",
            "Tianyu Liu",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser\u2019s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.124.pdf"
    },
    {
        "title": "Discourse-Level Representations can Improve Prediction of Degree of Anxiety",
        "authors": [
            "Swanie Juhng",
            "Matthew Matero",
            "Vasudha Varadarajan",
            "Johannes Eichstaedt",
            "Adithya V Ganesan",
            "H. Andrew Schwartz"
        ],
        "published": "2023",
        "summary": "Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourse-level information in addition to lexical-level large language model embeddings. We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations. Interpreting the model, we find that discourse patterns of causal explanations, among others, were used significantly more by those scoring high in anxiety, dovetailing with psychological literature.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.128.pdf"
    },
    {
        "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
        "authors": [
            "Mustafa Ozdayi",
            "Charith Peris",
            "Jack FitzGerald",
            "Christophe Dupuy",
            "Jimit Majmudar",
            "Haidar Khan",
            "Rahil Parikh",
            "Rahul Gupta"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.129.pdf"
    },
    {
        "title": "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting",
        "authors": [
            "Tatsuro Inaba",
            "Hirokazu Kiyomaru",
            "Fei Cheng",
            "Sadao Kurohashi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.130.pdf"
    },
    {
        "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
        "authors": [
            "Zequn Liu",
            "Wei Zhang",
            "Yingce Xia",
            "Lijun Wu",
            "Shufang Xie",
            "Tao Qin",
            "Ming Zhang",
            "Tie-Yan Liu"
        ],
        "published": "2023",
        "summary": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.138.pdf"
    },
    {
        "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
        "authors": [
            "Haoxin Li",
            "Phillip Keung",
            "Daniel Cheng",
            "Jungo Kasai",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.146.pdf"
    },
    {
        "title": "S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
        "authors": [
            "Fangyu Lei",
            "Xiang Li",
            "Yifan Wei",
            "Shizhu He",
            "Yiming Huang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.147.pdf"
    },
    {
        "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models",
        "authors": [
            "Siheng Li",
            "Cheng Yang",
            "Yichun Yin",
            "Xinyu Zhu",
            "Zesen Cheng",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.149.pdf"
    },
    {
        "title": "Teaching Small Language Models to Reason",
        "authors": [
            "Lucie Charlotte Magister",
            "Jonathan Mallinson",
            "Jakub Adamek",
            "Eric Malmi",
            "Aliaksei Severyn"
        ],
        "published": "2023",
        "summary": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.151.pdf"
    },
    {
        "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification",
        "authors": [
            "Rohan Bhambhoria",
            "Lei Chen",
            "Xiaodan Zhu"
        ],
        "published": "2023",
        "summary": "In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.152.pdf"
    },
    {
        "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
        "authors": [
            "Yulin Zhou",
            "Yiren Zhao",
            "Ilia Shumailov",
            "Robert Mullins",
            "Yarin Gal"
        ],
        "published": "2023",
        "summary": "Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompting. Our work suggests that, in addition to fine-tuning, manual prompting should be used as a baseline in this line of research.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.155.pdf"
    },
    {
        "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives",
        "authors": [
            "Xinpeng Wang",
            "Leonie Weissweiler",
            "Hinrich Sch\u00fctze",
            "Barbara Plank"
        ],
        "published": "2023",
        "summary": "Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.157.pdf"
    },
    {
        "title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification",
        "authors": [
            "Yu-Chen Lin",
            "Si-An Chen",
            "Jie-Jyun Liu",
            "Chih-Jen Lin"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.160.pdf"
    },
    {
        "title": "Human-in-the-loop Schema Induction",
        "authors": [
            "Tianyi Zhang",
            "Isaac Tham",
            "Zhaoyi Hou",
            "Jiaxuan Ren",
            "Leon Zhou",
            "Hainiu Xu",
            "Li Zhang",
            "Lara J. Martin",
            "Rotem Dror",
            "Sha Li",
            "Heng Ji",
            "Martha Palmer",
            "Susan Windisch Brown",
            "Reece Suchocki",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.1.pdf"
    },
    {
        "title": "Lingxi: A Diversity-aware Chinese Modern Poetry Generation System",
        "authors": [
            "Xinran Zhang",
            "Maosong Sun",
            "Jiafeng Liu",
            "Xiaobing Li"
        ],
        "published": "2023",
        "summary": "Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose a novel framework that incorporates CWS in the tokenization process. The proposed method can achieve a high vocabulary coverage rate with a reasonable vocabulary size. For the decoding method and the diversity issue, we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry. Empirical results show that even when the top 60% of cumulative probability mass of the predicted distribution is flattened, our method achieves comparable or even better performance than baseline sampling methods. Our system is available at http://lingxi.website.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.6.pdf"
    },
    {
        "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
        "authors": [
            "Victor Dibia"
        ],
        "published": "2023",
        "summary": "Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.11.pdf"
    },
    {
        "title": "The ROOTS Search Tool: Data Transparency for LLMs",
        "authors": [
            "Aleksandra Piktus",
            "Christopher Akiki",
            "Paulo Villegas",
            "Hugo Lauren\u00e7on",
            "G\u00e9rard Dupont",
            "Sasha Luccioni",
            "Yacine Jernite",
            "Anna Rogers"
        ],
        "published": "2023",
        "summary": "ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces: https://huggingface.co/spaces/bigscience-data/roots-search. We describe our implementation and the possible use cases of our tool.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.29.pdf"
    },
    {
        "title": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
        "authors": [
            "Gabriele Sarti",
            "Nils Feldhus",
            "Ludwig Sickert",
            "Oskar van der Wal"
        ],
        "published": "2023",
        "summary": "Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models\u2019 internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.40.pdf"
    },
    {
        "title": "Pipeline for modeling causal beliefs from natural language",
        "authors": [
            "John Priniski",
            "Ishaan Verma",
            "Fred Morstatter"
        ],
        "published": "2023",
        "summary": "We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.41.pdf"
    },
    {
        "title": "Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals",
        "authors": [
            "Robin Chan",
            "Afra Amini",
            "Mennatallah El-Assady"
        ],
        "published": "2023",
        "summary": "We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models\u2019 robustness by creating adversarial test suites.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.44.pdf"
    },
    {
        "title": "OpenICL: An Open-Source Framework for In-context Learning",
        "authors": [
            "Zhenyu Wu",
            "Yaoxiang Wang",
            "Jiacheng Ye",
            "Zhiyong Wu",
            "Jiangtao Feng",
            "Jingjing Xu",
            "Yu Qiao"
        ],
        "published": "2023",
        "summary": "In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.47.pdf"
    },
    {
        "title": "A System for Answering Simple Questions in Multiple Languages",
        "authors": [
            "Anton Razzhigaev",
            "Mikhail Salnikov",
            "Valentin Malykh",
            "Pavel Braslavski",
            "Alexander Panchenko"
        ],
        "published": "2023",
        "summary": "Our research focuses on the most prevalent type of queries\u2014 simple questions \u2014exemplified by questions like \u201cWhat is the capital of France?\u201d. These questions reference an entity such as \u201cFrance\u201d, which is directly connected (one hop) to the answer entity \u201cParis\u201d in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question\u2019s text embeddings and the answer\u2019s graph embeddings. A system incorporating this novel method is also described in our work. Through comprehensive experimentation using various English and multilingual datasets and two KGs \u2014 Freebase and Wikidata \u2014 we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.51.pdf"
    },
    {
        "title": "Disease Network Constructor: a Pathway Extraction and Visualization",
        "authors": [
            "Mohammad Golam Sohrab",
            "Khoa Duong",
            "Goran Topi\u0107",
            "Masami Ikeda",
            "Nozomi Nagano",
            "Yayoi Natsume-Kitatani",
            "Masakata Kuroda",
            "Mari Itoh",
            "Hiroya Takamura"
        ],
        "published": "2023",
        "summary": "We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC\u2019s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from https://youtu.be/rFhWwAgcXE8. We release an online system for end users and the source code is available at https://github.com/aistairc/PRISM-APIs/.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.53.pdf"
    },
    {
        "title": "Petals: Collaborative Inference and Fine-tuning of Large Models",
        "authors": [
            "Alexander Borzunov",
            "Dmitry Baranchuk",
            "Tim Dettmers",
            "Maksim Riabinin",
            "Younes Belkada",
            "Artem Chumachenko",
            "Pavel Samygin",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with \u22481 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.54.pdf"
    },
    {
        "title": "DeepPavlov Dream: Platform for Building Generative AI Assistants",
        "authors": [
            "Diliara Zharikova",
            "Daniel Kornev",
            "Fedor Ignatov",
            "Maxim Talimanchuk",
            "Dmitry Evseev",
            "Ksenya Petukhova",
            "Veronika Smilga",
            "Dmitry Karpov",
            "Yana Shishkina",
            "Dmitry Kosenko",
            "Mikhail Burtsev"
        ],
        "published": "2023",
        "summary": "An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants.",
        "pdf_link": "https://aclanthology.org/2023.acl-demo.58.pdf"
    },
    {
        "title": "ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer",
        "authors": [
            "Dongqi Pu",
            "Vera Demberg"
        ],
        "published": "2023",
        "summary": "Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT\u2019s performance in two controllable generation tasks, with respect to ChatGPT\u2019s ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model\u2019s performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.1.pdf"
    },
    {
        "title": "The Turing Quest: Can Transformers Make Good NPCs?",
        "authors": [
            "Qi Chen Gao",
            "Ali Emami"
        ],
        "published": "2023",
        "summary": "In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of repetition. Finally, we propose a new benchmark, called The Turing Quest, which we use to show that the pipeline, when applied to GPT-3, can generate for a variety of game genres and contexts, NPC scripts that can fool judges in thinking they have been written by humans. We believe that these findings can greatly benefit both the gaming industry and its global community of users, since many current games continue to base their NPCs on manually-curated scripts that are resource-demanding and may curb the immersiveness and enjoyment of the user.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.17.pdf"
    },
    {
        "title": "Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section",
        "authors": [
            "Hongyi Zheng",
            "Yixin Zhu",
            "Lavender Jiang",
            "Kyunghyun Cho",
            "Eric Oermann"
        ],
        "published": "2023",
        "summary": "Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.18.pdf"
    },
    {
        "title": "MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries",
        "authors": [
            "Yang Cui",
            "Lifeng Han",
            "Goran Nenadic"
        ],
        "published": "2023",
        "summary": "Discharge summaries are comprehensive medical records that encompass vital information about a patient\u2019s hospital stay. A crucial aspect of discharge summaries is the temporal information of treatments administered throughout the patient\u2019s illness. With an extensive volume of clinical documents, manually extracting and compiling a patient\u2019s medication list can be laborious, time-consuming, and susceptible to errors. The objective of this paper is to build upon the recent development on clinical NLP by temporally classifying treatments in clinical texts, specifically determining whether a treatment was administered between the time of admission and discharge from the hospital. State-of-the-art NLP methods including prompt-based learning on Generative Pre-trained Transformers (GPTs) models and fine-tuning on pre-trained language models (PLMs) such as BERT were employed to classify temporal relations between treatments and hospitalisation periods in discharge summaries. Fine-tuning with the BERT model achieved an F1 score of 92.45% and a balanced accuracy of 77.56%, while prompt learning using the T5 model and mixed templates resulted in an F1 score of 90.89% and a balanced accuracy of 72.07%.Our codes and data are available at https://github.com/HECTA-UoM/MedTem.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.27.pdf"
    },
    {
        "title": "Building a Buzzer-quiz Answering System",
        "authors": [
            "Naoya Sugiura",
            "Kosuke Yamada",
            "Ryohei Sasano",
            "Koichi Takeda",
            "Katsuhiko Toyama"
        ],
        "published": "2023",
        "summary": "A buzzer quiz is a genre of quiz in which multiple players simultaneously listen to a quiz being read aloud and respond it by buzzing in as soon as they can predict the answer. Because incorrect answers often result in penalties, a buzzer-quiz answering system must not only predict the answer from only part of a question but also estimate the predicted answer\u2019s accuracy. In this paper, we introduce two types of buzzer-quiz answering systems: (1) a system that directly generates an answer from part of a question by using an autoregressive language model; and (2) a system that first reconstructs the entire question by using an autoregressive language model and then determines the answer according to the reconstructed question. We then propose a method to estimate the accuracy of the answers for each system by using the internal scores of each model.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.29.pdf"
    },
    {
        "title": "Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference",
        "authors": [
            "Chong Shen",
            "Carina Silberer"
        ],
        "published": "2023",
        "summary": "Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal\u2013Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best existing methods encode texts and images either with independent encoders, or with object-level multimodal encoders using blackbox transformers. This stands in contrast to early, linguistically inspired methods for event representations, which focus on capturing the most crucial information, namely actions and the participants, to learn stereotypical event sequences and hence procedural knowledge. In this work, we study various methods and their effects on PKU of injecting the early shallow event representations to nowadays multimodal deep learning-based models. We find that the early, linguistically inspired methods for representing event knowledge does contribute to understand procedures in combination with modern vision-and-language models. In the future, we are going to explore more complex structure of events and study how to exploit it on top of large language models.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.36.pdf"
    },
    {
        "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
        "authors": [
            "Stephanie Schoch",
            "Ritwick Mishra",
            "Yangfeng Ji"
        ],
        "published": "2023",
        "summary": "Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the entire training set, and 2) a value transfer method that leverages value information extracted from a simple classifier trained using representations from the target language model. Our experiments applying TS-DShapley to select data for fine-tuning BERT-based language models on benchmark natural language understanding (NLU) datasets show that TS-DShapley outperforms existing data selection methods. Further, TS-DShapley can filter fine-tuning data to increase language model performance compared to training with the full fine-tuning dataset.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.37.pdf"
    },
    {
        "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
        "authors": [
            "Gabriel Simmons"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.40.pdf"
    },
    {
        "title": "LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism",
        "authors": [
            "Jingfan Zhang",
            "Ming Tan",
            "Pengyu Dai",
            "Wei Zhu"
        ],
        "published": "2023",
        "summary": "Recently, dynamic early exiting has attracted much attention since it can accelerate the inference speed of pre-trained models (PTMs). However, previous work on early exiting has neglected the intermediate exits\u2019 architectural designs. In this work, we propose a novel framework, Learned Exits and COmparison-based early exiting (LECO) to improve PTMs\u2019 early exiting performances. First, to fully uncover the potentials of multi-exit BERT, we design a novel search space for intermediate exits and employ the idea of differentiable neural architecture search (DNAS) to design proper exit architectures for different intermediate layers automatically. Second, we propose a simple-yet-effective comparison-based early exiting mechanism (COBEE), which can help PTMs achieve better performance and speedup tradeoffs. Extensive experiments show that our LECO achieves the SOTA performances for multi-exit BERT training and dynamic early exiting.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.43.pdf"
    },
    {
        "title": "Authorship Attribution of Late 19th Century Novels using GAN-BERT",
        "authors": [
            "Kanishka Silva",
            "Burcu Can",
            "Fr\u00e9d\u00e9ric Blain",
            "Raheem Sarwar",
            "Laura Ugolini",
            "Ruslan Mitkov"
        ],
        "published": "2023",
        "summary": "Authorship attribution aims to identify the author of an anonymous text. The task becomes even more worthwhile when it comes to literary works. For example, pen names were commonly used by female authors in the 19th century resulting in some literary works being incorrectly attributed or claimed. With this motivation, we collated a dataset of late 19th century novels in English. Due to the imbalance in the dataset and the unavailability of enough data per author, we employed the GANBERT model along with data sampling strategies to fine-tune a transformer-based model for authorship attribution. Differently from the earlier studies on the GAN-BERT model, we conducted transfer learning on comparatively smaller author subsets to train more focused author-specific models yielding performance over 0.88 accuracy and F1 scores. Furthermore, we observed that increasing the sample size has a negative impact on the model\u2019s performance. Our research mainly contributes to the ongoing authorship attribution research using GAN-BERT architecture, especially in attributing disputed novelists in the late 19th century.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.44.pdf"
    },
    {
        "title": "Semantic Accuracy in Natural Language Generation: A Thesis Proposal",
        "authors": [
            "Patricia Schmidtova"
        ],
        "published": "2023",
        "summary": "With the fast-growing popularity of current large pre-trained language models (LLMs), it is necessary to dedicate efforts to making them more reliable. In this thesis proposal, we aim to improve the reliability of natural language generation systems (NLG) by researching the semantic accuracy of their outputs. We look at this problem from the outside (evaluation) and from the inside (interpretability). We propose a novel method for evaluating semantic accuracy and discuss the importance of working towards a unified and objective benchmark for NLG metrics. We also review interpretability approaches which could help us pinpoint the sources of inaccuracies within the models and explore potential mitigation strategies.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.48.pdf"
    },
    {
        "title": "Math Word Problem Solving by Generating Linguistic Variants of Problem Statements",
        "authors": [
            "Syed Rifat Raiyan",
            "Md Nafis Faiyaz",
            "Shah Md. Jawad Kabir",
            "Mohsinul Kabir",
            "Hasan Mahmud",
            "Md Kamrul Hasan"
        ],
        "published": "2023",
        "summary": "The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) \u2014 a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, ParaMAWPS, consisting of paraphrased, adversarial, and inverse variants of selectively sampled MWPs from the benchmark Mawps dataset. We extensively experiment on this dataset along with other benchmark datasets using some baseline MWP solver models. We show that training on linguistic variants of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model. We make our code and data publicly available.",
        "pdf_link": "https://aclanthology.org/2023.acl-srw.49.pdf"
    },
    {
        "title": "CWSeg: An Efficient and General Approach to Chinese Word Segmentation",
        "authors": [
            "Dedong Li",
            "Rui Zhao",
            "Fei Tan"
        ],
        "published": "2023",
        "summary": "In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.1.pdf"
    },
    {
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
        "authors": [
            "Shima Imani",
            "Liang Du",
            "Harsh Shrivastava"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.4.pdf"
    },
    {
        "title": "Label efficient semi-supervised conversational intent classification",
        "authors": [
            "Mandar Kulkarni",
            "Kyung Kim",
            "Nikesh Garera",
            "Anusua Trivedi"
        ],
        "published": "2023",
        "summary": "To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user\u2019s intention from the query text. Due to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. This paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. Experimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.11.pdf"
    },
    {
        "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023",
        "summary": "Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.15.pdf"
    },
    {
        "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Gunhee Kim",
            "Jung-woo Ha"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.21.pdf"
    },
    {
        "title": "The economic trade-offs of large language models: A case study",
        "authors": [
            "Kristen Howell",
            "Gwen Christian",
            "Pavel Fomitchov",
            "Gitit Kehat",
            "Julianne Marzulla",
            "Leanne Rolston",
            "Jadin Tredup",
            "Ilana Zimmerman",
            "Ethan Selfridge",
            "Joseph Bradley"
        ],
        "published": "2023",
        "summary": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model\u2019s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM \u2014 prompt engineering, fine-tuning, and knowledge distillation \u2014 using feedback from the brand\u2019s customer service agents. We find that the usability of a model\u2019s responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.24.pdf"
    },
    {
        "title": "Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy",
        "authors": [
            "Zekai Chen",
            "Mariann Micsinai Balan",
            "Kevin Brown"
        ],
        "published": "2023",
        "summary": "Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients\u2019 clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.32.pdf"
    },
    {
        "title": "A Static Evaluation of Code Completion by Large Language Models",
        "authors": [
            "Hantian Ding",
            "Varun Kumar",
            "Yuchen Tian",
            "Zijian Wang",
            "Rob Kwiatkowski",
            "Xiaopeng Li",
            "Murali Krishna Ramanathan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Sudipta Sengupta"
        ],
        "published": "2023",
        "summary": "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven\u2019t been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.34.pdf"
    },
    {
        "title": "SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels",
        "authors": [
            "Zhenting Qi",
            "Xiaoyu Tan",
            "Chao Qu",
            "Yinghui Xu",
            "Yuan Qi"
        ],
        "published": "2023",
        "summary": "Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.38.pdf"
    },
    {
        "title": "Chemical Language Understanding Benchmark",
        "authors": [
            "Yunsoo Kim",
            "Hyuk Ko",
            "Jane Lee",
            "Hyun Young Heo",
            "Jinyoung Yang",
            "Sungsoo Lee",
            "Kyu-hwang Lee"
        ],
        "published": "2023",
        "summary": "In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.39.pdf"
    },
    {
        "title": "HyperT5: Towards Compute-Efficient Korean Language Modeling",
        "authors": [
            "Dongju Park",
            "Soonwon Ka",
            "Kang Min Yoo",
            "Gichang Lee",
            "Jaewook Kang"
        ],
        "published": "2023",
        "summary": "Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.40.pdf"
    },
    {
        "title": "Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",
        "authors": [
            "Jonathan Hueser",
            "Judith Gaspers",
            "Thomas Gueudre",
            "Chandana Prakash",
            "Jin Cao",
            "Daniil Sorokin",
            "Quynh Do",
            "Nicolas Anastassacos",
            "Tobias Falke",
            "Turan Gojayev"
        ],
        "published": "2023",
        "summary": "Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks. Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).We evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT encoders with language-, domain- and task-decoupled finetuning.Running inference with a larger shared encoder on GPU is latency neutral and reduces infrastructure cost compared to running inference for decoupled smaller encoders on CPU machines. The larger shared encoder reduces semantic error rates by 4.62% for test sets representing user requests to voice-controlled devices and 5.79% on the tail of the test sets on average across four languages.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.43.pdf"
    },
    {
        "title": "DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction",
        "authors": [
            "Paul Darm",
            "Antonio Valerio Miceli Barone",
            "Shay B. Cohen",
            "Annalisa Riccardi"
        ],
        "published": "2023",
        "summary": "Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge Base (KB) databases are an effective way of storing and accessing such information to scale. In this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.47.pdf"
    },
    {
        "title": "BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting",
        "authors": [
            "Wei Zhu",
            "Peng Wang",
            "Yuan Ni",
            "Guotong Xie",
            "Xiaoling Wang"
        ],
        "published": "2023",
        "summary": "Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs\u2019 early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers\u2019 exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\footnote{Code will be made publicly available to the research community upon acceptance.}",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.48.pdf"
    },
    {
        "title": "Evaluating Embedding APIs for Information Retrieval",
        "authors": [
            "Ehsan Kamalloo",
            "Xinyu Zhang",
            "Odunayo Ogundepo",
            "Nandan Thakur",
            "David Alfonso-hermelo",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023",
        "summary": "The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.50.pdf"
    },
    {
        "title": "RadLing: Towards Efficient Radiology Report Understanding",
        "authors": [
            "Rikhiya Ghosh",
            "Oladimeji Farri",
            "Sanjeev Kumar Karn",
            "Manuela Danu",
            "Ramya Vunikili",
            "Larisa Micu"
        ],
        "published": "2023",
        "summary": "Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.61.pdf"
    },
    {
        "title": "Exploring Zero and Few-shot Techniques for Intent Classification",
        "authors": [
            "Soham Parikh",
            "Mitul Tiwari",
            "Prashil Tumbade",
            "Quaizar Vohra"
        ],
        "published": "2023",
        "summary": "Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.71.pdf"
    },
    {
        "title": "Weakly supervised hierarchical multi-task classification of customer questions",
        "authors": [
            "Jitenkumar Rana",
            "Promod Yenigalla",
            "Chetan Aggarwal",
            "Sandeep Sricharan Mukku",
            "Manan Soni",
            "Rashmi Patange"
        ],
        "published": "2023",
        "summary": "Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification Framework (HMCF) to identify topics from customer questions at various granularities. Complexity lies in creating a list of granular topics (taxonomy) for 1000s of product categories and building a scalable classification system. To this end, we introduce a clustering based Taxonomy Creation and Data Labeling (TCDL) module for creating taxonomy and labelled data with minimal supervision. Using TCDL module, taxonomy and labelled data creation task reduces to 2 hours as compared to 2 weeks of manual efforts by a subject matter expert. For classification, we propose a two level HMCF that performs multi-class classification to identify coarse level-1 topic and leverages NLI based label-aware approach to identify granular level-2 topic. We showcase that HMCF (based on BERT and NLI) a) achieves absolute improvement of 13% in Top-1 accuracy over single-task non-hierarchical baselines b) learns a generic domain invariant function that can adapt to constantly evolving taxonomy (open label set) without need of re-training. c) reduces model deployment efforts significantly since it needs only one model that caters to 1000s of product categories.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.75.pdf"
    },
    {
        "title": "Complex Reasoning in Natural Language",
        "authors": [
            "Wenting Zhao",
            "Mor Geva",
            "Bill Yuchen Lin",
            "Michihiro Yasunaga",
            "Aman Madaan",
            "Tao Yu"
        ],
        "published": "2023",
        "summary": "Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area. We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.",
        "pdf_link": "https://aclanthology.org/2023.acl-tutorials.2.pdf"
    },
    {
        "title": "Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",
        "authors": [
            "Sunayana Sitaram",
            "Monojit Choudhury",
            "Barun Patra",
            "Vishrav Chaudhary",
            "Kabir Ahuja",
            "Kalika Bali"
        ],
        "published": "2023",
        "summary": "This tutorial will describe various aspects of scaling up language technologies to many of the world\u2019s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.",
        "pdf_link": "https://aclanthology.org/2023.acl-tutorials.3.pdf"
    },
    {
        "title": "Generating Text from Language Models",
        "authors": [
            "Afra Amini",
            "Ryan Cotterell",
            "John Hewitt",
            "Luca Malagutti",
            "Clara Meister",
            "Tiago Pimentel"
        ],
        "published": "2023",
        "summary": "An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms\u2014like top-p sampling or beam search\u2014which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.",
        "pdf_link": "https://aclanthology.org/2023.acl-tutorials.4.pdf"
    }
]