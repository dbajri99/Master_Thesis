[
    {
        "title": "The Geometry of Multilingual Language Model Representations",
        "authors": [
            "Tyler Chang",
            "Zhuowen Tu",
            "Benjamin Bergen"
        ],
        "published": "2022",
        "summary": "We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.9.pdf"
    },
    {
        "title": "Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment",
        "authors": [
            "Yechun Tang",
            "Xiaoxia Cheng",
            "Weiming Lu"
        ],
        "published": "2022",
        "summary": "Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88% improvements in the F1 metric on CQA dataset. Our source code is available at https://github.com/TTTTTTTTy/ALCQA.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.10.pdf"
    },
    {
        "title": "RankGen: Improving Text Generation with Large Ranking Models",
        "authors": [
            "Kalpesh Krishna",
            "Yapei Chang",
            "John Wieting",
            "Mohit Iyyer"
        ],
        "published": "2022",
        "summary": "Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.15.pdf"
    },
    {
        "title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation",
        "authors": [
            "Jannis Bulian",
            "Christian Buck",
            "Wojciech Gajewski",
            "Benjamin B\u00f6rschinger",
            "Tal Schuster"
        ],
        "published": "2022",
        "summary": "The predictions of question answering (QA) systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match (EM) with predefined rules or with the token-level F1 measure.In this paper, we present the first systematic conceptual and data-driven analysis to examine the shortcomings of token-level equivalence measures.To this end, we define the asymmetric notion of answer equivalence (AE), accepting answers that are equivalent to or improve over the reference, and publish over 23k human judgements for candidates produced by multiple QA systems on SQuAD.Through a careful analysis of this data, we reveal and quantify several concrete limitations of the F1 measure, such as a false impression of graduality, or missing dependence on the question.Since collecting AE annotations for each evaluated model is expensive, we learn a BERT matching (BEM) measure to approximate this task. Being a simpler task than QA, we find BEM to provide significantly better AE approximations than F1, and to more accurately reflect the performance of systems.Finally, we demonstrate the practical utility of AE and BEM on the concrete application of minimal accurate prediction sets, reducing the number of required answers by up to X2.6.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.20.pdf"
    },
    {
        "title": "Prompting for Multimodal Hateful Meme Classification",
        "authors": [
            "Rui Cao",
            "Roy Ka-Wei Lee",
            "Wen-Haw Chong",
            "Jing Jiang"
        ],
        "published": "2022",
        "summary": "Hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. Ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. However, there is no known explicit external knowledge base that could provide such hate speech contextual information. To address this gap, we propose PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification. Specifically, we construct simple prompts and provide a few in-context examples to exploit the implicit knowledge in the pre-trained RoBERTa language model for hateful meme classification. We conduct extensive experiments on two publicly available hateful and offensive meme datasets. Our experiment results show that PromptHate is able to achieve a high AUC of 90.96, outperforming state-of-the-art baselines on the hateful meme classification task. We also perform fine-grain analyses and case studies on various prompt settings and demonstrate the effectiveness of the prompts on hateful meme classification.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.22.pdf"
    },
    {
        "title": "Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling",
        "authors": [
            "Peijie Jiang",
            "Dingkun Long",
            "Yanzhao Zhang",
            "Pengjun Xie",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "published": "2022",
        "summary": "Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary information. However, to ensure the quality of the lexicon, great human effort is always necessary, which has been generally ignored. In this work, we suggest unsupervised statistical boundary information instead, and propose an architecture to encode the information directly into pre-trained language models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature induction of Chinese sequence labeling tasks. Experimental results on ten benchmarks of Chinese sequence labeling demonstrate that BABERT can provide consistent improvements on all datasets. In addition, our method can complement previous supervised lexicon exploration, where further improvements can be achieved when integrated with external lexicon information.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.34.pdf"
    },
    {
        "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
        "authors": [
            "Shitao Xiao",
            "Zheng Liu",
            "Yingxia Shao",
            "Zhao Cao"
        ],
        "published": "2022",
        "summary": "Despite pre-training\u2019s progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs. 1) A novel MAE workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder\u2019s masked input; then, the original sentence is recovered based on the sentence embedding and the decoder\u2019s masked input via masked language modeling. 2) Asymmetric model structure, with a full-scale BERT like transformer as encoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios, with a moderate ratio for encoder: 15 30%, and an aggressive ratio for decoder: 50 70%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the SOTA performances on a wide range of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and pre-trained models are made publicly available at https://github.com/staoxiao/RetroMAE so as to inspire more interesting research.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.35.pdf"
    },
    {
        "title": "QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance",
        "authors": [
            "Xiaoqiang Wang",
            "Bang Liu",
            "Siliang Tang",
            "Lingfei Wu"
        ],
        "published": "2022",
        "summary": "Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly penalize a legitimate and reasonable candidate question when it (1) involves complicated reasoning with the context or (2) can be grounded by multiple evidences in the context.In this paper, we propose QRelScore, a context-aware Relevance evaluation metric for Question Generation.Based on off-the-shelf language models such as BERT and GPT2, QRelScore employs both word-level hierarchical matching and sentence-level prompt-based generation to cope with the complicated reasoning and diverse generation from multiple evidences, respectively.Compared with existing metrics, our experiments demonstrate that QRelScore is able to achieve a higher correlation with human judgments while being much more robust to adversarial samples.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.37.pdf"
    },
    {
        "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
        "authors": [
            "Tianbao Xie",
            "Chen Henry Wu",
            "Peng Shi",
            "Ruiqi Zhong",
            "Torsten Scholak",
            "Michihiro Yasunaga",
            "Chien-Sheng Wu",
            "Ming Zhong",
            "Pengcheng Yin",
            "Sida I. Wang",
            "Victor Zhong",
            "Bailin Wang",
            "Chengzu Li",
            "Connor Boyle",
            "Ansong Ni",
            "Ziyu Yao",
            "Dragomir Radev",
            "Caiming Xiong",
            "Lingpeng Kong",
            "Rui Zhang",
            "Noah A. Smith",
            "Luke Zettlemoyer",
            "Tao Yu"
        ],
        "published": "2022",
        "summary": "Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.39.pdf"
    },
    {
        "title": "Segmenting Numerical Substitution Ciphers",
        "authors": [
            "Nada Aldarrab",
            "Jonathan May"
        ],
        "published": "2022",
        "summary": "Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a challenging task. Segmentation (i.e. finding substitution units) is essential for cracking those ciphers. In this work, we propose the first automatic methods to segment those ciphers using Byte Pair Encoding (BPE) and unigram language models. Our methods achieve an average segmentation error of 2% on 100 randomly-generated monoalphabetic ciphers and 27% on 3 real historical homophonic ciphers. We also propose a method for solving non-deterministic ciphers with existing keys using a lattice and a pretrained language model. Our method leads to the full solution of the IA cipher; a real historical cipher that has not been fully solved until this work.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.44.pdf"
    },
    {
        "title": "Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning",
        "authors": [
            "Yifan Chen",
            "Devamanyu Hazarika",
            "Mahdi Namazifar",
            "Yang Liu",
            "Di Jin",
            "Dilek Hakkani-Tur"
        ],
        "published": "2022",
        "summary": "Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose to understand and further develop prefix-tuning through the kernel lens. Specifically, we make an analogy between prefixes and inducing variables in kernel methods and hypothesize that prefixes serving as inducing variables would improve their overall mechanism. From the kernel estimator perspective, we suggest a new variant of prefix-tuning\u2014inducer-tuning, which shares the exact mechanism as prefix-tuning while leveraging the residual form found in adapter-tuning. This mitigates the initialization issue in prefix-tuning. Through comprehensive empirical experiments on natural language understanding and generation tasks, we demonstrate that inducer-tuning can close the performance gap between prefix-tuning and fine-tuning.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.50.pdf"
    },
    {
        "title": "DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection",
        "authors": [
            "Puneet Mathur",
            "Gautam Kunapuli",
            "Riyaz Bhat",
            "Manish Shrivastava",
            "Dinesh Manocha",
            "Maneesh Singh"
        ],
        "published": "2022",
        "summary": "We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.51.pdf"
    },
    {
        "title": "Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization",
        "authors": [
            "Zexuan Qiu",
            "Qinliang Su",
            "Jianxing Yu",
            "Shijing Si"
        ],
        "published": "2022",
        "summary": "Efficient document retrieval heavily relies on the technique of semantic hashing, which learns a binary code for every document and employs Hamming distance to evaluate document distances. However, existing semantic hashing methods are mostly established on outdated TFIDF features, which obviously do not contain lots of important semantic information about documents. Furthermore, the Hamming distance can only be equal to one of several integer values, significantly limiting its representational ability for document distances. To address these issues, in this paper, we propose to leverage BERT embeddings to perform efficient retrieval based on the product quantization technique, which will assign for every document a real-valued codeword from the codebook, instead of a binary code as in semantic hashing. Specifically, we first transform the original BERT embeddings via a learnable mapping and feed the transformed embedding into a probabilistic product quantization module to output the assigned codeword. The refining and quantizing modules can be optimized in an end-to-end manner by minimizing the probabilistic contrastive loss. A mutual information maximization based method is further proposed to improve the representativeness of codewords, so that documents can be quantized more accurately. Extensive experiments conducted on three benchmarks demonstrate that our proposed method significantly outperforms current state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.54.pdf"
    },
    {
        "title": "Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models",
        "authors": [
            "Aniruddha Mahapatra",
            "Sharmila Reddy Nangi",
            "Aparna Garimella",
            "Anandhavelu N"
        ],
        "published": "2022",
        "summary": "Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using unlabeled data from target domains can help improve the performances of these language models on the downstream tasks. However, using all of the available unlabeled data for pretraining can be time-intensive; also, it can be detrimental to the performance of the downstream tasks, if the unlabeled data is not aligned with the data distribution for the target tasks. Previous works employed external supervision in the form of ontologies for selecting appropriate data samples for pretraining, but external supervision can be quite hard to obtain in low-resource domains. In this paper, we introduce effective ways to select data from unlabeled corpora of target domains for language model pretraining to improve the performances in target entity extraction tasks. Our data selection strategies do not require any external supervision. We conduct extensive experiments for the task of named entity recognition (NER) on seven different domains and show that language models pretrained on target domain unlabeled data obtained using our data selection strategies achieve better performances compared to those using data selection strategies in previous works that use external supervision. We also show that these pretrained language models using our data selection strategies outperform those pretrained on all of the available unlabeled target domain data.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.61.pdf"
    },
    {
        "title": "How Large Language Models are Transforming Machine-Paraphrase Plagiarism",
        "authors": [
            "Jan Philip Wahle",
            "Terry Ruas",
            "Frederic Kirstein",
            "Bela Gipp"
        ],
        "published": "2022",
        "summary": "The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their detection is still incipient in the literature.This work explores T5 and GPT3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia.We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples.Our results suggest that large language models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.).Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).The best-performing detection model (GPT-3) achieves 66% F1-score in detecting paraphrases.We make our code, data, and findings publicly available to facilitate the development of detection solutions.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.62.pdf"
    },
    {
        "title": "\u201cWill You Find These Shortcuts?\u201d A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification",
        "authors": [
            "Jasmijn Bastings",
            "Sebastian Ebert",
            "Polina Zablotskaia",
            "Anders Sandholm",
            "Katja Filippova"
        ],
        "published": "2022",
        "summary": "Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model\u2019s prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared.Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and lexical shortcuts for BERT and LSTM models. We demonstrate that some of the most popular method configurations provide poor results even for simple shortcuts while a method judged to be too simplistic works remarkably well for BERT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.64.pdf"
    },
    {
        "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation",
        "authors": [
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2022",
        "summary": "We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many advantages of AR generation, it usually suffers from inefficient inference. Therefore, non-autoregressive (NAR) models are proposed to generate all target tokens simultaneously. However, NAR models usually generate texts of lower quality due to the absence of token dependency in the output text. In this paper, we propose ELMER: an efficient and effective PLM for NAR text generation to explicitly model the token dependency during NAR generation. By leveraging the early exit technique, ELMER enables the token generations at different layers, according to their prediction confidence (a more confident token will exit at a lower layer). Besides, we propose a novel pre-training objective, Layer Permutation Language Modeling, to pre-train ELMER by permuting the exit layer for each token in sequences. Experiments on three text generation tasks show that ELMER significantly outperforms NAR models and further narrows the performance gap with AR PLMs (ELMER (29.92) vs BART (30.61) ROUGE-L in XSUM) while achieving over 10 times inference speedup.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.68.pdf"
    },
    {
        "title": "Fine-grained Contrastive Learning for Relation Extraction",
        "authors": [
            "William Hogan",
            "Jiacheng Li",
            "Jingbo Shang"
        ],
        "published": "2022",
        "summary": "Recent relation extraction (RE) works have shown encouraging improvements by conducting contrastive learning on silver labels generated by distant supervision before fine-tuning on gold labels. Existing methods typically assume all these silver labels are accurate and treat them equally; however, distant supervision is inevitably noisy\u2013some silver labels are more reliable than others. In this paper, we propose fine-grained contrastive learning (FineCL) for RE, which leverages fine-grained information about which silver labels are and are not noisy to improve the quality of learned relationship representations for RE. We first assess the quality of silver labels via a simple and automatic approach we call \u201clearning order denoising,\u201d where we train a language model to learn these relations and record the order of learned training instances. We show that learning order largely corresponds to label accuracy\u2013early-learned silver labels have, on average, more accurate labels than later-learned silver labels. Then, during pre-training, we increase the weights of accurate labels within a novel contrastive learning objective. Experiments on several RE benchmarks show that FineCL makes consistent and significant performance gains over state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.71.pdf"
    },
    {
        "title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
        "authors": [
            "Yue Yang",
            "Wenlin Yao",
            "Hongming Zhang",
            "Xiaoyang Wang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2022",
        "summary": "Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., \u201dan orange is orange\u201d. To overcome this limitation, we develop a novel approach, Z-LaVI, to endow language models with visual imagination capabilities. Specifically, we leverage two complementary types of \u201dimaginations\u201d: (i) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-to-image generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks. Notably, fueling language models with imagination can effectively leverage visual knowledge to solve plain language tasks. In consequence, Z-LaVI consistently improves the zero-shot performance of existing language models across a diverse set of language tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.78.pdf"
    },
    {
        "title": "Using Commonsense Knowledge to Answer Why-Questions",
        "authors": [
            "Yash Kumar Lal",
            "Niket Tandon",
            "Tanvi Aggarwal",
            "Horace Liu",
            "Nathanael Chambers",
            "Raymond Mooney",
            "Niranjan Balasubramanian"
        ],
        "published": "2022",
        "summary": "Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TellMeWhy dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 and GPT3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models. Injecting external knowledge helps models of various sizes, but the amount of improvement decreases with larger model size. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.79.pdf"
    },
    {
        "title": "Successive Prompting for Decomposing Complex Questions",
        "authors": [
            "Dheeru Dua",
            "Shivanshu Gupta",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "published": "2022",
        "summary": "Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce \u201cSuccessive Prompting\u201d where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model\u2019s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.81.pdf"
    },
    {
        "title": "Language Models of Code are Few-Shot Commonsense Learners",
        "authors": [
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Yiming Yang",
            "Graham Neubig"
        ],
        "published": "2022",
        "summary": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches \u2018serialize\u2019 the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.90.pdf"
    },
    {
        "title": "Generative Multi-hop Retrieval",
        "authors": [
            "Hyunji Lee",
            "Sohee Yang",
            "Hanseok Oh",
            "Minjoon Seo"
        ],
        "published": "2022",
        "summary": "A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so that it can retrieve different documents at each hop. However, such a bi-encoder approach has limitations in multi-hop settings; (1) the reformulated query gets longer as the number of hops increases, which further tightens the embedding bottleneck of the query vector, and (2) it is prone to error propagation. In this paper, we focus on alleviating these limitations in multi-hop settings by formulating the problem in a fully generative way. We propose an encoder-decoder model that performs multi-hop retrieval by simply generating the entire text sequences of the retrieval targets, which means the query and the documents interact in the language model\u2019s parametric space rather than L2 or inner product space as in the bi-encoder approach. Our approach, Generative Multi-hop Retrieval (GMR), consistently achieves comparable or higher performance than bi-encoder models in five datasets while demonstrating superior GPU memory and storage footprint.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.92.pdf"
    },
    {
        "title": "COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
        "authors": [
            "Yue Yu",
            "Chenyan Xiong",
            "Si Sun",
            "Chao Zhang",
            "Arnold Overwijk"
        ],
        "published": "2022",
        "summary": "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERT_Base scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At BERT_Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model which has 500x more parameters. Our analysis shows the correlation between COCO-DR\u2019s effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at https://github.com/OpenMatch/COCO-DR.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.95.pdf"
    },
    {
        "title": "Language Model Pre-Training with Sparse Latent Typing",
        "authors": [
            "Liliang Ren",
            "Zixuan Zhang",
            "Han Wang",
            "Clare Voss",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "published": "2022",
        "summary": "Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In this paper, we manage to push the language models to obtain a deeper understanding of sentences by proposing a new pre-training objective, Sparse Latent Typing, which enables the model to sparsely extract sentence-level keywords with diverse latent types. Experimental results show that our model is able to learn interpretable latent type categories in a self-supervised manner without using any external knowledge. Besides, the language model pre-trained with such an objective also significantly improves Information Extraction related downstream tasks in both supervised and few-shot settings. Our code is publicly available at https://github.com/renll/SparseLT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.96.pdf"
    },
    {
        "title": "Extracted BERT Model Leaks More Information than You Think!",
        "authors": [
            "Xuanli He",
            "Lingjuan Lyu",
            "Chen Chen",
            "Qiongkai Xu"
        ],
        "published": "2022",
        "summary": "The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.99.pdf"
    },
    {
        "title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization",
        "authors": [
            "Yuxian Gu",
            "Pei Ke",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022",
        "summary": "Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of human-annotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT\u2019s effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.105.pdf"
    },
    {
        "title": "COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models",
        "authors": [
            "Bowen Shen",
            "Zheng Lin",
            "Yuanxin Liu",
            "Zhengxiao Liu",
            "Lei Wang",
            "Weiping Wang"
        ],
        "published": "2022",
        "summary": "Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers. Motivated by such considerations, we propose a collaborative optimization for PLMs that integrates static model compression and dynamic inference acceleration. Specifically, the PLM is slenderized in width while the depth remains intact, complementing layer-wise early exiting to speed up inference dynamically. To address the trade-off of early exiting, we propose a joint training approach that calibrates slenderization and preserves contributive structures to each exit instead of only the final layer. Experiments are conducted on GLUE benchmark and the results verify the Pareto optimality of our approach at high compression and acceleration rate with 1/8 parameters and 1/19 FLOPs of BERT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.112.pdf"
    },
    {
        "title": "An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Archit Uniyal",
            "Tianhao Wang",
            "David Evans",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022",
        "summary": "Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the \u201cpre-train and fine-tune\u201d paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.119.pdf"
    },
    {
        "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
        "authors": [
            "Evangelia Spiliopoulou",
            "Artidoro Pagnoni",
            "Yonatan Bisk",
            "Eduard Hovy"
        ],
        "published": "2022",
        "summary": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.129.pdf"
    },
    {
        "title": "Large language models are few-shot clinical information extractors",
        "authors": [
            "Monica Agrawal",
            "Stefan Hegselmann",
            "Hunter Lang",
            "Yoon Kim",
            "David Sontag"
        ],
        "published": "2022",
        "summary": "A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.130.pdf"
    },
    {
        "title": "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations",
        "authors": [
            "Swarnadeep Saha",
            "Peter Hase",
            "Nazneen Rajani",
            "Mohit Bansal"
        ],
        "published": "2022",
        "summary": "Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question \u2013 \u201cAre LLMs and humans equally good at explaining data labels for both easy and hard samples?\u201d We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.137.pdf"
    },
    {
        "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
        "authors": [
            "Ben Zhou",
            "Kyle Richardson",
            "Xiaodong Yu",
            "Dan Roth"
        ],
        "published": "2022",
        "summary": "Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this effort, the majority have small-scale annotations and limited scope, which is insufficient to solve general decomposition tasks. In this paper, we look at large-scale intermediate pre-training of decomposition-based transformers using distant supervision from comparable texts, particularly large-scale parallel news. We show that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible. For example, on semantic parsing, our model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model. We further use DecompT5 to build a novel decomposition-based QA system named DecompEntail, improving over state-of-the-art models, including GPT-3, on both HotpotQA and StrategyQA by 8% and 4%, respectively.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.142.pdf"
    },
    {
        "title": "Gradient-based Constrained Sampling from Language Models",
        "authors": [
            "Sachin Kumar",
            "Biswajit Paria",
            "Yulia Tsvetkov"
        ],
        "published": "2022",
        "summary": "Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model\u2019s performance in a downstream task. We propose MuCoLa\u2014a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.144.pdf"
    },
    {
        "title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
        "authors": [
            "Hung-Ting Chen",
            "Michael Zhang",
            "Eunsol Choi"
        ],
        "published": "2022",
        "summary": "Question answering models can use rich knowledge sources \u2014 up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors. We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledgein their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally. To address this issue, we present a new calibration study, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.146.pdf"
    },
    {
        "title": "When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain",
        "authors": [
            "Raj Shah",
            "Kunal Chawla",
            "Dheeraj Eidnani",
            "Agam Shah",
            "Wendi Du",
            "Sudheer Chava",
            "Natraj Raman",
            "Charese Smiley",
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.148.pdf"
    },
    {
        "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models",
        "authors": [
            "Sharon Levy",
            "Emily Allaway",
            "Melanie Subbiah",
            "Lydia Chilton",
            "Desmond Patton",
            "Kathleen McKeown",
            "William Yang Wang"
        ],
        "published": "2022",
        "summary": "Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.154.pdf"
    },
    {
        "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
        "authors": [
            "Kang Min Yoo",
            "Junyeob Kim",
            "Hyuhng Joon Kim",
            "Hyunsoo Cho",
            "Hwiyeol Jo",
            "Sang-Woo Lee",
            "Sang-goo Lee",
            "Taeuk Kim"
        ],
        "published": "2022",
        "summary": "Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought.Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning.With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations.Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.155.pdf"
    },
    {
        "title": "Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models",
        "authors": [
            "Hao Zhang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its variants, have led to significant improvements on various NLP tasks in past years. However, a theoretical framework for studying their relationships is still missing. In this paper, we fill this gap by investigating the linear dependency between pre-trained LMs. The linear dependency of LMs is defined analogously to the linear dependency of vectors. We propose Language Model Decomposition (LMD) to represent a LM using a linear combination of other LMs as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD similar to the coefficient of determination is defined and used to measure the linear dependency of a set of LMs. In experiments, we find that BERT and eleven (11) BERT-like LMs are 91% linearly dependent. This observation suggests that current state-of-the-art (SOTA) LMs are highly \u201ccorrelated\u201d. To further advance SOTA we need more diverse and novel LMs that are less dependent on existing LMs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.161.pdf"
    },
    {
        "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection",
        "authors": [
            "Suchin Gururangan",
            "Dallas Card",
            "Sarah Dreier",
            "Emily Gade",
            "Leroy Wang",
            "Zeyu Wang",
            "Luke Zettlemoyer",
            "Noah A. Smith"
        ],
        "published": "2022",
        "summary": "Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles\u2014written by students from across the country\u2014we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban zones (ZIP codes) are more likely to be classified as high quality. We also show that this quality measurement is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.165.pdf"
    },
    {
        "title": "Calibrating Zero-shot Cross-lingual (Un-)structured Predictions",
        "authors": [
            "Zhengping Jiang",
            "Anqi Liu",
            "Benjamin Van Durme"
        ],
        "published": "2022",
        "summary": "We investigate model calibration in the setting of zero-shot cross-lingual transfer with large-scale pre-trained language models. The level of model calibration is an important metric for evaluating the trustworthiness of predictive models. There exists an essential need for model calibration when natural language models are deployed in critical tasks. We study different post-training calibration methods in structured and unstructured prediction tasks. We find that models trained with data from the source language become less calibrated when applied to the target language and that calibration errors increase with intrinsic task difficulty and relative sparsity of training data. Moreover, we observe a potential connection between the level of calibration error and an earlier proposed measure of the distance from English to other languages. Finally, our comparison demonstrates that among other methods Temperature Scaling (TS) generalizes well to distant languages, but TS fails to calibrate more complex confidence estimation in structured predictions compared to more expressive alternatives like Gaussian Process Calibration.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.170.pdf"
    },
    {
        "title": "Measuring Context-Word Biases in Lexical Semantic Datasets",
        "authors": [
            "Qianchu Liu",
            "Diana McCarthy",
            "Anna Korhonen"
        ],
        "published": "2022",
        "summary": "State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. We question this assumption by presenting the first quantitative analysis on the context-word interaction being tested in major contextual lexical semantic tasks. To achieve this, we run probing baselines on masked input, and propose measures to calculate and visualize the degree of context or word biases in existing datasets. The analysis was performed on both models and humans. Our findings demonstrate that models are usually not being tested for word-in-context semantics in the same way as humans are in these tasks, which helps us better understand the model-human gap. Specifically, to PCMs, most existing datasets fall into the extreme ends (the retrieval-based tasks exhibit strong target word bias while WiC-style tasks and WSD show strong context bias); In comparison, humans are less biased and achieve much better performance when both word and context are available than with masked input. We recommend our framework for understanding and controlling these biases for model interpretation and future task design.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.173.pdf"
    },
    {
        "title": "Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation",
        "authors": [
            "Xiang Hu",
            "Haitao Mi",
            "Liang Li",
            "Gerard de Melo"
        ],
        "published": "2022",
        "summary": "Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n\u00b3) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback\u2013Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.181.pdf"
    },
    {
        "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Peter Clark",
            "Yiming Yang"
        ],
        "published": "2022",
        "summary": "Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \u201cWhat word is similar to good?\u201d to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user\u2019s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.183.pdf"
    },
    {
        "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
        "authors": [
            "Lan Jiang",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Rui Jiang"
        ],
        "published": "2022",
        "summary": "Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.In this work, we present a novel fine-tuning approach called RObust SEletive fine-tuning (ROSE) to address this issue.ROSE conducts selective updates when adapting pre-trained models to downstream tasks, filtering out invaluable and unrobust updates of parameters.Specifically, we propose two strategies: the first-order and second-order ROSE for selecting target robust parameters.The experimental results show that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above.Furthermore, ROSE can be easily incorporated into existing fine-tuning methods to improve their adversarial robustness further.The empirical analysis confirms that ROSE eliminates unrobust spurious updates during fine-tuning, leading to solutions corresponding to flatter and wider optima than the conventional method.Code is available at https://github.com/jiangllan/ROSE.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.186.pdf"
    },
    {
        "title": "Reproducibility Issues for BERT-based Evaluation Metrics",
        "authors": [
            "Yanran Chen",
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022",
        "summary": "Reproducibility is of utmost concern in machine learning and natural language processing (NLP). In the field of natural language generation (especially machine translation), the seminal paper of Post (2018) has pointed out problems of reproducibility of the dominant metric, BLEU, at the time of publication. Nowadays, BERT-based evaluation metrics considerably outperform BLEU. In this paper, we ask whether results and claims from four recent BERT-based metrics can be reproduced. We find that reproduction of claims and results often fails because of (i) heavy undocumented preprocessing involved in the metrics, (ii) missing code and (iii) reporting weaker results for the baseline metrics. (iv) In one case, the problem stems from correlating not to human scores but to a wrong column in the csv file, inflating scores by 5 points. Motivated by the impact of preprocessing, we then conduct a second study where we examine its effects more closely (for one of the metrics). We find that preprocessing can have large effects, especially for highly inflectional languages. In this case, the effect of preprocessing may be larger than the effect of the aggregation mechanism (e.g., greedy alignment vs. Word Mover Distance).",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.192.pdf"
    },
    {
        "title": "Generative Entity Typing with Curriculum Learning",
        "authors": [
            "Siyu Yuan",
            "Deqing Yang",
            "Jiaqing Liang",
            "Zhixu Li",
            "Jinxi Liu",
            "Jingyue Huang",
            "Yanghua Xiao"
        ],
        "published": "2022",
        "summary": "Entity typing aims to assign types to the entity mentions in given texts. The traditional classification-based entity typing paradigm has two unignorable drawbacks: 1) it fails to assign an entity to the types beyond the predefined type set, and 2) it can hardly handle few-shot and zero-shot situations where many long-tail types only have few or even no training instances. To overcome these drawbacks, we propose a novel generative entity typing (GET) paradigm: given a text with an entity mention, the multiple types for the role that the entity plays in the text are generated with a pre-trained language model (PLM). However, PLMs tend to generate coarse-grained types after fine-tuning upon the entity typing dataset. In addition, only the heterogeneous training data consisting of a small portion of human-annotated data and a large portion of auto-generated but low-quality data are provided for model training. To tackle these problems, we employ curriculum learning (CL) to train our GET model on heterogeneous data, where the curriculum could be self-adjusted with the self-paced learning according to its comprehension of the type granularity and data heterogeneity. Our extensive experiments upon the datasets of different languages and downstream tasks justify the superiority of our GET model over the state-of-the-art entity typing models. The code has been released on https://github.com/siyuyuan/GET.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.199.pdf"
    },
    {
        "title": "Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model",
        "authors": [
            "Mingqi Li",
            "Fei Ding",
            "Dan Zhang",
            "Long Cheng",
            "Hongxin Hu",
            "Feng Luo"
        ],
        "published": "2022",
        "summary": "Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level Multilingual Knowledge Distillation (MMKD), a novel method for improving multilingual language models. Specifically, we employ a teacher-student framework to adopt rich semantic representation knowledge in English BERT. We propose token-, word-, sentence-, and structure-level alignment objectives to encourage multiple levels of consistency between source-target pairs and correlation similarity between teacher and student models. We conduct experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and XQuAD. Experimental results show that MMKD outperforms other baseline models of similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X. Especially, MMKD obtains significant performance gains on low-resource languages.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.202.pdf"
    },
    {
        "title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing",
        "authors": [
            "Abbas Ghaddar",
            "Yimeng Wu",
            "Sunyam Bagga",
            "Ahmad Rashid",
            "Khalil Bibi",
            "Mehdi Rezagholizadeh",
            "Chao Xing",
            "Yasheng Wang",
            "Xinyu Duan",
            "Zhefeng Wang",
            "Baoxing Huai",
            "Xin Jiang",
            "Qun Liu",
            "Phillippe Langlais"
        ],
        "published": "2022",
        "summary": "There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields. First, existing Arabic PLMs are not well-explored and their pre-training can be improved significantly using a more methodical approach. Second, there is a lack of systematic and reproducible evaluation of these models in the literature. We revisit both the pre-training and evaluation of Arabic PLMs. In terms of pre-training, we explore the impact of the quality of the pretraining data, the size of the model, and the incorporation of character-level information on Arabic PLM. As a result, we release three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and two T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a comprehensive empirical study to systematically evaluate the performance of existing state-of-the-art models on ALUE, a leaderboard-powered benchmark for Arabic NLU tasks, and on a subset of the Arabic generative tasks. We show that our models significantly outperform existing Arabic PLMs and achieve a new state-of-the-art performance on discriminative and generative Arabic NLU and NLG tasks. Our models and source code to reproduce results will be made available upon acceptance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.205.pdf"
    },
    {
        "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding",
        "authors": [
            "Jianing Wang",
            "Wenkang Huang",
            "Minghui Qiu",
            "Qiuhui Shi",
            "Hongbin Wang",
            "Xiang Li",
            "Ming Gao"
        ],
        "published": "2022",
        "summary": "Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a knowledge-prompting-based PLM framework KP-PLM. This framework can be flexibly combined with existing mainstream PLMs. Specifically, we first construct a knowledge sub-graph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub-graph into natural language prompts. To further leverage the factual knowledge from these prompts, we propose two novel knowledge-aware self-supervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP-PLM over other state-of-the-art methods in both full-resource and low-resource settings. Our source codes will be released upon the acceptance of the paper.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.207.pdf"
    },
    {
        "title": "Nearest Neighbor Zero-Shot Inference",
        "authors": [
            "Weijia Shi",
            "Julian Michael",
            "Suchin Gururangan",
            "Luke Zettlemoyer"
        ],
        "published": "2022",
        "summary": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zeroshot baselines (13.4% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.214.pdf"
    },
    {
        "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
        "authors": [
            "Mingkai Deng",
            "Jianyu Wang",
            "Cheng-Ping Hsieh",
            "Yihan Wang",
            "Han Guo",
            "Tianmin Shu",
            "Meng Song",
            "Eric Xing",
            "Zhiting Hu"
        ],
        "published": "2022",
        "summary": "Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by \u201cenumeration (e.g., paraphrasing)-then-selection\u201d heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.222.pdf"
    },
    {
        "title": "Discovering Differences in the Representation of People using Contextualized Semantic Axes",
        "authors": [
            "Li Lucy",
            "Divya Tadimeti",
            "David Bamman"
        ],
        "published": "2022",
        "summary": "A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against \u201csemantic axes\u201d that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men\u2019s communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.228.pdf"
    },
    {
        "title": "BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation",
        "authors": [
            "Tianxiang Sun",
            "Junliang He",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2022",
        "summary": "Automatic evaluation metrics are crucial to the development of generative systems. In recent years, pre-trained language model (PLM) based metrics, such as BERTScore, have been commonly adopted in various generation tasks. However, it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern about the fairness of PLMs as metrics. To that end, this work presents the first systematic study on the social bias in PLM-based metrics. We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely race, gender, religion, physical appearance, age, and socioeconomic status. In-depth analysis suggests that choosing paradigms (matching, regression, or generation) of the metric has a greater impact on fairness than choosing PLMs. In addition, we develop debiasing adapters that are injected into PLM layers, mitigating bias in PLM-based metrics while retaining high performance for evaluating text generation.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.245.pdf"
    },
    {
        "title": "HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification",
        "authors": [
            "Zihan Wang",
            "Peiyi Wang",
            "Tianyu Liu",
            "Binghuai Lin",
            "Yunbo Cao",
            "Zhifang Sui",
            "Houfeng Wang"
        ],
        "published": "2022",
        "summary": "Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex label hierarchy.Recently, the pretrained language models (PLM)have been widely adopted in HTC through a fine-tuning paradigm. However, in this paradigm, there exists a huge gap between the classification tasks with sophisticated label hierarchy and the masked language model (MLM) pretraining tasks of PLMs and thus the potential of PLMs cannot be fully tapped.To bridge the gap, in this paper, we propose HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label MLM perspective.Specifically, we construct a dynamic virtual template and label words that take the form of soft prompts to fuse the label hierarchy knowledge and introduce a zero-bounded multi-label cross-entropy loss to harmonize the objectives of HTC and MLM.Extensive experiments show HPT achieves state-of-the-art performances on 3 popular HTC datasets and is adept at handling the imbalance and low resource situations. Our code is available at https://github.com/wzh9969/HPT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.246.pdf"
    },
    {
        "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
        "authors": [
            "Maarten Sap",
            "Ronan Le Bras",
            "Daniel Fried",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today\u2019s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models\u2019 ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.248.pdf"
    },
    {
        "title": "Improving Passage Retrieval with Zero-Shot Question Generation",
        "authors": [
            "Devendra Sachan",
            "Mike Lewis",
            "Mandar Joshi",
            "Armen Aghajanyan",
            "Wen-tau Yih",
            "Joelle Pineau",
            "Luke Zettlemoyer"
        ],
        "published": "2022",
        "summary": "We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.249.pdf"
    },
    {
        "title": "BBTv2: Towards a Gradient-Free Future with Large Language Models",
        "authors": [
            "Tianxiang Sun",
            "Zhengfu He",
            "Hong Qian",
            "Yunhua Zhou",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2022",
        "summary": "Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size.By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment.Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks versatility across tasks and PTMs.In this paper, we present BBTv2, an improved version of Black-Box Tuning, to drive PTMs for few-shot learning.We prepend continuous prompts to every layer of the PTM and propose a divide-and-conquer gradient-free algorithm to optimize the prompts at different layers alternately.Extensive experiments across various tasks and PTMs show that BBTv2 can achieve comparable performance to full model tuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit, etc.) under few-shot settings while maintaining much fewer tunable parameters.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.259.pdf"
    },
    {
        "title": "Mixed-effects transformers for hierarchical adaptation",
        "authors": [
            "Julia White",
            "Noah Goodman",
            "Robert Hawkins"
        ],
        "published": "2022",
        "summary": "Language differs dramatically from context to context. To some degree, large language models like GPT-3 account for such variation by conditioning on strings of initial input text, or prompts. However, prompting can be ineffective when contexts are sparse, out-of-sample, or extra-textual. In this paper, we introduce the mixed-effects transformer (MET), a novel approach for learning hierarchically-structured prefixes\u2014 lightweight modules prepended to an input sequence\u2014 to account for structured variation in language use. Specifically, we show how the popular class of mixed-effects regression models may be extended to transformer-based architectures using a regularized prefix-tuning procedure with dropout. We evaluate this approach on several domain-adaptation benchmarks, finding that it learns contextual variation from minimal data while generalizing well to unseen contexts.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.261.pdf"
    },
    {
        "title": "Exploiting Global and Local Hierarchies for Hierarchical Text Classification",
        "authors": [
            "Ting Jiang",
            "Deqing Wang",
            "Leilei Sun",
            "Zhongzhi Chen",
            "Fuzhen Zhuang",
            "Qinghong Yang"
        ],
        "published": "2022",
        "summary": "Hierarchical text classification aims to leverage label hierarchy in multi-label text classification. Existing methods encode label hierarchy in a global view, where label hierarchy is treated as the static hierarchical structure containing all labels. Since global hierarchy is static and irrelevant to text samples, it makes these methods hard to exploit hierarchical information. Contrary to global hierarchy, local hierarchy as a structured labels hierarchy corresponding to each text sample. It is dynamic and relevant to text samples, which is ignored in previous methods. To exploit global and local hierarchies, we propose Hierarchy-guided BERT with Global and Local hierarchies (HBGL), which utilizes the large-scale parameters and prior language knowledge of BERT to model both global and local hierarchies. Moreover, HBGL avoids the intentional fusion of semantic and hierarchical modules by directly modeling semantic and hierarchical information with BERT. Compared with the state-of-the-art method HGCLR, our method achieves significant improvement on three benchmark datasets.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.268.pdf"
    },
    {
        "title": "Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding",
        "authors": [
            "SongYang Gao",
            "Shihan Dou",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022",
        "summary": "Dataset bias has attracted increasing attention recently for its detrimental effect on the generalization ability of fine-tuned models. The current mainstream solution is designing an additional shallow model to pre-identify biased instances. However, such two-stage methods scale up the computational complexity of training process and obstruct valid feature information while mitigating bias.To address this issue, we utilize the representation normalization method which aims at disentangling the correlations between features of encoded sentences. We find it also promising in eliminating the bias problem by providing isotropic data distribution. We further propose Kernel-Whitening, a Nystrom kernel approximation method to achieve more thorough debiasing on nonlinear spurious correlations. Our framework is end-to-end with similar time consumption to fine-tuning. Experiments show that Kernel-Whitening significantly improves the performance of BERT on out-of-distribution datasets while maintaining in-distribution accuracy.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.275.pdf"
    },
    {
        "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
        "authors": [
            "Kumar Shridhar",
            "Jakub Macina",
            "Mennatallah El-Assady",
            "Tanmay Sinha",
            "Manu Kapur",
            "Mrinmaya Sachan"
        ],
        "published": "2022",
        "summary": "Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.277.pdf"
    },
    {
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
        "authors": [
            "Eldar Kurtic",
            "Daniel Campos",
            "Tuan Nguyen",
            "Elias Frantar",
            "Mark Kurtz",
            "Benjamin Fineran",
            "Michael Goin",
            "Dan Alistarh"
        ],
        "published": "2022",
        "summary": "In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and 29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.279.pdf"
    },
    {
        "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
        "authors": [
            "Kevin Yang",
            "Yuandong Tian",
            "Nanyun Peng",
            "Dan Klein"
        ],
        "published": "2022",
        "summary": "We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3\u2019s stories as having a coherent overarching plot (by 14% absolute increase), and relevant to the given initial premise (by 20%).",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.296.pdf"
    },
    {
        "title": "Continued Pretraining for Better Zero- and Few-Shot Promptability",
        "authors": [
            "Zhaofeng Wu",
            "Robert L Logan IV",
            "Pete Walsh",
            "Akshita Bhagia",
            "Dirk Groeneveld",
            "Sameer Singh",
            "Iz Beltagy"
        ],
        "published": "2022",
        "summary": "Recently introduced language model prompting methods can achieve high accuracy in zero- and few-shot settings while requiring few to no learned task-specific parameters. Nevertheless, these methods still often trail behind full model finetuning. In this work, we investigate if a dedicated continued pretraining stage could improve \u201cpromptability\u201d, i.e., zero-shot performance with natural language prompts or few-shot performance with prompt tuning. We reveal settings where existing continued pretraining methods lack promptability. We also identify current methodological gaps, which we fill with thorough large-scale experiments. We demonstrate that a simple recipe, continued pretraining that incorporates a trainable prompt during multi-task learning, leads to improved promptability in both zero- and few-shot settings compared to existing methods, up to 31% relative. On the other hand, we find that continued pretraining using MAML-style meta-learning, a method that directly optimizes few-shot promptability, yields subpar performance. We validate our findings with two prompt tuning methods, and, based on our results, we provide concrete recommendations to optimize promptability for different use cases.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.300.pdf"
    },
    {
        "title": "Is a Question Decomposition Unit All We Need?",
        "authors": [
            "Pruthvi Patel",
            "Swaroop Mishra",
            "Mihir Parmar",
            "Chitta Baral"
        ],
        "published": "2022",
        "summary": "Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and environmental impact associated with it. We explore an alternative route: can we modify data by expressing it in terms of the model\u2019s strengths, so that a question becomes easier for models to answer? We investigate if humans can decompose a hard question into a set of simpler questions that are relatively easier for models to solve. We analyze a range of datasets involving various forms of reasoning and find that it is indeed possible to significantly improve model performance (24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via decomposition. Our approach provides a viable option to involve people in NLP research in a meaningful way. Our findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.302.pdf"
    },
    {
        "title": "Discourse-Aware Soft Prompting for Text Generation",
        "authors": [
            "Marjan Ghazvininejad",
            "Vladimir Karpukhin",
            "Vera Gor",
            "Asli Celikyilmaz"
        ],
        "published": "2022",
        "summary": "Current efficient fine-tuning methods(e.g., adapters, prefix-tuning, etc.) have optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they don\u2019t generalize across all generation tasks. We show that soft-prompt based conditional text generation can be improved with simple and efficient methods that simulate modeling the discourse structure of human written text.We investigate two design choices: First, we apply hierarchical blocking on the prefix parameters to simulate a higher-level discourse structure of human written text. Second, we apply attention sparsity on the prefix parameters at different layers of the network and learn sparse transformations on the softmax-function. We show that structured design of prefix parameters yields more coherent, faithful and relevant generations than the baseline prefix-tuning on all generation tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.303.pdf"
    },
    {
        "title": "SLING: Sino Linguistic Evaluation of Large Language Models",
        "authors": [
            "Yixiao Song",
            "Kalpesh Krishna",
            "Rajesh Bhatt",
            "Mohit Iyyer"
        ],
        "published": "2022",
        "summary": "To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP\u2019s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7% vs. 97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.305.pdf"
    },
    {
        "title": "Efficient Nearest Neighbor Emotion Classification with BERT-whitening",
        "authors": [
            "Wenbiao Yin",
            "Lin Shang"
        ],
        "published": "2022",
        "summary": "Retrieval-based methods have been proven effective in many NLP tasks. Previous methods use representations from the pre-trained model for similarity search directly. However, the sentence representations from the pre-trained model like BERT perform poorly in retrieving semantically similar sentences, resulting in poor performance of the retrieval-based methods. In this paper, we propose kNN-EC, a simple and efficient non-parametric emotion classification (EC) method using nearest neighbor retrieval. We use BERT-whitening to get better sentence semantics, ensuring that nearest neighbor retrieval works. Meanwhile, BERT-whitening can also reduce memory storage of datastore and accelerate retrieval speed, solving the efficiency problem of the previous methods. kNN-EC average improves the pre-trained model by 1.17 F1-macro on two emotion classification datasets.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.312.pdf"
    },
    {
        "title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation",
        "authors": [
            "Jiuzhou Han",
            "Ehsan Shareghi"
        ],
        "published": "2022",
        "summary": "Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.321.pdf"
    },
    {
        "title": "Differentially Private Language Models for Secure Data Sharing",
        "authors": [
            "Justus Mattern",
            "Zhijing Jin",
            "Benjamin Weggenmann",
            "Bernhard Schoelkopf",
            "Mrinmaya Sachan"
        ],
        "published": "2022",
        "summary": "To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.323.pdf"
    },
    {
        "title": "Conditional set generation using Seq2seq models",
        "authors": [
            "Aman Madaan",
            "Dheeraj Rajagopal",
            "Niket Tandon",
            "Yiming Yang",
            "Antoine Bosselut"
        ],
        "published": "2022",
        "summary": "Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models are a popular choice to model set generation but they treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. Further, we jointly model the set cardinality and output by listing the set size as the first element and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this new augmented data (without any additional annotations), gets an average relative improvement of 20% for four benchmarks datasets across models spanning from BART-base, T5-11B, and GPT-3. We will release all code and data upon acceptance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.324.pdf"
    },
    {
        "title": "Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables",
        "authors": [
            "Erxin Yu",
            "Lan Du",
            "Yuan Jin",
            "Zhepei Wei",
            "Yi Chang"
        ],
        "published": "2022",
        "summary": "Recently, discrete latent variable models have received a surge of interest in both Natural Language Processing (NLP) and Computer Vision (CV), attributed to their comparable performance to the continuous counterparts in representation learning, while being more interpretable in their predictions. In this paper, we develop a topic-informed discrete latent variable model for semantic textual similarity, which learns a shared latent space for sentence-pair representation via vector quantization. Compared with previous models limited to local semantic contexts, our model can explore richer semantic information via topic modeling. We further boost the performance of semantic similarity by injecting the quantized representation into a transformer-based language model with a well-designed semantic-driven attention mechanism. We demonstrate, through extensive experiments across various English language datasets, that our model is able to surpass several strong neural baselines in semantic textual similarity tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.328.pdf"
    },
    {
        "title": "Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis",
        "authors": [
            "Shuai Fan",
            "Chen Lin",
            "Haonan Li",
            "Zhenghao Lin",
            "Jinsong Su",
            "Hang Zhang",
            "Yeyun Gong",
            "JIan Guo",
            "Nan Duan"
        ],
        "published": "2022",
        "summary": "Most existing pre-trained language representation models (PLMs) are sub-optimal in sentiment analysis tasks, as they capture the sentiment information from word-level while under-considering sentence-level information. In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained language model with combined Word-level and Sentence-level Pre-training tasks.The word level pre-training task detects replaced sentiment words, via a generator-discriminator framework, to enhance the PLM\u2019s knowledge about sentiment words.The sentence level pre-training task further strengthens the discriminator via a contrastive learning framework, with similar sentences as negative samples, to encode sentiments in a sentence.Extensive experimental results show that SentiWSP achieves new state-of-the-art performance on various sentence-level and aspect-level sentiment classification benchmarks. We have made our code and model publicly available at https://github.com/XMUDM/SentiWSP.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.332.pdf"
    },
    {
        "title": "MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks",
        "authors": [
            "Zequn Liu",
            "Kefei Duan",
            "Junwei Yang",
            "Hanwen Xu",
            "Ming Zhang",
            "Sheng Wang"
        ],
        "published": "2022",
        "summary": "Heterogeneous information network (HIN) is essential to study complicated networks containing multiple edge types and node types. Meta-path, a sequence of node types and edge types, is the core technique to embed HINs. Since manually curating meta-paths is time-consuming, there is a pressing need to develop automated meta-path generation approaches. Existing meta-path generation approaches cannot fully exploit the rich textual information in HINs, such as node names and edge type names. To address this problem, we propose MetaFill, a text-infilling-based approach for meta-path generation. The key idea of MetaFill is to formulate meta-path identification problem as a word sequence infilling problem, which can be advanced by pretrained language models (PLMs). We observed the superior performance of MetaFill against existing meta-path generation methods and graph embedding methods that do not leverage meta-paths in both link prediction and node classification on two real-world HIN datasets. We further demonstrated how MetaFill can accurately classify edges in the zero-shot setting, where existing approaches cannot generate any meta-paths. MetaFill exploits PLMs to generate meta-paths for graph embedding, opening up new avenues for language model applications in graph analysis.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.341.pdf"
    },
    {
        "title": "DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering",
        "authors": [
            "Miao Zhang",
            "Rufeng Dai",
            "Ming Dong",
            "Tingting He"
        ],
        "published": "2022",
        "summary": "In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context representation to interact with multiple layers of KG, which results in a restricted interaction. In this paper, we propose DRLK (Dynamic Hierarchical Reasoning with Language Model and Knowledge Graphs), a novel model that utilizes dynamic hierarchical interactions between the QA context and KG for reasoning. DRLK extracts dynamic hierarchical features in the QA context, and performs inter-layer and intra-layer interactions on each iteration, allowing the KG representation to be grounded with the hierarchical features of the QA context. We conduct extensive experiments on four benchmark datasets in medical QA and commonsense reasoning. The experimental results demonstrate that DRLK achieves state-of-the-art performances on two benchmark datasets and performs competitively on the others.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.342.pdf"
    },
    {
        "title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering",
        "authors": [
            "Minchul Lee",
            "Kijong Han",
            "Myeong Cheol Shin"
        ],
        "published": "2022",
        "summary": "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem.However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy.In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases(ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective.The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pre-trained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain.As a result, our experiments show that LittleBird works very well in a variety of languages, achieving high performance in question answering tasks, particularly in KorQuAD2.0, Korean Question Answering Dataset for long paragraphs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.352.pdf"
    },
    {
        "title": "WeTS: A Benchmark for Translation Suggestion",
        "authors": [
            "Zhen Yang",
            "Fandong Meng",
            "Yingxue Zhang",
            "Ernan Li",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Translation suggestion (TS), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (MT), has been proven to play a significant role in post-editing (PE). There are two main pitfalls for existing researches in this line. First, most conventional works only focus on the overall performance of PE but ignore the exact performance of TS, which makes the progress of PE sluggish and less explainable; Second, as no publicly available golden dataset exists to support in-depth research for TS, almost all of the previous works conduct experiments on their in-house datasets or the noisy datasets built automatically, which makes their experiments hard to be reproduced and compared. To break these limitations mentioned above and spur the research in TS, we create a benchmark dataset, called WeTS, which is a golden corpus annotated by expert translators on four translation directions. Apart from the golden corpus, we also propose several methods to generate synthetic corpora which can be used to improve the performance substantially through pre-training. As for the model, we propose the segment-aware self-attention based Transformer for TS. Experimental results show that our approach achieves the best results on all four directions, including English-to-German, German-to-English, Chinese-to-English, and English-to-Chinese.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.353.pdf"
    },
    {
        "title": "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model",
        "authors": [
            "Hojun Cho",
            "Dohee Kim",
            "Seungwoo Ryu",
            "ChaeHun Park",
            "Hyungjong Noh",
            "Jeong-in Hwang",
            "Minseok Choi",
            "Edward Choi",
            "Jaegul Choo"
        ],
        "published": "2022",
        "summary": "Style control, content preservation, and fluency determine the quality of text style transfer models. To train on a nonparallel corpus, several existing approaches aim to deceive the style discriminator with an adversarial loss. However, adversarial training significantly degrades fluency compared to the other two metrics. In this work, we explain this phenomenon using energy-based interpretation, and leverage a pretrained language model to improve fluency. Specifically, we propose a novel approach which applies the pretrained language model to the text style transfer framework by restructuring the discriminator and the model itself, allowing the generator and the discriminator to also take advantage of the power of the pretrained model. We evaluated our model on three public benchmarks GYAFC, Amazon, and Yelp and achieved state-of-the-art performance on the overall metrics.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.366.pdf"
    },
    {
        "title": "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation",
        "authors": [
            "Ao Liu",
            "Haoyu Dong",
            "Naoaki Okazaki",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022",
        "summary": "Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models. However, directly learning the logical inference knowledge from table-text pairs is very difficult for neural models because of the ambiguity of natural language and the scarcity of parallel data. Hence even large-scale pre-trained language models present low logical fidelity on logical table-to-text. In this work, we propose a Pretrained Logical Form Generator (PLOG) framework to improve generation fidelity. Specifically, PLOG is first pretrained on a table-to-logical-form generation (table-to-logic) task, then finetuned on downstream table-to-text tasks. The logical forms are formally defined with unambiguous semantics. Hence we can collect a large amount of accurate logical forms from tables without human annotation. In addition, PLOG can learn logical inference from table-logic pairs much more reliably than from table-text pairs. To evaluate our model, we further collect a controlled logical table-to-text dataset CONTLOG based on an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms strong baselines by a large margin on the logical fidelity, demonstrating the effectiveness of table-to-logic pretraining.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.373.pdf"
    },
    {
        "title": "Rethinking the Authorship Verification Experimental Setups",
        "authors": [
            "Florin Brad",
            "Andrei Manolache",
            "Elena Burceanu",
            "Antonio Barbalau",
            "Radu Tudor Ionescu",
            "Marius Popescu"
        ],
        "published": "2022",
        "summary": "One of the main drivers of the recent advances in authorship verification is the PAN large-scale authorship dataset. Despite generating significant progress in the field, inconsistent performance differences between the closed and open test sets have been reported. To this end, we improve the experimental setup by proposing five new public splits over the PAN dataset, specifically designed to isolate and identify biases related to the text topic and to the author\u2019s writing style. We evaluate several BERT-like baselines on these splits, showing that such models are competitive with authorship verification state-of-the-art methods. Furthermore, using explainable AI, we find that these baselines are biased towards named entities. We show that models trained without the named entities obtain better results and generalize better when tested on DarkReddit, our new dataset for authorship verification.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.380.pdf"
    },
    {
        "title": "Training Language Models with Memory Augmentation",
        "authors": [
            "Zexuan Zhong",
            "Tao Lei",
            "Danqi Chen"
        ],
        "published": "2022",
        "summary": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories\u2014local, long-term, and external memory\u2014at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.382.pdf"
    },
    {
        "title": "Invariant Language Modeling",
        "authors": [
            "Maxime Peyrard",
            "Sarvjeet Ghotra",
            "Martin Josifoski",
            "Vidhan Agarwal",
            "Barun Patra",
            "Dean Carignan",
            "Emre Kiciman",
            "Saurabh Tiwary",
            "Robert West"
        ],
        "published": "2022",
        "summary": "Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose invariant language modeling, a framework for learning invariant representations that generalize better across multiple environments. In particular, we adapt a game-theoretic implementation of IRM (IRM-games) to language models, where the invariance emerges from a specific training schedule in which all the environments compete to optimize their own environment-specific loss by updating subsets of the model in a round-robin fashion.We focused on controlled experiments to precisely demonstrate the ability of our method to (i) remove structured noise, (ii) ignore specific spurious correlations without affecting global performance, and (iii) achieve better out-of-domain generalization.These benefits come with a negligible computational overhead compared to standard training, do not require changing the local loss, and can be applied to any language model. We believe this framework is promising to help mitigate spurious correlations and biases in language models.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.387.pdf"
    },
    {
        "title": "InforMask: Unsupervised Informative Masking for Language Model Pretraining",
        "authors": [
            "Nafis Sadeq",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2022",
        "summary": "Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training masked language models. InforMask exploits Pointwise Mutual Information (PMI) to select the most informative tokens to mask. We further propose two optimizations for InforMask to improve its efficiency. With a one-off preprocessing step, InforMask outperforms random masking and previously proposed masking strategies on the factual recall benchmark LAMA and the question answering benchmark SQuAD v1 and v2.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.395.pdf"
    },
    {
        "title": "Mutual Information Alleviates Hallucinations in Abstractive Summarization",
        "authors": [
            "Liam van der Poel",
            "Ryan Cotterell",
            "Clara Meister"
        ],
        "published": "2022",
        "summary": "Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix\u2014or at least uncover the source of\u2014the problem with limited success. In this paper, we identify a simple criterion under which models are significantly more likely to assign more probability to hallucinated content during generation: high model uncertainty. This finding offers a potential explanation for hallucinations: models default to favoring text with high marginal probability, i.e., high-frequency occurrences in the training set, when uncertain about a continuation. It also motivates possible routes for real-time intervention during decoding to prevent such hallucinations. We propose a decoding strategy that switches to optimizing for pointwise mutual information of the source and target token\u2014rather than purely the probability of the target token\u2014when the model exhibits uncertainty. Experiments on the dataset show that our method decreases the probability of hallucinated tokens while maintaining the Rouge and BERT-S scores of top-performing decoding strategies.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.399.pdf"
    },
    {
        "title": "The Authenticity Gap in Human Evaluation",
        "authors": [
            "Kawin Ethayarajh",
            "Dan Jurafsky"
        ],
        "published": "2022",
        "summary": "Human ratings are the gold standard in NLG evaluation. The standard protocol is to collect ratings of generated text, average across annotators, and rank NLG systems by their average scores. However, little consideration has been given as to whether this approach faithfully captures human preferences. Analyzing this standard protocol through the lens of utility theory in economics, we identify the implicit assumptions it makes about annotators. These assumptions are often violated in practice, in which case annotator ratings cease to reflect their preferences. The most egregious violations come from using Likert scales, which provably reverse the direction of the true preference in certain cases. We suggest improvements to the standard protocol to make it more theoretically sound, but even in its improved form, it cannot be used to evaluate open-ended tasks like story generation. For the latter, we propose a new human evaluation protocol called system-level probabilistic assessment (SPA). When human evaluation of stories is done with SPA, we can recover the ordering of GPT-3 models by size, with statistically significant results. However, when human evaluation is done with the standard protocol, less than half of the expected preferences can be recovered (e.g., there is no significant difference between curie and davinci, despite using a highly powered test).",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.406.pdf"
    },
    {
        "title": "BERT in Plutarch\u2019s Shadows",
        "authors": [
            "Ivan Yamshchikov",
            "Alexey Tikhonov",
            "Yorgos Pantis",
            "Charlotte Schubert",
            "J\u00fcrgen Jost"
        ],
        "published": "2022",
        "summary": "The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea (ca. 45-120 CE) also contains several texts which, according to current scholarly opinion, did not originate with him and are therefore attributed to an anonymous author Pseudo-Plutarch. These include, in particular, the work Placita Philosophorum (Quotations and Opinions of the Ancient Philosophers), which is extremely important for the history of ancient philosophy. Little is known about the identity of that anonymous author and its relation to other authors from the same period. This paper presents a BERT language model for Ancient Greek. The model discovers previously unknown statistical properties relevant to these literary, philosophical, and historical problems and can shed new light on this authorship question. In particular, the Placita Philosophorum, together with one of the other Pseudo-Plutarch texts, shows similarities with the texts written by authors from an Alexandrian context (2nd/3rd century CE).",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.407.pdf"
    },
    {
        "title": "Fine-tuned Language Models are Continual Learners",
        "authors": [
            "Thomas Scialom",
            "Tuhin Chakrabarty",
            "Smaranda Muresan"
        ],
        "published": "2022",
        "summary": "Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that Fine-tuned Language Models can be continual learners.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.410.pdf"
    },
    {
        "title": "AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis",
        "authors": [
            "Sabyasachi Kamila",
            "Walid Magdy",
            "Sourav Dutta",
            "MingXue Wang"
        ],
        "published": "2022",
        "summary": "Aspect Based Sentiment Analysis is a dominant research area with potential applications in social media analytics, business, finance, and health. Prior works in this area are primarily based on supervised methods, with a few techniques using weak supervision limited to predicting a single aspect category per review sentence. In this paper, we present an extremely weakly supervised multi-label Aspect Category Sentiment Analysis framework which does not use any labelled data. We only rely on a single word per class as an initial indicative information. We further propose an automatic word selection technique to choose these seed categories and sentiment words. We explore unsupervised language model post-training to improve the overall performance, and propose a multi-label generator model to generate multiple aspect category-sentiment pairs per review sentence. Experiments conducted on four benchmark datasets showcase our method to outperform other weakly supervised baselines by a significant margin.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.412.pdf"
    },
    {
        "title": "Bernice: A Multilingual Pre-trained Encoder for Twitter",
        "authors": [
            "Alexandra DeLucia",
            "Shijie Wu",
            "Aaron Mueller",
            "Carlos Aguirre",
            "Philip Resnik",
            "Mark Dredze"
        ],
        "published": "2022",
        "summary": "The language of Twitter differs significantly from that of other domains commonly included in large language model training. While tweets are typically multilingual and contain informal language, including emoji and hashtags, most pre-trained language models for Twitter are either monolingual, adapted from other domains rather than trained exclusively on Twitter, or are trained on a limited amount of in-domain Twitter data.We introduce Bernice, the first multilingual RoBERTa language model trained from scratch on 2.5 billion tweets with a custom tweet-focused tokenizer. We evaluate on a variety of monolingual and multilingual Twitter benchmarks, finding that our model consistently exceeds or matches the performance of a variety of models adapted to social media data as well as strong multilingual baselines, despite being trained on less data overall.We posit that it is more efficient compute- and data-wise to train completely on in-domain data with a specialized domain-specific tokenizer.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.415.pdf"
    },
    {
        "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
        "authors": [
            "Weiyan Shi",
            "Ryan Shea",
            "Si Chen",
            "Chiyuan Zhang",
            "Ruoxi Jia",
            "Zhou Yu"
        ],
        "published": "2022",
        "summary": "Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.425.pdf"
    },
    {
        "title": "Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change",
        "authors": [
            "Zhaochen Su",
            "Zecheng Tang",
            "Xinyan Guan",
            "Lijun Wu",
            "Min Zhang",
            "Juntao Li"
        ],
        "published": "2022",
        "summary": "Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a misalignment. While effective to some extent but is far from being addressed on both the language modeling and downstream tasks. In this paper, we empirically observe that temporal generalization is closely affiliated with lexical semantic change, which is one of the essential phenomena of natural languages. Based on this observation, we propose a simple yet effective lexical-level masking strategy to post-train a converged language model. Experiments on two pre-trained language models, two different classification tasks, and four benchmark datasets demonstrate the effectiveness of our proposed method over existing temporal adaptation methods, i.e., continual training with new data. Our code is available at https://github.com/zhaochen0110/LMLM.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.428.pdf"
    },
    {
        "title": "ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Zhongyang Li",
            "Li Du",
            "Ting Liu",
            "Bing Qin",
            "Yi Zheng",
            "Baoxing Huai"
        ],
        "published": "2022",
        "summary": "Causal chain reasoning (CCR) is an essential ability for many decision-making AI systems, which requires the model to build reliable causal chains by connecting causal pairs. However, CCR suffers from two main transitive problems: threshold effect and scene drift. In other words, the causal pairs to be spliced may have a conflicting threshold boundary or scenario.To address these issues, we propose a novel Reliable Causal chain reasoning framework (ReCo), which introduces exogenous variables to represent the threshold and scene factors of each causal pair within the causal chain, and estimates the threshold and scene contradictions across exogenous variables via structural causal recurrent neural networks (SRNN). Experiments show that ReCo outperforms a series of strong baselines on both Chinese and English CCR datasets. Moreover, by injecting reliable causal chain knowledge distilled by ReCo, BERT can achieve better performances on four downstream causal-related tasks than BERT models enhanced by other kinds of knowledge.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.431.pdf"
    },
    {
        "title": "G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks",
        "authors": [
            "Zhongwei Wan",
            "Yichun Yin",
            "Wei Zhang",
            "Jiaxin Shi",
            "Lifeng Shang",
            "Guangyong Chen",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "General pre-trained language models (PLMs), such as BERT, have achieved remarkable performance on various NLP tasks. Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this domain-adaptive pre-training (DAPT (CITATION)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of Memory-Augmented Pre-trained Language Model (MAP), which augments the domain-specific PLM by a memory built from the frozen general PLM without losing the general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmentation strategies are explored to build memory and fusion memory into domain-specific PLM. We demonstrate the effectiveness of MAP on different domains (biomedical and computer science publications, news, and reviews) and different kinds (text classification, QA, NER) of tasks, and the extensive results show that the proposed MAP can achieve SOTA results on these tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.441.pdf"
    },
    {
        "title": "Textual Manifold-based Defense Against Natural Language Adversarial Examples",
        "authors": [
            "Dang Nguyen Minh",
            "Anh Tuan Luu"
        ],
        "published": "2022",
        "summary": "Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.443.pdf"
    },
    {
        "title": "Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters",
        "authors": [
            "Hongyu Zhao",
            "Hao Tan",
            "Hongyuan Mei"
        ],
        "published": "2022",
        "summary": "Adapter-tuning is a paradigm that transfers a pretrained language model to downstream tasks by adding and tuning a small number of new parameters. Previously proposed adapter architectures are all feed-forward neural networks. In this paper, we investigate the effectiveness of using tiny-attention\u2014i.e., attention with extremely small per-head dimensionality\u2014as adapters. Our tiny-attention adapter learns to modify the hidden states at each position directly conditioned on the hidden states at all the other positions, which is missed by the previously proposed adapters. Moreover, we view its multiple attention heads as a mixture of experts and propose to average their weights during deployment, which further reduces its inference computation cost. On the GLUE benchmark, our tiny-attention adapter outperforms the other parameter-efficient transfer learning methods as well as full fine-tuning while only updating 0.05% of the parameters. On the FewGLUE benchmark, its performance is comparable to that of GPT-3 and PET.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.444.pdf"
    },
    {
        "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts",
        "authors": [
            "Akari Asai",
            "Mohammadreza Salehi",
            "Matthew Peters",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022",
        "summary": "This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts\u2014small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.446.pdf"
    },
    {
        "title": "Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders",
        "authors": [
            "Minsoo Kim",
            "Sihwa Lee",
            "Suk-Jin Hong",
            "Du-Seong Chang",
            "Jungwook Choi"
        ],
        "published": "2022",
        "summary": "Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve the accuracy of the student model with the reduced-precision weight parameters. However, little is understood about which of the various KD approaches best fits the QAT of Transformers. In this work, we provide an in-depth analysis of the mechanism of KD on attention recovery of quantized large Transformers. In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information. Therefore, we propose two KD methods; attention-map and attention-output losses. Furthermore, we explore the unification of both losses to address task-dependent preference between attention-map and output losses. The experimental results on various Transformer encoder models demonstrate that the proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit weight quantization.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.450.pdf"
    },
    {
        "title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding",
        "authors": [
            "Rishabh Bhardwaj",
            "Amrita Saha",
            "Steven C.H. Hoi",
            "Soujanya Poria"
        ],
        "published": "2022",
        "summary": "Prompt Tuning has been largely successful as a parameter-efficient method of conditioning large-scale pre-trained language models to perform downstream tasks. Thus far, soft prompt tuning learns a fixed set of task-specific continuous vectors, i.e., soft tokens that remain static across the task samples. A fixed prompt, however, may not generalize well to the diverse kinds of inputs the task comprises. In order to address this, we propose Vector-quantized Input-contextualized Prompts (VIP) as an extension to the soft prompt tuning framework. VIP particularly focuses on two aspects\u2014contextual prompts that learns input-specific contextualization of the soft prompt tokens through a small-scale sentence encoder and quantized prompts that maps the contextualized prompts to a set of learnable codebook vectors through a Vector quantization network. On various language understanding tasks like SuperGLUE, QA, Relation classification, NER and NLI, VIP outperforms the soft prompt tuning (PT) baseline by an average margin of 1.19%. Further, our generalization studies show that VIP learns more robust prompt representations, surpassing PT by a margin of 0.6% - 5.3% on Out-of-domain QA and NLI tasks respectively, and by 0.75% on Multi-Task setup over 4 tasks spanning across 12 domains.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.455.pdf"
    },
    {
        "title": "Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
        "authors": [
            "Tuhin Chakrabarty",
            "Vishakh Padmakumar",
            "He He"
        ],
        "published": "2022",
        "summary": "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present CoPoet, a collaborative poetry writing system, with the goal of to study if LLM\u2019s actually improve the quality of the generated content. In contrast to auto-completing a user\u2019s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about \u2018love\u2019 or Write a sentence ending in \u2018fly\u2019. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change, which are preferred by third-party evaluators over poems written without the system.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.460.pdf"
    },
    {
        "title": "Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples",
        "authors": [
            "Linlin Liu",
            "Xin Li",
            "Ruidan He",
            "Lidong Bing",
            "Shafiq Joty",
            "Luo Si"
        ],
        "published": "2022",
        "summary": "Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.462.pdf"
    },
    {
        "title": "XLM-D: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation",
        "authors": [
            "Yong Wang",
            "Shilin He",
            "Guanhua Chen",
            "Yun Chen",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "Pre-training language models have achieved thriving success in numerous natural language understanding and autoregressive generation tasks, but non-autoregressive generation in applications such as machine translation has not sufficiently benefited from the pre-training paradigm. In this work, we establish the connection between a pre-trained masked language model (MLM) and non-autoregressive generation on machine translation. From this perspective, we present XLM-D, which seamlessly transforms an off-the-shelf cross-lingual pre-training model into a non-autoregressive translation (NAT) model with a lightweight yet effective decorator. Specifically, the decorator ensures the representation consistency of the pre-trained model and brings only one additional trainable parameter. Extensive experiments on typical translation datasets show that our models obtain state-of-the-art performance while realizing the inference speed-up by 19.9x. One striking result is that on WMT14 En-De, our XLM-D obtains 29.80 BLEU points with multiple iterations, which outperforms the previous mask-predict model by 2.77 points.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.466.pdf"
    },
    {
        "title": "FLUTE: Figurative Language Understanding through Textual Explanations",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Debanjan Ghosh",
            "Smaranda Muresan"
        ],
        "published": "2022",
        "summary": "Figurative language understanding has been recently framed as a recognizing textual entailment (RTE) task (a.k.a. natural language inference (NLI)). However, similar to classical RTE/NLI datasets they suffer from spurious correlations and annotation artifacts. To tackle this problem, work on NLI has built explanation-based datasets such as eSNLI, allowing us to probe whether language models are right for the right reasons. Yet no such data exists for figurative language, making it harder to assess genuine understanding of such expressions. To address this issue, we release FLUTE, a dataset of 9,000 figurative NLI instances with explanations, spanning four categories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through a Human-AI collaboration framework based on GPT-3, crowd workers, and expert annotators. We show how utilizing GPT-3 in conjunction with human annotators (novices and experts) can aid in scaling up the creation of datasets even for such complex linguistic phenomena as figurative language. The baseline performance of the T5 model fine-tuned on FLUTE shows that our dataset can bring us a step closer to developing models that understand figurative language through textual explanations.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.481.pdf"
    },
    {
        "title": "Let the CAT out of the bag: Contrastive Attributed explanations for Text",
        "authors": [
            "Saneem Chemmengath",
            "Amar Prakash Azad",
            "Ronny Luss",
            "Amit Dhurandhar"
        ],
        "published": "2022",
        "summary": "Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural language text data with a novel twist as we build and exploit attribute classifiers leading to more semantically meaningful explanations. To ensure that our contrastive generated text has the fewest possible edits with respect to the original text, while also being fluent and close to a human generated contrastive, we resort to a minimal perturbation approach regularized using a BERT language model and attribute classifiers trained on available attributes. We show through qualitative examples and a user study that our method not only conveys more insight because of these attributes, but also leads to better quality (contrastive) text. Quantitatively, we show that our method outperforms other state-of-the-art methods across four data sets on four benchmark metrics.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.484.pdf"
    },
    {
        "title": "One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks",
        "authors": [
            "Manuel Senge",
            "Timour Igamberdiev",
            "Ivan Habernal"
        ],
        "published": "2022",
        "summary": "Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this short paper, we provide an extensive analysis of different privacy preserving strategies on seven downstream datasets in five different \u2018typical\u2019 NLP tasks with varying complexity using modern neural models based on BERT and XtremeDistil architectures. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.496.pdf"
    },
    {
        "title": "Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network",
        "authors": [
            "Junho Kim",
            "Jun-Hyung Park",
            "Mingyu Lee",
            "Wing-Lam Mok",
            "Joon-Young Choi",
            "SangKeun Lee"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have achieved remarkable successes in natural language processing tasks, coming at the cost of increasing model size. To address this issue, knowledge distillation (KD) has been widely applied to compress language models. However, typical KD approaches for language models have overlooked the difficulty of training examples, suffering from incorrect teacher prediction transfer and sub-efficient training. In this paper, we propose a novel KD framework, Tutor-KD, which improves the distillation effectiveness by controlling the difficulty of training examples during pre-training. We introduce a tutor network that generates samples that are easy for the teacher but difficult for the student, with training on a carefully designed policy gradient method. Experimental results show that Tutor-KD significantly and consistently outperforms the state-of-the-art KD methods with variously sized student models on the GLUE benchmark, demonstrating that the tutor can effectively generate training examples for the student.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.498.pdf"
    },
    {
        "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
        "authors": [
            "Mingyu Lee",
            "Jun-Hyung Park",
            "Junho Kim",
            "Kang-Min Kim",
            "SangKeun Lee"
        ],
        "published": "2022",
        "summary": "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.502.pdf"
    },
    {
        "title": "A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss",
        "authors": [
            "Wenbiao Li",
            "Wang Ziyang",
            "Yunfang Wu"
        ],
        "published": "2022",
        "summary": "Readability assessment is a basic research task in the field of education. Traditional methods mainly employ machine learning classifiers with hundreds of linguistic features. Although the deep learning model has become the prominent approach for almost all NLP tasks, it is less explored for readability assessment. In this paper, we propose a BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL) to determine the difficulty level of a given text. First, we introduce topic features guided by difficulty knowledge to complement the traditional linguistic features. From the linguistic features, we extract really useful orthogonal features to supplement BERT representations by means of projection filtering. Furthermore, we design a length-balanced loss to handle the greatly varying length distribution of the readability data. We conduct experiments on three English benchmark datasets and one Chinese dataset, and the experimental results show that our proposed model achieves significant improvements over baseline models. Interestingly, our proposed model achieves comparable results with human experts in consistency test.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.504.pdf"
    },
    {
        "title": "Don\u2019t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",
        "authors": [
            "Mozes van de Kar",
            "Mengzhou Xia",
            "Danqi Chen",
            "Mikel Artetxe"
        ],
        "published": "2022",
        "summary": "Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternative mining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.509.pdf"
    },
    {
        "title": "Parameter-Efficient Tuning Makes a Good Classification Head",
        "authors": [
            "Zhuoyi Yang",
            "Ming Ding",
            "Yanhui Guo",
            "Qingsong Lv",
            "Jie Tang"
        ],
        "published": "2022",
        "summary": "In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.514.pdf"
    },
    {
        "title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning",
        "authors": [
            "Hongkuan Zhang",
            "Saku Sugawara",
            "Akiko Aizawa",
            "Lei Zhou",
            "Ryohei Sasano",
            "Koichi Takeda"
        ],
        "published": "2022",
        "summary": "Image captioning models require the high-level generalization ability to describe the contents of various images in words. Most existing approaches treat the image\u2013caption pairs equally in their training without considering the differences in their learning difficulties. Several image captioning approaches introduce curriculum learning methods that present training data with increasing levels of difficulty. However, their difficulty measurements are either based on domain-specific features or prior model training. In this paper, we propose a simple yet efficient difficulty measurement for image captioning using cross-modal similarity calculated by a pretrained vision\u2013language model. Experiments on the COCO and Flickr30k datasets show that our proposed approach achieves superior performance and competitive convergence speed to baselines without requiring heuristics or incurring additional training costs. Moreover, the higher model performance on difficult examples and unseen data also demonstrates the generalization ability.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.516.pdf"
    },
    {
        "title": "Differentiable Data Augmentation for Contrastive Sentence Representation Learning",
        "authors": [
            "Tianduo Wang",
            "Wei Lu"
        ],
        "published": "2022",
        "summary": "Fine-tuning a pre-trained language model via the contrastive learning framework with a large amount of unlabeled sentences or labeled sentence pairs is a common way to obtain high-quality sentence representations. Although the contrastive learning framework has shown its superiority on sentence representation learning over previous methods, the potential of such a framework is under-explored so far due to the simple method it used to construct positive pairs. Motivated by this, we propose a method that makes hard positives from the original training examples. A pivotal ingredient of our approach is the use of prefix that attached to a pre-trained language model, which allows for differentiable data augmentation during contrastive learning. Our method can be summarized in two steps: supervised prefix-tuning followed by joint contrastive fine-tuning with unlabeled or labeled examples. Our experiments confirm the effectiveness of our data augmentation approach. The proposed method yields significant improvements over existing methods under both semi-supervised and supervised settings. Our experiments under a low labeled data setting also show that our method is more label-efficient than the state-of-the-art contrastive learning methods.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.520.pdf"
    },
    {
        "title": "Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation",
        "authors": [
            "Mengting Hu",
            "Yike Wu",
            "Hang Gao",
            "Yinhao Bai",
            "Shiwan Zhao"
        ],
        "published": "2022",
        "summary": "Recently, aspect sentiment quad prediction (ASQP) has become a popular task in the field of aspect-level sentiment analysis. Previous work utilizes a predefined template to paraphrase the original sentence into a structure target sequence, which can be easily decoded as quadruplets of the form (aspect category, aspect term, opinion term, sentiment polarity). The template involves the four elements in a fixed order. However, we observe that this solution contradicts with the order-free property of the ASQP task, since there is no need to fix the template order as long as the quadruplet is extracted correctly. Inspired by the observation, we study the effects of template orders and find that some orders help the generative model achieve better performance. It is hypothesized that different orders provide various views of the quadruplet. Therefore, we propose a simple but effective method to identify the most proper orders, and further combine multiple proper templates as data augmentation to improve the ASQP task. Specifically, we use the pre-trained language model to select the orders with minimal entropy. By fine-tuning the pre-trained language model with these template orders, our approach improves the performance of quad prediction, and outperforms state-of-the-art methods significantly in low-resource settings.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.538.pdf"
    },
    {
        "title": "LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling",
        "authors": [
            "Dongsheng Chen",
            "Chaofan Tao",
            "Lu Hou",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022",
        "summary": "Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose LiteVL, which adapts a pre-trained image-language model BLIP into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed LiteVL even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.545.pdf"
    },
    {
        "title": "Normalizing Mutual Information for Robust Adaptive Training for Translation",
        "authors": [
            "Youngwon Lee",
            "Changmin Lee",
            "Hojin Lee",
            "Seung-won Hwang"
        ],
        "published": "2022",
        "summary": "Despite the success of neural machine translation models, tensions between fluency of optimizing target language modeling and source-faithfulness remain as challenges. Previously, Conditional Bilingual Mutual Information (CBMI), a scoring metric for the importance of target sentences and tokens, was proposed to encourage fluent and faithful translations. The score is obtained by combining the probability from the translation model and the target language model, which is then used to assign different weights to losses from sentences and tokens. Meanwhile, we argue this metric is not properly normalized, for which we propose Normalized Pointwise Mutual Information (NPMI). NPMI utilizes an additional language model on source language to approximate the joint likelihood of source-target pair and the likelihood of the source, which is then used for normalizing the score. We showed that NPMI better captures the dependence between source-target and that NPMI-based token-level adaptive training brings improvements over baselines with empirical results from En-De, De-En, and En-Ro translation tasks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.547.pdf"
    },
    {
        "title": "Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?",
        "authors": [
            "Ningyu Xu",
            "Tao Gui",
            "Ruotian Ma",
            "Qi Zhang",
            "Jingting Ye",
            "Menghan Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022",
        "summary": "Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.552.pdf"
    },
    {
        "title": "FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information",
        "authors": [
            "Yijia Shao",
            "Mengyu Zhou",
            "Yifan Zhong",
            "Tao Wu",
            "Hongwei Han",
            "Shi Han",
            "Gideon Huang",
            "Dongmei Zhang"
        ],
        "published": "2022",
        "summary": "Online forms are widely used to collect data from human and have a multi-billion market. Many software products provide online services for creating semi-structured forms where questions and descriptions are organized by predefined structures. However, the design and creation process of forms is still tedious and requires expert knowledge. To assist form designers, in this work we present FormLM to model online forms (by enhancing pre-trained language model with form structural information) and recommend form creation ideas (including question / options recommendations and block type suggestion). For model training and evaluation, we collect the first public online form dataset with 62K online forms. Experiment results show that FormLM significantly outperforms general-purpose language models on all tasks, with an improvement by 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms of ROUGE-1 and Macro-F1, respectively.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.557.pdf"
    },
    {
        "title": "Multitask Instruction-based Prompting for Fallacy Recognition",
        "authors": [
            "Tariq Alhindi",
            "Tuhin Chakrabarty",
            "Elena Musi",
            "Smaranda Muresan"
        ],
        "published": "2022",
        "summary": "Fallacies are used as seemingly valid arguments to support a position and persuade the audience about its validity. Recognizing fallacies is an intrinsically difficult task both for humans and machines. Moreover, a big challenge for computational models lies in the fact that fallacies are formulated differently across the datasets with differences in the input format (e.g., question-answer pair, sentence with fallacy fragment), genre (e.g., social media, dialogue, news), as well as types and number of fallacies (from 5 to 18 types per dataset). To move towards solving the fallacy recognition task, we approach these differences across datasets as multiple tasks and show how instruction-based prompting in a multitask setup based on the T5 model improves the results against approaches built for a specific dataset such as T5, BERT or GPT-3. We show the ability of this multitask prompting approach to recognize 28 unique fallacies across domains and genres and study the effect of model size and prompt choice by analyzing the per-class (i.e., fallacy type) results. Finally, we analyze the effect of annotation quality on model performance, and the feasibility of complementing this approach with external knowledge.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.560.pdf"
    },
    {
        "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
        "authors": [
            "Miao Chen",
            "Xinjiang Lu",
            "Tong Xu",
            "Yanyan Li",
            "Zhou Jingbo",
            "Dejing Dou",
            "Hui Xiong"
        ],
        "published": "2022",
        "summary": "Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.562.pdf"
    },
    {
        "title": "IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models",
        "authors": [
            "Chenguang Wang",
            "Xiao Liu",
            "Dawn Song"
        ],
        "published": "2022",
        "summary": "We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer \u201cfill-in-the-blank\u201d questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.576.pdf"
    },
    {
        "title": "Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding",
        "authors": [
            "Keqin Chen",
            "Richong Zhang",
            "Samuel Mensah",
            "Yongyi Mao"
        ],
        "published": "2022",
        "summary": "Weakly supervised phrase grounding aims to learn an alignment between phrases in a caption and objects in a corresponding image using only caption-image annotations, i.e., without phrase-object annotations. Previous methods typically use a caption-image contrastive loss to indirectly supervise the alignment between phrases and objects, which hinders the maximum use of the intrinsic structure of the multimodal data and leads to unsatisfactory performance. In this work, we directly use the phrase-object contrastive loss in the condition that no positive annotation is available in the first place. Specifically, we propose a novel contrastive learning framework based on the expectation-maximization algorithm that adaptively refines the target prediction. Experiments on two widely used benchmarks, Flickr30K Entities and RefCOCO+, demonstrate the effectiveness of our framework. We obtain 63.05% top-1 accuracy on Flickr30K Entities and 59.51%/43.46% on RefCOCO+ TestA/TestB, outperforming the previous methods by a large margin, even surpassing a previous SoTA that uses a pre-trained vision-language model. Furthermore, we deliver a theoretical analysis of the effectiveness of our method from the perspective of the maximum likelihood estimate with latent variables.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.586.pdf"
    },
    {
        "title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts",
        "authors": [
            "Ting Jiang",
            "Jian Jiao",
            "Shaohan Huang",
            "Zihan Zhang",
            "Deqing Wang",
            "Fuzhen Zhuang",
            "Furu Wei",
            "Haizhen Huang",
            "Denvy Deng",
            "Qi Zhang"
        ],
        "published": "2022",
        "summary": "We propose PromptBERT, a novel contrastive learning method for learning better sentence representation. We firstly analysis the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose the first prompt-based sentence embeddings method and discuss two prompt representing methods and three prompt searching methods to make BERT achieve better sentence embeddings .Moreover, we propose a novel unsupervised training objective by the technology of template denoising, which substantially shortens the performance gap between the supervised and unsupervised settings. Extensive experiments show the effectiveness of our method. Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.603.pdf"
    },
    {
        "title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering",
        "authors": [
            "Jiacheng Liu",
            "Skyler Hallinan",
            "Ximing Lu",
            "Pengfei He",
            "Sean Welleck",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent.We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.611.pdf"
    },
    {
        "title": "Few-shot Learning with Multilingual Generative Language Models",
        "authors": [
            "Xi Victoria Lin",
            "Todor Mihaylov",
            "Mikel Artetxe",
            "Tianlu Wang",
            "Shuohui Chen",
            "Daniel Simig",
            "Myle Ott",
            "Naman Goyal",
            "Shruti Bhosale",
            "Jingfei Du",
            "Ramakanth Pasunuru",
            "Sam Shleifer",
            "Punit Singh Koura",
            "Vishrav Chaudhary",
            "Brian O\u2019Horo",
            "Jeff Wang",
            "Luke Zettlemoyer",
            "Zornitsa Kozareva",
            "Mona Diab",
            "Veselin Stoyanov",
            "Xian Li"
        ],
        "published": "2022",
        "summary": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.616.pdf"
    },
    {
        "title": "Active Example Selection for In-Context Learning",
        "authors": [
            "Yiming Zhang",
            "Shi Feng",
            "Chenhao Tan"
        ],
        "published": "2022",
        "summary": "With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.622.pdf"
    },
    {
        "title": "Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing",
        "authors": [
            "Linlu Qiu",
            "Peter Shaw",
            "Panupong Pasupat",
            "Tianze Shi",
            "Jonathan Herzig",
            "Emily Pitler",
            "Fei Sha",
            "Kristina Toutanova"
        ],
        "published": "2022",
        "summary": "Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.624.pdf"
    },
    {
        "title": "Improving Large-scale Paraphrase Acquisition and Generation",
        "authors": [
            "Yao Dou",
            "Chao Jiang",
            "Wei Xu"
        ],
        "published": "2022",
        "summary": "This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert (MultiPIT_expert) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MultiPIT_NMR) and a large automatically constructed training set (MultiPIT_Auto) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MultiPIT_Auto generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.631.pdf"
    },
    {
        "title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2022",
        "summary": "Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process of cue-based retrieval, in which attention over multiple targets is taken to generate interference and latency during retrieval. Under this framework, this work first defines an entropy-based predictor that quantifies the diffuseness of self-attention, as well as distance-based predictors that capture the incremental change in attention patterns across timesteps. Moreover, following recent studies that question the informativeness of attention weights, we also experiment with alternative methods for incorporating vector norms into attention weights. Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.632.pdf"
    },
    {
        "title": "Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations",
        "authors": [
            "Minh Van Nguyen",
            "Bonan Min",
            "Franck Dernoncourt",
            "Thien Nguyen"
        ],
        "published": "2022",
        "summary": "Extracting entities, events, event arguments, and relations (i.e., task instances) from text represents four main challenging tasks in information extraction (IE), which have been solved jointly (JointIE) to boost the overall performance for IE. As such, previous work often leverages two types of dependencies between the tasks, i.e., cross-instance and cross-type dependencies representing relatedness between task instances and correlations between information types of the tasks. However, the cross-task dependencies in prior work are not optimal as they are only designed manually according to some task heuristics. To address this issue, we propose a novel model for JointIE that aims to learn cross-task dependencies from data. In particular, we treat each task instance as a node in a dependency graph where edges between the instances are inferred through information from different layers of a pretrained language model (e.g., BERT). Furthermore, we utilize the Chow-Liu algorithm to learn a dependency tree between information types for JointIE by seeking to approximate the joint distribution of the types from data. Finally, the Chow-Liu dependency tree is used to generate cross-type patterns, serving as anchor knowledge to guide the learning of representations and dependencies between instances for JointIE. Experimental results show that our proposed model significantly outperforms strong JointIE baselines over four datasets with different languages.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.634.pdf"
    },
    {
        "title": "Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence",
        "authors": [
            "Chris Callison-Burch",
            "Gaurav Singh Tomar",
            "Lara J. Martin",
            "Daphne Ippolito",
            "Suma Bailis",
            "David Reitter"
        ],
        "published": "2022",
        "summary": "AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game\u2014i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.637.pdf"
    },
    {
        "title": "Analogical Math Word Problems Solving with Enhanced Problem-Solution Association",
        "authors": [
            "Zhenwen Liang",
            "Jipeng Zhang",
            "Xiangliang Zhang"
        ],
        "published": "2022",
        "summary": "Math word problem (MWP) solving is an important task in question answering which requires human-like reasoning ability. Analogical reasoning has long been used in mathematical education, as it enables students to apply common relational structures of mathematical situations to solve new problems. In this paper, we propose to build a novel MWP solver by leveraging analogical MWPs, which advance the solver\u2019s generalization ability across different kinds of MWPs. The key idea, named analogy identification, is to associate the analogical MWP pairs in a latent space, i.e., encoding an MWP close to another analogical MWP, while leaving away from the non-analogical ones. Moreover, a solution discriminator is integrated into the MWP solver to enhance the association between an MWP and its true solution. The evaluation results verify that our proposed analogical learning strategy promotes the performance of MWP-BERT on Math23k over the state-of-the-art model Generate2Rank, with 5 times fewer parameters in the encoder. We also find that our model has a stronger generalization ability in solving difficult MWPs due to the analogical learning from easy MWPs.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.643.pdf"
    },
    {
        "title": "Perturbation Augmentation for Fairer NLP",
        "authors": [
            "Rebecca Qian",
            "Candace Ross",
            "Jude Fernandes",
            "Eric Michael Smith",
            "Douwe Kiela",
            "Adina Williams"
        ],
        "published": "2022",
        "summary": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.646.pdf"
    },
    {
        "title": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
        "authors": [
            "Ziniu Hu",
            "Yichong Xu",
            "Wenhao Yu",
            "Shuohang Wang",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Kai-Wei Chang",
            "Yizhou Sun"
        ],
        "published": "2022",
        "summary": "Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model(OREO-LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM.By adopting OREO-LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning\u2019s capacity to infer missing relational facts. In addition, OREO-LM provides reasoning paths as rationales to interpret the model\u2019s decision.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.650.pdf"
    },
    {
        "title": "Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model",
        "authors": [
            "Dongkyu Lee",
            "Zhiliang Tian",
            "Yingxiu Zhao",
            "Ka Chun Cheung",
            "Nevin Zhang"
        ],
        "published": "2022",
        "summary": "In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: \u201cwhen to distill such knowledge.\u201d The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.665.pdf"
    },
    {
        "title": "Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling",
        "authors": [
            "Vidhisha Balachandran",
            "Hannaneh Hajishirzi",
            "William Cohen",
            "Yulia Tsvetkov"
        ],
        "published": "2022",
        "summary": "Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets\u2014 CNN/DM and XSum\u2014we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model\u2014FactEdit\u2014improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.667.pdf"
    },
    {
        "title": "An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks",
        "authors": [
            "Changlong Yu",
            "Tianyi Xiao",
            "Lingpeng Kong",
            "Yangqiu Song",
            "Wilfred Ng"
        ],
        "published": "2022",
        "summary": "Though linguistic knowledge emerges during large-scale language model pretraining, recent work attempt to explicitly incorporate human-defined linguistic priors into task-specific fine-tuning. Infusing language models with syntactic or semantic knowledge from parsers has shown improvements on many language understanding tasks. To further investigate the effectiveness of structural linguistic priors, we conduct empirical study of replacing parsed graphs or trees with trivial ones (rarely carrying linguistic knowledge e.g., balanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs achieves competitive or even better performance in fully-supervised and few-shot settings. It reveals that the gains might not be significantly attributed to explicit linguistic priors but rather to more feature interactions brought by fusion layers. Hence we call for attention to using trivial graphs as necessary baselines to design advanced knowledge fusion methods in the future.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.684.pdf"
    },
    {
        "title": "Adapting a Language Model While Preserving its General Knowledge",
        "authors": [
            "Zixuan Ke",
            "Yijia Shao",
            "Haowei Lin",
            "Hu Xu",
            "Lei Shu",
            "Bing Liu"
        ],
        "published": "2022",
        "summary": "Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.693.pdf"
    },
    {
        "title": "Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation",
        "authors": [
            "Raymond Li",
            "Wen Xiao",
            "Linzi Xing",
            "Lanjun Wang",
            "Gabriel Murray",
            "Giuseppe Carenini"
        ],
        "published": "2022",
        "summary": "The multi-head self-attention mechanism of the transformer model has been thoroughly investigated recently. In one vein of study, researchers are interested in understanding why and how transformers work. In another vein, researchers propose new attention augmentation methods to make transformers more accurate, efficient and interpretable. In this paper, we combine these two lines of research in a human-in-the-loop pipeline to first discover important task-specific attention patterns. Then those patterns are injected, not only to smaller models, but also to the original model. The benefits of our pipeline and discovered patterns are demonstrated in two case studies with extractive summarization and topic segmentation. After discovering interpretable patterns in BERT-based models fine-tuned for the two downstream tasks, experiments indicate that when we inject the patterns into attention heads, the models show considerable improvements in accuracy and efficiency.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.694.pdf"
    },
    {
        "title": "Continual Training of Language Models for Few-Shot Learning",
        "authors": [
            "Zixuan Ke",
            "Haowei Lin",
            "Yijia Shao",
            "Hu Xu",
            "Lei Shu",
            "Bing Liu"
        ],
        "published": "2022",
        "summary": "Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.695.pdf"
    },
    {
        "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
        "authors": [
            "Huanru Henry Mao"
        ],
        "published": "2022",
        "summary": "Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99% of attention\u2019s performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.697.pdf"
    },
    {
        "title": "Graph-Induced Transformers for Efficient Multi-Hop Question Answering",
        "authors": [
            "Giwon Hong",
            "Jeonghwan Kim",
            "Junmo Kang",
            "Sung-Hyon Myaeng"
        ],
        "published": "2022",
        "summary": "A graph is a suitable data structure to represent the structural information of text. Recently, multi-hop question answering (MHQA) tasks, which require inter-paragraph/sentence linkages, have come to exploit such properties of a graph. Previous approaches to MHQA relied on leveraging the graph information along with the pre-trained language model (PLM) encoders. However, this trend exhibits the following drawbacks: (i) sample inefficiency while training in a low-resource setting; (ii) lack of reusability due to changes in the model structure or input. Our work proposes the Graph-Induced Transformer (GIT) that applies graph-derived attention patterns directly into a PLM, without the need to employ external graph modules. GIT can leverage the useful inductive bias of graphs while retaining the unperturbed Transformer structure and parameters. Our experiments on HotpotQA successfully demonstrate both the sample efficient characteristic of GIT and its capacity to replace the graph modules while preserving model performance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.702.pdf"
    },
    {
        "title": "A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism",
        "authors": [
            "Jianzhu Bao",
            "Yuhang He",
            "Yang Sun",
            "Bin Liang",
            "Jiachen Du",
            "Bing Qin",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2022",
        "summary": "Argument mining (AM) is a challenging task as it requires recognizing the complex argumentation structures involving multiple subtasks.To handle all subtasks of AM in an end-to-end fashion, previous works generally transform AM into a dependency parsing task.However, such methods largely require complex pre- and post-processing to realize the task transformation.In this paper, we investigate the end-to-end AM task from a novel perspective by proposing a generative framework, in which the expected outputs of AM are framed as a simple target sequence. Then, we employ a pre-trained sequence-to-sequence language model with a constrained pointer mechanism (CPM) to model the clues for all the subtasks of AM in the light of the target sequence. Furthermore, we devise a reconstructed positional encoding (RPE) to alleviate the order biases induced by the autoregressive generation paradigm.Experimental results show that our proposed framework achieves new state-of-the-art performance on two AM benchmarks.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.713.pdf"
    },
    {
        "title": "Quality Scoring of Source Words in Neural Translation Models",
        "authors": [
            "Priyesh Jain",
            "Sunita Sarawagi",
            "Tushar Tomar"
        ],
        "published": "2022",
        "summary": "Word-level quality scores on input source sentences can provide useful feedback to an end-user when translating into an unfamiliar target language. Recent approaches either require training special word-scoring models based on synthetic data or require repeated invocation of the translation model. We propose a simple approach based on comparing the difference of probabilities from two language models. The basic premise of our method is to reason how well each source word is explained by the target sentence as against the source language model. Our approach provides up to five points higher F1 scores and is significantly faster than the state of the art methods on three language pairs. Also, our method does not require training any new model. We release a public dataset on word omissions and mistranslations on a new language pair.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.732.pdf"
    },
    {
        "title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task",
        "authors": [
            "Nyoungwoo Lee",
            "ChaeHun Park",
            "Ho-Jin Choi",
            "Jaegul Choo"
        ],
        "published": "2022",
        "summary": "In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are semantically similar but not relevant to the dialogue context. Recent studies have shown that leveraging these adversarial responses as negative training samples is useful for improving the discriminating power of the selection model. Nevertheless, collecting human-written adversarial responses is expensive, and existing synthesizing methods often have limited scalability. To overcome these limitations, this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model. Experimental results on dialogue selection tasks show that our method outperforms other methods of synthesizing adversarial negative responses. These results suggest that our method can be an effective alternative to human annotators in generating adversarial responses. Our code and dataset will be released if the paper is accepted.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.733.pdf"
    },
    {
        "title": "Towards Compositional Generalization in Code Search",
        "authors": [
            "Hojae Han",
            "Seung-won Hwang",
            "Shuai Lu",
            "Nan Duan",
            "Seungtaek Choi"
        ],
        "published": "2022",
        "summary": "We study compositional generalization, which aims to generalize on unseen combinations of seen structural elements, for code search. Unlike existing approaches of partially pursuing this goal, we study how to extract structural elements, which we name a template that directly targets compositional generalization. Thus we propose CTBERT, or Code Template BERT, representing codes using automatically extracted templates as building blocks. We empirically validate CTBERT on two public code search benchmarks, AdvTest and CSN. Further, we show that templates are complementary to data flow graphs in GraphCodeBERT, by enhancing structural context around variables.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.737.pdf"
    },
    {
        "title": "KOLD: Korean Offensive Language Dataset",
        "authors": [
            "Younghoon Jeong",
            "Juhyun Oh",
            "Jongwon Lee",
            "Jaimeen Ahn",
            "Jihyung Moon",
            "Sungjoon Park",
            "Alice Oh"
        ],
        "published": "2022",
        "summary": "Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because of cultural and linguistic differences. In this paper, we present the Korean Offensive Language Dataset (KOLD) comprising 40,429 comments, which are annotated hierarchically with the type and the target of offensive language, accompanied by annotations of the corresponding text spans. We collect the comments from NAVER news and YouTube platform and provide the titles of the articles and videos as the context information for the annotation process. We use these annotated comments as training data for Korean BERT and RoBERTa models and find that they are effective at offensiveness detection, target classification, and target span detection while having room for improvement for target group classification and offensive span detection. We discover that the target group distribution differs drastically from the existing English datasets, and observe that providing the context information improves the model performance in offensiveness detection (+0.3), target classification (+1.5), and target group classification (+13.1). We publicly release the dataset and baseline models.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.744.pdf"
    },
    {
        "title": "The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",
        "authors": [
            "Leonie Weissweiler",
            "Valentin Hofmann",
            "Abdullatif K\u00f6ksal",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2022",
        "summary": "Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. As a first step towards assessing the compatibility of CxG with the syntactic and semantic knowledge demonstrated by state-of-the-art pretrained language models (PLMs), we present an investigation of their capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC). We conduct experiments examining the classification accuracy of a syntactic probe on the one hand and the models\u2019 behaviour in a semantic application task on the other, with BERT, RoBERTa, and DeBERTa as the example PLMs. Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.746.pdf"
    },
    {
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "authors": [
            "Sewon Min",
            "Xinxi Lyu",
            "Ari Holtzman",
            "Mikel Artetxe",
            "Mike Lewis",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022",
        "summary": "Large language models (LMs) are able to in-context learn\u2014perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\u2014randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.759.pdf"
    },
    {
        "title": "Instance Regularization for Discriminative Language Model Pre-training",
        "authors": [
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Ming Zhou"
        ],
        "published": "2022",
        "summary": "Discriminative pre-trained language models (PrLMs) can be generalized as denoising auto-encoders that work with two procedures, ennoising and denoising. First, an ennoising process corrupts texts with arbitrary noising functions to construct training instances. Then, a denoising language model is trained to restore the corrupted tokens. Existing studies have made progress by optimizing independent strategies of either ennoising or denosing. They treat training instances equally throughout the training process, with little attention on the individual contribution of those instances. To model explicit signals of instance contribution, this work proposes to estimate the complexity of restoring the original sentences from corrupted ones in language model pre-training. The estimations involve the corruption degree in the ennoising data construction process and the prediction confidence in the denoising counterpart. Experimental results on natural language understanding and reading comprehension benchmarks show that our approach improves pre-training efficiency, effectiveness, and robustness. Code is publicly available at https://github.com/cooelf/InstanceReg.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.773.pdf"
    },
    {
        "title": "Improved grammatical error correction by ranking elementary edits",
        "authors": [
            "Alexey Sorokin"
        ],
        "published": "2022",
        "summary": "We offer a two-stage reranking method for grammatical error correction: the first model serves as edit generator, while the second classifies the proposed edits as correct or false. We show how to use both encoder-decoder and sequence labeling models for the first step of our pipeline. We achieve state-of-the-art quality on BEA 2019 English dataset even using weak BERT-GEC edit generator. Combining our roberta-base scorer with state-of-the-art GECToR edit generator, we surpass GECToR by 2-3%. With a larger model we establish a new SOTA on BEA development and test sets. Our model also sets a new SOTA on Russian, despite using smaller models and less data than the previous approaches.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.785.pdf"
    },
    {
        "title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
        "authors": [
            "Malte Ostendorff",
            "Nils Rethmeier",
            "Isabelle Augenstein",
            "Bela Gipp",
            "Georg Rehm"
        ],
        "published": "2022",
        "summary": "Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However, discrete citations enforce a hard cut-off to similarity. This is counter-intuitive to similarity-based learning and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead, we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity, to sample hard-to-learn negatives and positives, and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-the-art on the SciDocs benchmark. Furthermore, we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods. Perhaps surprisingly, even training a general-domain language model this way outperforms baselines pretrained in-domain.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.802.pdf"
    },
    {
        "title": "MedJEx: A Medical Jargon Extraction Model with Wiki\u2019s Hyperlink Span and Contextualized Masked Language Model Score",
        "authors": [
            "Sunjae Kwon",
            "Zonghai Yao",
            "Harmon Jordan",
            "David Levy",
            "Brian Corner",
            "Hong Yu"
        ],
        "published": "2022",
        "summary": "This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms from 18K+ EHR note sentences (MedJ). Then, we introduce a novel medical jargon extraction (MedJEx) model which has been shown to outperform existing state-of-the-art NLP models. First, MedJEx improved the overall performance when it was trained on an auxiliary Wikipedia hyperlink span dataset, where hyperlink spans provide additional Wikipedia articles to explain the spans (or terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that a contextualized masked language model score was beneficial for detecting domain-specific unfamiliar jargon terms. Moreover, our results show that training on the auxiliary Wikipedia hyperlink span datasets improved six out of eight biomedical named entity recognition benchmark datasets. MedJEx is publicly available.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.805.pdf"
    },
    {
        "title": "A Systematic Investigation of Commonsense Knowledge in Large Language Models",
        "authors": [
            "Xiang Lorraine Li",
            "Adhiguna Kuncoro",
            "Jordan Hoffmann",
            "Cyprien de Masson d\u2019Autume",
            "Phil Blunsom",
            "Aida Nematzadeh"
        ],
        "published": "2022",
        "summary": "Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge \u2014 a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs\u2019 ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation is insufficient to achieve human-level commonsense performance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.812.pdf"
    },
    {
        "title": "LogiTorch: A PyTorch-based library for logical reasoning on natural language",
        "authors": [
            "Chadi Helwe",
            "Chlo\u00e9 Clavel",
            "Fabian Suchanek"
        ],
        "published": "2022",
        "summary": "Logical reasoning on natural language is one of the most challenging tasks for deep learning models. There has been an increasing interest in developing new benchmarks to evaluate the reasoning capabilities of language models such as BERT. In parallel, new models based on transformers have emerged to achieve ever better performance on these datasets. However, there is currently no library for logical reasoning that includes such benchmarks and models. This paper introduces LogiTorch, a PyTorch-based library that includes different logical reasoning benchmarks, different models, as well as utility functions such as co-reference resolution. This makes it easy to directly use the preprocessed datasets, to run the models, or to finetune them with different hyperparameters. LogiTorch is open source and can be found on GitHub.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-demos.25.pdf"
    },
    {
        "title": "SEAL: Interactive Tool for Systematic Error Analysis and Labeling",
        "authors": [
            "Nazneen Rajani",
            "Weixin Liang",
            "Lingjiao Chen",
            "Margaret Mitchell",
            "James Zou"
        ],
        "published": "2022",
        "summary": "With the advent of Transformers, large language models (LLMs) have saturated well-known NLP benchmarks and leaderboards with high aggregate performance. However, many times these models systematically fail on tail data or rare groups not obvious in aggregate evaluation. Identifying such problematic data groups is even more challenging when there are no explicit labels (e.g., ethnicity, gender, etc.) and further compounded for NLP datasets due to the lack of visual features to characterize failure modes (e.g., Asian males, animals indoors, waterbirds on land etc.). This paper introduces an interactive Systematic Error Analysis and Labeling (SEAL) tool that uses a two-step approach to first identify high-error slices of data and then, in the second step, introduce methods to give human-understandable semantics to those underperforming slices. We explore a variety of methods for coming up with coherent semantics for the error groups using language models for semantic labeling and a text-to-image model for generating visual features.SEAL is available at https://huggingface.co/spaces/nazneen/seal.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-demos.36.pdf"
    },
    {
        "title": "Paraphrastic Representations at Scale",
        "authors": [
            "John Wieting",
            "Kevin Gimpel",
            "Graham Neubig",
            "Taylor Berg-kirkpatrick"
        ],
        "published": "2022",
        "summary": "We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We release trained models for English, Arabic, German, Spanish, French, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving significantly improved performance from our original papers on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on efficient unsupervised semantic textual similarity, even significantly outperforming supervised BERT-based models like Sentence-BERT (Reimers and Gurevych, 2019). Most importantly, our models are orders of magnitude faster than other strong similarity models and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finally, we add significantly increased functionality to the code bases for training paraphrastic sentence models, easing their use for both inference and for training them for any desired language with parallel data. We also include code to automatically download and preprocess training data.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-demos.38.pdf"
    },
    {
        "title": "Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance",
        "authors": [
            "Yasaman Razeghi",
            "Raja Sekhar Reddy Mekala",
            "Robert L Logan Iv",
            "Matt Gardner",
            "Sameer Singh"
        ],
        "published": "2022",
        "summary": "Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides term frequency statistics for the Pile, which is an 800 GB corpus, accompanied by the precomputed performance of EleutherAI/GPT models on more than 20 NLP benchmarks, including numerical, commonsense reasoning, natural language understanding, and question-answering tasks. Snoopy allows a user to interactively align specific terms in test instances with their frequency in the Pile, enabling exploratory analysis of how term frequency is related to the accuracy of the models, which are hard to discover through automated means. A user can look at correlations over various model sizes and numbers of in-context examples and visualize the result across multiple (potentially aggregated) datasets. Using Snoopy, we show that a researcher can quickly replicate prior analyses for numerical tasks, while simultaneously allowing for much more expansive exploration that was previously challenging. Snoopy is available at https://nlp.ics.uci.edu/snoopy.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-demos.39.pdf"
    },
    {
        "title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
        "authors": [
            "Xiaodi Sun",
            "Sunny Rajagopalan",
            "Priyanka Nigam",
            "Weiyi Lu",
            "Yi Xu",
            "Iman Keivanloo",
            "Belinda Zeng",
            "Trishul Chilimbi"
        ],
        "published": "2022",
        "summary": "Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a fine-tuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end. However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks. Inspired by this work, we study discrete prompt technologies in practice. There are two issues that arise with the standard prompt approach. First, it can overfit on the prompt template. Second, it requires manual effort to formulate the downstream task as a language model problem. In this paper, we propose an improvement to prompt-based fine-tuning that addresses these two issues. We refer to our approach as DynaMaR \u2013 Dynamic Prompt with Mask Token Representation. Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.2.pdf"
    },
    {
        "title": "PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding",
        "authors": [
            "Niranjan Uma Naresh",
            "Ziyan Jiang",
            "Ankit Ankit",
            "Sungjin Lee",
            "Jie Hao",
            "Xing Fan",
            "Chenlei Guo"
        ],
        "published": "2022",
        "summary": "Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer\u2019s query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. Such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. Viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, PENTATRON. The system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. In addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. We also investigate the utility of language model prompts. Through extensive experiments, we show a significant upward movement of the key metric (Exact Match) by up to 500.97% (relative to the baseline).",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.7.pdf"
    },
    {
        "title": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU Tasks",
        "authors": [
            "Charith Peris",
            "Lizhen Tan",
            "Thomas Gueudre",
            "Turan Gojayev",
            "Pan Wei",
            "Gokmen Oz"
        ],
        "published": "2022",
        "summary": "Teacher-student knowledge distillation is a popular technique for compressing today\u2019s prevailing large language models into manageable sizes that fit low-latency downstream applications. Both the teacher and the choice of transfer set used for distillation are crucial ingredients in creating a high quality student. Yet, the generic corpora used to pretrain the teacher and the corpora associated with the downstream target domain are often significantly different, which raises a natural question: should the student be distilled over the generic corpora, so as to learn from high-quality teacher predictions, or over the downstream task corpora to align with finetuning? Our study investigates this trade-off using Domain Classification (DC) and Intent Classification/Named Entity Recognition (ICNER) as downstream tasks. We distill several multilingual students from a larger multilingual LM with varying proportions of generic and task-specific datasets, and report their performance after finetuning on DC and ICNER. We observe significant improvements across tasks and test sets when only task-specific corpora is used. We also report on how the impact of adding task-specific data to the transfer set correlates with the similarity between generic and task-specific data. Our results clearly indicate that, while distillation from a generic LM benefits downstream tasks, students learn better using target domain data even if it comes at the price of noisier teacher predictions. In other words, target domain data still trumps teacher knowledge.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.12.pdf"
    },
    {
        "title": "Grafting Pre-trained Models for Multimodal Headline Generation",
        "authors": [
            "Lingfeng Qiao",
            "Chen Wu",
            "Ye Liu",
            "Haoyuan Peng",
            "Di Yin",
            "Bo Ren"
        ],
        "published": "2022",
        "summary": "Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained language models and video-language models have achieved significant progress in related downstream tasks. However, none of them can be directly applied to multimodal headline architecture where we need both multimodal encoder and sentence decoder. A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities. In this paper, we propose a novel approach to graft the video encoder from the pre-trained video-language model on the generative pre-trained language model. We also present a consensus fusion mechanism for the integration of different components, via inter/intra modality relation. Empirically, experiments show that the grafted model achieves strong results on a brand-new dataset collected from real-world applications.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.25.pdf"
    },
    {
        "title": "Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks",
        "authors": [
            "Arijit Sehanobish",
            "Kawshik Kannan",
            "Nabila Abraham",
            "Anasuya Das",
            "Benjamin Odry"
        ],
        "published": "2022",
        "summary": "Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive. In this work, we propose a simple extension of the Prototypical Networks for few-shot text classification. Our main idea is to replace the class prototypes by Gaussians and introduce a regularization term that encourages the examples to be clustered near the appropriate class centroids. Experimental results show that our method outperforms various strong baselines on 13 public and 4 internal datasets. Furthermore, we use the class distributions as a tool for detecting potential out-of-distribution (OOD) data points during deployment.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.34.pdf"
    },
    {
        "title": "Fast Vocabulary Transfer for Language Model Compression",
        "authors": [
            "Leonidas Gee",
            "Andrea Zugarini",
            "Leonardo Rigutini",
            "Paolo Torroni"
        ],
        "published": "2022",
        "summary": "Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.41.pdf"
    },
    {
        "title": "Zero-Shot Dynamic Quantization for Transformer Inference",
        "authors": [
            "Yousef El-kurdi",
            "Jerry Quinn",
            "Avi Sil"
        ],
        "published": "2022",
        "summary": "We introduce a novel run-time method for significantly reducing the accuracy loss associated with quantizing BERT-like models to 8-bit integers. Existing methods for quantizing models either modify the training procedure, or they require an additional calibration step to adjust parameters that also requires a selected held-out dataset. Our method permits taking advantage of quantization without the need for these adjustments. We present results on several NLP tasks demonstrating the usefulness of this technique.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.45.pdf"
    },
    {
        "title": "QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation",
        "authors": [
            "Krishna Srinivasan",
            "Karthik Raman",
            "Anupam Samanta",
            "Lingrui Liao",
            "Luca Bertelli",
            "Michael Bendersky"
        ],
        "published": "2022",
        "summary": "Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrieval-augmented query understanding.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.50.pdf"
    },
    {
        "title": "Deploying Unified BERT Moderation Model for E-Commerce Reviews",
        "authors": [
            "Ravindra Nayak",
            "Nikesh Garera"
        ],
        "published": "2022",
        "summary": "Moderation of user-generated e-commerce content has become crucial due to the large and diverse user base on the platforms. Product reviews and ratings have become an integral part of the shopping experience to build trust among users. Due to the high volume of reviews generated on a vast catalog of products, manual moderation is infeasible, making machine moderation a necessity. In this work, we described our deployed system and models for automated moderation of user-generated content. At the heart of our approach, we outline several rejection reasons for review & rating moderation and explore a unified BERT model to moderate them. We convey the importance of product vertical embeddings for the relevancy of the review for a given product and highlight the advantages of pre-training the BERT models with monolingual data to cope with the domain gap in the absence of huge labelled datasets. We observe a 4.78% F1 increase with less labelled data and a 2.57% increase in F1 score on the review data compared to the publicly available BERT-based models. Our best model In-House-BERT-vertical sends only 5.89% of total reviews to manual moderation and has been deployed in production serving live traffic for millions of users.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.55.pdf"
    },
    {
        "title": "Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training",
        "authors": [
            "Taolin Zhang",
            "Junwei Dong",
            "Jianing Wang",
            "Chengyu Wang",
            "Ang Wang",
            "Yinghui Liu",
            "Jun Huang",
            "Yong Li",
            "Xiaofeng He"
        ],
        "published": "2022",
        "summary": "Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve context-aware representations via learning from structured relations in knowledge bases, and/or linguistic knowledge from syntactic or dependency analysis. Unlike English, there is a lack of high-performing open-source Chinese KEPLMs in the natural language processing (NLP) community to support various language understanding applications. In this paper, we revisit and advance the development of Chinese natural language understanding with a series of novel Chinese KEPLMs released in various parameter sizes, namely CKBERT (Chinese knowledge-enhanced BERT). Specifically, both relational and linguistic knowledge is effectively injected into CKBERT based on two novel pre-training tasks, i.e., linguistic-aware masked language modeling and contrastive multi-hop relation modeling. Based on the above two pre-training paradigms and our in-house implemented TorchAccelerator, we have pre-trained base (110M), large (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters. Experiments demonstrate that CKBERT consistently outperforms strong baselines for Chinese over various benchmark NLP tasks and in terms of different model sizes.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.57.pdf"
    },
    {
        "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
        "authors": [
            "Yuto Oikawa",
            "Yuki Nakayama",
            "Koji Murakami"
        ],
        "published": "2022",
        "summary": "In a live streaming chat on a video streaming service, it is crucial to filter out toxic comments with online processing to prevent users from reading comments in real-time. However, recent toxic language detection methods rely on deep learning methods, which can not be scalable considering inference speed. Also, these methods do not consider constraints of computational resources expected depending on a deployed system (e.g., no GPU resource).This paper presents an efficient method for toxic language detection that is aware of real-world scenarios. Our proposed architecture is based on partial stacking that feeds initial results with low confidence to meta-classifier. Experimental results show that our method achieves a much faster inference speed than BERT-based models with comparable performance.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.58.pdf"
    },
    {
        "title": "A Comprehensive Evaluation of Biomedical Entity-centric Search",
        "authors": [
            "Elena Tutubalina",
            "Zulfat Miftahutdinov",
            "Vladimir Muravlev",
            "Anastasia Shneyderman"
        ],
        "published": "2022",
        "summary": "Biomedical information retrieval has often been studied as a task of detecting whether a system correctly detects entity spans and links these entities to concepts from a given terminology. Most academic research has focused on evaluation of named entity recognition (NER) and entity linking (EL) models which are key components to recognizing diseases and genes in PubMed abstracts. In this work, we perform a fine-grained evaluation intended to understand the efficiency of state-of-the-art BERT-based information extraction (IE) architecture as a biomedical search engine. We present a novel manually annotated dataset of abstracts for disease and gene search. The dataset contains 23K query-abstract pairs, where 152 queries are selected from logs of our target discovery platform and PubMed abstracts annotated with relevance judgments. Specifically, the query list also includes a subset of concepts with at least one ambiguous concept name. As a baseline, we use off-she-shelf Elasticsearch with BM25. Our experiments on NER, EL, and retrieval in a zero-shot setup show the neural IE architecture shows superior performance for both disease and gene concept queries.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.61.pdf"
    },
    {
        "title": "Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset",
        "authors": [
            "Anton Eklund",
            "Mona Forsman"
        ],
        "published": "2022",
        "summary": "Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate these models because there is no ground truth and automatic measurements may not mimic human judgment. To address this problem, we created a tool called STELLAR for interactive topic browsing which we used for human evaluation of topics created from a real-world dataset used in industry. Embeddings created with BERT were used together with UMAP and HDBSCAN to model the topics. The human evaluation found that our topic model creates coherent topics. The following discussion revolves around the requirements of industry and what research is needed for production-ready systems.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-industry.65.pdf"
    }
]