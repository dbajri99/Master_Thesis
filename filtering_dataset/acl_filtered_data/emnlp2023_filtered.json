[
    {
        "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
        "authors": [
            "Zhebin Zhang",
            "Xinyu Zhang",
            "Yuanhang Ren",
            "Saijiang Shi",
            "Meng Han",
            "Yongkang Wu",
            "Ruofei Lai",
            "Zhao Cao"
        ],
        "published": "2023",
        "summary": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1.pdf"
    },
    {
        "title": "Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting",
        "authors": [
            "Chenkai Sun",
            "Jinning Li",
            "Yi Fung",
            "Hou Chan",
            "Tarek Abdelzaher",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework\u2019s capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.4.pdf"
    },
    {
        "title": "Primacy Effect of ChatGPT",
        "authors": [
            "Yiwei Wang",
            "Yujun Cai",
            "Muhao Chen",
            "Yuxuan Liang",
            "Bryan Hooi"
        ],
        "published": "2023",
        "summary": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans\u2019 cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT\u2019s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.8.pdf"
    },
    {
        "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
        "authors": [
            "Akira Kawabata",
            "Saku Sugawara"
        ],
        "published": "2023",
        "summary": "To precisely evaluate a language model\u2019s capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiple-choice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.9.pdf"
    },
    {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
            "Huao Li",
            "Yu Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Charles Lewis",
            "Katia Sycara"
        ],
        "published": "2023",
        "summary": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents\u2019 planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.13.pdf"
    },
    {
        "title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
        "authors": [
            "Robert Litschko",
            "Max M\u00fcller-Eberstein",
            "Rob van der Goot",
            "Leon Weber-Genzel",
            "Barbara Plank"
        ],
        "published": "2023",
        "summary": "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model\u2019s functional capacity, and provide recommendations for more multi-faceted evaluation protocols.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.14.pdf"
    },
    {
        "title": "Let\u2019s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought",
        "authors": [
            "Vaishnavi Himakunthala",
            "Andy Ouyang",
            "Daniel Rose",
            "Ryan He",
            "Alex Mei",
            "Yujie Lu",
            "Chinmay Sonar",
            "Michael Saxon",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Despite exciting recent results showing vision-language systems\u2019 capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP, an inference-time challenge dataset designed to explore models\u2019 reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays, we propose two formats for keyframe description: unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video reasoning, we propose two tasks: Video Infilling and Video Prediction, which test abilities to generate multiple intermediate keyframes and predict future keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP, demonstrate the performance gap in these complex video reasoning tasks, and encourage future work to prioritize language models for efficient and generalized video reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.15.pdf"
    },
    {
        "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Abdul Waheed",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023",
        "summary": "ChatGPT\u2019s emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model\u2019s efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT\u2019s capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT\u2019s deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4\u2019s Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA. Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.16.pdf"
    },
    {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.20.pdf"
    },
    {
        "title": "Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients",
        "authors": [
            "Feihu Jin",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "published": "2023",
        "summary": "Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods. This paper proposes a novel parameter-efficient tuning method for LLMs without calculating their gradients. Leveraging the discernible similarities between the parameter-efficient modules of the same task learned by both large and small language models, we put forward a strategy for transferring the parameter-efficient modules, originally derived from small language models to much larger ones. To ensure a smooth and effective adaptation process, we further introduce a Bridge model to guarantee dimensional consistency while also stimulating a dynamic interaction between the models. We demonstrate the effectiveness of our method using the T5 and GPT-2 series of language models on the SuperGLUE benchmark. Our method achieves comparable performance to both fine-tuning and parameter-efficient tuning on large language models without needing gradient-based optimization. Additionally, our method achieves up to 5.7x memory reduction compared to parameter-efficient tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.22.pdf"
    },
    {
        "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
        "authors": [
            "Benjamin Minixhofer",
            "Jonas Pfeiffer",
            "Ivan Vuli\u0107"
        ],
        "published": "2023",
        "summary": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.24.pdf"
    },
    {
        "title": "Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy",
        "authors": [
            "Yahan Yang",
            "Elior Sulem",
            "Insup Lee",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters. Relying on this promising result, we explore in this paper the performance of BabyBERTa-based models in downstream tasks, focusing on Semantic Role Labeling (SRL) and two Extractive Question Answering tasks, with the aim of building more efficient systems that rely on less data and smaller models. We investigate the influence of these models both alone and as a starting point to larger pre-trained models, separately examining the contribution of the pre-training data, the vocabulary, and the masking policy on the downstream task performance. Our results show that BabyBERTa trained with unmasking-removal policy is a much stronger starting point for downstream tasks compared to the use of RoBERTa masking policy when 10M words are used for training and that this tendency persists, although to a lesser extent, when adding more training data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.30.pdf"
    },
    {
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "authors": [
            "Hang Chen",
            "Xinyu Yang",
            "Jing Luo",
            "Wenjing Zhu"
        ],
        "published": "2023",
        "summary": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable \u201cimplicit causes.\u201d Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.33.pdf"
    },
    {
        "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
        "authors": [
            "Dong-Ho Lee",
            "Kian Ahrabian",
            "Woojeong Jin",
            "Fred Morstatter",
            "Jay Pujara"
        ],
        "published": "2023",
        "summary": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures (\u00b10.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.36.pdf"
    },
    {
        "title": "Knowledge Graph Compression Enhances Diverse Commonsense Generation",
        "authors": [
            "EunJeong Hwang",
            "Veronika Thost",
            "Vered Shwartz",
            "Tengfei Ma"
        ],
        "published": "2023",
        "summary": "Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.37.pdf"
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "authors": [
            "Shih-yang Liu",
            "Zechun Liu",
            "Xijie Huang",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "published": "2023",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.39.pdf"
    },
    {
        "title": "Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting",
        "authors": [
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been \u201ctuned\u201d for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance. This paper tackles the problem of how to optimize explanation-infused prompts in a blackbox fashion. We first generate sets of candidate explanations for each example in the prompt using a leave-one-out scheme, then find an effective combination of these explanations with a two-stage framework. We first evaluate explanations for each in-context example in isolation according to two proxy metrics, log likelihood and accuracy on new examples. Then, we search over combinations of explanations to find one that yields high performance against a silver-labeled development set. Across four textual reasoning tasks spanning question answering, mathematical reasoning, and natural language inference, results show that our proxy metrics correlate with ground truth accuracy and our overall method can effectively improve prompts over crowdworker annotations and naive search strategies",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.41.pdf"
    },
    {
        "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance",
        "authors": [
            "Chenxi Whitehouse",
            "Monojit Choudhury",
            "Alham Aji"
        ],
        "published": "2023",
        "summary": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.44.pdf"
    },
    {
        "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition",
        "authors": [
            "Chenxu Wang",
            "Ping Jian",
            "Mu Huang"
        ],
        "published": "2023",
        "summary": "Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics, we design a novel self-supervised learning objective based on mutual information maximization to derive enhanced representations of logical semantics for IDRR. Experimental results on PDTB 2.0 and CoNLL16 datasets demonstrate that our method achieves outstanding and consistent performance against the current state-of-the-art models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.45.pdf"
    },
    {
        "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
        "authors": [
            "Jiwan Chung",
            "Youngjae Yu"
        ],
        "published": "2023",
        "summary": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.46.pdf"
    },
    {
        "title": "Conceptual structure coheres in human cognition but not in large language models",
        "authors": [
            "Siddharth Suresh",
            "Kushin Mukherjee",
            "Xizheng Yu",
            "Wei-Chun Huang",
            "Lisa Padua",
            "Timothy Rogers"
        ],
        "published": "2023",
        "summary": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses three common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known large language model, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses\u2013responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates. The results suggest one important way that knowledge inhering in contemporary LLMs can differ from human cognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.47.pdf"
    },
    {
        "title": "Towards LLM-driven Dialogue State Tracking",
        "authors": [
            "Yujie Feng",
            "Zexin Lu",
            "Bo Liu",
            "Liming Zhan",
            "Xiao-Ming Wu"
        ],
        "published": "2023",
        "summary": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT\u2019s capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.48.pdf"
    },
    {
        "title": "We\u2019re Afraid Language Models Aren\u2019t Modeling Ambiguity",
        "authors": [
            "Alisa Liu",
            "Zhaofeng Wu",
            "Julian Michael",
            "Alane Suhr",
            "Peter West",
            "Alexander Koller",
            "Swabha Swayamdipta",
            "Noah Smith",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.51.pdf"
    },
    {
        "title": "Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Hsin-Hsi Chen",
            "Chung-Chi Chen"
        ],
        "published": "2023",
        "summary": "In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms. FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness in the generated text. We demonstrate its effectiveness across two tasks prone to hallucination: abstractive summarization and dialogue generation. Results show that FECS consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.54.pdf"
    },
    {
        "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",
        "authors": [
            "Seungju Han",
            "Junhyeok Kim",
            "Jack Hessel",
            "Liwei Jiang",
            "Jiwan Chung",
            "Yejin Son",
            "Yejin Choi",
            "Youngjae Yu"
        ],
        "published": "2023",
        "summary": "Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.57.pdf"
    },
    {
        "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
        "authors": [
            "Tianhang Zhang",
            "Lin Qiu",
            "Qipeng Guo",
            "Cheng Deng",
            "Yue Zhang",
            "Zheng Zhang",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Luoyi Fu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.58.pdf"
    },
    {
        "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
        "authors": [
            "Gangwoo Kim",
            "Sungdong Kim",
            "Byeongguk Jeon",
            "Joonsuk Park",
            "Jaewoo Kang"
        ],
        "published": "2023",
        "summary": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ\u2014via few-shot prompting leveraging external knowledge\u2014and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.63.pdf"
    },
    {
        "title": "Large Language Models Can Self-Improve",
        "authors": [
            "Jiaxin Huang",
            "Shixiang Gu",
            "Le Hou",
            "Yuexin Wu",
            "Xuezhi Wang",
            "Hongkun Yu",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \u201chigh-confidence\u201d rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%\u219282.1% on GSM8K, 90.0%\u219294.4% on OpenBookQA, and 63.4%\u219267.9% on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.67.pdf"
    },
    {
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "authors": [
            "Yue Wang",
            "Hung Le",
            "Akhilesh Gotmare",
            "Nghi Bui",
            "Junnan Li",
            "Steven Hoi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \u201cCodeT5+\u201d, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.68.pdf"
    },
    {
        "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
        "authors": [
            "Andrea W Wen-Yi",
            "David Mimno"
        ],
        "published": "2023",
        "summary": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.71.pdf"
    },
    {
        "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
        "authors": [
            "Pengyu Wang",
            "Linyang Li",
            "Ke Ren",
            "Botian Jiang",
            "Dong Zhang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose Sequence X (Check) GPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like waves in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.73.pdf"
    },
    {
        "title": "QTSumm: Query-Focused Summarization over Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Zhenting Qi",
            "Linyong Nan",
            "Boyu Mi",
            "Yixin Liu",
            "Weijin Zou",
            "Simeng Han",
            "Ruizhe Chen",
            "Xiangru Tang",
            "Yumo Xu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023",
        "summary": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users\u2019 information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.74.pdf"
    },
    {
        "title": "\u2018Don\u2019t Get Too Technical with Me\u2019: A Discourse Structure-Based Framework for Automatic Science Journalism",
        "authors": [
            "Ronald Cardenas",
            "Bingsheng Yao",
            "Dakuo Wang",
            "Yufang Hou"
        ],
        "published": "2023",
        "summary": "Science journalism refers to the task of reporting technical findings of a scientific paper as a less technical news article to the general public audience. We aim to design an automated system to support this real-world task (i.e., automatic science journalism ) by 1) introducing a newly-constructed and real-world dataset (SciTechNews), with tuples of a publicly-available scientific paper, its corresponding news article, and an expert-written short summary snippet; 2) proposing a novel technical framework that integrates a paper\u2019s discourse structure with its metadata to guide generation; and, 3) demonstrating with extensive automatic and human experiments that our model outperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a content plan meaningful for the target audience, simplify the information selected, and produce a coherent final report in a layman\u2019s style.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.76.pdf"
    },
    {
        "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
        "authors": [
            "Jianwei Li",
            "Qi Lei",
            "Wei Cheng",
            "Dongkuan Xu"
        ],
        "published": "2023",
        "summary": "The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer\u2019s reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.79.pdf"
    },
    {
        "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
        "authors": [
            "Jiacheng Liu",
            "Wenya Wang",
            "Dianzhuo Wang",
            "Noah Smith",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Today\u2019s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, Vera is trained on ~7M commonsense statements that are automatically converted from 19 QA datasets and two commonsense knowledge bases, and using a combination of three training objectives. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, even including GPT-3.5/ChatGPT/GPT-4, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering machine-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.81.pdf"
    },
    {
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "authors": [
            "Jiaxin Wen",
            "Pei Ke",
            "Hao Sun",
            "Zhexin Zhang",
            "Chengfei Li",
            "Jinfeng Bai",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.84.pdf"
    },
    {
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Zhuosheng Zhang",
            "Jiaao Chen",
            "Michihiro Yasunaga",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot\u2014i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.85.pdf"
    },
    {
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": [
            "Xunjian Yin",
            "Baizhou Huang",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models\u2019 capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs\u2019 ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs\u2019 abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model\u2019s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.87.pdf"
    },
    {
        "title": "Location-Aware Visual Question Generation with Lightweight Models",
        "authors": [
            "Nicholas Suwono",
            "Justin Chen",
            "Tun Hung",
            "Ting-Hao Huang",
            "I-Bin Liao",
            "Yung-Hui Li",
            "Lun-Wei Ku",
            "Shao-Hua Sun"
        ],
        "published": "2023",
        "summary": "This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task and fit on an edge device, such as a mobile phone. To this end, we propose a method which can reliably generate engaging questions from location-aware information. Our proposed method outperforms baselines regarding human evaluation (e.g., engagement, grounding, coherence) and automatic evaluation metrics (e.g., BERTScore, ROUGE-2). Moreover, we conduct extensive ablation studies to justify our proposed techniques for both generating the dataset and solving the task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.88.pdf"
    },
    {
        "title": "Transcending Scaling Laws with 0.1% Extra Compute",
        "authors": [
            "Yi Tay",
            "Jason Wei",
            "Hyung Chung",
            "Vinh Tran",
            "David So",
            "Siamak Shakeri",
            "Xavier Garcia",
            "Steven Zheng",
            "Jinfeng Rao",
            "Aakanksha Chowdhery",
            "Denny Zhou",
            "Donald Metzler",
            "Slav Petrov",
            "Neil Houlsby",
            "Quoc Le",
            "Mostafa Dehghani"
        ],
        "published": "2023",
        "summary": "Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model on a few more steps with UL2\u2019s mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training a baseline language model, PaLM, with ULR2, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ~4.4 million TPUv4 hours). We further show that this improved scaling curve leads to \u201cemergent abilities\u201d on challenging BIG-Bench tasks\u2014for instance, U-PaLM does much better on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, including reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.91.pdf"
    },
    {
        "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
        "authors": [
            "Minzhi Li",
            "Taiwei Shi",
            "Caleb Ziems",
            "Min-Yen Kan",
            "Nancy Chen",
            "Zhengyuan Liu",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs\u2019 annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.92.pdf"
    },
    {
        "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination",
        "authors": [
            "Moshe Berchansky",
            "Peter Izsak",
            "Avi Caciularu",
            "Ido Dagan",
            "Moshe Wasserblat"
        ],
        "published": "2023",
        "summary": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.93.pdf"
    },
    {
        "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Yixin Cao",
            "Jizhi Zhang",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework , which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.95.pdf"
    },
    {
        "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
        "authors": [
            "Martin Josifoski",
            "Marija Sakota",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at anonymous.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.96.pdf"
    },
    {
        "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
        "authors": [
            "Simone Conia",
            "Min Li",
            "Daniel Lee",
            "Umar Minhas",
            "Ihab Ilyas",
            "Yunyao Li"
        ],
        "published": "2023",
        "summary": "Recent work in Natural Language Processing and Computer Vision has been using textual information \u2013 e.g., entity names and descriptions \u2013 available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Completion (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.100.pdf"
    },
    {
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
        "authors": [
            "Xiuying Wei",
            "Yunchen Zhang",
            "Yuhang Li",
            "Xiangguo Zhang",
            "Ruihao Gong",
            "Jinyang Guo",
            "Xianglong Liu"
        ],
        "published": "2023",
        "summary": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/Outlier_Suppression_Plus.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.102.pdf"
    },
    {
        "title": "Interpreting Embedding Spaces by Conceptualization",
        "authors": [
            "Adi Simhi",
            "Shaul Markovitch"
        ],
        "published": "2023",
        "summary": "One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.106.pdf"
    },
    {
        "title": "Knowledge-Augmented Language Model Verification",
        "authors": [
            "Jinheon Baek",
            "Soyeong Jeong",
            "Minki Kang",
            "Jong Park",
            "Sung Hwang"
        ],
        "published": "2023",
        "summary": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.107.pdf"
    },
    {
        "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
        "authors": [
            "Zeyuan Yang",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.109.pdf"
    },
    {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "authors": [
            "Po-Nien Kung",
            "Fan Yin",
            "Di Wu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.112.pdf"
    },
    {
        "title": "\u201cFifty Shades of Bias\u201d: Normative Ratings of Gender Bias in GPT Generated English Text",
        "authors": [
            "Rishav Hada",
            "Agrima Seth",
            "Harshita Diddee",
            "Kalika Bali"
        ],
        "published": "2023",
        "summary": "Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best\u2013Worst Scaling \u2013 an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.115.pdf"
    },
    {
        "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
        "authors": [
            "Jan Cegin",
            "Jakub Simko",
            "Peter Brusilovsky"
        ],
        "published": "2023",
        "summary": "The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent classification. We apply data collection methodology of an existing crowdsourcing study (similar scale, prompts and seed data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases are more diverse and lead to at least as robust models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.117.pdf"
    },
    {
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
        "authors": [
            "Zhaoyang Wang",
            "Shaohan Huang",
            "Yuxuan Liu",
            "Jiahai Wang",
            "Minghui Song",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student\u2019s learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.120.pdf"
    },
    {
        "title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization",
        "authors": [
            "Shmuel Amar",
            "Liat Schiff",
            "Ori Ernst",
            "Asi Shefer",
            "Ori Shapira",
            "Ido Dagan"
        ],
        "published": "2023",
        "summary": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.121.pdf"
    },
    {
        "title": "Byte Pair Encoding for Symbolic Music",
        "authors": [
            "Nathan Fradet",
            "Nicolas Gutowski",
            "Fabien Chhel",
            "Jean-Pierre Briot"
        ],
        "published": "2023",
        "summary": "When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better results and faster inference in generation and classification tasks. The [source code is shared on Github](https://github.com/Natooz/bpe-symbolic-music), along with a [companion website](https://Natooz.github.io/BPE-Symbolic-Music). Finally, BPE is directly implemented in [MidiTok](https://github.com/Natooz/MidiTok), allowing the reader to easily benefit from this method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.123.pdf"
    },
    {
        "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
        "authors": [
            "Megh Thakkar",
            "Tolga Bolukbasi",
            "Sriram Ganapathy",
            "Shikhar Vashishth",
            "Sarath Chandar",
            "Partha Talukdar"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pretraining data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.125.pdf"
    },
    {
        "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
        "authors": [
            "Zorik Gekhman",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Chen Elkind",
            "Idan Szpektor"
        ],
        "published": "2023",
        "summary": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.127.pdf"
    },
    {
        "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
        "authors": [
            "Namrata Shivagunde",
            "Vladislav Lialin",
            "Anna Rumshisky"
        ],
        "published": "2023",
        "summary": "Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing. The datasets and code are available on Github.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.130.pdf"
    },
    {
        "title": "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data",
        "authors": [
            "Zhehao Zhang",
            "Xitao Li",
            "Yan Gao",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions\u2019 directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs\u2019 reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.132.pdf"
    },
    {
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "authors": [
            "Arkil Patel",
            "Satwik Bhattamishra",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023",
        "summary": "Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.134.pdf"
    },
    {
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
        "authors": [
            "Eric Zelikman",
            "Wanjing Ma",
            "Jasmine Tran",
            "Diyi Yang",
            "Jason Yeatman",
            "Nick Haber"
        ],
        "published": "2023",
        "summary": "Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students\u2019 progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students\u2019 reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item\u2019s difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test\u2019s difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.135.pdf"
    },
    {
        "title": "Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)",
        "authors": [
            "Megha Chakraborty",
            "S.M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Shreya Gautam",
            "Tanay Kumar",
            "Krish Sharma",
            "Niyar Barman",
            "Chandan Gupta",
            "Vinija Jain",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that \u201cif the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright\u201d. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.136.pdf"
    },
    {
        "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
        "authors": [
            "Zilin Xiao",
            "Ming Gong",
            "Jie Wu",
            "Xingyao Zhang",
            "Linjun Shou",
            "Daxin Jiang"
        ],
        "published": "2023",
        "summary": "Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods of equipping language models with EL ability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4\u00d7 speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully-engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.139.pdf"
    },
    {
        "title": "Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?",
        "authors": [
            "Boxi Cao",
            "Qiaoyu Tang",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models within pretrain-finetune paradigm. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze this issue by perturbing the factual knowledge sources at different scales and comparing the performance of pre-trained language models before and after the perturbation. Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models\u2019 downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.143.pdf"
    },
    {
        "title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
        "authors": [
            "Jasper Jian",
            "Siva Reddy"
        ],
        "published": "2023",
        "summary": "Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we explore the hypothesis that syntactic dependencies can be represented in language model attention distributions and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that words at either end of a dependency can be substituted with words from the same category. Substitutions can be used to generate a set of syntactically invariant sentences whose representations are then used for parsing. We show that increasing the number of substitutions used improves parsing accuracy on natural data. On long-distance subject-verb agreement constructions, our method achieves 79.5% recall compared to 8.9% using a previous method. Our method also provides improvements when transferred to a different parsing setup, demonstrating that it generalizes.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.144.pdf"
    },
    {
        "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
        "authors": [
            "Siru Ouyang",
            "Shuohang Wang",
            "Yang Liu",
            "Ming Zhong",
            "Yizhu Jiao",
            "Dan Iter",
            "Reid Pryzant",
            "Chenguang Zhu",
            "Heng Ji",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between academic research in NLP and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as \u201cdesign\u201d and \u201cplanning\u201d are prevalent in user interactions but largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges, and provide insights toward a roadmap to make LLMs better aligned with user needs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.146.pdf"
    },
    {
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
        "authors": [
            "Hannah Kirk",
            "Andrew Bean",
            "Bertie Vidgen",
            "Paul Rottger",
            "Scott Hale"
        ],
        "published": "2023",
        "summary": "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.148.pdf"
    },
    {
        "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
        "authors": [
            "Vivek Gupta",
            "Pranshu Kandoi",
            "Mahek Vora",
            "Shuo Zhang",
            "Yujie He",
            "Ridho Reinanda",
            "Vivek Srikumar"
        ],
        "published": "2023",
        "summary": "Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.149.pdf"
    },
    {
        "title": "Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task",
        "authors": [
            "Chunhui Du",
            "Jidong Tian",
            "Haoran Liao",
            "Jindou Chen",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.150.pdf"
    },
    {
        "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
        "authors": [
            "Fengji Zhang",
            "Bei Chen",
            "Yue Zhang",
            "Jacky Keung",
            "Jin Liu",
            "Daoguang Zan",
            "Yi Mao",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2023",
        "summary": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoBench, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark will be publicly available after the paper review.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.151.pdf"
    },
    {
        "title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
        "authors": [
            "Yang Liu",
            "Dan Iter",
            "Yichong Xu",
            "Shuohang Wang",
            "Ruochen Xu",
            "Chenguang Zhu"
        ],
        "published": "2023",
        "summary": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.153.pdf"
    },
    {
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Vipula Rawte",
            "Swagata Chakraborty",
            "Agnibh Pathak",
            "Anubhav Sarkar",
            "S.M Towhidul Islam Tonmoy",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination. We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.155.pdf"
    },
    {
        "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
        "authors": [
            "Livio Soares",
            "Daniel Gillick",
            "Jeremy Cole",
            "Tom Kwiatkowski"
        ],
        "published": "2023",
        "summary": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer\u2019s FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce nail (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.156.pdf"
    },
    {
        "title": "Analyzing Modular Approaches for Visual Question Decomposition",
        "authors": [
            "Apoorv Khandelwal",
            "Ellie Pavlick",
            "Chen Sun"
        ],
        "published": "2023",
        "summary": "Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision\u2013language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT\u2019s reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. We also compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code. Our code is fully available at https://github.com/brown-palm/visual-question-decomposition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.157.pdf"
    },
    {
        "title": "Improving Summarization with Human Edits",
        "authors": [
            "Zonghai Yao",
            "Benjamin Schloss",
            "Sai Selvaraj"
        ],
        "published": "2023",
        "summary": "Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback \u2013 Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data \u2013 Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) \u2013 DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect, and better use different human feedback approaches scalably.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.158.pdf"
    },
    {
        "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
        "authors": [
            "Chiyu Zhang",
            "Khai Doan",
            "Qisheng Liao",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023",
        "summary": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.160.pdf"
    },
    {
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": [
            "Gustavo Gon\u00e7alves",
            "Emma Strubell"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model\u2019s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.161.pdf"
    },
    {
        "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
        "authors": [
            "Odhran O\u2019Donoghue",
            "Aleksandar Shtedritski",
            "John Ginger",
            "Ralph Abboud",
            "Ali Ghareeb",
            "Samuel Rodriques"
        ],
        "published": "2023",
        "summary": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM\u2019s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.162.pdf"
    },
    {
        "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
        "authors": [
            "Libo Qin",
            "Qiguang Chen",
            "Fuxuan Wei",
            "Shijue Huang",
            "Wanxiang Che"
        ],
        "published": "2023",
        "summary": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt \u201cLet\u2019s think step by step!\u201d. Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we introduce cross-lingual prompting (CLP), aiming to improve zero-shot CoT reasoning across languages. Specifically, CLP consists of two main components: (1) cross-lingual alignment prompting and (2) task-specific solver prompting. The cross-lingual alignment prompting is responsible for aligning representations across different languages, whereas the task-specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition, we further introduce cross-lingual self-consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state-of-the-art performance. We hope this work will inspire further breakthroughs in cross-lingual CoT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.163.pdf"
    },
    {
        "title": "FinGPT: Large Generative Models for a Small Language",
        "authors": [
            "Risto Luukkonen",
            "Ville Komulainen",
            "Jouni Luoma",
            "Anni Eskelinen",
            "Jenna Kanerva",
            "Hanna-Mari Kupari",
            "Filip Ginter",
            "Veronika Laippala",
            "Niklas Muennighoff",
            "Aleksandra Piktus",
            "Thomas Wang",
            "Nouamane Tazi",
            "Teven Scao",
            "Thomas Wolf",
            "Osma Suominen",
            "Samuli Sairanen",
            "Mikko Merioksa",
            "Jyrki Heinonen",
            "Aija Vahtola",
            "Samuel Antao",
            "Sampo Pyysalo"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.164.pdf"
    },
    {
        "title": "Indicative Summarization of Long Discussions",
        "authors": [
            "Shahbaz Syed",
            "Dominik Schwabe",
            "Khalid Al-Khatib",
            "Martin Potthast"
        ],
        "published": "2023",
        "summary": "Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one\u2019s own arguments, but may also gather a broad cross-section of others\u2019 arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19 LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called **Discussion Explorer**: It shows that our proposed indicative summaries serve as a convenient navigation tool to explore long discussions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.166.pdf"
    },
    {
        "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
        "authors": [
            "Tengxiao Liu",
            "Qipeng Guo",
            "Yuqing Yang",
            "Xiangkun Hu",
            "Yue Zhang",
            "Xipeng Qiu",
            "Zheng Zhang"
        ],
        "published": "2023",
        "summary": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.169.pdf"
    },
    {
        "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation",
        "authors": [
            "Mateusz Lango",
            "Ondrej Dusek"
        ],
        "published": "2023",
        "summary": "Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special \u201ctext critic\u201d classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM\u2019s architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM\u2019s training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.172.pdf"
    },
    {
        "title": "Can Language Models Laugh at YouTube Short-form Videos?",
        "authors": [
            "Dayoon Ko",
            "Sangho Lee",
            "Gunhee Kim"
        ],
        "published": "2023",
        "summary": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs\u2019 ability for humor explanation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.176.pdf"
    },
    {
        "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
        "authors": [
            "Robin Algayres",
            "Yossi Adi",
            "Tu Nguyen",
            "Jade Copet",
            "Gabriel Synnaeve",
            "Beno\u00eet Sagot",
            "Emmanuel Dupoux"
        ],
        "published": "2023",
        "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.182.pdf"
    },
    {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "authors": [
            "Ning Ding",
            "Yulin Chen",
            "Bokai Xu",
            "Yujia Qin",
            "Shengding Hu",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Bowen Zhou"
        ],
        "published": "2023",
        "summary": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.183.pdf"
    },
    {
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
        "authors": [
            "Minghao Li",
            "Yingxiu Zhao",
            "Bowen Yu",
            "Feifan Song",
            "Hangyu Li",
            "Haiyang Yu",
            "Zhoujun Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs\u2019 ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs\u2019 capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca\u2019s tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.187.pdf"
    },
    {
        "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Chunkit Chan",
            "Mingyang Chen",
            "Wei Wang"
        ],
        "published": "2023",
        "summary": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any \u201cfeedback\u201d\u2013identifying challenging instructions where the student model\u2019s performance falls short\u2013to boost the student model\u2019s proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify \u201chard\u201d instructions and generate new \u201chard\u201d instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.189.pdf"
    },
    {
        "title": "Evaluating Large Language Models on Controlled Generation Tasks",
        "authors": [
            "Jiao Sun",
            "Yufei Tian",
            "Wangchunshu Zhou",
            "Nan Xu",
            "Qian Hu",
            "Rahul Gupta",
            "John Wieting",
            "Nanyun Peng",
            "Xuezhe Ma"
        ],
        "published": "2023",
        "summary": "While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present an extensive analysis of various benchmarks including a sentence planning benchmark with different granularities. After comparing large language models against state-of-the-start finetuned smaller models, we present a spectrum showing large language models falling behind, are comparable, or exceed the ability of smaller models. We conclude that *large language models struggle at meeting fine-grained hard constraints*.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.190.pdf"
    },
    {
        "title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
        "authors": [
            "Xiao-Yu Guo",
            "Yuan-Fang Li",
            "Reza Haf"
        ],
        "published": "2023",
        "summary": "Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question. We introduce DeSIQ, a new challenging dataset, constructed by applying simple perturbations to Social-IQ. Our empirical analysis shows De-SIQ significantly reduces the biases in the original Social-IQ dataset. Furthermore, we examine and shed light on the effect of model size, model style, learning settings, commonsense knowledge, and multi-modality on the new benchmark performance. Our new dataset, observations and findings open up important research questions for the study of social intelligence.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.191.pdf"
    },
    {
        "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
        "authors": [
            "Adam Bouyamourn"
        ],
        "published": "2023",
        "summary": "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language generation setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that satisfies evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not synonymous with claims for which the LLM has evidence.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.192.pdf"
    },
    {
        "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
        "authors": [
            "Basel Mousi",
            "Nadir Durrani",
            "Fahim Dalvi"
        ],
        "published": "2023",
        "summary": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.196.pdf"
    },
    {
        "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
        "authors": [
            "Zhengxuan Wu",
            "Alex Tamkin",
            "Isabel Papadimitriou"
        ],
        "published": "2023",
        "summary": "When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model\u2019s downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that researchers should most focus on when designing language transfer scenarios.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.198.pdf"
    },
    {
        "title": "Knowledge Rumination for Pre-trained Language Models",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Shengyu Mao",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving them from the external corpus. By simply adding a prompt like \u201cAs far as I know\u201d to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which proves that the knowledge stored in PLMs can be better exploited to enhance performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.206.pdf"
    },
    {
        "title": "Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning",
        "authors": [
            "Linjuan Wu",
            "Weiming Lu"
        ],
        "published": "2023",
        "summary": "Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs). Additionally, existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages. To address this gap, we propose Struct-XLM, a novel multilingual language model that leverages reinforcement learning (RL) to autonomously discover universal syntactic structures for improving the cross-lingual representation alignment of PLM. Struct-XLM integrates a policy network (PNet) and a translation ranking task. The PNet is designed to discover structural information and integrate it into the last layer of the PLM through the structural multi-head attention module to obtain structural representation. The translation ranking task obtains a delayed reward based on the structural representation to optimize the PNet while improving the alignment of cross-lingual representation. Experiments show the effectiveness of the proposed approach for enhancing cross-lingual transfer of multilingual PLM on the XTREME benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.207.pdf"
    },
    {
        "title": "AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification",
        "authors": [
            "Yongxin Huang",
            "Kexin Wang",
            "Sourav Dutta",
            "Raj Patel",
            "Goran Glava\u0161",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent\u2019s effectiveness in extensive experiments on 17 different few-shot sentence classification datasets. AdaSent matches or surpasses the performance of full SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code for AdaSent is available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.208.pdf"
    },
    {
        "title": "Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models",
        "authors": [
            "Xibo Li",
            "Bowei Zou",
            "Yifan Fan",
            "Yanling Li",
            "Ai Ti Aw",
            "Yu Hong"
        ],
        "published": "2023",
        "summary": "Conversational Question Answering (CQA) aims to provide natural language answers to users in information-seeking dialogues. Existing CQA benchmarks often evaluate models using pre-collected human-human conversations. However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation. While previous studies proposed using predicted history and rewriting techniques to address unresolved coreferences and incoherencies, this approach renders the question self-contained from the conversation. In this paper, we propose a novel automatic evaluation approach, interview evaluation. Specifically, ChatGPT acts as the interviewer (Q agent) with a set of carefully designed prompts, and the CQA model under test serves as the interviewee (A agent). During the interview evaluation, questions are dynamically generated by the Q agent to guide the A agent in predicting the correct answer through an interactive process. We evaluated four different models on QuAC and two models on CoQA in our experiments. The experiment results demonstrate that our interview evaluation has advantages over previous CQA evaluation approaches, particularly in terms of naturalness and coherence. The source code is made publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.209.pdf"
    },
    {
        "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
        "authors": [
            "David Heineman",
            "Yao Dou",
            "Mounica Maddela",
            "Wei Xu"
        ],
        "published": "2023",
        "summary": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems\u2019 specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.211.pdf"
    },
    {
        "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
        "authors": [
            "Zhen Wan",
            "Fei Cheng",
            "Zhuoyuan Mao",
            "Qianying Liu",
            "Haiyue Song",
            "Jiwei Li",
            "Sadao Kurohashi"
        ],
        "published": "2023",
        "summary": "In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of ICL for RE: (1) low relevance regarding entity and relation in existing sentence-level demonstration retrieval approaches for ICL; and (2) the lack of explaining input-label mappings of demonstrations leading to poor ICL effectiveness. In this paper, we propose GPT-RE to successfully address the aforementioned issues by (1) incorporating task-aware representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets. Additionally, a critical issue of LLMs revealed by previous work, the strong inclination to wrongly classify NULL examples into other pre-defined labels, is substantially alleviated by our method. We show an empirical analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.214.pdf"
    },
    {
        "title": "INFORM : Information eNtropy based multi-step reasoning FOR large language Models",
        "authors": [
            "Chuyue Zhou",
            "Wangjie You",
            "Juntao Li",
            "Jing Ye",
            "Kehai Chen",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.216.pdf"
    },
    {
        "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
        "authors": [
            "Jiamin Li",
            "Qiang Su",
            "Yitao Yang",
            "Yimin Jiang",
            "Cong Wang",
            "Hong Xu"
        ],
        "published": "2023",
        "summary": "Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a fixed gating network where each token is computed by the same number of experts. This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. Adaptive gating preserves sparsity while improving training efficiency. We further draw upon curriculum learning to better align the order of training samples and maximize the training time savings. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the gating decisions and present our insights on which tokens are inherently difficult to process, depending on the specific language task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.217.pdf"
    },
    {
        "title": "On the Automatic Generation and Simplification of Children\u2019s Stories",
        "authors": [
            "Maria Valentini",
            "Jennifer Weber",
            "Jesus Salcido",
            "T\u00e9a Wright",
            "Eliana Colunga",
            "Katharina von der Wense"
        ],
        "published": "2023",
        "summary": "With recent advances in large language models (LLMs), the concept of automatically generating children\u2019s educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children\u2019s stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.218.pdf"
    },
    {
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
        "authors": [
            "Aviv Slobodkin",
            "Omer Goldman",
            "Avi Caciularu",
            "Ido Dagan",
            "Shauli Ravfogel"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.220.pdf"
    },
    {
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
        "authors": [
            "Gurusha Juneja",
            "Subhabrata Dutta",
            "Soumen Chakrabarti",
            "Sunny Manchanda",
            "Tanmoy Chakraborty"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver\u2019s capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.225.pdf"
    },
    {
        "title": "Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models",
        "authors": [
            "James Michaelov",
            "Catherine Arnett",
            "Tyler Chang",
            "Ben Bergen"
        ],
        "published": "2023",
        "summary": "Abstract grammatical knowledge\u2014of parts of speech and grammatical patterns\u2014is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.227.pdf"
    },
    {
        "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Xin Zhao",
            "Yaliang Li",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.228.pdf"
    },
    {
        "title": "Deep Natural Language Feature Learning for Interpretable Prediction",
        "authors": [
            "Felipe Urrutia",
            "Cristian Calderon",
            "Valentin Barriere"
        ],
        "published": "2023",
        "summary": "We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language Inference (NLI) fashion, using weak labels automatically obtained from a Large Language Model (LLM). We show that the LLM normally struggles for the main task using in-context learning, but can handle these easiest subtasks and produce useful weak labels to train a BERT. The NLI-like training of the BERT allows for tackling zero-shot inference with any binary question, and not necessarily the ones seen during the training. We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree. This decision tree is interpretable but also reaches high performances, surpassing those of a pre-trained transformer in some cases. We have successfully applied this method to two completely different tasks: detecting incoherence in students\u2019 answers to open-ended mathematics exam questions, and screening abstracts for a systematic literature review of scientific papers on climate change and agroecology.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.229.pdf"
    },
    {
        "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
        "authors": [
            "David Esiobu",
            "Xiaoqing Tan",
            "Saghar Hosseini",
            "Megan Ung",
            "Yuchen Zhang",
            "Jude Fernandes",
            "Jane Dwivedi-Yu",
            "Eleonora Presani",
            "Adina Williams",
            "Eric Smith"
        ],
        "published": "2023",
        "summary": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.230.pdf"
    },
    {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.232.pdf"
    },
    {
        "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",
        "authors": [
            "Shubham Mittal",
            "Megha Sundriyal",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.236.pdf"
    },
    {
        "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
        "authors": [
            "Jongin Kim",
            "Byeo Rhee Bak",
            "Aditya Agrawal",
            "Jiaxi Wu",
            "Veronika Wirtz",
            "Traci Hong",
            "Derry Wijaya"
        ],
        "published": "2023",
        "summary": "This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing COVID-19 vaccine misinformation detection models: domain-specific pre-training and text augmentation using a large language model. Our best misinformation detection models demonstrate improvements ranging from 2.7 to 15.9 percentage points in macro F1-score compared to the baseline models. Additionally, we apply our misinformation detection models in a large-scale study of 19 million unlabeled tweets from the three countries between 2020 and 2022, showcasing the practical application of our dataset and models for detecting and analyzing vaccine misinformation in multiple countries and languages. Our analysis indicates that percentage changes in the number of new COVID-19 cases are positively associated with COVID-19 vaccine misinformation rates in a staggered manner for Brazil and Indonesia, and there are significant positive associations between the misinformation rates across the three countries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.237.pdf"
    },
    {
        "title": "Contrastive Learning of Sentence Embeddings from Scratch",
        "authors": [
            "Junlei Zhang",
            "Zhenzhong Lan",
            "Junxian He"
        ],
        "published": "2023",
        "summary": "Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy formats, among other factors. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthetic data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences SynCSE-partial, and (2) generating sentences along with their corresponding annotations from scratch SynCSE-scratch. Notably, SynCSE-scratch constitutes the first contrastive learning method to learn sentence embeddings from scratch without manually collecting any data sample. Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.238.pdf"
    },
    {
        "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
        "authors": [
            "Weifeng Jiang",
            "Qianren Mao",
            "Chenghua Lin",
            "Jianxin Li",
            "Ting Deng",
            "Weiyi Yang",
            "Zheng Wang"
        ],
        "published": "2023",
        "summary": "Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize a cohort of multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6\u00d7 smaller and 4.8 \u00d7 faster in inference than the baseline PLMs while maintaining comparable performance. We also show that DisCo-generated student models outperform the similar-sized models elaborately tuned in distinct tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.244.pdf"
    },
    {
        "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
        "authors": [
            "Da Yin",
            "Xiao Liu",
            "Fan Yin",
            "Ming Zhong",
            "Hritik Bansal",
            "Jiawei Han",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.245.pdf"
    },
    {
        "title": "Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition",
        "authors": [
            "Zui Chen",
            "Jiaqi Han",
            "Chaofan Yang",
            "Yi Zhou"
        ],
        "published": "2023",
        "summary": "Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges. This article investigates the distinctive language characteristics of LaTeX mathematical expressions, revealing two key observations: 1) the presence of explicit structural symbols, and 2) the treatment of symbols, particularly letters, as minimal units with context-dependent semantics, representing variables or constants. Rooted in these properties, we propose that language models have the potential to synchronously and complementarily provide both structural and semantic information, making them suitable for correction of HMER. To validate our proposition, we propose an architecture called Recognize and Language Fusion Network (RLFN), which integrates recognition and language features to output corrected sequences while jointly optimizing with a string decoder recognition model. Experiments show that RLFN outperforms existing state-of-the-art methods on the CROHME 2014/2016/2019 datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.247.pdf"
    },
    {
        "title": "mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images",
        "authors": [
            "Keighley Overbay",
            "Jaewoo Ahn",
            "Fatemeh Pesaran zadeh",
            "Joonsuk Park",
            "Gunhee Kim"
        ],
        "published": "2023",
        "summary": "The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present mRedditSum, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. We annotate each thread with a human-written summary that captures both the essential information from the text, as well as the details available only in the image. Experiments show that popular summarization models\u2014GPT-3.5, BART, and T5\u2014consistently improve in performance when visual information is incorporated. We also introduce a novel method, cluster-based multi-stage summarization, that outperforms existing baselines and serves as a competitive baseline for future work.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.251.pdf"
    },
    {
        "title": "Sparse Low-rank Adaptation of Pre-trained Language Models",
        "authors": [
            "Ning Ding",
            "Xingtai Lv",
            "Qiaosen Wang",
            "Yulin Chen",
            "Bowen Zhou",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model\u2019s memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.252.pdf"
    },
    {
        "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
        "authors": [
            "Jingyuan Qi",
            "Zhiyang Xu",
            "Ying Shen",
            "Minqian Liu",
            "Di Jin",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e. iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans\u2019 recursively thinking process of complex reasoning problems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.255.pdf"
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "authors": [
            "Kabir Ahuja",
            "Harshita Diddee",
            "Rishav Hada",
            "Millicent Ochieng",
            "Krithika Ramesh",
            "Prachi Jain",
            "Akshay Nambi",
            "Tanuja Ganu",
            "Sameer Segal",
            "Mohamed Ahmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023",
        "summary": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.258.pdf"
    },
    {
        "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
        "authors": [
            "Dohwan Ko",
            "Ji Lee",
            "Woo-Young Kang",
            "Byungseok Roh",
            "Hyunwoo Kim"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as \u2018ungrounded guesses\u2019 or \u2018hallucinations\u2019. To address this problem while leveraging LLMs\u2019 prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of \u27e8V, Q, A\u27e9 triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.261.pdf"
    },
    {
        "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
        "authors": [
            "Yuanyuan Liang",
            "Jianing Wang",
            "Hanlun Zhu",
            "Lei Wang",
            "Weining Qian",
            "Yunshi Lan"
        ],
        "published": "2023",
        "summary": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.263.pdf"
    },
    {
        "title": "TrojanSQL: SQL Injection against Natural Language Interface to Database",
        "authors": [
            "Jinchuan Zhang",
            "Yan Zhou",
            "Binyuan Hui",
            "Yaxin Liu",
            "Ziming Li",
            "Songlin Hu"
        ],
        "published": "2023",
        "summary": "The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about the database. The study explores two specific injection attacks, namely boolean-based injection and union-based injection, which use different types of triggers to achieve distinct goals in compromising the parser. Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively. We hope that this study will raise more concerns about the potential security risks of building natural language interfaces to databases.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.264.pdf"
    },
    {
        "title": "Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models",
        "authors": [
            "Aly Kassem",
            "Omar Mahmoud",
            "Sherif Saad"
        ],
        "published": "2023",
        "summary": "Large Language models (LLMs) are trained on vast amounts of data, including sensitive information that poses a risk to personal privacy if exposed. LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data pre-processing. However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models. To tackle this challenge more effectively, we propose \u201cDeMem,\u201d a novel unlearning approach that utilizes an efficient reinforcement learning feedback loop via proximal policy optimization. By fine-tuning the language model with a negative similarity score as a reward signal, we incentivize the LLMs to learn a paraphrasing policy to unlearn the pre-training data. Our experiments demonstrate that DeMem surpasses strong baselines and state-of-the-art methods in terms of its ability to generalize and strike a balance between maintaining privacy and LLM performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.265.pdf"
    },
    {
        "title": "Meta-Learning Online Adaptation of Language Models",
        "authors": [
            "Nathan Hu",
            "Eric Mitchell",
            "Christopher Manning",
            "Chelsea Finn"
        ],
        "published": "2023",
        "summary": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model\u2019s effective \u201cshelf life.\u201d While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model\u2019s ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.268.pdf"
    },
    {
        "title": "Self-Detoxifying Language Models via Toxification Reversal",
        "authors": [
            "Chak Tou Leong",
            "Yi Cheng",
            "Jiashuo Wang",
            "Jian Wang",
            "Wenjie Li"
        ],
        "published": "2023",
        "summary": "Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve \u201cself-detoxification\u201d. Our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. At the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. Drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. Experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.269.pdf"
    },
    {
        "title": "POE: Process of Elimination for Multiple Choice Reasoning",
        "authors": [
            "Chenkai Ma",
            "Xinya Du"
        ],
        "published": "2023",
        "summary": "Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.273.pdf"
    },
    {
        "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
        "authors": [
            "Hongyi Zheng",
            "Abulhair Saparov"
        ],
        "published": "2023",
        "summary": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.277.pdf"
    },
    {
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": [
            "Noah Lee",
            "Na Min An",
            "James Thorne"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.278.pdf"
    },
    {
        "title": "DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
        "authors": [
            "Ratish Puduppully",
            "Anoop Kunchukuttan",
            "Raj Dabre",
            "Ai Ti Aw",
            "Nancy Chen"
        ],
        "published": "2023",
        "summary": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.279.pdf"
    },
    {
        "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
        "authors": [
            "Lang Qin",
            "Yao Zhang",
            "Hongru Liang",
            "Jun Wang",
            "Zhenglu Yang"
        ],
        "published": "2023",
        "summary": "Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose \\tt{GATE}, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of \\tt{GATE}, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.285.pdf"
    },
    {
        "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
        "authors": [
            "Yunxiang Zhang",
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to \u201challucinate\u201d content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.286.pdf"
    },
    {
        "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
        "authors": [
            "Jiaao Chen",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffuse LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffuse LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.289.pdf"
    },
    {
        "title": "Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?",
        "authors": [
            "Kevin Liu",
            "Stephen Casper",
            "Dylan Hadfield-Menell",
            "Jacob Andreas"
        ],
        "published": "2023",
        "summary": "Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs \u201clie\u2019 or otherwise encode non-cooperative communicative intents. Is this an accurate description of today\u2019s LMs, or can query\u2013probe disagreement arise in other ways? We identify three different classes of disagreement, which we term confabulation, deception, and heterogeneity. In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers. In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.291.pdf"
    },
    {
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "authors": [
            "Ce Zheng",
            "Lei Li",
            "Qingxiu Dong",
            "Yuxuan Fan",
            "Zhiyong Wu",
            "Jingjing Xu",
            "Baobao Chang"
        ],
        "published": "2023",
        "summary": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.296.pdf"
    },
    {
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "authors": [
            "Joshua Ainslie",
            "James Lee-Thorp",
            "Michiel de Jong",
            "Yury Zemlyanskiy",
            "Federico Lebron",
            "Sumit Sanghai"
        ],
        "published": "2023",
        "summary": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.298.pdf"
    },
    {
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
        "authors": [
            "Yifan Hou",
            "Jiaoda Li",
            "Yu Fei",
            "Alessandro Stolfo",
            "Wangchunshu Zhou",
            "Guangtao Zeng",
            "Antoine Bosselut",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model\u2019s attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model\u2019s attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.299.pdf"
    },
    {
        "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
        "authors": [
            "Sander Schulhoff",
            "Jeremy Pinto",
            "Anaum Khan",
            "Louis-Fran\u00e7ois Bouchard",
            "Chenglei Si",
            "Svetlina Anati",
            "Valen Tagliabue",
            "Anson Kost",
            "Christopher Carnahan",
            "Jordan Boyd-Graber"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.302.pdf"
    },
    {
        "title": "Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge",
        "authors": [
            "Te-Lin Wu",
            "Yu Zhou",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human actions/interactions to the environment without being told exactly what/where to ground (e.g., localizing and tracking the \u2018sponge\u2018 in video from the instruction \u201cDip the sponge into the bucket.\u201d). While existing works approach this problem from a pure vision perspective, we investigate to which extent the textual modality (i.e., task instructions) and their interaction with visual modality can be beneficial. Specifically, we propose to improve phrase grounding models\u2019 ability on localizing the active objects by: (1) learning the role of \u2018objects undergoing change\u2018 and extracting them accurately from the instructions, (2) leveraging pre- and post-conditions of the objects during actions, and (3) recognizing the objects more robustly with descriptional knowledge. We leverage large language models (LLMs) to extract the aforementioned action-object knowledge, and design a per-object aggregation masking technique to effectively perform joint inference on object phrases and symbolic knowledge. We evaluate our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments demonstrate the effectiveness of our proposed framework, which leads to>54% improvements in all standard metrics on the TREK-150-OPE-Det localization + tracking task, >7% improvements in all standard metrics on the TREK-150-OPE tracking task, and >3% improvements in average precision (AP) on the Ego4D SCOD task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.304.pdf"
    },
    {
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "authors": [
            "Jennifer Hu",
            "Roger Levy"
        ],
        "published": "2023",
        "summary": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models\u2019 probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models\u2019 linguistic knowledge. Broadly, we find that LLMs\u2019 metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.306.pdf"
    },
    {
        "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings",
        "authors": [
            "Josip Juki\u0107",
            "Jan Snajder"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. In parallel, adapter modules designed for parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. We present an empirical study of PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. We further examine the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, where we find that PEFT yields more stable representations of early and middle layers compared to FFT. Our research underscores the synergistic potential of AL and PEFT in low-resource settings, paving the way for advancements in efficient and effective fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.307.pdf"
    },
    {
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
        "authors": [
            "Theo Olausson",
            "Alex Gu",
            "Ben Lipkin",
            "Cedegao Zhang",
            "Armando Solar-Lezama",
            "Joshua Tenenbaum",
            "Roger Levy"
        ],
        "published": "2023",
        "summary": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.313.pdf"
    },
    {
        "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing",
        "authors": [
            "Nam Nguyen",
            "Thang Phan",
            "Duc-Vu Nguyen",
            "Kiet Nguyen"
        ],
        "published": "2023",
        "summary": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes. Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.315.pdf"
    },
    {
        "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
        "authors": [
            "Zekun Li",
            "Wenxuan Zhou",
            "Yao-Yi Chiang",
            "Muhao Chen"
        ],
        "published": "2023",
        "summary": "Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a geospatially grounded language model that enhances the understanding of geo-entities in natural language. GeoLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GeoLM connects the two types of context through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations to capture geospatial context. In the experiment, we demonstrate that GeoLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences. The code is publicly available at https://github.com/knowledge-computing/geolm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.317.pdf"
    },
    {
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
        "authors": [
            "Zhiqiang Hu",
            "Lei Wang",
            "Yihuai Lan",
            "Wanyu Xu",
            "Ee-Peng Lim",
            "Lidong Bing",
            "Xing Xu",
            "Soujanya Poria",
            "Roy Lee"
        ],
        "published": "2023",
        "summary": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.319.pdf"
    },
    {
        "title": "Query Rewriting in Retrieval-Augmented Large Language Models",
        "authors": [
            "Xinbei Ma",
            "Yeyun Gong",
            "Pengcheng He",
            "Hai Zhao",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.322.pdf"
    },
    {
        "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
        "authors": [
            "Gaurav Sahu",
            "Olga Vechtomova",
            "Dzmitry Bahdanau",
            "Issam Laradji"
        ],
        "published": "2023",
        "summary": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM\u2019s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT-base and BERT-base. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.323.pdf"
    },
    {
        "title": "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing",
        "authors": [
            "Yating Wu",
            "Ritika Mangla",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "published": "2023",
        "summary": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.325.pdf"
    },
    {
        "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
        "authors": [
            "Haoyan Yang",
            "Zhitao Li",
            "Yong Zhang",
            "Jianzong Wang",
            "Ning Cheng",
            "Ming Li",
            "Jing Xiao"
        ],
        "published": "2023",
        "summary": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA\u2019s effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.326.pdf"
    },
    {
        "title": "Exploring Chain of Thought Style Prompting for Text-to-SQL",
        "authors": [
            "Chang-Yu Tai",
            "Ziru Chen",
            "Tianshu Zhang",
            "Xiang Deng",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs\u2019 reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting and least-to-most prompting. Our experiments demonstrate that iterative prompting as in least-to-most prompting may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.327.pdf"
    },
    {
        "title": "Harnessing Black-Box Control to Boost Commonsense in LM\u2019s Generation",
        "authors": [
            "Yufei Tian",
            "Felix Zhang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a fixed PTLM to better satisfy the oracle. We test our framework on a series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two constrained concept-to-sentence benchmarks. Human evaluation results demonstrate that our method consistently leads to the most commonsensical outputs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.329.pdf"
    },
    {
        "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Allan Zhou",
            "Archit Sharma",
            "Rafael Rafailov",
            "Huaxiu Yao",
            "Chelsea Finn",
            "Christopher Manning"
        ],
        "published": "2023",
        "summary": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model\u2019s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.330.pdf"
    },
    {
        "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
        "authors": [
            "Lovisa Hagstr\u00f6m",
            "Denitsa Saynova",
            "Tobias Norlund",
            "Moa Johansson",
            "Richard Johansson"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer \u201cEdinburgh\u201d to \u201cAnne Redpath passed away in X.\u201d and \u201cLondon\u201d to \u201cAnne Redpath\u2019s life ended in X.\u201d In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a passage retrieval database. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency but that retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.332.pdf"
    },
    {
        "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
        "authors": [
            "Junpeng Li",
            "Zixia Jia",
            "Zilong Zheng"
        ],
        "published": "2023",
        "summary": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating an LLM and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.334.pdf"
    },
    {
        "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents",
        "authors": [
            "Hyungjoo Chae",
            "Yongho Song",
            "Kai Ong",
            "Taeyoon Kwon",
            "Minjin Kim",
            "Youngjae Yu",
            "Dongha Lee",
            "Dongyeop Kang",
            "Jinyoung Yeo"
        ],
        "published": "2023",
        "summary": "Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We conduct extensive experiments to show that enhancing dialogue agents with high-quality rationales from DOCTOR significantly improves the quality of their responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.342.pdf"
    },
    {
        "title": "C-STS: Conditional Semantic Textual Similarity",
        "authors": [
            "Ameet Deshpande",
            "Carlos Jimenez",
            "Howard Chen",
            "Vishvak Murahari",
            "Victoria Graf",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Danqi Chen",
            "Karthik Narasimhan"
        ],
        "published": "2023",
        "summary": "Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called conditional STS (C-STS) which measures similarity conditioned on an aspect elucidated in natural language (hereon, condition). As an example, the similarity between the sentences \u201cThe NBA player shoots a three-pointer.\u201d and \u201cA man throws a tennis ball into the air to serve.\u201d is higher for the condition \u201cThe motion of the ball.\u201d (both upward) and lower for \u201cThe size of the ball.\u201d (one large and one small). C-STS\u2019s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS, and (2) enables fine-grained similarity evaluation using diverse conditions. C-STS contains almost 20,000 instances from diverse domains and we evaluate several state-of-the-art models to demonstrate that even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging, with Spearman correlation scores of <50. We encourage the community to evaluate their models on C-STS to provide a more holistic view of semantic similarity and natural language understanding.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.345.pdf"
    },
    {
        "title": "Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks",
        "authors": [
            "Chang Yang",
            "Peng Zhang",
            "Wenbo Qiao",
            "Hui Gao",
            "Jiaming Zhao"
        ],
        "published": "2023",
        "summary": "In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text encoding sequences, difficulties in domain knowledge coverage and effective information extraction with knowledge graph-based methods, and insufficient mining of semantic structural information. To address these issues, we propose a Crowd Intelligence and ChatGPT-Assisted Network(CICAN) for rumor classification. Specifically, we present a crowd intelligence-based semantic feature learning module to capture textual content\u2019s sequential and hierarchical features. Then, we design a knowledge-based semantic structural mining module that leverages ChatGPT for knowledge enhancement. Finally, we construct an entity-sentence heterogeneous graph and design Entity-Aware Heterogeneous Attention to effectively integrate diverse structural information meta-paths. Experimental results demonstrate that CICAN achieves performance improvement in rumor detection tasks, validating the effectiveness and rationality of using large language models as auxiliary tools.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.347.pdf"
    },
    {
        "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts",
        "authors": [
            "Truong Giang Do",
            "Le Khiem",
            "Quang Pham",
            "TrungTin Nguyen",
            "Thanh-Nam Doan",
            "Binh Nguyen",
            "Chenghao Liu",
            "Savitha Ramasamy",
            "Xiaoli Li",
            "Steven Hoi"
        ],
        "published": "2023",
        "summary": "By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains. This work introduces HyperRouter, which dynamically generates the router\u2019s parameters through a fixed hypernetwork and trainable embeddings to achieve a balance between training the routers and freezing them to learn an improved routing policy. Extensive experiments across a wide range of tasks demonstrate the superior performance and efficiency gains of HyperRouter compared to existing routing methods. Our implementation is publicly available at https://github.com/giangdip2410/HyperRouter.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.351.pdf"
    },
    {
        "title": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",
        "authors": [
            "Silei Xu",
            "Shicheng Liu",
            "Theo Culhane",
            "Elizaveta Pertseva",
            "Meng-Hsi Wu",
            "Sina Semnani",
            "Monica Lam"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation. This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the unique domain and property names instead of their IDs. We train the parser to use either the results from an entity linker or mentions in the query. We fine-tune LLaMA by adding the few-shot training data to that used to fine-tune Alpaca. Our experimental results demonstrate the effectiveness of this methodology, establishing a strong baseline of 76% and 65% answer accuracy in the dev and test sets of WikiWebQuestions, respectively. By pairing our semantic parser with GPT-3, we combine verifiable results with qualified GPT-3 guesses to provide useful answers to 96% of the questions in dev. We also show that our method outperforms the state-of-the-art for the QALD-7 Wikidata dataset by 3.6% in F1 score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.353.pdf"
    },
    {
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Jason Wolfe",
            "Subhro Roy"
        ],
        "published": "2023",
        "summary": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. We address this by fine-tuning a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can zero-shot parse \u2248 16% of utterances in the MTOP dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.354.pdf"
    },
    {
        "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
        "authors": [
            "Qian Chen",
            "Wen Wang",
            "Qinglin Zhang",
            "Siqi Zheng",
            "Chong Deng",
            "Hai Yu",
            "Jiaqing Liu",
            "Yukun Ma",
            "Chong Zhang"
        ],
        "published": "2023",
        "summary": "Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on the STS benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.359.pdf"
    },
    {
        "title": "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
        "authors": [
            "Ji Qi",
            "Chuchun Zhang",
            "Xiaozhi Wang",
            "Kaisheng Zeng",
            "Jifan Yu",
            "Jinxin Liu",
            "Lei Hou",
            "Juanzi Li",
            "Xu Bin"
        ],
        "published": "2023",
        "summary": "The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code will be publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.360.pdf"
    },
    {
        "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought",
        "authors": [
            "Ori Yoran",
            "Tomer Wolfson",
            "Ben Bogin",
            "Uri Katz",
            "Daniel Deutch",
            "Jonathan Berant"
        ],
        "published": "2023",
        "summary": "Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.364.pdf"
    },
    {
        "title": "INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback",
        "authors": [
            "Wenda Xu",
            "Danqing Wang",
            "Liangming Pan",
            "Zhenqiao Song",
            "Markus Freitag",
            "William Wang",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.365.pdf"
    },
    {
        "title": "Multi-level Contrastive Learning for Script-based Character Understanding",
        "authors": [
            "Dawei Li",
            "Hengyuan Zhang",
            "Yanran Li",
            "Shiping Yang"
        ],
        "published": "2023",
        "summary": "In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters\u2019 personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters\u2019 global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work in this URL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.366.pdf"
    },
    {
        "title": "CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients",
        "authors": [
            "Jaehyung Seo",
            "Hyeonseok Moon",
            "Jaewook Lee",
            "Sugyeong Eo",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the syntactic-semantic integration of functional morphemes (i.e., affixes) to lexical morphemes (i.e., roots). With this in mind, we propose a method - CHEF, replicating the morphological transformations inherent in sentences based on lexical and functional morpheme combinations through generative data augmentation. CHEF operates using a morpheme blender and a label discriminator, thereby enhancing the diversity of Korean sentence forms by capturing the properties of agglutination while maintaining label consistency. We conduct experiments on Korean multiple classification datasets, improving model performance in full- and few-shot settings. Our proposed method boosts performance beyond the preceding data augmentation methods without incurring external data usage. We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.367.pdf"
    },
    {
        "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
        "authors": [
            "Kailai Yang",
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Sophia Ananiadou"
        ],
        "published": "2023",
        "summary": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.370.pdf"
    },
    {
        "title": "Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue",
        "authors": [
            "Aishwarya Padmakumar",
            "Mert Inan",
            "Spandana Gella",
            "Patrick Lange",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023",
        "summary": "Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and ego-centric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of abstraction called a plan, which helps make agent actions more interpretable and can be obtained from the appropriate prompting of large language models. We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans. Since collecting human-human dialogues for embodied environments is expensive and time-consuming, we propose a method to synthetically generate such dialogues, which we then use as training data for plan prediction. We demonstrate that multimodal transformer models can attain strong zero-shot performance from our synthetic data, outperforming language-only models trained on human-human data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.374.pdf"
    },
    {
        "title": "GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree",
        "authors": [
            "Zirui Shao",
            "Feiyu Gao",
            "Zhongda Qi",
            "Hangdi Xing",
            "Jiajun Bu",
            "Zhi Yu",
            "Qi Zheng",
            "Xiaozhong Liu"
        ],
        "published": "2023",
        "summary": "Inexhaustible web content carries abundant perceptible information beyond text. Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded. Intuitively, those perceptible web information can provide essential intelligence to facilitate content understanding tasks. This study presents an innovative Gestalt Enhanced Markup (GEM) Language Model inspired by Gestalt psychological theory for hosting heterogeneous visual information from the render tree into the language model without requiring additional visual input. Comprehensive experiments on multiple downstream tasks, i.e., web question answering and web information extraction, validate GEM superiority.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.375.pdf"
    },
    {
        "title": "Solving Hard Analogy Questions with Relation Embedding Chains",
        "authors": [
            "Nitesh Kumar",
            "Steven Schockaert"
        ],
        "published": "2023",
        "summary": "Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.382.pdf"
    },
    {
        "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
        "authors": [
            "Canwen Xu",
            "Daya Guo",
            "Nan Duan",
            "Julian McAuley"
        ],
        "published": "2023",
        "summary": "Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Additionally, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.385.pdf"
    },
    {
        "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
        "authors": [
            "Liang Chen",
            "Yang Deng",
            "Yatao Bian",
            "Zeyu Qin",
            "Bingzhe Wu",
            "Tat-Seng Chua",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives \u2013 Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.390.pdf"
    },
    {
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
            "Yucheng Li",
            "Bo Dong",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM\u2019s fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.391.pdf"
    },
    {
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
        "authors": [
            "Xiaonan Li",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, **MoT**, to let the LLM self-improve through **M**emory **o**f **T**houghts, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.392.pdf"
    },
    {
        "title": "Can You Follow Me? Testing Situational Understanding for ChatGPT",
        "authors": [
            "Chenghao Yang",
            "Allyson Ettinger"
        ],
        "published": "2023",
        "summary": "Understanding sentence meanings and updating information states appropriately across time\u2014what we call \u201csituational understanding\u201d (SU)\u2014is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models\u2019 ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model\u2019s performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates\u2014including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/yangalan123/SituationalTesting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.394.pdf"
    },
    {
        "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
        "authors": [
            "Kellin Pelrine",
            "Anne Imouza",
            "Camille Thibault",
            "Meilina Reksoprodjo",
            "Caleb Gupta",
            "Joel Christoph",
            "Jean-Fran\u00e7ois Godbout",
            "Reihaneh Rabbany"
        ],
        "published": "2023",
        "summary": "Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.395.pdf"
    },
    {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.397.pdf"
    },
    {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs\u2019 Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions\u2014fluency, correctness, and citation quality\u2014and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement\u2014For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.398.pdf"
    },
    {
        "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
        "authors": [
            "Shuwen Deng",
            "Paul Prasse",
            "David Reich",
            "Tobias Scheffer",
            "Lena J\u00e4ger"
        ],
        "published": "2023",
        "summary": "Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of human-like scanpaths during reading have been developed, the potential of synthetic gaze data across NLP tasks remains largely unexplored. We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. Since the model\u2019s error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks. We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data. Our code is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.400.pdf"
    },
    {
        "title": "Counting the Bugs in ChatGPT\u2019s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
        "authors": [
            "Leonie Weissweiler",
            "Valentin Hofmann",
            "Anjali Kantharuban",
            "Anna Cai",
            "Ritam Dutt",
            "Amey Hengle",
            "Anubha Kabra",
            "Atharva Kulkarni",
            "Abhishek Vijayakumar",
            "Haofei Yu",
            "Hinrich Schuetze",
            "Kemal Oflazer",
            "David Mortensen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko\u2019s (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results\u2014through the lens of morphology\u2014cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.401.pdf"
    },
    {
        "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
        "authors": [
            "Quanyu Long",
            "Wenya Wang",
            "Sinno Pan"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.402.pdf"
    },
    {
        "title": "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
        "authors": [
            "Davis Brown",
            "Charles Godfrey",
            "Nicholas Konz",
            "Jonathan Tu",
            "Henry Kvinge"
        ],
        "published": "2023",
        "summary": "As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model\u2019s internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions, (ii) evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance, and (iii) new evaluations of how language model features vary as width and depth are increased. Our results suggest that dissimilarity measures are a promising set of tools for shedding light on the inner workings of language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.403.pdf"
    },
    {
        "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
        "authors": [
            "Deepak Nathani",
            "David Wang",
            "Liangming Pan",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through *self-improvement* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose **Multi-Aspect Feedback**, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.407.pdf"
    },
    {
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "authors": [
            "Hailin Chen",
            "Amrita Saha",
            "Steven Hoi",
            "Shafiq Joty"
        ],
        "published": "2023",
        "summary": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher\u2019s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.417.pdf"
    },
    {
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "authors": [
            "Raghav Jain",
            "Daivik Sojitra",
            "Arkadeep Acharya",
            "Sriparna Saha",
            "Adam Jatowt",
            "Sandipan Dandapat"
        ],
        "published": "2023",
        "summary": "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.418.pdf"
    },
    {
        "title": "Evaluation of African American Language Bias in Natural Language Generation",
        "authors": [
            "Nicholas Deas",
            "Jessica Grieser",
            "Shana Kleiner",
            "Desmond Patton",
            "Elsbeth Turcan",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged \u201cstandard\u201d form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, and a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.421.pdf"
    },
    {
        "title": "A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems",
        "authors": [
            "Songbo Hu",
            "Han Zhou",
            "Moy Yuan",
            "Milan Gritta",
            "Guchun Zhang",
            "Ignacio Iacobacci",
            "Anna Korhonen",
            "Ivan Vuli\u0107"
        ],
        "published": "2023",
        "summary": "Achieving robust language technologies that can perform well across the world\u2019s many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of insights into the performance disparities of ToD systems in different languages, our analyses offer practical tips on how to approach ToD data collection and system development for new languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.422.pdf"
    },
    {
        "title": "Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction",
        "authors": [
            "V.S.D.S.Mahesh Akavarapu",
            "Arnab Bhattacharya"
        ],
        "published": "2023",
        "summary": "Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in this area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model outperforms the existing models on both the tasks, especially when it is pre-trained on masked word prediction task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.423.pdf"
    },
    {
        "title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning",
        "authors": [
            "Ximing Lu",
            "Faeze Brahman",
            "Peter West",
            "Jaehun Jung",
            "Khyathi Chandu",
            "Abhilasha Ravichander",
            "Prithviraj Ammanabrolu",
            "Liwei Jiang",
            "Sahana Ramnath",
            "Nouha Dziri",
            "Jillian Fisher",
            "Bill Lin",
            "Skyler Hallinan",
            "Lianhui Qin",
            "Xiang Ren",
            "Sean Welleck",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.424.pdf"
    },
    {
        "title": "EtiCor: Corpus for Analyzing LLMs for Etiquettes",
        "authors": [
            "Ashutosh Dwivedi",
            "Pradhyumna Lavania",
            "Ashutosh Modi"
        ],
        "published": "2023",
        "summary": "Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity. We experiment with state-of-the-art LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.428.pdf"
    },
    {
        "title": "An Investigation of LLMs\u2019 Inefficacy in Understanding Converse Relations",
        "authors": [
            "Chengwen Qi",
            "Bowen Li",
            "Binyuan Hui",
            "Bailin Wang",
            "Jinyang Li",
            "Jinwang Wu",
            "Yuanjun Laili"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs\u2019 ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.429.pdf"
    },
    {
        "title": "ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters",
        "authors": [
            "Vipul Rathore",
            "Rajdeep Dhingra",
            "Parag Singla",
            "Mausam"
        ],
        "published": "2023",
        "summary": "We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and other strong baselines on POS tagging and NER tasks. We also extend ZGUL to settings where either (1) some unlabeled data or (2) few-shot training examples are available for the target language. We find that ZGUL continues to outperform baselines in these settings too.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.431.pdf"
    },
    {
        "title": "Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue",
        "authors": [
            "Xue Han",
            "Yitong Wang",
            "Qian Hu",
            "Pengwei Hu",
            "Chao Deng",
            "Junlan Feng"
        ],
        "published": "2023",
        "summary": "Fine-grained address entity recognition (FGAER) from multi-turn spoken dialogues is particularly challenging. The major reason lies in that a full address is often formed through a conversation process. Different parts of an address are distributed through multiple turns of a dialogue with spoken noises. It is nontrivial to extract by turn and combine them. This challenge has not been well emphasized by main-stream entity extraction algorithms. To address this issue, we propose in this paper a logic-guided fine-grained address recognition method (Log-FGAER), where we formulate the address hierarchy relationship as the logic rule and softly apply it in a probabilistic manner to improve the accuracy of FGAER. In addition, we provide an ontology-based data augmentation methodology that employs ChatGPT to augment a spoken dialogue dataset with labeled address entities. Experiments are conducted using datasets generated by the proposed data augmentation technique and derived from real-world scenarios. The results of the experiment demonstrate the efficacy of our proposal.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.432.pdf"
    },
    {
        "title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning",
        "authors": [
            "Sarkar Snigdha Sarathi Das",
            "Haoran Zhang",
            "Peng Shi",
            "Wenpeng Yin",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings. The source code of FISH-DIP will be available at [this URL](https://github.com/psunlpgroup/FISH-DIP)",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.433.pdf"
    },
    {
        "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
        "authors": [
            "Alessandro Stolfo",
            "Yonatan Belinkov",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.435.pdf"
    },
    {
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "authors": [
            "Adithya Bhaskar",
            "Tushar Tomar",
            "Ashutosh Sathe",
            "Sunita Sarawagi"
        ],
        "published": "2023",
        "summary": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.436.pdf"
    },
    {
        "title": "Translating away Translationese without Parallel Data",
        "authors": [
            "Rricha Jalota",
            "Koel Chowdhury",
            "Cristina Espa\u00f1a-Bonet",
            "Josef van Genabith"
        ],
        "published": "2023",
        "summary": "Translated texts exhibit systematic linguistic differences compared to original texts in the same language, and these differences are referred to as translationese. Translationese has effects on various cross-lingual natural language processing tasks, potentially leading to biased results. In this paper, we explore a novel approach to reduce translationese in translated texts: translation-based style transfer. As there are no parallel human-translated and original data in the same language, we use a self-supervised approach that can learn from comparable (rather than parallel) mono-lingual original and translated data. However, even this self-supervised approach requires some parallel data for validation. We show how we can eliminate the need for parallel validation data by combining the self-supervised loss with an unsupervised loss. This unsupervised loss leverages the original language model loss over the style-transferred output and a semantic similarity loss between the input and style-transferred output. We evaluate our approach in terms of original vs. translationese binary classification in addition to measuring content preservation and target-style fluency. The results show that our approach is able to reduce translationese classifier accuracy to a level of a random classifier after style transfer while adequately preserving the content and fluency in the target original style.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.438.pdf"
    },
    {
        "title": "Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning",
        "authors": [
            "Xiao Yu",
            "Maximillian Chen",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.439.pdf"
    },
    {
        "title": "HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
        "authors": [
            "William Watson",
            "Nicole Cho",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "published": "2023",
        "summary": "A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed \u201cHiddenTables\u201d as a potential resolution to this challenge. In essence, \u201cHiddenTables\u201d is played between the code-generating LLM \u201cSolver\u201d and the \u201cOracle\u201d which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM\u2019s collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of \u201cHiddenTables\u201d to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset \u201cPyQTax\u201d that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs\u2019 deficiency in TableQA tasks, \u201cHiddenTables\u201d is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.442.pdf"
    },
    {
        "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
        "authors": [
            "Kent Chang",
            "Mackenzie Cramer",
            "Sandeep Soni",
            "David Bamman"
        ],
        "published": "2023",
        "summary": "In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.453.pdf"
    },
    {
        "title": "Copyright Violations and Large Language Models",
        "authors": [
            "Antonia Karamolegkou",
            "Jiaang Li",
            "Li Zhou",
            "Anders S\u00f8gaard"
        ],
        "published": "2023",
        "summary": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.458.pdf"
    },
    {
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": [
            "Jue Hou",
            "Anisia Katinskaia",
            "Anh-Duc Vu",
            "Roman Yangarber"
        ],
        "published": "2023",
        "summary": "Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation \u2014 Morfessor and StateMorph. We train the models for several languages \u2014 including ones with very rich morphology \u2014 and compare their performance with different segmentation algorithms, vocabulary sizes, and model sizes. The results show that training with morphological segmentation allows the LMs to: (1) achieve lower perplexity, (2) converge more efficiently in terms of training time, and (3) achieve equivalent or better evaluation scores on downstream tasks. Lastly, we show that (4) LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE \u2014 both in terms of (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.459.pdf"
    },
    {
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "authors": [
            "Justin Chiu",
            "Wenting Zhao",
            "Derek Chen",
            "Saujas Vaduguru",
            "Alexander Rush",
            "Daniel Fried"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.460.pdf"
    },
    {
        "title": "Universal Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Hanjun Dai",
            "Julian Eisenschlos",
            "Sercan Arik",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.461.pdf"
    },
    {
        "title": "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
        "authors": [
            "Biru Zhu",
            "Lifan Yuan",
            "Ganqu Cui",
            "Yangyi Chen",
            "Chong Fu",
            "Bingxiang He",
            "Yangdong Deng",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Ming Gu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a zero-shot black-box method for detecting LLM-generated texts. The key idea is to revise the text to be detected using the ChatGPT model. Our method is based on the intuition that the ChatGPT model will make fewer revisions to LLM-generated texts than it does to human-written texts, because the texts generated by LLMs are more in accord with the generation logic and statistical patterns learned by LLMs like ChatGPT. Thus, if the text to be detected and its ChatGPT-revised version have a higher degree of similarity, the text is more likely to be LLM-generated. Extensive experiments on various datasets and tasks show that our method can effectively detect LLM-generated texts. Moreover, compared with other detection methods, our method has better generalization ability and is more stable across various datasets. The codes are publicly available at https://github.com/thunlp/LLM-generated-text-detection.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.463.pdf"
    },
    {
        "title": "Faithful Model Evaluation for Model-Based Metrics",
        "authors": [
            "Qian Hu",
            "Palash Goyal",
            "Rahul Gupta"
        ],
        "published": "2023",
        "summary": "Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a function of sample variance. Sample variance calculation is straightforward when evaluating against ground truth. However, in many cases, a metric model is often used for evaluation. For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions. In this work, we establish the mathematical foundation of significance testing for model-based metrics. With experiments on public benchmark datasets and a production system, we show that considering metric model errors to calculate sample variances for model-based metrics changes the conclusions in certain experiments.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.464.pdf"
    },
    {
        "title": "Content- and Topology-Aware Representation Learning for Scientific Multi-Literature",
        "authors": [
            "Kai Zhang",
            "Kaisong Song",
            "Yangyang Kang",
            "Xiaozhong Liu"
        ],
        "published": "2023",
        "summary": "Representation learning forms an essential building block in the development of natural language processing architectures. To date, mainstream approaches focus on learning textual information at the sentence- or document-level, unfortunately, overlooking the inter-document connections. This omission decreases the potency of downstream applications, particularly in multi-document settings. To address this issue, embeddings equipped with latent semantic and rich relatedness information are needed. In this paper, we propose SMRC2, which extends representation learning to the multi-document level. Our model jointly learns latent semantic information from content and rich relatedness information from topological networks. Unlike previous studies, our work takes multi-document as input and integrates both semantic and relatedness information using a shared space via language model and graph structure. Our extensive experiments confirm the superiority and effectiveness of our approach. To encourage further research in scientific multi-literature representation learning, we will release our code and a new dataset from the biomedical domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.465.pdf"
    },
    {
        "title": "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
        "authors": [
            "Ethan Wilcox",
            "Clara Meister",
            "Ryan Cotterell",
            "Tiago Pimentel"
        ],
        "published": "2023",
        "summary": "Surprisal theory (Hale, 2001; Levy, 2008) posits that a word\u2019s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word\u2019s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior\u2014a conjecture we dub the quality\u2013power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.466.pdf"
    },
    {
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "authors": [
            "Daman Arora",
            "Himanshu Singh",
            "Mausam"
        ],
        "published": "2023",
        "summary": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.468.pdf"
    },
    {
        "title": "WiCE: Real-World Entailment for Claims in Wikipedia",
        "authors": [
            "Ryo Kamoi",
            "Tanya Goyal",
            "Juan Diego Rodriguez",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models\u2019 performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.470.pdf"
    },
    {
        "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
        "authors": [
            "Boxin Wang",
            "Wei Ping",
            "Peng Xu",
            "Lawrence McAfee",
            "Zihan Liu",
            "Mohammad Shoeybi",
            "Yi Dong",
            "Oleksii Kuchaiev",
            "Bo Li",
            "Chaowei Xiao",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "published": "2023",
        "summary": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our implementation at: https://github.com/NVIDIA/Megatron-LM/tree/main/tools/retro.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.482.pdf"
    },
    {
        "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
        "authors": [
            "Xinyuan Lu",
            "Liangming Pan",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan"
        ],
        "published": "2023",
        "summary": "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.483.pdf"
    },
    {
        "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
        "authors": [
            "Zedian Xiao",
            "William Held",
            "Yanchen Liu",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior work on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performance across 5 dialects in a zero-shot setting. In this way, our approach facilitates access to language technology for billions of English dialect speakers who are traditionally underrepresented.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.487.pdf"
    },
    {
        "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
        "authors": [
            "Tianshi Che",
            "Ji Liu",
            "Yang Zhou",
            "Jiaxiang Ren",
            "Jiwen Zhou",
            "Victor Sheng",
            "Huaiyu Dai",
            "Dejing Dou"
        ],
        "published": "2023",
        "summary": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8% in terms of accuracy) and efficiency (up to 97.59% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.488.pdf"
    },
    {
        "title": "TheoremQA: A Theorem-driven Question Answering Dataset",
        "authors": [
            "Wenhu Chen",
            "Ming Yin",
            "Max Ku",
            "Pan Lu",
            "Yixin Wan",
            "Xueguang Ma",
            "Jianyu Xu",
            "Xinyi Wang",
            "Tony Xia"
        ],
        "published": "2023",
        "summary": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models\u2019 capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4\u2019s capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs\u2019 capabilities to solve challenging science problems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.489.pdf"
    },
    {
        "title": "Don\u2019t Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
        "authors": [
            "Xiang Zhang",
            "Senyu Li",
            "Bradley Hauer",
            "Ning Shi",
            "Grzegorz Kondrak"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.491.pdf"
    },
    {
        "title": "Empirical Study of Zero-Shot NER with ChatGPT",
        "authors": [
            "Tingyu Xie",
            "Qi Li",
            "Jian Zhang",
            "Yan Zhang",
            "Zuozhu Liu",
            "Hongwei Wang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model\u2019s intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.493.pdf"
    },
    {
        "title": "Automatic Prompt Optimization with \u201cGradient Descent\u201d and Beam Search",
        "authors": [
            "Reid Pryzant",
            "Dan Iter",
            "Jerry Li",
            "Yin Lee",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language \u201cgradients\u201d that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then \u201cpropagated\u201d into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt\u2019s performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.494.pdf"
    },
    {
        "title": "Active Retrieval Augmented Generation",
        "authors": [
            "Zhengbao Jiang",
            "Frank Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2023",
        "summary": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.495.pdf"
    },
    {
        "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
        "authors": [
            "Chengcheng Han",
            "Xiaowei Du",
            "Che Zhang",
            "Yixin Lian",
            "Xiang Li",
            "Ming Gao",
            "Baoyuan Wang"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. In this paper, we propose Dialogue-guided Chain-of-Thought (DialCoT) to improve the reasoning capabilities of SLMs, with the aim of generating intermediate reasoning steps in a dialogue format to guide the model to the final answer. Furthermore, we optimize the model to choose the optimal reasoning path through the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Compared to previous methods, our advantages lie in: 1) We transform the process of solving complex reasoning problems into decomposing problems and solving a series of simpler sub-questions, significantly reducing task difficulty and making it more suitable for SLMs. 2) We optimize the model to choose the optimal reasoning path through the PPO algorithm. Comprehensive experiments on four arithmetic reasoning datasets show that our method can achieve significant performance gains over state-of-the-art competitors.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.501.pdf"
    },
    {
        "title": "Background Summarization of Event Timelines",
        "authors": [
            "Adithya Pratapa",
            "Kevin Small",
            "Markus Dreyer"
        ],
        "published": "2023",
        "summary": "Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experiments show the effectiveness of instruction fine-tuned systems such as Flan-T5, in addition to strong zero-shot performance using GPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.505.pdf"
    },
    {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Hong",
            "Zhen Wang",
            "Daisy Wang",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs\u2019 absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.507.pdf"
    },
    {
        "title": "LLM-enhanced Self-training for Cross-domain Constituency Parsing",
        "authors": [
            "Jianling Li",
            "Meishan Zhang",
            "Peiming Guo",
            "Min Zhang",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM\u2019s performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.508.pdf"
    },
    {
        "title": "Editing Common Sense in Transformers",
        "authors": [
            "Anshita Gupta",
            "Debanjan Mondal",
            "Akshay Sheshadri",
            "Wenlong Zhao",
            "Xiang Li",
            "Sarah Wiegreffe",
            "Niket Tandon"
        ],
        "published": "2023",
        "summary": "Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers\u2019 reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., MEMITCSK. GPT-2 Large and XL models edited using MEMITCSK outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE\\ SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. MEMITCSK performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.511.pdf"
    },
    {
        "title": "Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System",
        "authors": [
            "Weizhou Shen",
            "Yingqi Gao",
            "Canbin Huang",
            "Fanqi Wan",
            "Xiaojun Quan",
            "Wei Bi"
        ],
        "published": "2023",
        "summary": "Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage high-quality knowledge records from the retriever and enhance the quality of generated responses. The code of this work is available at https://github.com/shenwzh3/MK-TOD.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.514.pdf"
    },
    {
        "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions",
        "authors": [
            "Wenhao Yu",
            "Meng Jiang",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023",
        "summary": "Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA, where each question is based on a counterfactual presupposition via an \u201cif\u201d clause. Such questions require models to go beyond retrieving direct factual knowledge from the Web: they must identify the right information to retrieve and reason about an imagined situation that may even go against the facts built into their parameters. The IfQA dataset contains 3,800 questions that were annotated by crowdworkers on relevant Wikipedia passages. Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (F1 score 44.5), as well as recent few-shot approaches such as chain-of-thought prompting with ChatGPT (F1 score 57.2). We hope the unique challenges posed by IfQA will push open-domain QA research on both retrieval and reasoning fronts, while also helping endow counterfactual reasoning abilities to today\u2019s language understanding models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.515.pdf"
    },
    {
        "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad",
            "Jun Wang"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to facilitate research in this field.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.516.pdf"
    },
    {
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Fei Liu"
        ],
        "published": "2023",
        "summary": "Human preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativeness, fluency, and factual consistency are still not well understood. It is also unclear if there are other hidden factors influencing human judgments. In this paper, we conduct an in-depth examination of a collection of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in these human judgments. We find that the most favored factors vary across tasks and genres, whereas the least favored factors tend to be consistent, e.g., outputs are too brief, contain excessive off-focus content or hallucinated facts. Our findings have implications on the construction of balanced datasets in human preference evaluations, which is a crucial step in shaping the behaviors of future LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.519.pdf"
    },
    {
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": [
            "Jiacheng Ye",
            "Chengzu Li",
            "Lingpeng Kong",
            "Tao Yu"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.523.pdf"
    },
    {
        "title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP",
        "authors": [
            "Sreyan Ghosh",
            "Chandra Kiran Reddy Evuru",
            "Sonal Kumar",
            "S Ramaneswaran",
            "S Sakshi",
            "Utkarsh Tyagi",
            "Dinesh Manocha"
        ],
        "published": "2023",
        "summary": "We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans help DALE acquire broad legal knowledge and develop the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.528.pdf"
    },
    {
        "title": "trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback",
        "authors": [
            "Alexander Havrilla",
            "Maksym Zhuravinskyi",
            "Duy Phung",
            "Aman Tiwari",
            "Jonathan Tow",
            "Stella Biderman",
            "Quentin Anthony",
            "Louis Castricato"
        ],
        "published": "2023",
        "summary": "Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (ILQL) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.530.pdf"
    },
    {
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": [
            "Iker Garc\u00eda-Ferrero",
            "Bego\u00f1a Altuna",
            "Javier Alvez",
            "Itziar Gonzalez-Dios",
            "German Rigau"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.531.pdf"
    },
    {
        "title": "MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning",
        "authors": [
            "Chunyou Li",
            "Mingtong Liu",
            "Hongxiao Zhang",
            "Yufeng Chen",
            "Jinan Xu",
            "Ming Zhou"
        ],
        "published": "2023",
        "summary": "Sentence-level translation, document-level translation, translation memory, and terminology constrained translation play an important role in machine translation. Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction. In this work, we explore the potential of pre-trained language model in machine translation tasks and propose a Multi-Task Machine Translation (MT2) model to integrate these translation tasks. We design a novel translation-specific In-Context Learning (ICL) paradigm for model training, in which all of the translation tasks can be modeled as context-learning tasks that integrate contextual information for performance improvement. Specifically, we propose a retrieval and alignment method to obtain a large scale context-enhancement training data, then we train the model in an in-context learning manner. Furthermore, we adopt two context-dependent training strategies to encourage the model to better understand and utilize contextual information for translation. Extensive experiments on translation memory, terminology constrained translation, document-level translation, and few-shot domain-adaptation tasks demonstrate the superior performance of our model, verifying the effectiveness of our proposed approach.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.532.pdf"
    },
    {
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": [
            "Jia Peng Lim",
            "Hady Lauw"
        ],
        "published": "2023",
        "summary": "Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based Language Models (TLM) similarly consist of decoder weights. However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable. Therefore, we posit that we can interpret TLM as superposed NTM by proposing a novel weight-based, model-agnostic and corpus-agnostic approach to search and disentangle decoder-only TLM, potentially mapping individual neurons to multiple coherent topics. Our results show that it is empirically feasible to disentangle coherent topics from GPT-2 models using the Wikipedia corpus. We validate this approach for GPT-2 models using Zero-Shot Topic Modelling. Finally, we extend the proposed approach to disentangle and analyse LLaMA models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.534.pdf"
    },
    {
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Pan",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.538.pdf"
    },
    {
        "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
        "authors": [
            "Catalina Goanta",
            "Nikolaos Aletras",
            "Ilias Chalkidis",
            "Sofia Ranchord\u00e1s",
            "Gerasimos Spanakis"
        ],
        "published": "2023",
        "summary": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.539.pdf"
    },
    {
        "title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",
        "authors": [
            "Zexue He",
            "Yu Wang",
            "An Yan",
            "Yao Liu",
            "Eric Chang",
            "Amilcare Gentili",
            "Julian McAuley",
            "Chun-Nan Hsu"
        ],
        "published": "2023",
        "summary": "Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.540.pdf"
    },
    {
        "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
        "authors": [
            "Andrea Sottana",
            "Bin Liang",
            "Kai Zou",
            "Zheng Yuan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models\u2019 performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models\u2019 outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models\u2019 outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.543.pdf"
    },
    {
        "title": "Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering",
        "authors": [
            "Wang Zhu",
            "Jesse Thomason",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "We propose Chain-of-Questions, a framework that trains a model to robustly answer multistep questions by generating and answering sub-questions. We obtain supervision for sub-questions from human-annotated question decomposition meaning representation (QDMR), but QDMR does not include annotated answers to sub-questions. To overcome this technical challenge, we treat sub-answers as latent variables and infer them with a novel dynamic mixture of Hard-EM and MAPO. Chain-of-Questions is effective and robust, greatly outperforming strong neuro-symbolic methods by 9.0 F1 on a DROP contrast set and GPT-3.5 by 24.3 F1 on a HotpotQA adversarial set.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.547.pdf"
    },
    {
        "title": "\u201cMistakes Help Us Grow\u201d: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
        "authors": [
            "Kunal Handa",
            "Margarett Clapper",
            "Jessica Boyle",
            "Rose Wang",
            "Diyi Yang",
            "David Yeager",
            "Dorottya Demszky"
        ],
        "published": "2023",
        "summary": "Teachers\u2019 growth mindset supportive language (GMSL)\u2014rhetoric emphasizing that one\u2019s skills can be improved over time\u2014has been shown to significantly reduce disparities in academic achievement and enhance students\u2019 learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers\u2019 use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers\u2019 unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs\u2019 potentiality for supporting students\u2019 learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.549.pdf"
    },
    {
        "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
        "authors": [
            "Qi Cao",
            "Takeshi Kojima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "published": "2023",
        "summary": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that multiple advanced LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.550.pdf"
    },
    {
        "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
        "authors": [
            "Yifu Qiu",
            "Yftah Ziser",
            "Anna Korhonen",
            "Edoardo Ponti",
            "Shay Cohen"
        ],
        "published": "2023",
        "summary": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where summarisation requires cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. Through extensive experiments in multiple languages, we demonstrate that mFACT is best suited to detect hallucinations compared to alternative metrics. With mFACT, we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English. We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. This method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.551.pdf"
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "Mark Gales"
        ],
        "published": "2023",
        "summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \u201cSelfCheckGPT\u201d, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.557.pdf"
    },
    {
        "title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications",
        "authors": [
            "Manuel Faysse",
            "Gautier Viaud",
            "C\u00e9line Hudelot",
            "Pierre Colombo"
        ],
        "published": "2023",
        "summary": "Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.559.pdf"
    },
    {
        "title": "Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer",
        "authors": [
            "Hsiu-Wen Li",
            "Ying-Jia Lin",
            "Yi-Ting Li",
            "Chun Lin",
            "Hung-Yu Kao"
        ],
        "published": "2023",
        "summary": "Unsupervised Chinese word segmentation (UCWS) has made progress by incorporating linguistic knowledge from pre-trained language models using parameter-free probing techniques. However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation. This work introduces a novel way to enhance UCWS performance while maintaining training efficiency. Our proposed method integrates the segmentation signal from the unsupervised segmental language model to the pre-trained BERT classifier under a pseudo-labeling framework. Experimental results demonstrate that our approach achieves state-of-the-art performance on the eight UCWS tasks while considerably reducing the training time compared to previous approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.564.pdf"
    },
    {
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "authors": [
            "Hanlin Tang",
            "Yifu Sun",
            "Decheng Wu",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.565.pdf"
    },
    {
        "title": "APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models",
        "authors": [
            "Qifan Wang",
            "Yuning Mao",
            "Jingang Wang",
            "Hanchao Yu",
            "Shaoliang Nie",
            "Sinong Wang",
            "Fuli Feng",
            "Lifu Huang",
            "Xiaojun Quan",
            "Zenglin Xu",
            "Dongfang Liu"
        ],
        "published": "2023",
        "summary": "With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely APrompt, for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce APrompt, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.567.pdf"
    },
    {
        "title": "Learning Preference Model for LLMs via Automatic Preference Data Generation",
        "authors": [
            "Shijia Huang",
            "Jianqiao Zhao",
            "Yanyang Li",
            "Liwei Wang"
        ],
        "published": "2023",
        "summary": "Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability. In this paper, we propose learning the preference model for LLMs via automatic preference data generation (AutoPM). Our approach involves both In-Breadth Data Generation, which elicits pairwise preference data from LLMs following the helpful-honest-harmless (HHH) criteria, and In-Depth Data Generation, which enriches the dataset with responses spanning a wide quality range. With HHH-guided preference data, our approach simultaneously enables the LLMs to learn human preferences and align with human values. Quantitative assessments on five benchmark datasets demonstrate the reliability and potential of AutoPM, pointing out a more general and scalable way to improve LLM performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.570.pdf"
    },
    {
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Zican Dong",
            "Keming Ye",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs, under the few-shot and zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.574.pdf"
    },
    {
        "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
        "authors": [
            "Mrigank Raman",
            "Pratyush Maini",
            "J Kolter",
            "Zachary Lipton",
            "Danish Pruthi"
        ],
        "published": "2023",
        "summary": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token\u2019s hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.576.pdf"
    },
    {
        "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
        "authors": [
            "Dominik Stammbach",
            "Vil\u00e9m Zouhar",
            "Alexander Hoyle",
            "Mrinmaya Sachan",
            "Elliott Ash"
        ],
        "published": "2023",
        "summary": "Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial \u2014 LLMs perform better on coherence ratings of word sets than on intrustion detection. We find that LLMs can also assist us in guiding us towards a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimating the optimal number of topics.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.581.pdf"
    },
    {
        "title": "ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization",
        "authors": [
            "Xiutian Zhao",
            "Ke Wang",
            "Wei Peng"
        ],
        "published": "2023",
        "summary": "Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.582.pdf"
    },
    {
        "title": "Query2doc: Query Expansion with Large Language Models",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.585.pdf"
    },
    {
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Tao Yang",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023",
        "summary": "Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model\u2019s performance demonstrates considerable advancements over multiple baselines, including those utilizing domain-specific data enhancement. Our findings offer a promising opportunity to improve instruction coverage, especially in domain-specific contexts, thereby advancing the development of adaptable language models. Our code, model weights, and data are public at https://github.com/fanqiwan/Explore-Instruct.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.587.pdf"
    },
    {
        "title": "Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts",
        "authors": [
            "Jiashu Pu",
            "Ling Cheng",
            "Lu Fan",
            "Tangjie Lv",
            "Rongsheng Zhang"
        ],
        "published": "2023",
        "summary": "The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large language models (LLMs) that can be readily applied. However, many of these LLM-based metrics require selecting specific datasets and developing specialized training tasks for different evaluation dimensions (e.g., coherence, informative). The developing step can be time-consuming and may need to be repeated for new evaluation dimensions. To enable efficient and flexible adaptation to diverse needs of dialogue evaluation, we propose a dimension-agnostic scoring method that leverages the in-context learning (ICL) capability of LLMs to learn from human scoring to the fullest extent. Our method has three key features. To begin with, rather than manual prompt crafting, we propose automatically generating prompts, allowing the LLM to observe human labels and summarize the most suitable prompt. Additionally, since the LLM has a token limit and ICL is sensitive to demonstration variations, we train a selector to finely customize demonstrations and prompts for each dialogue input. Finally, during inference, we propose to request the LLM multiple times with a subgraph of demonstrations and prompts that are diverse and suitable to maximize ICL from various human scoring. We validate the efficacy of our method on five datasets, even with a small amount of annotated data, our method outperforms all strong baselines. Code is available at https://github.com/iamlxb3/EMNLP2023-ADOROR.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.590.pdf"
    },
    {
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "authors": [
            "Gabriele Prato",
            "Jerry Huang",
            "Prasanna Parthasarathi",
            "Shagun Sodhani",
            "Sarath Chandar"
        ],
        "published": "2023",
        "summary": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents\u2014a crucial ability in numerous applications\u2014remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs\u2019 proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.593.pdf"
    },
    {
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": [
            "Yaoyiran Li",
            "Anna Korhonen",
            "Ivan Vuli\u0107"
        ],
        "published": "2023",
        "summary": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.595.pdf"
    },
    {
        "title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",
        "authors": [
            "Kaiyan Zhang",
            "Ning Ding",
            "Biqing Qi",
            "Xuekai Zhu",
            "Xinwei Long",
            "Bowen Zhou"
        ],
        "published": "2023",
        "summary": "Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.597.pdf"
    },
    {
        "title": "Large Language Models are biased to overestimate profoundness",
        "authors": [
            "Eugenio Herrera-Berg",
            "Tom\u00e1s Browne",
            "Pablo Le\u00f3n-Villagr\u00e1",
            "Marc-Llu\u00eds Vives",
            "Cristian Calderon"
        ],
        "published": "2023",
        "summary": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.599.pdf"
    },
    {
        "title": "SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization",
        "authors": [
            "Philippe Laban",
            "Wojciech Kryscinski",
            "Divyansh Agarwal",
            "Alexander Fabbri",
            "Caiming Xiong",
            "Shafiq Joty",
            "Chien-Sheng Wu"
        ],
        "published": "2023",
        "summary": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs\u2019 ability to reason about facts and detect inconsistencies when they occur.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.600.pdf"
    },
    {
        "title": "Quantifying the redundancy between prosody and text",
        "authors": [
            "Lukas Wolf",
            "Tiago Pimentel",
            "Evelina Fedorenko",
            "Ryan Cotterell",
            "Alex Warstadt",
            "Ethan Wilcox",
            "Tamar Regev"
        ],
        "published": "2023",
        "summary": "Prosody\u2014the suprasegmental component of speech, including pitch, loudness, and tempo\u2014carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word\u2019s prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.606.pdf"
    },
    {
        "title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot",
        "authors": [
            "Aanisha Bhattacharyya",
            "Yaman K Singla",
            "Balaji Krishnamurthy",
            "Rajiv Ratn Shah",
            "Changyou Chen"
        ],
        "published": "2023",
        "summary": "Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.608.pdf"
    },
    {
        "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning",
        "authors": [
            "Lean Wang",
            "Lei Li",
            "Damai Dai",
            "Deli Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers\u2019 processing; (2) the consolidated information in label words serves as a reference for LLMs\u2019 final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.609.pdf"
    },
    {
        "title": "Prompting Scientific Names for Zero-Shot Species Recognition",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Yanan Li",
            "Shu Kong"
        ],
        "published": "2023",
        "summary": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., \u201ca photo of Lepus Timidus\u201d (which is a scientific name in Latin). This is because these names are usually not included in CLIP\u2019s training set. To improve performance, we explore using large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. However, this method improves only marginally. Instead, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP\u2019s training set, and prompting them achieves 2~5 times higher accuracy on benchmarking datasets of fine-grained species recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.610.pdf"
    },
    {
        "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
        "authors": [
            "Orevaoghene Ahia",
            "Sachin Kumar",
            "Hila Gonen",
            "Jungo Kasai",
            "David Mortensen",
            "Noah Smith",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of \u201ctokens\u201d processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API\u2019s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI\u2019s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs\u2019 pricing policies and encourage the vendors to make them more equitable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.614.pdf"
    },
    {
        "title": "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark",
        "authors": [
            "Dominik Macko",
            "Robert Moro",
            "Adaku Uchendu",
            "Jason Lucas",
            "Michiharu Yamashita",
            "Mat\u00fa\u0161 Pikuliak",
            "Ivan Srba",
            "Thai Le",
            "Dongwon Lee",
            "Jakub Simko",
            "Maria Bielikova"
        ],
        "published": "2023",
        "summary": "There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.616.pdf"
    },
    {
        "title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?",
        "authors": [
            "Cheng Zhang",
            "Jianyi Cheng",
            "Ilia Shumailov",
            "George Constantinides",
            "Yiren Zhao"
        ],
        "published": "2023",
        "summary": "The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a 19\u00d7 higher arithmetic density and 5\u00d7 memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by 2.5\u00d7 in arithmetic density and 1.2\u00d7 in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.617.pdf"
    },
    {
        "title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition",
        "authors": [
            "Srijith Radhakrishnan",
            "Chao-Han Yang",
            "Sumeer Khan",
            "Rohit Kumar",
            "Narsis Kiani",
            "David Gomez-Cabrero",
            "Jesper Tegn\u00e9r"
        ],
        "published": "2023",
        "summary": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we assess our fusion technique, demonstrating a 37.66% improvement in word error rate (WER) relative performance compared to the n-best Oracle. To encourage future research, we have made our code and pre-trained models open source at [https://github.com/Srijith-rkr/Whispering-LLaMA](https://github.com/Srijith-rkr/Whispering-LLaMA)",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.618.pdf"
    },
    {
        "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.619.pdf"
    },
    {
        "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
        "authors": [
            "Yizhu Jiao",
            "Ming Zhong",
            "Sha Li",
            "Ruining Zhao",
            "Siru Ouyang",
            "Heng Ji",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction \u2013 a classic task in natural language processing \u2013 most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.620.pdf"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
        "authors": [
            "Xiaolei Wang",
            "Xinyu Tang",
            "Xin Zhao",
            "Jingyuan Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To overcome the limitation, we further propose an **i**nteractive **Eva**luation approach based on **L**L**M**s, named **iEvaLM**, which harnesses LLM-based user simulators. Our evaluation approach can simulate various system-user interaction scenarios. Through the experiments on two public CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and realistic evaluation approach for future research about LLM-based CRSs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.621.pdf"
    },
    {
        "title": "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking",
        "authors": [
            "Arian Askari",
            "Mohammad Aliannejadi",
            "Chuan Meng",
            "Evangelos Kanoulas",
            "Suzan Verberne"
        ],
        "published": "2023",
        "summary": "Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance between generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize DocGen pipeline. Extensive experiments demonstrate that DocGen pipeline and DocGen-RL significantly outperform existing state-of-theart data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.623.pdf"
    },
    {
        "title": "Transformer-based Live Update Generation for Soccer Matches from Microblog Posts",
        "authors": [
            "Masashi Oshika",
            "Kosuke Yamada",
            "Ryohei Sasano",
            "Koichi Takeda"
        ],
        "published": "2023",
        "summary": "It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match\u2019s progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.624.pdf"
    },
    {
        "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
        "authors": [
            "Hye Yun",
            "Iain Marshall",
            "Thomas Trikalinos",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.626.pdf"
    },
    {
        "title": "SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2",
        "authors": [
            "Byungha Kang",
            "Youhyun Shin"
        ],
        "published": "2023",
        "summary": "We propose a novel unsupervised keyphrase extraction approach, called SAMRank, which uses only a self-attention map in a pre-trained language model (PLM) to determine the importance of phrases. Most recent approaches for unsupervised keyphrase extraction mainly utilize contextualized embeddings to capture semantic relevance between words, sentences, and documents. However, due to the anisotropic nature of contextual embeddings, these approaches may not be optimal for semantic similarity measurements. SAMRank as proposed here computes the importance of phrases solely leveraging a self-attention map in a PLM, in this case BERT and GPT-2, eliminating the need to measure embedding similarities. To assess the level of importance, SAMRank combines both global and proportional attention scores through calculations using a self-attention map. We evaluate the SAMRank on three keyphrase extraction datasets: Inspec, SemEval2010, and SemEval2017. The experimental results show that SAMRank outperforms most embedding-based models on both long and short documents and demonstrating that it is possible to use only a self-attention map for keyphrase extraction without relying on embeddings. Source code is available at https://github.com/kangnlp/SAMRank.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.630.pdf"
    },
    {
        "title": "Contrastive Learning for Inference in Dialogue",
        "authors": [
            "Etsuko Ishii",
            "Yan Xu",
            "Bryan Wilie",
            "Ziwei Ji",
            "Holy Lovenia",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023",
        "summary": "Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap \u2013 which distinguishes inductive and deductive reasoning. Our analysis reveals that the information gap between dialogue contexts and desired inferences renders the inductive inference process more challenging. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.631.pdf"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Bozhong Tian",
            "Siyuan Cheng",
            "Zhoubo Li",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.632.pdf"
    },
    {
        "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
        "authors": [
            "Xiaoshuai Song",
            "Keqing He",
            "Pei Wang",
            "Guanting Dong",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies has been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.636.pdf"
    },
    {
        "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining",
        "authors": [
            "Ting-Rui Chiang",
            "Dani Yogatama"
        ],
        "published": "2023",
        "summary": "We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data\u2019s distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct an analysis over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.637.pdf"
    },
    {
        "title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset",
        "authors": [
            "Arthur Amalvy",
            "Vincent Labatut",
            "Richard Dufour"
        ],
        "published": "2023",
        "summary": "While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an instruction-tuned large language model (LLM). Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.642.pdf"
    },
    {
        "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
        "authors": [
            "Preethi Lahoti",
            "Nicholas Blumm",
            "Xiao Ma",
            "Raghavendra Kotikalapudi",
            "Sahitya Potluri",
            "Qijun Tan",
            "Hansa Srinivasan",
            "Ben Packer",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023",
        "summary": "A crucial challenge for generative large language models (LLMs) is diversity: when a user\u2019s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize the problem diversity of representation in LLM generations. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.643.pdf"
    },
    {
        "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
        "authors": [
            "Xinlin Peng",
            "Ying Zhou",
            "Ben He",
            "Le Sun",
            "Yingfei Sun"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain. Code and data are released for public use.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.644.pdf"
    },
    {
        "title": "Contextual Interaction for Argument Post Quality Assessment",
        "authors": [
            "Yiran Wang",
            "Xuanang Chen",
            "Ben He",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a narrow quality margin. To address this limitation, this paper delves into two alternative methods for modeling the relative quality of different arguments. These approaches include: 1) Supervised contrastive learning that captures the intricate interactions between arguments. By incorporating this approach, we aim to enhance the assessment of argument quality by effectively distinguishing between arguments with subtle differences in quality. 2) Large language models (LLMs) with in-context examples that harness the power of LLMs and enrich them with in-context examples. Through extensive evaluation and analysis on the publicly available IBM-Rank-30k dataset, we demonstrate the superiority of our contrastive argument quality assessment approach over state-of-the-art baselines. On the other hand, while LLMs with in-context examples showcase a commendable ability to identify high-quality argument posts, they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.645.pdf"
    },
    {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "authors": [
            "Zhuoyan Li",
            "Hangxiao Zhu",
            "Zhuoran Lu",
            "Ming Yin"
        ],
        "published": "2023",
        "summary": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.647.pdf"
    },
    {
        "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection",
        "authors": [
            "Indira Sen",
            "Dennis Assenmacher",
            "Mattia Samory",
            "Isabelle Augenstein",
            "Wil Aalst",
            "Claudia Wagner"
        ],
        "published": "2023",
        "summary": "NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.649.pdf"
    },
    {
        "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
        "authors": [
            "Xiaobing Sun",
            "Jiaxi Li",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the ReLU activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our work may offer insights into the roles and capacities of fundamental modules within deep neural networks including large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.650.pdf"
    },
    {
        "title": "Goal-Driven Explainable Clustering via Language Descriptions",
        "authors": [
            "Zihan Wang",
            "Jingbo Shang",
            "Ruiqi Zhong"
        ],
        "published": "2023",
        "summary": "Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users\u2019 goals nor explain clusters\u2019 meanings. We propose a new task formulation, \u201cGoal-Driven Clustering with Explanations\u201d (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and a goal description \u201ccluster the comments based on why the annotators think the summary is imperfect.\u201d; the outputs are text clusters each with an explanation (\u201cthis cluster mentions that the summary misses important context information.\u201d), which relates to the goal and accurately explains which comments should (not) belong to a cluster. To tackle GoalEx, we prompt a language model with \u201c[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\u201d; then we classify whether each sample belongs to a cluster based on its explanation; finally, we use integer linear programming to select a subset of candidate clusters to cover most samples while minimizing overlaps. Under both automatic and human evaluation on corpora with or without labels, our method produces more accurate and goal-related explanations than prior methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.657.pdf"
    },
    {
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": [
            "Danqing Wang",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.659.pdf"
    },
    {
        "title": "Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models",
        "authors": [
            "Joan Nwatu",
            "Oana Ignat",
            "Rada Mihalcea"
        ],
        "published": "2023",
        "summary": "Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from low-income households are often overlooked in data collection and model evaluation. We evaluate the performance of a state-of-the-art vision-language model (CLIP) on a geo-diverse dataset containing household images associated with different income values (DollarStreet) and show that performance inequality exists among households of different income levels. Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries. We highlight insights that can help mitigate these issues and propose actionable steps for economic-level inclusive AI development.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.660.pdf"
    },
    {
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": [
            "Li Yifei",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc"
        ],
        "published": "2023",
        "summary": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use *conceptors*\u2013a soft projection method\u2013to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs\u2019 performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT\u2019s training takes all layers\u2019 bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.661.pdf"
    },
    {
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "authors": [
            "Myra Cheng",
            "Tiziano Piccardi",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations\u2019 susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.669.pdf"
    },
    {
        "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
        "authors": [
            "Saibo Geng",
            "Martin Josifoski",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023",
        "summary": "Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.674.pdf"
    },
    {
        "title": "Systematic word meta-sense extension",
        "authors": [
            "Lei Yu"
        ],
        "published": "2023",
        "summary": "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models\u2019 ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.675.pdf"
    },
    {
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "authors": [
            "Ziang Xiao",
            "Susu Zhang",
            "Vivian Lai",
            "Q. Vera Liao"
        ],
        "published": "2023",
        "summary": "We address a fundamental challenge in Natural Language Generation (NLG) model evaluation\u2014the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.676.pdf"
    },
    {
        "title": "Revisiting the Knowledge Injection Frameworks",
        "authors": [
            "Peng Fu",
            "Yiming Zhang",
            "Haobo Wang",
            "Weikang Qiu",
            "Junbo Zhao"
        ],
        "published": "2023",
        "summary": "In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample. However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique roots in an ideological emphasis on the pruning and purification of the external knowledge base to be injected into LLMs. At last, we show that by integrating this technique into most (if not all) knowledge injection frameworks and recent LLMs, it manages to overcome the aforementioned sanity problem and further pushes the boundary of the performance of the domain-adaptive LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.677.pdf"
    },
    {
        "title": "We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses",
        "authors": [
            "Benjamin Kane",
            "Lenhart Schubert"
        ],
        "published": "2023",
        "summary": "Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge \u2013 knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dialogue generation that retrieves relevant schemas to condition a large language model to generate persona-based responses. Furthermore, we demonstrate a method for bootstrapping the creation of such schemas by first generating generic passages from a set of simple facts, and then inducing schemas from the generated passages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.678.pdf"
    },
    {
        "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model",
        "authors": [
            "Qi Jia",
            "Siyu Ren",
            "Yizhu Liu",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.679.pdf"
    },
    {
        "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
        "authors": [
            "Wanrong Zhu",
            "Xinyi Wang",
            "Yujie Lu",
            "Tsu-Jui Fu",
            "Xin Wang",
            "Miguel Eckstein",
            "William Wang"
        ],
        "published": "2023",
        "summary": "The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than predicting spontaneous changes in the primary subject matters. Adopting the edit suggested by GPT-k models may reduce the percentage of remaining edits by 20-30%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.685.pdf"
    },
    {
        "title": "Explicit Planning Helps Language Models in Logical Reasoning",
        "authors": [
            "Hongyu Zhao",
            "Kangrui Wang",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "published": "2023",
        "summary": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system\u2019s performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.688.pdf"
    },
    {
        "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "authors": [
            "Kranti Chalamalasetti",
            "Jana G\u00f6tze",
            "Sherzod Hakimov",
            "Brielen Madureira",
            "Philipp Sadler",
            "David Schlangen"
        ],
        "published": "2023",
        "summary": "Recent work has proposed a methodology for the systematic evaluation of \u201cSituated Language Understanding Agents\u201d \u2014 agents that operate in rich linguistic and non-linguistic contexts \u2014 through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable of following game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models generally performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.689.pdf"
    },
    {
        "title": "Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models",
        "authors": [
            "Tim Schott",
            "Daniel Furman",
            "Shreshta Bhat"
        ],
        "published": "2023",
        "summary": "In this work, we assess the ability of foundation models to recall encyclopedic knowledge across a wide range of linguistic contexts. To support this, we: 1) produce a 20-language dataset that contains 303k factual associations paired with counterfactuals, 2) evaluate 5 models in a multilingual test, and 3) benchmark a diverse set of 24 models in an English-only test. Meta\u2019s LLaMA achieves the highest scores in both multilingual and English-only evaluations. Yet, an analysis of LLaMA\u2019s errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects. Overall, our findings suggest that today\u2019s foundation models are far from polyglots.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.691.pdf"
    },
    {
        "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
        "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Keshav Santhanam",
            "Radu Florian",
            "Martin Franz",
            "Salim Roukos",
            "Avirup Sil",
            "Md Sultan",
            "Christopher Potts"
        ],
        "published": "2023",
        "summary": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.693.pdf"
    },
    {
        "title": "Data Similarity is Not Enough to Explain Language Model Performance",
        "authors": [
            "Gregory Yauney",
            "Emily Reif",
            "David Mimno"
        ],
        "published": "2023",
        "summary": "Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model\u2019s pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.695.pdf"
    },
    {
        "title": "Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models",
        "authors": [
            "Miaoxi Zhu",
            "Qihuang Zhong",
            "Li Shen",
            "Liang Ding",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate that our method brings consistent and significant performance gains on both discriminative and generative PLMs, i.e., up to +6.98 average score. Furthermore, we empirically validate that our method can effectively improve the model generalization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.696.pdf"
    },
    {
        "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives",
        "authors": [
            "Young Min Cho",
            "Sunny Rai",
            "Lyle Ungar",
            "Jo\u00e3o Sedoc",
            "Sharath Guntuku"
        ],
        "published": "2023",
        "summary": "Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.698.pdf"
    },
    {
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "authors": [
            "Minje Choi",
            "Jiaxin Pei",
            "Sagar Kumar",
            "Chang Shu",
            "David Jurgens"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.699.pdf"
    },
    {
        "title": "Axiomatic Preference Modeling for Longform Question Answering",
        "authors": [
            "Corby Rosset",
            "Guoqing Zheng",
            "Victor Dibia",
            "Ahmed Awadallah",
            "Paul Bennett"
        ],
        "published": "2023",
        "summary": "The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for the scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We intend to release our axiomatic data and model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.702.pdf"
    },
    {
        "title": "Countering Misinformation via Emotional Response Generation",
        "authors": [
            "Daniel Russo",
            "Shane Kaszefski-Yaschuk",
            "Jacopo Staiano",
            "Marco Guerini"
        ],
        "published": "2023",
        "summary": "The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread \u2013 often in good faith \u2013 misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.703.pdf"
    },
    {
        "title": "Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection",
        "authors": [
            "Dennis Fucci",
            "Marco Gaido",
            "Sara Papi",
            "Mauro Cettolo",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2023",
        "summary": "When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers\u2019 preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en\u2192es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers\u2019 vocal traits conflict with their gender.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.705.pdf"
    },
    {
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Tsz Chan",
            "Tianqing Fang",
            "Weiqi Wang",
            "Chunkit Chan",
            "Dongyu Ru",
            "Qipeng Guo",
            "Hongming Zhang",
            "Yangqiu Song",
            "Yue Zhang",
            "Zheng Zhang"
        ],
        "published": "2023",
        "summary": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.706.pdf"
    },
    {
        "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models",
        "authors": [
            "Jing Xiong",
            "Jianhao Shen",
            "Ye Yuan",
            "Haiming Wang",
            "Yichun Yin",
            "Zhengying Liu",
            "Lin Li",
            "Zhijiang Guo",
            "Qingxing Cao",
            "Yinya Huang",
            "Chuanyang Zheng",
            "Xiaodan Liang",
            "Ming Zhang",
            "Qun Liu"
        ],
        "published": "2023",
        "summary": "Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM\u2019s reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the \u201cLean\u201d formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM\u2019s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM\u2019s ability on both formal and mathematical reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.711.pdf"
    },
    {
        "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
        "authors": [
            "Taha Aksu",
            "Devamanyu Hazarika",
            "Shikib Mehri",
            "Seokhwan Kim",
            "Dilek Hakkani-Tur",
            "Yang Liu",
            "Mahdi Namazifar"
        ],
        "published": "2023",
        "summary": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.717.pdf"
    },
    {
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
        "authors": [
            "Haikang Deng",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.721.pdf"
    },
    {
        "title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces",
        "authors": [
            "Usashi Chatterjee",
            "Amit Gajbhiye",
            "Steven Schockaert"
        ],
        "published": "2023",
        "summary": "The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.725.pdf"
    },
    {
        "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
        "authors": [
            "Ant\u00f3nio Farinhas",
            "Jos\u00e9 de Souza",
            "Andre Martins"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.733.pdf"
    },
    {
        "title": "FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning",
        "authors": [
            "Jaemin Shin",
            "Hyungjun Yoon",
            "Seungjoo Lee",
            "Sungjoon Park",
            "Yunxin Liu",
            "Jinho Choi",
            "Sung-Ju Lee"
        ],
        "published": "2023",
        "summary": "Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones\u2019 large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improvement and 8.21% MAE reduction.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.734.pdf"
    },
    {
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "authors": [
            "Geewook Kim",
            "Hodong Lee",
            "Daehee Kim",
            "Haeji Jung",
            "Sanghee Park",
            "Yoonsik Kim",
            "Sangdoo Yun",
            "Taeho Kil",
            "Bado Lee",
            "Seunghyun Park"
        ],
        "published": "2023",
        "summary": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.735.pdf"
    },
    {
        "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
        "authors": [
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.738.pdf"
    },
    {
        "title": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration",
        "authors": [
            "Yiquan Wu",
            "Siying Zhou",
            "Yifei Liu",
            "Weiming Lu",
            "Xiaozhong Liu",
            "Yating Zhang",
            "Changlong Sun",
            "Fei Wu",
            "Kun Kuang"
        ],
        "published": "2023",
        "summary": "Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP) \u2013 a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.740.pdf"
    },
    {
        "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
        "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs\u2014InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI\u2014and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via \u2018pip install factscore\u2018.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.741.pdf"
    },
    {
        "title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems",
        "authors": [
            "Marek Kadl\u010d\u00edk",
            "Michal \u0160tef\u00e1nik",
            "Ondrej Sotolar",
            "Vlastimil Martinek"
        ],
        "published": "2023",
        "summary": "Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.742.pdf"
    },
    {
        "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
        "authors": [
            "Hoang Nguyen",
            "Ye Liu",
            "Chenwei Zhang",
            "Tao Zhang",
            "Philip Yu"
        ],
        "published": "2023",
        "summary": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.743.pdf"
    },
    {
        "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
        "authors": [
            "Sullam Jeoung",
            "Yubin Ge",
            "Jana Diesner"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs\u2019 perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs\u2019 judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.752.pdf"
    },
    {
        "title": "Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations",
        "authors": [
            "Minh-Quang Pham",
            "Sathish Indurthi",
            "Shamil Chollampatt",
            "Marco Turchi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using high-quality, in-domain datasets. However, it can be prohibitively expensive to get such high-quality training data. This issue has been mitigated by generating weakly supervised data via knowledge distillation (KD) of LLMs. We propose a three-step approach to distill ChatGPT and fine-tune smaller LMs for summarizing forum conversations. More specifically, we design a method to selectively sample a large unannotated corpus of forum conversation using a semantic similarity metric. Then, we use the same metric to retrieve suitable prompts for ChatGPT from a small annotated validation set in the same domain. The generated dataset is then filtered to remove low-quality instances. Our proposed select-prompt-filter KD approach leads to significant improvements of up to 6.6 ROUGE-2 score by leveraging sufficient in-domain pseudo-labeled data over a standard KD approach given the same size of training data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.753.pdf"
    },
    {
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "authors": [
            "Daixuan Cheng",
            "Shaohan Huang",
            "Junyu Bi",
            "Yuefeng Zhan",
            "Jianfeng Liu",
            "Yujing Wang",
            "Hao Sun",
            "Furu Wei",
            "Weiwei Deng",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.758.pdf"
    },
    {
        "title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
        "authors": [
            "Xiao Yu",
            "Qingyang Wu",
            "Kun Qian",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an approach to create a more efficient RL-based algorithm to improve TOD performance in an offline setting. First, we use a faster generation procedure that samples from independent next-word distributions after training the language model (LM) with supervised learning. We then introduce a fine-grained reward function to help the model focus on learning key information in a dialog, by measuring the importance and semantic closeness of each generated token. Experiments on the MultiWoZ dataset show our new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), achieves state-of-the-art performance on the end-to-end response generation task, with a 15% training time reduction compared to a standard RL algorithm using auto-regressive generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.759.pdf"
    },
    {
        "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
        "authors": [
            "Fajri Koto",
            "Nurul Aisyah",
            "Haonan Li",
            "Timothy Baldwin"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.760.pdf"
    },
    {
        "title": "Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "authors": [
            "Pranjal Aggarwal",
            "Aman Madaan",
            "Yiming Yang",
            "Mausam"
        ],
        "published": "2023",
        "summary": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.761.pdf"
    },
    {
        "title": "Bridging Information-Theoretic and Geometric Compression in Language Models",
        "authors": [
            "Emily Cheng",
            "Corentin Kervadec",
            "Marco Baroni"
        ],
        "published": "2023",
        "summary": "For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.762.pdf"
    },
    {
        "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
        "authors": [
            "Zifeng Wang",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "published": "2023",
        "summary": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial\u2019s success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.766.pdf"
    },
    {
        "title": "Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation",
        "authors": [
            "Xuanfan Ni",
            "Hongliang Dai",
            "Zhaochun Ren",
            "Piji Li"
        ],
        "published": "2023",
        "summary": "Open-domain multi-turn dialogue generation encounters the significant challenge of lacking various types of knowledge from diverse sources. Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements. Recently, large language models (LLMs) have shown impressive performance on natural language processing tasks. To harness the knowledge storage of LLMs, we propose a framework named KnowEE that explores multi-source multi-type knowledge from LLMs by leveraging diverse datasets and then exploits the obtained knowledge for response generation. Our framework comprises two phases: First, we leverage five external datasets encompassing various types of knowledge to extract the most relevant samples to the dialogue context which are served as prompts to generate corresponding type of knowledge; Second, we inject the acquired knowledge into the ongoing dialogue context in fine-grained and coarse-grained manners, which is then fed into LLMs to generate the final dialogue response. Both automatic and manual evaluation results validate the effectiveness of our framework in exploring and exploiting multi-source multi-type knowledge to generate coherent, informative, and fluent responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.771.pdf"
    },
    {
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "authors": [
            "Ruochen Zhang",
            "Samuel Cahyawijaya",
            "Jan Christian Blaise Cruz",
            "Genta Winata",
            "Alham Aji"
        ],
        "published": "2023",
        "summary": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current \u201cmultilingualism\u2019 in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.774.pdf"
    },
    {
        "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
        "authors": [
            "Bram van Dijk",
            "Tom Kouwenhoven",
            "Marco Spruit",
            "Max Johannes van Duijn"
        ],
        "published": "2023",
        "summary": "Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of \u2018real\u2019 understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.779.pdf"
    },
    {
        "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
        "authors": [
            "Seungone Kim",
            "Se Joo",
            "Doyoung Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Jamin Shin",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.782.pdf"
    },
    {
        "title": "Explaining Interactions Between Text Spans",
        "authors": [
            "Sagnik Ray Choudhury",
            "Pepa Atanasova",
            "Isabelle Augenstein"
        ],
        "published": "2023",
        "summary": "Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with respect to the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations. We make the code and the dataset available on [Github](https://github.com/copenlu/spanex). The dataset is also available on [Huggingface datasets](https://huggingface.co/datasets/copenlu/spanex).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.783.pdf"
    },
    {
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "authors": [
            "Xinyu Zhu",
            "Cheng Yang",
            "Bei Chen",
            "Siheng Li",
            "Jian-Guang Lou",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs\u2019 inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs\u2019 superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.787.pdf"
    },
    {
        "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens",
        "authors": [
            "Siyu Ren",
            "Qi Jia",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.794.pdf"
    },
    {
        "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
        "authors": [
            "Hyunwoo Kim",
            "Jack Hessel",
            "Liwei Jiang",
            "Peter West",
            "Ximing Lu",
            "Youngjae Yu",
            "Pei Zhou",
            "Ronan Bras",
            "Malihe Alikhani",
            "Gunhee Kim",
            "Maarten Sap",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.799.pdf"
    },
    {
        "title": "Generative Adversarial Training with Perturbed Token Detection for Model Robustness",
        "authors": [
            "Jiahao Zhao",
            "Wenji Mao"
        ],
        "published": "2023",
        "summary": "Adversarial training is the dominant strategy towards model robustness. Current adversarial training methods typically apply perturbations to embedding representations, whereas actual text-based attacks introduce perturbations as discrete tokens. Thus there exists a gap between the continuous embedding representations and discrete text tokens that hampers the effectiveness of adversarial training. Moreover, the continuous representations of perturbations cannot be further utilized, resulting in the suboptimal performance. To bridge this gap for adversarial robustness, in this paper, we devise a novel generative adversarial training framework that integrates gradient-based learning, adversarial example generation and perturbed token detection. Our proposed framework consists of generative adversarial attack and adversarial training process. Specifically, in generative adversarial attack, the embeddings are shared between the classifier and the generative model, which enables the generative model to leverage the gradients from the classifier for generating perturbed tokens. Then, adversarial training process combines adversarial regularization with perturbed token detection to provide token-level supervision and improve the efficiency of sample utilization. Extensive experiments on five datasets from the AdvGLUE benchmark demonstrate that our framework significantly enhances the model robustness, surpassing the state-of-the-art results of ChatGPT by 10% in average accuracy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.804.pdf"
    },
    {
        "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
        "authors": [
            "Anastasia Kritharoula",
            "Maria Lymperaiou",
            "Giorgos Stamou"
        ],
        "published": "2023",
        "summary": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.807.pdf"
    },
    {
        "title": "Doolittle: Benchmarks and Corpora for Academic Writing Formalization",
        "authors": [
            "Shizhe Diao",
            "Yongyu Lei",
            "Liangming Pan",
            "Tianqing Fang",
            "Wangchunshu Zhou",
            "Sedrick Keh",
            "Min-Yen Kan",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.809.pdf"
    },
    {
        "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
        "authors": [
            "Yutian Chen",
            "Hao Kang",
            "Vivian Zhai",
            "Liangze Li",
            "Rita Singh",
            "Bhiksha Raj"
        ],
        "published": "2023",
        "summary": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.810.pdf"
    },
    {
        "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
        "authors": [
            "Davis Liang",
            "Hila Gonen",
            "Yuning Mao",
            "Rui Hou",
            "Naman Goyal",
            "Marjan Ghazvininejad",
            "Luke Zettlemoyer",
            "Madian Khabsa"
        ],
        "published": "2023",
        "summary": "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.813.pdf"
    },
    {
        "title": "Character-LLM: A Trainable Agent for Role-Playing",
        "authors": [
            "Yunfan Shao",
            "Linyang Li",
            "Junqi Dai",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.814.pdf"
    },
    {
        "title": "Natural Language Decompositions of Implicit Content Enable Better Text Representations",
        "authors": [
            "Alexander Hoyle",
            "Rupak Sarkar",
            "Pranav Goel",
            "Philip Resnik"
        ],
        "published": "2023",
        "summary": "When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of propositions that are inferentially related to the text that has been observed, then validate the plausibility of the generated content via human judgments. Incorporating these explicit representations of implicit content proves useful in multiple problem settings that involve the human interpretation of utterances: assessing the similarity of arguments, making sense of a body of opinion data, and modeling legislative behavior. Our results suggest that modeling the meanings behind observed language, rather than the literal text alone, is a valuable direction for NLP and particularly its applications to social science.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.815.pdf"
    },
    {
        "title": "NameGuess: Column Name Expansion for Tabular Data",
        "authors": [
            "Jiani Zhang",
            "Zhengyuan Shen",
            "Balasubramaniam Srinivasan",
            "Shen Wang",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "published": "2023",
        "summary": "Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names \u2013 yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (on multiple LLMs) to validate the effectiveness of table content in NameGuess and identify promising future opportunities. Code has been made available at https://github.com/amazon-science/nameguess.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.820.pdf"
    },
    {
        "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
        "authors": [
            "Tannon Kew",
            "Alison Chi",
            "Laura V\u00e1squez-Rodr\u00edguez",
            "Sweta Agrawal",
            "Dennis Aumiller",
            "Fernando Alva-Manchego",
            "Matthew Shardlow"
        ],
        "published": "2023",
        "summary": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.821.pdf"
    },
    {
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
        "authors": [
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Chin-Yew Lin",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.825.pdf"
    },
    {
        "title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games",
        "authors": [
            "Ruoyao Wang",
            "Graham Todd",
            "Xingdi Yuan",
            "Ziang Xiao",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Peter Jansen"
        ],
        "published": "2023",
        "summary": "In this work we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32, a corpus of 32 reasoning-focused text games totalling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 58%. While evaluating simulation fidelity is labor intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high-degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.830.pdf"
    },
    {
        "title": "Skill-Based Few-Shot Selection for In-Context Learning",
        "authors": [
            "Shengnan An",
            "Bo Zhou",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Weizhu Chen",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "*In-context learning* is the paradigm that adapts large language models to downstream tasks by providing a few examples. *Few-shot selection*\u2014selecting appropriate examples for each test instance separately\u2014is important for in-context learning. In this paper, we propose **Skill-KNN**, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.831.pdf"
    },
    {
        "title": "PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer",
        "authors": [
            "Lichang Chen",
            "Jiuhai Chen",
            "Heng Huang",
            "Minhao Cheng"
        ],
        "published": "2023",
        "summary": "Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer (PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-based regularizers, including random-noise-based and adversarial-based. In particular, our proposed perturbations are flexible on both text space and embedding space. Extensive experiments show the effectiveness of our proposed methods in stabilizing the training. Our new algorithms improve the state-of-the-art prompt tuning methods by 1.94% and 2.34% on SuperGLUE and FewGLUE benchmarks, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.833.pdf"
    },
    {
        "title": "Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents",
        "authors": [
            "Jannis Vamvas",
            "Rico Sennrich"
        ],
        "published": "2023",
        "summary": "Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.835.pdf"
    },
    {
        "title": "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA",
        "authors": [
            "Jonathan Tonglet",
            "Manon Reusens",
            "Philipp Borchert",
            "Bart Baesens"
        ],
        "published": "2023",
        "summary": "Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.837.pdf"
    },
    {
        "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations",
        "authors": [
            "Jihyoung Jang",
            "Minseong Boo",
            "Hyounghun Kim"
        ],
        "published": "2023",
        "summary": "In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and the relationships between speakers would be particularly important. Despite their importance, current research efforts have not sufficiently addressed these dialogical components. In this paper, we introduce a new 1M multi-session dialogue dataset, called Conversation Chronicles, for implementing a long-term conversation setup in which time intervals and fine-grained speaker relationships are incorporated. Following recent works, we exploit a large language model to produce the data. The extensive human evaluation shows that dialogue episodes in Conversation Chronicles reflect those properties while maintaining coherent and consistent interactions across all the sessions. We also propose a dialogue model, called ReBot, which consists of chronological summarization and dialogue generation modules using only around 630M parameters. When trained on Conversation Chronicles, ReBot demonstrates long-term context understanding with a high human engagement score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.838.pdf"
    },
    {
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "authors": [
            "David Chan",
            "Suzanne Petryk",
            "Joseph Gonzalez",
            "Trevor Darrell",
            "John Canny"
        ],
        "published": "2023",
        "summary": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.841.pdf"
    },
    {
        "title": "MoPe: Model Perturbation based Privacy Attacks on Language Models",
        "authors": [
            "Marvin Li",
            "Jason Wang",
            "Jeffrey Wang",
            "Seth Neel"
        ],
        "published": "2023",
        "summary": "Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point x, a statistic we show approximates the trace of the Hessian matrix with respect to model parameters. Across language models ranging from 70M to 12B parameters, we show that MoPe is more effective than existing loss-based attacks and recently proposed perturbation-based methods. We also examine the role of training point order and model size in attack success, and empirically demonstrate that MoPe accurately approximate the trace of the Hessian in practice. Our results show that the loss of a point alone is insufficient to determine extractability\u2014there are training points we can recover using our method that have average loss. This casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.842.pdf"
    },
    {
        "title": "q2d: Turning Questions into Dialogs to Teach Models How to Search",
        "authors": [
            "Yonatan Bitton",
            "Shlomi Cohen-Ganor",
            "Ido Hakimi",
            "Yoad Lewenberg",
            "Roee Aharoni",
            "Enav Weinreb"
        ],
        "published": "2023",
        "summary": "One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%-97% of the performance of models trained on the human-generated data; (2) We can successfully generate data for training dialog models in new domains without any existing dialog data as demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We perform a thorough analysis of the generated dialogs showing that humans find them of high quality and struggle to distinguish them from human-written dialogs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.843.pdf"
    },
    {
        "title": "Aligning Large Language Models through Synthetic Feedback",
        "authors": [
            "Sungdong Kim",
            "Sanghwan Bae",
            "Jamin Shin",
            "Soyoung Kang",
            "Donghyun Kwak",
            "Kang Yoo",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.844.pdf"
    },
    {
        "title": "You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models",
        "authors": [
            "Alexander Baranov",
            "Vladimir Kniazhevsky",
            "Pavel Braslavski"
        ],
        "published": "2023",
        "summary": "In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing datasets, train RoBERTa-based and Na\u00efve Bayes classifiers on each of them, and test on the rest. Training and testing on the same dataset yields good results, but the transferability of the models varies widely. Models trained on datasets with jokes from different sources show better transferability, while the amount of training data has a smaller impact. The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics. An adversarial attack shows that models trained on pun datasets are less robust. We also evaluate the sense of humor of the chatGPT and Flan-UL2 models in a zero-shot scenario. The LLMs demonstrate competitive results on humor datasets and a more stable behavior on out-of-domain data. We believe that the obtained results will facilitate the development of new datasets and evaluation methodologies in the field of computational humor. We\u2019ve made all the data from the study and the trained models publicly available at https://github.com/Humor-Research/Humor-detection.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.845.pdf"
    },
    {
        "title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning",
        "authors": [
            "Hongqiu Wu",
            "Linfeng Liu",
            "Hai Zhao",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method Curriculum Logical Reasoning (Clr), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.847.pdf"
    },
    {
        "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
        "authors": [
            "Yanchen Liu",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.850.pdf"
    },
    {
        "title": "Can We Edit Multimodal Large Language Models?",
        "authors": [
            "Siyuan Cheng",
            "Bozhong Tian",
            "Qingbin Liu",
            "Xi Chen",
            "Yongheng Wang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.856.pdf"
    },
    {
        "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
        "authors": [
            "Yuwei Zhang",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon \u201csmall\u201d embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user\u2019s preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions <does A better correspond to B than C>, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions <do A and B belong to the same category>, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.858.pdf"
    },
    {
        "title": "Syllogistic Reasoning for Legal Judgment Analysis",
        "authors": [
            "Wentao Deng",
            "Jiahuan Pei",
            "Keyi Kong",
            "Zhe Chen",
            "Furu Wei",
            "Yujun Li",
            "Zhaochun Ren",
            "Zhumin Chen",
            "Pengjie Ren"
        ],
        "published": "2023",
        "summary": "Legal judgment assistants are developing fast due to impressive progress of large language models (LLMs). However, people can hardly trust the results generated by a model without reliable analysis of legal judgement. For legal practitioners, it is common practice to utilize syllogistic reasoning to select and evaluate the arguments of the parties as part of the legal decision-making process. But the development of syllogistic reasoning for legal judgment analysis is hindered by the lack of resources: (1) there is no large-scale syllogistic reasoning dataset for legal judgment analysis, and (2) there is no set of established benchmarks for legal judgment analysis. In this paper, we construct and manually correct a syllogistic reasoning dataset for legal judgment analysis. The dataset contains 11,239 criminal cases which cover 4 criminal elements, 80 charges and 124 articles. We also select a set of large language models as benchmarks, and conduct a in-depth analysis of the capacity of their legal judgment analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.864.pdf"
    },
    {
        "title": "SUT: Active Defects Probing for Transcompiler Models",
        "authors": [
            "Mengnan Qi",
            "Yufan Huang",
            "Maoquan Wang",
            "Yongqiang Yao",
            "Zihan Liu",
            "Bin Gu",
            "Colin Clement",
            "Neel Sundaresan"
        ],
        "published": "2023",
        "summary": "Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these metrics address these basic syntax errors. We develop a novel active defects probing suite called Syntactic Unit Tests (SUT) which includes a highly interpretable evaluation harness for accuracy and test scoring. Experiments have shown that even powerful models like ChatGPT still make mistakes on these basic unit tests. Specifically, compared to previous program translation task evaluation dataset, its pass rate on our unit tests has decreased by 26.15%. Further our evaluation harness reveal syntactic element errors in which these models exhibit deficiencies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.866.pdf"
    },
    {
        "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
        "authors": [
            "Sehyun Choi",
            "Tianqing Fang",
            "Zhaowei Wang",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of KCTS as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.867.pdf"
    },
    {
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "authors": [
            "Mayank Kothyari",
            "Dhruva Dhingra",
            "Sunita Sarawagi",
            "Soumen Chakrabarti"
        ],
        "published": "2023",
        "summary": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination \u2014 generally considered a nuisance \u2014 turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.868.pdf"
    },
    {
        "title": "DetGPT: Detect What You Need via Reasoning",
        "authors": [
            "Renjie Pi",
            "Jiahui Gao",
            "Shizhe Diao",
            "Rui Pan",
            "Hanze Dong",
            "Jipeng Zhang",
            "Lewei Yao",
            "Jianhua Han",
            "Hang Xu",
            "Lingpeng Kong",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user\u2019s instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user\u2019s expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interactive and versatile object detection systems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.876.pdf"
    },
    {
        "title": "Language Models with Rationality",
        "authors": [
            "Nora Kassner",
            "Oyvind Tafjord",
            "Ashish Sabharwal",
            "Kyle Richardson",
            "Hinrich Schuetze",
            "Peter Clark"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \u201cbeliefs\u201d. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a **rational, self-reflecting layer** on top of the LLM. First, given a question, we construct a **belief graph** using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.877.pdf"
    },
    {
        "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
        "authors": [
            "Michael Zhang",
            "Eunsol Choi"
        ],
        "published": "2023",
        "summary": "While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.879.pdf"
    },
    {
        "title": "Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance",
        "authors": [
            "Karthic Madanagopal",
            "James Caverlee"
        ],
        "published": "2023",
        "summary": "Objectivity is a goal for Wikipedia and many news sites, as well as a guiding principle of many large language models. Indeed, several methods have recently been developed for automatic subjective bias neutralization. These methods, however, typically rely on parallel text for training (i.e. a biased sentence coupled with a non-biased sentence), demonstrate poor transfer to new domains, and can lose important bias-independent context. Toward expanding the reach of bias neutralization, we propose in this paper a new approach called FairBalance. Three of its unique features are: i) a cycle consistent adversarial network enables bias neutralization without the need for parallel text; ii) the model design preserves bias-independent content; and iii) through auxiliary guidance, the model highlights sequences of bias-inducing words, yielding strong results in terms of bias neutralization quality. Extensive experiments demonstrate how FairBalance significantly improves subjective bias neutralization compared to other methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.882.pdf"
    },
    {
        "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
        "authors": [
            "Jason Lucas",
            "Adaku Uchendu",
            "Michiharu Yamashita",
            "Jooyoung Lee",
            "Shaurya Rohatgi",
            "Dongwon Lee"
        ],
        "published": "2023",
        "summary": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*). To combat this emerging risk of LLMs, we propose a novel \u201c***Fighting Fire with Fire***\u201d (F3) strategy that harnesses modern LLMs\u2019 generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo\u2019s zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.883.pdf"
    },
    {
        "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
        "authors": [
            "Yifan Jiang",
            "Filip Ilievski",
            "Kaixin Ma",
            "Zhivar Sourati"
        ],
        "published": "2023",
        "summary": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model\u2019s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.885.pdf"
    },
    {
        "title": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation",
        "authors": [
            "Zhenwen Liang",
            "Wenhao Yu",
            "Tanmay Rajpurohit",
            "Peter Clark",
            "Xiangliang Zhang",
            "Ashwin Kalyan"
        ],
        "published": "2023",
        "summary": "In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model\u2019s weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model\u2019s current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.889.pdf"
    },
    {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023",
        "summary": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.890.pdf"
    },
    {
        "title": "Exploring the Boundaries of GPT-4 in Radiology",
        "authors": [
            "Qianchu Liu",
            "Stephanie Hyland",
            "Shruthi Bannur",
            "Kenza Bouzid",
            "Daniel Castro",
            "Maria Wetscherek",
            "Robert Tinn",
            "Harshita Sharma",
            "Fernando P\u00e9rez-Garc\u00eda",
            "Anton Schwaighofer",
            "Pranav Rajpurkar",
            "Sameer Khanna",
            "Hoifung Poon",
            "Naoto Usuyama",
            "Anja Thieme",
            "Aditya Nori",
            "Matthew Lungren",
            "Ozan Oktay",
            "Javier Alvarez-Valle"
        ],
        "published": "2023",
        "summary": "The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the text-based applications for radiology reports, comparing against state-of-the-art (SOTA) radiology-specific models. Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and we found GPT-4 either outperforms or is on par with current SOTA radiology models. With zero-shot prompting, GPT-4 already obtains substantial gains (\u2248 10% absolute improvement) over radiology models in temporal sentence similarity classification (accuracy) and natural language inference (F1). For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge. For findings summarisation, GPT-4 outputs are found to be overall comparable with existing manually-written impressions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.891.pdf"
    },
    {
        "title": "A Frustratingly Easy Post-Training Quantization Scheme for LLMs",
        "authors": [
            "Yongkweon Jeon",
            "Chungman Lee",
            "Kyungphil Park",
            "Ho-young Kim"
        ],
        "published": "2023",
        "summary": "Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance. This necessity holds true regardless of the computing environment, whether it be mobile devices or cloud servers. Quantization emerges as a solution to alleviate the computational burden during inference. By representing models with a reduced bit-width, quantization minimizes the frequency of DRAM access while fully exploiting the parallelism of operations through a dense matrix format. Consequently, quantized models achieve low end-to-end latency and optimize resource utilization by addressing both memory and computing bottlenecks. In this paper, we propose a straightforward post-training quantization scheme, called Z-Fold, that fully utilizes the feature of the Transformer structure widely employed in large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.892.pdf"
    },
    {
        "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
        "authors": [
            "Ruixuan Xiao",
            "Yiwen Dong",
            "Junbo Zhao",
            "Runze Wu",
            "Minmin Lin",
            "Gang Chen",
            "Haobo Wang"
        ],
        "published": "2023",
        "summary": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.896.pdf"
    },
    {
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "authors": [
            "Yihan Cao",
            "Shuyi Chen",
            "Ryan Liu",
            "Zhiruo Wang",
            "Daniel Fried"
        ],
        "published": "2023",
        "summary": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures \u2014 relational, multi-table, and hierarchical matrix shapes \u2014 and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.897.pdf"
    },
    {
        "title": "Outlier Dimensions Encode Task Specific Knowledge",
        "authors": [
            "William Rudman",
            "Catherine Chen",
            "Carsten Eickhoff"
        ],
        "published": "2023",
        "summary": "Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.901.pdf"
    },
    {
        "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization",
        "authors": [
            "Janghwan Lee",
            "Minsoo Kim",
            "Seungcheol Baek",
            "Seok Hwang",
            "Wonyong Sung",
            "Jungwook Choi"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency\u2014a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2\u00d7 hardware efficiency improvement compared to 8-bit integer MAC unit.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.910.pdf"
    },
    {
        "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
        "authors": [
            "Mengyu Ye",
            "Tatsuki Kuribayashi",
            "Jun Suzuki",
            "Goro Kobayashi",
            "Hiroaki Funayama"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional entities) to evaluate the logical reasoning abilities of the models. We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible\u2192implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.912.pdf"
    },
    {
        "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding",
        "authors": [
            "Caoyun Fan",
            "Jidong Tian",
            "Yitian Li",
            "Wenqing Chen",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT\u2019s two-step framework enables MLMs to implement task decomposition; CoTT\u2019s prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.913.pdf"
    },
    {
        "title": "Large Language Models are Complex Table Parsers",
        "authors": [
            "Bowen Zhao",
            "Changkai Ji",
            "Yuejie Zhang",
            "Wen He",
            "Yingwen Wang",
            "Qing Wang",
            "Rui Feng",
            "Xiaobo Zhang"
        ],
        "published": "2023",
        "summary": "With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell\u2019s hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.914.pdf"
    },
    {
        "title": "R2H: Building Multimodal Navigation Helpers that Respond to Help Requests",
        "authors": [
            "Yue Fan",
            "Jing Gu",
            "Kaizhi Zheng",
            "Xin Wang"
        ],
        "published": "2023",
        "summary": "Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent\u2019s ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.915.pdf"
    },
    {
        "title": "Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries",
        "authors": [
            "Ashish Mittal",
            "Sunita Sarawagi",
            "Preethi Jyothi",
            "George Saon",
            "Gakuto Kurata"
        ],
        "published": "2023",
        "summary": "Despite the impressive performance of ASR models on mainstream benchmarks, their performance on rare words is unsatisfactory. In enterprise settings, often a focused list of entities (such as locations, names, etc) are available which can be used to adapt the model to the terminology of specific domains. In this paper, we present a novel inference algorithm that improves the prediction of state-of-the-art ASR models using nearest-neighbor-based matching on an inference-time word list. We consider both the Transducer architecture that is useful in the streaming setting, and state-of-the-art encoder-decoder models such as Whisper. In our approach, a list of rare entities is indexed in a memory by synthesizing speech for each entry, and then storing the internal acoustic and language model states obtained from the best possible alignment on the ASR model. The memory is organized as a trie which we harness to perform a stateful lookup during inference. A key property of our extension is that we prevent spurious matches by restricting to only word-level matches. In our experiments on publicly available datasets and private benchmarks, we show that our method is effective in significantly improving rare word recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.916.pdf"
    },
    {
        "title": "Learning to Describe for Predicting Zero-shot Drug-Drug Interactions",
        "authors": [
            "Fangqi Zhu",
            "Yongqi Zhang",
            "Lei Chen",
            "Bing Qin",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Adverse drug-drug interactions (DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning (RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot DDI prediction, and the selected texts are semantically relevant. Our code and data are available at https://github.com/zhufq00/DDIs-Prediction.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.918.pdf"
    },
    {
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "authors": [
            "Alexandros Xenos",
            "Themos Stafylakis",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ],
        "published": "2023",
        "summary": "This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler and readily reproducible pipeline which, in a nutshell, is based on efficient in-context learning by prompting LLaMA (1 and 2) using question-informative captions as contextual information. Contrary to recent approaches, our method is training-free, does not require access to external databases or APIs, and yet achieves state-of-the-art accuracy on the OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to understand important aspects of our method. Our code is publicly available at https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.919.pdf"
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Pengjie Ren",
            "Zhumin Chen",
            "Dawei Yin",
            "Zhaochun Ren"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model\u2019s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.923.pdf"
    },
    {
        "title": "DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization",
        "authors": [
            "Chengang Hu",
            "Xiao Liu",
            "Yansong Feng"
        ],
        "published": "2023",
        "summary": "Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation. While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack of diversity in the forms of combinations. To better investigate compositional generalization with more linguistic phenomena and compositional diversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large realistic Chinese dataset. Given a recipe instruction, models are required to recognize the dish name composed of diverse combinations of food, actions, and flavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves plenty of linguistic phenomena such as anaphora, omission and ambiguity. We provide two strong baselines based on T5 and large language models (LLMs). This work contributes a challenging task, baseline methods to tackle the task, and insights into compositional generalization in the context of dish name recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.924.pdf"
    },
    {
        "title": "It Ain\u2019t Over: A Multi-aspect Diverse Math Word Problem Dataset",
        "authors": [
            "Jiwoo Kim",
            "Youngbin Kim",
            "Ilwoong Baek",
            "JinYeong Bak",
            "Jongwuk Lee"
        ],
        "published": "2023",
        "summary": "The math word problem (MWP) is a complex task that requires natural language understanding and logical reasoning to extract key knowledge from natural language narratives. Previous studies have provided various MWP datasets but lack diversity in problem types, lexical usage patterns, languages, and annotations for intermediate solutions. To address these limitations, we introduce a new MWP dataset, named DMath (Diverse Math Word Problems), offering a wide range of diversity in problem types, lexical usage patterns, languages, and intermediate solutions. The problems are available in English and Korean and include an expression tree and Python code as intermediate solutions. Through extensive experiments, we demonstrate that the DMath dataset provides a new opportunity to evaluate the capability of large language models, i.e., GPT-4 only achieves about 75% accuracy on the DMath dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.927.pdf"
    },
    {
        "title": "Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness",
        "authors": [
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2023",
        "summary": "This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.928.pdf"
    },
    {
        "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
        "authors": [
            "Zeyu Liu",
            "Tim Dettmers",
            "Xi Lin",
            "Veselin Stoyanov",
            "Xian Li"
        ],
        "published": "2023",
        "summary": "Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method \u2014 Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.930.pdf"
    },
    {
        "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Cheng Chang",
            "Qipeng Guo",
            "Junqi Dai",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.936.pdf"
    },
    {
        "title": "Connecting degree and polarity: An artificial language learning study",
        "authors": [
            "Lisa Bylinina",
            "Alexey Tikhonov",
            "Ekaterina Garmash"
        ],
        "published": "2023",
        "summary": "We investigate a new linguistic generalisation in pre-trained language models (taking BERT Devlin et al. 2019 as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier\u2019s sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial Language Learning experimental paradigm from psycholinguistics to a neural language model. Our experimental results suggest that BERT generalizes in line with existing linguistic observations that relate de- gree semantics to polarity sensitivity, including the main one: low degree semantics is associated with preference towards positive polarity.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.938.pdf"
    },
    {
        "title": "Prompting with Pseudo-Code Instructions",
        "authors": [
            "Mayank Mishra",
            "Prince Kumar",
            "Riyaz Bhat",
            "Rudra Murthy",
            "Danish Contractor",
            "Srikanth Tamilselvam"
        ],
        "published": "2023",
        "summary": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.939.pdf"
    },
    {
        "title": "CRAB: Assessing the Strength of Causal Relationships Between Real-world Events",
        "authors": [
            "Angelika Romanou",
            "Syrielle Montariol",
            "Debjit Paul",
            "Leo Laugier",
            "Karl Aberer",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.940.pdf"
    },
    {
        "title": "NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly",
        "authors": [
            "Yi Fung",
            "Tuhin Chakrabarty",
            "Hao Guo",
            "Owen Rambow",
            "Smaranda Muresan",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Knowledge of norms is needed to understand and reason about acceptable behavior in human communication and interactions across sociocultural scenarios. Most computational research on norms has focused on a single culture, and manually built datasets, from non-conversational settings. We address these limitations by proposing a new framework, NormSage, to automatically extract culture-specific norms from multi-lingual conversations. NormSage uses GPT-3 prompting to 1) extract candidate norms directly from conversations and 2) provide explainable self-verification to ensure correctness and relevance. Comprehensive empirical results show the promise of our approach to extract high-quality culture-aware norms from multi-lingual conversations (English and Chinese), across several quality metrics. Further, our relevance verification can be extended to assess the adherence and violation of any norm with respect to a conversation on-the-fly, along with textual explanation. NormSage achieves an AUC of 94.6% in this grounding setup, with generated explanations matching human-written quality.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.941.pdf"
    },
    {
        "title": "Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond",
        "authors": [
            "Siyang Liu",
            "Naihao Deng",
            "Sahand Sabour",
            "Yilin Jia",
            "Minlie Huang",
            "Rada Mihalcea"
        ],
        "published": "2023",
        "summary": "We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model\u2019s tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.944.pdf"
    },
    {
        "title": "FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering",
        "authors": [
            "Megha Chakraborty",
            "Khushbu Pahwa",
            "Anku Rani",
            "Shreyas Chatterjee",
            "Dwip Dalal",
            "Harshit Dave",
            "Ritvik G",
            "Preethi Gurumurthy",
            "Adarsh Mahor",
            "Samahriti Mukherjee",
            "Aditya Pakala",
            "Ishan Paul",
            "Janvita Reddy",
            "Arghya Sarkar",
            "Kinjal Sensharma",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "Combating disinformation is one of the burning societal crises - about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multimodal fake news dataset, in addition to offering explainability through the concept of 5W question-answering. Salient features of the dataset include: (i) textual claims, (ii) ChatGPT-generated paraphrased claims, (iii) associated images, (iv) stable diffusion-generated additional images (i.e., visual paraphrases), (v) pixel-level image heatmap to foster image-text explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news stories.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.945.pdf"
    },
    {
        "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks",
        "authors": [
            "Chufan Shi",
            "Yixuan Su",
            "Cheng Yang",
            "Yujiu Yang",
            "Deng Cai"
        ],
        "published": "2023",
        "summary": "The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broadcoverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demonstrates that generalist instruction tuning improves understanding and reasoning abilities. However, for tasks requiring factual knowledge, generalist data containing hallucinatory information may negatively affect the model\u2019s performance. Overall, our work provides a systematic guide for developing specialist models with general instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.947.pdf"
    },
    {
        "title": "Making Large Language Models Better Data Creators",
        "authors": [
            "Dong-Ho Lee",
            "Jay Pujara",
            "Mohit Sewak",
            "Ryen White",
            "Sujay Jauhar"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.948.pdf"
    },
    {
        "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
        "authors": [
            "Xiaohua Wang",
            "Yuliang Yan",
            "Longtao Huang",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed \u201challucinations\u201d, remains a significant challenge. Current hallucination detection methods often necessitate the retrieval of great numbers of relevant evidence, thereby increasing response times. We introduce a unique framework that leverages statistical decision theory and Bayesian sequential analysis to optimize the trade-off between costs and benefits during the hallucination detection process. This approach does not require a predetermined number of observations. Instead, the analysis proceeds in a sequential manner, enabling an expeditious decision towards \u201cbelief\u201d or \u201cdisbelief\u201d through a stop-or-continue strategy. Extensive experiments reveal that this novel framework surpasses existing methods in both efficiency and precision of hallucination detection. Furthermore, it requires fewer retrieval steps on average, thus decreasing response times.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.949.pdf"
    },
    {
        "title": "Guideline Learning for In-Context Information Extraction",
        "authors": [
            "Chaoxu Pang",
            "Yixuan Cao",
            "Qiang Ding",
            "Ping Luo"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.950.pdf"
    },
    {
        "title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities",
        "authors": [
            "Sheng Xu",
            "Peifeng Li",
            "Qiaoming Zhu"
        ],
        "published": "2023",
        "summary": "Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the \u201cencoding first, then scoring\u201d framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM (masked language model) task. This allows for simultaneous event modeling and coreference discrimination within a single template, with a fully shared context. In addition, we introduce two auxiliary prompt tasks, event-type compatibility and argument compatibility, to explicitly demonstrate the reasoning process of ECR, which helps the model make final predictions. Experimental results show that our method CorefPrompt performs well in a state-of-the-art (SOTA) benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.954.pdf"
    },
    {
        "title": "FinEntity: Entity-level Sentiment Classification for Financial Texts",
        "authors": [
            "Yixuan Tang",
            "Yi Yang",
            "Allen Huang",
            "Andy Tam",
            "Justin Tang"
        ],
        "published": "2023",
        "summary": "In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called FinEntity, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in the paper. Additionally, we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on entity-level sentiment classification. In a case study, we demonstrate the practical utility of using FinEntity in monitoring cryptocurrency markets. The data and code of FinEntity is available at https://github.com/yixuantt/FinEntity.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.956.pdf"
    },
    {
        "title": "Rationale-Enhanced Language Models are Better Continual Relation Learners",
        "authors": [
            "Weimin Xiong",
            "Yifan Song",
            "Peiyi Wang",
            "Sujian Li"
        ],
        "published": "2023",
        "summary": "Continual relation extraction (CRE) aims to solve the problem of catastrophic forgetting when learning a sequence of newly emerging relations. Recent CRE studies have found that catastrophic forgetting arises from the model\u2019s lack of robustness against future analogous relations. To address the issue, we introduce rationale, i.e., the explanations of relation classification results generated by Large Language Models (LLM), into CRE task. Specifically, we design the multi-task rationale tuning strategy to help the model learn current relations robustly. We also conduct contrastive rationale replay to further distinguish analogous relations. Experimental results on two standard benchmarks demonstrate that our method outperforms the state-of-the-art CRE models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.958.pdf"
    },
    {
        "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
        "authors": [
            "Dongjun Kang",
            "Joonsuk Park",
            "Yohan Jo",
            "JinYeong Bak"
        ],
        "published": "2023",
        "summary": "Being able to predict people\u2019s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people\u2019s opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods\u2014argument generation and question answering\u2014designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.961.pdf"
    },
    {
        "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Yun-Nung Chen",
            "Hsin-Hsi Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL\u2014a simple framework which bootstraps LMs\u2019 intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves results comparable to using real demonstrations. Additionally, we conduct a range of analyses to validate Self-ICL\u2019s effectiveness and provide insights for its behaviors under different settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.968.pdf"
    },
    {
        "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
        "authors": [
            "Zexuan Zhong",
            "Zhengxuan Wu",
            "Christopher Manning",
            "Christopher Potts",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model\u2019s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.971.pdf"
    },
    {
        "title": "Stance Detection on Social Media with Background Knowledge",
        "authors": [
            "Ang Li",
            "Bin Liang",
            "Jingqian Zhao",
            "Bowen Zhang",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Identifying users\u2019 stances regarding specific targets/topics is a significant route to learning public opinion from social media platforms. Most existing studies of stance detection strive to learn stance information about specific targets from the context, in order to determine the user\u2019s stance on the target. However, in real-world scenarios, we usually have a certain understanding of a target when we express our stance on it. In this paper, we investigate stance detection from a novel perspective, where the background knowledge of the targets is taken into account for better stance detection. To be specific, we categorize background knowledge into two categories: episodic knowledge and discourse knowledge, and propose a novel Knowledge-Augmented Stance Detection (KASD) framework. For episodic knowledge, we devise a heuristic retrieval algorithm based on the topic to retrieve the Wikipedia documents relevant to the sample. Further, we construct a prompt for ChatGPT to filter the Wikipedia documents to derive episodic knowledge. For discourse knowledge, we construct a prompt for ChatGPT to paraphrase the hashtags, references, etc., in the sample, thereby injecting discourse knowledge into the sample. Experimental results on four benchmark datasets demonstrate that our KASD achieves state-of-the-art performance in in-target and zero-shot stance detection.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.972.pdf"
    },
    {
        "title": "NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation",
        "authors": [
            "Oliver Li",
            "Mallika Subramanian",
            "Arkadiy Saakyan",
            "Sky CH-Wang",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset is synthetically generated in both Chinese and English using a human-in-the-loop pipeline by prompting large language models with a small collection of expert-annotated social norms. We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task. Our findings point towards new directions for understanding the nuances of social norms as they manifest in conversational contexts that span across languages and cultures.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.974.pdf"
    },
    {
        "title": "Are Compressed Language Models Less Subgroup Robust?",
        "authors": [
            "Leonidas Gee",
            "Andrea Zugarini",
            "Novi Quadrianto"
        ],
        "published": "2023",
        "summary": "To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.983.pdf"
    },
    {
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
        "authors": [
            "Yongchao Chen",
            "Rujul Gandhi",
            "Yang Zhang",
            "Chuchu Fan"
        ],
        "published": "2023",
        "summary": "Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 23K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation largely enhances corpus richness. We test the generalization of trained models on five varied domains. To achieve full NL-TL transformation, we either combine the lifted model with AP recognition task or do the further finetuning on each specific domain. During the further finetuning, our model achieves higher accuracy (> 95%) using only <10% training data, compared with the baseline sequence to sequence (Seq2Seq) model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.985.pdf"
    },
    {
        "title": "Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia.",
        "authors": [
            "Dimitris Gkoumas",
            "Matthew Purver",
            "Maria Liakata"
        ],
        "published": "2023",
        "summary": "Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability estimates from the best model to construct digital linguistic markers measuring the overall quality in communication and the intensity of a variety of language disorders. We investigate how the digital markers characterize dementia speech from a longitudinal perspective. We find that our proposed communication marker is able to robustly and reliably characterize the language of people with dementia, outperforming existing linguistic approaches; and shows external validity via significant correlation with clinical markers of behaviour. Finally, our proposed linguistic disorder markers provide useful insights into gradual language impairment associated with disease progression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.986.pdf"
    },
    {
        "title": "Consistency Analysis of ChatGPT",
        "authors": [
            "Myeongjun Jang",
            "Thomas Lukasiewicz"
        ],
        "published": "2023",
        "summary": "ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, adding extra support to the claim that AI can now assist and even replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding logically consistent behaviour, focusing specifically on semantic consistency and the properties of negation, symmetric, and transitive consistency. Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.991.pdf"
    },
    {
        "title": "AnyTOD: A Programmable Task-Oriented Dialog System",
        "authors": [
            "Jeffrey Zhao",
            "Yuan Cao",
            "Raghav Gupta",
            "Harrison Lee",
            "Abhinav Rastogi",
            "Mingqiu Wang",
            "Hagen Soltau",
            "Izhak Shafran",
            "Yonghui Wu"
        ],
        "published": "2023",
        "summary": "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1006.pdf"
    },
    {
        "title": "Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs",
        "authors": [
            "Souvika Sarkar",
            "Dongji Feng",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2023",
        "summary": "In this paper, we conducted a comprehensive study with the latest Sentence Encoders and Large Language Models (LLMs) on the challenging task of \u201cdefinition-wild zero-shot topic inference\u201d, where users define or provide the topics of interest in real-time. Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX. Furthermore, Sentence-BERT, a BERT-based classical sentence encoder, outperformed PaLM and achieved performance comparable to ChatGPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1008.pdf"
    },
    {
        "title": "TaskDiff: A Similarity Metric for Task-Oriented Conversations",
        "authors": [
            "Ankita Bhaumik",
            "Praveen Venkateswaran",
            "Yara Rizk",
            "Vatche Isahagian"
        ],
        "published": "2023",
        "summary": "The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other related approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1009.pdf"
    },
    {
        "title": "Euphemistic Abuse \u2013 A New Dataset and Classification Experiments for Implicitly Abusive Language",
        "authors": [
            "Michael Wiegand",
            "Jana Kampfmeier",
            "Elisabeth Eder",
            "Josef Ruppenhofer"
        ],
        "published": "2023",
        "summary": "We address the task of identifying euphemistic abuse (e.g. \u201cYou inspire me to fall asleep\u201d) paraphrasing simple explicitly abusive utterances (e.g. \u201cYou are boring\u201d). For this task, we introduce a novel dataset that has been created via crowdsourcing. Special attention has been paid to the generation of appropriate negative (non-abusive) data. We report on classification experiments showing that classifiers trained on previous datasets are less capable of detecting such abuse. Best automatic results are obtained by a classifier that augments training data from our new dataset with automatically-generated GPT-3 completions. We also present a classifier that combines a few manually extracted features that exemplify the major linguistic phenomena constituting euphemistic abuse.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1012.pdf"
    },
    {
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "authors": [
            "Shushan Arakelyan",
            "Rocktim Das",
            "Yi Mao",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1013.pdf"
    },
    {
        "title": "A Benchmark for Reasoning with Spatial Prepositions",
        "authors": [
            "Iulia Comsa",
            "Srini Narayanan"
        ],
        "published": "2023",
        "summary": "Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1015.pdf"
    },
    {
        "title": "Composable Text Controls in Latent Space with ODEs",
        "authors": [
            "Guangyi Liu",
            "Zeyu Feng",
            "Yuan Gao",
            "Zichao Yang",
            "Xiaodan Liang",
            "Junwei Bao",
            "Xiaodong He",
            "Shuguang Cui",
            "Zhen Li",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1030.pdf"
    },
    {
        "title": "Document-Level Machine Translation with Large Language Models",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs\u2019 ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs (We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1036.pdf"
    },
    {
        "title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation",
        "authors": [
            "Jiayu Lin",
            "Rong Ye",
            "Meng Han",
            "Qi Zhang",
            "Ruofei Lai",
            "Xinyu Zhang",
            "Zhao Cao",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2023",
        "summary": "Counter-argument generation\u2014a captivating area in computational linguistics\u2014seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics. In this paper, we present the ArgTersely benchmark for sentence-level counter-argument generation, drawing from a manually annotated dataset from the ChangeMyView debate forum. We also propose Arg-LlaMA for generating high-quality counter-argument. For better evaluation, we trained a BERT-based evaluator Arg-Judge with human preference data. We conducted comparative experiments involving various baselines such as LlaMA, Alpaca, GPT-3, and others. The results show the competitiveness of our proposed framework and evaluator in counter-argument generation tasks. Code and data are available at https://github.com/amazingljy1206/ArgTersely.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1039.pdf"
    },
    {
        "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
        "authors": [
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed",
            "AbdelRahim Elmadany",
            "Alcides Inciarte",
            "Md Tawkat Islam Khondaker"
        ],
        "published": "2023",
        "summary": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset ( 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1040.pdf"
    },
    {
        "title": "Designing, Evaluating, and Learning from Humans Interacting with NLP Models",
        "authors": [
            "Tongshuang Wu",
            "Diyi Yang",
            "Sebastin Santy"
        ],
        "published": "2023",
        "summary": "The rapid advancement of natural language processing (NLP) research has led to various applications spanning a wide range of domains that require models to interact with humans \u2013 e.g., chatbots responding to human inquiries, machine translation systems assisting human translators, designers prompting Large Language Models for co-creation or prototyping AI-infused applications, etc. In these cases, humans interaction is key to the success of NLP applications; any potential misconceptions or differences might lead to error cascades at the subsequent stages. Such interaction involves a lot of design choices around models, e.g. the sensitivity of interfaces, the impact of design choice and evaluation questions, etc. This tutorial aims to provide a systematic and up-to-date overview of key considerations and effective approaches for studying human-NLP model interactions. Our tutorial will focus specifically on the scenario where end users \u2013 lay people and domain experts who have access to NLP models but are less familiar with NLP techniques \u2013 use or collaborate with deployed models. Throughout the tutorial, we will use five case studies (on classifier-assisted decision making, machine-aided translation, dialog systems, and prompting) to cover three major themes: (1) how to conduct human-in-the-loop usability evaluations to ensure that models are capable of interacting with humans; (2) how to design user interfaces (UIs) and interaction mechanisms that provide end users with easy access to NLP models; (3) how to learn and improve NLP models through the human interactions. We will use best practices from HCI to ground our discussion, and will highlight current challenges and future directions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-tutorial.3.pdf"
    },
    {
        "title": "LLM-driven Instruction Following: Progresses and Concerns",
        "authors": [
            "Wenpeng Yin",
            "Qinyuan Ye",
            "Pengfei Liu",
            "Xiang Ren",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2023",
        "summary": "The progress of natural language processing (NLP) is primarily driven by machine learning that optimizes a system on a large-scale set of task-specific labeled examples. This learning paradigm limits the ability of machines to have the same capabilities as humans in handling new tasks since humans can often solve unseen tasks with a couple of examples accompanied by task instructions. In addition, we may not have a chance to prepare task-specific examples of large-volume for new tasks because we cannot foresee what task needs to be addressed next and how complex to annotate for it. Therefore, task instructions act as a novel and promising resource for supervision. This tutorial targets researchers and practitioners who are interested in AI and ML technologies for NLP generalization in a low-shot scenario. In particular, we will present a diverse thread of instruction-driven NLP studies that try to answer the following questions: (i) What is task instruction? (ii) How is the process of creating datasets and evaluating systems conducted? (iii) How to encode task instructions? (iv) When and why do some instructions work better? (v) What concerns remain in LLM-driven instruction following? We will discuss several lines of frontier research that tackle those challenges and will conclude the tutorial by outlining directions for further investigation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-tutorial.4.pdf"
    },
    {
        "title": "Mitigating Societal Harms in Large Language Models",
        "authors": [
            "Sachin Kumar",
            "Vidhisha Balachandran",
            "Lucille Njoo",
            "Antonios Anastasopoulos",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts \u2013 e.g., detecting and mitigating gender bias in NLP models \u2013 no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-tutorial.5.pdf"
    },
    {
        "title": "Creative Natural Language Generation",
        "authors": [
            "Tuhin Chakrabarty",
            "Vishakh Padmakumar",
            "He He",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Large language models such as GPT-3, GPT4, Claude etc., have advanced the state of the art in several natural language generation tasks such as text summarization and machine translation. However when it comes to open-ended tasks with a focus on creativity such as generating stories, poetry, or various forms of figurative language, these state-of-the-art language models are often found to be inadequate. This tutorial aims to bring awareness of the important and emerging research area of open-domain creative generation, with a focus on language generation while also touching on multi-modal generation (e.g., image captioning, visual metaphors). It targets natural language processing (NLP) and artificial intelligence (AI) researchers as well as creative writing practitioners who are interested in building systems that are capable of emulating as well as augmenting human creativity. In particular, we will review recent studies on creative language generation both at the sentence level as well as longer forms of text. We will provide the audiences with a holistic view of 1) the importance and challenges of building creative language generation systems; 2) how we incorporate content planning, domain knowledge and creativity specific heuristics for different forms of creative language generation such as story, poetry, humor, metaphors etc 3) how can we build better evaluation methods for creative text generation? In particular, how could the recent advancement of AI shape the future workforce for creativity? We will conclude the tutorial by outlining future research directions in this area.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-tutorial.6.pdf"
    },
    {
        "title": "Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs",
        "authors": [
            "Jonas Golde",
            "Patrick Haller",
            "Felix Hamborg",
            "Julian Risch",
            "Alan Akbik"
        ],
        "published": "2023",
        "summary": "Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to \u201cgenerate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment.\u201d The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classification, question answering, and entity recognition), and is integrated with well-known libraries to facilitate quick experimentation. With Fabricator, we aim to support researchers in conducting reproducible dataset generation experiments using LLMs and help practitioners apply this approach to train models for downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.1.pdf"
    },
    {
        "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools",
        "authors": [
            "Jingwei Ni",
            "Julia Bingler",
            "Chiara Colesanti-Senni",
            "Mathias Kraus",
            "Glen Gostlow",
            "Tobias Schimanski",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Nicolas Webersinke",
            "Tobias Wekhof",
            "Tingyu Yu",
            "Markus Leippold"
        ],
        "published": "2023",
        "summary": "In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we introduce ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available. Video Introduction: https://www.youtube.com/watch?v=Q5AzaKzPE4M Github: https://github.com/EdisonNi-hku/chatreport Live web app: reports.chatclimate.ai",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.3.pdf"
    },
    {
        "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
        "authors": [
            "Yasuto Hoshi",
            "Daisuke Miyashita",
            "Youyang Ng",
            "Kento Tatsuno",
            "Yasuhiro Morioka",
            "Osamu Torii",
            "Jun Deguchi"
        ],
        "published": "2023",
        "summary": "Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.4.pdf"
    },
    {
        "title": "VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog",
        "authors": [
            "Henrik Voigt",
            "Nuno Carvalhais",
            "Monique Meuschke",
            "Markus Reichstein",
            "Sina Zarrie",
            "Kai Lawonn"
        ],
        "published": "2023",
        "summary": "The advent of large language models has brought about new ways of interacting with data intuitively via natural language. In recent years, a variety of visualization systems have explored the use of natural language to create and modify visualizations through visualization-oriented dialog. However, the majority of these systems rely on tailored dialog agents to analyze domain-specific data and operate domain-specific visualization tools and libraries. This is a major challenge when trying to transfer functionalities between dialog interfaces of different visualization applications. To address this issue, we propose VIST5, a visualization-oriented dialog system that focuses on easy adaptability to an application domain as well as easy transferability of language-controllable visualization library functions between applications. Its architecture is based on a retrieval-augmented T5 language model that leverages few-shot learning capabilities to enable a rapid adaptation of the system.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.5.pdf"
    },
    {
        "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
        "authors": [
            "Arno Candel",
            "Jon McKinney",
            "Philipp Singer",
            "Pascal Pfeiffer",
            "Maximilian Jeblick",
            "Chun Ming Lee",
            "Marcos Conde"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. Our demo is available at: https://gpt.h2o.ai/",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.6.pdf"
    },
    {
        "title": "Koala: An Index for Quantifying Overlaps with Pre-training Corpora",
        "authors": [
            "Thuy-Trang Vu",
            "Xuanli He",
            "Gholamreza Haffari",
            "Ehsan Shareghi"
        ],
        "published": "2023",
        "summary": "In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we launch Koala, a searchable index over large pre-training corpora using lossless compressed suffix arrays with highly efficient compression rate and search support. In its first release we index the public proportion of OPT 175B, GPT-3, GPT-Neo, GPT-Neo, LLaMA, BERT, ELECTRA, RoBERTA, XLNet pre-training corpora. Koala provides a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs. Koala is available for public use at https://koala-index.erc.monash.edu/.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.7.pdf"
    },
    {
        "title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge",
        "authors": [
            "Farima Fatahi Bayat",
            "Kun Qian",
            "Benjamin Han",
            "Yisi Sang",
            "Anton Belyy",
            "Samira Khorshidi",
            "Fei Wu",
            "Ihab Ilyas",
            "Yunyao Li"
        ],
        "published": "2023",
        "summary": "Detecting factual errors of textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs\u2019 inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual er- rors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85% F1) shows the potential of our tool.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.10.pdf"
    },
    {
        "title": "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning",
        "authors": [
            "Clifton Poth",
            "Hannah Sterz",
            "Indraneil Paul",
            "Sukannya Purkayastha",
            "Leon Engl\u00e4nder",
            "Timo Imhof",
            "Ivan Vuli\u0107",
            "Sebastian Ruder",
            "Iryna Gurevych",
            "Jonas Pfeiffer"
        ],
        "published": "2023",
        "summary": "We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library\u2019s efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.13.pdf"
    },
    {
        "title": "CLEVA: Chinese Language Models EVAluation Platform",
        "authors": [
            "Yanyang Li",
            "Jianqiao Zhao",
            "Duo Zheng",
            "Zi-Yuan Hu",
            "Zhi Chen",
            "Xiaohui Su",
            "Yongfeng Huang",
            "Shijia Huang",
            "Dahua Lin",
            "Michael Lyu",
            "Liwei Wang"
        ],
        "published": "2023",
        "summary": "With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model\u2019s capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model\u2019s performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs. Our platform employs a standardized workflow to assess LLMs\u2019 performance across various dimensions, regularly updating a competitive leaderboard. To alleviate contamination, CLEVA curates a significant proportion of new data and develops a sampling strategy that guarantees a unique subset for each leaderboard round. Empowered by an easy-to-use interface that requires just a few mouse clicks and a model API, users can conduct a thorough evaluation with minimal coding. Large-scale experiments featuring 23 Chinese LLMs have validated CLEVA\u2019s efficacy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.17.pdf"
    },
    {
        "title": "Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs",
        "authors": [
            "Binfeng Xu",
            "Xukun Liu",
            "Hua Shen",
            "Zeyu Han",
            "Yuhan Li",
            "Murong Yue",
            "Zhiyuan Peng",
            "Yuchen Liu",
            "Ziyu Yao",
            "Dongkuan Xu"
        ],
        "published": "2023",
        "summary": "Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. This paper proposes Gentopia, a lightweight and extensible framework for ALMs. Gentopia allows the flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish Gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in Gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, Gentbench, an integral component of Gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release Gentopia on Github and will continuously move forward.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.20.pdf"
    },
    {
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "authors": [
            "Dingyao Yu",
            "Kaitao Song",
            "Peiling Lu",
            "Tianyu He",
            "Xu Tan",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian"
        ],
        "published": "2023",
        "summary": "AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience. The code is available on GitHub along with a brief instructional video.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.21.pdf"
    },
    {
        "title": "MiniChain: A Small Library for Coding with Large Language Models",
        "authors": [
            "Alexander Rush"
        ],
        "published": "2023",
        "summary": "Programming augmented by large language models (LLMs) opens up many new application areas, but also requires care. LLMs are accurate enough, on average, to replace core functionality, yet make basic mistakes that demonstrate a lack of robustness. An ecosystem of prompting tools, from intelligent agents to new programming languages, have emerged with different solutions for patching LLMs with other tools. In this work, we introduce MiniChain, an opinionated tool for LLM augmented programming, with the design goals of ease-of-use of prototyping, transparency through automatic visualization, and a minimalistic approach to advanced features. The MiniChain library provides core primitives for coding LLM calls, separating out prompt templates, and capturing program structure. The library includes demo implementations of the main applications papers in the area, including chat-bots, code generation, retrieval-based question answering, and complex information extraction. The library is open-source and available at https://github.com/srush/MiniChain, with code demos available at https://srush-minichain.hf.space/, and video demo at https://www.youtube.com/watch?v=VszZ1VnO7sk.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.27.pdf"
    },
    {
        "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
        "authors": [
            "Viet Lai",
            "Chien Nguyen",
            "Nghia Ngo",
            "Thuat Nguyen",
            "Franck Dernoncourt",
            "Ryan Rossi",
            "Thien Nguyen"
        ],
        "published": "2023",
        "summary": "A key technology for large language models (LLMs) involves instruction tuning that helps align the models\u2019 responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are applied to produce the best commercial LLMs. To improve the accessibility of LLMs, various instruction-tuned open-source LLMs have also been introduced recently. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their accessibility to many other languages in the world. In addition, SFT has been used as the only approach to instruction-tune open-source LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework with created resources, fine-tuned LLMs, interaction scripts are released at https://github.com/nlp-uoregon/Okapi. A demo video to show our framework can also be found at: https://youtu.be/QFV2fkPwvi0.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.28.pdf"
    },
    {
        "title": "InsightPilot: An LLM-Empowered Automated Data Exploration System",
        "authors": [
            "Pingchuan Ma",
            "Rui Ding",
            "Shuai Wang",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023",
        "summary": "Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process. InsightPilot features a set of carefully designed analysis actions that streamline the data exploration process. Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights. We demonstrate the effectiveness of InsightPilot in a user study and a case study, showing how it can help users gain valuable insights from their datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.31.pdf"
    },
    {
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
        "authors": [
            "Vijay Viswanathan",
            "Chenyang Zhao",
            "Amanda Bertsch",
            "Tongshuang Wu",
            "Graham Neubig"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model. Our demo video is posted at youtu.be/LYYQ_EhGd-Q.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.38.pdf"
    },
    {
        "title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
        "authors": [
            "Traian Rebedea",
            "Razvan Dinu",
            "Makesh Narsimhan Sreedhar",
            "Christopher Parisien",
            "Jonathan Cohen"
        ],
        "published": "2023",
        "summary": "NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Using a runtime inspired from dialogue management, NeMo Guardrails provides a different approach by allowing developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.40.pdf"
    },
    {
        "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
        "authors": [
            "Ekaterina Fadeeva",
            "Roman Vashurin",
            "Akim Tsvigun",
            "Artem Vazhentsev",
            "Sergey Petrakov",
            "Kirill Fedyanin",
            "Daniil Vasilev",
            "Elizaveta Goncharova",
            "Alexander Panchenko",
            "Maxim Panov",
            "Timothy Baldwin",
            "Artem Shelmanov"
        ],
        "published": "2023",
        "summary": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often \u201challucinate\u201d, i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.41.pdf"
    },
    {
        "title": "Descriptive Knowledge Graph in Biomedical Domain",
        "authors": [
            "Kerui Zhu",
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023",
        "summary": "We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.42.pdf"
    },
    {
        "title": "Prompterator: Iterate Efficiently towards More Effective Prompts",
        "authors": [
            "Samuel Su\u010dik",
            "Daniel Skala",
            "Andrej \u0160vec",
            "Peter Hra\u0161ka",
            "Marek \u0160uppa"
        ],
        "published": "2023",
        "summary": "With the advent of Large Language Models (LLMs) the process known as prompting, which entices the LLM to solve an arbitrary language processing task without the need for finetuning, has risen to prominence. Finding well-performing prompts, however, is a non-trivial task which requires experimentation in order to arrive at a prompt that solves a specific task. When a given task does not readily reduce to one that can be easily measured with well established metrics, human evaluation of the results obtained by prompting is often necessary. In this work we present prompterator, a tool that helps the user interactively iterate over various potential prompts and choose the best performing one based on human feedback. It is distributed as an open source package with out-of-the-box support for various LLM providers and was designed to be easily extensible.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.43.pdf"
    },
    {
        "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",
        "authors": [
            "Baoli Zhang",
            "Haining Xie",
            "Pengfan Du",
            "Junhao Chen",
            "Pengfei Cao",
            "Yubo Chen",
            "Shengping Liu",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "The unprecedented performance of LLMs requires comprehensive and accurate evaluation. We argue that for LLMs evaluation, benchmarks need to be comprehensive and systematic. To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks. Especially, we also propose a new benchmark that focus on knowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering benchmark that fully assesses LLMs in Chinese, while also providing equally robust evaluation abilities in English. (4) Avoiding potential data leakage: To avoid data leakage, we construct evaluation data specifically for 37 tasks. We evaluate 10 current mainstream LLMs, and conduct an in-depth discussion and analysis of their results. The ZhuJiu benchmark and open-participation leaderboard are publicly released at http://www.zhujiu-benchmark.com and we also provide a demo video at https://youtu.be/qypkJ89L1Ic.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.44.pdf"
    },
    {
        "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
        "authors": [
            "Kai Lv",
            "Shuo Zhang",
            "Tianle Gu",
            "Shuhao Xing",
            "Jiawei Hong",
            "Keyu Chen",
            "Xiaoran Liu",
            "Yuqing Yang",
            "Honglin Guo",
            "Tengxiao Liu",
            "Yu Sun",
            "Qipeng Guo",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, and LOMO. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.48.pdf"
    },
    {
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "authors": [
            "Hang Zhang",
            "Xin Li",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM\u2019s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.49.pdf"
    },
    {
        "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
        "authors": [
            "Chenliang Li",
            "He Chen",
            "Ming Yan",
            "Weizhou Shen",
            "Haiyang Xu",
            "Zhikai Wu",
            "Zhicheng Zhang",
            "Wenmeng Zhou",
            "Yingda Chen",
            "Chen Cheng",
            "Hongzhu Shi",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent frameworks that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with a customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent online demo, library are now publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.51.pdf"
    },
    {
        "title": "EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge",
        "authors": [
            "Tom Bryan",
            "Jacob Carlson",
            "Abhishek Arora",
            "Melissa Dell"
        ],
        "published": "2023",
        "summary": "Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters\u2019 visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code and include lightweight models designed for mobile phones that are extremely cheap to deploy. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing a Japanese document collection for which all other OCR solutions failed.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-demo.52.pdf"
    },
    {
        "title": "Enhancing Language Model with Unit Test Techniques for Efficient Regular Expression Generation",
        "authors": [
            "Chenhui Mao",
            "Xiexiong Lin",
            "Xin Jin",
            "Xin Zhang"
        ],
        "published": "2023",
        "summary": "Recent research has investigated the use of generative language models to produce regular expressions with semantic-based approaches. However, these approaches have shown shortcomings in practical applications, particularly in terms of functional correctness, which refers to the ability to reproduce the intended function inputs by the user. To address this issue, we present a novel method called Unit-Test Driven Reinforcement Learning (UTD-RL). Our approach differs from previous methods by taking into account the crucial aspect of functional correctness and transforming it into a differentiable gradient feedback using policy gradient techniques. In which functional correctness can be evaluated through Unit Tests, a testing method that ensures regular expressions meets its design and performs as intended. Experiments conducted on three public datasets demonstrate the effectiveness of the proposed method in generating regular expressions. This method has been employed in a regulatory scenario where regular expressions can be utilized to ensure that all online content is free from non-compliant elements, thereby significantly reducing the workload of relevant personnel.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.2.pdf"
    },
    {
        "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
        "authors": [
            "Takuma Udagawa",
            "Aashka Trivedi",
            "Michele Merler",
            "Bishwaranjan Bhattacharjee"
        ],
        "published": "2023",
        "summary": "Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reasons behind its success. Moreover, we show that HS transfer remains as a competitive baseline, especially under a sophisticated layer mapping strategy, while OD transfer consistently lags behind other approaches. Findings from this study helped us deploy efficient yet effective student models for latency-critical applications.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.3.pdf"
    },
    {
        "title": "Gatekeeper to save COGS and improve efficiency of Text Prediction",
        "authors": [
            "Nidhi Tiwari",
            "Sneha Kola",
            "Milos Milunovic",
            "Si-qing Chen",
            "Marjan Slavkovski"
        ],
        "published": "2023",
        "summary": "The text prediction (TP) workflow calls a Large Language Model (LLM), almost, after every character to get subsequent sequence of characters, till user accepts a suggestion. The confidence score of the prediction is commonly used for filtering the results to ensure that only correct predictions are shown to user. As LLMs require massive amounts of computation and storage, such an approach incurs network and high execution cost. So, we propose a Model gatekeeper (GK) to stop the LLM calls that will result in incorrect predictions at client application level itself. This way a GK can save cost of model inference and improve user experience by not showing the incorrect predictions. We demonstrate that use of a model gatekeeper saved approx 46.6% of COGS for TP, at the cost of approx 4.5% loss in character saving. Use of GK also improved the efficiency (suggestion rate) of TP model by 73%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.5.pdf"
    },
    {
        "title": "Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities",
        "authors": [
            "Fengjun Wang",
            "Moran Beladev",
            "Ofri Kleinfeld",
            "Elina Frayerman",
            "Tal Shachar",
            "Eran Fainman",
            "Karen Lastmann Assaraf",
            "Sarai Mizrachi",
            "Benjamin Wang"
        ],
        "published": "2023",
        "summary": "Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic. Text2Topic also supports zero-shot predictions, produces domain-specific text embeddings, and enables production-scale batch-inference with high throughput. The final model achieves accurate and comprehensive results compared to state-of-the-art baselines, including large language models (LLMs). In this study, a total of 239 topics are defined, and around 1.6 million text-topic pairs annotations (in which 200K are positive) are collected on approximately 120K texts from 3 main data sources on Booking.com. The data is collected with optimized smart sampling and partial labeling. The final Text2Topic model is deployed on a real-world stream processing platform, and it outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP score. We summarize the modeling choices which are extensively tested through ablation studies, and share detailed in-production decision-making steps.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.10.pdf"
    },
    {
        "title": "A Pretrained Language Model for Cyber Threat Intelligence",
        "authors": [
            "Youngja Park",
            "Weiqiu You"
        ],
        "published": "2023",
        "summary": "We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats. We provide detailed information about the domain corpus collection, the training methodology and its effectiveness for a variety of NLP tasks for the cybersecurity domain. The experiments show that CTI-BERT significantly outperforms several general-domain and security-domain models for these cybersecurity applications indicating that the training data and methodology have a significant impact on the model performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.12.pdf"
    },
    {
        "title": "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios",
        "authors": [
            "Yilun Zhao",
            "Haowei Zhang",
            "Shengyun Si",
            "Linyong Nan",
            "Xiangru Tang",
            "Arman Cohan"
        ],
        "published": "2023",
        "summary": "Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users\u2019 information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https://github.com/yale-nlp/LLM-T2T.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.17.pdf"
    },
    {
        "title": "WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models",
        "authors": [
            "Jun-Yan He",
            "Zhi-Qi Cheng",
            "Chenyang Li",
            "Jingdong Sun",
            "Wangmeng Xiang",
            "Xianhui Lin",
            "Xiaoyang Kang",
            "Zengke Jin",
            "Yusen Hu",
            "Bin Luo",
            "Yifeng Geng",
            "Xuansong Xie"
        ],
        "published": "2023",
        "summary": "This paper introduces WordArt Designer, a user-driven framework for artistic typography synthesis, relying on the Large Language Model (LLM). The system incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo modules. 1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The SemTypo module optimizes font designs using semantic concepts, striking a balance between artistic transformation and readability. 3) Building on the semantic layout provided by the SemTypo module, the StyTypo module creates smooth, refined images. 4) The TexTypo module further enhances the design\u2019s aesthetics through texture rendering, enabling the generation of inventive textured fonts. Notably, WordArt Designer highlights the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: https://www.modelscope.cn/studios/WordArt/WordArt.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.23.pdf"
    },
    {
        "title": "Unveiling Identity Biases in Toxicity Detection : A Game-Focused Dataset and Reactivity Analysis Approach",
        "authors": [
            "Josiane Van Dorpe",
            "Zachary Yang",
            "Nicolas Grenon-Godbout",
            "Gr\u00e9goire Winterstein"
        ],
        "published": "2023",
        "summary": "Identity biases arise commonly from annotated datasets, can be propagated in language models and can cause further harm to marginal groups. Existing bias benchmarking datasets are mainly focused on gender or racial biases and are made to pinpoint which class the model is biased towards. They also are not designed for the gaming industry, a concern for models built for toxicity detection in videogames\u2019 chat. We propose a dataset and a method to highlight oversensitive terms using reactivity analysis and the model\u2019s performance. We test our dataset against ToxBuster, a language model developed by Ubisoft fine-tuned for toxicity detection on multiplayer videogame\u2019s written chat, and Perspective API. We find that these toxicity models often automatically tag terms related to a community\u2019s identity as toxic, which prevents members of already marginalized groups to make their presence known or have a mature / normal conversation. Through this process, we have generated an interesting list of terms that trigger the models to varying degrees, along with insights on establishing a baseline through human annotations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.26.pdf"
    },
    {
        "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
        "authors": [
            "Fangkai Yang",
            "Pu Zhao",
            "Zezhong Wang",
            "Lu Wang",
            "Bo Qiao",
            "Jue Zhang",
            "Mohit Garg",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "published": "2023",
        "summary": "Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average due to its lack of specific domain knowledge. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, centered around Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, an area not extensively covered in general LLMs, making it well-suited for evaluating methods aiming to enhance LLMs\u2019 domain-specific capabilities. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our method outperforms the commonly used LLM with retrieval methods. We make our source code and sample data available at: https://aka.ms/Microsoft_QA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.29.pdf"
    },
    {
        "title": "Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation",
        "authors": [
            "Dan Li",
            "Zi Long Zhu",
            "Janneke van de Loo",
            "Agnes Masip Gomez",
            "Vikrant Yadav",
            "Georgios Tsatsaronis",
            "Zubair Afzal"
        ],
        "published": "2023",
        "summary": "Extreme multi-label text classification is a prevalent task in industry, but it frequently encounters challenges in terms of machine learning perspectives, including model limitations, data scarcity, and time-consuming evaluation. This paper aims to mitigate these issues by introducing novel approaches. Firstly, we propose a label ranking model as an alternative to the conventional SciBERT-based classification model, enabling efficient handling of large-scale labels and accommodating new labels. Secondly, we present an active learning-based pipeline that addresses the data scarcity of new labels during the update of a classification system. Finally, we introduce ChatGPT to assist with model evaluation. Our experiments demonstrate the effectiveness of these techniques in enhancing the extreme multi-label text classification task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.30.pdf"
    },
    {
        "title": "Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Xue-Yong Fu",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2023",
        "summary": "This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT-3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA-2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.33.pdf"
    },
    {
        "title": "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications",
        "authors": [
            "Bhaktipriya Radharapu",
            "Kevin Robinson",
            "Lora Aroyo",
            "Preethi Lahoti"
        ],
        "published": "2023",
        "summary": "Adversarially testing large language models (LLMs) is crucial for their safe and responsible deployment in practice. We introduce an AI-assisted approach for automated generation of adversarial evaluation datasets to test the safety of LLM generations on new downstream applications. We call it AART AI-assisted Red-Teaming - an automated alternative to current manual red-teaming efforts. AART offers a data generation and augmentation pipeline of reusable and customizable recipes that reduce significantly human effort and enable integration of adversarial testing earlier in new product development. AART generates evaluation datasets with high diversity of content characteristics critical for effective adversarial testing (e.g. sensitive and harmful concepts, specific to a wide range of cultural and geographic regions and application scenarios). The data generation is steered by AI-assisted recipes to define, scope and prioritize diversity within a new application context. This feeds into a structured LLM-generation process that scales up evaluation priorities. This provides transparency of developers evaluation intentions and enables quick adaptation to new use cases and newly discovered model weaknesses. Compared to some of the state-of-the-art tools AART shows promising results in terms of concept coverage and data quality.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.37.pdf"
    },
    {
        "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks",
        "authors": [
            "Xianzhi Li",
            "Samuel Chan",
            "Xiaodan Zhu",
            "Yulong Pei",
            "Zhiqiang Ma",
            "Xiaomo Liu",
            "Sameena Shah"
        ],
        "published": "2023",
        "summary": "The most recent large language models (LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the finance domain? Understanding this basic question would have a significant impact on many downstream financial analytical tasks. In this paper, we conduct empirical studies and provide experimental evidences of their performance on a wide variety of financial text analytical problems, using eight benchmark datasets from five categories of tasks. We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models. We hope our study can help to understand the capability of the existing models in the financial domain and facilitate further improvements.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.39.pdf"
    },
    {
        "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
        "authors": [
            "Zhenting Qi",
            "Xiaoyu Tan",
            "Shaojie Shi",
            "Chao Qu",
            "Yinghui Xu",
            "Yuan Qi"
        ],
        "published": "2023",
        "summary": "Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently, Low-Rank Adaptation (LoRA) has become a promising alternative, offering tuning capabilities with reduced resource overhead. However, attaining satisfactory performance through the fine-tuning of LoRA is a non-trivial challenge. In this paper, we propose PILLOW, which aims to improve LoRA\u2019s performance by leveraging LLM\u2019s in-context learning capability through prompt matching via reinforcement learning in resource-constrained environments. Specifically, PILLOW incorporates a matching network that selects prompts from a user-defined pool, concatenates the optimal prompts given the user instruction, and performs inference using the LoRA-fine-tuned LLMs. Compared with typical instruction fine-tuning methods, PILLOW exhibits commensurate performance on various evaluation metrics, utilizing only consumer-grade GPU resources and exhibiting a large increase in training efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.45.pdf"
    },
    {
        "title": "Automatic Linking of Judgements to UK Supreme Court Hearings",
        "authors": [
            "Hadeel Saadany",
            "Constantin Orasan"
        ],
        "published": "2023",
        "summary": "One the most important archived legal material in the UK is the Supreme Court published judgements and video recordings of court sittings for the decided cases. The impact of Supreme Court published material extends far beyond the parties involved in any given case as it provides landmark rulings on arguable points of law of the greatest public and constitutional importance. However, the recordings of a case are usually very long which makes it both time and effort consuming for legal professionals to study the critical arguments in the legal deliberations. In this research, we summarise the second part of a combined research-industrial project for building an automated tool designed specifically to link segments in the text judgement to semantically relevant timespans in the videos of the hearings. The tool is employed as a User-Interface (UI) platform that provides a better access to justice by bookmarking the timespans in the videos which contributed to the final judgement of the case. We explain how we employ AI generative technology to retrieve the relevant links and show that the customisation of the GPT text embeddings to our dataset achieves the best accuracy for our automatic linking system.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.47.pdf"
    },
    {
        "title": "Automatic Marketing Theme and Commodity Construction System for E-commerce",
        "authors": [
            "Zhiping Wang",
            "Peng Lin",
            "Hainan Zhang",
            "Hongshen Chen",
            "Tianhao Li",
            "Zhuoye Ding",
            "Sulong Xu",
            "Jinghe Hu"
        ],
        "published": "2023",
        "summary": "When consumers\u2019 shopping needs are concentrated, they are more interested in the collection of commodities under the specific marketing theme. Therefore, mining marketing themes and their commodities collections can help customers save shopping costs and improve user clicks and purchases for recommendation system. However, the current system invites experts to write marketing themes and select the relevant commodities, which suffer from difficulty in mass production, poor timeliness and low online indicators. Therefore, we propose a automatic marketing theme and commodity construction system, which can not only generate popular marketing themes and select the relevant commodities automatically, but also improve the theme online effectiveness in the recommendation system. Specifically, we firstly utilize the pretrained language model to generate the marketing themes. And then, we utilize the theme-commodity consistency module to select the relevant commodities for the above generative theme. What\u2019s more, we also build the indicator simulator to evaluate the effectiveness of the above generative theme. When the indicator is lower, the above selective commodities will be input into the theme-rewriter module to generate more efficient marketing themes. Finally, we utilize the human screening to control the system quality. Both the offline experiments and online A/B test demonstrate the superior performance of our proposed system compared with state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.48.pdf"
    },
    {
        "title": "InsightNet : Structured Insight Mining from Customer Feedback",
        "authors": [
            "Sandeep Sricharan Mukku",
            "Manan Soni",
            "Chetan Aggarwal",
            "Jitenkumar Rana",
            "Promod Yenigalla",
            "Rashmi Patange",
            "Shyam Mohan"
        ],
        "published": "2023",
        "summary": "We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data. The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and employs a multi-task insight extraction architecture by fine-tuning an LLM. InsightNet identifies granular actionable topics with customer sentiments and verbatim for each topic. Evaluations on real-world customer review data show that InsightNet performs better than existing solutions in terms of structure, hierarchy and completeness. We empirically demonstrate that InsightNet outperforms the current state-of-the-art methods in multi-label topic classification, achieving an F1 score of 0.85, which is an improvement of 11% F1-score over the previous best results. Additionally, InsightNet generalises well for unseen aspects and suggests new topics to be added to the taxonomy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.53.pdf"
    },
    {
        "title": "Generative Models for Product Attribute Extraction",
        "authors": [
            "Ansel Blume",
            "Nasser Zalmout",
            "Heng Ji",
            "Xian Li"
        ],
        "published": "2023",
        "summary": "Product attribute extraction is an emerging field in information extraction and e-commerce, with applications including knowledge base construction, product recommendation, and enhancing customer experiences. In this work, we explore the use of generative models for product attribute extraction. We analyze their utility with hard and soft prompting methods, and demonstrate their ability to generate implicit attribute values, which state-of-the-art sequence tagging models are unable to extract. We perform a wide range of experiments on Amazon and MAVE product attribute datasets, and are the first to present results on multilingual attribute extraction. Our results show that generative models can outperform state- of-the-art tagging models for explicit product attribute extraction while having greater data efficiency, that they have the unique ability to perform implicit attribute extraction, and that in certain settings large language models can perform competitively with finetuned models with as little as two in-context examples.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.55.pdf"
    },
    {
        "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
        "authors": [
            "Md Rashad Al Hasan Rony",
            "Christian Suess",
            "Sinchana Ramakanth Bhat",
            "Viju Sudhi",
            "Julia Schneider",
            "Maximilian Vogel",
            "Roman Teucher",
            "Ken Friedl",
            "Soumya Sahoo"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain. In this paper, we propose CarExpert, an in-car retrieval-augmented conversational question-answering system leveraging LLMs for different tasks. Specifically, CarExpert employs LLMs to control the input, provide domain-specific documents to the extractive and generative answering components, and controls the output to ensure safe and domain-specific answers. A comprehensive empirical evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in generating natural, safe and car-specific answers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.56.pdf"
    },
    {
        "title": "Multi-word Tokenization for Sequence Compression",
        "authors": [
            "Leonidas Gee",
            "Leonardo Rigutini",
            "Marco Ernandes",
            "Andrea Zugarini"
        ],
        "published": "2023",
        "summary": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.58.pdf"
    },
    {
        "title": "JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization",
        "authors": [
            "Shang-Ching Liu",
            "ShengKun Wang",
            "Tsungyao Chang",
            "Wenqi Lin",
            "Chung-Wei Hsiung",
            "Yi-Chen Hsieh",
            "Yu-Ping Cheng",
            "Sian-Hong Luo",
            "Jianwei Zhang"
        ],
        "published": "2023",
        "summary": "In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results drawn from an extensive data analysis pipeline. Moreover, JarviX incorporates an automated machine learning (AutoML) pipeline for predictive modeling. This integration forms a comprehensive and automated optimization cycle, which proves particularly advantageous for optimizing machine configuration. The efficacy and adaptability of JarviX are substantiated through a series of practical use case studies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.59.pdf"
    },
    {
        "title": "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
        "authors": [
            "Xiaoyu Tan",
            "Shaojie Shi",
            "Xihe Qiu",
            "Chao Qu",
            "Zhenting Qi",
            "Yinghui Xu",
            "Yuan Qi"
        ],
        "published": "2023",
        "summary": "Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However, RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts. Therefore, we explore the possibility of aligning LLMs with their own understanding of HHH through IF and in-context learning (ICL). In this study, we propose a novel framework called Self-Criticism, which allows LLMs to align themselves with HHH based on the definition they learned from a large-scale text corpus. We begin by employing IF on a given instruction set and learning HHH discrimination through few-shot ICL. Subsequently, the LLMs evaluate their own generated responses and learn to produce \u201cbetter\u201d responses based on self-judgment. Finally, the model is retrained based on the self-generated responses to distill the whole process. By analyzing our proposed method, we also find interesting connections between Self-Criticism and goal-conditioned reinforcement learning, and pseudo-labeling. Experimental results demonstrate that this method achieves nearly identical performance to RLHF in terms of both human evaluation and evaluation by other LLMs, with only a minimal alignment tax.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.62.pdf"
    },
    {
        "title": "InstructPTS: Instruction-Tuning LLMs for Product Title Summarization",
        "authors": [
            "Besnik Fetahu",
            "Zhiyu Chen",
            "Oleg Rokhlenko",
            "Shervin Malmasi"
        ],
        "published": "2023",
        "summary": "E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization. Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUGE points, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.63.pdf"
    },
    {
        "title": "LLM4Vis: Explainable Visualization Recommendation using ChatGPT",
        "authors": [
            "Lei Wang",
            "Songheng Zhang",
            "Yun Wang",
            "Ee-Peng Lim",
            "Yong Wang"
        ],
        "published": "2023",
        "summary": "Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples. Our approach involves feature description, demonstration example selection, explanation generation, demonstration example construction, and inference steps. To obtain demonstration examples with high-quality explanations, we propose a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-based hint. Evaluations on the VizML dataset show that LLM4Vis outperforms or performs similarly to supervised learning models like Random Forest, Decision Tree, and MLP, in both few-shot and zero-shot settings. The qualitative evaluation also shows the effectiveness of explanations generated by LLM4Vis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.64.pdf"
    },
    {
        "title": "DocumentNet: Bridging the Data Gap in Document Pre-training",
        "authors": [
            "Lijun Yu",
            "Jin Miao",
            "Xiaoyu Sun",
            "Jiayi Chen",
            "Alexander Hauptmann",
            "Hanjun Dai",
            "Wei Wei"
        ],
        "published": "2023",
        "summary": "Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.66.pdf"
    },
    {
        "title": "Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting",
        "authors": [
            "Xinli Yu",
            "Zheng Chen",
            "Yanbin Lu"
        ],
        "published": "2023",
        "summary": "Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100 stocks, we utilize public historical stock data, company metadata, and economic/financial news. Our experiments employ GPT-4 for zero-shot/few-shot inference and Open LLaMA for instruction-based fine-tuning. The study demonstrates LLMs\u2019 ability to generate well-reasoned decisions by leveraging cross-sequence information and extracting insights from text and price time series. We show that our LLM-based approach outperforms classic ARMA-GARCH and gradient-boosting tree models. Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.69.pdf"
    },
    {
        "title": "ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations",
        "authors": [
            "Minh Thuan Nguyen",
            "Khanh Tung Tran",
            "Nhu Van Nguyen",
            "Xuan-Son Vu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) and their applications in low-resource languages (such as in Vietnamese) are limited due to lack of training data and benchmarking datasets. This paper introduces a practical real-world implementation of a question answering system for Vietnamese, called ViGPTQA, leveraging the power of LLM. Since there is no effective LLM in Vietnamese to date, we also propose, evaluate, and open-source an instruction-tuned LLM for Vietnamese, named ViGPT. ViGPT demonstrates exceptional performances, especially on real-world scenarios. We curate a new set of benchmark datasets that encompass both AI and human-generated data, providing a comprehensive evaluation framework for Vietnamese LLMs. By achieving state-of-the-art results and approaching other multilingual LLMs, our instruction-tuned LLM underscores the need for dedicated Vietnamese-specific LLMs. Our open-source model supports customized and privacy-fulfilled Vietnamese language processing systems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.70.pdf"
    },
    {
        "title": "On Sample-Efficient Code Generation",
        "authors": [
            "Hojae Han",
            "Yu Jin Kim",
            "Byoungjip Kim",
            "Youngwon Lee",
            "Kyungjae Lee",
            "Kyungmin Lee",
            "Moontae Lee",
            "Kyunghoon Bae",
            "Seung-won Hwang"
        ],
        "published": "2023",
        "summary": "Large language models often struggle to predict runtime behavior in code generation tasks, leading to a reliance on rejection sampling (best-of-n) to generate multiple code snippets then select the best. Our distinction is reducing sampling costs, without compromising generation quality. We introduce EFFICODE, a novel framework that prioritizes sampling on test problems that models can solve. We show how EFFICODE estimates solvability to optimize computational costs during multiple sampling. Based on empirical evidence, EFFICODE consistently demonstrates reduced sampling budgets while maintaining comparable code generation performance, especially when problems are challenging. In addition, utilizing EFFICODE to rank sampled code snippets also shows its effectiveness in answer code selection for reducing temporal costs, by not requiring any execution or test case generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.73.pdf"
    },
    {
        "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
        "authors": [
            "Zhoujun Cheng",
            "Jungo Kasai",
            "Tao Yu"
        ],
        "published": "2023",
        "summary": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly (up to 5\u00d7 with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code is released at the site https://github.com/xlang-ai/batch-prompting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.74.pdf"
    },
    {
        "title": "Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding",
        "authors": [
            "Zheng Chen",
            "Ziyan Jiang",
            "Fan Yang",
            "Eunah Cho",
            "Xing Fan",
            "Xiaojiang Huang",
            "Yanbin Lu",
            "Aram Galstyan"
        ],
        "published": "2023",
        "summary": "A Personalized Query Rewriting system strives to minimize defective queries to ensure robust conversational functionality by considering individual user behavior and preferences. It\u2019s designed as a search-based system, maintaining a user index of past successful interactions with the conversational AI. However, this method faces challenges with unseen interactions, which refers to novel user interactions not covered by the user\u2019s historical index. This paper introduces our Collaborative Query Rewriting approach, which utilizes underlying topological information to assist in rewriting defective queries arising from unseen user interactions. This approach begins by constructing a \u201cUser Feedback Interaction Graph\u201d (FIG) using historical user-entity interactions. Subsequently, we traverse through the graph edges to establish an enhanced user index, referred to as the \u201ccollaborative user index\u201d. This paper then further explores the use of Large Language Models (LLMs) in conjunction with graph traversal, leading to a significant increase in index coverage for unseen interactions. The effectiveness of our proposed approach has been proven through experiments on a large-scale real-world dataset and online A/B experiments.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.75.pdf"
    },
    {
        "title": "DELPHI: Data for Evaluating LLMs\u2019 Performance in Handling Controversial Issues",
        "authors": [
            "David Sun",
            "Artem Abzaliev",
            "Hadas Kotek",
            "Christopher Klein",
            "Zidi Xiu",
            "Jason Williams"
        ],
        "published": "2023",
        "summary": "Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs\u2019 interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-industry.76.pdf"
    }
]