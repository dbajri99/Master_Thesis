[
    {
        "title": "Learning Natural Language Generation with Truncated Reinforcement Learning",
        "authors": [
            "Alice Martin",
            "Guillaume Quispe",
            "Charles Ollion",
            "Sylvain Le Corff",
            "Florian Strub",
            "Olivier Pietquin"
        ],
        "published": "2022",
        "summary": "This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original approach to train conditional languagemodels without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pretrained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual question generation tasks, for which we report positive results over performance and language metrics, which we then corroborate with a human evaluation. To our knowledge, it is the first approach that successfully learns a language generation policy without pre-training, using only reinforcement learning.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.2.pdf"
    },
    {
        "title": "Language Model Augmented Monotonic Attention for Simultaneous Translation",
        "authors": [
            "Sathish Reddy Indurthi",
            "Mohd Abbas Zaidi",
            "Beomseok Lee",
            "Nikhil Kumar Lakumarapu",
            "Sangha Kim"
        ],
        "published": "2022",
        "summary": "The state-of-the-art adaptive policies for Simultaneous Neural Machine Translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge. In this work, we propose a framework to aid monotonic attention with an external language model to improve its decisions. Experiments on MuST-C English-German and English-French speech-to-text translation tasks show the future information from the language model improves the state-of-the-art monotonic multi-head attention model further.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.3.pdf"
    },
    {
        "title": "Enhancing Self-Attention with Knowledge-Assisted Attention Maps",
        "authors": [
            "Jiangang Bai",
            "Yujing Wang",
            "Hong Sun",
            "Ruonan Wu",
            "Tianmeng Yang",
            "Pengfei Tang",
            "Defu Cao",
            "Mingliang Zhang1",
            "Yunhai Tong",
            "Yaming Yang",
            "Jing Bai",
            "Ruofei Zhang",
            "Hao Sun",
            "Wei Shen"
        ],
        "published": "2022",
        "summary": "Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.8.pdf"
    },
    {
        "title": "Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia",
        "authors": [
            "Samee Ibraheem",
            "Gaoyue Zhou",
            "John DeNero"
        ],
        "published": "2022",
        "summary": "While neural networks demonstrate a remarkable ability to model linguistic content, capturing contextual information related to a speaker\u2019s conversational role is an open area of research. In this work, we analyze the effect of speaker role on language use through the game of Mafia, in which participants are assigned either an honest or a deceptive role. In addition to building a framework to collect a dataset of Mafia game records, we demonstrate that there are differences in the language produced by players with different roles. We confirm that classification models are able to rank deceptive players as more suspicious than honest ones based only on their use of language. Furthermore, we show that training models on two auxiliary tasks outperforms a standard BERT-based text classification approach. We also present methods for using our trained models to identify features that distinguish between player roles, which could be used to assist players during the Mafia game.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.11.pdf"
    },
    {
        "title": "Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation",
        "authors": [
            "Yu Li",
            "Baolin Peng",
            "Yelong Shen",
            "Yi Mao",
            "Lars Liden",
            "Zhou Yu",
            "Jianfeng Gao"
        ],
        "published": "2022",
        "summary": "Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.15.pdf"
    },
    {
        "title": "SwahBERT: Language Model of Swahili",
        "authors": [
            "Gati Martin",
            "Medard Edmund Mswahili",
            "Young-Seob Jeong",
            "Jiyoung Woo"
        ],
        "published": "2022",
        "summary": "The rapid development of social networks, electronic commerce, mobile Internet, and other technologies, has influenced the growth of Web data. Social media and Internet forums are valuable sources of citizens\u2019 opinions, which can be analyzed for community development and user behavior analysis. Unfortunately, the scarcity of resources (i.e., datasets or language models) become a barrier to the development of natural language processing applications in low-resource languages. Thanks to the recent growth of online forums and news platforms of Swahili, we introduce two datasets of Swahili in this paper: a pre-training dataset of approximately 105MB with 16M words and annotated dataset of 13K instances for the emotion classification task. The emotion classification dataset is manually annotated by two native Swahili speakers. We pre-trained a new monolingual language model for Swahili, namely SwahBERT, using our collected pre-training data, and tested it with four downstream tasks including emotion classification. We found that SwahBERT outperforms multilingual BERT, a well-known existing language model, in almost all downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.23.pdf"
    },
    {
        "title": "Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding",
        "authors": [
            "Hao Huang",
            "Xiubo Geng",
            "Guodong Long",
            "Daxin Jiang"
        ],
        "published": "2022",
        "summary": "This work studies temporal reading comprehension (TRC), which reads a free-text passage and answers temporal ordering questions. Precise question understanding is critical for temporal reading comprehension. For example, the question \u201cWhat happened before the victory\u201d and \u201cWhat happened after the victory\u201d share almost all words except one, while their answers are totally different. Moreover, even if two questions query about similar temporal relations, different varieties might also lead to various answers. For example, although both the question \u201cWhat usually happened during the press release?\u201d and \u201cWhat might happen during the press release\u201d query events which happen after \u201cthe press release\u201d, they convey divergent semantics. To this end, we propose a novel reading comprehension approach with precise question understanding. Specifically, a temporal ordering question is embedded into two vectors to capture the referred event and the temporal relation. Then we evaluate the temporal relation between candidate events and the referred event based on that. Such fine-grained representations offer two benefits. First, it enables a better understanding of the question by focusing on different elements of a question. Second, it provides good interpretability when evaluating temporal relations. Furthermore, we also harness an auxiliary contrastive loss for representation learning of temporal relations, which aims to distinguish relations with subtle but critical changes. The proposed approach outperforms strong baselines and achieves state-of-the-art performance on the TORQUE dataset. It also increases the accuracy of four pre-trained language models (BERT base, BERT large, RoBERTa base, and RoBETRa large), demonstrating its generic effectiveness on divergent models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.28.pdf"
    },
    {
        "title": "Machine-in-the-Loop Rewriting for Creative Image Captioning",
        "authors": [
            "Vishakh Padmakumar",
            "He He"
        ],
        "published": "2022",
        "summary": "Machine-in-the-loop writing aims to build models that assist humans to accomplish their writing tasks more effectively. Prior work has found that providing users a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from users\u2019 intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user\u2019s original draft to introduce descriptive and figurative elements in the text. We evaluate the model on its ability to collaborate with humans on the task of creative image captioning. On a user study through Amazon Mechanical Turk, our model is rated to be more helpful by users than a baseline infilling language model. In addition, third-party evaluation shows that users write more descriptive and figurative captions when collaborating with our model compared to completing the task alone. However, the improvement is not uniform across user groups: the model is more helpful to skilled users, which risks widening the gap between skilled and novice users, highlighting a need for careful, user-centric evaluation of interactive systems.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.42.pdf"
    },
    {
        "title": "Reframing Human-AI Collaboration for Generating Free-Text Explanations",
        "authors": [
            "Sarah Wiegreffe",
            "Jack Hessel",
            "Swabha Swayamdipta",
            "Mark Riedl",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using human-written examples in a few-shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced explanations in existing datasets. Our human studies also show, however, that while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates binary acceptability judgments from humans in the loop. Despite the intrinsic subjectivity of acceptability judgments, we demonstrate that acceptability is partially correlated with various fine-grained attributes of explanations. Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.47.pdf"
    },
    {
        "title": "NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics",
        "authors": [
            "Ximing Lu",
            "Sean Welleck",
            "Peter West",
            "Liwei Jiang",
            "Jungo Kasai",
            "Daniel Khashabi",
            "Ronan Le Bras",
            "Lianhui Qin",
            "Youngjae Yu",
            "Rowan Zellers",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.57.pdf"
    },
    {
        "title": "Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection",
        "authors": [
            "Angelica Chen",
            "Vicky Zayats",
            "Daniel Walker",
            "Dirk Padfield"
        ],
        "published": "2022",
        "summary": "In modern interactive speech-based systems, speech is consumed and transcribed incrementally prior to having disfluencies removed. While this post-processing step is crucial for producing clean transcripts and high performance on downstream tasks (e.g. machine translation), most current state-of-the-art NLP models such as the Transformer operate non-incrementally, potentially causing unacceptable delays for the user. In this work we propose a streaming BERT-based sequence tagging model that, combined with a novel training objective, is capable of detecting disfluencies in real-time while balancing accuracy and latency. This is accomplished by training the model to decide whether to immediately output a prediction for the current input or to wait for further context, in essence learning to dynamically size the lookahead window. Our results demonstrate that our model produces comparably accurate predictions and does so sooner than our baselines, with lower flicker. Furthermore, the model attains state-of-the-art latency and stability scores when compared with recent work on incremental disfluency detection.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.60.pdf"
    },
    {
        "title": "Generating Repetitions with Appropriate Repeated Words",
        "authors": [
            "Toshiki Kawamoto",
            "Hidetaka Kamigaito",
            "Kotaro Funakoshi",
            "Manabu Okumura"
        ],
        "published": "2022",
        "summary": "A repetition is a response that repeats words in the previous speaker\u2019s utterance in a dialogue. Repetitions are essential in communication to build trust with others, as investigated in linguistic studies. In this work, we focus on repetition generation. To the best of our knowledge, this is the first neural approach to address repetition generation. We propose Weighted Label Smoothing, a smoothing method for explicitly learning which words to repeat during fine-tuning, and a repetition scoring method that can output more appropriate repetitions during decoding. We conducted automatic and human evaluations involving applying these methods to the pre-trained language model T5 for generating repetitions. The experimental results indicate that our methods outperformed baselines in both evaluations.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.62.pdf"
    },
    {
        "title": "Abstraction not Memory: BERT and the English Article System",
        "authors": [
            "Harish Tayyar Madabushi",
            "Dagmar Divjak",
            "Petar Milin"
        ],
        "published": "2022",
        "summary": "Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to humans at detecting the zero article, possibly because we insert them using rules that the deep neural model can easily pick up. More interestingly, we find that BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high but switches to agreeing more with the corpus as inter-annotator agreement drops. We contend that this alignment with annotators, despite being trained on the corpus, suggests that BERT is not memorising article use, but captures a high level generalisation of article use akin to human intuition.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.67.pdf"
    },
    {
        "title": "Provably Confidential Language Modelling",
        "authors": [
            "Xuandong Zhao",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "published": "2022",
        "summary": "Large language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter these privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and show that our method is able to provably prevent unintended memorization by randomizing parts of the training process. Moreover, we show that redaction with an approximately correct screening policy amplifies the confidentiality guarantee. We implement the method for both LSTM and GPT language models. Our experimental results show that the models trained by CRT obtain almost the same perplexity while preserving strong confidentiality.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.69.pdf"
    },
    {
        "title": "When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",
        "authors": [
            "Sebastian Schuster",
            "Tal Linzen"
        ],
        "published": "2022",
        "summary": "Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as \u2018a dog\u2019, frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, \u2018a dog\u2019 in \u2018Arthur doesn\u2019t own a dog\u2019 does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduce an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs. We use this evaluation suite for a fine-grained investigation of the entity tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.71.pdf"
    },
    {
        "title": "DocTime: A Document-level Temporal Dependency Graph Parser",
        "authors": [
            "Puneet Mathur",
            "Vlad Morariu",
            "Verena Kaynig-Fittkau",
            "Jiuxiang Gu",
            "Franck Dernoncourt",
            "Quan Tran",
            "Ani Nenkova",
            "Dinesh Manocha",
            "Rajiv Jain"
        ],
        "published": "2022",
        "summary": "We introduce DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependency graph. It outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies. This work also demonstrates how the TDG graph can be used to improve the downstream tasks of temporal questions answering and NLI by a relative 4-10% with a new framework that incorporates the temporal dependency graph into the self-attention layer of Transformer models (Time-transformer). Finally, we develop and evaluate on a new temporal dependency graph dataset for the domain of contractual documents, which has not been previously explored in this setting.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.73.pdf"
    },
    {
        "title": "Towards a Progression-Aware Autonomous Dialogue Agent",
        "authors": [
            "Abraham Sanders",
            "Tomek Strzalkowski",
            "Mei Si",
            "Albert Chang",
            "Deepanshu Dey",
            "Jonas Braasch",
            "Dakuo Wang"
        ],
        "published": "2022",
        "summary": "Recent advances in large-scale language modeling and generation have enabled the creation of dialogue agents that exhibit human-like responses in a wide range of conversational scenarios spanning a diverse set of tasks, from general chit-chat to focused goal-oriented discourse. While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of task success inherent therein. Thus, we propose a framework in which dialogue agents can evaluate the progression of a conversation toward or away from desired outcomes, and use this signal to inform planning for subsequent responses. Our framework is composed of three key elements: (1) the notion of a \u201cglobal\u201d dialogue state (GDS) space, (2) a task-specific progression function (PF) computed in terms of a conversation\u2019s trajectory through this space, and (3) a planning mechanism based on dialogue rollouts by which an agent may use progression signals to select its next response.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.87.pdf"
    },
    {
        "title": "Cross-Domain Detection of GPT-2-Generated Technical Text",
        "authors": [
            "Juan Diego Rodriguez",
            "Todd Hay",
            "David Gros",
            "Zain Shamsi",
            "Ravi Srinivasan"
        ],
        "published": "2022",
        "summary": "Machine-generated text presents a potential threat not only to the public sphere, but also to the scientific enterprise, whereby genuine research is undermined by convincing, synthetic text. In this paper we examine the problem of detecting GPT-2-generated technical research text. We first consider the realistic scenario where the defender does not have full information about the adversary\u2019s text generation pipeline, but is able to label small amounts of in-domain genuine and synthetic text in order to adapt to the target distribution. Even in the extreme scenario of adapting a physics-domain detector to a biomedical detector, we find that only a few hundred labels are sufficient for good performance. Finally, we show that paragraph-level detectors can be used to detect the tampering of full-length documents under a variety of threat models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.88.pdf"
    },
    {
        "title": "Context-Aware Abbreviation Expansion Using Large Language Models",
        "authors": [
            "Shanqing Cai",
            "Subhashini Venugopalan",
            "Katrin Tomanek",
            "Ajit Narayanan",
            "Meredith Morris",
            "Michael Brenner"
        ],
        "published": "2022",
        "summary": "Motivated by the need for accelerating text entry in augmentative and alternative communication (AAC) for people with severe motor impairments, we propose a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters. Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs). Through zero-shot, few-shot, and fine-tuning experiments on four public conversation datasets, we show that for replies to the initial turn of a dialog, an LLM with 64B parameters is able to exactly expand over 70% of phrases with abbreviation length up to 10, leading to an effective keystroke saving rate of up to about 77% on these exact expansions. Including a small amount of context in the form of a single conversation turn more than doubles abbreviation expansion accuracies compared to having no context, an effect that is more pronounced for longer phrases. Additionally, the robustness of models against typo noise can be enhanced through fine-tuning on noisy data.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.91.pdf"
    },
    {
        "title": "Sort by Structure: Language Model Ranking as Dependency Probing",
        "authors": [
            "Max M\u00fcller-Eberstein",
            "Rob van der Goot",
            "Barbara Plank"
        ],
        "published": "2022",
        "summary": "Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM\u2019s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM\u2014RemBERT\u2014and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.93.pdf"
    },
    {
        "title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models",
        "authors": [
            "Alexandra Chronopoulou",
            "Matthew Peters",
            "Jesse Dodge"
        ],
        "published": "2022",
        "summary": "The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.96.pdf"
    },
    {
        "title": "SKILL: Structured Knowledge Infusion for Large Language Models",
        "authors": [
            "Fedor Moiseev",
            "Zhe Dong",
            "Enrique Alfonseca",
            "Martin Jaggi"
        ],
        "published": "2022",
        "summary": "Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.113.pdf"
    },
    {
        "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation",
        "authors": [
            "Simiao Zuo",
            "Qingru Zhang",
            "Chen Liang",
            "Pengcheng He",
            "Tuo Zhao",
            "Weizhu Chen"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications. Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We propose MoEBERT, which uses a Mixture-of-Experts structure to increase model capacity and inference speed. We initialize MoEBERT by adapting the feed-forward neural networks in a pre-trained model into multiple experts. As such, representation power of the pre-trained model is largely retained. During inference, only one of the experts is activated, such that speed can be improved. We also propose a layer-wise distillation method to train MoEBERT. We validate the efficiency and efficacy of MoEBERT on natural language understanding and question answering tasks. Results show that the proposed method outperforms existing task-specific distillation algorithms. For example, our method outperforms previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is publicly available at https://github.com/SimiaoZuo/MoEBERT.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.116.pdf"
    },
    {
        "title": "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
        "authors": [
            "Pieter Delobelle",
            "Ewoenam Tokpo",
            "Toon Calders",
            "Bettina Berendt"
        ],
        "published": "2022",
        "summary": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify \u2018bias\u2019 and \u2018fairness\u2019 in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.122.pdf"
    },
    {
        "title": "Representation Learning for Conversational Data using Discourse Mutual Information Maximization",
        "authors": [
            "Bishal Santra",
            "Sumegh Roychowdhury",
            "Aishik Mandal",
            "Vasu Gurram",
            "Atharva Naik",
            "Manish Gupta",
            "Pawan Goyal"
        ],
        "published": "2022",
        "summary": "Although many pretrained models exist for text or images, there have been relatively fewer attempts to train representations specifically for dialog understanding. Prior works usually relied on finetuned representations based on generic text representation models like BERT or GPT-2. But such language modeling pretraining objectives do not take the structural information of conversational text into consideration. Although generative dialog models can learn structural features too, we argue that the structure-unaware word-by-word generation is not suitable for effective conversation modeling. We empirically demonstrate that such representations do not perform consistently across various dialog understanding tasks. Hence, we propose a structure-aware Mutual Information based loss-function DMI (Discourse Mutual Information) for training dialog-representation models, that additionally captures the inherent uncertainty in response prediction. Extensive evaluation on nine diverse dialog modeling tasks shows that our proposed DMI-based models outperform strong baselines by significant margins.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.124.pdf"
    },
    {
        "title": "ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model",
        "authors": [
            "Chuyun Deng",
            "Mingxuan Liu",
            "Yue Qin",
            "Jia Zhang",
            "Hai-Xin Duan",
            "Donghong Sun"
        ],
        "published": "2022",
        "summary": "Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms. However, existing word-level attack methods trap in a one-to-one attack pattern, i.e., only a single word can be modified in one transformation round, and they ignore the interactions between several consecutive words. In this paper, we propose ValCAT, a black-box attack framework that misleads the language model by applying variable-length contextualized transformations to the original text. Compared to word-level methods, ValCAT expands the basic units of perturbation from single words to spans composed of multiple consecutive words, enhancing the perturbation capability. Experiments show that our method outperforms state-of-the-art methods in terms of attack success rate, perplexity, and semantic similarity on several classification tasks and inference tasks. The comprehensive human evaluation demonstrates that ValCAT has a significant advantage in ensuring the fluency of the adversarial examples and achieves better semantic consistency. We release the code at https://github.com/linerxliner/ValCAT.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.125.pdf"
    },
    {
        "title": "TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages",
        "authors": [
            "Zihan Zhao",
            "Lu Chen",
            "Ruisheng Cao",
            "Hongshen Xu",
            "Xingyu Chen",
            "Kai Yu"
        ],
        "published": "2022",
        "summary": "Recently, the structural reading comprehension (SRC) task on web pages has attracted increasing research interests. Although previous SRC work has leveraged extra information such as HTML tags or XPaths, the informative topology of web pages is not effectively exploited. In this work, we propose a Topological Information Enhanced model (TIE), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that, TIE integrates Graph Attention Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological information of both logical structures and spatial structures. Experimental results demonstrate that our model outperforms strong baselines and achieves state-of-the-art performances on the web-based SRC benchmark WebSRC at the time of writing. The code of TIE will be publicly available at https://github.com/X-LANCE/TIE.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.132.pdf"
    },
    {
        "title": "KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation",
        "authors": [
            "Marzieh Tahaei",
            "Ella Charlaix",
            "Vahid Nia",
            "Ali Ghodsi",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2022",
        "summary": "The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT_BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7x and 21x outperforms state-of-the-art compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.154.pdf"
    },
    {
        "title": "Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models",
        "authors": [
            "Sanghwan Bae",
            "Donghyun Kwak",
            "Sungdong Kim",
            "Donghoon Ham",
            "Soyoung Kang",
            "Sang-Woo Lee",
            "Woomyoung Park"
        ],
        "published": "2022",
        "summary": "Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.155.pdf"
    },
    {
        "title": "Sentence-Level Resampling for Named Entity Recognition",
        "authors": [
            "Xiaochen Wang",
            "Yue Wang"
        ],
        "published": "2022",
        "summary": "As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve span-level macro F1-scores of the evaluated NER models on multiple corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.156.pdf"
    },
    {
        "title": "Locally Aggregated Feature Attribution on Natural Language Model Understanding",
        "authors": [
            "Sheng Zhang",
            "Jin Wang",
            "Haitao Jiang",
            "Rui Song"
        ],
        "published": "2022",
        "summary": "With the growing popularity of deep-learning models, model understanding becomes more important. Much effort has been devoted to demystify deep neural networks for better explainability. Some feature attribution methods have shown promising results in computer vision, especially the gradient-based methods where effectively smoothing the gradients with reference data is the key to a robust and faithful result. However, direct application of these gradient-based methods to NLP tasks is not trivial due to the fact that the input consists of discrete tokens and the \u201creference\u201d tokens are not explicitly defined. In this work, we propose Locally Aggregated Feature Attribution (LAFA), a novel gradient-based feature attribution method for NLP models. Instead of relying on obscure reference tokens, it smooths gradients by aggregating similar reference texts derived from language model embeddings. For evaluation purpose, we also design experiments on different NLP tasks including Entity Recognition and Sentiment Analysis on public datasets and key words detection on constructed Amazon catalogue dataset. The superior performance of the proposed method is demonstrated through experiments.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.159.pdf"
    },
    {
        "title": "A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony",
        "authors": [
            "Tulika Saha",
            "Saichethan Reddy",
            "Anindya Das",
            "Sriparna Saha",
            "Pushpak Bhattacharyya"
        ],
        "published": "2022",
        "summary": "Mental Health Disorders continue plaguing humans worldwide. Aggravating this situation is the severe shortage of qualified and competent mental health professionals (MHPs), which underlines the need for developing Virtual Assistants (VAs) that can assist MHPs. The data+ML for automation can come from platforms that allow visiting and posting messages in peer-to-peer anonymous manner for sharing their experiences (frequently stigmatized) and seeking support. In this paper, we propose a VA that can act as the first point of contact and comfort for mental health patients. We curate a dataset, Motivational VA: MotiVAte comprising of 7k dyadic conversations collected from a peer-to-peer support platform. The system employs two mechanisms: (i) Mental Illness Classification: an attention based BERT classifier that outputs the mental disorder category out of the 4 categories, viz., Major Depressive Disorder (MDD), Anxiety, Obsessive Compulsive Disorder (OCD) and Post-traumatic Stress Disorder (PTSD), based on the input ongoing dialog between the support seeker and the VA; and (ii) Mental Illness Conditioned Motivational Dialogue Generation (MI-MDG): a sentiment driven Reinforcement Learning (RL) based motivational response generator. The empirical evaluation demonstrates the system capability by way of outperforming several baselines.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.174.pdf"
    },
    {
        "title": "What do tokens know about their characters and how do they know it?",
        "authors": [
            "Ayush Kaushal",
            "Kyle Mahowald"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require character-level information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for \u201ccat\u201d encodes that it contains the character \u201ca\u201d). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.179.pdf"
    },
    {
        "title": "Data Augmentation with Dual Training for Offensive Span Detection",
        "authors": [
            "Nasim Nouri"
        ],
        "published": "2022",
        "summary": "Recognizing offensive text is an important requirement for every content management system, especially for social networks. While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. One of the challenges to train a model for this novel setting is the lack of enough training data. To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD. In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples. Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.185.pdf"
    },
    {
        "title": "Learning To Retrieve Prompts for In-Context Learning",
        "authors": [
            "Ohad Rubin",
            "Jonathan Herzig",
            "Jonathan Berant"
        ],
        "published": "2022",
        "summary": "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.191.pdf"
    },
    {
        "title": "Re2G: Retrieve, Rerank, Generate",
        "authors": [
            "Michael Glass",
            "Gaetano Rossiello",
            "Md Faisal Mahbub Chowdhury",
            "Ankita Naik",
            "Pengshan Cai",
            "Alfio Gliozzo"
        ],
        "published": "2022",
        "summary": "As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of research, proposing Re2G, which combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation. Our reranking approach also permits merging retrieval results from sources with incomparable scores, enabling an ensemble of BM25 and neural initial retrieval. To train our system end-to-end, we introduce a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output. We find large gains in four diverse tasks: zero-shot slot filling, question answering, fact checking and dialog, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make our code available as open source.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.194.pdf"
    },
    {
        "title": "MetaICL: Learning to Learn In Context",
        "authors": [
            "Sewon Min",
            "Mike Lewis",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022",
        "summary": "We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.201.pdf"
    },
    {
        "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers",
        "authors": [
            "Ninareh Mehrabi",
            "Ahmad Beirami",
            "Fred Morstatter",
            "Aram Galstyan"
        ],
        "published": "2022",
        "summary": "Warning: this paper contains content that maybe offensive or upsetting. Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss. In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language. We then propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow. Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy. Lastly, we establish the generalizability of such a defense mechanism on language generation models beyond conversational agents.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.204.pdf"
    },
    {
        "title": "Modal Dependency Parsing via Language Model Priming",
        "authors": [
            "Jiarui Yao",
            "Nianwen Xue",
            "Bonan Min"
        ],
        "published": "2022",
        "summary": "The task of modal dependency parsing aims to parse a text into its modal dependency structure, which is a representation for the factuality of events in the text. We design a modal dependency parser that is based on priming pre-trained language models, and evaluate the parser on two data sets. Compared to baselines, we show an improvement of 2.6% in F-score for English and 4.6% for Chinese. To the best of our knowledge, this is also the first work on Chinese modal dependency parsing.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.211.pdf"
    },
    {
        "title": "PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding",
        "authors": [
            "Antoine Chaffin",
            "Vincent Claveau",
            "Ewa Kijak"
        ],
        "published": "2022",
        "summary": "Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.215.pdf"
    },
    {
        "title": "Progressive Class Semantic Matching for Semi-supervised Text Classification",
        "authors": [
            "Haiming Xu",
            "Lingqiao Liu",
            "Ehsan Abbasnejad"
        ],
        "published": "2022",
        "summary": "Semi-supervised learning is a promising way to reduce the annotation cost for text-classification. Combining with pre-trained language models (PLMs), e.g., BERT, recent semi-supervised learning methods achieved impressive performance. In this work, we further investigate the marriage between semi-supervised learning and a pre-trained language model. Unlike existing approaches that utilize PLMs only for model parameter initialization, we explore the inherent topic matching capability inside PLMs for building a more powerful semi-supervised learning approach. Specifically, we propose a joint semi-supervised learning process that can progressively build a standard K-way classifier and a matching network for the input text and the Class Semantic Representation (CSR). The CSR will be initialized from the given labeled sentences and progressively updated through the training process. By means of extensive experiments, we show that our method can not only bring remarkable improvement to baselines, but also overall be more stable, and achieves state-of-the-art performance in semi-supervised text classification.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.219.pdf"
    },
    {
        "title": "Low Resource Style Transfer via Domain Adaptive Meta Learning",
        "authors": [
            "Xiangyang Li",
            "Xiang Long",
            "Yu Xia",
            "Sujian Li"
        ],
        "published": "2022",
        "summary": "Text style transfer (TST) without parallel data has achieved some practical success. However, most of the existing unsupervised text style transfer methods suffer from (i) requiring massive amounts of non-parallel data to guide transferring different text styles. (ii) colossal performance degradation when fine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain Adaptive Meta-Learning with Adversarial Transfer Model), which consists of two parts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn general knowledge in multiple heterogeneous source domains, capable of adapting to new unseen domains with a small amount of data. Moreover, we propose a new unsupervised TST approach Adversarial Transfer Model (ATM), composed of a sequence-to-sequence pre-trained language model and uses adversarial style training for better content preservation and style transfer. Results on multi-domain datasets demonstrate that our approach generalizes well on unseen low-resource domains, achieving state-of-the-art results against ten strong baselines.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.220.pdf"
    },
    {
        "title": "Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation",
        "authors": [
            "Shumpei Inoue",
            "Tsungwei Liu",
            "Son Nguyen",
            "Minh-Tien Nguyen"
        ],
        "published": "2022",
        "summary": "This paper introduces a model for incomplete utterance restoration (IUR) called JET (Joint learning token Extraction and Text generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we design two label creation methods (soft and hard labels), which can work in cases of no annotation data for the omitted tokens. The restoration is done by using a Generator with the help of the Picker on joint learning. Promising results on four benchmark datasets in extraction and abstraction scenarios show that our model is better than the pretrained T5 and non-generative language model methods in both rich and limited training data settings.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.229.pdf"
    },
    {
        "title": "Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption",
        "authors": [
            "Garam Lee",
            "Minsoo Kim",
            "Jai Hyun Park",
            "Seung-won Hwang",
            "Jung Hee Cheon"
        ],
        "published": "2022",
        "summary": "Embeddings, which compress information in raw text into semantics-preserving low-dimensional vectors, have been widely adopted for their efficacy. However, recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text. To address these growing privacy challenges, we propose a privatization mechanism for embeddings based on homomorphic encryption, to prevent potential leakage of any piece of information in the process of text classification. In particular, our method performs text classification on the encryption of embeddings from state-of-the-art models like BERT, supported by an efficient GPU implementation of CKKS encryption scheme. We show that our method offers encrypted protection of BERT embeddings, while largely preserving their utility on downstream text classification tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.231.pdf"
    },
    {
        "title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao"
        ],
        "published": "2022",
        "summary": "In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models\u2019 abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.234.pdf"
    },
    {
        "title": "Unsupervised Paraphrasability Prediction for Compound Nominalizations",
        "authors": [
            "John Sie Yuen Lee",
            "Ho Hung Lim",
            "Carol Webster"
        ],
        "published": "2022",
        "summary": "Commonly found in academic and formal texts, a nominalization uses a deverbal noun to describe an event associated with its corresponding verb. Nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Automatic generation of clausal paraphrases for nominalizations can help disambiguate their meaning. However, previous work has not identified cases where it is awkward or impossible to paraphrase a compound nominalization. This paper investigates unsupervised prediction of paraphrasability, which determines whether the prenominal modifier of a nominalization can be re-written as a noun or adverb in a clausal paraphrase. We adopt the approach of overgenerating candidate paraphrases followed by candidate ranking with a neural language model. In experiments on an English dataset, we show that features from an Abstract Meaning Representation graph lead to statistically significant improvement in both paraphrasability prediction and paraphrase generation.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.237.pdf"
    },
    {
        "title": "Global Entity Disambiguation with BERT",
        "authors": [
            "Ikuya Yamada",
            "Koki Washio",
            "Hiroyuki Shindo",
            "Yuji Matsumoto"
        ],
        "published": "2022",
        "summary": "We propose a global entity disambiguation (ED) model based on BERT. To capture global contextual information for ED, our model treats not only words but also entities as input tokens, and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step. We train the model using a large entity-annotated corpus obtained from Wikipedia. We achieve new state-of-the-art results on five standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The source code and model checkpoint are available at https://github.com/studio-ousia/luke.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.238.pdf"
    },
    {
        "title": "Towards Efficient NLP: A Standard Evaluation and A Strong Baseline",
        "authors": [
            "Xiangyang Liu",
            "Tianxiang Sun",
            "Junliang He",
            "Jiawen Wu",
            "Lingling Wu",
            "Xinyu Zhang",
            "Hao Jiang",
            "Zhao Cao",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2022",
        "summary": "Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.240.pdf"
    },
    {
        "title": "On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation",
        "authors": [
            "Yongjie Wang",
            "Chuang Wang",
            "Ruobing Li",
            "Hui Lin"
        ],
        "published": "2022",
        "summary": "In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.249.pdf"
    },
    {
        "title": "Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation",
        "authors": [
            "Luyao Shi",
            "Tanveer Syeda-mahmood",
            "Tyler Baldwin"
        ],
        "published": "2022",
        "summary": "Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as ruled out findings, resulting in poor precision during retrieval. In this paper we combine clinical finding detection with supervised query match learning. Specifically, we use lexicon-driven concept detection to detect relevant findings in sentences. These findings are used as queries to train a Sentence-BERT (SBERT) model using triplet loss on matched and unmatched query-sentence pairs. We show that the proposed supervised training task remarkably improves the retrieval performance of SBERT. The trained model generalizes well to unseen queries and reports from different collections.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.253.pdf"
    },
    {
        "title": "Exposing the Limits of Video-Text Models through Contrast Sets",
        "authors": [
            "Jae Sung Park",
            "Sheng Shen",
            "Ali Farhadi",
            "Trevor Darrell",
            "Yejin Choi",
            "Anna Rohrbach"
        ],
        "published": "2022",
        "summary": "Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video to-text classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.261.pdf"
    },
    {
        "title": "When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer",
        "authors": [
            "Ameet Deshpande",
            "Partha Talukdar",
            "Karthik Narasimhan"
        ],
        "published": "2022",
        "summary": "While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., \ud835\udf0cs=0.94 on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.264.pdf"
    },
    {
        "title": "Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad Reza Namazi Rad"
        ],
        "published": "2022",
        "summary": "Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse topics than NTMs, achieving also higher efficiency and simplicity.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.285.pdf"
    },
    {
        "title": "Knowledge Inheritance for Pre-trained Language Models",
        "authors": [
            "Yujia Qin",
            "Yankai Lin",
            "Jing Yi",
            "Jiajie Zhang",
            "Xu Han",
            "Zhengyan Zhang",
            "Yusheng Su",
            "Zhiyuan Liu",
            "Peng Li",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named \u201cknowledge inheritance\u201d (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs\u2019 pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.288.pdf"
    },
    {
        "title": "WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models",
        "authors": [
            "Benjamin Minixhofer",
            "Fabian Paischer",
            "Navid Rekabsaz"
        ],
        "published": "2022",
        "summary": "Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method \u2013 called WECHSEL \u2013 to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.293.pdf"
    },
    {
        "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings",
        "authors": [
            "Yung-Sung Chuang",
            "Rumen Dangovski",
            "Hongyin Luo",
            "Yang Zhang",
            "Shiyu Chang",
            "Marin Soljacic",
            "Shang-Wen Li",
            "Scott Yih",
            "Yoon Kim",
            "James Glass"
        ],
        "published": "2022",
        "summary": "We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning, which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other \u201charmful\u201d types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.311.pdf"
    },
    {
        "title": "A Data Cartography based MixUp for Pre-trained Language Models",
        "authors": [
            "Seo Yeon Park",
            "Cornelia Caragea"
        ],
        "published": "2022",
        "summary": "MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al., 2020) to identify the characteristics of training samples (e.g., as easy-to-learn or ambiguous samples), and then interpolates these characterized samples. We empirically validate that our method not only achieves competitive performance using a smaller subset of the training data compared with strong baselines, but also yields lower expected calibration error on the pre-trained language model, BERT, on both in-domain and out-of-domain settings in a wide range of NLP tasks. We publicly release our code.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.314.pdf"
    },
    {
        "title": "FNet: Mixing Tokens with Fourier Transforms",
        "authors": [
            "James Lee-Thorp",
            "Joshua Ainslie",
            "Ilya Eckstein",
            "Santiago Ontanon"
        ],
        "published": "2022",
        "summary": "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.319.pdf"
    },
    {
        "title": "Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling",
        "authors": [
            "Jakob Prange",
            "Nathan Schneider",
            "Lingpeng Kong"
        ],
        "published": "2022",
        "summary": "We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance\u2014outpacing syntactic constituency structures as well as syntactic and semantic dependency structures.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.325.pdf"
    },
    {
        "title": "Show, Don\u2019t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",
        "authors": [
            "Raghav Gupta",
            "Harrison Lee",
            "Jeffrey Zhao",
            "Yuan Cao",
            "Abhinav Rastogi",
            "Yonghui Wu"
        ],
        "published": "2022",
        "summary": "Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don\u2019t Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.336.pdf"
    },
    {
        "title": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge",
        "authors": [
            "Ian Porada",
            "Alessandro Sordoni",
            "Jackie Cheung"
        ],
        "published": "2022",
        "summary": "Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the pre-training minibatches of BERT and evaluate how well the model generalizes to supported inferences after pre-training on the injected knowledge. We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.337.pdf"
    },
    {
        "title": "Using Paraphrases to Study Properties of Contextual Embeddings",
        "authors": [
            "Laura Burdick",
            "Jonathan K. Kummerfeld",
            "Rada Mihalcea"
        ],
        "published": "2022",
        "summary": "We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database\u2019s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases. We confirm previous findings that BERT is sensitive to word order, but find slightly different patterns than prior work in terms of the level of contextualization across BERT\u2019s layers.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.338.pdf"
    },
    {
        "title": "Learning to Generate Examples for Semantic Processing Tasks",
        "authors": [
            "Danilo Croce",
            "Simone Filice",
            "Giuseppe Castellucci",
            "Roberto Basili"
        ],
        "published": "2022",
        "summary": "Even if recent Transformer-based architectures, such as BERT, achieved impressive results in semantic processing tasks, their fine-tuning stage still requires large scale training resources. Usually, Data Augmentation (DA) techniques can help to deal with low resource settings. In Text Classification tasks, the objective of DA is the generation of well-formed sentences that i) represent the desired task category and ii) are novel with respect to existing sentences. In this paper, we propose a neural approach to automatically learn to generate new examples using a pre-trained sequence-to-sequence model. We first learn a task-oriented similarity function that we use to pair similar examples. Then, we use these example pairs to train a model to generate examples. Experiments in low resource settings show that augmenting the training material with the proposed strategy systematically improves the results on text classification and natural language inference tasks by up to 10% accuracy, outperforming existing DA approaches.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.340.pdf"
    },
    {
        "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
        "authors": [
            "Peter West",
            "Chandra Bhagavatula",
            "Jack Hessel",
            "Jena Hwang",
            "Liwei Jiang",
            "Ronan Le Bras",
            "Ximing Lu",
            "Sean Welleck",
            "Yejin Choi"
        ],
        "published": "2022",
        "summary": "The common practice for training commonsense models has gone from\u2013human\u2013to\u2013corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.341.pdf"
    },
    {
        "title": "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks",
        "authors": [
            "Belinda Li",
            "Jane Yu",
            "Madian Khabsa",
            "Luke Zettlemoyer",
            "Alon Halevy",
            "Jacob Andreas"
        ],
        "published": "2022",
        "summary": "When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.346.pdf"
    },
    {
        "title": "A Study of the Attention Abnormality in Trojaned BERTs",
        "authors": [
            "Weimin Lyu",
            "Songzhu Zheng",
            "Tengfei Ma",
            "Chao Chen"
        ],
        "published": "2022",
        "summary": "Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based Trojan detector to distinguish Trojaned models from clean ones. To the best of our knowledge, we are the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer\u2019s attention.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.348.pdf"
    },
    {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "authors": [
            "Xisen Jin",
            "Dejiao Zhang",
            "Henghui Zhu",
            "Wei Xiao",
            "Shang-Wen Li",
            "Xiaokai Wei",
            "Andrew Arnold",
            "Xiang Ren"
        ],
        "published": "2022",
        "summary": "Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM\u2019s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.351.pdf"
    },
    {
        "title": "A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank",
        "authors": [
            "Dan Malkin",
            "Tomasz Limisiewicz",
            "Gabriel Stanovsky"
        ],
        "published": "2022",
        "summary": "We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zero-shot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential exhaustive computation of all possible combinations. We find that our method is effective on a diverse set of languages spanning different linguistic features and two downstream tasks. Our findings can inform developers of large-scale multilingual language models in choosing better pretraining configurations.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.361.pdf"
    },
    {
        "title": "SkillSpan: Hard and Soft Skill Extraction from English Job Postings",
        "authors": [
            "Mike Zhang",
            "Kristian Jensen",
            "Sif Sonniks",
            "Barbara Plank"
        ],
        "published": "2022",
        "summary": "Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.366.pdf"
    },
    {
        "title": "Modeling Multi-Granularity Hierarchical Features for Relation Extraction",
        "authors": [
            "Xinnian Liang",
            "Shuangzhi Wu",
            "Mu Li",
            "Zhoujun Li"
        ],
        "published": "2022",
        "summary": "Relation extraction is a key task in Natural Language Processing (NLP), which aims to extract relations between entity pairs from given texts. Recently, relation extraction (RE) has achieved remarkable progress with the development of deep neural networks. Most existing research focuses on constructing explicit structured features using external knowledge such as knowledge graph and dependency tree. In this paper, we propose a novel method to extract multi-granularity features based solely on the original input sentences. We show that effective structured features can be attained even without external knowledge. Three kinds of features based on the input sentences are fully exploited, which are in entity mention level, segment level, and sentence level. All the three are jointly and hierarchically modeled. We evaluate our method on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred Revisited. To verify the effectiveness, we apply our method to different encoders such as LSTM and BERT. Experimental results show that our method significantly outperforms existing state-of-the-art models that even use external knowledge. Extensive analyses demonstrate that the performance of our model is contributed by the capture of multi-granularity features and the model of their hierarchical structure.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.375.pdf"
    },
    {
        "title": "Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances",
        "authors": [
            "Seungju Han",
            "Beomsu Kim",
            "Jin Yong Yoo",
            "Seokjun Seo",
            "Sangbum Kim",
            "Enkhbayar Erdenee",
            "Buru Chang"
        ],
        "published": "2022",
        "summary": "In this paper, we consider mimicking fictional characters as a promising direction for building engaging conversation models. To this end, we present a new practical task where only a few utterances of each fictional character are available to generate responses mimicking them. Furthermore, we propose a new method named Pseudo Dialog Prompting (PDP) that generates responses by leveraging the power of large-scale language models with prompts containing the target character\u2019s utterances. To better reflect the style of the character, PDP builds the prompts in the form of dialog that includes the character\u2019s utterances as dialog history. Since only utterances of the characters are available in the proposed task, PDP matches each utterance with an appropriate pseudo-context from a predefined set of context candidates using a retrieval model. Through human and automatic evaluation, we show that PDP generates responses that better reflect the style of fictional characters than baseline methods.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.377.pdf"
    },
    {
        "title": "KALA: Knowledge-Augmented Language Model Adaptation",
        "authors": [
            "Minki Kang",
            "Jinheon Baek",
            "Sung Ju Hwang"
        ],
        "published": "2022",
        "summary": "Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM\u2019s performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.379.pdf"
    },
    {
        "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
        "authors": [
            "Seongjin Shin",
            "Sang-Woo Lee",
            "Hwijeen Ahn",
            "Sungdong Kim",
            "HyoungSeok Kim",
            "Boseop Kim",
            "Kyunghyun Cho",
            "Gichang Lee",
            "Woomyoung Park",
            "Jung-Woo Ha",
            "Nako Sung"
        ],
        "published": "2022",
        "summary": "Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.380.pdf"
    },
    {
        "title": "When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes",
        "authors": [
            "Mycal Tucker",
            "Tiwalayo Eisape",
            "Peng Qian",
            "Roger Levy",
            "Julie Shah"
        ],
        "published": "2022",
        "summary": "Recent causal probing literature reveals when language models and syntactic probes use similar representations. Such techniques may yield \u201cfalse negative\u201d causality results: models may use representations of syntax, but probes may have learned to use redundant encodings of the same syntactic information. We demonstrate that models do encode syntactic information redundantly and introduce a new probe design that guides probes to consider all syntactic information present in embeddings. Using these probes, we find evidence for the use of syntax in models where prior methods did not, allowing us to boost model performance by injecting syntactic information into representations.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.394.pdf"
    },
    {
        "title": "Few-Shot Semantic Parsing with Language Models Trained on Code",
        "authors": [
            "Richard Shin",
            "Benjamin Van Durme"
        ],
        "published": "2022",
        "summary": "Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.396.pdf"
    },
    {
        "title": "ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence",
        "authors": [
            "Yibo Hu",
            "MohammadSaleh Hosseini",
            "Erick Skorupa Parolin",
            "Javier Osorio",
            "Latifur Khan",
            "Patrick Brandt",
            "Vito D\u2019Orazio"
        ],
        "published": "2022",
        "summary": "Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build ConfliBERT using two approaches: pre-training from scratch and continual pre-training. To evaluate ConfliBERT, we collect 12 datasets and implement 18 tasks to assess the models\u2019 practical application in conflict research. Finally, we evaluate several versions of ConfliBERT in multiple experiments. Results consistently show that ConfliBERT outperforms BERT when analyzing political violence and conflict.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.400.pdf"
    },
    {
        "title": "Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification",
        "authors": [
            "Han Wang",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2022",
        "summary": "Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves competitive performance on the GLUE benchmark without human effort or external resources.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.401.pdf"
    },
    {
        "title": "Few-shot Subgoal Planning with Language Models",
        "authors": [
            "Lajanugen Logeswaran",
            "Yao Fu",
            "Moontae Lee",
            "Honglak Lee"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have shown successful progress in many text understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.402.pdf"
    },
    {
        "title": "IDPG: An Instance-Dependent Prompt Generation Method",
        "authors": [
            "Zhuofeng Wu",
            "Sinong Wang",
            "Jiatao Gu",
            "Rui Hou",
            "Yuxiao Dong",
            "V.G.Vinod Vydiswaran",
            "Hao Ma"
        ],
        "published": "2022",
        "summary": "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.403.pdf"
    },
    {
        "title": "Embedding Hallucination for Few-shot Language Fine-tuning",
        "authors": [
            "Yiren Jian",
            "Chongyang Gao",
            "Soroush Vosoughi"
        ],
        "published": "2022",
        "summary": "Few-shot language learners adapt knowledge from a pre-trained model to recognize novel classes from a few-labeled sentences. In such settings, fine-tuning a pre-trained language model can cause severe over-fitting. In this paper, we propose an Embedding Hallucination (EmbedHalluc) method, which generates auxiliary embedding-label pairs to expand the fine-tuning dataset. The hallucinator is trained by playing an adversarial game with the discriminator, such that the hallucinated embedding is indiscriminative to the real ones in the fine-tuning dataset. By training with the extended dataset, the language learner effectively learns from the diverse hallucinated embeddings to overcome the over-fitting issue. Experiments demonstrate that our proposed method is effective in a wide range of language tasks, outperforming current fine-tuning methods. Further, we show that EmbedHalluc outperforms other methods that address this over-fitting problem, such as common data augmentation, semi-supervised pseudo-labeling, and regularization.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.404.pdf"
    },
    {
        "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling",
        "authors": [
            "Suchin Gururangan",
            "Mike Lewis",
            "Ari Holtzman",
            "Noah A. Smith",
            "Luke Zettlemoyer"
        ],
        "published": "2022",
        "summary": "We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.407.pdf"
    },
    {
        "title": "Contrastive Learning for Prompt-based Few-shot Language Learners",
        "authors": [
            "Yiren Jian",
            "Chongyang Gao",
            "Soroush Vosoughi"
        ],
        "published": "2022",
        "summary": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \u201cviews\u201d and repel the ones from different classes. We create different \u201cviews\u201d of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.408.pdf"
    },
    {
        "title": "CoMPM: Context Modeling with Speaker\u2019s Pre-trained Memory Tracking for Emotion Recognition in Conversation",
        "authors": [
            "Joosung Lee",
            "Wooin Lee"
        ],
        "published": "2022",
        "summary": "As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker\u2019s pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods. Our code is available on github .",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.416.pdf"
    },
    {
        "title": "Template-free Prompt Tuning for Few-shot NER",
        "authors": [
            "Ruotian Ma",
            "Xin Zhou",
            "Tao Gui",
            "Yiding Tan",
            "Linyang Li",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2022",
        "summary": "Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding the complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot settings. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.420.pdf"
    },
    {
        "title": "Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training",
        "authors": [
            "Yuanxin Liu",
            "Fandong Meng",
            "Zheng Lin",
            "Peng Fu",
            "Yanan Cao",
            "Weiping Wang",
            "Jie Zhou"
        ],
        "published": "2022",
        "summary": "Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which correlates with the downstream transferability. Inspired by this, we propose to directly optimize the subnetwork structure towards the pre-training objectives, which can better preserve the pre-training performance. Specifically, we train binary masks over model weights on the pre-training tasks, with the aim of preserving the universal transferability of the subnetwork, which is agnostic to any specific downstream tasks. We then fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The results show that, compared with magnitude pruning, mask training can effectively find BERT subnetworks with improved overall performance on downstream tasks. Moreover, our method is also more efficient in searching subnetworks and more advantageous when fine-tuning within a certain range of data scarcity. Our code is available at https://github.com/llyx97/TAMT.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.428.pdf"
    },
    {
        "title": "You Don\u2019t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers\u2019 Private Personas",
        "authors": [
            "Haoran Li",
            "Yangqiu Song",
            "Lixin Fan"
        ],
        "published": "2022",
        "summary": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers\u2019 personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models\u2019 powerful generation ability.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.429.pdf"
    },
    {
        "title": "MOVER: Mask, Over-generate and Rank for Hyperbole Generation",
        "authors": [
            "Yunxiang Zhang",
            "Xiaojun Wan"
        ],
        "published": "2022",
        "summary": "Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.440.pdf"
    },
    {
        "title": "Regularized Training of Nearest Neighbor Language Models",
        "authors": [
            "Jean-Francois Ton",
            "Walter Talbott",
            "Shuangfei Zhai",
            "Joshua M. Susskind"
        ],
        "published": "2022",
        "summary": "Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon kNN-LM (CITATION), which uses a pre-trained language model together with an exhaustive kNN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the kNN-LM performance by instead training a LM with the knowledge that we will be using a kNN post-hoc. We achieved significant improvement using our method on language modeling tasks on WIKI-2 and WIKI-103. The main phenomenon that we encounter is that adding a simple L2 regularization on the activations (not weights) of the model, a transformer, improves the post-hoc kNN classification performance. We explore some possible reasons for this improvement. In particular, we find that the added L2 regularization seems to improve the performance for high-frequency words without deteriorating the performance for low frequency ones.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.4.pdf"
    },
    {
        "title": "Methods for Estimating and Improving Robustness of Language Models",
        "authors": [
            "Michal Stefanik"
        ],
        "published": "2022",
        "summary": "Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives leads to enhanced distributional robustness of neural models. Based on these findings, we present future research directions enhancing the robustness of LLMs.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.6.pdf"
    },
    {
        "title": "Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation",
        "authors": [
            "Xiruo Ding",
            "Kevin Lybarger",
            "Justin Tauscher",
            "Trevor Cohen"
        ],
        "published": "2022",
        "summary": "Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we address this sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The first approach includes Easy Data Augmentation, back translation, and mixup techniques. The second approach utilizes a domain-specific pretrained language model, MentalBERT. To examine the viability of different data augmentation methods, we utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking. We found that with optimized parameter settings, mixup was helpful for rare classes. Performance improvements with an augmented model, MentalBERT, exceed those obtained with data augmentation.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.9.pdf"
    },
    {
        "title": "Impact of Training Instance Selection on Domain-Specific Entity Extraction using BERT",
        "authors": [
            "Eileen Salhofer",
            "Xing Lan Liu",
            "Roman Kern"
        ],
        "published": "2022",
        "summary": "State of the art performances for entity extraction tasks are achieved by supervised learning, specifically, by fine-tuning pretrained language models such as BERT. As a result, annotating application specific data is the first step in many use cases. However, no practical guidelines are available for annotation requirements. This work supports practitioners by empirically answering the frequently asked questions (1) how many training samples to annotate? (2) which examples to annotate? We found that BERT achieves up to 80% F1 when fine-tuned on only 70 training examples, especially on biomedical domain. The key features for guiding the selection of high performing training instances are identified to be pseudo-perplexity and sentence-length. The best training dataset constructed using our proposed selection strategy shows F1 score that is equivalent to a random selection with twice the sample size. The requirement of only a small number of training data implies cheaper implementations and opens door to wider range of applications.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.11.pdf"
    },
    {
        "title": "Building a Personalized Dialogue System with Prompt-Tuning",
        "authors": [
            "Tomohito Kasahara",
            "Daisuke Kawahara",
            "Nguyen Tung",
            "Shengzhe Li",
            "Kenta Shinzato",
            "Toshinori Sato"
        ],
        "published": "2022",
        "summary": "Dialogue systems without consistent responses are not attractive. In this study, we build a dialogue system that can respond based on a given character setting (persona) to bring consistency. Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models. The results of the automatic and manual evaluations in English and Japanese show that it is possible to build a dialogue system with more natural and personalized responses with less computational resources than fine-tuning.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.13.pdf"
    },
    {
        "title": "Zuo Zhuan Ancient Chinese Dataset for Word Sense Disambiguation",
        "authors": [
            "Xiaomeng Pan",
            "Hongfei Wang",
            "Teruaki Oka",
            "Mamoru Komachi"
        ],
        "published": "2022",
        "summary": "Word Sense Disambiguation (WSD) is a core task in Natural Language Processing (NLP). Ancient Chinese has rarely been used in WSD tasks, however, as no public dataset for ancient Chinese WSD tasks exists. Creation of an ancient Chinese dataset is considered a significant challenge because determining the most appropriate sense in a context is difficult and time-consuming owing to the different usages in ancient and modern Chinese. Actually, no public dataset for ancient Chinese WSD tasks exists. To solve the problem of ancient Chinese WSD, we annotate part of Pre-Qin (221 BC) text Zuo Zhuan using a copyright-free dictionary to create a public sense-tagged dataset. Then, we apply a simple Nearest Neighbors (k-NN) method using a pre-trained language model to the dataset. Our code and dataset will be available on GitHub.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.17.pdf"
    },
    {
        "title": "How do people talk about images? A study on open-domain conversations with images.",
        "authors": [
            "Yi-Pei Chen",
            "Nobuyuki Shimizu",
            "Takashi Miyazaki",
            "Hideki Nakayama"
        ],
        "published": "2022",
        "summary": "This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, ImageChat. We examined the conversations with images from the perspectives of image relevancy and image information. We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable. After inspecting the causes, we suggested that understanding the overall scenario of image and connecting objects based on their high-level attributes might be very helpful to generate more engaging open-domain conversations when an image is presented. We proposed enriching the image information with image caption and object tags based on our analysis. With our proposed image+ features, we improved automatic metrics including BLEU and Bert Score, and increased the diversity and image-relevancy of generated responses to the strong baseline. The result verifies that our analysis provides valuable insights and could facilitate future research on open-domain conversations with images.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.20.pdf"
    },
    {
        "title": "Probe-Less Probing of BERT\u2019s Layer-Wise Linguistic Knowledge with Masked Word Prediction",
        "authors": [
            "Tatsuya Aoyama",
            "Nathan Schneider"
        ],
        "published": "2022",
        "summary": "The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT\u2019s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of likely potential candidates of the masked word prediction, we also investigate how the results differ for tokens within multiword expressions.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.25.pdf"
    },
    {
        "title": "Multimodal large language models for inclusive collaboration learning tasks",
        "authors": [
            "Armanda Lewis"
        ],
        "published": "2022",
        "summary": "This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.26.pdf"
    },
    {
        "title": "Automating Human Evaluation of Dialogue Systems",
        "authors": [
            "Sujan Reddy A"
        ],
        "published": "2022",
        "summary": "Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent, and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.29.pdf"
    },
    {
        "title": "Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations",
        "authors": [
            "Antoine Simoulin",
            "Benoit Crabb\u00e9"
        ],
        "published": "2022",
        "summary": "We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with Bert base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performances. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.33.pdf"
    },
    {
        "title": "Exploring the Effect of Dialect Mismatched Language Models in Telugu Automatic Speech Recognition",
        "authors": [
            "Aditya Yadavalli",
            "Ganesh Sai Mirishkar",
            "Anil Vuppala"
        ],
        "published": "2022",
        "summary": "Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions. Therefore, this degradation has an adverse effect on the ASR even when dialect-specific AM is used. We show a degradation of up to 13.13 perplexity points when LM is used under mismatched conditions. Furthermore, we show a degradation of over 9% and over 15% in Character Error Rate (CER) and Word Error Rate (WER), respectively, in the ASR systems when using mismatched LMs over matched LMs.",
        "pdf_link": "https://aclanthology.org/2022.naacl-srw.36.pdf"
    },
    {
        "title": "Towards Open-Domain Topic Classification",
        "authors": [
            "Hantian Ding",
            "Jinrui Yang",
            "Yuqian Deng",
            "Hongming Zhang",
            "Dan Roth"
        ],
        "published": "2022",
        "summary": "We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weakly-supervised models trained on in-domain data.",
        "pdf_link": "https://aclanthology.org/2022.naacl-demo.10.pdf"
    },
    {
        "title": "FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction",
        "authors": [
            "Minh Van Nguyen",
            "Nghia Ngo",
            "Bonan Min",
            "Thien Nguyen"
        ],
        "published": "2022",
        "summary": "This paper presents FAMIE, a comprehensive and efficient active learning (AL) toolkit for multilingual information extraction. FAMIE is designed to address a fundamental problem in existing AL frameworks where annotators need to wait for a long time between annotation batches due to the time-consuming nature of model training and data selection at each AL iteration. This hinders the engagement, productivity, and efficiency of annotators. Based on the idea of using a small proxy network for fast data selection, we introduce a novel knowledge distillation mechanism to synchronize the proxy network with the main large model (i.e., BERT-based) to ensure the appropriateness of the selected annotation examples for the main model. Our AL framework can support multiple languages. The experiments demonstrate the advantages of FAMIE in terms of competitive performance and time efficiency for sequence labeling with AL. We publicly release our code (https://github.com/nlp-uoregon/famie) and demo website (http://nlp.uoregon.edu:9000/). A demo video for FAMIE is provided at: https://youtu.be/I2i8n_jAyrY",
        "pdf_link": "https://aclanthology.org/2022.naacl-demo.14.pdf"
    },
    {
        "title": "Self-supervised Representation Learning for Speech Processing",
        "authors": [
            "Hung-yi Lee",
            "Abdelrahman Mohamed",
            "Shinji Watanabe",
            "Tara Sainath",
            "Karen Livescu",
            "Shang-Wen Li",
            "Shu-wen Yang",
            "Katrin Kirchhoff"
        ],
        "published": "2022",
        "summary": "There is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised representation learning (SSL) utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. Thus, SSL has the potential to scale up current machine learning technologies, especially for low-resourced, under-represented use cases, and democratize the technologies. Recently self-supervised approaches for speech processing are also gaining popularity. There are several workshops in relevant topics hosted at ICML 2020 (https://icml-sas.gitlab.io/), NeurIPS 2020 (https://neurips-sas-2020.github.io/), and AAAI 2022 (https://aaai-sas-2022.github.io/). However, there is no previous tutorial about a similar topic based on the authors\u2019 best knowledge. Due to the growing popularity of SSL, and the shared mission of the areas in bringing speech and language technologies to more use cases with better quality and scaling the technologies for under-represented languages, we propose this tutorial to systematically survey the latest SSL techniques, tools, datasets, and performance achievement in speech processing. The proposed tutorial is highly relevant to the special theme of ACL about language diversity. One of the main focuses of the tutorial is leveraging SSL to reduce the dependence of speech technologies on labeled data, and to scale up the technologies especially for under-represented languages and use cases.",
        "pdf_link": "https://aclanthology.org/2022.naacl-tutorials.2.pdf"
    },
    {
        "title": "An End-to-End Dialogue Summarization System for Sales Calls",
        "authors": [
            "Abedelkadir Asi",
            "Song Wang",
            "Roy Eisenstadt",
            "Dean Geckt",
            "Yarin Kuper",
            "Yi Mao",
            "Royi Ronen"
        ],
        "published": "2022",
        "summary": "Summarizing sales calls is a routine task performed manually by salespeople. We present a production system which combines generative models fine-tuned for customer-agent setting, with a human-in-the-loop user experience for an interactive summary curation process. We address challenging aspects of dialogue summarization task in a real-world setting including long input dialogues, content validation, lack of labeled data and quality evaluation. We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting. Experiments show significant improvements by our models in tackling the summarization and content validation tasks on public datasets.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.6.pdf"
    },
    {
        "title": "Self-supervised Product Title Rewrite for Product Listing Ads",
        "authors": [
            "Xue Zhao",
            "Dayiheng Liu",
            "Junwei Ding",
            "Liang Yao",
            "Mahone Yan",
            "Huibo Wang",
            "Wenqing Yao"
        ],
        "published": "2022",
        "summary": "Product Listing Ads (PLAs) are primary online advertisements merchants pay to attract more customers. However, merchants prefer to stack various attributes to the title and neglect the fluency and information priority. These seller-created titles are not suitable for PLAs as they fail to highlight the core information in the visible part in PLAs titles. In this work, we present a title rewrite solution. Specifically, we train a self-supervised language model to generate high-quality titles in terms of fluency and information priority. Extensive offline test and real-world online test have demonstrated that our solution is effective in reducing the cost and gaining more profit as it lowers our CPC, CPB while improving CTR in the online test by a large amount.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.10.pdf"
    },
    {
        "title": "Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning",
        "authors": [
            "Angelo Ziletti",
            "Alan Akbik",
            "Christoph Berns",
            "Thomas Herold",
            "Marion Legler",
            "Martina Viell"
        ],
        "published": "2022",
        "summary": "Medical coding (MC) is an essential pre-requisite for reliable data retrieval and reporting. Given a free-text reported term (RT) such as \u201cpain of right thigh to the knee\u201d, the task is to identify the matching lowest-level term (LLT) \u2013in this case \u201cunilateral leg pain\u201d\u2013 from a very large and continuously growing repository of standardized medical terms. However, automating this task is challenging due to a large number of LLT codes (as of writing over 80\\,000), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain.With this paper, we introduce the MC task, discuss its challenges, and present a novel approach called xTARS that combines traditional BERT-based classification with a recent zero/few-shot learning approach (TARS). We present extensive experiments that show that our combined approach outperforms strong baselines, especially in the few-shot regime. The approach is developed and deployed at Bayer, live since November 2021. As we believe our approach potentially promising beyond MC, and to ensure reproducibility, we release the code to the research community.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.21.pdf"
    },
    {
        "title": "Knowledge extraction from aeronautical messages (NOTAMs) with self-supervised language models for aircraft pilots",
        "authors": [
            "Alexandre Arnold",
            "Fares Ernez",
            "Catherine Kobus",
            "Marion-C\u00e9cile Martin"
        ],
        "published": "2022",
        "summary": "During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This self-supervised approach, where smaller amounts of labeled data are enough for task-specific fine-tuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models (by pilots, airlines or service providers), which is a first to the best of our knowledge.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.22.pdf"
    },
    {
        "title": "Lightweight Transformers for Conversational AI",
        "authors": [
            "Daniel Pressel",
            "Wenshuo Liu",
            "Michael Johnston",
            "Minhua Chen"
        ],
        "published": "2022",
        "summary": "To understand how training on conversational language impacts performance of pre-trained models on downstream dialogue tasks, we build compact Transformer-based Language Models from scratch on several large corpora of conversational data. We compare the performance and characteristics of these models against BERT and other strong baselines on dialogue probing tasks. Commercial dialogue systems typically require a small footprint and fast execution time, but recent trends are in the other direction, with an ever-increasing number of parameters, resulting in difficulties in model deployment. We focus instead on training fast, lightweight models that excel at natural language understanding (NLU) and can replace existing lower-capacity conversational AI models with similar size and speed. In the process, we develop a simple but unique curriculum-based approach that moves from general-purpose to dialogue-targeted both in terms of data and objective. Our resultant models have around 1/3 the number of parameters of BERT-base and produce better representations for a wide array of intent detection datasets using linear and Mutual-Information probing techniques. Additionally, the models can be easily fine-tuned on a single consumer GPU card and deployed in near real-time production environments.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.25.pdf"
    },
    {
        "title": "NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension",
        "authors": [
            "Anubhav Shrimal",
            "Avi Jain",
            "Kartik Mehta",
            "Promod Yenigalla"
        ],
        "published": "2022",
        "summary": "NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question per entity) are considered at the same time for a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC) architecture for this formulation. NER-MQMRC architecture considers all entities as input to BERT for learning token embeddings with self-attention and leverages BERT-based entity representation for further improving these token embeddings for NER task. Evaluation on three NER datasets show that our proposed architecture leads to average 2.5 times faster training and 2.3 times faster inference as compared to NER-SQMRC framework based models by considering all entities together in a single pass. Further, we show that our model performance does not degrade compared to single-question based MRC (NER-SQMRC) (Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for AE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this architecture primarily to solve large scale e-commerce attribute (or entity) extraction from unstructured text of a magnitude of 50k+ attributes to be extracted on a scalable production environment with high performance and optimised training and inference runtimes.",
        "pdf_link": "https://aclanthology.org/2022.naacl-industry.26.pdf"
    }
]