[
    {
        "title": "Language model compression with weighted low-rank factorization",
        "authors": [
            "Yen-Chang Hsu",
            "Ting Hua",
            "Sungen Chang",
            "Qian Lou",
            "Yilin Shen",
            "Hongxia Jin"
        ],
        "published": "2022-06-30T21:57:07Z",
        "summary": "Factorizing a large matrix into small matrices is a popular strategy for\nmodel compression. Singular value decomposition (SVD) plays a vital role in\nthis compression strategy, approximating a learned matrix with fewer\nparameters. However, SVD minimizes the squared error toward reconstructing the\noriginal matrix without gauging the importance of the parameters, potentially\ngiving a larger reconstruction error for those who affect the task accuracy\nmore. In other words, the optimization objective of SVD is not aligned with the\ntrained model's task accuracy. We analyze this previously unexplored problem,\nmake observations, and address it by introducing Fisher information to weigh\nthe importance of parameters affecting the model prediction. This idea leads to\nour method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from\nour approach do not result in smaller reconstruction errors, we find that our\nresulting task accuracy is much closer to the original model's performance. We\nperform analysis with the transformer-based language models, showing our\nweighted SVD largely alleviates the mismatched optimization objectives and can\nmaintain model performance with a higher compression rate. Our method can\ndirectly compress a task-specific model while achieving better performance than\nother compact model strategies requiring expensive model pre-training.\nMoreover, the evaluation of compressing an already compact model shows our\nmethod can further reduce 9% to 30% parameters with an insignificant impact on\ntask accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2207.00112v1.pdf"
    },
    {
        "title": "GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation",
        "authors": [
            "Mark Endo",
            "Kathleen L. Poston",
            "Edith V. Sullivan",
            "Li Fei-Fei",
            "Kilian M. Pohl",
            "Ehsan Adeli"
        ],
        "published": "2022-06-30T21:29:47Z",
        "summary": "Parkinson's disease (PD) is a neurological disorder that has a variety of\nobservable motor-related symptoms such as slow movement, tremor, muscular\nrigidity, and impaired posture. PD is typically diagnosed by evaluating the\nseverity of motor impairments according to scoring systems such as the Movement\nDisorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS).\nAutomated severity prediction using video recordings of individuals provides a\npromising route for non-intrusive monitoring of motor impairments. However, the\nlimited size of PD gait data hinders model ability and clinical potential.\nBecause of this clinical data scarcity and inspired by the recent advances in\nself-supervised large-scale language models like GPT-3, we use human motion\nforecasting as an effective self-supervised pre-training task for the\nestimation of motor impairment severity. We introduce GaitForeMer, Gait\nForecasting and impairment estimation transforMer, which is first pre-trained\non public datasets to forecast gait movements and then applied to clinical data\nto predict MDS-UPDRS gait impairment severity. Our method outperforms previous\napproaches that rely solely on clinical data by a large margin, achieving an F1\nscore of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we\nshow how public human movement data repositories can assist clinical use cases\nthrough learning universal motion representations. The code is available at\nhttps://github.com/markendo/GaitForeMer .",
        "pdf_link": "https://arxiv.org/pdf/2207.00106v1.pdf"
    },
    {
        "title": "Denoised MDPs: Learning World Models Better Than the World Itself",
        "authors": [
            "Tongzhou Wang",
            "Simon S. Du",
            "Antonio Torralba",
            "Phillip Isola",
            "Amy Zhang",
            "Yuandong Tian"
        ],
        "published": "2022-06-30T17:59:49Z",
        "summary": "The ability to separate signal from noise, and reason with clean\nabstractions, is critical to intelligence. With this ability, humans can\nefficiently perform real world tasks without considering all possible nuisance\nfactors.How can artificial agents do the same? What kind of information can\nagents safely discard as noises?\n  In this work, we categorize information out in the wild into four types based\non controllability and relation with reward, and formulate useful information\nas that which is both controllable and reward-relevant. This framework\nclarifies the kinds information removed by various prior work on representation\nlearning in reinforcement learning (RL), and leads to our proposed approach of\nlearning a Denoised MDP that explicitly factors out certain noise distractors.\nExtensive experiments on variants of DeepMind Control Suite and RoboDesk\ndemonstrate superior performance of our denoised world model over using raw\nobservations alone, and over prior works, across policy optimization control\ntasks as well as the non-control task of joint position regression.",
        "pdf_link": "https://arxiv.org/pdf/2206.15477v6.pdf"
    },
    {
        "title": "Forecasting Future World Events with Neural Networks",
        "authors": [
            "Andy Zou",
            "Tristan Xiao",
            "Ryan Jia",
            "Joe Kwon",
            "Mantas Mazeika",
            "Richard Li",
            "Dawn Song",
            "Jacob Steinhardt",
            "Owain Evans",
            "Dan Hendrycks"
        ],
        "published": "2022-06-30T17:59:14Z",
        "summary": "Forecasting future world events is a challenging but valuable task. Forecasts\nof climate, geopolitical conflict, pandemics and economic indicators help shape\npolicy and decision making. In these domains, the judgment of expert humans\ncontributes to the best forecasts. Given advances in language modeling, can\nthese forecasts be automated? To this end, we introduce Autocast, a dataset\ncontaining thousands of forecasting questions and an accompanying news corpus.\nQuestions are taken from forecasting tournaments, ensuring high quality,\nreal-world importance, and diversity. The news corpus is organized by date,\nallowing us to precisely simulate the conditions under which humans made past\nforecasts (avoiding leakage from the future). Motivated by the difficulty of\nforecasting numbers across orders of magnitude (e.g. global cases of COVID-19\nin 2022), we also curate IntervalQA, a dataset of numerical questions and\nmetrics for calibration. We test language models on our forecasting task and\nfind that performance is far below a human expert baseline. However,\nperformance improves with increased model size and incorporation of relevant\ninformation from the news corpus. In sum, Autocast poses a novel challenge for\nlarge language models and improved performance could bring large practical\nbenefits.",
        "pdf_link": "https://arxiv.org/pdf/2206.15474v2.pdf"
    },
    {
        "title": "Watch and Match: Supercharging Imitation with Regularized Optimal Transport",
        "authors": [
            "Siddhant Haldar",
            "Vaibhav Mathur",
            "Denis Yarats",
            "Lerrel Pinto"
        ],
        "published": "2022-06-30T17:58:18Z",
        "summary": "Imitation learning holds tremendous promise in learning policies efficiently\nfor complex decision making problems. Current state-of-the-art algorithms often\nuse inverse reinforcement learning (IRL), where given a set of expert\ndemonstrations, an agent alternatively infers a reward function and the\nassociated optimal policy. However, such IRL approaches often require\nsubstantial online interactions for complex control problems. In this work, we\npresent Regularized Optimal Transport (ROT), a new imitation learning algorithm\nthat builds on recent advances in optimal transport based trajectory-matching.\nOur key technical insight is that adaptively combining trajectory-matching\nrewards with behavior cloning can significantly accelerate imitation even with\nonly a few demonstrations. Our experiments on 20 visual control tasks across\nthe DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World\nBenchmark demonstrate an average of 7.8X faster imitation to reach 90% of\nexpert performance compared to prior state-of-the-art methods. On real-world\nrobotic manipulation, with just one demonstration and an hour of online\ntraining, ROT achieves an average success rate of 90.1% across 14 tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.15469v2.pdf"
    },
    {
        "title": "Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations",
        "authors": [
            "Ziyan Yang",
            "Kushal Kafle",
            "Franck Dernoncourt",
            "Vicente Ordonez"
        ],
        "published": "2022-06-30T17:55:12Z",
        "summary": "We propose a margin-based loss for tuning joint vision-language models so\nthat their gradient-based explanations are consistent with region-level\nannotations provided by humans for relatively smaller grounding datasets. We\nrefer to this objective as Attention Mask Consistency (AMC) and demonstrate\nthat it produces superior visual grounding results than previous methods that\nrely on using vision-language models to score the outputs of object detectors.\nParticularly, a model trained with AMC on top of standard vision-language\nmodeling objectives obtains a state-of-the-art accuracy of 86.49% in the\nFlickr30k visual grounding benchmark, an absolute improvement of 5.38% when\ncompared to the best previous model trained under the same level of\nsupervision. Our approach also performs exceedingly well on established\nbenchmarks for referring expression comprehension where it obtains 80.34%\naccuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC\nis effective, easy to implement, and is general as it can be adopted by any\nvision-language model, and can use any type of region annotations.",
        "pdf_link": "https://arxiv.org/pdf/2206.15462v4.pdf"
    },
    {
        "title": "Two-Stage Classifier for COVID-19 Misinformation Detection Using BERT: a Study on Indonesian Tweets",
        "authors": [
            "Douglas Raevan Faisal",
            "Rahmad Mahendra"
        ],
        "published": "2022-06-30T15:33:20Z",
        "summary": "The COVID-19 pandemic has caused globally significant impacts since the\nbeginning of 2020. This brought a lot of confusion to society, especially due\nto the spread of misinformation through social media. Although there were\nalready several studies related to the detection of misinformation in social\nmedia data, most studies focused on the English dataset. Research on COVID-19\nmisinformation detection in Indonesia is still scarce. Therefore, through this\nresearch, we collect and annotate datasets for Indonesian and build prediction\nmodels for detecting COVID-19 misinformation by considering the tweet's\nrelevance. The dataset construction is carried out by a team of annotators who\nlabeled the relevance and misinformation of the tweet data. In this study, we\npropose the two-stage classifier model using IndoBERT pre-trained language\nmodel for the Tweet misinformation detection task. We also experiment with\nseveral other baseline models for text classification. The experimental results\nshow that the combination of the BERT sequence classifier for relevance\nprediction and Bi-LSTM for misinformation detection outperformed other machine\nlearning models with an accuracy of 87.02%. Overall, the BERT utilization\ncontributes to the higher performance of most prediction models. We release a\nhigh-quality COVID-19 misinformation Tweet corpus in the Indonesian language,\nindicated by the high inter-annotator agreement.",
        "pdf_link": "https://arxiv.org/pdf/2206.15359v1.pdf"
    },
    {
        "title": "GitHub Copilot AI pair programmer: Asset or Liability?",
        "authors": [
            "Arghavan Moradi Dakhel",
            "Vahid Majdinasab",
            "Amin Nikanjam",
            "Foutse Khomh",
            "Michel C. Desmarais",
            "Zhen Ming",
            "Jiang"
        ],
        "published": "2022-06-30T15:00:03Z",
        "summary": "Automatic program synthesis is a long-lasting dream in software engineering.\nRecently, a promising Deep Learning (DL) based solution, called Copilot, has\nbeen proposed by OpenAI and Microsoft as an industrial product. Although some\nstudies evaluate the correctness of Copilot solutions and report its issues,\nmore empirical evaluations are necessary to understand how developers can\nbenefit from it effectively. In this paper, we study the capabilities of\nCopilot in two different programming tasks: (i) generating (and reproducing)\ncorrect and efficient solutions for fundamental algorithmic problems, and (ii)\ncomparing Copilot's proposed solutions with those of human programmers on a set\nof programming tasks. For the former, we assess the performance and\nfunctionality of Copilot in solving selected fundamental problems in computer\nscience, like sorting and implementing data structures. In the latter, a\ndataset of programming problems with human-provided solutions is used. The\nresults show that Copilot is capable of providing solutions for almost all\nfundamental algorithmic problems, however, some solutions are buggy and\nnon-reproducible. Moreover, Copilot has some difficulties in combining multiple\nmethods to generate a solution. Comparing Copilot to humans, our results show\nthat the correct ratio of humans' solutions is greater than Copilot's\nsuggestions, while the buggy solutions generated by Copilot require less effort\nto be repaired.",
        "pdf_link": "https://arxiv.org/pdf/2206.15331v2.pdf"
    },
    {
        "title": "The Topological BERT: Transforming Attention into Topology for Natural Language Processing",
        "authors": [
            "Ilan Perez",
            "Raphael Reinauer"
        ],
        "published": "2022-06-30T11:25:31Z",
        "summary": "In recent years, the introduction of the Transformer models sparked a\nrevolution in natural language processing (NLP). BERT was one of the first text\nencoders using only the attention mechanism without any recurrent parts to\nachieve state-of-the-art results on many NLP tasks.\n  This paper introduces a text classifier using topological data analysis. We\nuse BERT's attention maps transformed into attention graphs as the only input\nto that classifier. The model can solve tasks such as distinguishing spam from\nham messages, recognizing whether a sentence is grammatically correct, or\nevaluating a movie review as negative or positive. It performs comparably to\nthe BERT baseline and outperforms it on some tasks.\n  Additionally, we propose a new method to reduce the number of BERT's\nattention heads considered by the topological classifier, which allows us to\nprune the number of heads from 144 down to as few as ten with no reduction in\nperformance. Our work also shows that the topological model displays higher\nrobustness against adversarial attacks than the original BERT model, which is\nmaintained during the pruning process. To the best of our knowledge, this work\nis the first to confront topological-based models with adversarial attacks in\nthe context of NLP.",
        "pdf_link": "https://arxiv.org/pdf/2206.15195v1.pdf"
    },
    {
        "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing",
        "authors": [
            "Jason Alan Fries",
            "Leon Weber",
            "Natasha Seelam",
            "Gabriel Altay",
            "Debajyoti Datta",
            "Samuele Garda",
            "Myungsun Kang",
            "Ruisi Su",
            "Wojciech Kusa",
            "Samuel Cahyawijaya",
            "Fabio Barth",
            "Simon Ott",
            "Matthias Samwald",
            "Stephen Bach",
            "Stella Biderman",
            "Mario S\u00e4nger",
            "Bo Wang",
            "Alison Callahan",
            "Daniel Le\u00f3n Peri\u00f1\u00e1n",
            "Th\u00e9o Gigant",
            "Patrick Haller",
            "Jenny Chim",
            "Jose David Posada",
            "John Michael Giorgi",
            "Karthik Rangasai Sivaraman",
            "Marc P\u00e0mies",
            "Marianna Nezhurina",
            "Robert Martin",
            "Michael Cullan",
            "Moritz Freidank",
            "Nathan Dahlberg",
            "Shubhanshu Mishra",
            "Shamik Bose",
            "Nicholas Michio Broad",
            "Yanis Labrak",
            "Shlok S Deshmukh",
            "Sid Kiblawi",
            "Ayush Singh",
            "Minh Chien Vu",
            "Trishala Neeraj",
            "Jonas Golde",
            "Albert Villanova del Moral",
            "Benjamin Beilharz"
        ],
        "published": "2022-06-30T07:15:45Z",
        "summary": "Training and evaluating language models increasingly requires the\nconstruction of meta-datasets --diverse collections of curated data with clear\nprovenance. Natural language prompting has recently lead to improved zero-shot\ngeneralization by transforming existing, supervised datasets into a diversity\nof novel pretraining tasks, highlighting the benefits of meta-dataset curation.\nWhile successful in general-domain text, translating these data-centric\napproaches to biomedical language modeling remains challenging, as labeled\nbiomedical datasets are significantly underrepresented in popular data hubs. To\naddress this challenge, we introduce BigBIO a community library of 126+\nbiomedical NLP datasets, currently covering 12 task categories and 10+\nlanguages. BigBIO facilitates reproducible meta-dataset curation via\nprogrammatic access to datasets and their metadata, and is compatible with\ncurrent platforms for prompt engineering and end-to-end few/zero shot language\nmodel evaluation. We discuss our process for task schema harmonization, data\nauditing, contribution guidelines, and outline two illustrative use cases:\nzero-shot evaluation of biomedical prompts and large-scale, multi-task\nlearning. BigBIO is an ongoing community effort and is available at\nhttps://github.com/bigscience-workshop/biomedical",
        "pdf_link": "https://arxiv.org/pdf/2206.15076v1.pdf"
    },
    {
        "title": "Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding",
        "authors": [
            "Connor Holmes",
            "Minjia Zhang",
            "Yuxiong He",
            "Bo Wu"
        ],
        "published": "2022-06-30T04:33:50Z",
        "summary": "In recent years, large pre-trained Transformer networks have demonstrated\ndramatic improvements in many natural language understanding tasks. However,\nthe huge size of these models brings significant challenges to their\nfine-tuning and online deployment due to latency and cost constraints. New\nhardware supporting both N:M semi-structured sparsity and low-precision integer\ncomputation is a promising solution to boost DNN model serving efficiency.\nHowever, there have been very few studies that systematically investigate to\nwhat extent pre-trained Transformer networks benefit from the combination of\nthese techniques, as well as how to best compress each component of the\nTransformer. We propose a flexible compression framework NxMiFormer that\nperforms simultaneous sparsification and quantization using ADMM and STE-based\nQAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm\nthat identifies promising heterogeneous compression configurations that meet a\ncompression ratio constraint. When evaluated across the GLUE suite of NLU\nbenchmarks, our approach can achieve up to 93% compression of the encoders of a\nBERT model while retaining 98.2% of the original model accuracy and taking full\nadvantage of the hardware's capabilities. Heterogeneous configurations found\nthe by the search heuristic maintain 99.5% of the baseline accuracy while still\ncompressing the model by 87.5%.",
        "pdf_link": "https://arxiv.org/pdf/2206.15014v1.pdf"
    },
    {
        "title": "\"Diversity and Uncertainty in Moderation\" are the Key to Data Selection for Multilingual Few-shot Transfer",
        "authors": [
            "Shanu Kumar",
            "Sandipan Dandapat",
            "Monojit Choudhury"
        ],
        "published": "2022-06-30T04:22:27Z",
        "summary": "Few-shot transfer often shows substantial gain over zero-shot\ntransfer~\\cite{lauscher2020zero}, which is a practically useful trade-off\nbetween fully supervised and unsupervised learning approaches for multilingual\npretrained model-based systems. This paper explores various strategies for\nselecting data for annotation that can result in a better few-shot transfer.\nThe proposed approaches rely on multiple measures such as data entropy using\n$n$-gram language model, predictive entropy, and gradient embedding. We propose\na loss embedding method for sequence labeling tasks, which induces diversity\nand uncertainty sampling similar to gradient embedding. The proposed data\nselection strategies are evaluated and compared for POS tagging, NER, and NLI\ntasks for up to 20 languages. Our experiments show that the gradient and loss\nembedding-based strategies consistently outperform random data selection\nbaselines, with gains varying with the initial performance of the zero-shot\ntransfer. Furthermore, the proposed method shows similar trends in improvement\neven when the model is fine-tuned using a lower proportion of the original\ntask-specific labeled training data for zero-shot transfer.",
        "pdf_link": "https://arxiv.org/pdf/2206.15010v1.pdf"
    },
    {
        "title": "GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language",
        "authors": [
            "Zhiying Zhu",
            "Weixin Liang",
            "James Zou"
        ],
        "published": "2022-06-30T04:06:26Z",
        "summary": "Helping end users comprehend the abstract distribution shifts can greatly\nfacilitate AI deployment. Motivated by this, we propose a novel task, dataset\nexplanation. Given two image data sets, dataset explanation aims to\nautomatically point out their dataset-level distribution shifts with natural\nlanguage. Current techniques for monitoring distribution shifts provide\ninadequate information to understand datasets with the goal of improving data\nquality. Therefore, we introduce GSCLIP, a training-free framework to solve the\ndataset explanation task. In GSCLIP, we propose the selector as the first\nquantitative evaluation method to identify explanations that are proper to\nsummarize dataset shifts. Furthermore, we leverage this selector to demonstrate\nthe superiority of a generator based on language model generation. Systematic\nevaluation on natural data shift verifies that GSCLIP, a combined system of a\nhybrid generator group and an efficient selector is not only easy-to-use but\nalso powerful for dataset explanation at scale.",
        "pdf_link": "https://arxiv.org/pdf/2206.15007v1.pdf"
    },
    {
        "title": "GPTs at Factify 2022: Prompt Aided Fact-Verification",
        "authors": [
            "Pawan Kumar Sahu",
            "Saksham Aggarwal",
            "Taneesh Gupta",
            "Gyanendra Das"
        ],
        "published": "2022-06-29T21:07:39Z",
        "summary": "One of the most pressing societal issues is the fight against false news. The\nfalse claims, as difficult as they are to expose, create a lot of damage. To\ntackle the problem, fact verification becomes crucial and thus has been a topic\nof interest among diverse research communities. Using only the textual form of\ndata we propose our solution to the problem and achieve competitive results\nwith other approaches. We present our solution based on two approaches - PLM\n(pre-trained language model) based method and Prompt based method. The\nPLM-based approach uses the traditional supervised learning, where the model is\ntrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,\nPrompt-based learning reflects the idea to design input to fit the model such\nthat the original objective may be re-framed as a problem of (masked) language\nmodeling. We may further stimulate the rich knowledge provided by PLMs to\nbetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Our\nexperiments showed that the proposed method performs better than just\nfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset and\na 7th position on the competition leader-board.",
        "pdf_link": "https://arxiv.org/pdf/2206.14913v1.pdf"
    },
    {
        "title": "Two-Stage COVID19 Classification Using BERT Features",
        "authors": [
            "Weijun Tan",
            "Qi Yao",
            "Jingfeng Liu"
        ],
        "published": "2022-06-29T19:01:37Z",
        "summary": "We propose an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages using double BERT feature extraction. In the first BERT feature\nextraction, A 3D-CNN is first used to extract CNN internal feature maps.\nInstead of using the global average pooling, a late BERT temporal pooing is\nused to aggregate the temporal information in these feature maps, followed by a\nclassification layer. This 3D-CNN-BERT classification network is first trained\non sampled fixed number of slice images from every original CT scan volume. In\nthe second stage, the 3D-CNN-BERT embedding features are extracted on all slice\nimages of every CT scan volume, and these features are averaged into a fixed\nnumber of segments. Then another BERT network is used to aggregate these\nmultiple features into a single feature followed by another classification\nlayer. The classification results of both stages are combined to generate final\noutputs. On the validation dataset, we achieve macro F1 score of 0.9164.",
        "pdf_link": "https://arxiv.org/pdf/2206.14861v1.pdf"
    },
    {
        "title": "Solving Quantitative Reasoning Problems with Language Models",
        "authors": [
            "Aitor Lewkowycz",
            "Anders Andreassen",
            "David Dohan",
            "Ethan Dyer",
            "Henryk Michalewski",
            "Vinay Ramasesh",
            "Ambrose Slone",
            "Cem Anil",
            "Imanol Schlag",
            "Theo Gutman-Solo",
            "Yuhuai Wu",
            "Behnam Neyshabur",
            "Guy Gur-Ari",
            "Vedant Misra"
        ],
        "published": "2022-06-29T18:54:49Z",
        "summary": "Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.",
        "pdf_link": "https://arxiv.org/pdf/2206.14858v2.pdf"
    },
    {
        "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
        "authors": [
            "Ke Hu",
            "Tara N. Sainath",
            "Yanzhang He",
            "Rohit Prabhavalkar",
            "Trevor Strohman",
            "Sepand Mavandadi",
            "Weiran Wang"
        ],
        "published": "2022-06-29T15:30:44Z",
        "summary": "Text-only and semi-supervised training based on audio-only data has gained\npopularity recently due to the wide availability of unlabeled text and speech\ndata. In this work, we propose incorporating text-only and semi-supervised\ntraining into an attention-based deliberation model. By incorporating text-only\ndata in training a bidirectional encoder representation from transformer (BERT)\nfor the deliberation text encoder, and large-scale text-to-speech and\naudio-only utterances using joint acoustic and text decoder (JATD) and\nsemi-supervised training, we achieved 4%-12% WER reduction for various tasks\ncompared to the baseline deliberation. Compared to a state-of-the-art language\nmodel (LM) rescoring method, the deliberation model reduces the Google Voice\nSearch WER by 11% relative. We show that the deliberation model also achieves a\npositive human side-by-side evaluation compared to the state-of-the-art LM\nrescorer with reasonable endpointer latencies.",
        "pdf_link": "https://arxiv.org/pdf/2206.14716v1.pdf"
    },
    {
        "title": "Towards a Data-Driven Requirements Engineering Approach: Automatic Analysis of User Reviews",
        "authors": [
            "Jialiang Wei",
            "Anne-Lise Courbis",
            "Thomas Lambolais",
            "Binbin Xu",
            "Pierre Louis Bernard",
            "G\u00e9rard Dray"
        ],
        "published": "2022-06-29T14:14:54Z",
        "summary": "We are concerned by Data Driven Requirements Engineering, and in particular\nthe consideration of user's reviews. These online reviews are a rich source of\ninformation for extracting new needs and improvement requests. In this work, we\nprovide an automated analysis using CamemBERT, which is a state-of-the-art\nlanguage model in French. We created a multi-label classification dataset of\n6000 user reviews from three applications in the Health & Fitness field. The\nresults are encouraging and suggest that it's possible to identify\nautomatically the reviews concerning requests for new features.\n  Dataset is available at:\nhttps://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset.",
        "pdf_link": "https://arxiv.org/pdf/2206.14669v1.pdf"
    },
    {
        "title": "Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody",
        "authors": [
            "Peter Makarov",
            "Ammar Abbas",
            "Mateusz \u0141ajszczak",
            "Arnaud Joly",
            "Sri Karlapati",
            "Alexis Moinet",
            "Thomas Drugman",
            "Penny Karanasou"
        ],
        "published": "2022-06-29T13:37:03Z",
        "summary": "Generating expressive and contextually appropriate prosody remains a\nchallenge for modern text-to-speech (TTS) systems. This is particularly evident\nfor long, multi-sentence inputs. In this paper, we examine simple extensions to\na Transformer-based FastSpeech-like system, with the goal of improving prosody\nfor multi-sentence TTS. We find that long context, powerful text features, and\ntraining on multi-speaker data all improve prosody. More interestingly, they\nresult in synergies. Long context disambiguates prosody, improves coherence,\nand plays to the strengths of Transformers. Fine-tuning word-level features\nfrom a powerful language model, such as BERT, appears to profit from more\ntraining data, readily available in a multi-speaker setting. We look into\nobjective metrics on pausing and pacing and perform thorough subjective\nevaluations for speech naturalness. Our main system, which incorporates all the\nextensions, achieves consistently strong results, including statistically\nsignificant improvements in speech naturalness over all its competitors.",
        "pdf_link": "https://arxiv.org/pdf/2206.14643v1.pdf"
    },
    {
        "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
        "authors": [
            "Jes\u00fas Andr\u00e9s-Ferrer",
            "Dario Albesano",
            "Puming Zhan",
            "Paul Vozila"
        ],
        "published": "2022-06-29T13:12:46Z",
        "summary": "End-2-end (E2E) models have become increasingly popular in some ASR tasks\nbecause of their performance and advantages. These E2E models directly\napproximate the posterior distribution of tokens given the acoustic inputs.\nConsequently, the E2E systems implicitly define a language model (LM) over the\noutput tokens, which makes the exploitation of independently trained language\nmodels less straightforward than in conventional ASR systems. This makes it\ndifficult to dynamically adapt E2E ASR system to contextual profiles for better\nrecognizing special words such as named entities. In this work, we propose a\ncontextual density ratio approach for both training a contextual aware E2E\nmodel and adapting the language model to named entities. We apply the\naforementioned technique to an E2E ASR system, which transcribes doctor and\npatient conversations, for better adapting the E2E system to the names in the\nconversations. Our proposed technique achieves a relative improvement of up to\n46.5% on the names over an E2E baseline without degrading the overall\nrecognition accuracy of the whole test set. Moreover, it also surpasses a\ncontextual shallow fusion baseline by 22.1 % relative.",
        "pdf_link": "https://arxiv.org/pdf/2206.14623v1.pdf"
    },
    {
        "title": "Chinese Word Sense Embedding with SememeWSD and Synonym Set",
        "authors": [
            "Yangxi Zhou",
            "Junping Du",
            "Zhe Xue",
            "Ang Li",
            "Zeli Guan"
        ],
        "published": "2022-06-29T03:42:03Z",
        "summary": "Word embedding is a fundamental natural language processing task which can\nlearn feature of words. However, most word embedding methods assign only one\nvector to a word, even if polysemous words have multi-senses. To address this\nlimitation, we propose SememeWSD Synonym (SWSDS) model to assign a different\nvector to every sense of polysemous words with the help of word sense\ndisambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model,\nan unsupervised word sense disambiguation model based on OpenHowNet, to do word\nsense disambiguation and annotate the polysemous word with sense id. Then, we\nobtain top 10 synonyms of the word sense from OpenHowNet and calculate the\naverage vector of synonyms as the vector of the word sense. In experiments, We\nevaluate the SWSDS model on semantic similarity calculation with Gensim's\nwmdistance method. It achieves improvement of accuracy. We also examine the\nSememeWSD model on different BERT models to find the more effective model.",
        "pdf_link": "https://arxiv.org/pdf/2206.14388v1.pdf"
    },
    {
        "title": "Knowledge Distillation of Transformer-based Language Models Revisited",
        "authors": [
            "Chengqiang Lu",
            "Jianwei Zhang",
            "Yunfei Chu",
            "Zhengyu Chen",
            "Jingren Zhou",
            "Fei Wu",
            "Haiqing Chen",
            "Hongxia Yang"
        ],
        "published": "2022-06-29T02:16:56Z",
        "summary": "In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2206.14366v3.pdf"
    },
    {
        "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
        "authors": [
            "Pu Wang",
            "Hugo Van hamme"
        ],
        "published": "2022-06-28T23:08:32Z",
        "summary": "End-to-end spoken language understanding (SLU) systems benefit from\npretraining on large corpora, followed by fine-tuning on application-specific\ndata. The resulting models are too large for on-edge applications. For\ninstance, BERT-based systems contain over 110M parameters. Observing the model\nis overparameterized, we propose lean transformer structure where the dimension\nof the attention mechanism is automatically reduced using group sparsity. We\npropose a variant where the learned attention subspace is transferred to an\nattention bottleneck layer. In a low-resource setting and without pre-training,\nthe resulting compact SLU model achieves accuracies competitive with\npre-trained large models.",
        "pdf_link": "https://arxiv.org/pdf/2206.14318v1.pdf"
    },
    {
        "title": "Proton: Probing Schema Linking Information from Pre-trained Language Models for Text-to-SQL Parsing",
        "authors": [
            "Lihan Wang",
            "Bowen Qin",
            "Binyuan Hui",
            "Bowen Li",
            "Min Yang",
            "Bailin Wang",
            "Binhua Li",
            "Fei Huang",
            "Luo Si",
            "Yongbin Li"
        ],
        "published": "2022-06-28T14:05:25Z",
        "summary": "The importance of building text-to-SQL parsers which can be applied to new\ndatabases has long been acknowledged, and a critical step to achieve this goal\nis schema linking, i.e., properly recognizing mentions of unseen columns or\ntables when generating SQLs. In this work, we propose a novel framework to\nelicit relational structures from large-scale pre-trained language models\n(PLMs) via a probing procedure based on Poincar\\'e distance metric, and use the\ninduced relations to augment current graph-based parsers for better schema\nlinking. Compared with commonly-used rule-based methods for schema linking, we\nfound that probing relations can robustly capture semantic correspondences,\neven when surface forms of mentions and entities differ. Moreover, our probing\nprocedure is entirely unsupervised and requires no additional parameters.\nExtensive experiments show that our framework sets new state-of-the-art\nperformance on three benchmarks. We empirically verify that our probing\nprocedure can indeed find desired relational structures through qualitative\nanalysis. Our code can be found at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI.",
        "pdf_link": "https://arxiv.org/pdf/2206.14017v2.pdf"
    },
    {
        "title": "CC-Riddle: A Question Answering Dataset of Chinese Character Riddles",
        "authors": [
            "Fan Xu",
            "Yunxiang Zhang",
            "Xiaojun Wan"
        ],
        "published": "2022-06-28T06:23:13Z",
        "summary": "The Chinese character riddle is a unique form of cultural entertainment\nspecific to the Chinese language. It typically comprises two parts: the riddle\ndescription and the solution. The solution to the riddle is a single character,\nwhile the riddle description primarily describes the glyph of the solution,\noccasionally supplemented with its explanation and pronunciation. Solving\nChinese character riddles is a challenging task that demands understanding of\ncharacter glyph, general knowledge, and a grasp of figurative language. In this\npaper, we construct a \\textbf{C}hinese \\textbf{C}haracter riddle dataset named\nCC-Riddle, which covers the majority of common simplified Chinese characters.\nThe construction process is a combination of web crawling, language model\ngeneration and manual filtering. In generation stage, we input the Chinese\nphonetic alphabet, glyph and meaning of the solution character into the\ngeneration model, which then produces multiple riddle descriptions. The\ngenerated riddles are then manually filtered and the final CC-Riddle dataset is\ncomposed of both human-written riddles and these filtered, generated riddles.\nIn order to assess the performance of language models on the task of solving\ncharacter riddles, we use retrieval-based, generative and multiple-choice QA\nstrategies to test three language models: BERT, ChatGPT and ChatGLM. The test\nresults reveal that current language models still struggle to solve Chinese\ncharacter riddles. CC-Riddle is publicly available at\n\\url{https://github.com/pku0xff/CC-Riddle}.",
        "pdf_link": "https://arxiv.org/pdf/2206.13778v2.pdf"
    },
    {
        "title": "Exploring linguistic feature and model combination for speech recognition based automatic AD detection",
        "authors": [
            "Yi Wang",
            "Tianzi Wang",
            "Zi Ye",
            "Lingwei Meng",
            "Shoukang Hu",
            "Xixin Wu",
            "Xunying Liu",
            "Helen Meng"
        ],
        "published": "2022-06-28T05:09:01Z",
        "summary": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and delay progression. Speech based automatic AD screening\nsystems provide a non-intrusive and more scalable alternative to other clinical\nscreening techniques. Scarcity of such specialist data leads to uncertainty in\nboth model selection and feature learning when developing such systems. To this\nend, this paper investigates the use of feature and model combination\napproaches to improve the robustness of domain fine-tuning of BERT and Roberta\npre-trained text encoders on limited data, before the resulting embedding\nfeatures being fed into an ensemble of backend classifiers to produce the final\nAD detection decision via majority voting. Experiments conducted on the\nADReSS20 Challenge dataset suggest consistent performance improvements were\nobtained using model and feature combination in system development.\nState-of-the-art AD detection accuracies of 91.67 percent and 93.75 percent\nwere obtained using manual and ASR speech transcripts respectively on the\nADReSS20 test set consisting of 48 elderly speakers.",
        "pdf_link": "https://arxiv.org/pdf/2206.13758v2.pdf"
    },
    {
        "title": "Flexible text generation for counterfactual fairness probing",
        "authors": [
            "Zee Fryer",
            "Vera Axelrod",
            "Ben Packer",
            "Alex Beutel",
            "Jilin Chen",
            "Kellie Webster"
        ],
        "published": "2022-06-28T05:07:20Z",
        "summary": "A common approach for testing fairness issues in text-based classifiers is\nthrough the use of counterfactuals: does the classifier output change if a\nsensitive attribute in the input is changed? Existing counterfactual generation\nmethods typically rely on wordlists or templates, producing simple\ncounterfactuals that don't take into account grammar, context, or subtle\nsensitive attribute references, and could miss issues that the wordlist\ncreators had not considered. In this paper, we introduce a task for generating\ncounterfactuals that overcomes these shortcomings, and demonstrate how large\nlanguage models (LLMs) can be leveraged to make progress on this task. We show\nthat this LLM-based method can produce complex counterfactuals that existing\nmethods cannot, comparing the performance of various counterfactual generation\nmethods on the Civil Comments dataset and showing their value in evaluating a\ntoxicity classifier.",
        "pdf_link": "https://arxiv.org/pdf/2206.13757v1.pdf"
    },
    {
        "title": "Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible Products Prediction",
        "authors": [
            "Rongzhi Zhang",
            "Rebecca West",
            "Xiquan Cui",
            "Chao Zhang"
        ],
        "published": "2022-06-28T04:11:58Z",
        "summary": "On e-commerce platforms, predicting if two products are compatible with each\nother is an important functionality to achieve trustworthy product\nrecommendation and search experience for consumers. However, accurately\npredicting product compatibility is difficult due to the heterogeneous product\ndata and the lack of manually curated training data. We study the problem of\ndiscovering effective labeling rules that can enable weakly-supervised product\ncompatibility prediction. We develop AMRule, a multi-view rule discovery\nframework that can (1) adaptively and iteratively discover novel rulers that\ncan complement the current weakly-supervised model to improve compatibility\nprediction; (2) discover interpretable rules from both structured attribute\ntables and unstructured product descriptions. AMRule adaptively discovers\nlabeling rules from large-error instances via a boosting-style strategy, the\nhigh-quality rules can remedy the current model's weak spots and refine the\nmodel iteratively. For rule discovery from structured product attributes, we\ngenerate composable high-order rules from decision trees; and for rule\ndiscovery from unstructured product descriptions, we generate prompt-based\nrules from a pre-trained language model. Experiments on 4 real-world datasets\nshow that AMRule outperforms the baselines by 5.98% on average and improves\nrule quality and rule proposal efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2206.13749v1.pdf"
    },
    {
        "title": "NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts",
        "authors": [
            "Supriti Vijay",
            "Aman Priyanshu"
        ],
        "published": "2022-06-28T03:22:55Z",
        "summary": "With increasing applications in areas such as biomedical information\nextraction pipelines and social media analytics, Named Entity Recognition (NER)\nhas become an indispensable tool for knowledge extraction. However, with the\ngradual shift in language structure and vocabulary, NERs are plagued with\ndistribution shifts, making them redundant or not as profitable without\nre-training. Re-training NERs based on Large Language Models (LLMs) from\nscratch over newly acquired data poses economic disadvantages. In contrast,\nre-training only with newly acquired data will result in Catastrophic\nForgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a\npipeline for training NERs with LLM bases by incorporating the concept of\nElastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As\nwe believe our work has implications to be utilized in the pipeline of\ncontinual learning and NER, we open-source our code as well as provide the\nfine-tuning library of the same name NERDA-Con at\nhttps://github.com/SupritiVijay/NERDA-Con and\nhttps://pypi.org/project/NERDA-Con/.",
        "pdf_link": "https://arxiv.org/pdf/2206.14607v1.pdf"
    },
    {
        "title": "Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse",
        "authors": [
            "James Queeney",
            "Ioannis Ch. Paschalidis",
            "Christos G. Cassandras"
        ],
        "published": "2022-06-28T02:56:12Z",
        "summary": "Data-driven, learning-based control methods offer the potential to improve\noperations in complex systems, and model-free deep reinforcement learning\nrepresents a popular approach to data-driven control. However, existing classes\nof algorithms present a trade-off between two important deployment requirements\nfor real-world control: (i) practical performance guarantees and (ii) data\nefficiency. Off-policy algorithms make efficient use of data through sample\nreuse but lack theoretical guarantees, while on-policy algorithms guarantee\napproximate policy improvement throughout training but suffer from high sample\ncomplexity. In order to balance these competing goals, we develop a class of\nGeneralized Policy Improvement algorithms that combines the policy improvement\nguarantees of on-policy methods with the efficiency of sample reuse. We\ndemonstrate the benefits of this new class of algorithms through extensive\nexperimental analysis on a variety of continuous control tasks from the\nDeepMind Control Suite.",
        "pdf_link": "https://arxiv.org/pdf/2206.13714v2.pdf"
    },
    {
        "title": "Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for Science Education in West Africa",
        "authors": [
            "George Boateng",
            "Samuel John",
            "Andrew Glago",
            "Samuel Boateng",
            "Victor Kumbol"
        ],
        "published": "2022-06-28T02:27:23Z",
        "summary": "Africa has a high student-to-teacher ratio which limits students' access to\nteachers. Consequently, students struggle to get answers to their questions. In\nthis work, we extended Kwame, our previous AI teaching assistant, adapted it\nfor science education, and deployed it as a web app. Kwame for Science answers\nquestions of students based on the Integrated Science subject of the West\nAfrican Senior Secondary Certificate Examination (WASSCE). Kwame for Science is\na Sentence-BERT-based question-answering web app that displays 3 paragraphs as\nanswers along with a confidence score in response to science questions.\nAdditionally, it displays the top 5 related past exam questions and their\nanswers in addition to the 3 paragraphs. Our preliminary evaluation of the\nKwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy\nof 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will\nenable the delivery of scalable, cost-effective, and quality remote education\nto millions of people across Africa.",
        "pdf_link": "https://arxiv.org/pdf/2206.13703v2.pdf"
    },
    {
        "title": "Perspective (In)consistency of Paint by Text",
        "authors": [
            "Hany Farid"
        ],
        "published": "2022-06-27T19:52:33Z",
        "summary": "Type \"a sea otter with a pearl earring by Johannes Vermeer\" or \"a photo of a\nteddy bear on a skateboard in Times Square\" into OpenAI's DALL-E-2\npaint-by-text synthesis engine and you will not be disappointed by the\ndelightful and eerily pertinent results. The ability to synthesize highly\nrealistic images -- with seemingly no limitation other than our imagination --\nis sure to yield many exciting and creative applications. These images are also\nlikely to pose new challenges to the photo-forensic community. Motivated by the\nfact that paint by text is not based on explicit geometric modeling, and the\nhuman visual system's often obliviousness to even glaring geometric\ninconsistencies, we provide an initial exploration of the perspective\nconsistency of DALL-E-2 synthesized images to determine if geometric-based\nforensic analyses will prove fruitful in detecting this new breed of synthetic\nmedia.",
        "pdf_link": "https://arxiv.org/pdf/2206.14617v1.pdf"
    },
    {
        "title": "Materials Transformers Language Models for Generative Materials Design: a benchmark study",
        "authors": [
            "Nihang Fu",
            "Lai Wei",
            "Yuqi Song",
            "Qinyang Li",
            "Rui Xin",
            "Sadman Sadeed Omee",
            "Rongzhi Dong",
            "Edirisuriya M. Dilanga Siriwardane",
            "Jianjun Hu"
        ],
        "published": "2022-06-27T18:50:05Z",
        "summary": "Pre-trained transformer language models on large unlabeled corpus have\nproduced state-of-the-art results in natural language processing, organic\nmolecule design, and protein sequence generation. However, no such models have\nbeen applied to learn the composition patterns of inorganic materials. Here we\ntrain a series of seven modern transformer language models (GPT, GPT-2,\nGPT-Neo, GPT-J, BLMM, BART, and RoBERTa) using the expanded formulas from\nmaterial deposited in the ICSD, OQMD, and Materials Projects databases. Six\ndifferent datasets with/out non-charge-neutral or balanced electronegativity\nsamples are used to benchmark the performances and uncover the generation\nbiases of modern transformer models for the generative design of materials\ncompositions. Our extensive experiments showed that the causal language models\nbased materials transformers can generate chemically valid materials\ncompositions with as high as 97.54\\% to be charge neutral and 91.40\\% to be\nelectronegativity balanced, which has more than 6 times higher enrichment\ncompared to a baseline pseudo-random sampling algorithm. These models also\ndemonstrate high novelty and their potential in new materials discovery has\nbeen proved by their capability to recover the leave-out materials. We also\nfind that the properties of the generated samples can be tailored by training\nthe models with selected training sets such as high-bandgap materials. Our\nexperiments also showed that different models each have their own preference in\nterms of the properties of the generated samples and their running time\ncomplexity varies a lot. We have applied our materials transformer models to\ndiscover a set of new materials as validated using DFT calculations.",
        "pdf_link": "https://arxiv.org/pdf/2206.13578v1.pdf"
    },
    {
        "title": "PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning",
        "authors": [
            "Simone Martone",
            "Francesco Manigrasso",
            "Lamberti Fabrizio",
            "Lia Morra"
        ],
        "published": "2022-06-26T18:34:07Z",
        "summary": "Semantic image interpretation can vastly benefit from approaches that combine\nsub-symbolic distributed representation learning with the capability to reason\nat a higher level of abstraction. Logic Tensor Networks (LTNs) are a class of\nneuro-symbolic systems based on a differentiable, first-order logic grounded\ninto a deep neural network. LTNs replace the classical concept of training set\nwith a knowledge base of fuzzy logical axioms. By defining a set of\ndifferentiable operators to approximate the role of connectives, predicates,\nfunctions and quantifiers, a loss function is automatically specified so that\nLTNs can learn to satisfy the knowledge base. We focus here on the subsumption\nor \\texttt{isOfClass} predicate, which is fundamental to encode most semantic\nimage interpretation tasks. Unlike conventional LTNs, which rely on a separate\npredicate for each class (e.g., dog, cat), each with its own set of learnable\nweights, we propose a common \\texttt{isOfClass} predicate, whose level of truth\nis a function of the distance between an object embedding and the corresponding\nclass prototype. The PROTOtypical Logic Tensor Networks (PROTO-LTN) extend the\ncurrent formulation by grounding abstract concepts as parametrized class\nprototypes in a high-dimensional embedding space, while reducing the number of\nparameters required to ground the knowledge base. We show how this architecture\ncan be effectively trained in the few and zero-shot learning scenarios.\nExperiments on Generalized Zero Shot Learning benchmarks validate the proposed\nimplementation as a competitive alternative to traditional embedding-based\napproaches. The proposed formulation opens up new opportunities in zero shot\nlearning settings, as the LTN formalism allows to integrate background\nknowledge in the form of logical axioms to compensate for the lack of labelled\nexamples.",
        "pdf_link": "https://arxiv.org/pdf/2207.00433v1.pdf"
    },
    {
        "title": "Repository-Level Prompt Generation for Large Language Models of Code",
        "authors": [
            "Disha Shrivastava",
            "Hugo Larochelle",
            "Daniel Tarlow"
        ],
        "published": "2022-06-26T10:51:25Z",
        "summary": "With the success of large language models (LLMs) of code and their use as\ncode assistants (e.g. Codex used in GitHub Copilot), techniques for introducing\ndomain-specific knowledge in the prompt design process become important. In\nthis work, we propose a framework called Repo-Level Prompt Generator that\nlearns to generate example-specific prompts using prompt proposals. The prompt\nproposals take context from the entire repository, thereby incorporating both\nthe structure of the repository and the context from other relevant files (e.g.\nimports, parent class files). Our technique doesn't require any access to the\nweights of the LLM, making it applicable in cases where we only have black-box\naccess to the LLM. We conduct experiments on the task of single-line\ncode-autocompletion using code repositories taken from Google Code archives. We\ndemonstrate that an oracle constructed from our prompt proposals gives a\nremarkably high relative improvement of 36% over Codex, showing the quality of\nthese proposals. Further, we show that when we train a model to predict a\nprompt proposal, we can achieve significant performance gains over Codex and\nother baselines. We release our code, data, and trained checkpoints at:\n\\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.",
        "pdf_link": "https://arxiv.org/pdf/2206.12839v3.pdf"
    },
    {
        "title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction",
        "authors": [
            "Hajo Nils Krabbenh\u00f6ft",
            "Erhardt Barth"
        ],
        "published": "2022-06-25T16:42:05Z",
        "summary": "This paper presents TEVR, a speech recognition model designed to minimize the\nvariation in token entropy w.r.t. to the language model. This takes advantage\nof the fact that if the language model will reliably and accurately predict a\ntoken anyway, then the acoustic model doesn't need to be accurate in\nrecognizing it. We train German ASR models with 900 million parameters and show\nthat on CommonVoice German, TEVR scores a very competitive 3.64% word error\nrate, which outperforms the best reported results by a relative 16.89%\nreduction in word error rate. We hope that releasing our fully trained speech\nrecognition pipeline to the community will lead to privacy-preserving offline\nvirtual assistants in the future.",
        "pdf_link": "https://arxiv.org/pdf/2206.12693v1.pdf"
    },
    {
        "title": "Distilling a Pretrained Language Model to a Multilingual ASR Model",
        "authors": [
            "Kwanghee Choi",
            "Hyung-Min Park"
        ],
        "published": "2022-06-25T12:36:11Z",
        "summary": "Multilingual speech data often suffer from long-tailed language distribution,\nresulting in performance degradation. However, multilingual text data is much\neasier to obtain, yielding a more useful general language model. Hence, we are\nmotivated to distill the rich knowledge embedded inside a well-trained teacher\ntext model to the student speech model. We propose a novel method called the\nDistilling a Language model to a Speech model (Distill-L2S), which aligns the\nlatent representations of two different modalities. The subtle differences are\nhandled by the shrinking mechanism, nearest-neighbor interpolation, and a\nlearnable linear projection layer. We demonstrate the effectiveness of our\ndistillation method by applying it to the multilingual automatic speech\nrecognition (ASR) task. We distill the transformer-based cross-lingual language\nmodel (InfoXLM) while fine-tuning the large-scale multilingual ASR model\n(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on\n20 low-resource languages of the CommonVoice dataset with less than 100 hours\nof speech data.",
        "pdf_link": "https://arxiv.org/pdf/2206.12638v1.pdf"
    },
    {
        "title": "Adversarial Self-Attention for Language Understanding",
        "authors": [
            "Hongqiu Wu",
            "Ruixue Ding",
            "Hai Zhao",
            "Pengjun Xie",
            "Fei Huang",
            "Min Zhang"
        ],
        "published": "2022-06-25T09:18:10Z",
        "summary": "Deep neural models (e.g. Transformer) naturally learn spurious features,\nwhich create a ``shortcut'' between the labels and inputs, thus impairing the\ngeneralization and robustness. This paper advances the self-attention mechanism\nto its robust variant for Transformer-based pre-trained language models (e.g.\nBERT). We propose \\textit{Adversarial Self-Attention} mechanism (ASA), which\nadversarially biases the attentions to effectively suppress the model reliance\non features (e.g. specific keywords) and encourage its exploration of broader\nsemantics. We conduct a comprehensive evaluation across a wide range of tasks\nfor both pre-training and fine-tuning stages. For pre-training, ASA unfolds\nremarkable performance gains compared to naive training for longer steps. For\nfine-tuning, ASA-empowered models outweigh naive models by a large margin\nconsidering both generalization and robustness.",
        "pdf_link": "https://arxiv.org/pdf/2206.12608v3.pdf"
    },
    {
        "title": "Construct a Sentence with Multiple Specified Words",
        "authors": [
            "Yuanliang Meng"
        ],
        "published": "2022-06-25T05:49:51Z",
        "summary": "This paper demonstrates a task to finetune a BART model so it can construct a\nsentence from an arbitrary set of words, which used to be a difficult NLP task.\nThe training task is making sentences with four words, but the trained model\ncan generate sentences when fewer or more words are provided. The output\nsentences have high quality in general. The model can have some real-world\napplications, and this task can be used as an evaluation mechanism for any\nlanguage model as well.",
        "pdf_link": "https://arxiv.org/pdf/2206.12565v1.pdf"
    },
    {
        "title": "Value-Consistent Representation Learning for Data-Efficient Reinforcement Learning",
        "authors": [
            "Yang Yue",
            "Bingyi Kang",
            "Zhongwen Xu",
            "Gao Huang",
            "Shuicheng Yan"
        ],
        "published": "2022-06-25T03:02:25Z",
        "summary": "Deep reinforcement learning (RL) algorithms suffer severe performance\ndegradation when the interaction data is scarce, which limits their real-world\napplication. Recently, visual representation learning has been shown to be\neffective and promising for boosting sample efficiency in RL. These methods\nusually rely on contrastive learning and data augmentation to train a\ntransition model for state prediction, which is different from how the model is\nused in RL--performing value-based planning. Accordingly, the learned\nrepresentation by these visual methods may be good for recognition but not\noptimal for estimating state value and solving the decision problem. To address\nthis issue, we propose a novel method, called value-consistent representation\nlearning (VCR), to learn representations that are directly related to\ndecision-making. More specifically, VCR trains a model to predict the future\nstate (also referred to as the ''imagined state'') based on the current one and\na sequence of actions. Instead of aligning this imagined state with a real\nstate returned by the environment, VCR applies a $Q$-value head on both states\nand obtains two distributions of action values. Then a distance is computed and\nminimized to force the imagined state to produce a similar action value\nprediction as that by the real state. We develop two implementations of the\nabove idea for the discrete and continuous action spaces respectively. We\nconduct experiments on Atari 100K and DeepMind Control Suite benchmarks to\nvalidate their effectiveness for improving sample efficiency. It has been\ndemonstrated that our methods achieve new state-of-the-art performance for\nsearch-free RL algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2206.12542v2.pdf"
    },
    {
        "title": "A Test for Evaluating Performance in Human-Computer Systems",
        "authors": [
            "Andres Campero",
            "Michelle Vaccaro",
            "Jaeyoon Song",
            "Haoran Wen",
            "Abdullah Almaatouq",
            "Thomas W. Malone"
        ],
        "published": "2022-06-24T17:44:58Z",
        "summary": "The Turing test for comparing computer performance to that of humans is well\nknown, but, surprisingly, there is no widely used test for comparing how much\nbetter human-computer systems perform relative to humans alone, computers\nalone, or other baselines. Here, we show how to perform such a test using the\nratio of means as a measure of effect size. Then we demonstrate the use of this\ntest in three ways. First, in an analysis of 79 recently published experimental\nresults, we find that, surprisingly, over half of the studies find a decrease\nin performance, the mean and median ratios of performance improvement are both\napproximately 1 (corresponding to no improvement at all), and the maximum ratio\nis 1.36 (a 36% improvement). Second, we experimentally investigate whether a\nhigher performance improvement ratio is obtained when 100 human programmers\ngenerate software using GPT-3, a massive, state-of-the-art AI system. In this\ncase, we find a speed improvement ratio of 1.27 (a 27% improvement). Finally,\nwe find that 50 human non-programmers using GPT-3 can perform the task about as\nwell as--and less expensively than--the human programmers. In this case,\nneither the non-programmers nor the computer would have been able to perform\nthe task alone, so this is an example of a very strong form of human-computer\nsynergy.",
        "pdf_link": "https://arxiv.org/pdf/2206.12390v2.pdf"
    },
    {
        "title": "Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users",
        "authors": [
            "Akhter Al Amin",
            "Saad Hassan",
            "Cecilia O. Alm",
            "Matt Huenerfauth"
        ],
        "published": "2022-06-24T16:35:57Z",
        "summary": "Deaf and hard of hearing individuals regularly rely on captioning while\nwatching live TV. Live TV captioning is evaluated by regulatory agencies using\nvarious caption evaluation metrics. However, caption evaluation metrics are\noften not informed by preferences of DHH users or how meaningful the captions\nare. There is a need to construct caption evaluation metrics that take the\nrelative importance of words in a transcript into account. We conducted\ncorrelation analysis between two types of word embeddings and human-annotated\nlabeled word-importance scores in existing corpus. We found that normalized\ncontextualized word embeddings generated using BERT correlated better with\nmanually annotated importance scores than word2vec-based word embeddings. We\nmake available a pairing of word embeddings and their human-annotated\nimportance scores. We also provide proof-of-concept utility by training word\nimportance models, achieving an F1-score of 0.57 in the 6-class word importance\nclassification task.",
        "pdf_link": "https://arxiv.org/pdf/2206.12368v1.pdf"
    },
    {
        "title": "Text and author-level political inference using heterogeneous knowledge representations",
        "authors": [
            "Samuel Caetano da Silva",
            "Ivandre Paraboni"
        ],
        "published": "2022-06-24T13:45:36Z",
        "summary": "The inference of politically-charged information from text data is a popular\nresearch topic in Natural Language Processing (NLP) at both text- and\nauthor-level. In recent years, studies of this kind have been implemented with\nthe aid of representations from transformers such as BERT. Despite considerable\nsuccess, however, we may ask whether results may be improved even further by\ncombining transformed-based models with additional knowledge representations.\nTo shed light on this issue, the present work describes a series of experiments\nto compare alternative model configurations for political inference from text\nin both English and Portuguese languages. Results suggest that certain text\nrepresentations - in particular, the combined use of BERT pre-trained language\nmodels with a syntactic dependency model - may outperform the alternatives\nacross multiple experimental settings, making a potentially strong case for\nfurther research in the use of heterogeneous text representations in these and\npossibly other NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.12293v2.pdf"
    },
    {
        "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
        "authors": [
            "Tianyi Tang",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2022-06-24T07:49:47Z",
        "summary": "Pre-trained language models (PLMs) have achieved remarkable success in\nnatural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are\npre-trained in an unsupervised manner using the large-scale general corpus. In\nthe meanwhile, an increasing number of models pre-trained with labeled data\n(i.e. \"supervised pre-training\") showcase superior performance compared to\nunsupervised pre-trained models. Motivated by the success of supervised\npre-training, we propose Multi-task superVised Pre-training (MVP) for natural\nlanguage generation. We collect a large-scale natural language generation\ncorpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we\nunify these examples into a general text-to-text format to pre-train the text\ngeneration model MVP in a supervised manner. For each task, we further\npre-train specific soft prompts to stimulate the model's capacity to perform a\nspecific task. Our MVP model can be seen as a practice that utilizes recent\ninstruction tuning on relatively small PLMs. Extensive experiments have\ndemonstrated the effectiveness and generality of our MVP model in a number of\nNLG tasks, which achieves state-of-the-art performance on $13$ out of $17$\ndatasets, outperforming BART by $9.3\\%$ and Flan-T5 by $5.8\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2206.12131v3.pdf"
    },
    {
        "title": "Unified BERT for Few-shot Natural Language Understanding",
        "authors": [
            "Junyu Lu",
            "Ping Yang",
            "Ruyi Gan",
            "Jing Yang",
            "Jiaxing Zhang"
        ],
        "published": "2022-06-24T06:10:53Z",
        "summary": "Even as pre-trained language models share a semantic encoder, natural\nlanguage understanding suffers from a diversity of output schemas. In this\npaper, we propose UBERT, a unified bidirectional language understanding model\nbased on BERT framework, which can universally model the training objects of\ndifferent NLU tasks through a biaffine network. Specifically, UBERT encodes\nprior knowledge from various aspects, uniformly constructing learning\nrepresentations across multiple NLU tasks, which is conducive to enhancing the\nability to capture common semantic understanding. By using the biaffine to\nmodel scores pair of the start and end position of the original text, various\nclassification and extraction structures can be converted into a universal,\nspan-decoding approach. Experiments show that UBERT wins the first price in the\n2022 AIWIN - World Artificial Intelligence Innovation Competition, Chinese\ninsurance few-shot multi-task track, and realizes the unification of extensive\ninformation extraction and linguistic reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.12094v2.pdf"
    },
    {
        "title": "SC-Ques: A Sentence Completion Question Dataset for English as a Second Language Learners",
        "authors": [
            "Qiongqiong Liu",
            "Yaying Huang",
            "Zitao Liu",
            "Shuyan Huang",
            "Jiahao Chen",
            "Xiangyu Zhao",
            "Guimin Lin",
            "Yuyu Zhou",
            "Weiqi Luo"
        ],
        "published": "2022-06-24T02:17:13Z",
        "summary": "Sentence completion (SC) questions present a sentence with one or more blanks\nthat need to be filled in, three to five possible words or phrases as options.\nSC questions are widely used for students learning English as a Second Language\n(ESL). In this paper, we present a large-scale SC dataset, \\textsc{SC-Ques},\nwhich is made up of 289,148 ESL SC questions from real-world standardized\nEnglish examinations. Furthermore, we build a comprehensive benchmark of\nautomatically solving the SC questions by training the large-scale pre-trained\nlanguage models on the proposed \\textsc{SC-Ques} dataset. We conduct detailed\nanalysis of the baseline models performance, limitations and trade-offs. The\ndata and our code are available for research purposes from:\n\\url{https://github.com/ai4ed/SC-Ques}.",
        "pdf_link": "https://arxiv.org/pdf/2206.12036v2.pdf"
    },
    {
        "title": "A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages",
        "authors": [
            "Akhter Al Amin",
            "Kazi Sinthia Kabir"
        ],
        "published": "2022-06-23T21:57:08Z",
        "summary": "Language models (LM) are becoming prevalent in many language-based\napplication spaces globally. Although these LMs are improving our day-to-day\ninteractions with digital products, concerns remain whether open-ended\nlanguages or text generated from these models reveal any biases toward a\nspecific group of people, thereby risking the usability of a certain product.\nThere is a need to identify whether these models possess bias to improve the\nfairness in these models. This gap motivates our ongoing work, where we\nmeasured the two aspects of bias in GPT-3 generated text through a disability\nlens.",
        "pdf_link": "https://arxiv.org/pdf/2206.11993v1.pdf"
    },
    {
        "title": "BERT Rankers are Brittle: a Study using Adversarial Document Perturbations",
        "authors": [
            "Yumeng Wang",
            "Lijun Lyu",
            "Avishek Anand"
        ],
        "published": "2022-06-23T14:16:48Z",
        "summary": "Contextual ranking models based on BERT are now well established for a wide\nrange of passage and document ranking tasks. However, the robustness of\nBERT-based ranking models under adversarial inputs is under-explored. In this\npaper, we argue that BERT-rankers are not immune to adversarial attacks\ntargeting retrieved documents given a query. Firstly, we propose algorithms for\nadversarial perturbation of both highly relevant and non-relevant documents\nusing gradient-based optimization methods. The aim of our algorithms is to\nadd/replace a small number of tokens to a highly relevant or non-relevant\ndocument to cause a large rank demotion or promotion. Our experiments show that\na small number of tokens can already result in a large change in the rank of a\ndocument. Moreover, we find that BERT-rankers heavily rely on the document\nstart/head for relevance prediction, making the initial part of the document\nmore susceptible to adversarial attacks. More interestingly, we find a small\nset of recurring adversarial words that when added to documents result in\nsuccessful rank demotion/promotion of any relevant/non-relevant document\nrespectively. Finally, our adversarial tokens also show particular topic\npreferences within and across datasets, exposing potential biases from BERT\npre-training or downstream datasets.",
        "pdf_link": "https://arxiv.org/pdf/2206.11724v1.pdf"
    },
    {
        "title": "Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content",
        "authors": [
            "Chia-Hsuan Chang",
            "Lei Wang",
            "Christopher C. Yang"
        ],
        "published": "2022-06-23T10:46:39Z",
        "summary": "The online health community (OHC) is the primary channel for laypeople to\nshare health information. To analyze the health consumer-generated content\n(HCGC) from the OHCs, identifying the colloquial medical expressions used by\nlaypeople is a critical challenge. The open-access and collaborative consumer\nhealth vocabulary (OAC CHV) is the controlled vocabulary for addressing such a\nchallenge. Nevertheless, OAC CHV is only available in English, limiting its\napplicability to other languages. This research proposes a cross-lingual\nautomatic term recognition framework for extending the English CHV into a\ncross-lingual one. Our framework requires an English HCGC corpus and a\nnon-English (i.e., Chinese in this study) HCGC corpus as inputs. Two\nmonolingual word vector spaces are determined using the skip-gram algorithm so\nthat each space encodes common word associations from laypeople within a\nlanguage. Based on the isometry assumption, the framework aligns two\nmonolingual spaces into a bilingual word vector space, where we employ cosine\nsimilarity as a metric for identifying semantically similar words across\nlanguages. The experimental results demonstrate that our framework outperforms\nthe other two large language models in identifying CHV across languages. Our\nframework only requires raw HCGC corpora and a limited size of medical\ntranslations, reducing human efforts in compiling cross-lingual CHV.",
        "pdf_link": "https://arxiv.org/pdf/2206.11612v2.pdf"
    },
    {
        "title": "Mining Error Templates for Grammatical Error Correction",
        "authors": [
            "Yue Zhang",
            "Haochen Jiang",
            "Zuyi Bao",
            "Bo Zhang",
            "Chen Li",
            "Zhenghua Li"
        ],
        "published": "2022-06-23T09:29:52Z",
        "summary": "Some grammatical error correction (GEC) systems incorporate hand-crafted\nrules and achieve positive results. However, manually defining rules is\ntime-consuming and laborious. In view of this, we propose a method to mine\nerror templates for GEC automatically. An error template is a regular\nexpression aiming at identifying text errors. We use the web crawler to acquire\nsuch error templates from the Internet. For each template, we further select\nthe corresponding corrective action by using the language model perplexity as a\ncriterion. We have accumulated 1,119 error templates for Chinese GEC based on\nthis method. Experimental results on the newly proposed CTC-2021 Chinese GEC\nbenchmark show that combing our error templates can effectively improve the\nperformance of a strong GEC system, especially on two error types with very\nlittle training data. Our error templates are available at\n\\url{https://github.com/HillZhang1999/gec_error_template}.",
        "pdf_link": "https://arxiv.org/pdf/2206.11569v1.pdf"
    },
    {
        "title": "Evaluating Generative Patent Language Models",
        "authors": [
            "Jieh-Sheng Lee"
        ],
        "published": "2022-06-23T08:58:05Z",
        "summary": "Generative language models are promising for assisting human writing in\nvarious domains. This manuscript aims to build generative language models in\nthe patent domain and evaluate model performance from a human-centric\nperspective. The perspective is to measure the ratio of keystrokes that can be\nsaved by autocompletion based on generative patent language models. A higher\nratio means a more effective model which can save more keystrokes. This metric\ncan be used to benchmark model performance. The metric is different from\nconventional machine-centric metrics that are token-based instead of\nkeystroke-based. In terms of model size, the largest model built in this\nmanuscript is 6B, which is state-of-the-art in the patent domain. Based on the\nmetric, it is found that the largest model is not necessarily the best for the\nhuman-centric metric. The finding means that keeping increasing model sizes in\nthe patent domain might be unnecessary if the purpose is to assist human\nwriting with autocompletion. Several patent language models are pre-trained\nfrom scratch in this research. The pre-trained models are released for future\nresearchers. Several visualization tools are also provided. The importance of\nbuilding a generative language model in the patent domain is the potential to\nfacilitate creativity and innovations in the future.",
        "pdf_link": "https://arxiv.org/pdf/2206.14578v2.pdf"
    },
    {
        "title": "CGAR: Critic Guided Action Redistribution in Reinforcement Leaning",
        "authors": [
            "Tairan Huang",
            "Xu Li",
            "Hao Li",
            "Mingming Sun",
            "Ping Li"
        ],
        "published": "2022-06-23T06:33:14Z",
        "summary": "Training a game-playing reinforcement learning agent requires multiple\ninteractions with the environment. Ignorant random exploration may cause a\nwaste of time and resources. It's essential to alleviate such waste. As\ndiscussed in this paper, under the settings of the off-policy actor critic\nalgorithms, we demonstrate that the critic can bring more expected discounted\nrewards than or at least equal to the actor. Thus, the Q value predicted by the\ncritic is a better signal to redistribute the action originally sampled from\nthe policy distribution predicted by the actor. This paper introduces the novel\nCritic Guided Action Redistribution (CGAR) algorithm and tests it on the OpenAI\nMuJoCo tasks. The experimental results demonstrate that our method improves the\nsample efficiency and achieves state-of-the-art performance. Our code can be\nfound at https://github.com/tairanhuang/CGAR.",
        "pdf_link": "https://arxiv.org/pdf/2206.11494v1.pdf"
    },
    {
        "title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models",
        "authors": [
            "Virginia K. Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2022-06-23T05:30:47Z",
        "summary": "This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.",
        "pdf_link": "https://arxiv.org/pdf/2206.11484v2.pdf"
    },
    {
        "title": "DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon",
        "authors": [
            "Robin Algayres",
            "Tristan Ricoul",
            "Julien Karadayi",
            "Hugo Lauren\u00e7on",
            "Salah Zaiem",
            "Abdelrahman Mohamed",
            "Beno\u00eet Sagot",
            "Emmanuel Dupoux"
        ],
        "published": "2022-06-22T19:15:57Z",
        "summary": "Finding word boundaries in continuous speech is challenging as there is\nlittle or no equivalent of a 'space' delimiter between words. Popular Bayesian\nnon-parametric models for text segmentation use a Dirichlet process to jointly\nsegment sentences and build a lexicon of word types. We introduce DP-Parse,\nwhich uses similar principles but only relies on an instance lexicon of word\ntokens, avoiding the clustering errors that arise with a lexicon of word types.\nOn the Zero Resource Speech Benchmark 2017, our model sets a new speech\nsegmentation state-of-the-art in 5 languages. The algorithm monotonically\nimproves with better input representations, achieving yet higher scores when\nfed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can\nbe pipelined to a language model and learn semantic and syntactic\nrepresentations as assessed by a new spoken word embedding benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2206.11332v1.pdf"
    },
    {
        "title": "GODEL: Large-Scale Pre-Training for Goal-Directed Dialog",
        "authors": [
            "Baolin Peng",
            "Michel Galley",
            "Pengcheng He",
            "Chris Brockett",
            "Lars Liden",
            "Elnaz Nouri",
            "Zhou Yu",
            "Bill Dolan",
            "Jianfeng Gao"
        ],
        "published": "2022-06-22T18:19:32Z",
        "summary": "We introduce GODEL (Grounded Open Dialogue Language Model), a large\npre-trained language model for dialog. In contrast with earlier models such as\nDialoGPT, GODEL leverages a new phase of grounded pre-training designed to\nbetter support adapting GODEL to a wide range of downstream dialog tasks that\nrequire information external to the current conversation (e.g., a database or\ndocument) to produce good responses. Experiments against an array of benchmarks\nthat encompass task-oriented dialog, conversational QA, and grounded\nopen-domain dialog show that GODEL outperforms state-of-the-art pre-trained\ndialog models in few-shot fine-tuning setups, in terms of both human and\nautomatic evaluation. A novel feature of our evaluation methodology is the\nintroduction of a notion of utility that assesses the usefulness of responses\n(extrinsic evaluation) in addition to their communicative features (intrinsic\nevaluation). We show that extrinsic evaluation offers improved inter-annotator\nagreement and correlation with automated metrics. Code and data processing\nscripts are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2206.11309v1.pdf"
    },
    {
        "title": "Answer Fast: Accelerating BERT on the Tensor Streaming Processor",
        "authors": [
            "Ibrahim Ahmed",
            "Sahil Parmar",
            "Matthew Boyd",
            "Michael Beidler",
            "Kris Kang",
            "Bill Liu",
            "Kyle Roach",
            "John Kim",
            "Dennis Abts"
        ],
        "published": "2022-06-22T13:27:27Z",
        "summary": "Transformers have become a predominant machine learning workload, they are\nnot only the de-facto standard for natural language processing tasks, but they\nare also being deployed in other domains such as vision and speech recognition.\nMany of the transformer-based applications are real-time systems such as\nmachine translation and web search. These real time systems often come with\nstrict end-to-end inference latency requirements. Unfortunately, while the\nmajority of the transformer computation comes from matrix multiplications,\ntransformers also include several non-linear components that tend to become the\nbottleneck during an inference. In this work, we accelerate the inference of\nBERT models on the tensor streaming processor. By carefully fusing all the\nnonlinear components with the matrix multiplication components, we are able to\nefficiently utilize the on-chip matrix multiplication units resulting in a\ndeterministic tail latency of 130 $\\mu$s for a batch-1 inference through\nBERT-base, which is 6X faster than the current state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2206.11062v1.pdf"
    },
    {
        "title": "Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities",
        "authors": [
            "Zejiang Shen",
            "Kyle Lo",
            "Lauren Yu",
            "Nathan Dahlberg",
            "Margo Schlanger",
            "Doug Downey"
        ],
        "published": "2022-06-22T07:26:55Z",
        "summary": "With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2206.10883v3.pdf"
    },
    {
        "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
        "authors": [
            "Jiahui Yu",
            "Yuanzhong Xu",
            "Jing Yu Koh",
            "Thang Luong",
            "Gunjan Baid",
            "Zirui Wang",
            "Vijay Vasudevan",
            "Alexander Ku",
            "Yinfei Yang",
            "Burcu Karagol Ayan",
            "Ben Hutchinson",
            "Wei Han",
            "Zarana Parekh",
            "Xin Li",
            "Han Zhang",
            "Jason Baldridge",
            "Yonghui Wu"
        ],
        "published": "2022-06-22T01:11:29Z",
        "summary": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which\ngenerates high-fidelity photorealistic images and supports content-rich\nsynthesis involving complex compositions and world knowledge. Parti treats\ntext-to-image generation as a sequence-to-sequence modeling problem, akin to\nmachine translation, with sequences of image tokens as the target outputs\nrather than text tokens in another language. This strategy can naturally tap\ninto the rich body of prior work on large language models, which have seen\ncontinued advances in capabilities and performance through scaling data and\nmodel sizes. Our approach is simple: First, Parti uses a Transformer-based\nimage tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens.\nSecond, we achieve consistent quality improvements by scaling the\nencoder-decoder Transformer model up to 20B parameters, with a new\nstate-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on\nMS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts\n(P2), a new holistic benchmark of over 1600 English prompts, demonstrate the\neffectiveness of Parti across a wide variety of categories and difficulty\naspects. We also explore and highlight limitations of our models in order to\ndefine and exemplify key areas of focus for further improvements. See\nhttps://parti.research.google/ for high-resolution images.",
        "pdf_link": "https://arxiv.org/pdf/2206.10789v1.pdf"
    },
    {
        "title": "Efficient and effective training of language and graph neural network models",
        "authors": [
            "Vassilis N. Ioannidis",
            "Xiang Song",
            "Da Zheng",
            "Houyu Zhang",
            "Jun Ma",
            "Yi Xu",
            "Belinda Zeng",
            "Trishul Chilimbi",
            "George Karypis"
        ],
        "published": "2022-06-22T00:23:37Z",
        "summary": "Can we combine heterogenous graph structure with text to learn high-quality\nsemantic and behavioural representations? Graph neural networks (GNN)s encode\nnumerical node attributes and graph structure to achieve impressive performance\nin a variety of supervised learning tasks. Current GNN approaches are\nchallenged by textual features, which typically need to be encoded to a\nnumerical vector before provided to the GNN that may incur some information\nloss. In this paper, we put forth an efficient and effective framework termed\nlanguage model GNN (LM-GNN) to jointly train large-scale language models and\ngraph neural networks. The effectiveness in our framework is achieved by\napplying stage-wise fine-tuning of the BERT model first with heterogenous graph\ninformation and then with a GNN model. Several system and design optimizations\nare proposed to enable scalable and efficient training. LM-GNN accommodates\nnode and edge classification as well as link prediction tasks. We evaluate the\nLM-GNN framework in different datasets performance and showcase the\neffectiveness of the proposed approach. LM-GNN provides competitive results in\nan Amazon query-purchase-product application.",
        "pdf_link": "https://arxiv.org/pdf/2206.10781v1.pdf"
    },
    {
        "title": "Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information",
        "authors": [
            "Tomasz Limisiewicz",
            "David Mare\u010dek"
        ],
        "published": "2022-06-21T21:38:25Z",
        "summary": "The representations in large language models contain multiple types of gender\ninformation. We focus on two types of such signals in English texts: factual\ngender information, which is a grammatical or semantic property, and gender\nbias, which is the correlation between a word and specific gender. We can\ndisentangle the model's embeddings and identify components encoding both types\nof information with probing. We aim to diminish the stereotypical bias in the\nrepresentations while preserving the factual gender signal. Our filtering\nmethod shows that it is possible to decrease the bias of gender-neutral\nprofession names without significant deterioration of language modeling\ncapabilities. The findings can be applied to language generation to mitigate\nreliance on stereotypes while preserving gender agreement in coreferences.",
        "pdf_link": "https://arxiv.org/pdf/2206.10744v1.pdf"
    },
    {
        "title": "Using cognitive psychology to understand GPT-3",
        "authors": [
            "Marcel Binz",
            "Eric Schulz"
        ],
        "published": "2022-06-21T20:06:03Z",
        "summary": "We study GPT-3, a recent large language model, using tools from cognitive\npsychology. More specifically, we assess GPT-3's decision-making, information\nsearch, deliberation, and causal reasoning abilities on a battery of canonical\nexperiments from the literature. We find that much of GPT-3's behavior is\nimpressive: it solves vignette-based tasks similarly or better than human\nsubjects, is able to make decent decisions from descriptions, outperforms\nhumans in a multi-armed bandit task, and shows signatures of model-based\nreinforcement learning. Yet we also find that small perturbations to\nvignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures\nof directed exploration, and that it fails miserably in a causal reasoning\ntask. These results enrich our understanding of current large language models\nand pave the way for future investigations using tools from cognitive\npsychology to study increasingly capable and opaque artificial agents.",
        "pdf_link": "https://arxiv.org/pdf/2206.14576v1.pdf"
    },
    {
        "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing",
        "authors": [
            "Subhro Roy",
            "Sam Thomson",
            "Tongfei Chen",
            "Richard Shin",
            "Adam Pauls",
            "Jason Eisner",
            "Benjamin Van Durme"
        ],
        "published": "2022-06-21T18:34:11Z",
        "summary": "Recent work has shown that generation from a prompted or fine-tuned language\nmodel can perform well at semantic parsing when the output is constrained to be\na valid semantic representation. We introduce BenchCLAMP, a Benchmark to\nevaluate Constrained LAnguage Model Parsing, that includes context-free\ngrammars for seven semantic parsing datasets and two syntactic parsing datasets\nwith varied output representations, as well as a constrained decoding interface\nto generate only valid outputs covered by these grammars. We provide low,\nmedium, and high resource splits for each dataset, allowing accurate comparison\nof various language models under different data regimes. Our benchmark supports\nevaluation of language models using prompt-based learning as well as\nfine-tuning. We benchmark eight language models, including two GPT-3 variants\navailable only through an API. Our experiments show that encoder-decoder\npretrained language models can achieve similar performance or surpass\nstate-of-the-art methods for syntactic and semantic parsing when the model\noutput is constrained to be valid.",
        "pdf_link": "https://arxiv.org/pdf/2206.10668v2.pdf"
    },
    {
        "title": "Questions Are All You Need to Train a Dense Passage Retriever",
        "authors": [
            "Devendra Singh Sachan",
            "Mike Lewis",
            "Dani Yogatama",
            "Luke Zettlemoyer",
            "Joelle Pineau",
            "Manzil Zaheer"
        ],
        "published": "2022-06-21T18:16:31Z",
        "summary": "We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.",
        "pdf_link": "https://arxiv.org/pdf/2206.10658v4.pdf"
    },
    {
        "title": "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine",
        "authors": [
            "Jiayi Weng",
            "Min Lin",
            "Shengyi Huang",
            "Bo Liu",
            "Denys Makoviichuk",
            "Viktor Makoviychuk",
            "Zichen Liu",
            "Yufan Song",
            "Ting Luo",
            "Yukun Jiang",
            "Zhongwen Xu",
            "Shuicheng Yan"
        ],
        "published": "2022-06-21T17:36:15Z",
        "summary": "There has been significant progress in developing reinforcement learning (RL)\ntraining systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and\nothers, aim to improve the system's overall throughput. In this paper, we aim\nto address a common bottleneck in the RL training system, i.e., parallel\nenvironment execution, which is often the slowest part of the whole system but\nreceives little attention. With a curated design for paralleling RL\nenvironments, we have improved the RL environment simulation speed across\ndifferent hardware setups, ranging from a laptop and a modest workstation, to a\nhigh-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool\nachieves one million frames per second for the environment execution on Atari\nenvironments and three million frames per second on MuJoCo environments. When\nrunning EnvPool on a laptop, the speed is 2.8x that of the Python subprocess.\nMoreover, great compatibility with existing RL training libraries has been\ndemonstrated in the open-sourced community, including CleanRL, rl_games,\nDeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas\nat a much faster pace and has great potential to become the de facto RL\nenvironment execution engine. Example runs show that it only takes five minutes\nto train agents to play Atari Pong and MuJoCo Ant on a laptop. EnvPool is\nopen-sourced at https://github.com/sail-sg/envpool.",
        "pdf_link": "https://arxiv.org/pdf/2206.10558v2.pdf"
    },
    {
        "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Alberto Olmo",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2022-06-21T16:15:27Z",
        "summary": "Generating plans of action, and reasoning about change have long been\nconsidered a core competence of intelligent agents. It is thus no surprise that\nevaluating the planning and reasoning capabilities of large language models\n(LLMs) has become a hot topic of research. Most claims about LLM planning\ncapabilities are however based on common sense tasks-where it becomes hard to\ntell whether LLMs are planning or merely retrieving from their vast world\nknowledge. There is a strong need for systematic and extensible planning\nbenchmarks with sufficient diversity to evaluate whether LLMs have innate\nplanning capabilities. Motivated by this, we propose PlanBench, an extensible\nbenchmark suite based on the kinds of domains used in the automated planning\ncommunity, especially in the International Planning Competition, to test the\ncapabilities of LLMs in planning or reasoning about actions and change.\nPlanBench provides sufficient diversity in both the task domains and the\nspecific planning capabilities. Our studies also show that on many critical\ncapabilities-including plan generation-LLM performance falls quite short, even\nwith the SOTA models. PlanBench can thus function as a useful marker of\nprogress of LLMs in planning and reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2206.10498v4.pdf"
    },
    {
        "title": "An Automatic and Efficient BERT Pruning for Edge AI Systems",
        "authors": [
            "Shaoyi Huang",
            "Ning Liu",
            "Yueying Liang",
            "Hongwu Peng",
            "Hongjia Li",
            "Dongkuan Xu",
            "Mimi Xie",
            "Caiwen Ding"
        ],
        "published": "2022-06-21T15:10:29Z",
        "summary": "With the yearning for deep learning democratization, there are increasing\ndemands to implement Transformer-based natural language processing (NLP) models\non resource-constrained devices for low-latency and high accuracy. Existing\nBERT pruning methods require domain experts to heuristically handcraft\nhyperparameters to strike a balance among model size, latency, and accuracy. In\nthis work, we propose AE-BERT, an automatic and efficient BERT pruning\nframework with efficient evaluation to select a \"good\" sub-network candidate\n(with high accuracy) given the overall pruning ratio constraints. Our proposed\nmethod requires no human experts experience and achieves a better accuracy\nperformance on many NLP tasks. Our experimental results on General Language\nUnderstanding Evaluation (GLUE) benchmark show that AE-BERT outperforms the\nstate-of-the-art (SOTA) hand-crafted pruning methods on BERT$_{\\mathrm{BASE}}$.\nOn QNLI and RTE, we obtain 75\\% and 42.8\\% more overall pruning ratio while\nachieving higher accuracy. On MRPC, we obtain a 4.6 higher score than the SOTA\nat the same overall pruning ratio of 0.5. On STS-B, we can achieve a 40\\%\nhigher pruning ratio with a very small loss in Spearman correlation compared to\nSOTA hand-crafted pruning methods. Experimental results also show that after\nmodel compression, the inference time of a single BERT$_{\\mathrm{BASE}}$\nencoder on Xilinx Alveo U200 FPGA board has a 1.83$\\times$ speedup compared to\nIntel(R) Xeon(R) Gold 5218 (2.30GHz) CPU, which shows the reasonableness of\ndeploying the proposed method generated subnets of BERT$_{\\mathrm{BASE}}$ model\non computation restricted devices.",
        "pdf_link": "https://arxiv.org/pdf/2206.10461v1.pdf"
    },
    {
        "title": "CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework",
        "authors": [
            "Xiaofeng Li",
            "Bin Ren",
            "Xipeng Shen",
            "Yanzhi Wang"
        ],
        "published": "2022-06-21T14:10:22Z",
        "summary": "There is a growing demand for shifting the delivery of AI capability from\ndata centers on the cloud to edge or end devices, exemplified by the fast\nemerging real-time AI-based apps running on smartphones, AR/VR devices,\nautonomous vehicles, and various IoT devices. The shift has however been\nseriously hampered by the large growing gap between DNN computing demands and\nthe computing power on edge or end devices. This article presents the design of\nXGen, an optimizing framework for DNN designed to bridge the gap. XGen takes\ncross-cutting co-design as its first-order consideration. Its full-stack\nAI-oriented optimizations consist of a number of innovative optimizations at\nevery layer of the DNN software stack, all designed in a cooperative manner.\nThe unique technology makes XGen able to optimize various DNNs, including those\nwith an extreme depth (e.g., BERT, GPT, other transformers), and generate code\nthat runs several times faster than those from existing DNN frameworks, while\ndelivering the same level of accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2206.10620v1.pdf"
    },
    {
        "title": "TabText: A Flexible and Contextual Approach to Tabular Data Representation",
        "authors": [
            "Kimberly Villalobos Carballo",
            "Liangyuan Na",
            "Yu Ma",
            "L\u00e9onard Boussioux",
            "Cynthia Zeng",
            "Luis R. Soenksen",
            "Dimitris Bertsimas"
        ],
        "published": "2022-06-21T13:28:57Z",
        "summary": "Tabular data is essential for applying machine learning tasks across various\nindustries. However, traditional data processing methods do not fully utilize\nall the information available in the tables, ignoring important contextual\ninformation such as column header descriptions. In addition, pre-processing\ndata into a tabular format can remain a labor-intensive bottleneck in model\ndevelopment. This work introduces TabText, a processing and feature extraction\nframework that extracts contextual information from tabular data structures.\nTabText addresses processing difficulties by converting the content into\nlanguage and utilizing pre-trained large language models (LLMs). We evaluate\nour framework on nine healthcare prediction tasks ranging from patient\ndischarge, ICU admission, and mortality. We show that 1) applying our TabText\nframework enables the generation of high-performing and simple machine learning\nbaseline models with minimal data pre-processing, and 2) augmenting\npre-processed tabular data with TabText representations improves the average\nand worst-case AUC performance of standard machine learning models by as much\nas 6%.",
        "pdf_link": "https://arxiv.org/pdf/2206.10381v4.pdf"
    },
    {
        "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP",
        "authors": [
            "Yufei Wang",
            "Jiayi Zheng",
            "Can Xu",
            "Xiubo Geng",
            "Tao Shen",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "published": "2022-06-21T11:34:02Z",
        "summary": "This paper focuses on the data augmentation for low-resource NLP tasks where\nthe training set is limited. The existing solutions either leverage\ntask-independent heuristic rules (e.g., Synonym Replacement) or fine-tune\ngeneral-purpose pre-trained language models (e.g., GPT2) using the limited\ntraining instances to produce new synthetic data. Consequently, they have\ntrivial task-specific knowledge and are limited to yielding low-quality\nsynthetic data. To combat this issue, we propose Knowledge Mixture Data\nAugmentation Model (KnowDA) which is an Seq2Seq language model pre-trained on a\nmixture of diverse NLP tasks under a novel framework of Knowledge Mixture\nTraining (KoMT). The goal of KoMT is to condense diverse NLP task-specific\nknowledge into the single KnowDA model (i.e., all-in-one) such that KnowDA\ncould utilize these knowledge to quickly grasp the inherent synthesis law of\nthe target task through limited training instances. Specifically, KoMT\nreformulates input examples from various heterogeneous NLP tasks into a unified\ntext-to-text format, and employs denoising training objectives in different\ngranularity to learn to reconstruct partial or complete samples. To the best of\nour knowledge, we are the first attempt to apply 100+ NLP multi-task training\nfor data augmentation. Extensive experiments show that i) the synthetic data\nproduced by KnowDA successfully improves performance of the strong pre-trained\nlanguage models (i.e., Bert, ALBert and Deberta) by a large margin on the\nlow-resource NLP benchmark FewGLUE, CoNLL'03 and WikiAnn; ii) KnowDA\nsuccessfully transfers the task knowledge to NLP tasks whose types are seen and\nunseen in KoMT.",
        "pdf_link": "https://arxiv.org/pdf/2206.10265v2.pdf"
    },
    {
        "title": "TAPHSIR: Towards AnaPHoric Ambiguity Detection and ReSolution In Requirements",
        "authors": [
            "Saad Ezzini",
            "Sallam Abualhaija",
            "Chetan Arora",
            "Mehrdad Sabetzadeh"
        ],
        "published": "2022-06-21T09:53:13Z",
        "summary": "We introduce TAPHSIR, a tool for anaphoric ambiguity detection and anaphora\nresolution in requirements. TAPHSIR facilities reviewing the use of pronouns in\na requirements specification and revising those pronouns that can lead to\nmisunderstandings during the development process. To this end, TAPHSIR detects\nthe requirements which have potential anaphoric ambiguity and further attempts\ninterpreting anaphora occurrences automatically. TAPHSIR employs a hybrid\nsolution composed of an ambiguity detection solution based on machine learning\nand an anaphora resolution solution based on a variant of the BERT language\nmodel. Given a requirements specification, TAPHSIR decides for each pronoun\noccurrence in the specification whether the pronoun is ambiguous or\nunambiguous, and further provides an automatic interpretation for the pronoun.\nThe output generated by TAPHSIR can be easily reviewed and validated by\nrequirements engineers. TAPHSIR is publicly available on Zenodo (DOI:\n10.5281/zenodo.5902117).",
        "pdf_link": "https://arxiv.org/pdf/2206.10227v1.pdf"
    },
    {
        "title": "Knowledge Graph Fusion for Language Model Fine-tuning",
        "authors": [
            "Nimesh Bhana",
            "Terence L. van Zyl"
        ],
        "published": "2022-06-21T08:06:22Z",
        "summary": "Language Models such as BERT have grown in popularity due to their ability to\nbe pre-trained and perform robustly on a wide range of Natural Language\nProcessing tasks. Often seen as an evolution over traditional word embedding\ntechniques, they can produce semantic representations of text, useful for tasks\nsuch as semantic similarity. However, state-of-the-art models often have high\ncomputational requirements and lack global context or domain knowledge which is\nrequired for complete language understanding. To address these limitations, we\ninvestigate the benefits of knowledge incorporation into the fine-tuning stages\nof BERT. An existing K-BERT model, which enriches sentences with triplets from\na Knowledge Graph, is adapted for the English language and extended to inject\ncontextually relevant information into sentences. As a side-effect, changes\nmade to K-BERT for accommodating the English language also extend to other\nword-based languages. Experiments conducted indicate that injected knowledge\nintroduces noise. We see statistically significant improvements for\nknowledge-driven tasks when this noise is minimised. We show evidence that,\ngiven the appropriate task, modest injection with relevant, high-quality\nknowledge is most performant.",
        "pdf_link": "https://arxiv.org/pdf/2206.14574v1.pdf"
    },
    {
        "title": "General Framework for Reversible Data Hiding in Texts Based on Masked Language Modeling",
        "authors": [
            "Xiaoyan Zheng",
            "Yurun Fang",
            "Hanzhou Wu"
        ],
        "published": "2022-06-21T05:02:49Z",
        "summary": "With the fast development of natural language processing, recent advances in\ninformation hiding focus on covertly embedding secret information into texts.\nThese algorithms either modify a given cover text or directly generate a text\ncontaining secret information, which, however, are not reversible, meaning that\nthe original text not carrying secret information cannot be perfectly recovered\nunless much side information are shared in advance. To tackle with this\nproblem, in this paper, we propose a general framework to embed secret\ninformation into a given cover text, for which the embedded information and the\noriginal cover text can be perfectly retrieved from the marked text. The main\nidea of the proposed method is to use a masked language model to generate such\na marked text that the cover text can be reconstructed by collecting the words\nof some positions and the words of the other positions can be processed to\nextract the secret information. Our results show that the original cover text\nand the secret information can be successfully embedded and extracted.\nMeanwhile, the marked text carrying secret information has good fluency and\nsemantic quality, indicating that the proposed method has satisfactory\nsecurity, which has been verified by experimental results. Furthermore, there\nis no need for the data hider and data receiver to share the language model,\nwhich significantly reduces the side information and thus has good potential in\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2206.10112v1.pdf"
    },
    {
        "title": "Automatic Controllable Product Copywriting for E-Commerce",
        "authors": [
            "Xiaojie Guo",
            "Qingkai Zeng",
            "Meng Jiang",
            "Yun Xiao",
            "Bo Long",
            "Lingfei Wu"
        ],
        "published": "2022-06-21T04:18:52Z",
        "summary": "Automatic product description generation for e-commerce has witnessed\nsignificant advancement in the past decade. Product copywriting aims to attract\nusers' interest and improve user experience by highlighting product\ncharacteristics with textual descriptions. As the services provided by\ne-commerce platforms become diverse, it is necessary to adapt the patterns of\nautomatically-generated descriptions dynamically. In this paper, we report our\nexperience in deploying an E-commerce Prefix-based Controllable Copywriting\nGeneration (EPCCG) system into the JD.com e-commerce product recommendation\nplatform. The development of the system contains two main components: 1)\ncopywriting aspect extraction; 2) weakly supervised aspect labeling; 3) text\ngeneration with a prefix-based language model; 4) copywriting quality control.\nWe conduct experiments to validate the effectiveness of the proposed EPCCG. In\naddition, we introduce the deployed architecture which cooperates with the\nEPCCG into the real-time JD.com e-commerce recommendation platform and the\nsignificant payoff since deployment.",
        "pdf_link": "https://arxiv.org/pdf/2206.10103v1.pdf"
    },
    {
        "title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
        "authors": [
            "Yarden Tal",
            "Inbal Magar",
            "Roy Schwartz"
        ],
        "published": "2022-06-20T15:52:40Z",
        "summary": "The size of pretrained models is increasing, and so is their performance on a\nvariety of NLP tasks. However, as their memorization capacity grows, they might\npick up more social biases. In this work, we examine the connection between\nmodel size and its gender bias (specifically, occupational gender bias). We\nmeasure bias in three masked language model families (RoBERTa, DeBERTa, and T5)\nin two setups: directly using prompt based method, and using a downstream task\n(Winogender). We find on the one hand that larger models receive higher bias\nscores on the former task, but when evaluated on the latter, they make fewer\ngender errors. To examine these potentially conflicting results, we carefully\ninvestigate the behavior of the different models on Winogender. We find that\nwhile larger models outperform smaller ones, the probability that their\nmistakes are caused by gender bias is higher. Moreover, we find that the\nproportion of stereotypical errors compared to anti-stereotypical ones grows\nwith the model size. Our findings highlight the potential risks that can arise\nfrom increasing model size.",
        "pdf_link": "https://arxiv.org/pdf/2206.09860v1.pdf"
    },
    {
        "title": "SPBERTQA: A Two-Stage Question Answering System Based on Sentence Transformers for Medical Texts",
        "authors": [
            "Nhung Thi-Hong Nguyen",
            "Phuong Phan-Dieu Ha",
            "Luan Thanh Nguyen",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "published": "2022-06-20T07:07:59Z",
        "summary": "Question answering (QA) systems have gained explosive attention in recent\nyears. However, QA tasks in Vietnamese do not have many datasets.\nSignificantly, there is mostly no dataset in the medical domain. Therefore, we\nbuilt a Vietnamese Healthcare Question Answering dataset (ViHealthQA),\nincluding 10,015 question-answer passage pairs for this task, in which\nquestions from health-interested users were asked on prestigious health\nwebsites and answers from highly qualified experts. This paper proposes a\ntwo-stage QA system based on Sentence-BERT (SBERT) using multiple negatives\nranking (MNR) loss combined with BM25. Then, we conduct diverse experiments\nwith many bag-of-words models to assess our system's performance. With the\nobtained results, this system achieves better performance than traditional\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2206.09600v1.pdf"
    },
    {
        "title": "Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data",
        "authors": [
            "Tian Li",
            "Xiang Chen",
            "Zhen Dong",
            "Weijiang Yu",
            "Yijun Yan",
            "Kurt Keutzer",
            "Shanghang Zhang"
        ],
        "published": "2022-06-20T06:38:51Z",
        "summary": "Domain adaptive text classification is a challenging problem for the\nlarge-scale pretrained language models because they often require expensive\nadditional labeled data to adapt to new domains. Existing works usually fails\nto leverage the implicit relationships among words across domains. In this\npaper, we propose a novel method, called Domain Adaptation with Structured\nKnowledge (DASK), to enhance domain adaptation by exploiting word-level\nsemantic relationships. DASK first builds a knowledge graph to capture the\nrelationship between pivot terms (domain-independent words) and non-pivot terms\nin the target domain. Then during training, DASK injects pivot-related\nknowledge graph information into source domain texts. For the downstream task,\nthese knowledge-injected texts are fed into a BERT variant capable of\nprocessing knowledge-injected textual data. Thanks to the knowledge injection,\nour model learns domain-invariant features for non-pivots according to their\nrelationships with pivots. DASK ensures the pivots to have domain-invariant\nbehaviors by dynamically inferring via the polarity scores of candidate pivots\nduring training with pseudo-labels. We validate DASK on a wide range of\ncross-domain sentiment classification tasks and observe up to 2.9% absolute\nperformance improvement over baselines for 20 different domain pairs. Code will\nbe made available at https://github.com/hikaru-nara/DASK.",
        "pdf_link": "https://arxiv.org/pdf/2206.09591v1.pdf"
    },
    {
        "title": "Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning",
        "authors": [
            "Xiaolei Wang",
            "Kun Zhou",
            "Ji-Rong Wen",
            "Wayne Xin Zhao"
        ],
        "published": "2022-06-19T09:21:27Z",
        "summary": "Conversational recommender systems (CRS) aim to proactively elicit user\npreference and recommend high-quality items through natural language\nconversations. Typically, a CRS consists of a recommendation module to predict\npreferred items for users and a conversation module to generate appropriate\nresponses. To develop an effective CRS, it is essential to seamlessly integrate\nthe two modules. Existing works either design semantic alignment strategies, or\nshare knowledge resources and representations between the two modules. However,\nthese approaches still rely on different architectures or techniques to develop\nthe two modules, making it difficult for effective module integration.\n  To address this problem, we propose a unified CRS model named UniCRS based on\nknowledge-enhanced prompt learning. Our approach unifies the recommendation and\nconversation subtasks into the prompt learning paradigm, and utilizes\nknowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to\nfulfill both subtasks in a unified approach. In the prompt design, we include\nfused knowledge representations, task-specific soft tokens, and the dialogue\ncontext, which can provide sufficient contextual information to adapt the PLM\nfor the CRS task. Besides, for the recommendation subtask, we also incorporate\nthe generated response template as an important part of the prompt, to enhance\nthe information interaction between the two subtasks. Extensive experiments on\ntwo public CRS datasets have demonstrated the effectiveness of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2206.09363v1.pdf"
    },
    {
        "title": "An Embedded Feature Selection Framework for Control",
        "authors": [
            "Jiawen Wei",
            "Fangyuan Wang",
            "Wanxin Zeng",
            "Wenwei Lin",
            "Ning Gui"
        ],
        "published": "2022-06-19T07:03:40Z",
        "summary": "Reducing sensor requirements while keeping optimal control performance is\ncrucial to many industrial control applications to achieve robust, low-cost,\nand computation-efficient controllers. However, existing feature selection\nsolutions for the typical machine learning domain can hardly be applied in the\ndomain of control with changing dynamics. In this paper, a novel framework,\nnamely the Dual-world embedded Attentive Feature Selection (D-AFS), can\nefficiently select the most relevant sensors for the system under dynamic\ncontrol. Rather than the one world used in most Deep Reinforcement Learning\n(DRL) algorithms, D-AFS has both the real world and its virtual peer with\ntwisted features. By analyzing the DRL's response in two worlds, D-AFS can\nquantitatively identify respective features' importance towards control. A\nwell-known active flow control problem, cylinder drag reduction, is used for\nevaluation. Results show that D-AFS successfully finds an optimized five-probes\nlayout with 18.7\\% drag reduction than the state-of-the-art solution with 151\nprobes and 49.2\\% reduction than five-probes layout by human experts. We also\napply this solution to four OpenAI classical control cases. In all cases, D-AFS\nachieves the same or better sensor configurations than originally provided\nsolutions. Results highlight, we argued, a new way to achieve efficient and\noptimal sensor designs for experimental or industrial systems. Our source codes\nare made publicly available at https://github.com/G-AILab/DAFSFluid.",
        "pdf_link": "https://arxiv.org/pdf/2206.11064v1.pdf"
    },
    {
        "title": "Can Language Models Capture Graph Semantics? From Graphs to Language Model and Vice-Versa",
        "authors": [
            "Tarun Garg",
            "Kaushik Roy",
            "Amit Sheth"
        ],
        "published": "2022-06-18T18:12:20Z",
        "summary": "Knowledge Graphs are a great resource to capture semantic knowledge in terms\nof entities and relationships between the entities. However, current deep\nlearning models takes as input distributed representations or vectors. Thus,\nthe graph is compressed in a vectorized representation. We conduct a study to\nexamine if the deep learning model can compress a graph and then output the\nsame graph with most of the semantics intact. Our experiments show that\nTransformer models are not able to express the full semantics of the input\nknowledge graph. We find that this is due to the disparity between the\ndirected, relationship and type based information contained in a Knowledge\nGraph and the fully connected token-token undirected graphical interpretation\nof the Transformer Attention matrix.",
        "pdf_link": "https://arxiv.org/pdf/2206.09259v1.pdf"
    },
    {
        "title": "Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods",
        "authors": [
            "Valeriya Goloviznina",
            "Evgeny Kotelnikov"
        ],
        "published": "2022-06-18T17:28:04Z",
        "summary": "The development of large and super-large language models, such as GPT-3, T5,\nSwitch Transformer, ERNIE, etc., has significantly improved the performance of\ntext generation. One of the important research directions in this area is the\ngeneration of texts with arguments. The solution of this problem can be used in\nbusiness meetings, political debates, dialogue systems, for preparation of\nstudent essays. One of the main domains for these applications is the economic\nsphere. The key problem of the argument text generation for the Russian\nlanguage is the lack of annotated argumentation corpora. In this paper, we use\ntranslated versions of the Argumentative Microtext, Persuasive Essays and UKP\nSentential corpora to fine-tune RuBERT model. Further, this model is used to\nannotate the corpus of economic news by argumentation. Then the annotated\ncorpus is employed to fine-tune the ruGPT-3 model, which generates argument\ntexts. The results show that this approach improves the accuracy of the\nargument generation by more than 20 percentage points (63.2% vs. 42.5%)\ncompared to the original ruGPT-3 model.",
        "pdf_link": "https://arxiv.org/pdf/2206.09253v1.pdf"
    },
    {
        "title": "Argumentative Text Generation in Economic Domain",
        "authors": [
            "Irina Fishcheva",
            "Dmitriy Osadchiy",
            "Klavdiya Bochenina",
            "Evgeny Kotelnikov"
        ],
        "published": "2022-06-18T17:22:06Z",
        "summary": "The development of large and super-large language models, such as GPT-3, T5,\nSwitch Transformer, ERNIE, etc., has significantly improved the performance of\ntext generation. One of the important research directions in this area is the\ngeneration of texts with arguments. The solution of this problem can be used in\nbusiness meetings, political debates, dialogue systems, for preparation of\nstudent essays. One of the main domains for these applications is the economic\nsphere. The key problem of the argument text generation for the Russian\nlanguage is the lack of annotated argumentation corpora. In this paper, we use\ntranslated versions of the Argumentative Microtext, Persuasive Essays and UKP\nSentential corpora to fine-tune RuBERT model. Further, this model is used to\nannotate the corpus of economic news by argumentation. Then the annotated\ncorpus is employed to fine-tune the ruGPT-3 model, which generates argument\ntexts. The results show that this approach improves the accuracy of the\nargument generation by more than 20 percentage points (63.2\\% vs. 42.5\\%)\ncompared to the original ruGPT-3 model.",
        "pdf_link": "https://arxiv.org/pdf/2206.09251v1.pdf"
    },
    {
        "title": "RuArg-2022: Argument Mining Evaluation",
        "authors": [
            "Evgeny Kotelnikov",
            "Natalia Loukachevitch",
            "Irina Nikishina",
            "Alexander Panchenko"
        ],
        "published": "2022-06-18T17:13:37Z",
        "summary": "Argumentation analysis is a field of computational linguistics that studies\nmethods for extracting arguments from texts and the relationships between them,\nas well as building argumentation structure of texts. This paper is a report of\nthe organizers on the first competition of argumentation analysis systems\ndealing with Russian language texts within the framework of the Dialogue\nconference. During the competition, the participants were offered two tasks:\nstance detection and argument classification. A corpus containing 9,550\nsentences (comments on social media posts) on three topics related to the\nCOVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,\nannotated, and used for training and testing. The system that won the first\nplace in both tasks used the NLI (Natural Language Inference) variant of the\nBERT architecture, automatic translation into English to apply a specialized\nBERT model, retrained on Twitter posts discussing COVID-19, as well as\nadditional masking of target entities. This system showed the following\nresults: for the stance detection task an F1-score of 0.6968, for the argument\nclassification task an F1-score of 0.7404. We hope that the prepared dataset\nand baselines will help to foster further research on argument mining for the\nRussian language.",
        "pdf_link": "https://arxiv.org/pdf/2206.09249v1.pdf"
    },
    {
        "title": "VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection",
        "authors": [
            "Yu Cui",
            "Moshiur Farazi"
        ],
        "published": "2022-06-18T04:08:19Z",
        "summary": "Visual Relationship Detection (VRD) impels a computer vision model to 'see'\nbeyond an individual object instance and 'understand' how different objects in\na scene are related. The traditional way of VRD is first to detect objects in\nan image and then separately predict the relationship between the detected\nobject instances. Such a disjoint approach is prone to predict redundant\nrelationship tags (i.e., predicate) between the same object pair with similar\nsemantic meaning, or incorrect ones that have a similar meaning to the ground\ntruth but are semantically incorrect. To remedy this, we propose to jointly\ntrain a VRD model with visual object features and semantic relationship\nfeatures. To this end, we propose VReBERT, a BERT-like transformer model for\nVisual Relationship Detection with a multi-stage training strategy to jointly\nprocess visual and semantic features. We show that our simple BERT-like model\nis able to outperform the state-of-the-art VRD models in predicate prediction.\nFurthermore, we show that by using the pre-trained VReBERT model, our model\npushes the state-of-the-art zero-shot predicate prediction by a significant\nmargin (+8.49 R@50 and +8.99 R@100).",
        "pdf_link": "https://arxiv.org/pdf/2206.09111v1.pdf"
    },
    {
        "title": "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis",
        "authors": [
            "Shayegan Omidshafiei",
            "Andrei Kapishnikov",
            "Yannick Assogba",
            "Lucas Dixon",
            "Been Kim"
        ],
        "published": "2022-06-17T23:07:33Z",
        "summary": "Each year, expert-level performance is attained in increasingly-complex\nmultiagent domains, where notable examples include Go, Poker, and StarCraft II.\nThis rapid progression is accompanied by a commensurate need to better\nunderstand how such agents attain this performance, to enable their safe\ndeployment, identify limitations, and reveal potential means of improving them.\nIn this paper we take a step back from performance-focused multiagent learning,\nand instead turn our attention towards agent behavior analysis. We introduce a\nmodel-agnostic method for discovery of behavior clusters in multiagent domains,\nusing variational inference to learn a hierarchy of behaviors at the joint and\nlocal agent levels. Our framework makes no assumption about agents' underlying\nlearning algorithms, does not require access to their latent states or\npolicies, and is trained using only offline observational data. We illustrate\nthe effectiveness of our method for enabling the coupled understanding of\nbehaviors at the joint and local agent level, detection of behavior\nchangepoints throughout training, discovery of core behavioral concepts,\ndemonstrate the approach's scalability to a high-dimensional multiagent MuJoCo\ncontrol domain, and also illustrate that the approach can disentangle\npreviously-trained policies in OpenAI's hide-and-seek domain.",
        "pdf_link": "https://arxiv.org/pdf/2206.09046v3.pdf"
    },
    {
        "title": "niksss at HinglishEval: Language-agnostic BERT-based Contextual Embeddings with Catboost for Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text",
        "authors": [
            "Nikhil Singh"
        ],
        "published": "2022-06-17T17:36:03Z",
        "summary": "This paper describes the system description for the HinglishEval challenge at\nINLG 2022. The goal of this task was to investigate the factors influencing the\nquality of the code-mixed text generation system. The task was divided into two\nsubtasks, quality rating prediction and annotators disagreement prediction of\nthe synthetic Hinglish dataset. We attempted to solve these tasks using\nsentence-level embeddings, which are obtained from mean pooling the\ncontextualized word embeddings for all input tokens in our text. We\nexperimented with various classifiers on top of the embeddings produced for\nrespective tasks. Our best-performing system ranked 1st on subtask B and 3rd on\nsubtask A.",
        "pdf_link": "https://arxiv.org/pdf/2206.08910v1.pdf"
    },
    {
        "title": "Language with Vision: a Study on Grounded Word and Sentence Embeddings",
        "authors": [
            "Hassan Shahmohammadi",
            "Maria Heitmeier",
            "Elnaz Shafaei-Bajestan",
            "Hendrik P. A. Lensch",
            "Harald Baayen"
        ],
        "published": "2022-06-17T15:04:05Z",
        "summary": "Grounding language in vision is an active field of research seeking to\nconstruct cognitively plausible word and sentence representations by\nincorporating perceptual knowledge from vision into text-based representations.\nDespite many attempts at language grounding, achieving an optimal equilibrium\nbetween textual representations of the language and our embodied experiences\nremains an open field. Some common concerns are the following. Is visual\ngrounding advantageous for abstract words, or is its effectiveness restricted\nto concrete words? What is the optimal way of bridging the gap between text and\nvision? To what extent is perceptual knowledge from images advantageous for\nacquiring high-quality embeddings? Leveraging the current advances in machine\nlearning and natural language processing, the present study addresses these\nquestions by proposing a simple yet very effective computational grounding\nmodel for pre-trained word embeddings. Our model effectively balances the\ninterplay between language and vision by aligning textual embeddings with\nvisual information while simultaneously preserving the distributional\nstatistics that characterize word usage in text corpora. By applying a learned\nalignment, we are able to indirectly ground unseen words including abstract\nwords. A series of evaluations on a range of behavioural datasets shows that\nvisual grounding is beneficial not only for concrete words but also for\nabstract words, lending support to the indirect theory of abstract concepts.\nMoreover, our approach offers advantages for contextualized embeddings, such as\nthose generated by BERT, but only when trained on corpora of modest,\ncognitively plausible sizes. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.",
        "pdf_link": "https://arxiv.org/pdf/2206.08823v3.pdf"
    },
    {
        "title": "BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish Text Using Transformers",
        "authors": [
            "Shaz Furniturewala",
            "Vijay Kumari",
            "Amulya Ratna Dash",
            "Hriday Kedia",
            "Yashvardhan Sharma"
        ],
        "published": "2022-06-17T10:36:50Z",
        "summary": "Code-Mixed text data consists of sentences having words or phrases from more\nthan one language. Most multi-lingual communities worldwide communicate using\nmultiple languages, with English usually one of them. Hinglish is a Code-Mixed\ntext composed of Hindi and English but written in Roman script. This paper aims\nto determine the factors influencing the quality of Code-Mixed text data\ngenerated by the system. For the HinglishEval task, the proposed model uses\nmulti-lingual BERT to find the similarity between synthetically generated and\nhuman-generated sentences to predict the quality of synthetically generated\nHinglish sentences.",
        "pdf_link": "https://arxiv.org/pdf/2206.08680v1.pdf"
    },
    {
        "title": "Local Slot Attention for Vision-and-Language Navigation",
        "authors": [
            "Yifeng Zhuang",
            "Qiang Sun",
            "Yanwei Fu",
            "Lifeng Chen",
            "Xiangyang Xue"
        ],
        "published": "2022-06-17T09:21:26Z",
        "summary": "Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\n  Recently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\n  However, there exist two problems in current transformer-based models.\n  1) The models process each view independently without taking the integrity of\nthe objects into account.\n  2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\n  To address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2206.08645v2.pdf"
    },
    {
        "title": "MSDF: A General Open-Domain Multi-Skill Dialog Framework",
        "authors": [
            "Yu Zhao",
            "Xinshuo Hu",
            "Yunxin Li",
            "Baotian Hu",
            "Dongfang Li",
            "Sichao Chen",
            "Xiaolong Wang"
        ],
        "published": "2022-06-17T08:38:53Z",
        "summary": "Dialog systems have achieved significant progress and have been widely used\nin various scenarios. The previous researches mainly focused on designing\ndialog generation models in a single scenario, while comprehensive abilities\nare required to handle tasks under various scenarios in the real world. In this\npaper, we propose a general Multi-Skill Dialog Framework, namely MSDF, which\ncan be applied in different dialog tasks (e.g. knowledge grounded dialog and\npersona based dialog). Specifically, we propose a transferable response\ngenerator pre-trained on diverse large-scale dialog corpora as the backbone of\nMSDF, consisting of BERT-based encoders and a GPT-based decoder. To select the\nresponse consistent with dialog history, we propose a consistency selector\ntrained through negative sampling. Moreover, the flexible copy mechanism of\nexternal knowledge is also employed to enhance the utilization of multiform\nknowledge in various scenarios. We conduct experiments on knowledge grounded\ndialog, recommendation dialog, and persona based dialog tasks. The experimental\nresults indicate that our MSDF outperforms the baseline models with a large\nmargin. In the Multi-skill Dialog of 2021 Language and Intelligence Challenge,\nour general MSDF won the 3rd prize, which proves our MSDF is effective and\ncompetitive.",
        "pdf_link": "https://arxiv.org/pdf/2206.08626v1.pdf"
    },
    {
        "title": "Methods for Estimating and Improving Robustness of Language Models",
        "authors": [
            "Michal \u0160tef\u00e1nik"
        ],
        "published": "2022-06-16T21:02:53Z",
        "summary": "Despite their outstanding performance, large language models (LLMs) suffer\nnotorious flaws related to their preference for simple, surface-level textual\nrelations over full semantic complexity of the problem. This proposal\ninvestigates a common denominator of this problem in their weak ability to\ngeneralise outside of the training domain. We survey diverse research\ndirections providing estimations of model generalisation ability and find that\nincorporating some of these measures in the training objectives leads to\nenhanced distributional robustness of neural models. Based on these findings,\nwe present future research directions towards enhancing the robustness of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2206.08446v1.pdf"
    },
    {
        "title": "Deep Multi-Task Models for Misogyny Identification and Categorization on Arabic Social Media",
        "authors": [
            "Abdelkader El Mahdaouy",
            "Abdellah El Mekki",
            "Ahmed Oumar",
            "Hajar Mousannif",
            "Ismail Berrada"
        ],
        "published": "2022-06-16T18:54:37Z",
        "summary": "The prevalence of toxic content on social media platforms, such as hate\nspeech, offensive language, and misogyny, presents serious challenges to our\ninterconnected society. These challenging issues have attracted widespread\nattention in Natural Language Processing (NLP) community. In this paper, we\npresent the submitted systems to the first Arabic Misogyny Identification\nshared task. We investigate three multi-task learning models as well as their\nsingle-task counterparts. In order to encode the input text, our models rely on\nthe pre-trained MARBERT language model. The overall obtained results show that\nall our submitted models have achieved the best performances (top three ranked\nsubmissions) in both misogyny identification and categorization tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.08407v1.pdf"
    },
    {
        "title": "Know your audience: specializing grounded language models with listener subtraction",
        "authors": [
            "Aaditya K. Singh",
            "David Ding",
            "Andrew Saxe",
            "Felix Hill",
            "Andrew K. Lampinen"
        ],
        "published": "2022-06-16T17:52:08Z",
        "summary": "Effective communication requires adapting to the idiosyncrasies of each\ncommunicative context--such as the common ground shared with each partner.\nHumans demonstrate this ability to specialize to their audience in many\ncontexts, such as the popular game Dixit. We take inspiration from Dixit to\nformulate a multi-agent image reference game where a (trained) speaker model is\nrewarded for describing a target image such that one (pretrained) listener\nmodel can correctly identify it among distractors, but another listener cannot.\nTo adapt, the speaker must exploit differences in the knowledge it shares with\nthe different listeners. We show that finetuning an attention-based adapter\nbetween a CLIP vision encoder and a large language model in this contrastive,\nmulti-agent setting gives rise to context-dependent natural language\nspecialization from rewards only, without direct supervision. Through\ncontrolled experiments, we show that training a speaker with two listeners that\nperceive differently, using our method, allows the speaker to adapt to the\nidiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the\nspecialization to real-world data. Our experiments demonstrate a method for\nspecializing grounded language models without direct supervision and highlight\nthe interesting research challenges posed by complex multi-agent communication.",
        "pdf_link": "https://arxiv.org/pdf/2206.08349v2.pdf"
    },
    {
        "title": "Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models",
        "authors": [
            "Maribeth Rauh",
            "John Mellor",
            "Jonathan Uesato",
            "Po-Sen Huang",
            "Johannes Welbl",
            "Laura Weidinger",
            "Sumanth Dathathri",
            "Amelia Glaese",
            "Geoffrey Irving",
            "Iason Gabriel",
            "William Isaac",
            "Lisa Anne Hendricks"
        ],
        "published": "2022-06-16T17:28:01Z",
        "summary": "Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2206.08325v2.pdf"
    },
    {
        "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
        "authors": [
            "Zhifu Gao",
            "Shiliang Zhang",
            "Ian McLoughlin",
            "Zhijie Yan"
        ],
        "published": "2022-06-16T17:24:14Z",
        "summary": "Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.",
        "pdf_link": "https://arxiv.org/pdf/2206.08317v3.pdf"
    },
    {
        "title": "A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures",
        "authors": [
            "Prateek Verma"
        ],
        "published": "2022-06-16T16:57:43Z",
        "summary": "Modeling long-term dependencies for audio signals is a particularly\nchallenging problem, as even small-time scales yield on the order of a hundred\nthousand samples. With the recent advent of Transformers, neural architectures\nbecame good at modeling dependencies over longer time scales, but they suffered\nfrom quadratic constraints to scale them. We propose a generative\nauto-regressive architecture that can model audio waveforms over quite a large\ncontext, greater than 500,000 samples. Our work is adapted to learn time\ndependencies by learning a latent representation by a CNN front-end, and then\nlearning dependencies over these representations using Transformer encoders,\nfully trained end-to-end: thereby allowing to learn representations as it deems\nfit for the next sample. Unlike previous works that compared different time\nscales to show improvement, we use a standard dataset, with the same number of\nparameters/context to show improvements. We achieve a state-of-the-art\nperformance as compared to other approaches such as Wavenet, SaSHMI, and\nSample-RNN on a standard dataset for modeling long-term structure. This work\ngives very exciting direction for the field, given improvements in context\nmodeling that can be scaled with more data, as well as potentially better\nresults by using billions/trillions of parameters.",
        "pdf_link": "https://arxiv.org/pdf/2206.08297v2.pdf"
    },
    {
        "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator",
        "authors": [
            "Hyuhng Joon Kim",
            "Hyunsoo Cho",
            "Junyeob Kim",
            "Taeuk Kim",
            "Kang Min Yoo",
            "Sang-goo Lee"
        ],
        "published": "2022-06-16T10:52:13Z",
        "summary": "Large-scale pre-trained language models (PLMs) are well-known for being\ncapable of solving a task simply by conditioning a few input-label pairs dubbed\ndemonstrations on a prompt without being explicitly tuned for the desired\ndownstream task. Such a process (i.e., in-context learning), however, naturally\nleads to high reliance on the demonstrations which are usually selected from\nexternal datasets. In this paper, we propose self-generated in-context learning\n(SG-ICL), which generates demonstrations for in-context learning from PLM\nitself to minimize the reliance on the external demonstration. We conduct\nexperiments on four different text classification tasks and show SG-ICL\nsignificantly outperforms zero-shot learning and is generally worth\napproximately 0.6 gold training samples. Moreover, our generated demonstrations\nshow more consistent performance with low variance compared to randomly\nselected demonstrations from the training dataset.",
        "pdf_link": "https://arxiv.org/pdf/2206.08082v1.pdf"
    },
    {
        "title": "An Open-Domain QA System for e-Governance",
        "authors": [
            "Radu Ion",
            "Andrei-Marius Avram",
            "Vasile P\u0103i\u015f",
            "Maria Mitrofan",
            "Verginica Barbu Mititelu",
            "Elena Irimia",
            "Valentin Badea"
        ],
        "published": "2022-06-16T10:02:31Z",
        "summary": "The paper presents an open-domain Question Answering system for Romanian,\nanswering COVID-19 related questions. The QA system pipeline involves automatic\nquestion processing, automatic query generation, web searching for the top 10\nmost relevant documents and answer extraction using a fine-tuned BERT model for\nExtractive QA, trained on a COVID-19 data set that we have manually created.\nThe paper will present the QA system and its integration with the Romanian\nlanguage technologies portal RELATE, the COVID-19 data set and different\nevaluations of the QA performance.",
        "pdf_link": "https://arxiv.org/pdf/2206.08046v1.pdf"
    },
    {
        "title": "PreCogIIITH at HinglishEval : Leveraging Code-Mixing Metrics & Language Model Embeddings To Estimate Code-Mix Quality",
        "authors": [
            "Prashant Kodali",
            "Tanmay Sachan",
            "Akshay Goindani",
            "Anmol Goel",
            "Naman Ahuja",
            "Manish Shrivastava",
            "Ponnurangam Kumaraguru"
        ],
        "published": "2022-06-16T08:00:42Z",
        "summary": "Code-Mixing is a phenomenon of mixing two or more languages in a speech event\nand is prevalent in multilingual societies. Given the low-resource nature of\nCode-Mixing, machine generation of code-mixed text is a prevalent approach for\ndata augmentation. However, evaluating the quality of such machine generated\ncode-mixed text is an open problem. In our submission to HinglishEval, a\nshared-task collocated with INLG2022, we attempt to build models factors that\nimpact the quality of synthetically generated code-mix text by predicting\nratings for code-mix quality.",
        "pdf_link": "https://arxiv.org/pdf/2206.07988v1.pdf"
    },
    {
        "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
        "authors": [
            "Andrea Fasoli",
            "Chia-Yu Chen",
            "Mauricio Serrano",
            "Swagath Venkataramani",
            "George Saon",
            "Xiaodong Cui",
            "Brian Kingsbury",
            "Kailash Gopalakrishnan"
        ],
        "published": "2022-06-16T02:17:49Z",
        "summary": "We report on aggressive quantization strategies that greatly accelerate\ninference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit\ninteger representation for both weights and activations and apply Quantization\nAware Training (QAT) to retrain the full model (acoustic encoder and language\nmodel) and achieve near-iso-accuracy. We show that customized quantization\nschemes that are tailored to the local properties of the network are essential\nto achieve good performance while limiting the computational overhead of QAT.\n  Density ratio Language Model fusion has shown remarkable accuracy gains on\nRNN-T workloads but it severely increases the computational cost of inference.\nWe show that our quantization strategies enable using large beam widths for\nhypothesis search while achieving streaming-compatible runtimes and a full\nmodel compression ratio of 7.6$\\times$ compared to the full precision model.\n  Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to\nINT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a\nReal Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03\ntest sets, we retain most of the gains associated with LM fusion, improving the\naverage WER by $>$1.5%.",
        "pdf_link": "https://arxiv.org/pdf/2206.07882v1.pdf"
    },
    {
        "title": "Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals",
        "authors": [
            "Jamell Dacon",
            "Harry Shomer",
            "Shaylynn Crum-Dacon",
            "Jiliang Tang"
        ],
        "published": "2022-06-15T20:14:02Z",
        "summary": "Online discussions, panels, talk page edits, etc., often contain harmful\nconversational content i.e., hate speech, death threats and offensive language,\nespecially towards certain demographic groups. For example, individuals who\nidentify as members of the LGBTQIA+ community and/or BIPOC (Black, Indigenous,\nPeople of Color) are at higher risk for abuse and harassment online. In this\nwork, we first introduce a real-world dataset that will enable us to study and\nunderstand harmful online conversational content. Then, we conduct several\nexploratory data analysis experiments to gain deeper insights from the dataset.\nWe later describe our approach for detecting harmful online Anti-LGBTQIA+\nconversational content, and finally, we implement two baseline machine learning\nmodels (i.e., Support Vector Machine and Logistic Regression), and fine-tune 3\npre-trained large language models (BERT, RoBERTa, and HateBERT). Our findings\nverify that large language models can achieve very promising performance on\ndetecting online Anti-LGBTQIA+ conversational content detection tasks.",
        "pdf_link": "https://arxiv.org/pdf/2207.10032v1.pdf"
    },
    {
        "title": "Emergent Abilities of Large Language Models",
        "authors": [
            "Jason Wei",
            "Yi Tay",
            "Rishi Bommasani",
            "Colin Raffel",
            "Barret Zoph",
            "Sebastian Borgeaud",
            "Dani Yogatama",
            "Maarten Bosma",
            "Denny Zhou",
            "Donald Metzler",
            "Ed H. Chi",
            "Tatsunori Hashimoto",
            "Oriol Vinyals",
            "Percy Liang",
            "Jeff Dean",
            "William Fedus"
        ],
        "published": "2022-06-15T17:32:01Z",
        "summary": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.",
        "pdf_link": "https://arxiv.org/pdf/2206.07682v2.pdf"
    },
    {
        "title": "Estimating Confidence of Predictions of Individual Classifiers and Their Ensembles for the Genre Classification Task",
        "authors": [
            "Mikhail Lepekhin",
            "Serge Sharoff"
        ],
        "published": "2022-06-15T09:59:05Z",
        "summary": "Genre identification is a subclass of non-topical text classification. The\nmain difference between this task and topical classification is that genres,\nunlike topics, usually do not correspond to simple keywords, and thus they need\nto be defined in terms of their functions in communication. Neural models based\non pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA\nresults in many NLP tasks, including non-topical classification. However, in\nmany cases, their downstream application to very large corpora, such as those\nextracted from social media, can lead to unreliable results because of dataset\nshifts, when some raw texts do not match the profile of the training set. To\nmitigate this problem, we experiment with individual models as well as with\ntheir ensembles. To evaluate the robustness of all models we use a prediction\nconfidence metric, which estimates the reliability of a prediction in the\nabsence of a gold standard label. We can evaluate robustness via the confidence\ngap between the correctly classified texts and the misclassified ones on a\nlabeled test corpus, higher gaps make it easier to improve our confidence that\nour classifier made the right decision. Our results show that for all of the\nclassifiers tested in this study, there is a confidence gap, but for the\nensembles, the gap is bigger, meaning that ensembles are more robust than their\nindividual models.",
        "pdf_link": "https://arxiv.org/pdf/2206.07427v1.pdf"
    },
    {
        "title": "A Survey : Neural Networks for AMR-to-Text",
        "authors": [
            "Hongyu Hao",
            "Guangtong Li",
            "Zhiming Hu",
            "Huafeng Wang"
        ],
        "published": "2022-06-15T07:20:28Z",
        "summary": "AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2206.07328v1.pdf"
    },
    {
        "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt",
        "authors": [
            "S\u00f6ren Mindermann",
            "Jan Brauner",
            "Muhammed Razzak",
            "Mrinank Sharma",
            "Andreas Kirsch",
            "Winnie Xu",
            "Benedikt H\u00f6ltgen",
            "Aidan N. Gomez",
            "Adrien Morisot",
            "Sebastian Farquhar",
            "Yarin Gal"
        ],
        "published": "2022-06-14T19:49:52Z",
        "summary": "Training on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.",
        "pdf_link": "https://arxiv.org/pdf/2206.07137v3.pdf"
    },
    {
        "title": "SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features",
        "authors": [
            "Juri Opitz",
            "Anette Frank"
        ],
        "published": "2022-06-14T17:37:18Z",
        "summary": "Models based on large-pretrained language models, such as S(entence)BERT,\nprovide effective and efficient sentence embeddings that show high correlation\nto human similarity ratings, but lack interpretability. On the other hand,\ngraph metrics for graph-based meaning representations (e.g., Abstract Meaning\nRepresentation, AMR) can make explicit the semantic aspects in which two\nsentences are similar. However, such metrics tend to be slow, rely on parsers,\nand do not reach state-of-the-art performance when rating sentence similarity.\n  In this work, we aim at the best of both worlds, by learning to induce\n$S$emantically $S$tructured $S$entence BERT embeddings (S$^3$BERT). Our\nS$^3$BERT embeddings are composed of explainable sub-embeddings that emphasize\nvarious semantic sentence features (e.g., semantic roles, negation, or\nquantification). We show how to i) learn a decomposition of the sentence\nembeddings into semantic features, through approximation of a suite of\ninterpretable AMR graph metrics, and how to ii) preserve the overall power of\nthe neural embeddings by controlling the decomposition learning process with a\nsecond objective that enforces consistency with the similarity ratings of an\nSBERT teacher model. In our experimental studies, we show that our approach\noffers interpretability -- while fully preserving the effectiveness and\nefficiency of the neural sentence embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2206.07023v2.pdf"
    },
    {
        "title": "RDU: A Region-based Approach to Form-style Document Understanding",
        "authors": [
            "Fengbin Zhu",
            "Chao Wang",
            "Wenqiang Lei",
            "Ziyang Liu",
            "Tat Seng Chua"
        ],
        "published": "2022-06-14T14:47:48Z",
        "summary": "Key Information Extraction (KIE) is aimed at extracting structured\ninformation (e.g. key-value pairs) from form-style documents (e.g. invoices),\nwhich makes an important step towards intelligent document understanding.\nPrevious approaches generally tackle KIE by sequence tagging, which faces\ndifficulty to process non-flatten sequences, especially for table-text mixed\ndocuments. These approaches also suffer from the trouble of pre-defining a\nfixed set of labels for each type of documents, as well as the label imbalance\nissue. In this work, we assume Optical Character Recognition (OCR) has been\napplied to input documents, and reformulate the KIE task as a region prediction\nproblem in the two-dimensional (2D) space given a target field. Following this\nnew setup, we develop a new KIE model named Region-based Document Understanding\n(RDU) that takes as input the text content and corresponding coordinates of a\ndocument, and tries to predict the result by localizing a bounding-box-like\nregion. Our RDU first applies a layout-aware BERT equipped with a soft layout\nattention masking and bias mechanism to incorporate layout information into the\nrepresentations. Then, a list of candidate regions is generated from the\nrepresentations via a Region Proposal Module inspired by computer vision models\nwidely applied for object detection. Finally, a Region Categorization Module\nand a Region Selection Module are adopted to judge whether a proposed region is\nvalid and select the one with the largest probability from all proposed regions\nrespectively. Experiments on four types of form-style documents show that our\nproposed method can achieve impressive results. In addition, our RDU model can\nbe trained with different document types seamlessly, which is especially\nhelpful over low-resource documents.",
        "pdf_link": "https://arxiv.org/pdf/2206.06890v1.pdf"
    },
    {
        "title": "CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation",
        "authors": [
            "Daoguang Zan",
            "Bei Chen",
            "Dejian Yang",
            "Zeqi Lin",
            "Minsu Kim",
            "Bei Guan",
            "Yongji Wang",
            "Weizhu Chen",
            "Jian-Guang Lou"
        ],
        "published": "2022-06-14T14:44:34Z",
        "summary": "Code generation is a longstanding challenge, aiming to generate a code\nsnippet based on a natural language description. Usually, expensive text-code\npaired data is essential for training a code generation model. Recently, thanks\nto the success of pre-training techniques, large language models are trained on\nlarge-scale unlabelled code corpora and perform well in code generation. In\nthis paper, we investigate how to leverage an unlabelled code corpus to train a\nmodel for library-oriented code generation. Since it is a common practice for\nprogrammers to reuse third-party libraries, in which case the text-code paired\ndata are harder to obtain due to the huge number of libraries. We observe that\nlibrary-oriented code snippets are more likely to share similar code sketches.\nHence, we present CERT with two steps: a sketcher generates the sketch, then a\ngenerator fills the details in the sketch. Both the sketcher and the generator\nare continually pre-trained upon a base model using unlabelled data.\nFurthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate\nlibrary-oriented code generation. Experimental results demonstrate the\nimpressive performance of CERT. For example, it surpasses the base model by an\nabsolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is\navailable at https://github.com/microsoft/PyCodeGPT.",
        "pdf_link": "https://arxiv.org/pdf/2206.06888v1.pdf"
    },
    {
        "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning",
        "authors": [
            "Yi-Lin Sung",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "published": "2022-06-13T23:51:56Z",
        "summary": "Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that can reduce training memory requirements by\nmore substantial amounts. Unlike existing parameter-efficient methods that\ninsert additional parameters inside backbone networks, we train a ladder side\nnetwork, a small and separate network that takes intermediate activations as\ninput via shortcut connections (called ladders) from backbone networks and\nmakes predictions. LST has significantly lower memory requirements than\nprevious methods, because it does not require backpropagation through the\nbackbone network, but instead only through the side network and ladder\nconnections. We evaluate our method with various models (T5 and CLIP-T5) on\nboth NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST\nsaves 69% of the memory costs to fine-tune the whole network, while other\nmethods only save 26% of that in similar parameter usages (hence, 2.7x more\nmemory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA\nin a low-memory regime. To further show the advantage of this better memory\nefficiency, we also apply LST to larger T5 models, attaining better GLUE\nperformance than full fine-tuning and other PETL methods. The\naccuracy-efficiency trade-off also holds on VL tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.06522v2.pdf"
    },
    {
        "title": "Memory-Based Model Editing at Scale",
        "authors": [
            "Eric Mitchell",
            "Charles Lin",
            "Antoine Bosselut",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2022-06-13T23:40:34Z",
        "summary": "Even the largest neural networks make errors, and once-correct predictions\ncan become invalid as the world changes. Model editors make local updates to\nthe behavior of base (pre-trained) models to inject updated knowledge or\ncorrect undesirable behaviors. Existing model editors have shown promise, but\nalso suffer from insufficient expressiveness: they struggle to accurately model\nan edit's intended scope (examples affected by the edit), leading to inaccurate\npredictions for test inputs loosely related to the edit, and they often fail\naltogether after many edits. As a higher-capacity alternative, we propose\nSemi-Parametric Editing with a Retrieval-Augmented Counterfactual Model\n(SERAC), which stores edits in an explicit memory and learns to reason over\nthem to modulate the base model's predictions as needed. To enable more\nrigorous evaluation of model editors, we introduce three challenging language\nmodel editing problems based on question answering, fact-checking, and dialogue\ngeneration. We find that only SERAC achieves high performance on all three\nproblems, consistently outperforming existing approaches to model editing by a\nsignificant margin. Code, data, and additional project information will be made\navailable at https://sites.google.com/view/serac-editing.",
        "pdf_link": "https://arxiv.org/pdf/2206.06520v1.pdf"
    },
    {
        "title": "Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training",
        "authors": [
            "Charbel Sakr",
            "Steve Dai",
            "Rangharajan Venkatesan",
            "Brian Zimmer",
            "William J. Dally",
            "Brucek Khailany"
        ],
        "published": "2022-06-13T22:15:21Z",
        "summary": "Data clipping is crucial in reducing noise in quantization operations and\nimproving the achievable accuracy of quantization-aware training (QAT). Current\npractices rely on heuristics to set clipping threshold scalars and cannot be\nshown to be optimal. We propose Optimally Clipped Tensors And Vectors (OCTAV),\na recursive algorithm to determine MSE-optimal clipping scalars. Derived from\nthe fast Newton-Raphson method, OCTAV finds optimal clipping scalars on the\nfly, for every tensor, at every iteration of the QAT routine. Thus, the QAT\nalgorithm is formulated with provably minimum quantization noise at each step.\nIn addition, we reveal limitations in common gradient estimation techniques in\nQAT and propose magnitude-aware differentiation as a remedy to further improve\naccuracy. Experimentally, OCTAV-enabled QAT achieves state-of-the-art accuracy\non multiple tasks. These include training-from-scratch and retraining ResNets\nand MobileNets on ImageNet, and Squad fine-tuning using BERT models, where\nOCTAV-enabled QAT consistently preserves accuracy at low precision\n(4-to-6-bits). Our results require no modifications to the baseline training\nrecipe, except for the insertion of quantization operations where appropriate.",
        "pdf_link": "https://arxiv.org/pdf/2206.06501v1.pdf"
    },
    {
        "title": "Language Models are General-Purpose Interfaces",
        "authors": [
            "Yaru Hao",
            "Haoyu Song",
            "Li Dong",
            "Shaohan Huang",
            "Zewen Chi",
            "Wenhui Wang",
            "Shuming Ma",
            "Furu Wei"
        ],
        "published": "2022-06-13T17:34:22Z",
        "summary": "Foundation models have received much attention due to their effectiveness\nacross a broad range of downstream applications. Though there is a big\nconvergence in terms of architecture, most pretrained models are typically\nstill developed for specific tasks or modalities. In this work, we propose to\nuse language models as a general-purpose interface to various foundation\nmodels. A collection of pretrained encoders perceive diverse modalities (such\nas vision, and language), and they dock with a language model that plays the\nrole of a universal task layer. We propose a semi-causal language modeling\nobjective to jointly pretrain the interface and the modular encoders. We\nsubsume the advantages and capabilities from both causal and non-causal\nmodeling, thereby combining the best of two worlds. Specifically, the proposed\nmethod not only inherits the capabilities of in-context learning and open-ended\ngeneration from causal language modeling, but also is conducive to finetuning\nbecause of the bidirectional encoders. More importantly, our approach\nseamlessly unlocks the combinations of the above capabilities, e.g., enabling\nin-context learning or instruction following with finetuned encoders.\nExperimental results across various language-only and vision-language\nbenchmarks show that our model outperforms or is competitive with specialized\nmodels on finetuning, zero-shot generalization, and few-shot learning.",
        "pdf_link": "https://arxiv.org/pdf/2206.06336v1.pdf"
    },
    {
        "title": "JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding",
        "authors": [
            "Wayne Xin Zhao",
            "Kun Zhou",
            "Zheng Gong",
            "Beichen Zhang",
            "Yuanhang Zhou",
            "Jing Sha",
            "Zhigang Chen",
            "Shijin Wang",
            "Cong Liu",
            "Ji-Rong Wen"
        ],
        "published": "2022-06-13T17:03:52Z",
        "summary": "This paper aims to advance the mathematical intelligence of machines by\npresenting the first Chinese mathematical pre-trained language model~(PLM) for\neffectively understanding and representing mathematical problems. Unlike other\nstandard NLP tasks, mathematical texts are difficult to understand, since they\ninvolve mathematical terminology, symbols and formulas in the problem\nstatement. Typically, it requires complex mathematical logic and background\nknowledge for solving mathematical problems.\n  Considering the complex nature of mathematical texts, we design a novel\ncurriculum pre-training approach for improving the learning of mathematical\nPLMs, consisting of both basic and advanced courses. Specially, we first\nperform token-level pre-training based on a position-biased masking strategy,\nand then design logic-based pre-training tasks that aim to recover the shuffled\nsentences and formulas, respectively. Finally, we introduce a more difficult\npre-training task that enforces the PLM to detect and correct the errors in its\ngenerated solutions. We conduct extensive experiments on offline evaluation\n(including nine math-related tasks) and online $A/B$ test. Experimental results\ndemonstrate the effectiveness of our approach compared with a number of\ncompetitive baselines. Our code is available at:\n\\textcolor{blue}{\\url{https://github.com/RUCAIBox/JiuZhang}}.",
        "pdf_link": "https://arxiv.org/pdf/2206.06315v1.pdf"
    },
    {
        "title": "Transition-based Abstract Meaning Representation Parsing with Contextual Embeddings",
        "authors": [
            "Yichao Liang"
        ],
        "published": "2022-06-13T15:05:24Z",
        "summary": "The ability to understand and generate languages sets human cognition apart\nfrom other known life forms'. We study a way of combing two of the most\nsuccessful routes to meaning of language--statistical language models and\nsymbolic semantics formalisms--in the task of semantic parsing. Building on a\ntransition-based, Abstract Meaning Representation (AMR) parser, AmrEager, we\nexplore the utility of incorporating pretrained context-aware word\nembeddings--such as BERT and RoBERTa--in the problem of AMR parsing,\ncontributing a new parser we dub as AmrBerger. Experiments find these rich\nlexical features alone are not particularly helpful in improving the parser's\noverall performance as measured by the SMATCH score when compared to the\nnon-contextual counterpart, while additional concept information empowers the\nsystem to outperform the baselines. Through lesion study, we found the use of\ncontextual embeddings helps to make the system more robust against the removal\nof explicit syntactical features. These findings expose the strength and\nweakness of the contextual embeddings and the language models in the current\nform, and motivate deeper understanding thereof.",
        "pdf_link": "https://arxiv.org/pdf/2206.06229v1.pdf"
    },
    {
        "title": "Self-critiquing models for assisting human evaluators",
        "authors": [
            "William Saunders",
            "Catherine Yeh",
            "Jeff Wu",
            "Steven Bills",
            "Long Ouyang",
            "Jonathan Ward",
            "Jan Leike"
        ],
        "published": "2022-06-12T17:40:53Z",
        "summary": "We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a topic-based\nsummarization task, critiques written by our models help humans find flaws in\nsummaries that they would have otherwise missed. Our models help find naturally\noccurring flaws in both model and human written summaries, and intentional\nflaws in summaries written by humans to be deliberately misleading. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Larger models write more helpful critiques, and on most tasks,\nare better at self-critiquing, despite having harder-to-critique outputs.\nLarger models can also integrate their own self-critiques as feedback, refining\ntheir own summaries into better ones. Finally, we motivate and introduce a\nframework for comparing critiquing ability to generation and discrimination\nability. Our measurements suggest that even large models may still have\nrelevant knowledge they cannot or do not articulate as critiques. These results\nare a proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets, as well as samples from\nour critique assistance experiments.",
        "pdf_link": "https://arxiv.org/pdf/2206.05802v2.pdf"
    },
    {
        "title": "Improving Pre-trained Language Model Fine-tuning with Noise Stability Regularization",
        "authors": [
            "Hang Hua",
            "Xingjian Li",
            "Dejing Dou",
            "Cheng-Zhong Xu",
            "Jiebo Luo"
        ],
        "published": "2022-06-12T04:42:49Z",
        "summary": "The advent of large-scale pre-trained language models has contributed greatly\nto the recent progress in natural language processing. Many state-of-the-art\nlanguage models are first trained on a large text corpus and then fine-tuned on\ndownstream tasks. Despite its recent success and wide adoption, fine-tuning a\npre-trained language model often suffers from overfitting, which leads to poor\ngeneralizability due to the extremely high complexity of the model and the\nlimited training samples from downstream tasks. To address this problem, we\npropose a novel and effective fine-tuning framework, named Layerwise Noise\nStability Regularization (LNSR). Specifically, we propose to inject the\nstandard Gaussian noise or In-manifold noise and regularize hidden\nrepresentations of the fine-tuned model. We first provide theoretical analyses\nto support the efficacy of our method. We then demonstrate the advantages of\nthe proposed method over other state-of-the-art algorithms including L2-SP,\nMixout and SMART. While these previous works only verify the effectiveness of\ntheir methods on relatively simple text classification tasks, we also verify\nthe effectiveness of our method on question answering tasks, where the target\nproblem is much more difficult and more training examples are available.\nFurthermore, extensive experimental results indicate that the proposed\nalgorithm can not only enhance the in-domain performance of the language models\nbut also improve the domain generalization performance on out-of-domain data.",
        "pdf_link": "https://arxiv.org/pdf/2206.05658v2.pdf"
    },
    {
        "title": "Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies",
        "authors": [
            "Souradip Chakraborty",
            "Amrit Singh Bedi",
            "Alec Koppel",
            "Pratap Tokekar",
            "Dinesh Manocha"
        ],
        "published": "2022-06-12T04:09:39Z",
        "summary": "In this paper, we present a novel Heavy-Tailed Stochastic Policy Gradient\n(HT-PSG) algorithm to deal with the challenges of sparse rewards in continuous\ncontrol problems. Sparse reward is common in continuous control robotics tasks\nsuch as manipulation and navigation, and makes the learning problem hard due to\nnon-trivial estimation of value functions over the state space. This demands\neither reward shaping or expert demonstrations for the sparse reward\nenvironment. However, obtaining high-quality demonstrations is quite expensive\nand sometimes even impossible. We propose a heavy-tailed policy parametrization\nalong with a modified momentum-based policy gradient tracking scheme (HT-SPG)\nto induce a stable exploratory behavior to the algorithm. The proposed\nalgorithm does not require access to expert demonstrations. We test the\nperformance of HT-SPG on various benchmark tasks of continuous control with\nsparse rewards such as 1D Mario, Pathological Mountain Car, Sparse Pendulum in\nOpenAI Gym, and Sparse MuJoCo environments (Hopper-v2). We show consistent\nperformance improvement across all tasks in terms of high average cumulative\nreward. HT-SPG also demonstrates improved convergence speed with minimum\nsamples, thereby emphasizing the sample efficiency of our proposed algorithm.",
        "pdf_link": "https://arxiv.org/pdf/2206.05652v1.pdf"
    },
    {
        "title": "DeepEmotex: Classifying Emotion in Text Messages using Deep Transfer Learning",
        "authors": [
            "Maryam Hasan",
            "Elke Rundensteiner",
            "Emmanuel Agu"
        ],
        "published": "2022-06-12T03:23:40Z",
        "summary": "Transfer learning has been widely used in natural language processing through\ndeep pretrained language models, such as Bidirectional Encoder Representations\nfrom Transformers and Universal Sentence Encoder. Despite the great success,\nlanguage models get overfitted when applied to small datasets and are prone to\nforgetting when fine-tuned with a classifier. To remedy this problem of\nforgetting in transferring deep pretrained language models from one domain to\nanother domain, existing efforts explore fine-tuning methods to forget less. We\npropose DeepEmotex an effective sequential transfer learning method to detect\nemotion in text. To avoid forgetting problem, the fine-tuning step is\ninstrumented by a large amount of emotion-labeled data collected from Twitter.\nWe conduct an experimental study using both curated Twitter data sets and\nbenchmark data sets. DeepEmotex models achieve over 91% accuracy for\nmulti-class emotion classification on test dataset. We evaluate the performance\nof the fine-tuned DeepEmotex models in classifying emotion in EmoInt and\nStimulus benchmark datasets. The models correctly classify emotion in 73% of\nthe instances in the benchmark datasets. The proposed DeepEmotex-BERT model\noutperforms Bi-LSTM result on the benchmark datasets by 23%. We also study the\neffect of the size of the fine-tuning dataset on the accuracy of our models.\nOur evaluation results show that fine-tuning with a large set of\nemotion-labeled data improves both the robustness and effectiveness of the\nresulting target task model.",
        "pdf_link": "https://arxiv.org/pdf/2206.06775v1.pdf"
    },
    {
        "title": "Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models",
        "authors": [
            "Han Liu",
            "Bingning Wang",
            "Ting Yao",
            "Haijin Liang",
            "Jianjin Xu",
            "Xiaolin Hu"
        ],
        "published": "2022-06-11T12:52:32Z",
        "summary": "Large-scale pre-trained language models have achieved great success on\nnatural language generation tasks. However, it is difficult to control the\npre-trained language models to generate sentences with the desired attribute\nsuch as topic and sentiment, etc. Recently, Bayesian Controllable Language\nModels (BCLMs) have been shown to be efficient in controllable language\ngeneration. Rather than fine-tuning the parameters of pre-trained language\nmodels, BCLMs use external discriminators to guide the generation of\npre-trained language models. However, the mismatch between training and\ninference of BCLMs limits the performance of the models. To address the\nproblem, in this work we propose a \"Gemini Discriminator\" for controllable\nlanguage generation which alleviates the mismatch problem with a small\ncomputational cost. We tested our method on two controllable language\ngeneration tasks: sentiment control and topic control. On both tasks, our\nmethod reached achieved new state-of-the-art results in automatic and human\nevaluations.",
        "pdf_link": "https://arxiv.org/pdf/2206.05519v1.pdf"
    },
    {
        "title": "Comparative Snippet Generation",
        "authors": [
            "Saurabh Jain",
            "Yisong Miao",
            "Min-Yen Kan"
        ],
        "published": "2022-06-11T09:02:27Z",
        "summary": "We model product reviews to generate comparative responses consisting of\npositive and negative experiences regarding the product. Specifically, we\ngenerate a single-sentence, comparative response from a given positive and a\nnegative opinion. We contribute the first dataset for this task of Comparative\nSnippet Generation from contrasting opinions regarding a product, and a\nperformance analysis of a pre-trained BERT model to generate such snippets.",
        "pdf_link": "https://arxiv.org/pdf/2206.05473v1.pdf"
    },
    {
        "title": "From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams",
        "authors": [
            "Iddo Drori",
            "Sarah J. Zhang",
            "Reece Shuttleworth",
            "Sarah Zhang",
            "Keith Tyser",
            "Zad Chin",
            "Pedro Lantigua",
            "Saisamrit Surbehera",
            "Gregory Hunter",
            "Derek Austin",
            "Leonard Tang",
            "Yann Hicke",
            "Sage Simhon",
            "Sathwik Karnik",
            "Darnell Granberry",
            "Madeleine Udell"
        ],
        "published": "2022-06-11T06:38:06Z",
        "summary": "A final exam in machine learning at a top institution such as MIT, Harvard,\nor Cornell typically takes faculty days to write, and students hours to solve.\nWe demonstrate that large language models pass machine learning finals at a\nhuman level, on finals available online after the models were trained, and\nautomatically generate new human-quality final exam questions in seconds.\nPrevious work has developed program synthesis and few-shot learning methods to\nsolve university-level problem set questions in mathematics and STEM courses.\nIn this work, we develop and compare methods that solve final exams, which\ndiffer from problem sets in several ways: the questions are longer, have\nmultiple parts, are more complicated, and span a broader set of topics. We\ncurate a dataset and benchmark of questions from machine learning final exams\navailable online and code for answering these questions and generating new\nquestions. We show how to generate new questions from other questions and\ncourse notes. For reproducibility and future research on this final exam\nbenchmark, we use automatic checkers for multiple-choice, numeric, and\nquestions with expression answers. We perform ablation studies comparing\nzero-shot learning with few-shot learning and chain-of-thought prompting using\nGPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that\nfew-shot learning methods perform best. We highlight the transformative\npotential of language models to streamline the writing and solution of\nlarge-scale assessments, significantly reducing the workload from human days to\nmere machine seconds. Our results suggest that rather than banning large\nlanguage models such as ChatGPT in class, instructors should teach students to\nharness them by asking students meta-questions about correctness, completeness,\nand originality of the responses generated, encouraging critical thinking in\nacademic studies.",
        "pdf_link": "https://arxiv.org/pdf/2206.05442v7.pdf"
    },
    {
        "title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "authors": [
            "Jesse Dodge",
            "Taylor Prewitt",
            "Remi Tachet Des Combes",
            "Erika Odmark",
            "Roy Schwartz",
            "Emma Strubell",
            "Alexandra Sasha Luccioni",
            "Noah A. Smith",
            "Nicole DeCario",
            "Will Buchanan"
        ],
        "published": "2022-06-10T17:04:04Z",
        "summary": "By providing unprecedented access to computational resources, cloud computing\nhas enabled rapid growth in technologies such as machine learning, the\ncomputational demands of which incur a high energy cost and a commensurate\ncarbon footprint. As a result, recent scholarship has called for better\nestimates of the greenhouse gas impact of AI: data scientists today do not have\neasy or reliable access to measurements of this information, precluding\ndevelopment of actionable tactics. Cloud providers presenting information about\nsoftware carbon intensity to users is a fundamental stepping stone towards\nminimizing emissions. In this paper, we provide a framework for measuring\nsoftware carbon intensity, and propose to measure operational carbon emissions\nby using location-based and time-specific marginal emissions data per energy\nunit. We provide measurements of operational software carbon intensity for a\nset of modern models for natural language processing and computer vision, and a\nwide range of model sizes, including pretraining of a 6.1 billion parameter\nlanguage model. We then evaluate a suite of approaches for reducing emissions\non the Microsoft Azure cloud compute platform: using cloud instances in\ndifferent geographic regions, using cloud instances at different times of day,\nand dynamically pausing cloud instances when the marginal carbon intensity is\nabove a certain threshold. We confirm previous results that the geographic\nregion of the data center plays a significant role in the carbon intensity for\na given cloud instance, and find that choosing an appropriate region can have\nthe largest operational emissions reduction impact. We also show that the time\nof day has notable impact on operational software carbon intensity. Finally, we\nconclude with recommendations for how machine learning practitioners can use\nsoftware carbon intensity information to reduce environmental impact.",
        "pdf_link": "https://arxiv.org/pdf/2206.05229v1.pdf"
    },
    {
        "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction",
        "authors": [
            "Wonseok Hwang",
            "Dongjun Lee",
            "Kyoungyeon Cho",
            "Hanuhl Lee",
            "Minjoon Seo"
        ],
        "published": "2022-06-10T16:51:45Z",
        "summary": "The recent advances of deep learning have dramatically changed how machine\nlearning, especially in the domain of natural language processing, can be\napplied to legal domain. However, this shift to the data-driven approaches\ncalls for larger and more diverse datasets, which are nevertheless still small\nin number, especially in non-English languages. Here we present the first\nlarge-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of\none legal corpus, two classification tasks, two legal judgement prediction\n(LJP) tasks, and one summarization task. The legal corpus consists of 147k\nKorean precedents (259M tokens), of which 63k are sentenced in last 4 years and\n96k are from the first and the second level courts in which factual issues are\nreviewed. The two classification tasks are case names (11.3k) and statutes\n(2.8k) prediction from the factual description of individual cases. The LJP\ntasks consist of (1) 10.5k criminal examples where the model is asked to\npredict fine amount, imprisonment with labor, and imprisonment without labor\nranges for the given facts, and (2) 4.7k civil examples where the inputs are\nfacts and claim for relief and outputs are the degrees of claim acceptance. The\nsummarization task consists of the Supreme Court precedents and the\ncorresponding summaries (20k). We also release realistic variants of the\ndatasets by extending the domain (1) to infrequent case categories in case name\n(31k examples) and statute (17.7k) classification tasks, and (2) to long input\nsequences in the summarization task (51k). Finally, we release LCUBE, the first\nKorean legal language model trained on the legal corpus from this study. Given\nthe uniqueness of the Law of South Korea and the diversity of the legal tasks\ncovered in this work, we believe that LBOX OPEN contributes to the\nmultilinguality of global legal research. LBOX OPEN and LCUBE will be publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2206.05224v2.pdf"
    },
    {
        "title": "Putting GPT-3's Creativity to the (Alternative Uses) Test",
        "authors": [
            "Claire Stevenson",
            "Iris Smal",
            "Matthijs Baas",
            "Raoul Grasman",
            "Han van der Maas"
        ],
        "published": "2022-06-10T15:36:45Z",
        "summary": "AI large language models have (co-)produced amazing written works from\nnewspaper articles to novels and poetry. These works meet the standards of the\nstandard definition of creativity: being original and useful, and sometimes\neven the additional element of surprise. But can a large language model\ndesigned to predict the next text fragment provide creative, out-of-the-box,\nresponses that still solve the problem at hand? We put Open AI's generative\nnatural language model, GPT-3, to the test. Can it provide creative solutions\nto one of the most commonly used tests in creativity research? We assessed\nGPT-3's creativity on Guilford's Alternative Uses Test and compared its\nperformance to previously collected human responses on expert ratings of\noriginality, usefulness and surprise of responses, flexibility of each set of\nideas as well as an automated method to measure creativity based on the\nsemantic distance between a response and the AUT object in question. Our\nresults show that -- on the whole -- humans currently outperform GPT-3 when it\ncomes to creative output. But, we believe it is only a matter of time before\nGPT-3 catches up on this particular task. We discuss what this work reveals\nabout human and AI creativity, creativity testing and our definition of\ncreativity.",
        "pdf_link": "https://arxiv.org/pdf/2206.08932v1.pdf"
    },
    {
        "title": "Unsupervised and Few-shot Parsing from Pretrained Language Models",
        "authors": [
            "Zhiyuan Zeng",
            "Deyi Xiong"
        ],
        "published": "2022-06-10T10:29:15Z",
        "summary": "Pretrained language models are generally acknowledged to be able to encode\nsyntax [Tenney et al., 2019, Jawahar et al., 2019, Hewitt and Manning, 2019].\nIn this article, we propose UPOA, an Unsupervised constituent Parsing model\nthat calculates an Out Association score solely based on the self-attention\nweight matrix learned in a pretrained language model as the syntactic distance\nfor span segmentation. We further propose an enhanced version, UPIO, which\nexploits both inside association and outside association scores for estimating\nthe likelihood of a span. Experiments with UPOA and UPIO disclose that the\nlinear projection matrices for the query and key in the self-attention\nmechanism play an important role in parsing. We therefore extend the\nunsupervised models to few-shot parsing models (FPOA, FPIO) that use a few\nannotated trees to learn better linear projection matrices for parsing.\nExperiments on the Penn Treebank demonstrate that our unsupervised parsing\nmodel UPIO achieves results comparable to the state of the art on short\nsentences (length <= 10). Our few-shot parsing model FPIO trained with only 20\nannotated trees outperforms a previous few-shot parsing method trained with 50\nannotated trees. Experiments on cross-lingual parsing show that both\nunsupervised and few-shot parsing methods are better than previous methods on\nmost languages of SPMRL [Seddah et al., 2013].",
        "pdf_link": "https://arxiv.org/pdf/2206.04980v1.pdf"
    },
    {
        "title": "Sort by Structure: Language Model Ranking as Dependency Probing",
        "authors": [
            "Max M\u00fcller-Eberstein",
            "Rob van der Goot",
            "Barbara Plank"
        ],
        "published": "2022-06-10T08:10:29Z",
        "summary": "Making an informed choice of pre-trained language model (LM) is critical for\nperformance, yet environmentally costly, and as such widely underexplored. The\nfield of Computer Vision has begun to tackle encoder ranking, with promising\nforays into Natural Language Processing, however they lack coverage of\nlinguistic tasks such as structured prediction. We propose probing to rank LMs,\nspecifically for parsing dependencies in a given language, by measuring the\ndegree to which labeled trees are recoverable from an LM's contextualized\nembeddings. Across 46 typologically and architecturally diverse LM-language\npairs, our probing approach predicts the best LM choice 79% of the time using\norders of magnitude less compute than training a full parser. Within this\nstudy, we identify and analyze one recently proposed decoupled LM - RemBERT -\nand find it strikingly contains less inherent dependency information, but often\nyields the best parser after full fine-tuning. Without this outlier our\napproach identifies the best LM in 89% of cases.",
        "pdf_link": "https://arxiv.org/pdf/2206.04935v1.pdf"
    },
    {
        "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
        "authors": [
            "Aarohi Srivastava",
            "Abhinav Rastogi",
            "Abhishek Rao",
            "Abu Awal Md Shoeb",
            "Abubakar Abid",
            "Adam Fisch",
            "Adam R. Brown",
            "Adam Santoro",
            "Aditya Gupta",
            "Adri\u00e0 Garriga-Alonso",
            "Agnieszka Kluska",
            "Aitor Lewkowycz",
            "Akshat Agarwal",
            "Alethea Power",
            "Alex Ray",
            "Alex Warstadt",
            "Alexander W. Kocurek",
            "Ali Safaya",
            "Ali Tazarv",
            "Alice Xiang",
            "Alicia Parrish",
            "Allen Nie",
            "Aman Hussain",
            "Amanda Askell",
            "Amanda Dsouza",
            "Ambrose Slone",
            "Ameet Rahane",
            "Anantharaman S. Iyer",
            "Anders Andreassen",
            "Andrea Madotto",
            "Andrea Santilli",
            "Andreas Stuhlm\u00fcller",
            "Andrew Dai",
            "Andrew La",
            "Andrew Lampinen",
            "Andy Zou",
            "Angela Jiang",
            "Angelica Chen",
            "Anh Vuong",
            "Animesh Gupta",
            "Anna Gottardi",
            "Antonio Norelli",
            "Anu Venkatesh",
            "Arash Gholamidavoodi",
            "Arfa Tabassum",
            "Arul Menezes",
            "Arun Kirubarajan",
            "Asher Mullokandov",
            "Ashish Sabharwal",
            "Austin Herrick",
            "Avia Efrat",
            "Aykut Erdem",
            "Ayla Karaka\u015f",
            "B. Ryan Roberts",
            "Bao Sheng Loe",
            "Barret Zoph",
            "Bart\u0142omiej Bojanowski",
            "Batuhan \u00d6zyurt",
            "Behnam Hedayatnia",
            "Behnam Neyshabur",
            "Benjamin Inden",
            "Benno Stein",
            "Berk Ekmekci",
            "Bill Yuchen Lin",
            "Blake Howald",
            "Bryan Orinion",
            "Cameron Diao",
            "Cameron Dour",
            "Catherine Stinson",
            "Cedrick Argueta",
            "C\u00e9sar Ferri Ram\u00edrez",
            "Chandan Singh",
            "Charles Rathkopf",
            "Chenlin Meng",
            "Chitta Baral",
            "Chiyu Wu",
            "Chris Callison-Burch",
            "Chris Waites",
            "Christian Voigt",
            "Christopher D. Manning",
            "Christopher Potts",
            "Cindy Ramirez",
            "Clara E. Rivera",
            "Clemencia Siro",
            "Colin Raffel",
            "Courtney Ashcraft",
            "Cristina Garbacea",
            "Damien Sileo",
            "Dan Garrette",
            "Dan Hendrycks",
            "Dan Kilman",
            "Dan Roth",
            "Daniel Freeman",
            "Daniel Khashabi",
            "Daniel Levy",
            "Daniel Mosegu\u00ed Gonz\u00e1lez",
            "Danielle Perszyk",
            "Danny Hernandez",
            "Danqi Chen",
            "Daphne Ippolito",
            "Dar Gilboa",
            "David Dohan",
            "David Drakard",
            "David Jurgens",
            "Debajyoti Datta",
            "Deep Ganguli",
            "Denis Emelin",
            "Denis Kleyko",
            "Deniz Yuret",
            "Derek Chen",
            "Derek Tam",
            "Dieuwke Hupkes",
            "Diganta Misra",
            "Dilyar Buzan",
            "Dimitri Coelho Mollo",
            "Diyi Yang",
            "Dong-Ho Lee",
            "Dylan Schrader",
            "Ekaterina Shutova",
            "Ekin Dogus Cubuk",
            "Elad Segal",
            "Eleanor Hagerman",
            "Elizabeth Barnes",
            "Elizabeth Donoway",
            "Ellie Pavlick",
            "Emanuele Rodola",
            "Emma Lam",
            "Eric Chu",
            "Eric Tang",
            "Erkut Erdem",
            "Ernie Chang",
            "Ethan A. Chi",
            "Ethan Dyer",
            "Ethan Jerzak",
            "Ethan Kim",
            "Eunice Engefu Manyasi",
            "Evgenii Zheltonozhskii",
            "Fanyue Xia",
            "Fatemeh Siar",
            "Fernando Mart\u00ednez-Plumed",
            "Francesca Happ\u00e9",
            "Francois Chollet",
            "Frieda Rong",
            "Gaurav Mishra",
            "Genta Indra Winata",
            "Gerard de Melo",
            "Germ\u00e1n Kruszewski",
            "Giambattista Parascandolo",
            "Giorgio Mariani",
            "Gloria Wang",
            "Gonzalo Jaimovitch-L\u00f3pez",
            "Gregor Betz",
            "Guy Gur-Ari",
            "Hana Galijasevic",
            "Hannah Kim",
            "Hannah Rashkin",
            "Hannaneh Hajishirzi",
            "Harsh Mehta",
            "Hayden Bogar",
            "Henry Shevlin",
            "Hinrich Sch\u00fctze",
            "Hiromu Yakura",
            "Hongming Zhang",
            "Hugh Mee Wong",
            "Ian Ng",
            "Isaac Noble",
            "Jaap Jumelet",
            "Jack Geissinger",
            "Jackson Kernion",
            "Jacob Hilton",
            "Jaehoon Lee",
            "Jaime Fern\u00e1ndez Fisac",
            "James B. Simon",
            "James Koppel",
            "James Zheng",
            "James Zou",
            "Jan Koco\u0144",
            "Jana Thompson",
            "Janelle Wingfield",
            "Jared Kaplan",
            "Jarema Radom",
            "Jascha Sohl-Dickstein",
            "Jason Phang",
            "Jason Wei",
            "Jason Yosinski",
            "Jekaterina Novikova",
            "Jelle Bosscher",
            "Jennifer Marsh",
            "Jeremy Kim",
            "Jeroen Taal",
            "Jesse Engel",
            "Jesujoba Alabi",
            "Jiacheng Xu",
            "Jiaming Song",
            "Jillian Tang",
            "Joan Waweru",
            "John Burden",
            "John Miller",
            "John U. Balis",
            "Jonathan Batchelder",
            "Jonathan Berant",
            "J\u00f6rg Frohberg",
            "Jos Rozen",
            "Jose Hernandez-Orallo",
            "Joseph Boudeman",
            "Joseph Guerr",
            "Joseph Jones",
            "Joshua B. Tenenbaum",
            "Joshua S. Rule",
            "Joyce Chua",
            "Kamil Kanclerz",
            "Karen Livescu",
            "Karl Krauth",
            "Karthik Gopalakrishnan",
            "Katerina Ignatyeva",
            "Katja Markert",
            "Kaustubh D. Dhole",
            "Kevin Gimpel",
            "Kevin Omondi",
            "Kory Mathewson",
            "Kristen Chiafullo",
            "Ksenia Shkaruta",
            "Kumar Shridhar",
            "Kyle McDonell",
            "Kyle Richardson",
            "Laria Reynolds",
            "Leo Gao",
            "Li Zhang",
            "Liam Dugan",
            "Lianhui Qin",
            "Lidia Contreras-Ochando",
            "Louis-Philippe Morency",
            "Luca Moschella",
            "Lucas Lam",
            "Lucy Noble",
            "Ludwig Schmidt",
            "Luheng He",
            "Luis Oliveros Col\u00f3n",
            "Luke Metz",
            "L\u00fctfi Kerem \u015eenel",
            "Maarten Bosma",
            "Maarten Sap",
            "Maartje ter Hoeve",
            "Maheen Farooqi",
            "Manaal Faruqui",
            "Mantas Mazeika",
            "Marco Baturan",
            "Marco Marelli",
            "Marco Maru",
            "Maria Jose Ram\u00edrez Quintana",
            "Marie Tolkiehn",
            "Mario Giulianelli",
            "Martha Lewis",
            "Martin Potthast",
            "Matthew L. Leavitt",
            "Matthias Hagen",
            "M\u00e1ty\u00e1s Schubert",
            "Medina Orduna Baitemirova",
            "Melody Arnaud",
            "Melvin McElrath",
            "Michael A. Yee",
            "Michael Cohen",
            "Michael Gu",
            "Michael Ivanitskiy",
            "Michael Starritt",
            "Michael Strube",
            "Micha\u0142 Sw\u0119drowski",
            "Michele Bevilacqua",
            "Michihiro Yasunaga",
            "Mihir Kale",
            "Mike Cain",
            "Mimee Xu",
            "Mirac Suzgun",
            "Mitch Walker",
            "Mo Tiwari",
            "Mohit Bansal",
            "Moin Aminnaseri",
            "Mor Geva",
            "Mozhdeh Gheini",
            "Mukund Varma T",
            "Nanyun Peng",
            "Nathan A. Chi",
            "Nayeon Lee",
            "Neta Gur-Ari Krakover",
            "Nicholas Cameron",
            "Nicholas Roberts",
            "Nick Doiron",
            "Nicole Martinez",
            "Nikita Nangia",
            "Niklas Deckers",
            "Niklas Muennighoff",
            "Nitish Shirish Keskar",
            "Niveditha S. Iyer",
            "Noah Constant",
            "Noah Fiedel",
            "Nuan Wen",
            "Oliver Zhang",
            "Omar Agha",
            "Omar Elbaghdadi",
            "Omer Levy",
            "Owain Evans",
            "Pablo Antonio Moreno Casares",
            "Parth Doshi",
            "Pascale Fung",
            "Paul Pu Liang",
            "Paul Vicol",
            "Pegah Alipoormolabashi",
            "Peiyuan Liao",
            "Percy Liang",
            "Peter Chang",
            "Peter Eckersley",
            "Phu Mon Htut",
            "Pinyu Hwang",
            "Piotr Mi\u0142kowski",
            "Piyush Patil",
            "Pouya Pezeshkpour",
            "Priti Oli",
            "Qiaozhu Mei",
            "Qing Lyu",
            "Qinlang Chen",
            "Rabin Banjade",
            "Rachel Etta Rudolph",
            "Raefer Gabriel",
            "Rahel Habacker",
            "Ramon Risco",
            "Rapha\u00ebl Milli\u00e8re",
            "Rhythm Garg",
            "Richard Barnes",
            "Rif A. Saurous",
            "Riku Arakawa",
            "Robbe Raymaekers",
            "Robert Frank",
            "Rohan Sikand",
            "Roman Novak",
            "Roman Sitelew",
            "Ronan LeBras",
            "Rosanne Liu",
            "Rowan Jacobs",
            "Rui Zhang",
            "Ruslan Salakhutdinov",
            "Ryan Chi",
            "Ryan Lee",
            "Ryan Stovall",
            "Ryan Teehan",
            "Rylan Yang",
            "Sahib Singh",
            "Saif M. Mohammad",
            "Sajant Anand",
            "Sam Dillavou",
            "Sam Shleifer",
            "Sam Wiseman",
            "Samuel Gruetter",
            "Samuel R. Bowman",
            "Samuel S. Schoenholz",
            "Sanghyun Han",
            "Sanjeev Kwatra",
            "Sarah A. Rous",
            "Sarik Ghazarian",
            "Sayan Ghosh",
            "Sean Casey",
            "Sebastian Bischoff",
            "Sebastian Gehrmann",
            "Sebastian Schuster",
            "Sepideh Sadeghi",
            "Shadi Hamdan",
            "Sharon Zhou",
            "Shashank Srivastava",
            "Sherry Shi",
            "Shikhar Singh",
            "Shima Asaadi",
            "Shixiang Shane Gu",
            "Shubh Pachchigar",
            "Shubham Toshniwal",
            "Shyam Upadhyay",
            "Shyamolima",
            "Debnath",
            "Siamak Shakeri",
            "Simon Thormeyer",
            "Simone Melzi",
            "Siva Reddy",
            "Sneha Priscilla Makini",
            "Soo-Hwan Lee",
            "Spencer Torene",
            "Sriharsha Hatwar",
            "Stanislas Dehaene",
            "Stefan Divic",
            "Stefano Ermon",
            "Stella Biderman",
            "Stephanie Lin",
            "Stephen Prasad",
            "Steven T. Piantadosi",
            "Stuart M. Shieber",
            "Summer Misherghi",
            "Svetlana Kiritchenko",
            "Swaroop Mishra",
            "Tal Linzen",
            "Tal Schuster",
            "Tao Li",
            "Tao Yu",
            "Tariq Ali",
            "Tatsu Hashimoto",
            "Te-Lin Wu",
            "Th\u00e9o Desbordes",
            "Theodore Rothschild",
            "Thomas Phan",
            "Tianle Wang",
            "Tiberius Nkinyili",
            "Timo Schick",
            "Timofei Kornev",
            "Titus Tunduny",
            "Tobias Gerstenberg",
            "Trenton Chang",
            "Trishala Neeraj",
            "Tushar Khot",
            "Tyler Shultz",
            "Uri Shaham",
            "Vedant Misra",
            "Vera Demberg",
            "Victoria Nyamai",
            "Vikas Raunak",
            "Vinay Ramasesh",
            "Vinay Uday Prabhu",
            "Vishakh Padmakumar",
            "Vivek Srikumar",
            "William Fedus",
            "William Saunders",
            "William Zhang",
            "Wout Vossen",
            "Xiang Ren",
            "Xiaoyu Tong",
            "Xinran Zhao",
            "Xinyi Wu",
            "Xudong Shen",
            "Yadollah Yaghoobzadeh",
            "Yair Lakretz",
            "Yangqiu Song",
            "Yasaman Bahri",
            "Yejin Choi",
            "Yichi Yang",
            "Yiding Hao",
            "Yifu Chen",
            "Yonatan Belinkov",
            "Yu Hou",
            "Yufang Hou",
            "Yuntao Bai",
            "Zachary Seid",
            "Zhuoye Zhao",
            "Zijian Wang",
            "Zijie J. Wang",
            "Zirui Wang",
            "Ziyi Wu"
        ],
        "published": "2022-06-09T17:05:34Z",
        "summary": "Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.",
        "pdf_link": "https://arxiv.org/pdf/2206.04615v3.pdf"
    },
    {
        "title": "Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding",
        "authors": [
            "William Chen",
            "Siyi Hu",
            "Rajat Talak",
            "Luca Carlone"
        ],
        "published": "2022-06-09T16:05:35Z",
        "summary": "Semantic 3D scene understanding is a problem of critical importance in\nrobotics. While significant advances have been made in simultaneous\nlocalization and mapping algorithms, robots are still far from having the\ncommon sense knowledge about household objects and their locations of an\naverage human. We introduce a novel method for leveraging common sense embedded\nwithin large language models for labelling rooms given the objects contained\nwithin. This algorithm has the added benefits of (i) requiring no task-specific\npre-training (operating entirely in the zero-shot regime) and (ii) generalizing\nto arbitrary room and object labels, including previously-unseen ones -- both\nof which are highly desirable traits in robotic scene understanding algorithms.\nThe proposed algorithm operates on 3D scene graphs produced by modern spatial\nperception systems, and we hope it will pave the way to more generalizable and\nscalable high-level 3D scene understanding for robotics.",
        "pdf_link": "https://arxiv.org/pdf/2206.04585v2.pdf"
    },
    {
        "title": "SsciBERT: A Pre-trained Language Model for Social Science Texts",
        "authors": [
            "Si Shen",
            "Jiangfeng Liu",
            "Litao Lin",
            "Ying Huang",
            "Lin Zhang",
            "Chang Liu",
            "Yutong Feng",
            "Dongbo Wang"
        ],
        "published": "2022-06-09T13:49:04Z",
        "summary": "The academic literature of social sciences records human civilization and\nstudies human social problems. With its large-scale growth, the ways to quickly\nfind existing research on relevant issues have become an urgent demand for\nresearchers. Previous studies, such as SciBERT, have shown that pre-training\nusing domain-specific texts can improve the performance of natural language\nprocessing tasks. However, the pre-trained language model for social sciences\nis not available so far. In light of this, the present research proposes a\npre-trained model based on the abstracts published in the Social Science\nCitation Index (SSCI) journals. The models, which are available on GitHub\n(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent\nperformance on discipline classification, abstract structure-function\nrecognition, and named entity recognition tasks with the social sciences\nliterature.",
        "pdf_link": "https://arxiv.org/pdf/2206.04510v3.pdf"
    },
    {
        "title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR",
        "authors": [
            "Arunkumar A",
            "Umesh S"
        ],
        "published": "2022-06-09T12:45:29Z",
        "summary": "Self-supervised learning (SSL) has shown tremendous success in various\nspeech-related downstream tasks, including Automatic Speech Recognition (ASR).\nThe output embeddings of the SSL model are treated as powerful short-time\nrepresentations of the speech signal. However, in the ASR task, the main\nobjective is to get the correct sequence of acoustic units, characters, or\nbyte-pair encodings (BPEs). Usually, encoder-decoder architecture works\nexceptionally well for a sequence-to-sequence task like ASR. Therefore, in this\npaper, we propose a new paradigm that exploits the power of a decoder during\nself-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to\ncompute the conventional masked prediction loss for the encoder. In addition,\nwe have introduced a decoder in the SSL framework and proposed a target\npreparation strategy for the decoder. Finally, we use a multitask SSL setup\nwherein we jointly optimize both the encoder and decoder losses. We hypothesize\nthat the presence of a decoder in the SSL model helps it learn an acoustic\nunit-based language model, which might improve the performance of an ASR\ndownstream task. We compare our proposed SSL model with HuBERT and show up to\n25% relative improvement in performance on ASR by finetuning on various\nLibriSpeech subsets.",
        "pdf_link": "https://arxiv.org/pdf/2206.04465v1.pdf"
    },
    {
        "title": "Context-based out-of-vocabulary word recovery for ASR systems in Indian languages",
        "authors": [
            "Arun Baby",
            "Saranya Vinnaitherthan",
            "Akhil Kerhalkar",
            "Pranav Jawale",
            "Sharath Adavanne",
            "Nagaraj Adiga"
        ],
        "published": "2022-06-09T06:51:31Z",
        "summary": "Detecting and recovering out-of-vocabulary (OOV) words is always challenging\nfor Automatic Speech Recognition (ASR) systems. Many existing methods focus on\nmodeling OOV words by modifying acoustic and language models and integrating\ncontext words cleverly into models. To train such complex models, we need a\nlarge amount of data with context words, additional training time, and\nincreased model size. However, after getting the ASR transcription to recover\ncontext-based OOV words, the post-processing method has not been explored much.\nIn this work, we propose a post-processing technique to improve the performance\nof context-based OOV recovery. We created an acoustically boosted language\nmodel with a sub-graph made at phone level with an OOV words list. We proposed\ntwo methods to determine a suitable cost function to retrieve the OOV words\nbased on the context. The cost function is defined based on phonetic and\nacoustic knowledge for matching and recovering the correct context words in the\ndecode. The effectiveness of the proposed cost function is evaluated at both\nword-level and sentence-level. The evaluation results show that this approach\ncan recover an average of 50% context-based OOV words across multiple\ncategories.",
        "pdf_link": "https://arxiv.org/pdf/2206.04305v1.pdf"
    },
    {
        "title": "Abstraction not Memory: BERT and the English Article System",
        "authors": [
            "Harish Tayyar Madabushi",
            "Dagmar Divjak",
            "Petar Milin"
        ],
        "published": "2022-06-08T22:36:54Z",
        "summary": "Article prediction is a task that has long defied accurate linguistic\ndescription. As such, this task is ideally suited to evaluate models on their\nability to emulate native-speaker intuition. To this end, we compare the\nperformance of native English speakers and pre-trained models on the task of\narticle prediction set up as a three way choice (a/an, the, zero). Our\nexperiments with BERT show that BERT outperforms humans on this task across all\narticles. In particular, BERT is far superior to humans at detecting the zero\narticle, possibly because we insert them using rules that the deep neural model\ncan easily pick up. More interestingly, we find that BERT tends to agree more\nwith annotators than with the corpus when inter-annotator agreement is high but\nswitches to agreeing more with the corpus as inter-annotator agreement drops.\nWe contend that this alignment with annotators, despite being trained on the\ncorpus, suggests that BERT is not memorising article use, but captures a high\nlevel generalisation of article use akin to human intuition.",
        "pdf_link": "https://arxiv.org/pdf/2206.04184v1.pdf"
    },
    {
        "title": "Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning",
        "authors": [
            "Hsuan Su",
            "Pohan Chi",
            "Shih-Cheng Huang",
            "Chung Ho Lam",
            "Saurav Sahay",
            "Shang-Tse Chen",
            "Hung-yi Lee"
        ],
        "published": "2022-06-08T14:48:06Z",
        "summary": "Much literature has shown that prompt-based learning is an efficient method\nto make use of the large pre-trained language model. Recent works also exhibit\nthe possibility of steering a chatbot's output by plugging in an appropriate\nprompt. Gradient-based methods are often used to perturb the prompts. However,\nsome language models are not even available to the public. In this work, we\nfirst explored the combination of prompting and reinforcement learning (RL) to\nsteer models' generation without accessing any of the models' parameters.\nSecond, to reduce the training effort and enhance the generalizability to the\nunseen task, we apply multi-task learning to make the model learn to generalize\nto new tasks better. The experiment results show that our proposed method can\nsuccessfully control several state-of-the-art (SOTA) dialogue models without\naccessing their parameters. Furthermore, the model demonstrates the strong\nability to quickly adapt to an unseen task in fewer steps than the baseline\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2206.03931v3.pdf"
    },
    {
        "title": "Revealing Single Frame Bias for Video-and-Language Learning",
        "authors": [
            "Jie Lei",
            "Tamara L. Berg",
            "Mohit Bansal"
        ],
        "published": "2022-06-07T16:28:30Z",
        "summary": "Training an effective video-and-language model intuitively requires multiple\nframes as model inputs. However, it is unclear whether using multiple frames is\nbeneficial to downstream tasks, and if yes, whether the performance gain is\nworth the drastically-increased computation and memory costs resulting from\nusing more frames. In this work, we explore single-frame models for\nvideo-and-language learning. On a diverse set of video-and-language tasks\n(including text-to-video retrieval and video question answering), we show the\nsurprising result that, with large-scale pre-training and a proper frame\nensemble strategy at inference time, a single-frame trained model that does not\nconsider temporal information can achieve better performance than existing\nmethods that use multiple frames for training. This result reveals the\nexistence of a strong \"static appearance bias\" in popular video-and-language\ndatasets. Therefore, to allow for a more comprehensive evaluation of\nvideo-and-language models, we propose two new retrieval tasks based on existing\nfine-grained action recognition datasets that encourage temporal modeling. Our\ncode is available at https://github.com/jayleicn/singularity",
        "pdf_link": "https://arxiv.org/pdf/2206.03428v1.pdf"
    },
    {
        "title": "Always Keep your Target in Mind: Studying Semantics and Improving Performance of Neural Lexical Substitution",
        "authors": [
            "Nikolay Arefyev",
            "Boris Sheludko",
            "Alexander Podolskiy",
            "Alexander Panchenko"
        ],
        "published": "2022-06-07T16:16:19Z",
        "summary": "Lexical substitution, i.e. generation of plausible words that can replace a\nparticular target word in a given context, is an extremely powerful technology\nthat can be used as a backbone of various NLP applications, including word\nsense induction and disambiguation, lexical relation extraction, data\naugmentation, etc. In this paper, we present a large-scale comparative study of\nlexical substitution methods employing both rather old and most recent language\nand masked language models (LMs and MLMs), such as context2vec, ELMo, BERT,\nRoBERTa, XLNet. We show that already competitive results achieved by SOTA\nLMs/MLMs can be further substantially improved if information about the target\nword is injected properly. Several existing and new target word injection\nmethods are compared for each LM/MLM using both intrinsic evaluation on lexical\nsubstitution datasets and extrinsic evaluation on word sense induction (WSI)\ndatasets. On two WSI datasets we obtain new SOTA results. Besides, we analyze\nthe types of semantic relations between target words and their substitutes\ngenerated by different models or given by annotators.",
        "pdf_link": "https://arxiv.org/pdf/2206.11815v1.pdf"
    },
    {
        "title": "Searching for Optimal Subword Tokenization in Cross-domain NER",
        "authors": [
            "Ruotian Ma",
            "Yiding Tan",
            "Xin Zhou",
            "Xuanting Chen",
            "Di Liang",
            "Sirui Wang",
            "Wei Wu",
            "Tao Gui",
            "Qi Zhang"
        ],
        "published": "2022-06-07T14:39:31Z",
        "summary": "Input distribution shift is one of the vital problems in unsupervised domain\nadaptation (UDA). The most popular UDA approaches focus on domain-invariant\nrepresentation learning, trying to align the features from different domains\ninto similar feature distributions. However, these approaches ignore the direct\nalignment of input word distributions between domains, which is a vital factor\nin word-level classification tasks such as cross-domain NER. In this work, we\nshed new light on cross-domain NER by introducing a subword-level solution,\nX-Piece, for input word-level distribution shift in NER. Specifically, we\nre-tokenize the input words of the source domain to approach the target subword\ndistribution, which is formulated and solved as an optimal transport problem.\nAs this approach focuses on the input level, it can also be combined with\nprevious DIRL methods for further improvement. Experimental results show the\neffectiveness of the proposed method based on BERT-tagger on four benchmark NER\ndatasets. Also, the proposed method is proved to benefit DIRL methods such as\nDANN.",
        "pdf_link": "https://arxiv.org/pdf/2206.03352v1.pdf"
    },
    {
        "title": "OCHADAI at SemEval-2022 Task 2: Adversarial Training for Multilingual Idiomaticity Detection",
        "authors": [
            "Lis Kanashiro Pereira",
            "Ichiro Kobayashi"
        ],
        "published": "2022-06-07T05:52:43Z",
        "summary": "We propose a multilingual adversarial training model for determining whether\na sentence contains an idiomatic expression. Given that a key challenge with\nthis task is the limited size of annotated data, our model relies on\npre-trained contextual representations from different multi-lingual\nstate-of-the-art transformer-based language models (i.e., multilingual BERT and\nXLM-RoBERTa), and on adversarial training, a training method for further\nenhancing model generalization and robustness. Without relying on any\nhuman-crafted features, knowledge bases, or additional datasets other than the\ntarget datasets, our model achieved competitive results and ranked 6th place in\nSubTask A (zero-shot) setting and 15th place in SubTask A (one-shot) setting.",
        "pdf_link": "https://arxiv.org/pdf/2206.03025v1.pdf"
    },
    {
        "title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
        "authors": [
            "Xiaodi Sun",
            "Sunny Rajagopalan",
            "Priyanka Nigam",
            "Weiyi Lu",
            "Yi Xu",
            "Belinda Zeng",
            "Trishul Chilimbi"
        ],
        "published": "2022-06-07T02:54:36Z",
        "summary": "Recent research has shown that large language models pretrained using\nunsupervised approaches can achieve significant performance improvement on many\ndownstream tasks. Typically when adapting these language models to downstream\ntasks, like a classification or regression task, we employ a fine-tuning\nparadigm in which the sentence representation from the language model is input\nto a task-specific head; the model is then fine-tuned end-to-end. However, with\nthe emergence of models like GPT-3, prompt-based fine-tuning has been proven to\nbe a successful approach for few-shot tasks. Inspired by this work, we study\ndiscrete prompt technologies in practice. There are two issues that arise with\nthe standard prompt approach. First, it can overfit on the prompt template.\nSecond, it requires manual effort to formulate the downstream task as a\nlanguage model problem. In this paper, we propose an improvement to\nprompt-based fine-tuning that addresses these two issues. We refer to our\napproach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results\nshow that DynaMaR can achieve an average improvement of 10% in few-shot\nsettings and improvement of 3.7% in data-rich settings over the standard\nfine-tuning approach on four e-commerce applications.",
        "pdf_link": "https://arxiv.org/pdf/2206.02982v1.pdf"
    },
    {
        "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
        "authors": [
            "Yujie Lu",
            "Weixi Feng",
            "Wanrong Zhu",
            "Wenda Xu",
            "Xin Eric Wang",
            "Miguel Eckstein",
            "William Yang Wang"
        ],
        "published": "2022-06-06T22:09:52Z",
        "summary": "Procedural planning aims to implement complex high-level goals by\ndecomposition into sequential simpler low-level steps. Although procedural\nplanning is a basic skill set for humans in daily life, it remains a challenge\nfor large language models (LLMs) that lack a deep understanding of the\ncause-effect relations in procedures. Previous methods require manual exemplars\nto acquire procedural planning knowledge from LLMs in the zero-shot setting.\nHowever, such elicited pre-trained knowledge in LLMs induces spurious\ncorrelations between goals and steps, which impair the model generalization to\nunseen tasks. In contrast, this paper proposes a neuro-symbolic procedural\nPLANner (PLAN) that elicits procedural planning knowledge from the LLMs with\ncommonsense-infused prompting. To mitigate spurious goal-step correlations, we\nuse symbolic program executors on the latent procedural representations to\nformalize prompts from commonsense knowledge bases as a causal intervention\ntoward the Structural Causal Model. Both automatic and human evaluations on\nWikiHow and RobotHow show the superiority of PLAN on procedural planning\nwithout further training or manual exemplars.",
        "pdf_link": "https://arxiv.org/pdf/2206.02928v6.pdf"
    },
    {
        "title": "Global Mixup: Eliminating Ambiguity with Clustering",
        "authors": [
            "Xiangjin Xie",
            "Yangning Li",
            "Wang Chen",
            "Kai Ouyang",
            "Li Jiang",
            "Haitao Zheng"
        ],
        "published": "2022-06-06T16:42:22Z",
        "summary": "Data augmentation with \\textbf{Mixup} has been proven an effective method to\nregularize the current deep neural networks. Mixup generates virtual samples\nand corresponding labels at once through linear interpolation. However, this\none-stage generation paradigm and the use of linear interpolation have the\nfollowing two defects: (1) The label of the generated sample is directly\ncombined from the labels of the original sample pairs without reasonable\njudgment, which makes the labels likely to be ambiguous. (2) linear combination\nsignificantly limits the sampling space for generating samples. To tackle these\nproblems, we propose a novel and effective augmentation method based on global\nclustering relationships named \\textbf{Global Mixup}. Specifically, we\ntransform the previous one-stage augmentation process into two-stage,\ndecoupling the process of generating virtual samples from the labeling. And for\nthe labels of the generated samples, relabeling is performed based on\nclustering by calculating the global relationships of the generated samples. In\naddition, we are no longer limited to linear relationships but generate more\nreliable virtual samples in a larger sampling space. Extensive experiments for\n\\textbf{CNN}, \\textbf{LSTM}, and \\textbf{BERT} on five tasks show that Global\nMixup significantly outperforms previous state-of-the-art baselines. Further\nexperiments also demonstrate the advantage of Global Mixup in low-resource\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2206.02734v1.pdf"
    },
    {
        "title": "What do tokens know about their characters and how do they know it?",
        "authors": [
            "Ayush Kaushal",
            "Kyle Mahowald"
        ],
        "published": "2022-06-06T13:27:26Z",
        "summary": "Pre-trained language models (PLMs) that use subword tokenization schemes can\nsucceed at a variety of language tasks that require character-level\ninformation, despite lacking explicit access to the character composition of\ntokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),\nwe probe what word pieces encode about character-level information by training\nclassifiers to predict the presence or absence of a particular alphabetical\ncharacter in a token, based on its embedding (e.g., probing whether the model\nembedding for \"cat\" encodes that it contains the character \"a\"). We find that\nthese models robustly encode character-level information and, in general,\nlarger models perform better at the task. We show that these results generalize\nto characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).\nThen, through a series of experiments and analyses, we investigate the\nmechanisms through which PLMs acquire English-language character information\nduring training and argue that this knowledge is acquired through multiple\nphenomena, including a systematic relationship between particular characters\nand particular parts of speech, as well as natural variability in the\ntokenization of related strings.",
        "pdf_link": "https://arxiv.org/pdf/2206.02608v1.pdf"
    },
    {
        "title": "Improving Contrastive Learning of Sentence Embeddings with Case-Augmented Positives and Retrieved Negatives",
        "authors": [
            "Wei Wang",
            "Liangzhu Ge",
            "Jingqiao Zhang",
            "Cheng Yang"
        ],
        "published": "2022-06-06T09:46:12Z",
        "summary": "Following SimCSE, contrastive learning based methods have achieved the\nstate-of-the-art (SOTA) performance in learning sentence embeddings. However,\nthe unsupervised contrastive learning methods still lag far behind the\nsupervised counterparts. We attribute this to the quality of positive and\nnegative samples, and aim to improve both. Specifically, for positive samples,\nwe propose switch-case augmentation to flip the case of the first letter of\nrandomly selected words in a sentence. This is to counteract the intrinsic bias\nof pre-trained token embeddings to frequency, word cases and subwords. For\nnegative samples, we sample hard negatives from the whole dataset based on a\npre-trained language model. Combining the above two methods with SimCSE, our\nproposed Contrastive learning with Augmented and Retrieved Data for Sentence\nembedding (CARDS) method significantly surpasses the current SOTA on STS\nbenchmarks in the unsupervised setting.",
        "pdf_link": "https://arxiv.org/pdf/2206.02457v1.pdf"
    },
    {
        "title": "Spam Detection Using BERT",
        "authors": [
            "Thaer Sahmoud",
            "Dr. Mohammad Mikki"
        ],
        "published": "2022-06-06T09:09:40Z",
        "summary": "Emails and SMSs are the most popular tools in today communications, and as\nthe increase of emails and SMSs users are increase, the number of spams is also\nincreases. Spam is any kind of unwanted, unsolicited digital communication that\ngets sent out in bulk, spam emails and SMSs are causing major resource wastage\nby unnecessarily flooding the network links. Although most spam mail originate\nwith advertisers looking to push their products, some are much more malicious\nin their intent like phishing emails that aims to trick victims into giving up\nsensitive information like website logins or credit card information this type\nof cybercrime is known as phishing. To countermeasure spams, many researches\nand efforts are done to build spam detectors that are able to filter out\nmessages and emails as spam or ham. In this research we build a spam detector\nusing BERT pre-trained model that classifies emails and messages by\nunderstanding to their context, and we trained our spam detector model using\nmultiple corpuses like SMS collection corpus, Enron corpus, SpamAssassin\ncorpus, Ling-Spam corpus and SMS spam collection corpus, our spam detector\nperformance was 98.62%, 97.83%, 99.13% and 99.28% respectively. Keywords: Spam\nDetector, BERT, Machine learning, NLP, Transformer, Enron Corpus, SpamAssassin\nCorpus, SMS Spam Detection Corpus, Ling-Spam Corpus.",
        "pdf_link": "https://arxiv.org/pdf/2206.02443v2.pdf"
    },
    {
        "title": "A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time",
        "authors": [
            "Iria de-Dios-Flores",
            "Marcos Garcia"
        ],
        "published": "2022-06-06T09:03:11Z",
        "summary": "This paper explores the ability of Transformer models to capture subject-verb\nand noun-adjective agreement dependencies in Galician. We conduct a series of\nword prediction experiments in which we manipulate dependency length together\nwith the presence of an attractor noun that acts as a lure. First, we evaluate\nthe overall performance of the existing monolingual and multilingual models for\nGalician. Secondly, to observe the effects of the training process, we compare\nthe different degrees of achievement of two monolingual BERT models at\ndifferent training points. We also release their checkpoints and propose an\nalternative evaluation metric. Our results confirm previous findings by similar\nworks that use the agreement prediction task and provide interesting insights\ninto the number of training steps required by a Transformer model to solve\nlong-distance dependencies.",
        "pdf_link": "https://arxiv.org/pdf/2206.02440v1.pdf"
    },
    {
        "title": "Domain-specific Language Pre-training for Dialogue Comprehension on Clinical Inquiry-Answering Conversations",
        "authors": [
            "Zhengyuan Liu",
            "Pavitra Krishnaswamy",
            "Nancy F. Chen"
        ],
        "published": "2022-06-06T08:45:03Z",
        "summary": "There is growing interest in the automated extraction of relevant information\nfrom clinical dialogues. However, it is difficult to collect and construct\nlarge annotated resources for clinical dialogue tasks. Recent developments in\nnatural language processing suggest that large-scale pre-trained language\nbackbones could be leveraged for such machine comprehension and information\nextraction tasks. Yet, due to the gap between pre-training and downstream\nclinical domains, it remains challenging to exploit the generic backbones for\ndomain-specific applications. Therefore, in this work, we propose a\ndomain-specific language pre-training, to improve performance on downstream\ntasks like dialogue comprehension. Aside from the common token-level masking\npre-training method, according to the nature of human conversations and\ninteractive flow of multi-topic inquiry-answering dialogues, we further propose\nsample generation strategies with speaker and utterance manipulation. The\nconversational pre-training guides the language backbone to reconstruct the\nutterances coherently based on the remaining context, thus bridging the gap\nbetween general and specific domains. Experiments are conducted on a clinical\nconversation dataset for symptom checking, where nurses inquire and discuss\nsymptom information with patients. We empirically show that the neural model\nwith our proposed approach brings improvement in the dialogue comprehension\ntask, and can achieve favorable results in the low resource training scenario.",
        "pdf_link": "https://arxiv.org/pdf/2206.02428v1.pdf"
    },
    {
        "title": "A sentiment analysis model for car review texts based on adversarial training and whole word mask BERT",
        "authors": [
            "Xingchen Liu",
            "Yawen Li",
            "Yingxia Shao",
            "Ang Li",
            "Jian Liang"
        ],
        "published": "2022-06-06T06:45:43Z",
        "summary": "In the field of car evaluation, more and more netizens choose to express\ntheir opinions on the Internet platform, and these comments will affect the\ndecision-making of buyers and the trend of car word-of-mouth. As an important\nbranch of natural language processing (NLP), sentiment analysis provides an\neffective research method for analyzing the sentiment types of massive car\nreview texts. However, due to the lexical professionalism and large text noise\nof review texts in the automotive field, when a general sentiment analysis\nmodel is applied to car reviews, the accuracy of the model will be poor. To\novercome these above challenges, we aim at the sentiment analysis task of car\nreview texts. From the perspective of word vectors, pre-training is carried out\nby means of whole word mask of proprietary vocabulary in the automotive field,\nand then training data is carried out through the strategy of an adversarial\ntraining set. Based on this, we propose a car review text sentiment analysis\nmodel based on adversarial training and whole word mask BERT(ATWWM-BERT).",
        "pdf_link": "https://arxiv.org/pdf/2206.02389v1.pdf"
    },
    {
        "title": "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression",
        "authors": [
            "Wanhua Li",
            "Xiaoke Huang",
            "Zheng Zhu",
            "Yansong Tang",
            "Xiu Li",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "published": "2022-06-06T03:54:53Z",
        "summary": "This paper presents a language-powered paradigm for ordinal regression.\nExisting methods usually treat each rank as a category and employ a set of\nweights to learn these concepts. These methods are easy to overfit and usually\nattain unsatisfactory performance as the learned concepts are mainly derived\nfrom the training set. Recent large pre-trained vision-language models like\nCLIP have shown impressive performance on various visual tasks. In this paper,\nwe propose to learn the rank concepts from the rich semantic CLIP latent space.\nSpecifically, we reformulate this task as an image-language matching problem\nwith a contrastive objective, which regards labels as text and obtains a\nlanguage prototype from a text encoder for each rank. While prompt engineering\nfor CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable\nprompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists\nof learnable context tokens and learnable rank embeddings; The learnable rank\nembeddings are constructed by explicitly modeling numerical continuity,\nresulting in well-ordered, compact language prototypes in the CLIP space. Once\nlearned, we can only save the language prototypes and discard the huge language\nmodel, resulting in zero additional computational overhead compared with the\nlinear head counterpart. Experimental results show that our paradigm achieves\ncompetitive performance in general ordinal regression tasks, and gains\nimprovements in few-shot and distribution shift settings for age estimation.\nThe code is available at https://github.com/xk-huang/OrdinalCLIP.",
        "pdf_link": "https://arxiv.org/pdf/2206.02338v2.pdf"
    },
    {
        "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier",
        "authors": [
            "Yifei Li",
            "Zeqi Lin",
            "Shizhuo Zhang",
            "Qiang Fu",
            "Bei Chen",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2022-06-06T03:38:36Z",
        "summary": "Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "pdf_link": "https://arxiv.org/pdf/2206.02336v3.pdf"
    },
    {
        "title": "Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models",
        "authors": [
            "Daniil Moskovskiy",
            "Daryna Dementieva",
            "Alexander Panchenko"
        ],
        "published": "2022-06-05T20:02:30Z",
        "summary": "Detoxification is a task of generating text in polite style while preserving\nmeaning and fluency of the original toxic text. Existing detoxification methods\nare designed to work in one exact language. This work investigates multilingual\nand cross-lingual detoxification and the behavior of large multilingual models\nlike in this setting. Unlike previous works we aim to make large language\nmodels able to perform detoxification without direct fine-tuning in given\nlanguage. Experiments show that multilingual models are capable of performing\nmultilingual style transfer. However, models are not able to perform\ncross-lingual detoxification and direct fine-tuning on exact language is\ninevitable.",
        "pdf_link": "https://arxiv.org/pdf/2206.02252v1.pdf"
    },
    {
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
        "authors": [
            "Charlie Snell",
            "Ilya Kostrikov",
            "Yi Su",
            "Mengjiao Yang",
            "Sergey Levine"
        ],
        "published": "2022-06-05T18:38:42Z",
        "summary": "Large language models distill broad knowledge from text corpora. However,\nthey can be inconsistent when it comes to completing user specified tasks. This\nissue can be addressed by finetuning such models via supervised learning on\ncurated datasets, or via reinforcement learning. In this work, we propose a\nnovel offline RL method, implicit language Q-learning (ILQL), designed for use\non language models, that combines both the flexible utility maximization\nframework of RL algorithms with the ability of supervised learning to leverage\npreviously collected data, as well as its simplicity and stability. Our method\nemploys a combination of value conservatism alongside an implicit dataset\nsupport constraint in learning value functions, which are then used to guide\nlanguage model generations towards maximizing user-specified utility functions.\nIn addition to empirically validating ILQL, we present a detailed empirical\nanalysis of situations where offline RL can be useful in natural language\ngeneration settings, demonstrating how it can be a more effective utility\noptimizer than prior approaches for end-to-end dialogue, and how it can\neffectively optimize high variance reward functions based on subjective\njudgement, such as whether to label a comment as toxic or not.",
        "pdf_link": "https://arxiv.org/pdf/2206.11871v2.pdf"
    },
    {
        "title": "Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon",
        "authors": [
            "Jia Wang",
            "Junping Du",
            "Yingxia Shao",
            "Ang Li"
        ],
        "published": "2022-06-05T12:17:46Z",
        "summary": "With the development of online travel services, it has great application\nprospects to timely mine users' evaluation emotions for travel services and use\nthem as indicators to guide the improvement of online travel service quality.\nIn this paper, we study the text sentiment classification of online travel\nreviews based on social media online comments and propose the SCCL model based\non capsule network and sentiment lexicon. SCCL model aims at the lack of\nconsideration of local features and emotional semantic features of the text in\nthe language model that can efficiently extract text context features like BERT\nand GRU. Then make the following improvements to their shortcomings. On the one\nhand, based on BERT-BiGRU, the capsule network is introduced to extract local\nfeatures while retaining good context features. On the other hand, the\nsentiment lexicon is introduced to extract the emotional sequence of the text\nto provide richer emotional semantic features for the model. To enhance the\nuniversality of the sentiment lexicon, the improved SO-PMI algorithm based on\nTF-IDF is used to expand the lexicon, so that the lexicon can also perform well\nin the field of online travel reviews.",
        "pdf_link": "https://arxiv.org/pdf/2206.02160v1.pdf"
    },
    {
        "title": "Speech Detection Task Against Asian Hate: BERT the Central, While Data-Centric Studies the Crucial",
        "authors": [
            "Xin Lian"
        ],
        "published": "2022-06-05T07:41:24Z",
        "summary": "With the COVID-19 pandemic continuing, hatred against Asians is intensifying\nin countries outside Asia, especially among the Chinese. There is an urgent\nneed to detect and prevent hate speech towards Asians effectively. In this\nwork, we first create COVID-HATE-2022, an annotated dataset including 2,025\nannotated tweets fetched in early February 2022, which are labeled based on\nspecific criteria, and we present the comprehensive collection of scenarios of\nhate and non-hate tweets in the dataset. Second, we fine-tune the BERT model\nbased on the relevant datasets and demonstrate several strategies related to\nthe \"cleaning\" of the tweets. Third, we investigate the performance of advanced\nfine-tuning strategies with various model-centric and data-centric approaches,\nand we show that both strategies generally improve the performance, while\ndata-centric ones outperform the others, and it demonstrates the feasibility\nand effectiveness of the data-centric approaches in the associated tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.02114v2.pdf"
    },
    {
        "title": "Fault-Aware Neural Code Rankers",
        "authors": [
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Mei Yang",
            "Andres Codas",
            "Mark Encarnaci\u00f3n",
            "Shuvendu K Lahiri",
            "Madanlal Musuvathi",
            "Jianfeng Gao"
        ],
        "published": "2022-06-04T22:01:05Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\ngenerate code for various programming tasks. In many instances, LLMs can\ngenerate a correct program for a task when given numerous trials. Consequently,\na recent trend is to do large scale sampling of programs using a model and then\nfiltering/ranking the programs based on the program execution on a small number\nof known unit tests to select one candidate solution. However, these approaches\nassume that the unit tests are given and assume the ability to safely execute\nthe generated programs (which can do arbitrary dangerous operations such as\nfile manipulations). Both of the above assumptions are impractical in\nreal-world software development. In this paper, we propose CodeRanker, a neural\nranker that can predict the correctness of a sampled program without executing\nit. Our CodeRanker is fault-aware i.e., it is trained to predict different\nkinds of execution information such as predicting the exact compile/runtime\nerror type (e.g., an IndexError or a TypeError). We show that CodeRanker can\nsignificantly increase the pass@1 accuracy of various code generation models\n(including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets.",
        "pdf_link": "https://arxiv.org/pdf/2206.03865v2.pdf"
    },
    {
        "title": "Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context",
        "authors": [
            "Andreas Troxler",
            "J\u00fcrg Schelldorfer"
        ],
        "published": "2022-06-04T15:39:30Z",
        "summary": "This tutorial demonstrates workflows to incorporate text data into actuarial\nclassification and regression tasks. The main focus is on methods employing\ntransformer-based models. A dataset of car accident descriptions with an\naverage length of 400 words, available in English and German, and a dataset\nwith short property insurance claims descriptions are used to demonstrate these\ntechniques. The case studies tackle challenges related to a multi-lingual\nsetting and long input sequences. They also show ways to interpret model\noutput, to assess and improve model performance, by fine-tuning the models to\nthe domain of application or to a specific prediction task. Finally, the\ntutorial provides practical approaches to handle classification tasks in\nsituations with no or only few labeled data, including but not limited to\nChatGPT. The results achieved by using the language-understanding skills of\noff-the-shelf natural language processing (NLP) models with only minimal\npre-processing and fine-tuning clearly demonstrate the power of transfer\nlearning for practical applications.",
        "pdf_link": "https://arxiv.org/pdf/2206.02014v3.pdf"
    },
    {
        "title": "Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection",
        "authors": [
            "Juuso Eronen",
            "Michal Ptaszynski",
            "Fumito Masui"
        ],
        "published": "2022-06-04T09:11:41Z",
        "summary": "In most cases, word embeddings are learned only from raw tokens or in some\ncases, lemmas. This includes pre-trained language models like BERT. To\ninvestigate on the potential of capturing deeper relations between lexical\nitems and structures and to filter out redundant information, we propose to\npreserve the morphological, syntactic and other types of linguistic information\nby combining them with the raw tokens or lemmas. This means, for example,\nincluding parts-of-speech or dependency information within the used lexical\nfeatures. The word embeddings can then be trained on the combinations instead\nof just raw tokens. It is also possible to later apply this method to the\npre-training of huge language models and possibly enhance their performance.\nThis would aid in tackling problems which are more sophisticated from the point\nof view of linguistic representation, such as detection of cyberbullying.",
        "pdf_link": "https://arxiv.org/pdf/2206.01950v1.pdf"
    },
    {
        "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers",
        "authors": [
            "Zhewei Yao",
            "Reza Yazdani Aminabadi",
            "Minjia Zhang",
            "Xiaoxia Wu",
            "Conglong Li",
            "Yuxiong He"
        ],
        "published": "2022-06-04T00:28:21Z",
        "summary": "How to efficiently serve ever-larger trained natural language models in\npractice has become exceptionally challenging even for powerful cloud servers\ndue to their prohibitive memory/computation requirements. In this work, we\npresent an efficient and affordable post-training quantization approach to\ncompress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an\nend-to-end quantization and inference pipeline with three main components: (1)\na fine-grained hardware-friendly quantization scheme for both weight and\nactivations; (2) a novel affordable layer-by-layer knowledge distillation\nalgorithm (LKD) even without the access to the original training data; (3) a\nhighly-optimized quantization system backend support to remove the\nquantization/dequantization overhead. As such, we are able to show that: (1)\nZeroQuant can reduce the precision for weights and activations to INT8 in a\ncost-free way for both BERT and GPT3-style models with minimal accuracy impact,\nwhich leads to up to 5.19x/4.16x speedup on those models compared to FP16\ninference; (2) ZeroQuant plus LKD affordably quantize the weights in the\nfully-connected module to INT4 along with INT8 weights in the attention module\nand INT8 activations, resulting in 3x memory footprint reduction compared to\nthe FP16 model; (3) ZeroQuant can be directly applied to two of the largest\nopen-sourced language models, including GPT-J6B and GPT-NeoX20, for which our\nINT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x\nbetter efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2206.01861v1.pdf"
    },
    {
        "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient",
        "authors": [
            "Xiaoxia Wu",
            "Zhewei Yao",
            "Minjia Zhang",
            "Conglong Li",
            "Yuxiong He"
        ],
        "published": "2022-06-04T00:19:45Z",
        "summary": "Extreme compression, particularly ultra-low bit precision (binary/ternary)\nquantization, has been proposed to fit large NLP models on resource-constraint\ndevices. However, to preserve the accuracy for such aggressive compression\nschemes, cutting-edge methods usually introduce complicated compression\npipelines, e.g., multi-stage expensive knowledge distillation with extensive\nhyperparameter tuning. Also, they oftentimes focus less on smaller transformer\nmodels that have already been heavily compressed via knowledge distillation and\nlack a systematic study to show the effectiveness of their methods. In this\npaper, we perform a very comprehensive systematic study to measure the impact\nof many key hyperparameters and training strategies from previous works. As a\nresult, we find out that previous baselines for ultra-low bit precision\nquantization are significantly under-trained. Based on our study, we propose a\nsimple yet effective compression pipeline for extreme compression, named XTC.\nXTC demonstrates that (1) we can skip the pre-training knowledge distillation\nto obtain a 5-layer BERT while achieving better performance than previous\nstate-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization\nplus layer reduction is able to reduce the model size by 50x, resulting in new\nstate-of-the-art results on GLUE tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.01859v1.pdf"
    },
    {
        "title": "Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning",
        "authors": [
            "Yujia Xie",
            "Luowei Zhou",
            "Xiyang Dai",
            "Lu Yuan",
            "Nguyen Bach",
            "Ce Liu",
            "Michael Zeng"
        ],
        "published": "2022-06-03T22:33:09Z",
        "summary": "People say, \"A picture is worth a thousand words\". Then how can we get the\nrich information out of the image? We argue that by using visual clues to\nbridge large pretrained vision foundation models and language models, we can do\nso without any extra cross-modal training. Thanks to the strong zero-shot\ncapability of foundation models, we start by constructing a rich semantic\nrepresentation of the image (e.g., image tags, object attributes / locations,\ncaptions) as a structured textual prompt, called visual clues, using a vision\nfoundation model. Based on visual clues, we use large language model to produce\na series of comprehensive descriptions for the visual content, which is then\nverified by the vision model again to select the candidate that aligns best\nwith the image. We evaluate the quality of generated descriptions by\nquantitative and qualitative measurement. The results demonstrate the\neffectiveness of such a structured semantic representation.",
        "pdf_link": "https://arxiv.org/pdf/2206.01843v2.pdf"
    },
    {
        "title": "Differentially Private Model Compression",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Arturs Backurs",
            "Huseyin A Inan",
            "Lukas Wutschitz",
            "Janardhan Kulkarni"
        ],
        "published": "2022-06-03T22:04:36Z",
        "summary": "Recent papers have shown that large pre-trained language models (LLMs) such\nas BERT, GPT-2 can be fine-tuned on private data to achieve performance\ncomparable to non-private models for many downstream Natural Language\nProcessing (NLP) tasks while simultaneously guaranteeing differential privacy.\nThe inference cost of these models -- which consist of hundreds of millions of\nparameters -- however, can be prohibitively large. Hence, often in practice,\nLLMs are compressed before they are deployed in specific applications. In this\npaper, we initiate the study of differentially private model compression and\npropose frameworks for achieving 50% sparsity levels while maintaining nearly\nfull performance. We demonstrate these ideas on standard GLUE benchmarks using\nBERT models, setting benchmarks for future research on this topic.",
        "pdf_link": "https://arxiv.org/pdf/2206.01838v1.pdf"
    },
    {
        "title": "Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric",
        "authors": [
            "Ian Berlot-Attwell",
            "Frank Rudzicz"
        ],
        "published": "2022-06-03T21:23:05Z",
        "summary": "In this work, we evaluate various existing dialogue relevance metrics, find\nstrong dependency on the dataset, often with poor correlation with human scores\nof relevance, and propose modifications to reduce data requirements and domain\nsensitivity while improving correlation. Our proposed metric achieves\nstate-of-the-art performance on the HUMOD dataset while reducing measured\nsensitivity to dataset by 37%-66%. We achieve this without fine-tuning a\npretrained language model, and using only 3,750 unannotated human dialogues and\na single negative example. Despite these limitations, we demonstrate\ncompetitive performance on four datasets from different domains. Our code,\nincluding our metric and experiments, is open sourced.",
        "pdf_link": "https://arxiv.org/pdf/2206.01823v1.pdf"
    },
    {
        "title": "Extracting Similar Questions From Naturally-occurring Business Conversations",
        "authors": [
            "Xiliang Zhu",
            "David Rossouw",
            "Shayna Gardiner",
            "Simon Corston-Oliver"
        ],
        "published": "2022-06-03T14:13:44Z",
        "summary": "Pre-trained contextualized embedding models such as BERT are a standard\nbuilding block in many natural language processing systems. We demonstrate that\nthe sentence-level representations produced by some off-the-shelf\ncontextualized embedding models have a narrow distribution in the embedding\nspace, and thus perform poorly for the task of identifying semantically similar\nquestions in real-world English business conversations. We describe a method\nthat uses appropriately tuned representations and a small set of exemplars to\ngroup questions of interest to business users in a visualization that can be\nused for data exploration or employee coaching.",
        "pdf_link": "https://arxiv.org/pdf/2206.01585v1.pdf"
    },
    {
        "title": "Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian",
        "authors": [
            "Tatiana Shamardina",
            "Vladislav Mikhailov",
            "Daniil Chernianskii",
            "Alena Fenogenova",
            "Marat Saidov",
            "Anastasiya Valeeva",
            "Tatiana Shavrina",
            "Ivan Smurov",
            "Elena Tutubalina",
            "Ekaterina Artemova"
        ],
        "published": "2022-06-03T14:12:33Z",
        "summary": "We present the shared task on artificial text detection in Russian, which is\norganized as a part of the Dialogue Evaluation initiative, held in 2022. The\nshared task dataset includes texts from 14 text generators, i.e., one human\nwriter and 13 text generative models fine-tuned for one or more of the\nfollowing generation tasks: machine translation, paraphrase generation, text\nsummarization, text simplification. We also consider back-translation and\nzero-shot generation approaches. The human-written texts are collected from\npublicly available resources across multiple domains. The shared task consists\nof two sub-tasks: (i) to determine if a given text is automatically generated\nor written by a human; (ii) to identify the author of a given text. The first\ntask is framed as a binary classification problem. The second task is a\nmulti-class classification problem. We provide count-based and BERT-based\nbaselines, along with the human evaluation on the first sub-task. A total of 30\nand 8 systems have been submitted to the binary and multi-class sub-tasks,\ncorrespondingly. Most teams outperform the baselines by a wide margin. We\npublicly release our codebase, human evaluation results, and other materials in\nour GitHub repository (https://github.com/dialogue-evaluation/RuATD).",
        "pdf_link": "https://arxiv.org/pdf/2206.01583v1.pdf"
    },
    {
        "title": "TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models",
        "authors": [
            "Mohammed ElKomy",
            "Amany M. Sarhan"
        ],
        "published": "2022-06-03T13:00:48Z",
        "summary": "In recent years, we witnessed great progress in different tasks of natural\nlanguage understanding using machine learning. Question answering is one of\nthese tasks which is used by search engines and social media platforms for\nimproved user experience. Arabic is the language of the Holy Qur'an; the sacred\ntext for 1.8 billion people across the world. Arabic is a challenging language\nfor Natural Language Processing (NLP) due to its complex structures. In this\narticle, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which\nis a question answering challenge on the Holy Qur'an in Arabic. We propose an\nensemble learning model based on Arabic variants of BERT models. In addition,\nwe perform post-processing to enhance the model predictions. Our system\nachieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test\nset.",
        "pdf_link": "https://arxiv.org/pdf/2206.01550v1.pdf"
    },
    {
        "title": "Latent Topology Induction for Understanding Contextualized Representations",
        "authors": [
            "Yao Fu",
            "Mirella Lapata"
        ],
        "published": "2022-06-03T11:22:48Z",
        "summary": "In this work, we study the representation space of contextualized embeddings\nand gain insight into the hidden topology of large language models. We show\nthere exists a network of latent states that summarize linguistic properties of\ncontextualized representations. Instead of seeking alignments to existing\nwell-defined annotations, we infer this latent network in a fully unsupervised\nway using a structured variational autoencoder. The induced states not only\nserve as anchors that mark the topology (neighbors and connectivity) of the\nrepresentation manifold but also reveal the internal mechanism of encoding\nsentences. With the induced network, we: (1). decompose the representation\nspace into a spectrum of latent states which encode fine-grained word meanings\nwith lexical, morphological, syntactic and semantic information; (2). show\nstate-state transitions encode rich phrase constructions and serve as the\nbackbones of the latent space. Putting the two together, we show that sentences\nare represented as a traversal over the latent network where state-state\ntransition chains encode syntactic templates and state-word emissions fill in\nthe content. We demonstrate these insights with extensive experiments and\nvisualizations.",
        "pdf_link": "https://arxiv.org/pdf/2206.01512v1.pdf"
    },
    {
        "title": "Automatic Generation of Programming Exercises and Code Explanations using Large Language Models",
        "authors": [
            "Sami Sarsa",
            "Paul Denny",
            "Arto Hellas",
            "Juho Leinonen"
        ],
        "published": "2022-06-03T11:00:43Z",
        "summary": "This article explores the natural language generation capabilities of large\nlanguage models with application to the production of two types of learning\nresources common in programming courses. Using OpenAI Codex as the large\nlanguage model, we create programming exercises (including sample solutions and\ntest cases) and code explanations, assessing these qualitatively and\nquantitatively. Our results suggest that the majority of the automatically\ngenerated content is both novel and sensible, and in some cases ready to use as\nis. When creating exercises we find that it is remarkably easy to influence\nboth the programming concepts and the contextual themes they contain, simply by\nsupplying keywords as input to the model. Our analysis suggests that there is\nsignificant value in massive generative machine learning models as a tool for\ninstructors, although there remains a need for some oversight to ensure the\nquality of the generated content before it is delivered to students. We further\ndiscuss the implications of OpenAI Codex and similar tools for introductory\nprogramming education and highlight future research streams that have the\npotential to improve the quality of the educational experience for both\nteachers and students alike.",
        "pdf_link": "https://arxiv.org/pdf/2206.11861v2.pdf"
    },
    {
        "title": "Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code",
        "authors": [
            "Patrick Barei\u00df",
            "Beatriz Souza",
            "Marcelo d'Amorim",
            "Michael Pradel"
        ],
        "published": "2022-06-02T23:15:42Z",
        "summary": "Few-shot learning with large-scale, pre-trained language models is a powerful\nway to answer questions about code, e.g., how to complete a given code example,\nor even generate code snippets from scratch. The success of these models raises\nthe question whether they could serve as a basis for building a wide range code\ngeneration tools. Traditionally, such tools are built manually and separately\nfor each task. Instead, few-shot learning may allow to obtain different tools\nfrom a single pre-trained language model by simply providing a few examples or\na natural language description of the expected tool behavior. This paper\nstudies to what extent a state-of-the-art, pre-trained language model of code,\nCodex, may serve this purpose. We consider three code manipulation and code\ngeneration tasks targeted by a range of traditional tools: (i) code mutation;\n(ii) test oracle generation from natural language documentation; and (iii) test\ncase generation. For each task, we compare few-shot learning to a manually\nbuilt tool. Our results show that the model-based tools complement (code\nmutation), are on par (test oracle generation), or even outperform their\nrespective traditionally built tool (test case generation), while imposing far\nless effort to develop them. By comparing the effectiveness of different\nvariants of the model-based tools, we provide insights on how to design an\nappropriate input (\"prompt\") to the model and what influence the size of the\nmodel has. For example, we find that providing a small natural language\ndescription of the code generation task is an easy way to improve predictions.\nOverall, we conclude that few-shot language models are surprisingly effective,\nyet there is still more work to be done, such as exploring more diverse ways of\nprompting and tackling even more involved tasks.",
        "pdf_link": "https://arxiv.org/pdf/2206.01335v2.pdf"
    },
    {
        "title": "Decentralized Training of Foundation Models in Heterogeneous Environments",
        "authors": [
            "Binhang Yuan",
            "Yongjun He",
            "Jared Quincy Davis",
            "Tianyi Zhang",
            "Tri Dao",
            "Beidi Chen",
            "Percy Liang",
            "Christopher Re",
            "Ce Zhang"
        ],
        "published": "2022-06-02T20:19:51Z",
        "summary": "Training foundation models, such as GPT-3 and PaLM, can be extremely\nexpensive, often involving tens of thousands of GPUs running continuously for\nmonths. These models are typically trained in specialized clusters featuring\nfast, homogeneous interconnects and using carefully designed software systems\nthat support both data parallelism and model/pipeline parallelism. Such\ndedicated clusters can be costly and difficult to obtain. Can we instead\nleverage the much greater amount of decentralized, heterogeneous, and\nlower-bandwidth interconnected compute? Previous works examining the\nheterogeneous, decentralized setting focus on relatively small models that can\nbe trained in a purely data parallel manner. State-of-the-art schemes for model\nparallel foundation model training, such as Megatron, only consider the\nhomogeneous data center setting. In this paper, we present the first study of\ntraining large foundation models with model parallelism in a decentralized\nregime over a heterogeneous network. Our key technical contribution is a\nscheduling algorithm that allocates different computational \"tasklets\" in the\ntraining of foundation models to a group of decentralized GPU devices connected\nby a slow heterogeneous network. We provide a formal cost model and further\npropose an efficient evolutionary algorithm to find the optimal allocation\nstrategy. We conduct extensive experiments that represent different scenarios\nfor learning over geo-distributed devices simulated using real-world network\nmeasurements. In the most extreme case, across 8 different cities spanning 3\ncontinents, our approach is 4.8X faster than prior state-of-the-art training\nsystems (Megatron).",
        "pdf_link": "https://arxiv.org/pdf/2206.01288v4.pdf"
    },
    {
        "title": "A Multi-Policy Framework for Deep Learning-Based Fake News Detection",
        "authors": [
            "Jo\u00e3o Vitorino",
            "Tiago Dias",
            "Tiago Fonseca",
            "Nuno Oliveira",
            "Isabel Pra\u00e7a"
        ],
        "published": "2022-06-01T21:25:21Z",
        "summary": "Connectivity plays an ever-increasing role in modern society, with people all\naround the world having easy access to rapidly disseminated information.\nHowever, a more interconnected society enables the spread of intentionally\nfalse information. To mitigate the negative impacts of fake news, it is\nessential to improve detection methodologies. This work introduces Multi-Policy\nStatement Checker (MPSC), a framework that automates fake news detection by\nusing deep learning techniques to analyze a statement itself and its related\nnews articles, predicting whether it is seemingly credible or suspicious. The\nproposed framework was evaluated using four merged datasets containing real and\nfake news. Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and\nBidirectional Encoder Representations from Transformers (BERT) models were\ntrained to utilize both lexical and syntactic features, and their performance\nwas evaluated. The obtained results demonstrate that a multi-policy analysis\nreliably identifies suspicious statements, which can be advantageous for fake\nnews detection.",
        "pdf_link": "https://arxiv.org/pdf/2206.11866v1.pdf"
    },
    {
        "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting",
        "authors": [
            "Tomasz Korbak",
            "Hady Elsahar",
            "Germ\u00e1n Kruszewski",
            "Marc Dymetman"
        ],
        "published": "2022-06-01T20:54:41Z",
        "summary": "The availability of large pre-trained models is changing the landscape of\nMachine Learning research and practice, moving from a training-from-scratch to\na fine-tuning paradigm. While in some applications the goal is to \"nudge\" the\npre-trained distribution towards preferred outputs, in others it is to steer it\ntowards a different distribution over the sample space. Two main paradigms have\nemerged to tackle this challenge: Reward Maximization (RM) and, more recently,\nDistribution Matching (DM). RM applies standard Reinforcement Learning (RL)\ntechniques, such as Policy Gradients, to gradually increase the reward signal.\nDM prescribes to first make explicit the target distribution that the model is\nfine-tuned to approximate. Here we explore the theoretical connections between\nthe two paradigms, and show that methods such as KL-control developed for RM\ncan also be construed as belonging to DM. We further observe that while DM\ndiffers from RM, it can suffer from similar training difficulties, such as high\ngradient variance. We leverage connections between the two paradigms to import\nthe concept of baseline into DM methods. We empirically validate the benefits\nof adding a baseline on an array of controllable language generation tasks such\nas constraining topic, sentiment, and gender distributions in texts sampled\nfrom a language model. We observe superior performance in terms of constraint\nsatisfaction, stability and sample efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2206.00761v2.pdf"
    },
    {
        "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training",
        "authors": [
            "Yan Zeng",
            "Wangchunshu Zhou",
            "Ao Luo",
            "Ziming Cheng",
            "Xinsong Zhang"
        ],
        "published": "2022-06-01T16:45:24Z",
        "summary": "In this paper, we introduce Cross-View Language Modeling, a simple and\neffective pre-training framework that unifies cross-lingual and cross-modal\npre-training with shared architectures and objectives. Our approach is\nmotivated by a key observation that cross-lingual and cross-modal pre-training\nshare the same goal of aligning two different views of the same object into a\ncommon semantic space. To this end, the cross-view language modeling framework\nconsiders both multi-modal data (i.e., image-caption pairs) and multi-lingual\ndata (i.e., parallel sentence pairs) as two different views of the same object,\nand trains the model to align the two views by maximizing the mutual\ninformation between them with conditional masked language modeling and\ncontrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language\nModel, with the cross-view language modeling framework. Empirical results on\nIGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text\nretrieval datasets show that while conceptually simpler, CCLM significantly\noutperforms the prior state-of-the-art with an average absolute improvement of\nover 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained\nmodel that surpasses the translate-test performance of representative English\nvision-language models by zero-shot cross-lingual transfer.",
        "pdf_link": "https://arxiv.org/pdf/2206.00621v2.pdf"
    },
    {
        "title": "Romantic-Computing",
        "authors": [
            "Elizabeth Horishny"
        ],
        "published": "2022-06-01T14:27:17Z",
        "summary": "In this paper we compare various text generation models' ability to write\npoetry in the style of early English Romanticism. These models include:\nCharacter-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging\nFace's GPT-2, OpenAI's GPT-3, and EleutherAI's GPT-NEO. Quality was measured\nbased syllable count and coherence with the automatic evaluation metric GRUEN.\nCharacter-Level Recurrent Neural Networks performed far worse compared to\ntransformer models. And, as parameter-size increased, the quality of\ntransformer models' poems improved. These models are typically not compared in\na creative context, and we are happy to contribute.",
        "pdf_link": "https://arxiv.org/pdf/2206.11864v1.pdf"
    },
    {
        "title": "MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining",
        "authors": [
            "Pengyuan Lyu",
            "Chengquan Zhang",
            "Shanshan Liu",
            "Meina Qiao",
            "Yangliu Xu",
            "Liang Wu",
            "Kun Yao",
            "Junyu Han",
            "Errui Ding",
            "Jingdong Wang"
        ],
        "published": "2022-06-01T08:27:19Z",
        "summary": "Text images contain both visual and linguistic information. However, existing\npre-training techniques for text recognition mainly focus on either visual\nrepresentation learning or linguistic knowledge learning. In this paper, we\npropose a novel approach MaskOCR to unify vision and language pre-training in\nthe classical encoder-decoder recognition framework. We adopt the masked image\nmodeling approach to pre-train the feature encoder using a large set of\nunlabeled real text images, which allows us to learn strong visual\nrepresentations. In contrast to introducing linguistic knowledge with an\nadditional language model, we directly pre-train the sequence decoder.\nSpecifically, we transform text data into synthesized text images to unify the\ndata modalities of vision and language, and enhance the language modeling\ncapability of the sequence decoder using a proposed masked image-language\nmodeling scheme. Significantly, the encoder is frozen during the pre-training\nphase of the sequence decoder. Experimental results demonstrate that our\nproposed method achieves superior performance on benchmark datasets, including\nChinese and English text images.",
        "pdf_link": "https://arxiv.org/pdf/2206.00311v3.pdf"
    },
    {
        "title": "Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback",
        "authors": [
            "Emmy Liu",
            "Michael Henry Tessler",
            "Nicole Dubosh",
            "Katherine Mosher Hiller",
            "Roger Levy"
        ],
        "published": "2022-06-01T05:01:36Z",
        "summary": "Although approximately 50% of medical school graduates today are women,\nfemale physicians tend to be underrepresented in senior positions, make less\nmoney than their male counterparts and receive fewer promotions. There is a\ngrowing body of literature demonstrating gender bias in various forms of\nevaluation in medicine, but this work was mainly conducted by looking for\nspecific words using fixed dictionaries such as LIWC and focused on\nrecommendation letters. We use a dataset of written and quantitative\nassessments of medical student performance on individual shifts of work,\ncollected across multiple institutions, to investigate the extent to which\ngender bias exists in a day-to-day context for medical students. We investigate\ndifferences in the narrative comments given to male and female students by both\nmale or female faculty assessors, using a fine-tuned BERT model. This allows us\nto examine whether groups are written about in systematically different ways,\nwithout relying on hand-crafted wordlists or topic models. We compare these\nresults to results from the traditional LIWC method and find that, although we\nfind no evidence of group-level gender bias in this dataset, terms related to\nfamily and children are used more in feedback given to women.",
        "pdf_link": "https://arxiv.org/pdf/2206.00234v1.pdf"
    },
    {
        "title": "Order-sensitive Shapley Values for Evaluating Conceptual Soundness of NLP Models",
        "authors": [
            "Kaiji Lu",
            "Anupam Datta"
        ],
        "published": "2022-06-01T02:30:12Z",
        "summary": "Previous works show that deep NLP models are not always conceptually sound:\nthey do not always learn the correct linguistic concepts. Specifically, they\ncan be insensitive to word order. In order to systematically evaluate models\nfor their conceptual soundness with respect to word order, we introduce a new\nexplanation method for sequential data: Order-sensitive Shapley Values (OSV).\nWe conduct an extensive empirical evaluation to validate the method and surface\nhow well various deep NLP models learn word order. Using synthetic data, we\nfirst show that OSV is more faithful in explaining model behavior than\ngradient-based methods. Second, applying to the HANS dataset, we discover that\nthe BERT-based NLI model uses only the word occurrences without word orders.\nAlthough simple data augmentation improves accuracy on HANS, OSV shows that the\naugmented model does not fundamentally improve the model's learning of order.\nThird, we discover that not all sentiment analysis models learn negation\nproperly: some fail to capture the correct syntax of the negation construct.\nFinally, we show that pretrained language models such as BERT may rely on the\nabsolute positions of subject words to learn long-range Subject-Verb Agreement.\nWith each NLP task, we also demonstrate how OSV can be leveraged to generate\nadversarial examples.",
        "pdf_link": "https://arxiv.org/pdf/2206.00192v1.pdf"
    },
    {
        "title": "FHIST: A Benchmark for Few-shot Classification of Histological Images",
        "authors": [
            "Fereshteh Shakeri",
            "Malik Boudiaf",
            "Sina Mohammadi",
            "Ivaxi Sheth",
            "Mohammad Havaei",
            "Ismail Ben Ayed",
            "Samira Ebrahimi Kahou"
        ],
        "published": "2022-05-31T20:03:40Z",
        "summary": "Few-shot learning has recently attracted wide interest in image\nclassification, but almost all the current public benchmarks are focused on\nnatural images. The few-shot paradigm is highly relevant in medical-imaging\napplications due to the scarcity of labeled data, as annotations are expensive\nand require specialized expertise. However, in medical imaging, few-shot\nlearning research is sparse, limited to private data sets and is at its early\nstage. In particular, the few-shot setting is of high interest in histology due\nto the diversity and fine granularity of cancer related tissue classification\ntasks, and the variety of data-preparation techniques. This paper introduces a\nhighly diversified public benchmark, gathered from various public datasets, for\nfew-shot histology data classification. We build few-shot tasks and\nbase-training data with various tissue types, different levels of domain shifts\nstemming from various cancer sites, and different class-granularity levels,\nthereby reflecting realistic scenarios. We evaluate the performances of\nstate-of-the-art few-shot learning methods on our benchmark, and observe that\nsimple fine-tuning and regularization methods achieve better results than the\npopular meta-learning and episodic-training paradigm. Furthermore, we introduce\nthree scenarios based on the domain shifts between the source and target\nhistology data: near-domain, middle-domain and out-domain. Our experiments\ndisplay the potential of few-shot learning in histology classification, with\nstate-of-art few shot learning methods approaching the supervised-learning\nbaselines in the near-domain setting. In our out-domain setting, for 5-way\n5-shot, the best performing method reaches 60% accuracy. We believe that our\nwork could help in building realistic evaluations and fair comparisons of\nfew-shot learning methods and will further encourage research in the few-shot\nparadigm.",
        "pdf_link": "https://arxiv.org/pdf/2206.00092v1.pdf"
    },
    {
        "title": "The Contribution of Lyrics and Acoustics to Collaborative Understanding of Mood",
        "authors": [
            "Shahrzad Naseri",
            "Sravana Reddy",
            "Joana Correia",
            "Jussi Karlgren",
            "Rosie Jones"
        ],
        "published": "2022-05-31T19:58:41Z",
        "summary": "In this work, we study the association between song lyrics and mood through a\ndata-driven analysis. Our data set consists of nearly one million songs, with\nsong-mood associations derived from user playlists on the Spotify streaming\nplatform. We take advantage of state-of-the-art natural language processing\nmodels based on transformers to learn the association between the lyrics and\nmoods. We find that a pretrained transformer-based language model in a\nzero-shot setting -- i.e., out of the box with no further training on our data\n-- is powerful for capturing song-mood associations. Moreover, we illustrate\nthat training on song-mood associations results in a highly accurate model that\npredicts these associations for unseen songs. Furthermore, by comparing the\nprediction of a model using lyrics with one using acoustic features, we observe\nthat the relative importance of lyrics for mood prediction in comparison with\nacoustics depends on the specific mood. Finally, we verify if the models are\ncapturing the same information about lyrics and acoustics as humans through an\nannotation task where we obtain human judgments of mood-song relevance based on\nlyrics and acoustics.",
        "pdf_link": "https://arxiv.org/pdf/2207.05680v1.pdf"
    },
    {
        "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
        "authors": [
            "Yinlam Chow",
            "Aza Tulepbergenov",
            "Ofir Nachum",
            "MoonKyung Ryu",
            "Mohammad Ghavamzadeh",
            "Craig Boutilier"
        ],
        "published": "2022-05-31T19:00:41Z",
        "summary": "Despite recent advancements in language models (LMs), their application to\ndialogue management (DM) problems and ability to carry on rich conversations\nremain a challenge. We use reinforcement learning (RL) to develop a dialogue\nagent that avoids being short-sighted (outputting generic utterances) and\nmaximizes overall user satisfaction. Most existing RL approaches to DM train\nthe agent at the word-level, and thus, have to deal with a combinatorially\ncomplex action space even for a medium-size vocabulary. As a result, they\nstruggle to produce a successful and engaging dialogue even if they are\nwarm-started with a pre-trained LM. To address this issue, we develop a\nRL-based DM using a novel mixture of expert language model (MoE-LM) that\nconsists of (i) a LM capable of learning diverse semantics for conversation\nhistories, (ii) a number of {\\em specialized} LMs (or experts) capable of\ngenerating utterances corresponding to a particular attribute or personality,\nand (iii) a RL-based DM that performs dialogue planning with the utterances\ngenerated by the experts. Our MoE approach provides greater flexibility to\ngenerate sensible utterances with different intents and allows RL to focus on\nconversational-level DM. We compare it with SOTA baselines on open-domain\ndialogues and demonstrate its effectiveness both in terms of the diversity and\nsensibility of the generated utterances and the overall DM performance.",
        "pdf_link": "https://arxiv.org/pdf/2206.00059v1.pdf"
    },
    {
        "title": "On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation",
        "authors": [
            "Tiago Pimentel",
            "Clara Meister",
            "Ryan Cotterell"
        ],
        "published": "2022-05-31T17:58:49Z",
        "summary": "A good automatic evaluation metric for language generation ideally correlates\nhighly with human judgements of text quality. Yet, there is a dearth of such\nmetrics, which inhibits the rapid and efficient progress of language\ngenerators. One exception is the recently proposed Mauve. In theory, Mauve\nmeasures an information-theoretic divergence between two probability\ndistributions over strings: one representing the language generator under\nevaluation; the other representing the true natural language distribution.\nMauve's authors argue that its success comes from the qualitative properties of\ntheir proposed divergence. Yet in practice, as this divergence is uncomputable,\nMauve approximates it by measuring the divergence between multinomial\ndistributions over clusters instead, where cluster assignments are attained by\ngrouping strings based on a pre-trained language model's embeddings. As we\nshow, however, this is not a tight approximation -- in either theory or\npractice. This begs the question: why does Mauve work so well? In this work, we\nshow that Mauve was right for the wrong reasons, and that its newly proposed\ndivergence is not necessary for its high performance. In fact, classical\ndivergences paired with its proposed cluster-based approximation may actually\nserve as better evaluation metrics. We finish the paper with a probing\nanalysis; this analysis leads us to conclude that -- by encoding syntactic- and\ncoherence-level features of text, while ignoring surface-level features -- such\ncluster-based substitutes to string distributions may simply be better for\nevaluating state-of-the-art language generators.",
        "pdf_link": "https://arxiv.org/pdf/2205.16001v4.pdf"
    },
    {
        "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
        "authors": [
            "David Mguni",
            "Aivar Sootla",
            "Juliusz Ziomek",
            "Oliver Slumbers",
            "Zipeng Dai",
            "Kun Shao",
            "Jun Wang"
        ],
        "published": "2022-05-31T16:50:46Z",
        "summary": "Many real-world settings involve costs for performing actions; transaction\ncosts in financial systems and fuel costs being common examples. In these\nsettings, performing actions at each time step quickly accumulates costs\nleading to vastly suboptimal outcomes. Additionally, repeatedly acting produces\nwear and tear and ultimately, damage. Determining \\textit{when to act} is\ncrucial for achieving successful outcomes and yet, the challenge of efficiently\n\\textit{learning} to behave optimally when actions incur minimally bounded\ncosts remains unresolved. In this paper, we introduce a reinforcement learning\n(RL) framework named \\textbf{L}earnable \\textbf{I}mpulse \\textbf{C}ontrol\n\\textbf{R}einforcement \\textbf{A}lgorithm (LICRA), for learning to optimally\nselect both when to act and which actions to take when actions incur costs. At\nthe core of LICRA is a nested structure that combines RL and a form of policy\nknown as \\textit{impulse control} which learns to maximise objectives when\nactions incur costs. We prove that LICRA, which seamlessly adopts any RL\nmethod, converges to policies that optimally select when to perform actions and\ntheir optimal magnitudes. We then augment LICRA to handle problems in which the\nagent can perform at most $k<\\infty$ actions and more generally, faces a budget\nconstraint. We show LICRA learns the optimal value function and ensures budget\nconstraints are satisfied almost surely. We demonstrate empirically LICRA's\nsuperior performance against benchmark RL methods in OpenAI gym's \\textit{Lunar\nLander} and in \\textit{Highway} environments and a variant of the Merton\nportfolio problem within finance.",
        "pdf_link": "https://arxiv.org/pdf/2205.15953v4.pdf"
    },
    {
        "title": "Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain",
        "authors": [
            "Ankush Agarwal",
            "Raj Gite",
            "Shreya Laddha",
            "Pushpak Bhattacharyya",
            "Satyanarayan Kar",
            "Asif Ekbal",
            "Prabhjit Thind",
            "Rajesh Zele",
            "Ravi Shankar"
        ],
        "published": "2022-05-31T16:49:55Z",
        "summary": "In the commercial aviation domain, there are a large number of documents,\nlike, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a\nneed for a system to access these diverse repositories efficiently in order to\nservice needs in the aviation industry, like maintenance, compliance, and\nsafety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning\n(DL) based Question Answering (QA) system for aviation safety. We construct a\nKnowledge Graph from Aircraft Accident reports and contribute this resource to\nthe community of researchers. The efficacy of this resource is tested and\nproved by the aforesaid QA system. Natural Language Queries constructed from\nthe documents mentioned above are converted into SPARQL (the interface language\nof the RDF graph database) queries and answered. On the DL side, we have two\ndifferent QA models: (i) BERT QA which is a pipeline of Passage Retrieval\n(Sentence-BERT based) and Question Answering (BERT based), and (ii) the\nrecently released GPT-3. We evaluate our system on a set of queries created\nfrom the accident reports. Our combined QA system achieves 9.3% increase in\naccuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL\nperforms better than either singly.",
        "pdf_link": "https://arxiv.org/pdf/2205.15952v2.pdf"
    },
    {
        "title": "The CLRS Algorithmic Reasoning Benchmark",
        "authors": [
            "Petar Veli\u010dkovi\u0107",
            "Adri\u00e0 Puigdom\u00e8nech Badia",
            "David Budden",
            "Razvan Pascanu",
            "Andrea Banino",
            "Misha Dashevskiy",
            "Raia Hadsell",
            "Charles Blundell"
        ],
        "published": "2022-05-31T09:56:44Z",
        "summary": "Learning representations of algorithms is an emerging area of machine\nlearning, seeking to bridge concepts from neural networks with classical\nalgorithms. Several important works have investigated whether neural networks\ncan effectively reason like algorithms, typically by learning to execute them.\nThe common trend in the area, however, is to generate targeted kinds of\nalgorithmic data to evaluate specific hypotheses, making results hard to\ntransfer across publications, and increasing the barrier of entry. To\nconsolidate progress and work towards unified evaluation, we propose the CLRS\nAlgorithmic Reasoning Benchmark, covering classical algorithms from the\nIntroduction to Algorithms textbook. Our benchmark spans a variety of\nalgorithmic reasoning procedures, including sorting, searching, dynamic\nprogramming, graph algorithms, string algorithms and geometric algorithms. We\nperform extensive experiments to demonstrate how several popular algorithmic\nreasoning baselines perform on these tasks, and consequently, highlight links\nto several open challenges. Our library is readily available at\nhttps://github.com/deepmind/clrs.",
        "pdf_link": "https://arxiv.org/pdf/2205.15659v2.pdf"
    },
    {
        "title": "hmBERT: Historical Multilingual Language Models for Named Entity Recognition",
        "authors": [
            "Stefan Schweter",
            "Luisa M\u00e4rz",
            "Katharina Schmid",
            "Erion \u00c7ano"
        ],
        "published": "2022-05-31T07:30:33Z",
        "summary": "Compared to standard Named Entity Recognition (NER), identifying persons,\nlocations, and organizations in historical texts constitutes a big challenge.\nTo obtain machine-readable corpora, the historical text is usually scanned and\nOptical Character Recognition (OCR) needs to be performed. As a result, the\nhistorical corpora contain errors. Also, entities like location or organization\ncan change over time, which poses another challenge. Overall, historical texts\ncome with several peculiarities that differ greatly from modern texts and large\nlabeled corpora for training a neural tagger are hardly available for this\ndomain. In this work, we tackle NER for historical German, English, French,\nSwedish, and Finnish by training large historical language models. We\ncircumvent the need for large amounts of labeled data by using unlabeled data\nfor pretraining a language model. We propose hmBERT, a historical multilingual\nBERT-based language model, and release the model in several versions of\ndifferent sizes. Furthermore, we evaluate the capability of hmBERT by solving\ndownstream NER as part of this year's HIPE-2022 shared task and provide\ndetailed analysis and insights. For the Multilingual Classical Commentary\ncoarse-grained NER challenge, our tagger HISTeria outperforms the other teams'\nmodels for two out of three languages.",
        "pdf_link": "https://arxiv.org/pdf/2205.15575v2.pdf"
    },
    {
        "title": "A Unified Framework for Emotion Identification and Generation in Dialogues",
        "authors": [
            "Avinash Madasu",
            "Mauajama Firdaus",
            "Asif Eqbal"
        ],
        "published": "2022-05-31T02:58:49Z",
        "summary": "Social chatbots have gained immense popularity, and their appeal lies not\njust in their capacity to respond to the diverse requests from users, but also\nin the ability to develop an emotional connection with users. To further\ndevelop and promote social chatbots, we need to concentrate on increasing user\ninteraction and take into account both the intellectual and emotional quotient\nin the conversational agents. In this paper, we propose a multi-task framework\nthat jointly identifies the emotion of a given dialogue and generates response\nin accordance to the identified emotion. We employ a BERT based network for\ncreating an empathetic system and use a mixed objective function that trains\nthe end-to-end network with both the classification and generation loss.\nExperimental results show that our proposed framework outperforms current\nstate-of-the-art models",
        "pdf_link": "https://arxiv.org/pdf/2205.15513v1.pdf"
    },
    {
        "title": "Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking",
        "authors": [
            "Young-Ho Kim",
            "Sungdong Kim",
            "Minsuk Chang",
            "Sang-Woo Lee"
        ],
        "published": "2022-05-31T01:58:04Z",
        "summary": "Current natural language interaction for self-tracking tools largely depends\non bespoke implementation optimized for a specific tracking theme and data\nformat, which is neither generalizable nor scalable to a tremendous design\nspace of self-tracking. However, training machine learning models in the\ncontext of self-tracking is challenging due to the wide variety of tracking\ntopics and data formats. In this paper, we propose a novel NLP task for\nself-tracking that extracts close- and open-ended information from a\nretrospective activity log described as a plain text, and a domain-agnostic,\nGPT-3-based NLU framework that performs this task. The framework augments the\nprompt using synthetic samples to transform the task into 10-shot learning, to\naddress a cold-start problem in bootstrapping a new tracking topic. Our\npreliminary evaluation suggests that our approach significantly outperforms the\nbaseline QA models. Going further, we discuss future application domains toward\nwhich the NLP and HCI researchers can collaborate.",
        "pdf_link": "https://arxiv.org/pdf/2205.15503v3.pdf"
    },
    {
        "title": "FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm",
        "authors": [
            "Yuzhe Zhang",
            "Hong Zhang"
        ],
        "published": "2022-05-31T00:44:57Z",
        "summary": "Financial named entity recognition (FinNER) from literature is a challenging\ntask in the field of financial text information extraction, which aims to\nextract a large amount of financial knowledge from unstructured texts. It is\nwidely accepted to use sequence tagging frameworks to implement FinNER tasks.\nHowever, such sequence tagging models cannot fully take advantage of the\nsemantic information in the texts. Instead, we formulate the FinNER task as a\nmachine reading comprehension (MRC) problem and propose a new model termed\nFinBERT-MRC. This formulation introduces significant prior information by\nutilizing well-designed queries, and extracts start index and end index of\ntarget entities without decoding modules such as conditional random fields\n(CRF). We conduct experiments on a publicly available Chinese financial dataset\nChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model\nachieves average F1 scores of 92.78% and 96.80% on the two datasets,\nrespectively, with average F1 gains +3.94% and +0.89% over some sequence\ntagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code\nis available at https://github.com/zyz0000/FinBERT-MRC.",
        "pdf_link": "https://arxiv.org/pdf/2205.15485v1.pdf"
    },
    {
        "title": "Automatic Short Math Answer Grading via In-context Meta-learning",
        "authors": [
            "Mengxue Zhang",
            "Sami Baral",
            "Neil Heffernan",
            "Andrew Lan"
        ],
        "published": "2022-05-30T16:26:02Z",
        "summary": "Automatic short answer grading is an important research direction in the\nexploration of how to use artificial intelligence (AI)-based tools to improve\neducation. Current state-of-the-art approaches use neural language models to\ncreate vectorized representations of students responses, followed by\nclassifiers to predict the score. However, these approaches have several key\nlimitations, including i) they use pre-trained language models that are not\nwell-adapted to educational subject domains and/or student-generated text and\nii) they almost always train one model per question, ignoring the linkage\nacross a question and result in a significant model storage problem due to the\nsize of advanced language models. In this paper, we study the problem of\nautomatic short answer grading for students' responses to math questions and\npropose a novel framework for this task. First, we use MathBERT, a variant of\nthe popular language model BERT adapted to mathematical content, as our base\nmodel and fine-tune it for the downstream task of student response grading.\nSecond, we use an in-context learning approach that provides scoring examples\nas input to the language model to provide additional context information and\npromote generalization to previously unseen questions. We evaluate our\nframework on a real-world dataset of student responses to open-ended math\nquestions and show that our framework (often significantly) outperforms\nexisting approaches, especially for new questions that are not seen during\ntraining.",
        "pdf_link": "https://arxiv.org/pdf/2205.15219v3.pdf"
    },
    {
        "title": "Billions of Parameters Are Worth More Than In-domain Training Data: A case study in the Legal Case Entailment Task",
        "authors": [
            "Guilherme Moraes Rosa",
            "Luiz Bonifacio",
            "Vitor Jeronymo",
            "Hugo Abonizio",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2022-05-30T15:21:26Z",
        "summary": "Recent work has shown that language models scaled to billions of parameters,\nsuch as GPT-3, perform remarkably well in zero-shot and few-shot scenarios. In\nthis work, we experiment with zero-shot models in the legal case entailment\ntask of the COLIEE 2022 competition. Our experiments show that scaling the\nnumber of parameters in a language model improves the F1 score of our previous\nzero-shot result by more than 6 points, suggesting that stronger zero-shot\ncapability may be a characteristic of larger models, at least for this task.\nOur 3B-parameter zero-shot model outperforms all models, including ensembles,\nin the COLIEE 2021 test set and also achieves the best performance of a single\nmodel in the COLIEE 2022 competition, second only to the ensemble composed of\nthe 3B model itself and a smaller version of the same model. Despite the\nchallenges posed by large language models, mainly due to latency constraints in\nreal-time applications, we provide a demonstration of our zero-shot monoT5-3b\nmodel being used in production as a search engine, including for legal\ndocuments. The code for our submission and the demo of our system are available\nat https://github.com/neuralmind-ai/coliee and\nhttps://neuralsearchx.neuralmind.ai, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2205.15172v1.pdf"
    },
    {
        "title": "ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System",
        "authors": [
            "Chia-Chien Hung",
            "Tommaso Green",
            "Robert Litschko",
            "Tornike Tsereteli",
            "Sotaro Takeshita",
            "Marco Bombieri",
            "Goran Glava\u0161",
            "Simone Paolo Ponzetto"
        ],
        "published": "2022-05-30T10:31:08Z",
        "summary": "This paper introduces our proposed system for the MIA Shared Task on\nCross-lingual Open-retrieval Question Answering (COQA). In this challenging\nscenario, given an input question the system has to gather evidence documents\nfrom a multilingual pool and generate from them an answer in the language of\nthe question. We devised several approaches combining different model variants\nfor three main components: Data Augmentation, Passage Retrieval, and Answer\nGeneration. For passage retrieval, we evaluated the monolingual BM25 ranker\nagainst the ensemble of re-rankers based on multilingual pretrained language\nmodels (PLMs) and also variants of the shared task baseline, re-training it\nfrom scratch using a recently introduced contrastive loss that maintains a\nstrong gradient signal throughout training by means of mixed negative samples.\nFor answer generation, we focused on language- and domain-specialization by\nmeans of continued language model (LM) pretraining of existing multilingual\nencoders. Additionally, for both passage retrieval and answer generation, we\naugmented the training data provided by the task organizers with automatically\ngenerated question-answer pairs created from Wikipedia passages to mitigate the\nissue of data scarcity, particularly for the low-resource languages for which\nno training data were provided. Our results show that language- and\ndomain-specialization as well as data augmentation help, especially for\nlow-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2205.14981v1.pdf"
    },
    {
        "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem",
        "authors": [
            "Muning Wen",
            "Jakub Grudzien Kuba",
            "Runji Lin",
            "Weinan Zhang",
            "Ying Wen",
            "Jun Wang",
            "Yaodong Yang"
        ],
        "published": "2022-05-30T09:39:45Z",
        "summary": "Large sequence model (SM) such as GPT series and BERT has displayed\noutstanding performance and generalization capabilities on vision, language,\nand recently reinforcement learning tasks. A natural follow-up question is how\nto abstract multi-agent decision making into an SM problem and benefit from the\nprosperous development of SMs. In this paper, we introduce a novel architecture\nnamed Multi-Agent Transformer (MAT) that effectively casts cooperative\nmulti-agent reinforcement learning (MARL) into SM problems wherein the task is\nto map agents' observation sequence to agents' optimal action sequence. Our\ngoal is to build the bridge between MARL and SMs so that the modeling power of\nmodern sequence models can be unleashed for MARL. Central to our MAT is an\nencoder-decoder architecture which leverages the multi-agent advantage\ndecomposition theorem to transform the joint policy search problem into a\nsequential decision making process; this renders only linear time complexity\nfor multi-agent problems and, most importantly, endows MAT with monotonic\nperformance improvement guarantee. Unlike prior arts such as Decision\nTransformer fit only pre-collected offline data, MAT is trained by online\ntrials and errors from the environment in an on-policy fashion. To validate\nMAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo,\nDexterous Hands Manipulation, and Google Research Football benchmarks. Results\ndemonstrate that MAT achieves superior performance and data efficiency compared\nto strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that\nMAT is an excellent few-short learner on unseen tasks regardless of changes in\nthe number of agents. See our project page at\nhttps://sites.google.com/view/multi-agent-transformer.",
        "pdf_link": "https://arxiv.org/pdf/2205.14953v3.pdf"
    },
    {
        "title": "E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2022-05-30T08:25:36Z",
        "summary": "Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale\npretraining language models. However, the prior seq2seq pretraining models\ngenerally focus on reconstructive objectives on the decoder side and neglect\nthe effect of encoder-side supervision, which we argue may lead to sub-optimal\nperformance. To verify our hypothesis, we first empirically study the\nfunctionalities of the encoder and decoder in seq2seq pretrained language\nmodels, and find that the encoder takes an important but under-exploitation\nrole than the decoder regarding the downstream performance and neuron\nactivation. Therefore, we propose an encoding-enhanced seq2seq pretraining\nstrategy, namely E2S2, which improves the seq2seq models via integrating more\nefficient self-supervised information into the encoders. Specifically, E2S2\nadopts two self-supervised objectives on the encoder side from two aspects: 1)\nlocally denoising the corrupted sentence (denoising objective); and 2) globally\nlearning better sentence representations (contrastive objective). With the help\nof both objectives, the encoder can effectively distinguish the noise tokens\nand capture high-level (i.e., syntactic and semantic) knowledge, thus\nstrengthening the ability of seq2seq model to accurately achieve the\nconditional generation. On a large diversity of downstream natural language\nunderstanding and generation tasks, E2S2 dominantly improves the performance of\nits powerful backbone models, e.g., BART and T5. For example, upon BART\nbackbone, we achieve +1.1% averaged gain on the general language understanding\nevaluation (GLUE) benchmark and +1.75% F_0.5 score improvement on CoNLL2014\ndataset. We also provide in-depth analyses to show the improvement stems from\nbetter linguistic representation. We hope that our work will foster future\nself-supervision research on seq2seq language model pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2205.14912v3.pdf"
    },
    {
        "title": "Rites de Passage: Elucidating Displacement to Emplacement of Refugees on Twitter",
        "authors": [
            "Aparup Khatua",
            "Wolfgang Nejdl"
        ],
        "published": "2022-05-30T05:12:34Z",
        "summary": "Social media deliberations allow to explore refugee-related is-sues. AI-based\nstudies have investigated refugee issues mostly around a specific event and\nconsidered unimodal approaches. Contrarily, we have employed a multimodal\narchitecture for probing the refugee journeys from their home to host nations.\nWe draw insights from Arnold van Gennep's anthropological work 'Les Rites de\nPassage', which systematically analyzed an individual's transition from one\ngroup or society to another. Based on Gennep's\nseparation-transition-incorporation framework, we have identified four phases\nof refugee journeys: Arrival of Refugees, Temporal stay at Asylums,\nRehabilitation, and Integration of Refugees into the host nation. We collected\n0.23 million multimodal tweets from April 2020 to March 2021 for testing this\nproposed frame-work. We find that a combination of transformer-based language\nmodels and state-of-the-art image recognition models, such as fusion of\nBERT+LSTM and InceptionV4, can out-perform unimodal models. Subsequently, to\ntest the practical implication of our proposed model in real-time, we have\nconsidered 0.01 million multimodal tweets related to the 2022 Ukrainian refugee\ncrisis. An F1-score of 71.88 % for this 2022 crisis confirms the\ngeneralizability of our proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2206.03248v2.pdf"
    },
    {
        "title": "COVID-19 Literature Mining and Retrieval using Text Mining Approaches",
        "authors": [
            "Sanku Satya Uday",
            "Satti Thanuja Pavani",
            "T. Jaya Lakshmi",
            "Rohit Chivukula"
        ],
        "published": "2022-05-29T22:34:19Z",
        "summary": "The novel coronavirus disease (COVID-19) began in Wuhan, China, in late 2019\nand to date has infected over 148M people worldwide, resulting in 3.12M deaths.\nOn March 10, 2020, the World Health Organisation (WHO) declared it as a global\npandemic. Many academicians and researchers started to publish papers\ndescribing the latest discoveries on covid-19. The large influx of publications\nmade it hard for other researchers to go through a large amount of data and\nfind the appropriate one that helps their research. So, the proposed model\nattempts to extract relavent titles from the large corpus of research\npublications which makes the job easy for the researchers. Allen Institute for\nAI released the CORD-19 dataset, which consists of 2,00,000 journal articles\nrelated to coronavirus-related research publications from PubMed's PMC, WHO\n(World Health Organization), bioRxiv, and medRxiv pre-prints. Along with this\ndocument corpus, they have also provided a topics dataset named topics-rnd3\nconsisting of a list of topics. Each topic has three types of representations\nlike query, question, and narrative. These Datasets are made open for research,\nand also they released a TREC-COVID competition on Kaggle. Using these topics\nlike queries, our goal is to find out the relevant documents in the CORD-19\ndataset. In this research, relevant documents should be recognized for the\nposed topics in topics-rnd3 data set. The proposed model uses Natural Language\nProcessing(NLP) techniques like Bag-of-Words, Average Word-2-Vec, Average BERT\nBase model and Tf-Idf weighted Word2Vec model to fabricate vectors for query,\nquestion, narrative, and combinations of them. Similarly, fabricate vectors for\ntitles in the CORD-19 dataset. After fabricating vectors, cosine similarity is\nused for finding similarities between every two vectors. Cosine similarity\nhelps us to find relevant documents for the given topic.",
        "pdf_link": "https://arxiv.org/pdf/2205.14781v1.pdf"
    },
    {
        "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
        "authors": [
            "Wenyi Hong",
            "Ming Ding",
            "Wendi Zheng",
            "Xinghan Liu",
            "Jie Tang"
        ],
        "published": "2022-05-29T19:02:15Z",
        "summary": "Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2205.15868v1.pdf"
    },
    {
        "title": "Urdu News Article Recommendation Model using Natural Language Processing Techniques",
        "authors": [
            "Syed Zain Abbas",
            "Dr. Arif ur Rahman",
            "Abdul Basit Mughal",
            "Syed Mujtaba Haider"
        ],
        "published": "2022-05-29T12:43:32Z",
        "summary": "There are several online newspapers in urdu but for the users it is difficult\nto find the content they are looking for because these most of them contain\nirrelevant data and most users did not get what they want to retrieve. Our\nproposed framework will help to predict Urdu news in the interests of users and\nreduce the users searching time for news. For this purpose, NLP techniques are\nused for pre-processing, and then TF-IDF with cosine similarity is used for\ngaining the highest similarity and recommended news on user preferences.\nMoreover, the BERT language model is also used for similarity, and by using the\nBERT model similarity increases as compared to TF-IDF so the approach works\nbetter with the BERT language model and recommends news to the user on their\ninterest. The news is recommended when the similarity of the articles is above\n60 percent.",
        "pdf_link": "https://arxiv.org/pdf/2206.11862v1.pdf"
    },
    {
        "title": "Micro-Expression Recognition Based on Attribute Information Embedding and Cross-modal Contrastive Learning",
        "authors": [
            "Yanxin Song",
            "Jianzong Wang",
            "Tianbo Wu",
            "Zhangcheng Huang",
            "Jing Xiao"
        ],
        "published": "2022-05-29T12:28:10Z",
        "summary": "Facial micro-expressions recognition has attracted much attention recently.\nMicro-expressions have the characteristics of short duration and low intensity,\nand it is difficult to train a high-performance classifier with the limited\nnumber of existing micro-expressions. Therefore, recognizing micro-expressions\nis a challenge task. In this paper, we propose a micro-expression recognition\nmethod based on attribute information embedding and cross-modal contrastive\nlearning. We use 3D CNN to extract RGB features and FLOW features of\nmicro-expression sequences and fuse them, and use BERT network to extract text\ninformation in Facial Action Coding System. Through cross-modal contrastive\nloss, we embed attribute information in the visual network, thereby improving\nthe representation ability of micro-expression recognition in the case of\nlimited samples. We conduct extensive experiments in CASME II and MMEW\ndatabases, and the accuracy is 77.82% and 71.04%, respectively. The comparative\nexperiments show that this method has better recognition effect than other\nmethods for micro-expression recognition.",
        "pdf_link": "https://arxiv.org/pdf/2205.14643v1.pdf"
    },
    {
        "title": "Learning Locality and Isotropy in Dialogue Modeling",
        "authors": [
            "Han Wu",
            "Haochen Tan",
            "Mingjie Zhan",
            "Gangming Zhao",
            "Shaoqing Lu",
            "Ding Liang",
            "Linqi Song"
        ],
        "published": "2022-05-29T06:48:53Z",
        "summary": "Existing dialogue modeling methods have achieved promising performance on\nvarious dialogue tasks with the aid of Transformer and the large-scale\npre-trained language models. However, some recent studies revealed that the\ncontext representations produced by these methods suffer the problem of\nanisotropy. In this paper, we find that the generated representations are also\nnot conversational, losing the conversation structure information during the\ncontext modeling stage. To this end, we identify two properties in dialogue\nmodeling, i.e., locality and isotropy, and present a simple method for dialogue\nrepresentation calibration, namely SimDRC, to build isotropic and\nconversational feature spaces. Experimental results show that our approach\nsignificantly outperforms the current state-of-the-art models on three dialogue\ntasks across the automatic and human evaluation metrics. More in-depth analyses\nfurther confirm the effectiveness of our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2205.14583v2.pdf"
    },
    {
        "title": "MiniDisc: Minimal Distillation Schedule for Language Model Compression",
        "authors": [
            "Chen Zhang",
            "Yang Yang",
            "Qifan Wang",
            "Jiahao Liu",
            "Jingang Wang",
            "Wei Wu",
            "Dawei Song"
        ],
        "published": "2022-05-29T04:22:48Z",
        "summary": "Recent studies have uncovered that language model distillation is less\neffective when facing a large capacity gap between the teacher and the student,\nand introduced teacher assistant-based distillation to bridge the gap. As a\nconnection, the scale and the performance of the teacher assistant is of vital\nimportance to bring the knowledge from the teacher to the student. However,\nexisting teacher assistant-based methods require maximally many trials before\nscheduling an optimal teacher assistant. To this end, we propose a minimal\ndistillation schedule (MiniDisc) for scheduling the optimal teacher assistant\nin minimally one trial. In particular, motivated by the finding that the\nperformance of the student is positively correlated to the scale-performance\ntradeoff of the teacher assistant, MiniDisc is designed with a\n$\\lambda$-tradeoff to measure the optimality of the teacher assistant without\ntrial distillation to the student. MiniDisc then can schedule the optimal\nteacher assistant with the best $\\lambda$-tradeoff in a sandwich framework.\nMiniDisc is evaluated with an extensive set of experiments on GLUE.\nExperimental results demonstrate the improved efficiency our MiniDisc compared\nto several state-of-the-art baselines. We further apply MiniDisc to a language\nmodel with billions of parameters and show its scalability.",
        "pdf_link": "https://arxiv.org/pdf/2205.14570v3.pdf"
    },
    {
        "title": "Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit",
        "authors": [
            "Hans W. A. Hanley",
            "Deepak Kumar",
            "Zakir Durumeric"
        ],
        "published": "2022-05-28T16:54:53Z",
        "summary": "In the buildup to and in the weeks following the Russian Federation's\ninvasion of Ukraine, Russian state media outlets output torrents of misleading\nand outright false information. In this work, we study this coordinated\ninformation campaign in order to understand the most prominent state media\nnarratives touted by the Russian government to English-speaking audiences. To\ndo this, we first perform sentence-level topic analysis using the\nlarge-language model MPNet on articles published by ten different pro-Russian\npropaganda websites including the new Russian \"fact-checking\" website\nwaronfakes.com. Within this ecosystem, we show that smaller websites like\nkatehon.com were highly effective at publishing topics that were later echoed\nby other Russian sites. After analyzing this set of Russian information\nnarratives, we then analyze their correspondence with narratives and topics of\ndiscussion on the r/Russia and 10 other political subreddits. Using MPNet and a\nsemantic search algorithm, we map these subreddits' comments to the set of\ntopics extracted from our set of Russian websites, finding that 39.6% of\nr/Russia comments corresponded to narratives from pro-Russian propaganda\nwebsites compared to 8.86% on r/politics.",
        "pdf_link": "https://arxiv.org/pdf/2205.14484v3.pdf"
    },
    {
        "title": "Teaching Models to Express Their Uncertainty in Words",
        "authors": [
            "Stephanie Lin",
            "Jacob Hilton",
            "Owain Evans"
        ],
        "published": "2022-05-28T05:02:31Z",
        "summary": "We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n\"90% confidence\" or \"high confidence\"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples. To our knowledge, this is the first time a model\nhas been shown to express calibrated uncertainty about its own answers in\nnatural language. For testing calibration, we introduce the CalibratedMath\nsuite of tasks. We compare the calibration of uncertainty expressed in words\n(\"verbalized probability\") to uncertainty extracted from model logits. Both\nkinds of uncertainty are capable of generalizing calibration under distribution\nshift. We also provide evidence that GPT-3's ability to generalize calibration\ndepends on pre-trained latent representations that correlate with epistemic\nuncertainty over its answers.",
        "pdf_link": "https://arxiv.org/pdf/2205.14334v2.pdf"
    },
    {
        "title": "Multimodal Fake News Detection via CLIP-Guided Learning",
        "authors": [
            "Yangming Zhou",
            "Qichao Ying",
            "Zhenxing Qian",
            "Sheng Li",
            "Xinpeng Zhang"
        ],
        "published": "2022-05-28T02:43:18Z",
        "summary": "Multimodal fake news detection has attracted many research interests in\nsocial forensics. Many existing approaches introduce tailored attention\nmechanisms to guide the fusion of unimodal features. However, how the\nsimilarity of these features is calculated and how it will affect the\ndecision-making process in FND are still open questions. Besides, the potential\nof pretrained multi-modal feature learning models in fake news detection has\nnot been well exploited. This paper proposes a FND-CLIP framework, i.e., a\nmultimodal Fake News Detection network based on Contrastive Language-Image\nPretraining (CLIP). Given a targeted multimodal news, we extract the deep\nrepresentations from the image and text using a ResNet-based encoder, a\nBERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a\nconcatenation of the CLIP-generated features weighted by the standardized\ncross-modal similarity of the two modalities. The extracted features are\nfurther processed for redundancy reduction before feeding them into the final\nclassifier. We introduce a modality-wise attention module to adaptively\nreweight and aggregate the features. We have conducted extensive experiments on\ntypical fake news datasets. The results indicate that the proposed framework\nhas a better capability in mining crucial features for fake news detection. The\nproposed FND-CLIP can achieve better performances than previous works, i.e.,\n0.7\\%, 6.8\\% and 1.3\\% improvements in overall accuracy on Weibo, Politifact\nand Gossipcop, respectively. Besides, we justify that CLIP-based learning can\nallow better flexibility on multimodal feature selection.",
        "pdf_link": "https://arxiv.org/pdf/2205.14304v1.pdf"
    },
    {
        "title": "Few-shot Subgoal Planning with Language Models",
        "authors": [
            "Lajanugen Logeswaran",
            "Yao Fu",
            "Moontae Lee",
            "Honglak Lee"
        ],
        "published": "2022-05-28T01:03:30Z",
        "summary": "Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.",
        "pdf_link": "https://arxiv.org/pdf/2205.14288v1.pdf"
    },
    {
        "title": "Controllable Text Generation with Neurally-Decomposed Oracle",
        "authors": [
            "Tao Meng",
            "Sidi Lu",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2022-05-27T20:17:53Z",
        "summary": "We propose a general and efficient framework to control auto-regressive\ngeneration models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained\nbase language model and a sequence-level boolean oracle function, we propose to\ndecompose the oracle function into token-level guidance to steer the base model\nin text generation. Specifically, the token-level guidance is approximated by a\nneural model trained with examples sampled from the base model, demanding no\nadditional auxiliary labeled data. Based on posterior regularization, we\npresent the closed-form optimal solution to incorporate the token-level\nguidance into the base model for controllable generation. We further provide a\ntheoretical analysis of how the approximation quality of NADO affects the\ncontrollable generation results. Experiments conducted on two applications: (1)\ntext generation with lexical constraints and (2) machine translation with\nformality control demonstrate that our framework efficiently guides the base\nmodel towards the given oracle while maintaining high generation quality.",
        "pdf_link": "https://arxiv.org/pdf/2205.14219v2.pdf"
    },
    {
        "title": "Diffusion-LM Improves Controllable Text Generation",
        "authors": [
            "Xiang Lisa Li",
            "John Thickstun",
            "Ishaan Gulrajani",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2022-05-27T20:12:09Z",
        "summary": "Controlling the behavior of language models (LMs) without re-training is a\nmajor open problem in natural language generation. While recent works have\ndemonstrated successes on controlling simple sentence attributes (e.g.,\nsentiment), there has been little progress on complex, fine-grained controls\n(e.g., syntactic structure). To address this challenge, we develop a new\nnon-autoregressive language model based on continuous diffusions that we call\nDiffusion-LM. Building upon the recent successes of diffusion models in\ncontinuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian\nvectors into word vectors, yielding a sequence of intermediate latent\nvariables. The continuous, hierarchical nature of these intermediate variables\nenables a simple gradient-based algorithm to perform complex, controllable\ngeneration tasks. We demonstrate successful control of Diffusion-LM for six\nchallenging fine-grained control tasks, significantly outperforming prior work.",
        "pdf_link": "https://arxiv.org/pdf/2205.14217v1.pdf"
    },
    {
        "title": "Multimodal Masked Autoencoders Learn Transferable Representations",
        "authors": [
            "Xinyang Geng",
            "Hao Liu",
            "Lisa Lee",
            "Dale Schuurmans",
            "Sergey Levine",
            "Pieter Abbeel"
        ],
        "published": "2022-05-27T19:09:42Z",
        "summary": "Building scalable models to learn from diverse, multimodal data remains an\nopen challenge. For vision-language data, the dominant approaches are based on\ncontrastive learning objectives that train a separate encoder for each\nmodality. While effective, contrastive learning approaches introduce sampling\nbias depending on the data augmentations used, which can degrade performance on\ndownstream tasks. Moreover, these methods are limited to paired image-text\ndata, and cannot leverage widely-available unpaired data. In this paper, we\ninvestigate whether a large multimodal model trained purely via masked token\nprediction, without using modality-specific encoders or contrastive learning,\ncan learn transferable representations for downstream tasks. We propose a\nsimple and scalable network architecture, the Multimodal Masked Autoencoder\n(M3AE), which learns a unified encoder for both vision and language data via\nmasked token prediction. We provide an empirical study of M3AE trained on a\nlarge-scale image-text dataset, and find that M3AE is able to learn\ngeneralizable representations that transfer well to downstream tasks.\nSurprisingly, we find that M3AE benefits from a higher text mask ratio\n(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the\njoint training of two data modalities. We also provide qualitative analysis\nshowing that the learned representation incorporates meaningful information\nfrom both image and language. Lastly, we demonstrate the scalability of M3AE\nwith larger model size and training time, and its flexibility to train on both\npaired image-text data as well as unpaired data.",
        "pdf_link": "https://arxiv.org/pdf/2205.14204v3.pdf"
    },
    {
        "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
        "authors": [
            "Tri Dao",
            "Daniel Y. Fu",
            "Stefano Ermon",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "published": "2022-05-27T17:53:09Z",
        "summary": "Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K),\nand 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).",
        "pdf_link": "https://arxiv.org/pdf/2205.14135v2.pdf"
    },
    {
        "title": "StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes",
        "authors": [
            "Awantee Deshpande",
            "Dana Ruiter",
            "Marius Mosbach",
            "Dietrich Klakow"
        ],
        "published": "2022-05-27T15:09:56Z",
        "summary": "Analyzing ethnic or religious bias is important for improving fairness,\naccountability, and transparency of natural language processing models.\nHowever, many techniques rely on human-compiled lists of bias terms, which are\nexpensive to create and are limited in coverage. In this study, we present a\nfully data-driven pipeline for generating a knowledge graph (KG) of cultural\nknowledge and stereotypes. Our resulting KG covers 5 religious groups and 5\nnationalities and can easily be extended to include more entities. Our human\nevaluation shows that the majority (59.2%) of non-singleton entries are\ncoherent and complete stereotypes. We further show that performing intermediate\nmasked language model training on the verbalized KG leads to a higher level of\ncultural awareness in the model and has the potential to increase\nclassification performance on knowledge-crucial samples on a related task,\ni.e., hate speech detection.",
        "pdf_link": "https://arxiv.org/pdf/2205.14036v1.pdf"
    },
    {
        "title": "kNN-Prompt: Nearest Neighbor Zero-Shot Inference",
        "authors": [
            "Weijia Shi",
            "Julian Michael",
            "Suchin Gururangan",
            "Luke Zettlemoyer"
        ],
        "published": "2022-05-27T07:00:59Z",
        "summary": "Retrieval-augmented language models (LMs) use non-parametric memory to\nsubstantially outperform their non-retrieval counterparts on perplexity-based\nevaluations, but it is an open question whether they achieve similar gains in\nfew- and zero-shot end-task accuracy. We extensively study one such model, the\nk-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The\nmain challenge is to achieve coverage of the verbalizer tokens that define the\ndifferent end-task class labels. To address this challenge, we also introduce\nkNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy\nverbalizers (e.g. to expand terrible to also include silly and other\ntask-specific synonyms for sentiment classification). Across nine diverse\nend-tasks, using kNN-Prompt with GPT-2 large yields significant performance\nboosts over strong zero-shot baselines (13.4% absolute improvement over the\nbase LM on average). We also show that other advantages of non-parametric\naugmentation hold for end tasks; kNN-Prompt is effective for domain adaptation\nwith no further training, and gains increase with the size of the retrieval\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2205.13792v2.pdf"
    },
    {
        "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
        "authors": [
            "Pascal Notin",
            "Mafalda Dias",
            "Jonathan Frazer",
            "Javier Marchena-Hurtado",
            "Aidan Gomez",
            "Debora S. Marks",
            "Yarin Gal"
        ],
        "published": "2022-05-27T04:51:15Z",
        "summary": "The ability to accurately model the fitness landscape of protein sequences is\ncritical to a wide range of applications, from quantifying the effects of human\nvariants on disease likelihood, to predicting immune-escape mutations in\nviruses and designing novel biotherapeutic proteins. Deep generative models of\nprotein sequences trained on multiple sequence alignments have been the most\nsuccessful approaches so far to address these tasks. The performance of these\nmethods is however contingent on the availability of sufficiently deep and\ndiverse alignments for reliable training. Their potential scope is thus limited\nby the fact many protein families are hard, if not impossible, to align. Large\nlanguage models trained on massive quantities of non-aligned protein sequences\nfrom diverse families address these problems and show potential to eventually\nbridge the performance gap. We introduce Tranception, a novel transformer\narchitecture leveraging autoregressive predictions and retrieval of homologous\nsequences at inference to achieve state-of-the-art fitness prediction\nperformance. Given its markedly higher performance on multiple mutants,\nrobustness to shallow alignments and ability to score indels, our approach\noffers significant gain of scope over existing approaches. To enable more\nrigorous model testing across a broader range of protein families, we develop\nProteinGym -- an extensive set of multiplexed assays of variant effects,\nsubstantially increasing both the number and diversity of assays compared to\nexisting benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2205.13760v1.pdf"
    },
    {
        "title": "Quark: Controllable Text Generation with Reinforced Unlearning",
        "authors": [
            "Ximing Lu",
            "Sean Welleck",
            "Jack Hessel",
            "Liwei Jiang",
            "Lianhui Qin",
            "Peter West",
            "Prithviraj Ammanabrolu",
            "Yejin Choi"
        ],
        "published": "2022-05-26T21:11:51Z",
        "summary": "Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.",
        "pdf_link": "https://arxiv.org/pdf/2205.13636v2.pdf"
    },
    {
        "title": "Differentially Private Decoding in Large Language Models",
        "authors": [
            "Jimit Majmudar",
            "Christophe Dupuy",
            "Charith Peris",
            "Sami Smaili",
            "Rahul Gupta",
            "Richard Zemel"
        ],
        "published": "2022-05-26T20:50:58Z",
        "summary": "Recent large-scale natural language processing (NLP) systems use a\npre-trained Large Language Model (LLM) on massive and diverse corpora as a\nheadstart. In practice, the pre-trained model is adapted to a wide array of\ntasks via fine-tuning on task-specific datasets. LLMs, while effective, have\nbeen shown to memorize instances of training data thereby potentially revealing\nprivate information processed during pre-training. The potential leakage might\nfurther propagate to the downstream tasks for which LLMs are fine-tuned. On the\nother hand, privacy-preserving algorithms usually involve retraining from\nscratch, which is prohibitively expensive for LLMs. In this work, we propose a\nsimple, easy to interpret, and computationally lightweight perturbation\nmechanism to be applied to an already trained model at the decoding stage. Our\nperturbation mechanism is model-agnostic and can be used in conjunction with\nany LLM. We provide theoretical analysis showing that the proposed mechanism is\ndifferentially private, and experimental results showing a privacy-utility\ntrade-off.",
        "pdf_link": "https://arxiv.org/pdf/2205.13621v2.pdf"
    },
    {
        "title": "Training and Inference on Any-Order Autoregressive Models the Right Way",
        "authors": [
            "Andy Shih",
            "Dorsa Sadigh",
            "Stefano Ermon"
        ],
        "published": "2022-05-26T18:00:02Z",
        "summary": "Conditional inference on arbitrary subsets of variables is a core problem in\nprobabilistic inference with important applications such as masked language\nmodeling and image inpainting. In recent years, the family of Any-Order\nAutoregressive Models (AO-ARMs) -- closely related to popular models such as\nBERT and XLNet -- has shown breakthrough performance in arbitrary conditional\ntasks across a sweeping range of domains. But, in spite of their success, in\nthis paper we identify significant improvements to be made to previous\nformulations of AO-ARMs. First, we show that AO-ARMs suffer from redundancy in\ntheir probabilistic model, i.e., they define the same distribution in multiple\ndifferent ways. We alleviate this redundancy by training on a smaller set of\nunivariate conditionals that still maintains support for efficient arbitrary\nconditional inference. Second, we upweight the training loss for univariate\nconditionals that are evaluated more frequently during inference. Our method\nleads to improved performance with no compromises on tractability, giving\nstate-of-the-art likelihoods in arbitrary conditional modeling on text (Text8),\nimage (CIFAR10, ImageNet32), and continuous tabular data domains.",
        "pdf_link": "https://arxiv.org/pdf/2205.13554v2.pdf"
    },
    {
        "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality",
        "authors": [
            "Tom Zahavy",
            "Yannick Schroecker",
            "Feryal Behbahani",
            "Kate Baumli",
            "Sebastian Flennerhag",
            "Shaobo Hou",
            "Satinder Singh"
        ],
        "published": "2022-05-26T17:40:52Z",
        "summary": "Finding different solutions to the same problem is a key aspect of\nintelligence associated with creativity and adaptation to novel situations. In\nreinforcement learning, a set of diverse policies can be useful for\nexploration, transfer, hierarchy, and robustness. We propose DOMiNO, a method\nfor Diversity Optimization Maintaining Near Optimality. We formalize the\nproblem as a Constrained Markov Decision Process where the objective is to find\ndiverse policies, measured by the distance between the state occupancies of the\npolicies in the set, while remaining near-optimal with respect to the extrinsic\nreward. We demonstrate that the method can discover diverse and meaningful\nbehaviors in various domains, such as different locomotion patterns in the\nDeepMind Control Suite. We perform extensive analysis of our approach, compare\nit with other multi-objective baselines, demonstrate that we can control both\nthe quality and the diversity of the set via interpretable hyperparameters, and\nshow that the discovered set is robust to perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2205.13521v2.pdf"
    },
    {
        "title": "Federated Split BERT for Heterogeneous Text Classification",
        "authors": [
            "Zhengyang Li",
            "Shijing Si",
            "Jianzong Wang",
            "Jing Xiao"
        ],
        "published": "2022-05-26T12:21:57Z",
        "summary": "Pre-trained BERT models have achieved impressive performance in many natural\nlanguage processing (NLP) tasks. However, in many real-world situations,\ntextual data are usually decentralized over many clients and unable to be\nuploaded to a central server due to privacy protection and regulations.\nFederated learning (FL) enables multiple clients collaboratively to train a\nglobal model while keeping the local data privacy. A few researches have\ninvestigated BERT in federated learning setting, but the problem of performance\nloss caused by heterogeneous (e.g., non-IID) data over clients remain\nunder-explored. To address this issue, we propose a framework, FedSplitBERT,\nwhich handles heterogeneous data and decreases the communication cost by\nsplitting the BERT encoder layers into local part and global part. The local\npart parameters are trained by the local client only while the global part\nparameters are trained by aggregating gradients of multiple clients. Due to the\nsheer size of BERT, we explore a quantization method to further reduce the\ncommunication cost with minimal performance loss. Our framework is ready-to-use\nand compatible to many existing federated learning algorithms, including\nFedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of the\nproposed framework, which outperforms baseline methods by a significant margin,\nwhile FedSplitBERT with quantization can reduce the communication cost by\n$11.9\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2205.13299v1.pdf"
    },
    {
        "title": "Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks",
        "authors": [
            "Divyam Goel",
            "Raksha Sharma"
        ],
        "published": "2022-05-26T05:27:50Z",
        "summary": "The last few years have witnessed an exponential rise in the propagation of\noffensive text on social media. Identification of this text with high precision\nis crucial for the well-being of society. Most of the existing approaches tend\nto give high toxicity scores to innocuous statements (e.g., \"I am a gay man\").\nThese false positives result from over-generalization on the training data\nwhere specific terms in the statement may have been used in a pejorative sense\n(e.g., \"gay\"). Emphasis on such words alone can lead to discrimination against\nthe classes these systems are designed to protect. In this paper, we address\nthe problem of offensive language detection on Twitter, while also detecting\nthe type and the target of the offence. We propose a novel approach called\nSyLSTM, which integrates syntactic features in the form of the dependency parse\ntree of a sentence and semantic features in the form of word embeddings into a\ndeep learning architecture using a Graph Convolutional Network. Results show\nthat the proposed approach significantly outperforms the state-of-the-art BERT\nmodel with orders of magnitude fewer number of parameters.",
        "pdf_link": "https://arxiv.org/pdf/2205.13164v1.pdf"
    },
    {
        "title": "Matryoshka Representation Learning",
        "authors": [
            "Aditya Kusupati",
            "Gantavya Bhatt",
            "Aniket Rege",
            "Matthew Wallingford",
            "Aditya Sinha",
            "Vivek Ramanujan",
            "William Howard-Snyder",
            "Kaifeng Chen",
            "Sham Kakade",
            "Prateek Jain",
            "Ali Farhadi"
        ],
        "published": "2022-05-26T04:33:56Z",
        "summary": "Learned representations are a central component in modern ML systems, serving\na multitude of downstream tasks. When training such representations, it is\noften the case that computational and statistical constraints for each\ndownstream task are unknown. In this context rigid, fixed capacity\nrepresentations can be either over or under-accommodating to the task at hand.\nThis leads us to ask: can we design a flexible representation that can adapt to\nmultiple downstream tasks with varying computational resources? Our main\ncontribution is Matryoshka Representation Learning (MRL) which encodes\ninformation at different granularities and allows a single embedding to adapt\nto the computational constraints of downstream tasks. MRL minimally modifies\nexisting representation learning pipelines and imposes no additional cost\nduring inference and deployment. MRL learns coarse-to-fine representations that\nare at least as accurate and rich as independently trained low-dimensional\nrepresentations. The flexibility within the learned Matryoshka Representations\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\nlong-tail few-shot classification, all while being as robust as the original\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\nare open-sourced at https://github.com/RAIVNLab/MRL.",
        "pdf_link": "https://arxiv.org/pdf/2205.13147v4.pdf"
    },
    {
        "title": "BiT: Robustly Binarized Multi-distilled Transformer",
        "authors": [
            "Zechun Liu",
            "Barlas Oguz",
            "Aasish Pappu",
            "Lin Xiao",
            "Scott Yih",
            "Meng Li",
            "Raghuraman Krishnamoorthi",
            "Yashar Mehdad"
        ],
        "published": "2022-05-25T19:01:54Z",
        "summary": "Modern pre-trained transformers have rapidly advanced the state-of-the-art in\nmachine learning, but have also grown in parameters and computational\ncomplexity, making them increasingly difficult to deploy in\nresource-constrained environments. Binarization of the weights and activations\nof the network can significantly alleviate these issues, however, is\ntechnically challenging from an optimization perspective. In this work, we\nidentify a series of improvements that enables binary transformers at a much\nhigher accuracy than what was possible previously. These include a two-set\nbinarization scheme, a novel elastic binary activation function with learned\nparameters, and a method to quantize a network to its limit by successively\ndistilling higher precision models into lower precision students. These\napproaches allow for the first time, fully binarized transformer models that\nare at a practical level of accuracy, approaching a full-precision BERT\nbaseline on the GLUE language understanding benchmark within as little as 5.9%.\nCode and models are available at: https://github.com/facebookresearch/bit.",
        "pdf_link": "https://arxiv.org/pdf/2205.13016v2.pdf"
    },
    {
        "title": "Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling",
        "authors": [
            "Kaitao Song",
            "Yichong Leng",
            "Xu Tan",
            "Yicheng Zou",
            "Tao Qin",
            "Dongsheng Li"
        ],
        "published": "2022-05-25T18:00:09Z",
        "summary": "Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.",
        "pdf_link": "https://arxiv.org/pdf/2205.12986v4.pdf"
    },
    {
        "title": "NaturalProver: Grounded Mathematical Proof Generation with Language Models",
        "authors": [
            "Sean Welleck",
            "Jiacheng Liu",
            "Ximing Lu",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "2022-05-25T17:01:18Z",
        "summary": "Theorem proving in natural mathematical language - the mixture of symbolic\nand natural language used by humans - plays a central role in mathematical\nadvances and education, and tests aspects of reasoning that are core to\nintelligence. Yet it has remained underexplored with modern generative models.\nWe study large-scale language models on two new generation tasks: suggesting\nthe next step in a mathematical proof, and full proof generation. We develop\nNaturalProver, a language model that generates proofs by conditioning on\nbackground references (e.g. theorems and definitions that are either retrieved\nor human-provided), and optionally enforces their presence with constrained\ndecoding. On theorems from the NaturalProofs benchmark, NaturalProver improves\nthe quality of next-step suggestions and generated proofs over fine-tuned\nGPT-3, according to human evaluations from university-level mathematics\nstudents. NaturalProver is capable of proving some theorems that require short\n(2-6 step) proofs, and providing next-step suggestions that are rated as\ncorrect and useful over 40% of the time, which is to our knowledge the first\ndemonstration of these capabilities using neural language models.",
        "pdf_link": "https://arxiv.org/pdf/2205.12910v2.pdf"
    },
    {
        "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
        "authors": [
            "Liyan Tang",
            "Tanya Goyal",
            "Alexander R. Fabbri",
            "Philippe Laban",
            "Jiacheng Xu",
            "Semih Yavuz",
            "Wojciech Kry\u015bci\u0144ski",
            "Justin F. Rousseau",
            "Greg Durrett"
        ],
        "published": "2022-05-25T15:26:48Z",
        "summary": "The propensity of abstractive summarization models to make factual errors has\nbeen studied extensively, including design of metrics to detect factual errors\nand annotation of errors in current systems' outputs. However, the\never-evolving nature of summarization systems, metrics, and annotated\nbenchmarks makes factuality evaluation a moving target, and drawing clear\ncomparisons among metrics has become increasingly difficult. In this work, we\naggregate factuality error annotations from nine existing datasets and stratify\nthem according to the underlying summarization model. We compare performance of\nstate-of-the-art factuality metrics, including recent ChatGPT-based metrics, on\nthis stratified benchmark and show that their performance varies significantly\nacross different types of summarization models. Critically, our analysis shows\nthat much of the recent improvement in the factuality detection space has been\non summaries from older (pre-Transformer) models instead of more relevant\nrecent summarization models. We further perform a finer-grained analysis per\nerror-type and find similar performance variance across error types for\ndifferent factuality metrics. Our results show that no one metric is superior\nin all settings or for all error types, and we provide recommendations for best\npractices given these insights.",
        "pdf_link": "https://arxiv.org/pdf/2205.12854v2.pdf"
    },
    {
        "title": "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation",
        "authors": [
            "Ao Liu",
            "Haoyu Dong",
            "Naoaki Okazaki",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2022-05-25T11:55:54Z",
        "summary": "Logical table-to-text generation is a task that involves generating logically\nfaithful sentences from tables, which requires models to derive logical level\nfacts from table records via logical inference. It raises a new challenge on\nthe logical-level content planning of table-to-text models. However, directly\nlearning the logical inference knowledge from table-text pairs is very\ndifficult for neural models because of the ambiguity of natural language and\nthe scarcity of parallel data. Hence even large-scale pre-trained language\nmodels present low logical fidelity on logical table-to-text. In this work, we\npropose a PLOG (Pretrained Logical Form Generator) framework to improve the\ngeneration fidelity. Specifically, PLOG is first pretrained on a\ntable-to-logic-form generation (table-to-logic) task, then finetuned on\ndownstream table-to-text tasks. The formal definition of logical forms enables\nus to collect large amount of accurate logical forms from tables without human\nannotation. In addition, PLOG can learn logical inference from table-logic\npairs much more definitely than from table-text pairs. To evaluate our model,\nwe further collect a controlled logical table-to-text dataset CONTLOG based on\nan existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms\nstrong baselines by a large margin on the logical fidelity, demonstrating the\neffectiveness of table-to-logic pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2205.12697v2.pdf"
    },
    {
        "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models",
        "authors": [
            "Clara Na",
            "Sanket Vaibhav Mehta",
            "Emma Strubell"
        ],
        "published": "2022-05-25T11:54:37Z",
        "summary": "Model compression by way of parameter pruning, quantization, or distillation\nhas recently gained popularity as an approach for reducing the computational\nrequirements of modern deep neural network models for NLP. Inspired by prior\nworks suggesting a connection between simpler, more generalizable models and\nthose that lie within wider loss basins, we hypothesize that optimizing for\nflat minima should lead to simpler parameterizations and thus more compressible\nmodels. We propose to combine sharpness-aware minimization (SAM) with various\ntask-specific model compression methods, including iterative magnitude pruning\n(IMP), structured pruning with a distillation objective, and post-training\ndynamic quantization. Empirically, we show that optimizing for flatter minima\nconsistently leads to greater compressibility of parameters compared to vanilla\nAdam when fine-tuning BERT models, with little to no loss in accuracy on the\nGLUE text classification and SQuAD question answering benchmarks. Moreover, SAM\nfinds superior winning tickets during IMP that 1) are amenable to vanilla Adam\noptimization, and 2) transfer more effectively across tasks.",
        "pdf_link": "https://arxiv.org/pdf/2205.12694v2.pdf"
    },
    {
        "title": "Large Language Models are Few-Shot Clinical Information Extractors",
        "authors": [
            "Monica Agrawal",
            "Stefan Hegselmann",
            "Hunter Lang",
            "Yoon Kim",
            "David Sontag"
        ],
        "published": "2022-05-25T11:49:58Z",
        "summary": "A long-running goal of the clinical NLP community is the extraction of\nimportant variables trapped in clinical notes. However, roadblocks have\nincluded dataset shift from the general domain and a lack of public clinical\ncorpora and annotations. In this work, we show that large language models, such\nas InstructGPT, perform well at zero- and few-shot information extraction from\nclinical text despite not being trained specifically for the clinical domain.\nWhereas text classification and generation performance have already been\nstudied extensively in such models, here we additionally demonstrate how to\nleverage them to tackle a diverse set of NLP tasks which require more\nstructured outputs, including span identification, token-level sequence\nclassification, and relation extraction. Further, due to the dearth of\navailable data to evaluate these systems, we introduce new datasets for\nbenchmarking few-shot clinical information extraction based on a manual\nre-annotation of the CASI dataset for new tasks. On the clinical extraction\ntasks we studied, the GPT-3 systems significantly outperform existing zero- and\nfew-shot baselines.",
        "pdf_link": "https://arxiv.org/pdf/2205.12689v2.pdf"
    },
    {
        "title": "Investigating the Benefits of Free-Form Rationales",
        "authors": [
            "Jiao Sun",
            "Swabha Swayamdipta",
            "Jonathan May",
            "Xuezhe Ma"
        ],
        "published": "2022-05-25T11:46:11Z",
        "summary": "Free-form rationales aim to aid model interpretability by supplying the\nbackground knowledge that can help understand model decisions. Crowdsourced\nrationales are provided for commonsense QA instances in popular datasets such\nas CoS-E and ECQA, but their utility remains under-investigated. We present\nhuman studies which show that ECQA rationales indeed provide additional\nbackground information to understand a decision, while over 88% of CoS-E\nrationales do not. Inspired by this finding, we ask: can the additional context\nprovided by free-form rationales benefit models, similar to human users? We\ninvestigate the utility of rationales as an additional source of supervision,\nby varying the quantity and quality of rationales during training. After\ncontrolling for instances where rationales leak the correct answer while not\nproviding additional background knowledge, we find that incorporating only 5%\nof rationales during training can boost model performance by 47.22% for CoS-E\nand 57.14% for ECQA during inference. Moreover, we also show that rationale\nquality matters: compared to crowdsourced rationales, T5-generated rationales\nprovide not only weaker supervision to models, but are also not helpful for\nhumans in aiding model interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2206.11083v2.pdf"
    },
    {
        "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
        "authors": [
            "Kang Min Yoo",
            "Junyeob Kim",
            "Hyuhng Joon Kim",
            "Hyunsoo Cho",
            "Hwiyeol Jo",
            "Sang-Woo Lee",
            "Sang-goo Lee",
            "Taeuk Kim"
        ],
        "published": "2022-05-25T11:45:14Z",
        "summary": "Despite recent explosion of interests in in-context learning, the underlying\nmechanism and the precise impact of the quality of demonstrations remain\nelusive. Intuitively, ground-truth labels should have as much impact in\nin-context learning (ICL) as supervised learning, but recent work reported that\nthe input-label correspondence is significantly less important than previously\nthought. Intrigued by this counter-intuitive observation, we re-examine the\nimportance of ground-truth labels in in-context learning. With the introduction\nof two novel metrics, namely Label-Correctness Sensitivity and Ground-truth\nLabel Effect Ratio (GLER), we were able to conduct quantifiable analysis on the\nimpact of ground-truth label demonstrations. Through extensive analyses, we\nfind that the correct input-label mappings can have varying impacts on the\ndownstream in-context learning performances, depending on the experimental\nconfiguration. Through additional studies, we identify key components, such as\nthe verbosity of prompt templates and the language model size, as the\ncontrolling factor to achieve more noise-resilient ICL.",
        "pdf_link": "https://arxiv.org/pdf/2205.12685v2.pdf"
    },
    {
        "title": "Training Language Models with Memory Augmentation",
        "authors": [
            "Zexuan Zhong",
            "Tao Lei",
            "Danqi Chen"
        ],
        "published": "2022-05-25T11:37:29Z",
        "summary": "Recent work has improved language models (LMs) remarkably by equipping them\nwith a non-parametric memory component. However, most existing approaches only\nintroduce mem-ories at testing time or represent them using a separately\ntrained encoder, resulting in suboptimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining LMs with memory augmentation. Our approach uses a training objective\nthat directly takes in-batch examples as accessible memory. We also present new\nmethods for memory construction and data batching, which are used for adapting\nto different sets of memories--local, long-term, and external memory--at\ntesting time. We evaluate TRIME on multiple language modeling and machine\ntranslation benchmarks and show that it is able to achieve significant\nimprovements across all the settings. Concretely, TRIME reduces the perplexity\nfrom 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory\nset from the training corpus. Compared to standard LM training, TRIME adds\nnegligible computational overhead and is compatible with different neural\narchitectures, making it a versatile solution for training memory-augmented\nLMs.",
        "pdf_link": "https://arxiv.org/pdf/2205.12674v3.pdf"
    },
    {
        "title": "Few-shot Reranking for Multi-hop QA via Language Model Prompting",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2022-05-25T10:45:55Z",
        "summary": "We study few-shot reranking for multi-hop QA with open-domain questions. To\nalleviate the need for a large number of labeled question-document pairs for\nretriever training, we propose PromptRank, which relies on large language\nmodels prompting for multi-hop path reranking. PromptRank first constructs an\ninstruction-based prompt that includes a candidate document path and then\ncomputes the relevance score between a given question and the path based on the\nconditional likelihood of the question given the path prompt according to a\nlanguage model. PromptRank yields strong retrieval performance on HotpotQA with\nonly 128 training examples compared to state-of-the-art methods trained on\nthousands of examples -- 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever\nand 77.5 by multi-hop dense retrieval. Code available at\nhttps://github.com/mukhal/PromptRank",
        "pdf_link": "https://arxiv.org/pdf/2205.12650v3.pdf"
    },
    {
        "title": "Lifelong Learning Natural Language Processing Approach for Multilingual Data Classification",
        "authors": [
            "J\u0119drzej Kozal",
            "Micha\u0142 Le\u015b",
            "Pawe\u0142 Zyblewski",
            "Pawe\u0142 Ksieniewicz",
            "Micha\u0142 Wo\u017aniak"
        ],
        "published": "2022-05-25T10:34:04Z",
        "summary": "The abundance of information in digital media, which in today's world is the\nmain source of knowledge about current events for the masses, makes it possible\nto spread disinformation on a larger scale than ever before. Consequently,\nthere is a need to develop novel fake news detection approaches capable of\nadapting to changing factual contexts and generalizing previously or\nconcurrently acquired knowledge. To deal with this problem, we propose a\nlifelong learning-inspired approach, which allows for fake news detection in\nmultiple languages and the mutual transfer of knowledge acquired in each of\nthem. Both classical feature extractors, such as Term frequency-inverse\ndocument frequency or Latent Dirichlet Allocation, and integrated deep NLP\n(Natural Language Processing) BERT (Bidirectional Encoder Representations from\nTransformers) models paired with MLP (Multilayer Perceptron) classifier, were\nemployed. The results of experiments conducted on two datasets dedicated to the\nfake news classification task (in English and Spanish, respectively), supported\nby statistical analysis, confirmed that utilization of additional languages\ncould improve performance for traditional methods. Also, in some cases\nsupplementing the deep learning method with classical ones can positively\nimpact obtained results. The ability of models to generalize the knowledge\nacquired between the analyzed languages was also observed.",
        "pdf_link": "https://arxiv.org/pdf/2206.11867v1.pdf"
    },
    {
        "title": "Multimodal Knowledge Alignment with Reinforcement Learning",
        "authors": [
            "Youngjae Yu",
            "Jiwan Chung",
            "Heeseung Yun",
            "Jack Hessel",
            "JaeSung Park",
            "Ximing Lu",
            "Prithviraj Ammanabrolu",
            "Rowan Zellers",
            "Ronan Le Bras",
            "Gunhee Kim",
            "Yejin Choi"
        ],
        "published": "2022-05-25T10:12:17Z",
        "summary": "Large language models readily adapt to novel settings, even without\ntask-specific training data. Can their zero-shot capacity be extended to\nmultimodal inputs? In this work, we propose ESPER which extends language-only\nzero-shot models to unseen multimodal tasks, like image and audio captioning.\nOur key novelty is to use reinforcement learning to align multimodal inputs to\nlanguage model generations without direct supervision: for example, in the\nimage case our reward optimization relies only on cosine similarity derived\nfrom CLIP, and thus requires no additional explicitly paired (image, caption)\ndata. Because the parameters of the language model are left unchanged, the\nmodel maintains its capacity for zero-shot generalization. Experiments\ndemonstrate that ESPER outperforms baselines and prior work on a variety of\nzero-shot tasks; these include a new benchmark we collect+release, ESP dataset,\nwhich tasks models with generating several diversely-styled captions for each\nimage.",
        "pdf_link": "https://arxiv.org/pdf/2205.12630v1.pdf"
    },
    {
        "title": "Autoformalization with Large Language Models",
        "authors": [
            "Yuhuai Wu",
            "Albert Q. Jiang",
            "Wenda Li",
            "Markus N. Rabe",
            "Charles Staats",
            "Mateja Jamnik",
            "Christian Szegedy"
        ],
        "published": "2022-05-25T09:53:30Z",
        "summary": "Autoformalization is the process of automatically translating from natural\nlanguage mathematics to formal specifications and proofs. A successful\nautoformalization system could advance the fields of formal verification,\nprogram synthesis, and artificial intelligence. While the long-term goal of\nautoformalization seemed elusive for a long time, we show large language models\nprovide new prospects towards this goal. We make the surprising observation\nthat LLMs can correctly translate a significant portion ($25.3\\%$) of\nmathematical competition problems perfectly to formal specifications in\nIsabelle/HOL. We demonstrate the usefulness of this process by improving a\npreviously introduced neural theorem prover via training on these\nautoformalized theorems. Our methodology results in a new state-of-the-art\nresult on the MiniF2F theorem proving benchmark, improving the proof rate from\n$29.6\\%$ to $35.2\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2205.12615v1.pdf"
    },
    {
        "title": "ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data",
        "authors": [
            "Xiaochuang Han",
            "Yulia Tsvetkov"
        ],
        "published": "2022-05-25T09:25:06Z",
        "summary": "Large pretrained language models have been performing increasingly well in a\nvariety of downstream tasks via prompting. However, it remains unclear from\nwhere the model learns the task-specific knowledge, especially in a zero-shot\nsetup. In this work, we want to find evidence of the model's task-specific\ncompetence from pretraining and are specifically interested in locating a very\nsmall subset of pretraining data that directly supports the model in the task.\nWe call such a subset supporting data evidence and propose a novel method ORCA\nto effectively identify it, by iteratively using gradient information related\nto the downstream task. This supporting data evidence offers interesting\ninsights about the prompted language models: in the tasks of sentiment analysis\nand textual entailment, BERT shows a substantial reliance on BookCorpus, the\nsmaller corpus of BERT's two pretraining corpora, as well as on pretraining\nexamples that mask out synonyms to the task verbalizers.",
        "pdf_link": "https://arxiv.org/pdf/2205.12600v1.pdf"
    },
    {
        "title": "RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning",
        "authors": [
            "Soumya Sanyal",
            "Zeyi Liao",
            "Xiang Ren"
        ],
        "published": "2022-05-25T09:23:50Z",
        "summary": "Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in English natural\nlanguage. While the progress is promising, it is currently unclear if these\nmodels indeed perform logical reasoning by understanding the underlying logical\nsemantics in the language. To this end, we propose RobustLR, a suite of\nevaluation datasets that evaluate the robustness of these models to minimal\nlogical edits in rulebases and some standard logical equivalence conditions. In\nour experiments with RoBERTa and T5, we find that the models trained in prior\nworks do not perform consistently on the different perturbations in RobustLR,\nthus showing that the models are not robust to the proposed logical\nperturbations. Further, we find that the models find it especially hard to\nlearn logical negation and disjunction operators. Overall, using our evaluation\nsets, we demonstrate some shortcomings of the deductive reasoning-based\nlanguage models, which can eventually help towards designing better models for\nlogical reasoning over natural language. All the datasets and code base have\nbeen made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2205.12598v2.pdf"
    },
    {
        "title": "Perturbation Augmentation for Fairer NLP",
        "authors": [
            "Rebecca Qian",
            "Candace Ross",
            "Jude Fernandes",
            "Eric Smith",
            "Douwe Kiela",
            "Adina Williams"
        ],
        "published": "2022-05-25T09:00:29Z",
        "summary": "Unwanted and often harmful social biases are becoming ever more salient in\nNLP research, affecting both models and datasets. In this work, we ask whether\ntraining on demographically perturbed data leads to fairer language models. We\ncollect a large dataset of human annotated text perturbations and train a\nneural perturbation model, which we show outperforms heuristic alternatives. We\nfind that (i) language models (LMs) pre-trained on demographically perturbed\ncorpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE\ndatasets exhibit less demographic bias on downstream tasks, and (iii) fairness\nimprovements do not come at the expense of performance on downstream tasks.\nLastly, we discuss outstanding questions about how best to evaluate the\n(un)fairness of large language models. We hope that this exploration of neural\ndemographic perturbation will help drive more improvement towards fairer NLP.",
        "pdf_link": "https://arxiv.org/pdf/2205.12586v2.pdf"
    },
    {
        "title": "Gradient-Based Constrained Sampling from Language Models",
        "authors": [
            "Sachin Kumar",
            "Biswajit Paria",
            "Yulia Tsvetkov"
        ],
        "published": "2022-05-25T08:09:03Z",
        "summary": "Large pretrained language models generate fluent text but are notoriously\nhard to controllably sample from. In this work, we study constrained sampling\nfrom such language models: generating text that satisfies user-defined\nconstraints, while maintaining fluency and the model's performance in a\ndownstream task. We propose MuCoLa -- a sampling procedure that combines the\nlog-likelihood of the language model with arbitrary (differentiable)\nconstraints in a single energy function, and then generates samples in a\nnon-autoregressive manner. Specifically, it initializes the entire output\nsequence with noise and follows a Markov chain defined by Langevin Dynamics\nusing the gradients of the energy function. We evaluate MuCoLa on text\ngeneration with soft and hard constraints as well as their combinations\nobtaining significant improvements over competitive baselines for toxicity\navoidance, sentiment control, and keyword-guided generation.",
        "pdf_link": "https://arxiv.org/pdf/2205.12558v2.pdf"
    },
    {
        "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
        "authors": [
            "Mingkai Deng",
            "Jianyu Wang",
            "Cheng-Ping Hsieh",
            "Yihan Wang",
            "Han Guo",
            "Tianmin Shu",
            "Meng Song",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "published": "2022-05-25T07:50:31Z",
        "summary": "Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by \"enumeration\n(e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.",
        "pdf_link": "https://arxiv.org/pdf/2205.12548v3.pdf"
    },
    {
        "title": "Is a Question Decomposition Unit All We Need?",
        "authors": [
            "Pruthvi Patel",
            "Swaroop Mishra",
            "Mihir Parmar",
            "Chitta Baral"
        ],
        "published": "2022-05-25T07:24:09Z",
        "summary": "Large Language Models (LMs) have achieved state-of-the-art performance on\nmany Natural Language Processing (NLP) benchmarks. With the growing number of\nnew benchmarks, we build bigger and more complex LMs. However, building new LMs\nmay not be an ideal option owing to the cost, time and environmental impact\nassociated with it. We explore an alternative route: can we modify data by\nexpressing it in terms of the model's strengths, so that a question becomes\neasier for models to answer? We investigate if humans can decompose a hard\nquestion into a set of simpler questions that are relatively easier for models\nto solve. We analyze a range of datasets involving various forms of reasoning\nand find that it is indeed possible to significantly improve model performance\n(24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via\ndecomposition. Our approach provides a viable option to involve people in NLP\nresearch in a meaningful way. Our findings indicate that Human-in-the-loop\nQuestion Decomposition (HQD) can potentially provide an alternate path to\nbuilding large LMs. Code and data is available at\nhttps://github.com/Pruthvi98/QuestionDecomposition",
        "pdf_link": "https://arxiv.org/pdf/2205.12538v2.pdf"
    },
    {
        "title": "Segmenting Numerical Substitution Ciphers",
        "authors": [
            "Nada Aldarrab",
            "Jonathan May"
        ],
        "published": "2022-05-25T06:45:59Z",
        "summary": "Deciphering historical substitution ciphers is a challenging problem. Example\nproblems that have been previously studied include detecting cipher type,\ndetecting plaintext language, and acquiring the substitution key for segmented\nciphers. However, attacking unsegmented, space-free ciphers is still a\nchallenging task. Segmentation (i.e. finding substitution units) is the first\nstep towards cracking those ciphers. In this work, we propose the first\nautomatic methods to segment those ciphers using Byte Pair Encoding (BPE) and\nunigram language models. Our methods achieve an average segmentation error of\n2\\% on 100 randomly-generated monoalphabetic ciphers and 27\\% on 3 real\nhomophonic ciphers. We also propose a method for solving non-deterministic\nciphers with existing keys using a lattice and a pretrained language model. Our\nmethod leads to the full solution of the IA cipher; a real historical cipher\nthat has not been fully solved until this work.",
        "pdf_link": "https://arxiv.org/pdf/2205.12527v1.pdf"
    },
    {
        "title": "Text-to-Face Generation with StyleGAN2",
        "authors": [
            "D. M. A. Ayanthi",
            "Sarasi Munasinghe"
        ],
        "published": "2022-05-25T06:02:01Z",
        "summary": "Synthesizing images from text descriptions has become an active research area\nwith the advent of Generative Adversarial Networks. The main goal here is to\ngenerate photo-realistic images that are aligned with the input descriptions.\nText-to-Face generation (T2F) is a sub-domain of Text-to-Image generation (T2I)\nthat is more challenging due to the complexity and variation of facial\nattributes. It has a number of applications mainly in the domain of public\nsafety. Even though several models are available for T2F, there is still the\nneed to improve the image quality and the semantic alignment. In this research,\nwe propose a novel framework, to generate facial images that are well-aligned\nwith the input descriptions. Our framework utilizes the high-resolution face\ngenerator, StyleGAN2, and explores the possibility of using it in T2F. Here, we\nembed text in the input latent space of StyleGAN2 using BERT embeddings and\noversee the generation of facial images using text descriptions. We trained our\nframework on attribute-based descriptions to generate images of 1024x1024 in\nresolution. The images generated exhibit a 57% similarity to the ground truth\nimages, with a face semantic distance of 0.92, outperforming\nstate-of-the-artwork. The generated images have a FID score of 118.097 and the\nexperimental results show that our model generates promising images.",
        "pdf_link": "https://arxiv.org/pdf/2205.12512v1.pdf"
    },
    {
        "title": "Memorization in NLP Fine-tuning Methods",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Archit Uniyal",
            "Tianhao Wang",
            "David Evans",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022-05-25T05:49:31Z",
        "summary": "Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.",
        "pdf_link": "https://arxiv.org/pdf/2205.12506v2.pdf"
    },
    {
        "title": "Fine-grained Contrastive Learning for Relation Extraction",
        "authors": [
            "William Hogan",
            "Jiacheng Li",
            "Jingbo Shang"
        ],
        "published": "2022-05-25T05:03:01Z",
        "summary": "Recent relation extraction (RE) works have shown encouraging improvements by\nconducting contrastive learning on silver labels generated by distant\nsupervision before fine-tuning on gold labels. Existing methods typically\nassume all these silver labels are accurate and treat them equally; however,\ndistant supervision is inevitably noisy -- some silver labels are more reliable\nthan others. In this paper, we propose fine-grained contrastive learning\n(FineCL) for RE, which leverages fine-grained information about which silver\nlabels are and are not noisy to improve the quality of learned relationship\nrepresentations for RE. We first assess the quality of silver labels via a\nsimple and automatic approach we call \"learning order denoising,\" where we\ntrain a language model to learn these relations and record the order of learned\ntraining instances. We show that learning order largely corresponds to label\naccuracy -- early-learned silver labels have, on average, more accurate labels\nthan later-learned silver labels. Then, during pre-training, we increase the\nweights of accurate labels within a novel contrastive learning objective.\nExperiments on several RE benchmarks show that FineCL makes consistent and\nsignificant performance gains over state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2205.12491v2.pdf"
    },
    {
        "title": "Conditional set generation using Seq2seq models",
        "authors": [
            "Aman Madaan",
            "Dheeraj Rajagopal",
            "Niket Tandon",
            "Yiming Yang",
            "Antoine Bosselut"
        ],
        "published": "2022-05-25T04:17:50Z",
        "summary": "Conditional set generation learns a mapping from an input sequence of tokens\nto a set. Several NLP tasks, such as entity typing and dialogue emotion\ntagging, are instances of set generation. Seq2Seq models, a popular choice for\nset generation, treat a set as a sequence and do not fully leverage its key\nproperties, namely order-invariance and cardinality. We propose a novel\nalgorithm for effectively sampling informative orders over the combinatorial\nspace of label orders. We jointly model the set cardinality and output by\nprepending the set size and taking advantage of the autoregressive\nfactorization used by Seq2Seq models. Our method is a model-independent data\naugmentation approach that endows any Seq2Seq model with the signals of\norder-invariance and cardinality. Training a Seq2Seq model on this augmented\ndata (without any additional annotations) gets an average relative improvement\nof 20% on four benchmark datasets across various models: BART, T5, and GPT-3.\nCode to use SETAUG available at: https://setgen.structgen.com.",
        "pdf_link": "https://arxiv.org/pdf/2205.12485v2.pdf"
    },
    {
        "title": "Low Resource Style Transfer via Domain Adaptive Meta Learning",
        "authors": [
            "Xiangyang Li",
            "Xiang Long",
            "Yu Xia",
            "Sujian Li"
        ],
        "published": "2022-05-25T03:58:24Z",
        "summary": "Text style transfer (TST) without parallel data has achieved some practical\nsuccess. However, most of the existing unsupervised text style transfer methods\nsuffer from (i) requiring massive amounts of non-parallel data to guide\ntransferring different text styles. (ii) colossal performance degradation when\nfine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain\nAdaptive Meta-Learning with Adversarial Transfer Model), which consists of two\nparts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn\ngeneral knowledge in multiple heterogeneous source domains, capable of adapting\nto new unseen domains with a small amount of data. Moreover, we propose a new\nunsupervised TST approach Adversarial Transfer Model (ATM), composed of a\nsequence-to-sequence pre-trained language model and uses adversarial style\ntraining for better content preservation and style transfer. Results on\nmulti-domain datasets demonstrate that our approach generalizes well on unseen\nlow-resource domains, achieving state-of-the-art results against ten strong\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2205.12475v1.pdf"
    },
    {
        "title": "Improving CTC-based ASR Models with Gated Interlayer Collaboration",
        "authors": [
            "Yuting Yang",
            "Yuke Li",
            "Binbin Du"
        ],
        "published": "2022-05-25T03:21:27Z",
        "summary": "The CTC-based automatic speech recognition (ASR) models without the external\nlanguage model usually lack the capacity to model conditional dependencies and\ntextual interactions. In this paper, we present a Gated Interlayer\nCollaboration (GIC) mechanism to improve the performance of CTC-based models,\nwhich introduces textual information into the model and thus relaxes the\nconditional independence assumption of CTC-based models. Specifically, we\nconsider the weighted sum of token embeddings as the textual representation for\neach position, where the position-specific weights are the softmax probability\ndistribution constructed via inter-layer auxiliary CTC losses. The textual\nrepresentations are then fused with acoustic features by developing a gate\nunit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the\nproposed method outperforms several strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2205.12462v2.pdf"
    },
    {
        "title": "Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning",
        "authors": [
            "Mozhdeh Gheini",
            "Xuezhe Ma",
            "Jonathan May"
        ],
        "published": "2022-05-25T02:51:57Z",
        "summary": "A recent family of techniques, dubbed lightweight fine-tuning methods,\nfacilitates parameter-efficient transfer learning by updating only a small set\nof additional parameters while keeping the parameters of the pretrained\nlanguage model frozen. While proven to be an effective method, there are no\nexisting studies on if and how such knowledge of the downstream fine-tuning\napproach should affect the pretraining stage. In this work, we show that taking\nthe ultimate choice of fine-tuning method into consideration boosts the\nperformance of parameter-efficient fine-tuning. By relying on\noptimization-based meta-learning using MAML with certain modifications for our\ndistinct purpose, we prime the pretrained model specifically for\nparameter-efficient fine-tuning, resulting in gains of up to 1.7 points on\ncross-lingual NER fine-tuning. Our ablation settings and analyses further\nreveal that the tweaks we introduce in MAML are crucial for the attained gains.",
        "pdf_link": "https://arxiv.org/pdf/2205.12453v2.pdf"
    },
    {
        "title": "Sparse*BERT: Sparse Models Generalize To New tasks and Domains",
        "authors": [
            "Daniel Campos",
            "Alexandre Marques",
            "Tuan Nguyen",
            "Mark Kurtz",
            "ChengXiang Zhai"
        ],
        "published": "2022-05-25T02:51:12Z",
        "summary": "Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make using these models less costly, recent work has explored\nleveraging structured and unstructured pruning, quantization, and distillation\nto improve inference speed and decrease size. This paper studies how models\npruned using Gradual Unstructured Magnitude Pruning can transfer between\ndomains and tasks. Our experimentation shows that models that are pruned during\npretraining using general domain masked language models can transfer to novel\ndomains and tasks without extensive hyperparameter exploration or specialized\napproaches. We demonstrate that our general sparse model Sparse*BERT can become\nSparseBioBERT simply by pretraining the compressed architecture on unstructured\nbiomedical text. Moreover, we show that SparseBioBERT can match the quality of\nBioBERT with only 10\\% of the parameters.",
        "pdf_link": "https://arxiv.org/pdf/2205.12452v3.pdf"
    },
    {
        "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?",
        "authors": [
            "Ivan Kobyzev",
            "Aref Jafari",
            "Mehdi Rezagholizadeh",
            "Tianda Li",
            "Alan Do-Omri",
            "Peng Lu",
            "Pascal Poupart",
            "Ali Ghodsi"
        ],
        "published": "2022-05-25T01:26:31Z",
        "summary": "Knowledge Distillation (KD) is a prominent neural model compression technique\nthat heavily relies on teacher network predictions to guide the training of a\nstudent model. Considering the ever-growing size of pre-trained language models\n(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is\nevident that in KD, deploying the teacher network during training adds to the\nmemory and computational requirements of training. In the computer vision\nliterature, the necessity of the teacher network is put under scrutiny by\nshowing that KD is a label regularization technique that can be replaced with\nlighter teacher-free variants such as the label-smoothing technique. However,\nto the best of our knowledge, this issue is not investigated in NLP. Therefore,\nthis work concerns studying different label regularization techniques and\nwhether we actually need them to improve the fine-tuning of smaller PLM\nnetworks on downstream tasks. In this regard, we did a comprehensive set of\nexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600\ndistinct trials and ran each configuration five times. This investigation led\nto a surprising observation that KD and other label regularization techniques\ndo not play any meaningful role over regular fine-tuning when the student model\nis pre-trained. We further explore this phenomenon in different settings of NLP\nand computer vision tasks and demonstrate that pre-training itself acts as a\nkind of regularization, and additional label regularization is unnecessary.",
        "pdf_link": "https://arxiv.org/pdf/2205.12428v2.pdf"
    },
    {
        "title": "FLUTE: Figurative Language Understanding through Textual Explanations",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Debanjan Ghosh",
            "Smaranda Muresan"
        ],
        "published": "2022-05-24T23:25:02Z",
        "summary": "Figurative language understanding has been recently framed as a recognizing\ntextual entailment (RTE) task (a.k.a. natural language inference, or NLI).\nHowever, similar to classical RTE/NLI datasets, the current benchmarks suffer\nfrom spurious correlations and annotation artifacts. To tackle this problem,\nwork on NLI has built explanation-based datasets such as e-SNLI, allowing us to\nprobe whether language models are right for the right reasons.Yet no such data\nexists for figurative language, making it harder to assess genuine\nunderstanding of such expressions. To address this issue, we release FLUTE, a\ndataset of 9,000 figurative NLI instances with explanations, spanning four\ncategories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through\na model-in-the-loop framework based on GPT-3, crowd workers, and expert\nannotators. We show how utilizing GPT-3 in conjunction with human annotators\n(novices and experts) can aid in scaling up the creation of datasets even for\nsuch complex linguistic phenomena as figurative language. The baseline\nperformance of the T5 model fine-tuned on FLUTE shows that our dataset can\nbring us a step closer to developing models that understand figurative language\nthrough textual explanations.",
        "pdf_link": "https://arxiv.org/pdf/2205.12404v3.pdf"
    },
    {
        "title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT",
        "authors": [
            "James Lee-Thorp",
            "Joshua Ainslie"
        ],
        "published": "2022-05-24T23:08:54Z",
        "summary": "We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and\nSuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms BERT on SuperGLUE, but trains and runs nearly\ntwice as fast. We justify the design of these two models by carefully ablating\nthrough various mixing mechanisms, MoE configurations and hyperparameters.\nSparse Mixer overcomes many of the latency and stability concerns of MoE models\nand offers the prospect of serving sparse student models, without resorting to\ndistilling them to dense variants.",
        "pdf_link": "https://arxiv.org/pdf/2205.12399v2.pdf"
    },
    {
        "title": "Fine-tuned Language Models are Continual Learners",
        "authors": [
            "Thomas Scialom",
            "Tuhin Chakrabarty",
            "Smaranda Muresan"
        ],
        "published": "2022-05-24T22:53:34Z",
        "summary": "Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.",
        "pdf_link": "https://arxiv.org/pdf/2205.12393v4.pdf"
    },
    {
        "title": "Toxicity Detection with Generative Prompt-based Inference",
        "authors": [
            "Yau-Shian Wang",
            "Yingshan Chang"
        ],
        "published": "2022-05-24T22:44:43Z",
        "summary": "Due to the subtleness, implicity, and different possible interpretations\nperceived by different people, detecting undesirable content from text is a\nnuanced difficulty. It is a long-known risk that language models (LMs), once\ntrained on corpus containing undesirable content, have the power to manifest\nbiases and toxicity. However, recent studies imply that, as a remedy, LMs are\nalso capable of identifying toxic content without additional fine-tuning.\nPrompt-methods have been shown to effectively harvest this surprising\nself-diagnosing capability. However, existing prompt-based methods usually\nspecify an instruction to a language model in a discriminative way. In this\nwork, we explore the generative variant of zero-shot prompt-based toxicity\ndetection with comprehensive trials on prompt engineering. We evaluate on three\ndatasets with toxicity labels annotated on social media posts. Our analysis\nhighlights the strengths of our generative classification approach both\nquantitatively and qualitatively. Interesting aspects of self-diagnosis and its\nethical implications are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2205.12390v1.pdf"
    },
    {
        "title": "Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint",
        "authors": [
            "Heng-Yi Wu",
            "Jingqing Zhang",
            "Julia Ive",
            "Tong Li",
            "Vibhor Gupta",
            "Bingyuan Chen",
            "Yike Guo"
        ],
        "published": "2022-05-24T21:10:57Z",
        "summary": "Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.",
        "pdf_link": "https://arxiv.org/pdf/2205.12368v2.pdf"
    },
    {
        "title": "K-12BERT: BERT for K-12 education",
        "authors": [
            "Vasu Goel",
            "Dhruv Sahnan",
            "Venktesh V",
            "Gaurav Sharma",
            "Deep Dwivedi",
            "Mukesh Mohania"
        ],
        "published": "2022-05-24T19:35:41Z",
        "summary": "Online education platforms are powered by various NLP pipelines, which\nutilize models like BERT to aid in content curation. Since the inception of the\npre-trained language models like BERT, there have also been many efforts toward\nadapting these pre-trained models to specific domains. However, there has not\nbeen a model specifically adapted for the education domain (particularly K-12)\nacross subjects to the best of our knowledge. In this work, we propose to train\na language model on a corpus of data curated by us across multiple subjects\nfrom various sources for K-12 education. We also evaluate our model, K12-BERT,\non downstream tasks like hierarchical taxonomy tagging.",
        "pdf_link": "https://arxiv.org/pdf/2205.12335v1.pdf"
    },
    {
        "title": "Garden-Path Traversal in GPT-2",
        "authors": [
            "William Jurayj",
            "William Rudman",
            "Carsten Eickhoff"
        ],
        "published": "2022-05-24T18:21:58Z",
        "summary": "In recent years, large-scale transformer decoders such as the GPT-x family of\nmodels have become increasingly popular. Studies examining the behavior of\nthese models tend to focus only on the output of the language modeling head and\navoid analysis of the internal states of the transformer decoder. In this\nstudy, we present a collection of methods to analyze the hidden states of GPT-2\nand use the model's navigation of garden path sentences as a case study. To\nenable this, we compile the largest currently available dataset of garden path\nsentences. We show that Manhattan distances and cosine similarities provide\nmore reliable insights compared to established surprisal methods that analyze\nnext-token probabilities computed by a language modeling head. Using these\nmethods, we find that negating tokens have minimal impacts on the model's\nrepresentations for unambiguous forms of sentences with ambiguity solely over\nwhat the object of a verb is, but have a more substantial impact of\nrepresentations for unambiguous sentences whose ambiguity would stem from the\nvoice of a verb. Further, we find that analyzing the decoder model's hidden\nstates reveals periods of ambiguity that might conclude in a garden path effect\nbut happen not to, whereas surprisal analyses routinely miss this detail.",
        "pdf_link": "https://arxiv.org/pdf/2205.12302v2.pdf"
    },
    {
        "title": "Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing",
        "authors": [
            "Linlu Qiu",
            "Peter Shaw",
            "Panupong Pasupat",
            "Tianze Shi",
            "Jonathan Herzig",
            "Emily Pitler",
            "Fei Sha",
            "Kristina Toutanova"
        ],
        "published": "2022-05-24T17:57:39Z",
        "summary": "Despite their strong performance on many tasks, pre-trained language models\nhave been shown to struggle on out-of-distribution compositional\ngeneralization. Meanwhile, recent work has shown considerable improvements on\nmany NLP tasks from model scaling. Can scaling up model size also improve\ncompositional generalization in semantic parsing? We evaluate encoder-decoder\nmodels up to 11B parameters and decoder-only models up to 540B parameters, and\ncompare model scaling curves for three different methods for applying a\npre-trained language model to a new task: fine-tuning all parameters, prompt\ntuning, and in-context learning. We observe that fine-tuning generally has flat\nor negative scaling curves on out-of-distribution compositional generalization\nin semantic parsing evaluations. In-context learning has positive scaling\ncurves, but is generally outperformed by much smaller fine-tuned models.\nPrompt-tuning can outperform fine-tuning, suggesting further potential\nimprovements from scaling as it exhibits a more positive scaling curve.\nAdditionally, we identify several error trends that vary with model scale. For\nexample, larger models are generally better at modeling the syntax of the\noutput space, but are also more prone to certain types of overfitting. Overall,\nour study highlights limitations of current techniques for effectively\nleveraging model scale for compositional generalization, while our analysis\nalso suggests promising directions for future work.",
        "pdf_link": "https://arxiv.org/pdf/2205.12253v2.pdf"
    },
    {
        "title": "EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start",
        "authors": [
            "Jonathan Mallinson",
            "Jakub Adamek",
            "Eric Malmi",
            "Aliaksei Severyn"
        ],
        "published": "2022-05-24T17:13:22Z",
        "summary": "We present EdiT5 - a novel semi-autoregressive text-editing model designed to\ncombine the strengths of non-autoregressive text-editing and autoregressive\ndecoding. EdiT5 is faster during inference than conventional\nsequence-to-sequence (seq2seq) models, while being capable of modelling\nflexible input-output transformations.\n  This is achieved by decomposing the generation process into three sub-tasks:\n(1) tagging to decide on the subset of input tokens to be preserved in the\noutput, (2) re-ordering to define their order in the output text, and (3)\ninsertion to infill the missing tokens that are not present in the input. The\ntagging and re-ordering steps, which are responsible for generating the largest\nportion of the output, are non-autoregressive, while the insertion step uses an\nautoregressive decoder.\n  Depending on the task, EdiT5 on average requires significantly fewer\nautoregressive steps, demonstrating speedups of up to 25x when compared to\nseq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5\ncheckpoint yielding comparable performance to T5 in high-resource settings when\nevaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,\nand Decontextualization while clearly outperforming T5 in low-resource\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2205.12209v2.pdf"
    },
    {
        "title": "PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation",
        "authors": [
            "Aitor Ormazabal",
            "Mikel Artetxe",
            "Manex Agirrezabal",
            "Aitor Soroa",
            "Eneko Agirre"
        ],
        "published": "2022-05-24T17:09:55Z",
        "summary": "Formal verse poetry imposes strict constraints on the meter and rhyme scheme\nof poems. Most prior work on generating this type of poetry uses existing poems\nfor supervision, which are difficult to obtain for most languages and poetic\nforms. In this work, we propose an unsupervised approach to generate poems\nfollowing any given meter and rhyme scheme, without requiring any poetic text\nfor training. Our method works by splitting a regular, non-poetic corpus into\nphrases, prepending control codes that describe the length and end rhyme of\neach phrase, and training a transformer language model in the augmented corpus.\nDuring inference, we build control codes for the desired meter and rhyme\nscheme, and condition our language model on them to generate formal verse\npoetry. Experiments in Spanish and Basque show that our approach is able to\ngenerate valid poems, which are often comparable in quality to those written by\nhumans.",
        "pdf_link": "https://arxiv.org/pdf/2205.12206v2.pdf"
    },
    {
        "title": "Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift",
        "authors": [
            "Xueying Bai",
            "Jinghuan Shang",
            "Yifan Sun",
            "Niranjan Balasubramanian"
        ],
        "published": "2022-05-24T16:41:30Z",
        "summary": "Continual learning (CL) aims to learn a sequence of tasks over time, with\ndata distributions shifting from one task to another. When training on new task\ndata, data representations from old tasks may drift. Some negative\nrepresentation drift can result in catastrophic forgetting, by causing the\nlocally learned class prototypes and data representations to correlate poorly\nacross tasks. To mitigate such representation drift, we propose a method that\nfinds global prototypes to guide the learning, and learns data representations\nwith the regularization of the self-supervised information. Specifically, for\nNLP tasks, we formulate each task in a masked language modeling style, and\nlearn the task via a neighbor attention mechanism over a pre-trained language\nmodel. Experimental results show that our proposed method can learn fairly\nconsistent representations with less representation drift, and significantly\nreduce catastrophic forgetting in CL without resampling data from past tasks.",
        "pdf_link": "https://arxiv.org/pdf/2205.12186v2.pdf"
    },
    {
        "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
        "authors": [
            "Shitao Xiao",
            "Zheng Liu",
            "Yingxia Shao",
            "Zhao Cao"
        ],
        "published": "2022-05-24T12:43:04Z",
        "summary": "Despite pre-training's progress in many important NLP tasks, it remains to\nexplore effective pre-training strategies for dense retrieval. In this paper,\nwe propose RetroMAE, a new retrieval oriented pre-training paradigm based on\nMasked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs.\n1) A novel MAE workflow, where the input sentence is polluted for encoder and\ndecoder with different masks. The sentence embedding is generated from the\nencoder's masked input; then, the original sentence is recovered based on the\nsentence embedding and the decoder's masked input via masked language modeling.\n2) Asymmetric model structure, with a full-scale BERT like transformer as\nencoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios,\nwith a moderate ratio for encoder: 15~30%, and an aggressive ratio for decoder:\n50~70%. Our framework is simple to realize and empirically competitive: the\npre-trained models dramatically improve the SOTA performances on a wide range\nof dense retrieval benchmarks, like BEIR and MS MARCO. The source code and\npre-trained models are made publicly available at\nhttps://github.com/staoxiao/RetroMAE so as to inspire more interesting\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2205.12035v2.pdf"
    },
    {
        "title": "Word-order typology in Multilingual BERT: A case study in subordinate-clause detection",
        "authors": [
            "Dmitry Nikolaev",
            "Sebastian Pad\u00f3"
        ],
        "published": "2022-05-24T11:35:39Z",
        "summary": "The capabilities and limitations of BERT and similar models are still unclear\nwhen it comes to learning syntactic abstractions, in particular across\nlanguages. In this paper, we use the task of subordinate-clause detection\nwithin and across languages to probe these properties. We show that this task\nis deceptively simple, with easy gains offset by a long tail of harder cases,\nand that BERT's zero-shot performance is dominated by word-order effects,\nmirroring the SVO/VSO/SOV typology.",
        "pdf_link": "https://arxiv.org/pdf/2205.11987v1.pdf"
    },
    {
        "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts",
        "authors": [
            "Akari Asai",
            "Mohammadreza Salehi",
            "Matthew E. Peters",
            "Hannaneh Hajishirzi"
        ],
        "published": "2022-05-24T10:48:33Z",
        "summary": "This work introduces a new multi-task, parameter-efficient language model\n(LM) tuning method that learns to transfer knowledge across different tasks via\na mixture of soft prompts-small prefix embedding vectors pre-trained for\ndifferent tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt\nTuning), obtains source prompts as encodings of large-scale source tasks into a\nsmall number of parameters and trains an attention module to interpolate the\nsource prompts and a newly initialized target prompt for every instance in the\ntarget task. During training, only the target task prompt and the attention\nweights, which are shared between tasks in multi-task training, are updated,\nwhile the original LM and source prompts are intact. ATTEMPT is highly\nparameter-efficient (e.g., updates 2,300 times fewer parameters than full\nfine-tuning) while achieving high task performance using knowledge from\nhigh-resource tasks. Moreover, it is modular using pre-trained soft prompts,\nand can flexibly add or remove source prompts for effective knowledge transfer.\nOur experimental results across 21 diverse NLP datasets show that ATTEMPT\nsignificantly outperforms prompt tuning and outperforms or matches fully\nfine-tuned or other parameter-efficient tuning approaches that use over ten\ntimes more parameters. Finally, ATTEMPT outperforms previous work in few-shot\nlearning settings.",
        "pdf_link": "https://arxiv.org/pdf/2205.11961v2.pdf"
    },
    {
        "title": "The Authenticity Gap in Human Evaluation",
        "authors": [
            "Kawin Ethayarajh",
            "Dan Jurafsky"
        ],
        "published": "2022-05-24T09:51:27Z",
        "summary": "Human ratings are the gold standard in NLG evaluation. The standard protocol\nis to collect ratings of generated text, average across annotators, and rank\nNLG systems by their average scores. However, little consideration has been\ngiven as to whether this approach faithfully captures human preferences.\nAnalyzing this standard protocol through the lens of utility theory in\neconomics, we identify the implicit assumptions it makes about annotators.\nThese assumptions are often violated in practice, in which case annotator\nratings cease to reflect their preferences. The most egregious violations come\nfrom using Likert scales, which provably reverse the direction of the true\npreference in certain cases. We suggest improvements to the standard protocol\nto make it more theoretically sound, but even in its improved form, it cannot\nbe used to evaluate open-ended tasks like story generation. For the latter, we\npropose a new human evaluation protocol called $\\textit{system-level\nprobabilistic assessment}$ (SPA). When human evaluation of stories is done with\nSPA, we can recover the ordering of GPT-3 models by size, with statistically\nsignificant results. However, when human evaluation is done with the standard\nprotocol, less than half of the expected preferences can be recovered (e.g.,\nthere is no significant difference between $\\texttt{curie}$ and\n$\\texttt{davinci}$, despite using a highly powered test).",
        "pdf_link": "https://arxiv.org/pdf/2205.11930v2.pdf"
    },
    {
        "title": "Large Language Models are Zero-Shot Reasoners",
        "authors": [
            "Takeshi Kojima",
            "Shixiang Shane Gu",
            "Machel Reid",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "published": "2022-05-24T09:22:26Z",
        "summary": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.",
        "pdf_link": "https://arxiv.org/pdf/2205.11916v4.pdf"
    },
    {
        "title": "Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition",
        "authors": [
            "Zihan Wang",
            "Kewen Zhao",
            "Zilong Wang",
            "Jingbo Shang"
        ],
        "published": "2022-05-24T05:36:13Z",
        "summary": "Fine-tuning pre-trained language models has recently become a common practice\nin building NLP models for various tasks, especially few-shot tasks. We argue\nthat under the few-shot setting, formulating fine-tuning closer to the\npre-training objectives shall be able to unleash more benefits from the\npre-trained language models. In this work, we take few-shot named entity\nrecognition (NER) for a pilot study, where existing fine-tuning strategies are\nmuch different from pre-training. We propose a novel few-shot fine-tuning\nframework for NER, FFF-NER. Specifically, we introduce three new types of\ntokens, \"is-entity\", \"which-type\" and bracket, so we can formulate the NER\nfine-tuning as (masked) token prediction or generation, depending on the choice\nof pre-trained language models. In our experiments, we apply FFF-NER to\nfine-tune both BERT and BART for few-shot NER on several benchmark datasets and\nobserve significant improvements over existing fine-tuning strategies,\nincluding sequence labeling, prototype meta-learning, and prompt-based\napproaches. We further perform a series of ablation studies, showing few-shot\nNER performance is strongly correlated with the similarity between fine-tuning\nand pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2205.11799v2.pdf"
    },
    {
        "title": "PERT: A New Solution to Pinyin to Character Conversion Task",
        "authors": [
            "Jinghui Xiao",
            "Qun Liu",
            "Xin Jiang",
            "Yuanfeng Xiong",
            "Haiteng Wu",
            "Zhe Zhang"
        ],
        "published": "2022-05-24T03:08:27Z",
        "summary": "Pinyin to Character conversion (P2C) task is the key task of Input Method\nEngine (IME) in commercial input software for Asian languages, such as Chinese,\nJapanese, Thai language and so on. It's usually treated as sequence labelling\ntask and resolved by language model, i.e. n-gram or RNN. However, the low\ncapacity of the n-gram or RNN limits its performance. This paper introduces a\nnew solution named PERT which stands for bidirectional Pinyin Encoder\nRepresentations from Transformers. It achieves significant improvement of\nperformance over baselines. Furthermore, we combine PERT with n-gram under a\nMarkov framework, and improve performance further. Lastly, the external lexicon\nis incorporated into PERT so as to resolve the OOD issue of IME.",
        "pdf_link": "https://arxiv.org/pdf/2205.11737v1.pdf"
    },
    {
        "title": "On the Role of Bidirectionality in Language Model Pre-Training",
        "authors": [
            "Mikel Artetxe",
            "Jingfei Du",
            "Naman Goyal",
            "Luke Zettlemoyer",
            "Ves Stoyanov"
        ],
        "published": "2022-05-24T02:25:05Z",
        "summary": "Prior work on language model pre-training has explored different\narchitectures and learning objectives, but differences in data, hyperparameters\nand evaluation make a principled comparison difficult. In this work, we focus\non bidirectionality as a key factor that differentiates existing approaches,\nand present a comprehensive study of its role in next token prediction, text\ninfilling, zero-shot priming and fine-tuning. We propose a new framework that\ngeneralizes prior approaches, including fully unidirectional models like GPT,\nfully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.\nOur framework distinguishes between two notions of bidirectionality\n(bidirectional context and bidirectional attention) and allows us to control\neach of them separately. We find that the optimal configuration is largely\napplication-dependent (e.g., bidirectional attention is beneficial for\nfine-tuning and infilling, but harmful for next token prediction and zero-shot\npriming). We train models with up to 6.7B parameters, and find differences to\nremain consistent at scale. While prior work on scaling has focused on\nleft-to-right autoregressive models, our results suggest that this approach\ncomes with some trade-offs, and it might be worthwhile to develop very large\nbidirectional models.",
        "pdf_link": "https://arxiv.org/pdf/2205.11726v2.pdf"
    },
    {
        "title": "Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions",
        "authors": [
            "Emily Allaway",
            "Jena D. Hwang",
            "Chandra Bhagavatula",
            "Kathleen McKeown",
            "Doug Downey",
            "Yejin Choi"
        ],
        "published": "2022-05-23T22:45:53Z",
        "summary": "Generics express generalizations about the world (e.g., birds can fly) that\nare not universally true (e.g., newborn birds and penguins cannot fly).\nCommonsense knowledge bases, used extensively in NLP, encode some generic\nknowledge but rarely enumerate such exceptions and knowing when a generic\nstatement holds or does not hold true is crucial for developing a comprehensive\nunderstanding of generics. We present a novel framework informed by linguistic\ntheory to generate exemplars -- specific cases when a generic holds true or\nfalse. We generate ~19k exemplars for ~650 generics and show that our framework\noutperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis\nhighlights the importance of linguistic theory-based controllability for\ngenerating exemplars, the insufficiency of knowledge bases as a source of\nexemplars, and the challenges exemplars pose for the task of natural language\ninference.",
        "pdf_link": "https://arxiv.org/pdf/2205.11658v3.pdf"
    },
    {
        "title": "FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?",
        "authors": [
            "Shikhar Tuli",
            "Bhishma Dedhia",
            "Shreshth Tuli",
            "Niraj K. Jha"
        ],
        "published": "2022-05-23T22:44:34Z",
        "summary": "The existence of a plethora of language models makes the problem of selecting\nthe best one for a custom task challenging. Most state-of-the-art methods\nleverage transformer-based models (e.g., BERT) or their variants. Training such\nmodels and exploring their hyperparameter space, however, is computationally\nexpensive. Prior work proposes several neural architecture search (NAS) methods\nthat employ performance predictors (e.g., surrogate models) to address this\nissue; however, analysis has been limited to homogeneous models that use fixed\ndimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and flexible\nmodels, namely FlexiBERT, that have varied encoder layers with a diverse set of\npossible operations and different hidden dimensions. For better-posed surrogate\nmodeling in this expanded design space, we propose a new graph-similarity-based\nembedding scheme. We also propose a novel NAS policy, called BOSHNAS, that\nleverages this new scheme, Bayesian modeling, and second-order optimization, to\nquickly train and use a neural surrogate model to converge to the optimal\narchitecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance\nfrontier upwards compared to traditional models. FlexiBERT-Mini, one of our\nproposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%\nhigher GLUE score. A FlexiBERT model with equivalent performance as the best\nhomogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed\nmodel, achieves state-of-the-art results, outperforming the baseline models by\nat least 5.7% on the GLUE benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2205.11656v1.pdf"
    },
    {
        "title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
        "authors": [
            "Afra Feyza Aky\u00fcrek",
            "Sejin Paik",
            "Muhammed Yusuf Kocyigit",
            "Seda Akbiyik",
            "\u015eerife Leman Runyun",
            "Derry Wijaya"
        ],
        "published": "2022-05-23T20:01:20Z",
        "summary": "Large language models trained on a mixture of NLP tasks that are converted\ninto a text-to-text format using prompts, can generalize into novel forms of\nlanguage and handle novel tasks. A large body of work within prompt engineering\nattempts to understand the effects of input forms and prompts in achieving\nsuperior performance. We consider an alternative measure and inquire whether\nthe way in which an input is encoded affects social biases promoted in outputs.\nIn this paper, we study T0, a large-scale multi-task text-to-text language\nmodel trained using prompt-based learning. We consider two different forms of\nsemantically equivalent inputs: question-answer format and premise-hypothesis\nformat. We use an existing bias benchmark for the former BBQ and create the\nfirst bias benchmark in natural language inference BBNLI with hand-written\nhypotheses while also converting each benchmark into the other form. The\nresults on two benchmarks suggest that given two different formulations of\nessentially the same input, T0 conspicuously acts more biased in question\nanswering form, which is seen during training, compared to premise-hypothesis\nform which is unlike its training examples. Code and data are released under\nhttps://github.com/feyzaakyurek/bbnli.",
        "pdf_link": "https://arxiv.org/pdf/2205.11605v1.pdf"
    },
    {
        "title": "Challenges in Measuring Bias via Open-Ended Language Generation",
        "authors": [
            "Afra Feyza Aky\u00fcrek",
            "Muhammed Yusuf Kocyigit",
            "Sejin Paik",
            "Derry Wijaya"
        ],
        "published": "2022-05-23T19:57:15Z",
        "summary": "Researchers have devised numerous ways to quantify social biases vested in\npretrained language models. As some language models are capable of generating\ncoherent completions given a set of textual prompts, several prompting datasets\nhave been proposed to measure biases between social groups -- posing language\ngeneration as a way of identifying biases. In this opinion paper, we analyze\nhow specific choices of prompt sets, metrics, automatic tools and sampling\nstrategies affect bias results. We find out that the practice of measuring\nbiases through text completion is prone to yielding contradicting results under\ndifferent experiment settings. We additionally provide recommendations for\nreporting biases in open-ended language generation for a more complete outlook\nof biases exhibited by a given language model. Code to reproduce the results is\nreleased under https://github.com/feyzaakyurek/bias-textgen.",
        "pdf_link": "https://arxiv.org/pdf/2205.11601v1.pdf"
    },
    {
        "title": "Simple Recurrence Improves Masked Language Models",
        "authors": [
            "Tao Lei",
            "Ran Tian",
            "Jasmijn Bastings",
            "Ankur P. Parikh"
        ],
        "published": "2022-05-23T19:38:23Z",
        "summary": "In this work, we explore whether modeling recurrence into the Transformer\narchitecture can both be beneficial and efficient, by building an extremely\nsimple recurrent module into the Transformer. We compare our model to baselines\nfollowing the training and evaluation recipe of BERT. Our results confirm that\nrecurrence can indeed improve Transformer models by a consistent margin,\nwithout requiring low-level performance optimizations, and while keeping the\nnumber of parameters constant. For example, our base model achieves an absolute\nimprovement of 2.1 points averaged across 10 tasks and also demonstrates\nincreased stability in fine-tuning over a range of learning rates.",
        "pdf_link": "https://arxiv.org/pdf/2205.11588v1.pdf"
    },
    {
        "title": "Learning to Ignore Adversarial Attacks",
        "authors": [
            "Yiming Zhang",
            "Yangqiaoyu Zhou",
            "Samuel Carton",
            "Chenhao Tan"
        ],
        "published": "2022-05-23T18:01:30Z",
        "summary": "Despite the strong performance of current NLP models, they can be brittle\nagainst adversarial attacks. To enable effective learning against adversarial\ninputs, we introduce the use of rationale models that can explicitly learn to\nignore attack tokens. We find that the rationale models can successfully ignore\nover 90% of attack tokens. This approach leads to consistent sizable\nimprovements ($\\sim$10%) over baseline models in robustness on three datasets\nfor both BERT and RoBERTa, and also reliably outperforms data augmentation with\nadversarial examples alone. In many cases, we find that our method is able to\nclose the gap between model performance on a clean test set and an attacked\ntest set and hence reduce the effect of adversarial attacks.",
        "pdf_link": "https://arxiv.org/pdf/2205.11551v2.pdf"
    },
    {
        "title": "On the Paradox of Learning to Reason from Data",
        "authors": [
            "Honghua Zhang",
            "Liunian Harold Li",
            "Tao Meng",
            "Kai-Wei Chang",
            "Guy Van den Broeck"
        ],
        "published": "2022-05-23T17:56:48Z",
        "summary": "Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be\ntrained end-to-end to solve logical reasoning problems presented in natural\nlanguage? We attempt to answer this question in a confined problem space where\nthere exists a set of parameters that perfectly simulates logical reasoning. We\nmake observations that seem to contradict each other: BERT attains near-perfect\naccuracy on in-distribution test examples while failing to generalize to other\ndata distributions over the exact same problem space. Our study provides an\nexplanation for this paradox: instead of learning to emulate the correct\nreasoning function, BERT has in fact learned statistical features that\ninherently exist in logical reasoning problems. We also show that it is\ninfeasible to jointly remove statistical features from data, illustrating the\ndifficulty of learning to reason in general. Our result naturally extends to\nother neural models and unveils the fundamental difference between learning to\nreason and learning to achieve high performance on NLP benchmarks using\nstatistical features.",
        "pdf_link": "https://arxiv.org/pdf/2205.11502v2.pdf"
    },
    {
        "title": "HyperTree Proof Search for Neural Theorem Proving",
        "authors": [
            "Guillaume Lample",
            "Marie-Anne Lachaux",
            "Thibaut Lavril",
            "Xavier Martinet",
            "Amaury Hayat",
            "Gabriel Ebner",
            "Aur\u00e9lien Rodriguez",
            "Timoth\u00e9e Lacroix"
        ],
        "published": "2022-05-23T17:49:55Z",
        "summary": "We propose an online training procedure for a transformer-based automated\ntheorem prover. Our approach leverages a new search algorithm, HyperTree Proof\nSearch (HTPS), inspired by the recent success of AlphaZero. Our model learns\nfrom previous proof searches through online training, allowing it to generalize\nto domains far from the training distribution. We report detailed ablations of\nour pipeline's main components by studying performance on three environments of\nincreasing complexity. In particular, we show that with HTPS alone, a model\ntrained on annotated proofs manages to prove 65.4% of a held-out set of\nMetamath theorems, significantly outperforming the previous state of the art of\n56.5% by GPT-f. Online training on these unproved theorems increases accuracy\nto 82.6%. With a similar computational budget, we improve the state of the art\non the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2205.11491v1.pdf"
    },
    {
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "authors": [
            "Chitwan Saharia",
            "William Chan",
            "Saurabh Saxena",
            "Lala Li",
            "Jay Whang",
            "Emily Denton",
            "Seyed Kamyar Seyed Ghasemipour",
            "Burcu Karagol Ayan",
            "S. Sara Mahdavi",
            "Rapha Gontijo Lopes",
            "Tim Salimans",
            "Jonathan Ho",
            "David J Fleet",
            "Mohammad Norouzi"
        ],
        "published": "2022-05-23T17:42:53Z",
        "summary": "We present Imagen, a text-to-image diffusion model with an unprecedented\ndegree of photorealism and a deep level of language understanding. Imagen\nbuilds on the power of large transformer language models in understanding text\nand hinges on the strength of diffusion models in high-fidelity image\ngeneration. Our key discovery is that generic large language models (e.g. T5),\npretrained on text-only corpora, are surprisingly effective at encoding text\nfor image synthesis: increasing the size of the language model in Imagen boosts\nboth sample fidelity and image-text alignment much more than increasing the\nsize of the image diffusion model. Imagen achieves a new state-of-the-art FID\nscore of 7.27 on the COCO dataset, without ever training on COCO, and human\nraters find Imagen samples to be on par with the COCO data itself in image-text\nalignment. To assess text-to-image models in greater depth, we introduce\nDrawBench, a comprehensive and challenging benchmark for text-to-image models.\nWith DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,\nLatent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen\nover other models in side-by-side comparisons, both in terms of sample quality\nand image-text alignment. See https://imagen.research.google/ for an overview\nof the results.",
        "pdf_link": "https://arxiv.org/pdf/2205.11487v1.pdf"
    },
    {
        "title": "Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",
        "authors": [
            "Masato Mita",
            "Keisuke Sakaguchi",
            "Masato Hagiwara",
            "Tomoya Mizumoto",
            "Jun Suzuki",
            "Kentaro Inui"
        ],
        "published": "2022-05-23T17:37:20Z",
        "summary": "Natural language processing technology has rapidly improved automated\ngrammatical error correction tasks, and the community begins to explore\ndocument-level revision as one of the next challenges. To go beyond\nsentence-level automated grammatical error correction to NLP-based\ndocument-level revision assistant, there are two major obstacles: (1) there are\nfew public corpora with document-level revisions being annotated by\nprofessional editors, and (2) it is not feasible to elicit all possible\nreferences and evaluate the quality of revision with such references because\nthere are infinite possibilities of revision. This paper tackles these\nchallenges. First, we introduce a new document-revision corpus, TETRA, where\nprofessional editors revised academic papers sampled from the ACL anthology\nwhich contain few trivial grammatical errors that enable us to focus more on\ndocument- and paragraph-level edits such as coherence and consistency. Second,\nwe explore reference-less and interpretable methods for meta-evaluation that\ncan detect quality improvements by document revision. We show the uniqueness of\nTETRA compared with existing document revision corpora and demonstrate that a\nfine-tuned pre-trained language model can discriminate the quality of documents\nafter revision even when the difference is subtle. This promising result will\nencourage the community to further explore automated document revision models\nand metrics in future.",
        "pdf_link": "https://arxiv.org/pdf/2205.11484v1.pdf"
    },
    {
        "title": "A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations",
        "authors": [
            "Md Mosharaf Hossain",
            "Luke Holman",
            "Anusha Kakileti",
            "Tiffany Iris Kao",
            "Nathan Raul Brito",
            "Aaron Abraham Mathews",
            "Eduardo Blanco"
        ],
        "published": "2022-05-23T17:08:30Z",
        "summary": "This paper explores a question-answer driven approach to reveal affirmative\ninterpretations from verbal negations (i.e., when a negation cue grammatically\nmodifies a verb). We create a new corpus consisting of 4,472 verbal negations\nand discover that 67.1% of them convey that an event actually occurred.\nAnnotators generate and answer 7,277 questions for the 3,001 negations that\nconvey an affirmative interpretation. We first cast the problem of revealing\naffirmative interpretations from negations as a natural language inference\n(NLI) classification task. Experimental results show that state-of-the-art\ntransformers trained with existing NLI corpora are insufficient to reveal\naffirmative interpretations. We also observe, however, that fine-tuning brings\nsmall improvements. In addition to NLI classification, we also explore the more\nrealistic task of generating affirmative interpretations directly from\nnegations with the T5 transformer. We conclude that the generation task remains\na challenge as T5 substantially underperforms humans.",
        "pdf_link": "https://arxiv.org/pdf/2205.11467v1.pdf"
    },
    {
        "title": "Multilingual Extraction and Categorization of Lexical Collocations with Graph-aware Transformers",
        "authors": [
            "Luis Espinosa-Anke",
            "Alexander Shvets",
            "Alireza Mohammadshahi",
            "James Henderson",
            "Leo Wanner"
        ],
        "published": "2022-05-23T16:47:37Z",
        "summary": "Recognizing and categorizing lexical collocations in context is useful for\nlanguage learning, dictionary compilation and downstream NLP. However, it is a\nchallenging task due to the varying degrees of frozenness lexical collocations\nexhibit. In this paper, we put forward a sequence tagging BERT-based model\nenhanced with a graph-aware transformer architecture, which we evaluate on the\ntask of collocation recognition in context. Our results suggest that explicitly\nencoding syntactic dependencies in the model architecture is helpful, and\nprovide insights on differences in collocation typification in English, Spanish\nand French.",
        "pdf_link": "https://arxiv.org/pdf/2205.11456v1.pdf"
    },
    {
        "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency",
        "authors": [
            "Giovanni Puccetti",
            "Anna Rogers",
            "Aleksandr Drozd",
            "Felice Dell'Orletta"
        ],
        "published": "2022-05-23T15:19:09Z",
        "summary": "While Transformer-based language models are generally very robust to pruning,\nthere is the recently discovered outlier phenomenon: disabling only 48 out of\n110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We\nreplicate the original evidence for the outlier phenomenon and we link it to\nthe geometry of the embedding space. We find that in both BERT and RoBERTa the\nmagnitude of hidden state coefficients corresponding to outlier dimensions\ncorrelates with the frequency of encoded tokens in pre-training data, and it\nalso contributes to the \"vertical\" self-attention pattern enabling the model to\nfocus on the special tokens. This explains the drop in performance from\ndisabling the outliers, and it suggests that to decrease anisotropicity in\nfuture models we need pre-training schemas that would better take into account\nthe skewed token distributions.",
        "pdf_link": "https://arxiv.org/pdf/2205.11380v3.pdf"
    },
    {
        "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements",
        "authors": [
            "Conrad Borchers",
            "Dalia Sara Gala",
            "Benjamin Gilburt",
            "Eduard Oravkin",
            "Wilfried Bounsi",
            "Yuki M. Asano",
            "Hannah Rose Kirk"
        ],
        "published": "2022-05-23T15:05:27Z",
        "summary": "The growing capability and availability of generative language models has\nenabled a wide range of new downstream tasks. Academic research has identified,\nquantified and mitigated biases present in language models but is rarely\ntailored to downstream tasks where wider impact on individuals and society can\nbe felt. In this work, we leverage one popular generative language model,\nGPT-3, with the goal of writing unbiased and realistic job advertisements. We\nfirst assess the bias and realism of zero-shot generated advertisements and\ncompare them to real-world advertisements. We then evaluate prompt-engineering\nand fine-tuning as debiasing methods. We find that prompt-engineering with\ndiversity-encouraging prompts gives no significant improvement to bias, nor\nrealism. Conversely, fine-tuning, especially on unbiased real advertisements,\ncan improve realism and reduce bias.",
        "pdf_link": "https://arxiv.org/pdf/2205.11374v1.pdf"
    },
    {
        "title": "The Diminishing Returns of Masked Language Models to Science",
        "authors": [
            "Zhi Hong",
            "Aswathy Ajith",
            "Gregory Pauloski",
            "Eamon Duede",
            "Kyle Chard",
            "Ian Foster"
        ],
        "published": "2022-05-23T14:35:08Z",
        "summary": "Transformer-based masked language models such as BERT, trained on general\ncorpora, have shown impressive performance on downstream tasks. It has also\nbeen demonstrated that the downstream task performance of such models can be\nimproved by pretraining larger models for longer on more data. In this work, we\nempirically evaluate the extent to which these results extend to tasks in\nscience. We use 14 domain-specific transformer-based models (including\nScholarBERT, a new 770M-parameter science-focused masked language model\npretrained on up to 225B tokens) to evaluate the impact of training data, model\nsize, pretraining and finetuning time on 12 downstream scientific tasks.\nInterestingly, we find that increasing model sizes, training data, or compute\ntime does not always lead to significant improvements (i.e., >1% F1), if at\nall, in scientific information extraction tasks and offered possible\nexplanations for the surprising performance differences.",
        "pdf_link": "https://arxiv.org/pdf/2205.11342v2.pdf"
    },
    {
        "title": "KOLD: Korean Offensive Language Dataset",
        "authors": [
            "Younghoon Jeong",
            "Juhyun Oh",
            "Jaimeen Ahn",
            "Jongwon Lee",
            "Jihyung Moon",
            "Sungjoon Park",
            "Alice Oh"
        ],
        "published": "2022-05-23T13:58:45Z",
        "summary": "Recent directions for offensive language detection are hierarchical modeling,\nidentifying the type and the target of offensive language, and interpretability\nwith offensive span annotation and prediction. These improvements are focused\non English and do not transfer well to other languages because of cultural and\nlinguistic differences. In this paper, we present the Korean Offensive Language\nDataset (KOLD) comprising 40,429 comments, which are annotated hierarchically\nwith the type and the target of offensive language, accompanied by annotations\nof the corresponding text spans. We collect the comments from NAVER news and\nYouTube platform and provide the titles of the articles and videos as the\ncontext information for the annotation process. We use these annotated comments\nas training data for Korean BERT and RoBERTa models and find that they are\neffective at offensiveness detection, target classification, and target span\ndetection while having room for improvement for target group classification and\noffensive span detection. We discover that the target group distribution\ndiffers drastically from the existing English datasets, and observe that\nproviding the context information improves the model performance in\noffensiveness detection (+0.3), target classification (+1.5), and target group\nclassification (+13.1). We publicly release the dataset and baseline models.",
        "pdf_link": "https://arxiv.org/pdf/2205.11315v2.pdf"
    },
    {
        "title": "RL with KL penalties is better viewed as Bayesian inference",
        "authors": [
            "Tomasz Korbak",
            "Ethan Perez",
            "Christopher L Buckley"
        ],
        "published": "2022-05-23T12:47:13Z",
        "summary": "Reinforcement learning (RL) is frequently employed in fine-tuning large\nlanguage models (LMs), such as GPT-3, to penalize them for undesirable features\nof generated sequences, such as offensiveness, social bias, harmfulness or\nfalsehood. The RL formulation involves treating the LM as a policy and updating\nit to maximise the expected value of a reward function which captures human\npreferences, such as non-offensiveness. In this paper, we analyze challenges\nassociated with treating a language model as an RL policy and show how avoiding\nthose challenges requires moving beyond the RL paradigm. We start by observing\nthat the standard RL approach is flawed as an objective for fine-tuning LMs\nbecause it leads to distribution collapse: turning the LM into a degenerate\ndistribution. Then, we analyze KL-regularised RL, a widely used recipe for\nfine-tuning LMs, which additionally constrains the fine-tuned LM to stay close\nto its original distribution in terms of Kullback-Leibler (KL) divergence. We\nshow that KL-regularised RL is equivalent to variational inference:\napproximating a Bayesian posterior which specifies how to update a prior LM to\nconform with evidence provided by the reward function. We argue that this\nBayesian inference view of KL-regularised RL is more insightful than the\ntypically employed RL perspective. The Bayesian inference view explains how\nKL-regularised RL avoids the distribution collapse problem and offers a\nfirst-principles derivation for its objective. While this objective happens to\nbe equivalent to RL (with a particular choice of parametric reward), there\nexist other objectives for fine-tuning LMs which are no longer equivalent to\nRL. That observation leads to a more general point: RL is not an adequate\nformal framework for problems such as fine-tuning language models. These\nproblems are best viewed as Bayesian inference: approximating a pre-defined\ntarget distribution.",
        "pdf_link": "https://arxiv.org/pdf/2205.11275v2.pdf"
    },
    {
        "title": "BBTv2: Towards a Gradient-Free Future with Large Language Models",
        "authors": [
            "Tianxiang Sun",
            "Zhengfu He",
            "Hong Qian",
            "Yunhua Zhou",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2022-05-23T11:10:19Z",
        "summary": "Most downstream adaptation methods tune all or part of the parameters of\npre-trained models (PTMs) through gradient descent, where the tuning cost\nincreases linearly with the growth of the model size. By contrast,\ngradient-free methods only require the forward computation of the PTM to tune\nthe prompt, retaining the benefits of efficient tuning and deployment. Though,\npast work on gradient-free tuning often introduces gradient descent to seek a\ngood initialization of prompt and lacks versatility across tasks and PTMs. In\nthis paper, we present BBTv2, an improved version of Black-Box Tuning, to drive\nPTMs for few-shot learning. We prepend continuous prompts to every layer of the\nPTM and propose a divide-and-conquer gradient-free algorithm to optimize the\nprompts at different layers alternately. Extensive experiments across various\ntasks and PTMs show that BBTv2 can achieve comparable performance to full model\ntuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA,\nBitFit, etc.) under few-shot settings while maintaining much fewer tunable\nparameters.",
        "pdf_link": "https://arxiv.org/pdf/2205.11200v2.pdf"
    },
    {
        "title": "Prompt Tuning for Discriminative Pre-trained Language Models",
        "authors": [
            "Yuan Yao",
            "Bowen Dong",
            "Ao Zhang",
            "Zhengyan Zhang",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Leyu Lin",
            "Maosong Sun",
            "Jianyong Wang"
        ],
        "published": "2022-05-23T10:11:50Z",
        "summary": "Recent works have shown promising results of prompt tuning in stimulating\npre-trained language models (PLMs) for natural language processing (NLP) tasks.\nHowever, to the best of our knowledge, existing works focus on prompt-tuning\ngenerative PLMs that are pre-trained to generate target tokens, such as BERT.\nIt is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be\neffectively prompt-tuned. In this work, we present DPT, the first prompt tuning\nframework for discriminative PLMs, which reformulates NLP tasks into a\ndiscriminative language modeling problem. Comprehensive experiments on text\nclassification and question answering show that, compared with vanilla\nfine-tuning, DPT achieves significantly higher performance, and also prevents\nthe unstable problem in tuning large PLMs in both full-set and low-resource\nsettings. The source code and experiment details of this paper can be obtained\nfrom https://github.com/thunlp/DPT.",
        "pdf_link": "https://arxiv.org/pdf/2205.11166v1.pdf"
    },
    {
        "title": "RuNNE-2022 Shared Task: Recognizing Nested Named Entities",
        "authors": [
            "Ekaterina Artemova",
            "Maxim Zmeev",
            "Natalia Loukachevitch",
            "Igor Rozhkov",
            "Tatiana Batura",
            "Vladimir Ivanov",
            "Elena Tutubalina"
        ],
        "published": "2022-05-23T09:50:42Z",
        "summary": "The RuNNE Shared Task approaches the problem of nested named entity\nrecognition. The annotation schema is designed in such a way, that an entity\nmay partially overlap or even be nested into another entity. This way, the\nnamed entity \"The Yermolova Theatre\" of type \"organization\" houses another\nentity \"Yermolova\" of type \"person\". We adopt the Russian NEREL dataset for the\nRuNNE Shared Task. NEREL comprises news texts written in the Russian language\nand collected from the Wikinews portal. The annotation schema includes 29\nentity types. The nestedness of named entities in NEREL reaches up to six\nlevels. The RuNNE Shared Task explores two setups. (i) In the general setup all\nentities occur more or less with the same frequency. (ii) In the few-shot setup\nthe majority of entity types occur often in the training set. However, some of\nthe entity types are have lower frequency, being thus challenging to recognize.\nIn the test set the frequency of all entity types is even.\n  This paper reports on the results of the RuNNE Shared Task. Overall the\nshared task has received 156 submissions from nine teams. Half of the\nsubmissions outperform a straightforward BERT-based baseline in both setups.\nThis paper overviews the shared task setup and discusses the submitted systems,\ndiscovering meaning insights for the problem of nested NER. The links to the\nevaluation platform and the data from the shared task are available in our\ngithub repository: https://github.com/dialogue-evaluation/RuNNE.",
        "pdf_link": "https://arxiv.org/pdf/2205.11159v1.pdf"
    },
    {
        "title": "Supporting Vision-Language Model Inference with Confounder-pruning Knowledge Prompt",
        "authors": [
            "Jiangmeng Li",
            "Wenyi Mo",
            "Wenwen Qiang",
            "Bing Su",
            "Changwen Zheng",
            "Hui Xiong",
            "Ji-Rong Wen"
        ],
        "published": "2022-05-23T07:51:15Z",
        "summary": "Vision-language models are pre-trained by aligning image-text pairs in a\ncommon space to deal with open-set visual concepts. To boost the\ntransferability of the pre-trained models, recent works adopt fixed or\nlearnable prompts, i.e., classification weights are synthesized from natural\nlanguage describing task-relevant categories, to reduce the gap between tasks\nin the training and test phases. However, how and what prompts can improve\ninference performance remains unclear. In this paper, we explicitly clarify the\nimportance of including semantic information in prompts, while existing\nprompting methods generate prompts without exploring the semantic information\nof textual labels. Manually constructing prompts with rich semantics requires\ndomain expertise and is extremely time-consuming. To cope with this issue, we\npropose a semantic-aware prompt learning method, namely CPKP, which retrieves\nan ontological knowledge graph by treating the textual label as a query to\nextract task-relevant semantic information. CPKP further introduces a\ndouble-tier confounder-pruning procedure to refine the derived semantic\ninformation. The graph-tier confounders are gradually identified and phased\nout, inspired by the principle of Granger causality. The feature-tier\nconfounders are demolished by following the maximum entropy principle in\ninformation theory. Empirically, the evaluations demonstrate the effectiveness\nof CPKP, e.g., with two shots, CPKP outperforms the manual-prompt method by\n4.64% and the learnable-prompt method by 1.09% on average, and the superiority\nof CPKP in domain generalization compared to benchmark approaches. Our\nimplementation is available at https://github.com/Mowenyii/CPKP.",
        "pdf_link": "https://arxiv.org/pdf/2205.11100v2.pdf"
    },
    {
        "title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla",
        "authors": [
            "Abhik Bhattacharjee",
            "Tahmid Hasan",
            "Wasi Uddin Ahmad",
            "Rifat Shahriyar"
        ],
        "published": "2022-05-23T06:54:56Z",
        "summary": "This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.",
        "pdf_link": "https://arxiv.org/pdf/2205.11081v4.pdf"
    },
    {
        "title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding",
        "authors": [
            "Rishabh Bhardwaj",
            "Amrita Saha",
            "Steven C. H. Hoi",
            "Soujanya Poria"
        ],
        "published": "2022-05-23T03:51:27Z",
        "summary": "Prompt Tuning has been largely successful as a parameter-efficient method of\nconditioning large-scale pre-trained language models to perform downstream\ntasks. Thus far, soft prompt tuning learns a fixed set of task-specific\ncontinuous vectors, i.e., soft tokens that remain static across the task\nsamples. A fixed prompt, however, may not generalize well to the diverse kinds\nof inputs the task comprises. In order to address this, we propose\nVector-quantized Input-contextualized Prompts (VIP) as an extension to the soft\nprompt tuning framework. VIP particularly focuses on two aspects -- contextual\nprompts that learns input-specific contextualization of the soft prompt tokens\nthrough a small-scale sentence encoder and quantized prompts that maps the\ncontextualized prompts to a set of learnable codebook vectors through a Vector\nquantization network. On various language understanding tasks like SuperGLUE,\nQA, Relation classification, NER and NLI, VIP outperforms the soft prompt\ntuning (PT) baseline by an average margin of 1.19%. Further, our generalization\nstudies show that VIP learns more robust prompt representations, surpassing PT\nby a margin of 0.6% - 5.3% on Out-of-domain QA and NLI tasks respectively, and\nby 0.75% on Multi-Task setup over 4 tasks spanning across 12 domains.",
        "pdf_link": "https://arxiv.org/pdf/2205.11024v2.pdf"
    },
    {
        "title": "Artificial intelligence for topic modelling in Hindu philosophy: mapping themes between the Upanishads and the Bhagavad Gita",
        "authors": [
            "Rohitash Chandra",
            "Mukul Ranjan"
        ],
        "published": "2022-05-23T03:39:00Z",
        "summary": "A distinct feature of Hindu religious and philosophical text is that they\ncome from a library of texts rather than single source. The Upanishads is known\nas one of the oldest philosophical texts in the world that forms the foundation\nof Hindu philosophy. The Bhagavad Gita is core text of Hindu philosophy and is\nknown as a text that summarises the key philosophies of the Upanishads with\nmajor focus on the philosophy of karma. These texts have been translated into\nmany languages and there exists studies about themes and topics that are\nprominent; however, there is not much study of topic modelling using language\nmodels which are powered by deep learning. In this paper, we use advanced\nlanguage produces such as BERT to provide topic modelling of the key texts of\nthe Upanishads and the Bhagavad Gita. We analyse the distinct and overlapping\ntopics amongst the texts and visualise the link of selected texts of the\nUpanishads with Bhagavad Gita. Our results show a very high similarity between\nthe topics of these two texts with the mean cosine similarity of 73%. We find\nthat out of the fourteen topics extracted from the Bhagavad Gita, nine of them\nhave a cosine similarity of more than 70% with the topics of the Upanishads. We\nalso found that topics generated by the BERT-based models show very high\ncoherence as compared to that of conventional models. Our best performing model\ngives a coherence score of 73% on the Bhagavad Gita and 69% on The Upanishads.\nThe visualization of the low dimensional embeddings of these texts shows very\nclear overlapping among their topics adding another level of validation to our\nresults.",
        "pdf_link": "https://arxiv.org/pdf/2205.11020v1.pdf"
    },
    {
        "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning",
        "authors": [
            "Yuchao Li",
            "Fuli Luo",
            "Chuanqi Tan",
            "Mengdi Wang",
            "Songfang Huang",
            "Shen Li",
            "Junjie Bai"
        ],
        "published": "2022-05-23T02:43:45Z",
        "summary": "With the dramatically increased number of parameters in language models,\nsparsity methods have received ever-increasing research focus to compress and\naccelerate the models. While most research focuses on how to accurately retain\nappropriate weights while maintaining the performance of the compressed model,\nthere are challenges in the computational overhead and memory footprint of\nsparse training when compressing large-scale language models. To address this\nproblem, we propose a Parameter-efficient Sparse Training (PST) method to\nreduce the number of trainable parameters during sparse-aware training in\ndownstream tasks. Specifically, we first combine the data-free and data-driven\ncriteria to efficiently and accurately measure the importance of weights. Then\nwe investigate the intrinsic redundancy of data-driven weight importance and\nderive two obvious characteristics i.e., low-rankness and structuredness. Based\non that, two groups of small matrices are introduced to compute the data-driven\nimportance of weights, instead of using the original large importance score\nmatrix, which therefore makes the sparse training resource-efficient and\nparameter-efficient. Experiments with diverse networks (i.e., BERT, RoBERTa and\nGPT-2) on dozens of datasets demonstrate PST performs on par or better than\nprevious sparsity methods, despite only training a small number of parameters.\nFor instance, compared with previous sparsity methods, our PST only requires\n1.5% trainable parameters to achieve comparable performance on BERT.",
        "pdf_link": "https://arxiv.org/pdf/2205.11005v1.pdf"
    },
    {
        "title": "Improving Short Text Classification With Augmented Data Using GPT-3",
        "authors": [
            "Salvador Balkus",
            "Donghui Yan"
        ],
        "published": "2022-05-23T01:10:38Z",
        "summary": "GPT-3 is a large-scale natural language model developed by OpenAI that can\nperform many different tasks, including topic classification. Although\nresearchers claim that it requires only a small number of in-context examples\nto learn a task, in practice GPT-3 requires these training examples to be\neither of exceptional quality or a higher quantity than easily created by hand.\nTo address this issue, this study teaches GPT-3 to classify whether a question\nis related to data science by augmenting a small training set with additional\nexamples generated by GPT-3 itself. This study compares two classifiers: the\nGPT-3 Classification Endpoint with augmented examples, and the GPT-3 Completion\nEndpoint with an optimal training set chosen using a genetic algorithm. We find\nthat while the augmented Completion Endpoint achieves upwards of 80 percent\nvalidation accuracy, using the augmented Classification Endpoint yields more\nconsistent accuracy on unseen examples. In this way, giving large-scale machine\nlearning models like GPT-3 the ability to propose their own additional training\nexamples can result in improved classification performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.10981v1.pdf"
    },
    {
        "title": "What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys",
        "authors": [
            "Yubin Ge",
            "Ziang Xiao",
            "Jana Diesner",
            "Heng Ji",
            "Karrie Karahalios",
            "Hari Sundaram"
        ],
        "published": "2022-05-23T00:57:33Z",
        "summary": "Generating follow-up questions on the fly could significantly improve\nconversational survey quality and user experiences by enabling a more dynamic\nand personalized survey structure. In this paper, we proposed a novel task for\nknowledge-driven follow-up question generation in conversational surveys. We\nconstructed a new human-annotated dataset of human-written follow-up questions\nwith dialogue history and labeled knowledge in the context of conversational\nsurveys. Along with the dataset, we designed and validated a set of\nreference-free Gricean-inspired evaluation metrics to systematically evaluate\nthe quality of generated follow-up questions. We then propose a two-staged\nknowledge-driven model for the task, which generates informative and coherent\nfollow-up questions by using knowledge to steer the generation process. The\nexperiments demonstrate that compared to GPT-based baseline models, our\ntwo-staged model generates more informative, coherent, and clear follow-up\nquestions.",
        "pdf_link": "https://arxiv.org/pdf/2205.10977v2.pdf"
    },
    {
        "title": "The Geometry of Multilingual Language Model Representations",
        "authors": [
            "Tyler A. Chang",
            "Zhuowen Tu",
            "Benjamin K. Bergen"
        ],
        "published": "2022-05-22T23:58:24Z",
        "summary": "We assess how multilingual language models maintain a shared multilingual\nrepresentation space while still encoding language-sensitive information in\neach language. Using XLM-R as a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering, evaluated based on causal\neffects on language modeling performance and direct comparisons between\nsubspaces for 88 languages. The subspace means differ along language-sensitive\naxes that are relatively stable throughout middle layers, and these axes encode\ninformation such as token vocabularies. Shifting representations by language\nmeans is sufficient to induce token predictions in different languages.\nHowever, we also identify stable language-neutral axes that encode information\nsuch as token positions and part-of-speech. We visualize representations\nprojected onto language-sensitive and language-neutral axes, identifying\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\ncurves representing token position information. These results demonstrate that\nmultilingual language models encode information along orthogonal\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\nvariety of features for downstream tasks and cross-lingual transfer learning.",
        "pdf_link": "https://arxiv.org/pdf/2205.10964v2.pdf"
    },
    {
        "title": "Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers",
        "authors": [
            "Albert Q. Jiang",
            "Wenda Li",
            "Szymon Tworkowski",
            "Konrad Czechowski",
            "Tomasz Odrzyg\u00f3\u017ad\u017a",
            "Piotr Mi\u0142o\u015b",
            "Yuhuai Wu",
            "Mateja Jamnik"
        ],
        "published": "2022-05-22T18:03:03Z",
        "summary": "In theorem proving, the task of selecting useful premises from a large\nlibrary to unlock the proof of a given conjecture is crucially important. This\npresents a challenge for all theorem provers, especially the ones based on\nlanguage models, due to their relative inability to reason over huge volumes of\npremises in text form. This paper introduces Thor, a framework integrating\nlanguage models and automated theorem provers to overcome this difficulty. In\nThor, a class of methods called hammers that leverage the power of automated\ntheorem provers are used for premise selection, while all other tasks are\ndesignated to language models. Thor increases a language model's success rate\non the PISA dataset from $39\\%$ to $57\\%$, while solving $8.2\\%$ of problems\nneither language models nor automated theorem provers are able to solve on\ntheir own. Furthermore, with a significantly smaller computational budget, Thor\ncan achieve a success rate on the MiniF2F dataset that is on par with the best\nexisting methods. Thor can be instantiated for the majority of popular\ninteractive theorem provers via a straightforward protocol we provide.",
        "pdf_link": "https://arxiv.org/pdf/2205.10893v1.pdf"
    },
    {
        "title": "A Graph Enhanced BERT Model for Event Prediction",
        "authors": [
            "Li Du",
            "Xiao Ding",
            "Yue Zhang",
            "Kai Xiong",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2022-05-22T13:37:38Z",
        "summary": "Predicting the subsequent event for an existing event context is an important\nbut challenging task, as it requires understanding the underlying relationship\nbetween events. Previous methods propose to retrieve relational features from\nevent graph to enhance the modeling of event correlation. However, the sparsity\nof event graph may restrict the acquisition of relevant graph information, and\nhence influence the model performance. To address this issue, we consider\nautomatically building of event graph using a BERT model. To this end, we\nincorporate an additional structured variable into BERT to learn to predict the\nevent connections in the training process. Hence, in the test process, the\nconnection relationship for unseen events can be predicted by the structured\nvariable. Results on two event prediction tasks: script event prediction and\nstory ending prediction, show that our approach can outperform state-of-the-art\nbaseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2205.10822v1.pdf"
    },
    {
        "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders",
        "authors": [
            "Zhenyu Hou",
            "Xiao Liu",
            "Yukuo Cen",
            "Yuxiao Dong",
            "Hongxia Yang",
            "Chunjie Wang",
            "Jie Tang"
        ],
        "published": "2022-05-22T11:57:08Z",
        "summary": "Self-supervised learning (SSL) has been extensively explored in recent years.\nParticularly, generative SSL has seen emerging success in natural language\nprocessing and other AI fields, such as the wide adoption of BERT and GPT.\nDespite this, contrastive learning-which heavily relies on structural data\naugmentation and complicated training strategies-has been the dominant approach\nin graph SSL, while the progress of generative SSL on graphs, especially graph\nautoencoders (GAEs), has thus far not reached the potential as promised in\nother fields. In this paper, we identify and examine the issues that negatively\nimpact the development of GAEs, including their reconstruction objective,\ntraining robustness, and error metric. We present a masked graph autoencoder\nGraphMAE that mitigates these issues for generative self-supervised graph\npretraining. Instead of reconstructing graph structures, we propose to focus on\nfeature reconstruction with both a masking strategy and scaled cosine error\nthat benefit the robust training of GraphMAE. We conduct extensive experiments\non 21 public datasets for three different graph learning tasks. The results\nmanifest that GraphMAE-a simple graph autoencoder with careful designs-can\nconsistently generate outperformance over both contrastive and generative\nstate-of-the-art baselines. This study provides an understanding of graph\nautoencoders and demonstrates the potential of generative self-supervised\npre-training on graphs.",
        "pdf_link": "https://arxiv.org/pdf/2205.10803v3.pdf"
    },
    {
        "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
        "authors": [
            "Or Honovich",
            "Uri Shaham",
            "Samuel R. Bowman",
            "Omer Levy"
        ],
        "published": "2022-05-22T09:22:37Z",
        "summary": "Large language models are able to perform a task by conditioning on a few\ninput-output demonstrations - a paradigm known as in-context learning. We show\nthat language models can explicitly infer an underlying task from a few\ndemonstrations by prompting them to generate a natural language instruction\nthat fits the examples. To explore this ability, we introduce the instruction\ninduction challenge, compile a dataset consisting of 24 tasks, and define a\nnovel evaluation metric based on executing the generated instruction. We\ndiscover that, to a large extent, the ability to generate instructions does\nindeed emerge when using a model that is both large enough and aligned to\nfollow instructions; InstructGPT achieves 65.7% of human performance in our\nexecution-based metric, while the original GPT-3 model reaches only 9.8% of\nhuman performance. This surprising result suggests that instruction induction\nmight be a viable learning paradigm in and of itself, where instead of fitting\na set of latent continuous parameters to the data, one searches for the best\ndescription in the natural language hypothesis space.",
        "pdf_link": "https://arxiv.org/pdf/2205.10782v1.pdf"
    },
    {
        "title": "A Domain-adaptive Pre-training Approach for Language Bias Detection in News",
        "authors": [
            "Jan-David Krieger",
            "Timo Spinde",
            "Terry Ruas",
            "Juhi Kulshrestha",
            "Bela Gipp"
        ],
        "published": "2022-05-22T08:18:19Z",
        "summary": "Media bias is a multi-faceted construct influencing individual behavior and\ncollective decision-making. Slanted news reporting is the result of one-sided\nand polarized writing which can occur in various forms. In this work, we focus\non an important form of media bias, i.e. bias by word choice. Detecting biased\nword choices is a challenging task due to its linguistic complexity and the\nlack of representative gold-standard corpora. We present DA-RoBERTa, a new\nstate-of-the-art transformer-based model adapted to the media bias domain which\nidentifies sentence-level bias with an F1 score of 0.814. In addition, we also\ntrain, DA-BERT and DA-BART, two more transformer models adapted to the bias\ndomain. Our proposed domain-adapted models outperform prior bias detection\napproaches on the same data.",
        "pdf_link": "https://arxiv.org/pdf/2205.10773v1.pdf"
    },
    {
        "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
        "authors": [
            "Kushal Tirumala",
            "Aram H. Markosyan",
            "Luke Zettlemoyer",
            "Armen Aghajanyan"
        ],
        "published": "2022-05-22T07:43:50Z",
        "summary": "Despite their wide adoption, the underlying training and memorization\ndynamics of very large language models is not well understood. We empirically\nstudy exact memorization in causal and masked language modeling, across model\nsizes and throughout the training process. We measure the effects of dataset\nsize, learning rate, and model size on memorization, finding that larger\nlanguage models memorize training data faster across all settings.\nSurprisingly, we show that larger models can memorize a larger portion of the\ndata before over-fitting and tend to forget less throughout the training\nprocess. We also analyze the memorization dynamics of different parts of speech\nand find that models memorize nouns and numbers first; we hypothesize and\nprovide empirical evidence that nouns and numbers act as a unique identifier\nfor memorizing individual training examples. Together, these findings present\nanother piece of the broader puzzle of trying to understand what actually\nimproves as models get bigger.",
        "pdf_link": "https://arxiv.org/pdf/2205.10770v2.pdf"
    },
    {
        "title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners",
        "authors": [
            "Zhenhailong Wang",
            "Manling Li",
            "Ruochen Xu",
            "Luowei Zhou",
            "Jie Lei",
            "Xudong Lin",
            "Shuohang Wang",
            "Ziyi Yang",
            "Chenguang Zhu",
            "Derek Hoiem",
            "Shih-Fu Chang",
            "Mohit Bansal",
            "Heng Ji"
        ],
        "published": "2022-05-22T05:18:27Z",
        "summary": "The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets. Code and resources are publicly available for\nresearch purposes at https://github.com/MikeWangWZHL/VidIL .",
        "pdf_link": "https://arxiv.org/pdf/2205.10747v4.pdf"
    },
    {
        "title": "Housekeep: Tidying Virtual Households using Commonsense Reasoning",
        "authors": [
            "Yash Kant",
            "Arun Ramachandran",
            "Sriram Yenamandra",
            "Igor Gilitschenski",
            "Dhruv Batra",
            "Andrew Szot",
            "Harsh Agrawal"
        ],
        "published": "2022-05-22T02:37:09Z",
        "summary": "We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the\nhome for embodied AI. In Housekeep, an embodied agent must tidy a house by\nrearranging misplaced objects without explicit instructions specifying which\nobjects need to be rearranged. Instead, the agent must learn from and is\nevaluated against human preferences of which objects belong where in a tidy\nhouse. Specifically, we collect a dataset of where humans typically place\nobjects in tidy and untidy houses constituting 1799 objects, 268 object\ncategories, 585 placements, and 105 rooms. Next, we propose a modular baseline\napproach for Housekeep that integrates planning, exploration, and navigation.\nIt leverages a fine-tuned large language model (LLM) trained on an internet\ntext corpus for effective planning. We show that our baseline agent generalizes\nto rearranging unseen objects in unknown environments. See our webpage for more\ndetails: https://yashkant.github.io/housekeep/",
        "pdf_link": "https://arxiv.org/pdf/2205.10712v1.pdf"
    },
    {
        "title": "Life after BERT: What do Other Muppets Understand about Language?",
        "authors": [
            "Vladislav Lialin",
            "Kevin Zhao",
            "Namrata Shivagunde",
            "Anna Rumshisky"
        ],
        "published": "2022-05-21T23:57:17Z",
        "summary": "Existing pre-trained transformer analysis works usually focus only on one or\ntwo model families at a time, overlooking the variability of the architecture\nand pre-training objectives. In our work, we utilize the oLMpics benchmark and\npsycholinguistic probing datasets for a diverse set of 29 models including T5,\nBART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for\nautoregressive models and evaluate GPT networks of different sizes. Our\nfindings show that none of these models can resolve compositional questions in\na zero-shot fashion, suggesting that this skill is not learnable using existing\npre-training objectives. Furthermore, we find that global model decisions such\nas architecture, directionality, size of the dataset, and pre-training\nobjective are not predictive of a model's linguistic capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2205.10696v2.pdf"
    },
    {
        "title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding",
        "authors": [
            "Abbas Ghaddar",
            "Yimeng Wu",
            "Sunyam Bagga",
            "Ahmad Rashid",
            "Khalil Bibi",
            "Mehdi Rezagholizadeh",
            "Chao Xing",
            "Yasheng Wang",
            "Duan Xinyu",
            "Zhefeng Wang",
            "Baoxing Huai",
            "Xin Jiang",
            "Qun Liu",
            "Philippe Langlais"
        ],
        "published": "2022-05-21T22:38:19Z",
        "summary": "There is a growing body of work in recent years to develop pre-trained\nlanguage models (PLMs) for the Arabic language. This work concerns addressing\ntwo major problems in existing Arabic PLMs which constraint progress of the\nArabic NLU and NLG fields.First, existing Arabic PLMs are not well-explored and\ntheir pre-trainig can be improved significantly using a more methodical\napproach. Second, there is a lack of systematic and reproducible evaluation of\nthese models in the literature. In this work, we revisit both the pre-training\nand evaluation of Arabic PLMs. In terms of pre-training, we explore improving\nArabic LMs from three perspectives: quality of the pre-training data, size of\nthe model, and incorporating character-level information. As a result, we\nrelease three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and\ntwo T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a\ncomprehensive empirical study to systematically evaluate the performance of\nexisting state-of-the-art models on ALUE that is a leaderboard-powered\nbenchmark for Arabic NLU tasks, and on a subset of the ARGEN benchmark for\nArabic NLG tasks. We show that our models significantly outperform existing\nArabic PLMs and achieve a new state-of-the-art performance on discriminative\nand generative Arabic NLU and NLG tasks. Our models and source code to\nreproduce of results will be made available shortly.",
        "pdf_link": "https://arxiv.org/pdf/2205.10687v1.pdf"
    },
    {
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "authors": [
            "Denny Zhou",
            "Nathanael Sch\u00e4rli",
            "Le Hou",
            "Jason Wei",
            "Nathan Scales",
            "Xuezhi Wang",
            "Dale Schuurmans",
            "Claire Cui",
            "Olivier Bousquet",
            "Quoc Le",
            "Ed Chi"
        ],
        "published": "2022-05-21T15:34:53Z",
        "summary": "Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.",
        "pdf_link": "https://arxiv.org/pdf/2205.10625v3.pdf"
    },
    {
        "title": "HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking",
        "authors": [
            "Yanzhao Zhang",
            "Dingkun Long",
            "Guangwei Xu",
            "Pengjun Xie"
        ],
        "published": "2022-05-21T11:38:33Z",
        "summary": "Deep pre-trained language models (e,g. BERT) are effective at large-scale\ntext retrieval task. Existing text retrieval systems with state-of-the-art\nperformance usually adopt a retrieve-then-reranking architecture due to the\nhigh computational cost of pre-trained language models and the large corpus\nsize. Under such a multi-stage architecture, previous studies mainly focused on\noptimizing single stage of the framework thus improving the overall retrieval\nperformance. However, how to directly couple multi-stage features for\noptimization has not been well studied. In this paper, we design Hybrid List\nAware Transformer Reranking (HLATR) as a subsequent reranking module to\nincorporate both retrieval and reranking stage features. HLATR is lightweight\nand can be easily parallelized with existing text retrieval systems so that the\nreranking process can be performed in a single yet efficient processing.\nEmpirical experiments on two large-scale text retrieval datasets show that\nHLATR can efficiently improve the ranking performance of existing multi-stage\ntext retrieval methods.",
        "pdf_link": "https://arxiv.org/pdf/2205.10569v1.pdf"
    },
    {
        "title": "Scenario-based Multi-product Advertising Copywriting Generation for E-Commerce",
        "authors": [
            "Xueying Zhang",
            "Kai Shen",
            "Chi Zhang",
            "Xiaochuan Fan",
            "Yun Xiao",
            "Zhen He",
            "Bo Long",
            "Lingfei Wu"
        ],
        "published": "2022-05-21T07:45:53Z",
        "summary": "In this paper, we proposed an automatic Scenario-based Multi-product\nAdvertising Copywriting Generation system (SMPACG) for E-Commerce, which has\nbeen deployed on a leading Chinese e-commerce platform. The proposed SMPACG\nconsists of two main components: 1) an automatic multi-product combination\nselection module, which itself is consisted of a topic prediction model, a\npattern and attribute-based selection model and an arbitrator model; and 2) an\nautomatic multi-product advertising copywriting generation module, which\ncombines our proposed domain-specific pretrained language model and\nknowledge-based data enhancement model. The SMPACG is the first system that\nrealizes automatic scenario-based multi-product advertising contents\ngeneration, which achieves significant improvements over other state-of-the-art\nmethods. The SMPACG has been not only developed for directly serving for our\ne-commerce recommendation system, but also used as a real-time writing\nassistant tool for merchants.",
        "pdf_link": "https://arxiv.org/pdf/2205.10530v1.pdf"
    },
    {
        "title": "Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese",
        "authors": [
            "Kurt Micallef",
            "Albert Gatt",
            "Marc Tanti",
            "Lonneke van der Plas",
            "Claudia Borg"
        ],
        "published": "2022-05-21T06:44:59Z",
        "summary": "Multilingual language models such as mBERT have seen impressive cross-lingual\ntransfer to a variety of languages, but many languages remain excluded from\nthese models. In this paper, we analyse the effect of pre-training with\nmonolingual data for a low-resource language that is not included in mBERT --\nMaltese -- with a range of pre-training set ups. We conduct evaluations with\nthe newly pre-trained models on three morphosyntactic tasks -- dependency\nparsing, part-of-speech tagging, and named-entity recognition -- and one\nsemantic classification task -- sentiment analysis. We also present a newly\ncreated corpus for Maltese, and determine the effect that the pre-training data\nsize and domain have on the downstream performance. Our results show that using\na mixture of pre-training domains is often superior to using Wikipedia text\nonly. We also find that a fraction of this corpus is enough to make significant\nleaps in performance over Wikipedia-trained models. We pre-train and compare\ntwo models on the new corpus: a monolingual BERT model trained from scratch\n(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models\nachieve state-of-the-art performance on these tasks, despite the new corpus\nbeing considerably smaller than typically used corpora for high-resourced\nlanguages. On average, BERTu outperforms or performs competitively with mBERTu,\nand the largest gains are observed for higher-level tasks.",
        "pdf_link": "https://arxiv.org/pdf/2205.10517v2.pdf"
    },
    {
        "title": "Computable Artificial General Intelligence",
        "authors": [
            "Michael Timothy Bennett"
        ],
        "published": "2022-05-21T06:32:09Z",
        "summary": "Artificial general intelligence (AGI) may herald our extinction, according to\nAI safety research. Yet claims regarding AGI must rely upon mathematical\nformalisms -- theoretical agents we may analyse or attempt to build. AIXI\nappears to be the only such formalism supported by proof that its behaviour is\noptimal, a consequence of its use of compression as a proxy for intelligence.\nUnfortunately, AIXI is incomputable and claims regarding its behaviour highly\nsubjective. We argue that this is because AIXI formalises cognition as taking\nplace in isolation from the environment in which goals are pursued (Cartesian\ndualism). We propose an alternative, supported by proof and experiment, which\novercomes these problems. Integrating research from cognitive science with AI,\nwe formalise an enactive model of learning and reasoning to address the problem\nof subjectivity. This allows us to formulate a different proxy for\nintelligence, called weakness, which addresses the problem of incomputability.\nWe prove optimal behaviour is attained when weakness is maximised. This proof\nis supplemented by experimental results comparing weakness and description\nlength (the closest analogue to compression possible without reintroducing\nsubjectivity). Weakness outperforms description length, suggesting it is a\nbetter proxy. Furthermore we show that, if cognition is enactive, then\nminimisation of description length is neither necessary nor sufficient to\nattain optimal performance, undermining the notion that compression is closely\nrelated to intelligence. However, there remain open questions regarding the\nimplementation of scale-able AGI. In the short term, these results may be best\nutilised to improve the performance of existing systems. For example, our\nresults explain why Deepmind's Apperception Engine is able to generalise\neffectively, and how to replicate that performance by maximising weakness.",
        "pdf_link": "https://arxiv.org/pdf/2205.10513v7.pdf"
    },
    {
        "title": "A Study on Transformer Configuration and Training Objective",
        "authors": [
            "Fuzhao Xue",
            "Jianghai Chen",
            "Aixin Sun",
            "Xiaozhe Ren",
            "Zangwei Zheng",
            "Xiaoxin He",
            "Yongming Chen",
            "Xin Jiang",
            "Yang You"
        ],
        "published": "2022-05-21T05:17:11Z",
        "summary": "Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.",
        "pdf_link": "https://arxiv.org/pdf/2205.10505v3.pdf"
    },
    {
        "title": "Named Entity Linking with Entity Representation by Multiple Embeddings",
        "authors": [
            "Oleg Vasilyev",
            "Alex Dauenhauer",
            "Vedant Dharnidharka",
            "John Bohannon"
        ],
        "published": "2022-05-21T03:31:25Z",
        "summary": "We propose a simple and practical method for named entity linking (NEL),\nbased on entity representation by multiple embeddings. To explore this method,\nand to review its dependency on parameters, we measure its performance on\nNamesakes, a highly challenging dataset of ambiguously named entities. Our\nobservations suggest that the minimal number of mentions required to create a\nknowledge base (KB) entity is very important for NEL performance. The number of\nembeddings is less important and can be kept small, within as few as 10 or\nless. We show that our representations of KB entities can be adjusted using\nonly KB data, and the adjustment can improve NEL performance. We also compare\nNEL performance of embeddings obtained from tuning language model on diverse\nnews texts as opposed to tuning on more uniform texts from public datasets\nXSum, CNN / Daily Mail. We found that tuning on diverse news provides better\nembeddings.",
        "pdf_link": "https://arxiv.org/pdf/2205.10498v2.pdf"
    },
    {
        "title": "Scaling Laws and Interpretability of Learning from Repeated Data",
        "authors": [
            "Danny Hernandez",
            "Tom Brown",
            "Tom Conerly",
            "Nova DasSarma",
            "Dawn Drain",
            "Sheer El-Showk",
            "Nelson Elhage",
            "Zac Hatfield-Dodds",
            "Tom Henighan",
            "Tristan Hume",
            "Scott Johnston",
            "Ben Mann",
            "Chris Olah",
            "Catherine Olsson",
            "Dario Amodei",
            "Nicholas Joseph",
            "Jared Kaplan",
            "Sam McCandlish"
        ],
        "published": "2022-05-21T02:14:27Z",
        "summary": "Recent large language models have been trained on vast datasets, but also\noften on repeated data, either intentionally for the purpose of upweighting\nhigher quality data, or unintentionally because data deduplication is not\nperfect and the model is exposed to repeated data at the sentence, paragraph,\nor document level. Some works have reported substantial negative performance\neffects of this repeated data. In this paper we attempt to study repeated data\nsystematically and to understand its effects mechanistically. To do this, we\ntrain a family of models where most of the data is unique but a small fraction\nof it is repeated many times. We find a strong double descent phenomenon, in\nwhich repeated data can lead test loss to increase midway through training. A\npredictable range of repetition frequency leads to surprisingly severe\ndegradation in performance. For instance, performance of an 800M parameter\nmodel can be degraded to that of a 2x smaller model (400M params) by repeating\n0.1% of the data 100 times, despite the other 90% of the training tokens\nremaining unique. We suspect there is a range in the middle where the data can\nbe memorized and doing so consumes a large fraction of the model's capacity,\nand this may be where the peak of degradation occurs. Finally, we connect these\nobservations to recent mechanistic interpretability work - attempting to\nreverse engineer the detailed computations performed by the model - by showing\nthat data repetition disproportionately damages copying and internal structures\nassociated with generalization, such as induction heads, providing a possible\nmechanism for the shift from generalization to memorization. Taken together,\nthese results provide a hypothesis for why repeating a relatively small\nfraction of data in large language models could lead to disproportionately\nlarge harms to performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.10487v1.pdf"
    },
    {
        "title": "DeepStruct: Pretraining of Language Models for Structure Prediction",
        "authors": [
            "Chenguang Wang",
            "Xiao Liu",
            "Zui Chen",
            "Haoyun Hong",
            "Jie Tang",
            "Dawn Song"
        ],
        "published": "2022-05-21T00:58:22Z",
        "summary": "We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",
        "pdf_link": "https://arxiv.org/pdf/2205.10475v2.pdf"
    },
    {
        "title": "Current Trends and Approaches in Synonyms Extraction: Potential Adaptation to Arabic",
        "authors": [
            "Eman Naser-Karajah",
            "Nabil Arman",
            "Mustafa Jarrar"
        ],
        "published": "2022-05-20T19:05:10Z",
        "summary": "Extracting synonyms from dictionaries or corpora is gaining special attention\nas synonyms play an important role in improving NLP application performance.\nThis paper presents a survey of the different approaches and trends used in\nautomatically extracting the synonyms. These approaches can be divided into\nfour main categories. The first approach is to find the Synonyms using a\ntranslation graph. The second approach is to discover new transition pairs such\nas (Arabic-English) (English-France) then (Arabic-France). The third approach\nis to construct new WordNets by exploring synonymy graphs, and the fourth\napproach is to find similar words from corpora using Deep Learning methods,\nsuch as word embeddings and recently BERT models. The paper also presents a\ncomparative analysis between these approaches and highlights potential\nadaptation to generate synonyms automatically in the Arabic language as future\nwork.",
        "pdf_link": "https://arxiv.org/pdf/2205.10412v1.pdf"
    },
    {
        "title": "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes",
        "authors": [
            "Alexander Kolesnikov",
            "Andr\u00e9 Susano Pinto",
            "Lucas Beyer",
            "Xiaohua Zhai",
            "Jeremiah Harmsen",
            "Neil Houlsby"
        ],
        "published": "2022-05-20T17:47:59Z",
        "summary": "We introduce UViM, a unified approach capable of modeling a wide range of\ncomputer vision tasks. In contrast to previous models, UViM has the same\nfunctional form for all tasks; it requires no task-specific modifications which\nrequire extensive human expertise. The approach involves two components: (I) a\nbase model (feed-forward) which is trained to directly predict raw vision\noutputs, guided by a learned discrete code and (II) a language model\n(autoregressive) that is trained to generate the guiding code. These components\ncomplement each other: the language model is well-suited to modeling structured\ninterdependent data, while the base model is efficient at dealing with\nhigh-dimensional outputs. We demonstrate the effectiveness of UViM on three\ndiverse and challenging vision tasks: panoptic segmentation, depth prediction\nand image colorization, where we achieve competitive and near state-of-the-art\nresults. Our experimental results suggest that UViM is a promising candidate\nfor a unified modeling approach in computer vision.",
        "pdf_link": "https://arxiv.org/pdf/2205.10337v3.pdf"
    },
    {
        "title": "Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions",
        "authors": [
            "Rui Yang",
            "Jie Wang",
            "Zijie Geng",
            "Mingxuan Ye",
            "Shuiwang Ji",
            "Bin Li",
            "Feng Wu"
        ],
        "published": "2022-05-20T14:52:03Z",
        "summary": "Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.",
        "pdf_link": "https://arxiv.org/pdf/2205.10218v3.pdf"
    },
    {
        "title": "Progressive Class Semantic Matching for Semi-supervised Text Classification",
        "authors": [
            "Hai-Ming Xu",
            "Lingqiao Liu",
            "Ehsan Abbasnejad"
        ],
        "published": "2022-05-20T13:59:03Z",
        "summary": "Semi-supervised learning is a promising way to reduce the annotation cost for\ntext-classification. Combining with pre-trained language models (PLMs), e.g.,\nBERT, recent semi-supervised learning methods achieved impressive performance.\nIn this work, we further investigate the marriage between semi-supervised\nlearning and a pre-trained language model. Unlike existing approaches that\nutilize PLMs only for model parameter initialization, we explore the inherent\ntopic matching capability inside PLMs for building a more powerful\nsemi-supervised learning approach. Specifically, we propose a joint\nsemi-supervised learning process that can progressively build a standard\n$K$-way classifier and a matching network for the input text and the Class\nSemantic Representation (CSR). The CSR will be initialized from the given\nlabeled sentences and progressively updated through the training process. By\nmeans of extensive experiments, we show that our method can not only bring\nremarkable improvement to baselines, but also overall be more stable, and\nachieves state-of-the-art performance in semi-supervised text classification.",
        "pdf_link": "https://arxiv.org/pdf/2205.10189v1.pdf"
    },
    {
        "title": "Adversarial Body Shape Search for Legged Robots",
        "authors": [
            "Takaaki Azakami",
            "Hiroshi Kera",
            "Kazuhiko Kawamoto"
        ],
        "published": "2022-05-20T13:55:47Z",
        "summary": "We propose an evolutionary computation method for an adversarial attack on\nthe length and thickness of parts of legged robots by deep reinforcement\nlearning. This attack changes the robot body shape and interferes with\nwalking-we call the attacked body as adversarial body shape. The evolutionary\ncomputation method searches adversarial body shape by minimizing the expected\ncumulative reward earned through walking simulation. To evaluate the\neffectiveness of the proposed method, we perform experiments with three-legged\nrobots, Walker2d, Ant-v2, and Humanoid-v2 in OpenAI Gym. The experimental\nresults reveal that Walker2d and Ant-v2 are more vulnerable to the attack on\nthe length than the thickness of the body parts, whereas Humanoid-v2 is\nvulnerable to the attack on both of the length and thickness. We further\nidentify that the adversarial body shapes break left-right symmetry or shift\nthe center of gravity of the legged robots. Finding adversarial body shape can\nbe used to proactively diagnose the vulnerability of legged robot walking.",
        "pdf_link": "https://arxiv.org/pdf/2205.10187v1.pdf"
    },
    {
        "title": "Prototypical Calibration for Few-shot Learning of Language Models",
        "authors": [
            "Zhixiong Han",
            "Yaru Hao",
            "Li Dong",
            "Yutao Sun",
            "Furu Wei"
        ],
        "published": "2022-05-20T13:50:07Z",
        "summary": "In-context learning of GPT-like models has been recognized as fragile across\ndifferent hand-crafted templates, and demonstration permutations. In this work,\nwe propose prototypical calibration to adaptively learn a more robust decision\nboundary for zero- and few-shot classification, instead of greedy decoding.\nConcretely, our method first adopts Gaussian mixture distribution to estimate\nthe prototypical clusters for all categories. Then we assign each cluster to\nthe corresponding label by solving a weighted bipartite matching problem. Given\nan example, its prediction is calibrated by the likelihood of prototypical\nclusters. Experimental results show that prototypical calibration yields a\nsubstantial improvement on a diverse set of tasks. Extensive analysis across\ndifferent scales also indicates that our method calibrates the decision\nboundary as expected, greatly improving the robustness of GPT to templates,\npermutations, and class imbalance.",
        "pdf_link": "https://arxiv.org/pdf/2205.10183v2.pdf"
    },
    {
        "title": "Visually-Augmented Language Modeling",
        "authors": [
            "Weizhi Wang",
            "Li Dong",
            "Hao Cheng",
            "Haoyu Song",
            "Xiaodong Liu",
            "Xifeng Yan",
            "Jianfeng Gao",
            "Furu Wei"
        ],
        "published": "2022-05-20T13:41:12Z",
        "summary": "Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on text-only self-supervised training with massive text\ndata, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel latent text-image\nalignment method via an image retrieval module to fetch corresponding images\ngiven a textual context. With the visually-augmented context, VaLM uses a\nvisual knowledge fusion layer to enable multimodal grounded language modeling\nby attending to both text context and visual knowledge in images. We evaluate\nVaLM on various visual knowledge-intensive commonsense reasoning tasks, which\nrequire visual information to excel. The experimental results illustrate that\nVaLM outperforms all strong language-only and vision-language baselines with\nsubstantial gains in reasoning object commonsense including color, size, and\nshape. Our code is available at https://github.com/Victorwz/VaLM.",
        "pdf_link": "https://arxiv.org/pdf/2205.10178v2.pdf"
    },
    {
        "title": "Adversarial joint attacks on legged robots",
        "authors": [
            "Takuto Otomo",
            "Hiroshi Kera",
            "Kazuhiko Kawamoto"
        ],
        "published": "2022-05-20T11:30:23Z",
        "summary": "We address adversarial attacks on the actuators at the joints of legged\nrobots trained by deep reinforcement learning. The vulnerability to the joint\nattacks can significantly impact the safety and robustness of legged robots. In\nthis study, we demonstrate that the adversarial perturbations to the torque\ncontrol signals of the actuators can significantly reduce the rewards and cause\nwalking instability in robots. To find the adversarial torque perturbations, we\ndevelop black-box adversarial attacks, where, the adversary cannot access the\nneural networks trained by deep reinforcement learning. The black box attack\ncan be applied to legged robots regardless of the architecture and algorithms\nof deep reinforcement learning. We employ three search methods for the\nblack-box adversarial attacks: random search, differential evolution, and\nnumerical gradient descent methods. In experiments with the quadruped robot\nAnt-v2 and the bipedal robot Humanoid-v2, in OpenAI Gym environments, we find\nthat differential evolution can efficiently find the strongest torque\nperturbations among the three methods. In addition, we realize that the\nquadruped robot Ant-v2 is vulnerable to the adversarial perturbations, whereas\nthe bipedal robot Humanoid-v2 is robust to the perturbations. Consequently, the\njoint attacks can be used for proactive diagnosis of robot walking instability.",
        "pdf_link": "https://arxiv.org/pdf/2205.10098v1.pdf"
    },
    {
        "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models",
        "authors": [
            "Yuxin Ren",
            "Benyou Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022-05-20T09:16:55Z",
        "summary": "Recent work explored the potential of large-scale Transformer-based\npre-trained models, especially Pre-trained Language Models (PLMs) in natural\nlanguage processing. This raises many concerns from various perspectives, e.g.,\nfinancial costs and carbon emissions. Compressing PLMs like BERT with\nnegligible performance loss for faster inference and cheaper deployment has\nattracted much attention. In this work, we aim to explore larger compression\nratios for PLMs, among which tensor decomposition is a potential but\nunder-investigated one. Two decomposition and reconstruction protocols are\nfurther proposed to improve the effectiveness and efficiency during\ncompression. Our compressed BERT with ${1}/{7}$ parameters in Transformer\nlayers performs on-par with, sometimes slightly better than the original BERT\nin GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base\nwith $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding\nthe embedding layer) and $2.7 \\times$ faster on inference. To show that the\nproposed method is orthogonal to existing compression methods like knowledge\ndistillation, we also explore the benefit of the proposed method on a distilled\nBERT.",
        "pdf_link": "https://arxiv.org/pdf/2205.10036v1.pdf"
    },
    {
        "title": "Evaluating and Inducing Personality in Pre-trained Language Models",
        "authors": [
            "Guangyuan Jiang",
            "Manjie Xu",
            "Song-Chun Zhu",
            "Wenjuan Han",
            "Chi Zhang",
            "Yixin Zhu"
        ],
        "published": "2022-05-20T07:32:57Z",
        "summary": "Standardized and quantified evaluation of machine behaviors is a crux of\nunderstanding LLMs. In this study, we draw inspiration from psychometric\nstudies by leveraging human personality theory as a tool for studying machine\nbehaviors. Originating as a philosophical quest for human behaviors, the study\nof personality delves into how individuals differ in thinking, feeling, and\nbehaving. Toward building and understanding human-like social machines, we are\nmotivated to ask: Can we assess machine behaviors by leveraging human\npsychometric tests in a principled and quantitative manner? If so, can we\ninduce a specific personality in LLMs? To answer these questions, we introduce\nthe Machine Personality Inventory (MPI) tool for studying machine behaviors;\nMPI follows standardized personality tests, built upon the Big Five Personality\nFactors (Big Five) theory and personality assessment inventories. By\nsystematically evaluating LLMs with MPI, we provide the first piece of evidence\ndemonstrating the efficacy of MPI in studying LLMs behaviors. We further devise\na Personality Prompting (P^2) method to induce LLMs with specific personalities\nin a controllable way, capable of producing diverse and verifiable behaviors.\nWe hope this work sheds light on future studies by adopting personality as the\nessential indicator for various downstream tasks, and could further motivate\nresearch into equally intriguing human-like machine behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2206.07550v3.pdf"
    },
    {
        "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Peter J. Ramadge",
            "Alexander I. Rudnicky"
        ],
        "published": "2022-05-20T01:25:57Z",
        "summary": "Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets. Our implementation and pretrained checkpoints are released\nat https://github.com/chijames/KERPLE.git.",
        "pdf_link": "https://arxiv.org/pdf/2205.09921v2.pdf"
    },
    {
        "title": "Automated Scoring for Reading Comprehension via In-context BERT Tuning",
        "authors": [
            "Nigel Fernandez",
            "Aritra Ghosh",
            "Naiming Liu",
            "Zichao Wang",
            "Beno\u00eet Choffin",
            "Richard Baraniuk",
            "Andrew Lan"
        ],
        "published": "2022-05-19T21:16:15Z",
        "summary": "Automated scoring of open-ended student responses has the potential to\nsignificantly reduce human grader effort. Recent advances in automated scoring\noften leverage textual representations based on pre-trained language models\nsuch as BERT and GPT as input to scoring models. Most existing approaches train\na separate model for each item/question, which is suitable for scenarios such\nas essay scoring where items can be quite different from one another. However,\nthese approaches have two limitations: 1) they fail to leverage item linkage\nfor scenarios such as reading comprehension where multiple items may share a\nreading passage; 2) they are not scalable since storing one model per item\nbecomes difficult when models have a large number of parameters. In this paper,\nwe report our (grand prize-winning) solution to the National Assessment of\nEducation Progress (NAEP) automated scoring challenge for reading\ncomprehension. Our approach, in-context BERT fine-tuning, produces a single\nshared scoring model for all items with a carefully-designed input structure to\nprovide contextual information on each item. We demonstrate the effectiveness\nof our approach via local evaluations using the training dataset provided by\nthe challenge. We also discuss the biases, common error types, and limitations\nof our approach.",
        "pdf_link": "https://arxiv.org/pdf/2205.09864v2.pdf"
    },
    {
        "title": "Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation",
        "authors": [
            "Samhita Honnavalli",
            "Aesha Parekh",
            "Lily Ou",
            "Sophie Groenwold",
            "Sharon Levy",
            "Vicente Ordonez",
            "William Yang Wang"
        ],
        "published": "2022-05-19T20:05:02Z",
        "summary": "Women are often perceived as junior to their male counterparts, even within\nthe same job titles. While there has been significant progress in the\nevaluation of gender bias in natural language processing (NLP), existing\nstudies seldom investigate how biases toward gender groups change when\ncompounded with other societal biases. In this work, we investigate how\nseniority impacts the degree of gender bias exhibited in pretrained neural\ngeneration models by introducing a novel framework for probing compound bias.\nWe contribute a benchmark robustness-testing dataset spanning two domains, U.S.\nsenatorship and professorship, created using a distant-supervision method. Our\ndataset includes human-written text with underlying ground truth and paired\ncounterfactuals. We then examine GPT-2 perplexity and the frequency of gendered\nlanguage in generated text. Our results show that GPT-2 amplifies bias by\nconsidering women as junior and men as senior more often than the ground truth\nin both domains. These results suggest that NLP applications built using GPT-2\nmay harm women in professional capacities.",
        "pdf_link": "https://arxiv.org/pdf/2205.09830v1.pdf"
    },
    {
        "title": "Overcoming Language Disparity in Online Content Classification with Multimodal Learning",
        "authors": [
            "Gaurav Verma",
            "Rohit Mujumdar",
            "Zijie J. Wang",
            "Munmun De Choudhury",
            "Srijan Kumar"
        ],
        "published": "2022-05-19T17:56:02Z",
        "summary": "Advances in Natural Language Processing (NLP) have revolutionized the way\nresearchers and practitioners address crucial societal problems. Large language\nmodels are now the standard to develop state-of-the-art solutions for text\ndetection and classification tasks. However, the development of advanced\ncomputational techniques and resources is disproportionately focused on the\nEnglish language, sidelining a majority of the languages spoken globally. While\nexisting research has developed better multilingual and monolingual language\nmodels to bridge this language disparity between English and non-English\nlanguages, we explore the promise of incorporating the information contained in\nimages via multimodal machine learning. Our comparative analyses on three\ndetection tasks focusing on crisis information, fake news, and emotion\nrecognition, as well as five high-resource non-English languages, demonstrate\nthat: (a) detection frameworks based on pre-trained large language models like\nBERT and multilingual-BERT systematically perform better on the English\nlanguage compared against non-English languages, and (b) including images via\nmultimodal learning bridges this performance gap. We situate our findings with\nrespect to existing work on the pitfalls of large language models, and discuss\ntheir theoretical and practical implications. Resources for this paper are\navailable at https://multimodality-language-disparity.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2205.09744v1.pdf"
    },
    {
        "title": "Enhancing Slot Tagging with Intent Features for Task Oriented Natural Language Understanding using BERT",
        "authors": [
            "Shruthi Hariharan",
            "Vignesh Kumar Krishnamurthy",
            "Utkarsh",
            "Jayantha Gowda Sarapanahalli"
        ],
        "published": "2022-05-19T17:41:04Z",
        "summary": "Recent joint intent detection and slot tagging models have seen improved\nperformance when compared to individual models. In many real-world datasets,\nthe slot labels and values have a strong correlation with their intent labels.\nIn such cases, the intent label information may act as a useful feature to the\nslot tagging model. In this paper, we examine the effect of leveraging intent\nlabel features through 3 techniques in the slot tagging task of joint intent\nand slot detection models. We evaluate our techniques on benchmark spoken\nlanguage datasets SNIPS and ATIS, as well as over a large private Bixby dataset\nand observe an improved slot-tagging performance over state-of-the-art models.",
        "pdf_link": "https://arxiv.org/pdf/2205.09732v2.pdf"
    },
    {
        "title": "RankGen: Improving Text Generation with Large Ranking Models",
        "authors": [
            "Kalpesh Krishna",
            "Yapei Chang",
            "John Wieting",
            "Mohit Iyyer"
        ],
        "published": "2022-05-19T17:36:46Z",
        "summary": "Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues we present RankGen, a 1.2B parameter encoder\nmodel for English that scores model generations given a prefix. RankGen can be\nflexibly incorporated as a scoring function in beam search and used to decode\nfrom any pretrained language model. We train RankGen using large-scale\ncontrastive learning to map a prefix close to the ground-truth sequence that\nfollows it and far away from two types of negatives: (1) random sequences from\nthe same document as the prefix, and (2) sequences generated from a large\nlanguage model conditioned on the prefix. Experiments across four different\nlanguage models (345M-11B parameters) and two domains show that RankGen\nsignificantly outperforms decoding algorithms like nucleus, top-k, and typical\nsampling, as well as contrastive decoding and search, on both automatic metrics\n(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English\nwriters (74.5% human preference over nucleus sampling). Analysis reveals that\nRankGen outputs are more relevant to the prefix and improve continuity and\ncoherence compared to baselines. We release our model checkpoints, code, and\nhuman preference data with explanations to facilitate future research.",
        "pdf_link": "https://arxiv.org/pdf/2205.09726v3.pdf"
    },
    {
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "authors": [
            "Antonia Creswell",
            "Murray Shanahan",
            "Irina Higgins"
        ],
        "published": "2022-05-19T17:25:28Z",
        "summary": "Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.",
        "pdf_link": "https://arxiv.org/pdf/2205.09712v1.pdf"
    },
    {
        "title": "ArabGlossBERT: Fine-Tuning BERT on Context-Gloss Pairs for WSD",
        "authors": [
            "Moustafa Al-Hajj",
            "Mustafa Jarrar"
        ],
        "published": "2022-05-19T16:47:18Z",
        "summary": "Using pre-trained transformer models such as BERT has proven to be effective\nin many NLP tasks. This paper presents our work to fine-tune BERT models for\nArabic Word Sense Disambiguation (WSD). We treated the WSD task as a\nsentence-pair binary classification task. First, we constructed a dataset of\nlabeled Arabic context-gloss pairs (~167k pairs) we extracted from the Arabic\nOntology and the large lexicographic database available at Birzeit University.\nEach pair was labeled as True or False and target words in each context were\nidentified and annotated. Second, we used this dataset for fine-tuning three\npre-trained Arabic BERT models. Third, we experimented the use of different\nsupervised signals used to emphasize target words in context. Our experiments\nachieved promising results (accuracy of 84%) although we used a large set of\nsenses in the experiment.",
        "pdf_link": "https://arxiv.org/pdf/2205.09685v1.pdf"
    },
    {
        "title": "Wojood: Nested Arabic Named Entity Corpus and Recognition using BERT",
        "authors": [
            "Mustafa Jarrar",
            "Mohammed Khalilia",
            "Sana Ghanem"
        ],
        "published": "2022-05-19T16:06:49Z",
        "summary": "This paper presents Wojood, a corpus for Arabic nested Named Entity\nRecognition (NER). Nested entities occur when one entity mention is embedded\ninside another entity mention. Wojood consists of about 550K Modern Standard\nArabic (MSA) and dialect tokens that are manually annotated with 21 entity\ntypes including person, organization, location, event and date. More\nimportantly, the corpus is annotated with nested entities instead of the more\ncommon flat annotations. The data contains about 75K entities and 22.5% of\nwhich are nested. The inter-annotator evaluation of the corpus demonstrated a\nstrong agreement with Cohen's Kappa of 0.979 and an F1-score of 0.976. To\nvalidate our data, we used the corpus to train a nested NER model based on\nmulti-task learning and AraBERT (Arabic BERT). The model achieved an overall\nmicro F1-score of 0.884. Our corpus, the annotation guidelines, the source code\nand the pre-trained model are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2205.09651v2.pdf"
    },
    {
        "title": "Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",
        "authors": [
            "Joseph McDonald",
            "Baolin Li",
            "Nathan Frey",
            "Devesh Tiwari",
            "Vijay Gadepally",
            "Siddharth Samsi"
        ],
        "published": "2022-05-19T16:03:55Z",
        "summary": "The energy requirements of current natural language processing models\ncontinue to grow at a rapid, unsustainable pace. Recent works highlighting this\nproblem conclude there is an urgent need for methods that reduce the energy\nneeds of NLP and machine learning more broadly. In this article, we investigate\ntechniques that can be used to reduce the energy consumption of common NLP\napplications. In particular, we focus on techniques to measure energy usage and\ndifferent hardware and datacenter-oriented settings that can be tuned to reduce\nenergy consumption for training and inference for language models. We\ncharacterize the impact of these settings on metrics such as computational\nperformance and energy consumption through experiments conducted on a high\nperformance computing system as well as popular cloud computing platforms.\nThese techniques can lead to significant reduction in energy consumption when\ntraining language models or their use for inference. For example,\npower-capping, which limits the maximum power a GPU can consume, can enable a\n15\\% decrease in energy usage with marginal increase in overall computation\ntime when training a transformer-based language model.",
        "pdf_link": "https://arxiv.org/pdf/2205.09646v1.pdf"
    },
    {
        "title": "Acceptability Judgements via Examining the Topology of Attention Maps",
        "authors": [
            "Daniil Cherniavskii",
            "Eduard Tulchinskii",
            "Vladislav Mikhailov",
            "Irina Proskurina",
            "Laida Kushnareva",
            "Ekaterina Artemova",
            "Serguei Barannikov",
            "Irina Piontkovskaya",
            "Dmitri Piontkovski",
            "Evgeny Burnaev"
        ],
        "published": "2022-05-19T15:45:12Z",
        "summary": "The role of the attention mechanism in encoding linguistic knowledge has\nreceived special interest in NLP. However, the ability of the attention heads\nto judge the grammatical acceptability of a sentence has been underexplored.\nThis paper approaches the paradigm of acceptability judgments with topological\ndata analysis (TDA), showing that the geometric properties of the attention\ngraph can be efficiently exploited for two standard practices in linguistics:\nbinary judgments and linguistic minimal pairs. Topological features enhance the\nBERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three\nlanguages (English, Italian, and Swedish). By revealing the topological\ndiscrepancy between attention maps of minimal pairs, we achieve the human-level\nperformance on the BLiMP benchmark, outperforming nine statistical and\nTransformer LM baselines. At the same time, TDA provides the foundation for\nanalyzing the linguistic functions of attention heads and interpreting the\ncorrespondence between the graph features and grammatical phenomena.",
        "pdf_link": "https://arxiv.org/pdf/2205.09630v2.pdf"
    },
    {
        "title": "Automatic Spoken Language Identification using a Time-Delay Neural Network",
        "authors": [
            "Benjamin Kepecs",
            "Homayoon Beigi"
        ],
        "published": "2022-05-19T13:47:48Z",
        "summary": "Closed-set spoken language identification is the task of recognizing the\nlanguage being spoken in a recorded audio clip from a set of known languages.\nIn this study, a language identification system was built and trained to\ndistinguish between Arabic, Spanish, French, and Turkish based on nothing more\nthan recorded speech. A pre-existing multilingual dataset was used to train a\nseries of acoustic models based on the Tedlium TDNN model to perform automatic\nspeech recognition. The system was provided with a custom multilingual language\nmodel and a specialized pronunciation lexicon with language names prepended to\nphones. The trained model was used to generate phone alignments to test data\nfrom all four languages, and languages were predicted based on a voting scheme\nchoosing the most common language prepend in an utterance. Accuracy was\nmeasured by comparing predicted languages to known languages, and was\ndetermined to be very high in identifying Spanish and Arabic, and somewhat\nlower in identifying Turkish and French.",
        "pdf_link": "https://arxiv.org/pdf/2205.09564v1.pdf"
    },
    {
        "title": "Psychiatric Scale Guided Risky Post Screening for Early Detection of Depression",
        "authors": [
            "Zhiling Zhang",
            "Siyuan Chen",
            "Mengyue Wu",
            "Kenny Q. Zhu"
        ],
        "published": "2022-05-19T12:11:01Z",
        "summary": "Depression is a prominent health challenge to the world, and early risk\ndetection (ERD) of depression from online posts can be a promising technique\nfor combating the threat. Early depression detection faces the challenge of\nefficiently tackling streaming data, balancing the tradeoff between timeliness,\naccuracy and explainability. To tackle these challenges, we propose a\npsychiatric scale guided risky post screening method that can capture risky\nposts related to the dimensions defined in clinical depression scales, and\nproviding interpretable diagnostic basis. A Hierarchical Attentional Network\nequipped with BERT (HAN-BERT) is proposed to further advance explainable\npredictions. For ERD, we propose an online algorithm based on an evolving queue\nof risky posts that can significantly reduce the number of model inferences to\nboost efficiency. Experiments show that our method outperforms the competitive\nfeature-based and neural models under conventional depression detection\nsettings, and achieves simultaneous improvement in both efficacy and efficiency\nfor ERD.",
        "pdf_link": "https://arxiv.org/pdf/2205.09497v1.pdf"
    },
    {
        "title": "Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters",
        "authors": [
            "Yang Xiang",
            "Zhihua Wu",
            "Weibao Gong",
            "Siyu Ding",
            "Xianjie Mo",
            "Yuang Liu",
            "Shuohuan Wang",
            "Peng Liu",
            "Yongshuai Hou",
            "Long Li",
            "Bin Wang",
            "Shaohuai Shi",
            "Yaqian Han",
            "Yue Yu",
            "Ge Li",
            "Yu Sun",
            "Yanjun Ma",
            "Dianhai Yu"
        ],
        "published": "2022-05-19T11:10:14Z",
        "summary": "The ever-growing model size and scale of compute have attracted increasing\ninterests in training deep learning models over multiple nodes. However, when\nit comes to training on cloud clusters, especially across remote clusters, huge\nchallenges are faced. In this work, we introduce a general framework, Nebula-I,\nfor collaboratively training deep learning models over remote heterogeneous\nclusters, the connections between which are low-bandwidth wide area networks\n(WANs). We took natural language processing (NLP) as an example to show how\nNebula-I works in different training phases that include: a) pre-training a\nmultilingual language model using two remote clusters; and b) fine-tuning a\nmachine translation model using knowledge distilled from pre-trained models,\nwhich run through the most popular paradigm of recent deep learning. To balance\nthe accuracy and communication efficiency, in Nebula-I, parameter-efficient\ntraining strategies, hybrid parallel computing methods and adaptive\ncommunication acceleration techniques are jointly applied. Meanwhile, security\nstrategies are employed to guarantee the safety, reliability and privacy in\nintra-cluster computation and inter-cluster communication. Nebula-I is\nimplemented with the PaddlePaddle deep learning framework, which can support\ncollaborative training over heterogeneous hardware, e.g. GPU and NPU.\nExperiments demonstrate that the proposed framework could substantially\nmaximize the training efficiency while preserving satisfactory NLP performance.\nBy using Nebula-I, users can run large-scale training tasks over cloud clusters\nwith minimum developments, and the utility of existed large pre-trained models\ncould be further promoted. We also introduced new state-of-the-art results on\ncross-lingual natural language inference tasks, which are generated based upon\na novel learning framework and Nebula-I.",
        "pdf_link": "https://arxiv.org/pdf/2205.09470v1.pdf"
    },
    {
        "title": "A Weakly-Supervised Iterative Graph-Based Approach to Retrieve COVID-19 Misinformation Topics",
        "authors": [
            "Harry Wang",
            "Sharath Chandra Guntuku"
        ],
        "published": "2022-05-19T09:30:39Z",
        "summary": "The COVID-19 pandemic has been accompanied by an `infodemic' -- of accurate\nand inaccurate health information across social media. Detecting misinformation\namidst dynamically changing information landscape is challenging; identifying\nrelevant keywords and posts is arduous due to the large amount of human effort\nrequired to inspect the content and sources of posts. We aim to reduce the\nresource cost of this process by introducing a weakly-supervised iterative\ngraph-based approach to detect keywords, topics, and themes related to\nmisinformation, with a focus on COVID-19. Our approach can successfully detect\nspecific topics from general misinformation-related seed words in a few seed\ntexts. Our approach utilizes the BERT-based Word Graph Search (BWGS) algorithm\nthat builds on context-based neural network embeddings for retrieving\nmisinformation-related posts. We utilize Latent Dirichlet Allocation (LDA)\ntopic modeling for obtaining misinformation-related themes from the texts\nreturned by BWGS. Furthermore, we propose the BERT-based Multi-directional Word\nGraph Search (BMDWGS) algorithm that utilizes greater starting context\ninformation for misinformation extraction. In addition to a qualitative\nanalysis of our approach, our quantitative analyses show that BWGS and BMDWGS\nare effective in extracting misinformation-related content compared to common\nbaselines in low data resource settings. Extracting such content is useful for\nuncovering prevalent misconceptions and concerns and for facilitating precision\npublic health messaging campaigns to improve health behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2205.09416v1.pdf"
    },
    {
        "title": "Are Prompt-based Models Clueless?",
        "authors": [
            "Pride Kavumba",
            "Ryo Takahashi",
            "Yusuke Oda"
        ],
        "published": "2022-05-19T02:47:58Z",
        "summary": "Finetuning large pre-trained language models with a task-specific head has\nadvanced the state-of-the-art on many natural language understanding\nbenchmarks. However, models with a task-specific head require a lot of training\ndata, making them susceptible to learning and exploiting dataset-specific\nsuperficial cues that do not generalize to other datasets. Prompting has\nreduced the data requirement by reusing the language model head and formatting\nthe task input to match the pre-training objective. Therefore, it is expected\nthat few-shot prompt-based models do not exploit superficial cues. This paper\npresents an empirical examination of whether few-shot prompt-based models also\nexploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,\nHANS, and COPA has revealed that prompt-based models also exploit superficial\ncues. While the models perform well on instances with superficial cues, they\noften underperform or only marginally outperform random accuracy on instances\nwithout superficial cues.",
        "pdf_link": "https://arxiv.org/pdf/2205.09295v2.pdf"
    },
    {
        "title": "Transformer-based Program Synthesis for Low-Data Environments",
        "authors": [
            "Jack Roper"
        ],
        "published": "2022-05-18T23:33:33Z",
        "summary": "Recent advancements in large pre-trained transformer models (GPT2/3, T5) have\nfound use in program synthesis to generate programs that satisfy a set of\ninput/output examples. However, these models perform poorly on long-horizon and\nlow-data tasks, and often don't seem to understand the semantics of the\nlanguages they generate. We investigate an approach that tackles both of these\nissues, by using attributed context-free-grammars of programming languages to\ngenerate programs, and then analyzing generated programs so that they can be\nannotated with compile and runtime attributes, such as types, so that\ninformation about the program can be remembered during long-horizon generation.\nWe firstly find that synthesized datasets can be made efficiently and can\nprovide transformer models with enough data in order to perform well on some\nsynthesis tasks. We also find that giving models access to program attributes\nis especially effective in low-data environments, and tends improve the quality\nand reduce errors of transformer-generated programs.",
        "pdf_link": "https://arxiv.org/pdf/2205.09246v1.pdf"
    },
    {
        "title": "Debiasing Neural Retrieval via In-batch Balancing Regularization",
        "authors": [
            "Yuantong Li",
            "Xiaokai Wei",
            "Zijian Wang",
            "Shen Wang",
            "Parminder Bhatia",
            "Xiaofei Ma",
            "Andrew Arnold"
        ],
        "published": "2022-05-18T22:57:15Z",
        "summary": "People frequently interact with information retrieval (IR) systems, however,\nIR models exhibit biases and discrimination towards various demographics. The\nin-processing fair ranking methods provide a trade-offs between accuracy and\nfairness through adding a fairness-related regularization term in the loss\nfunction. However, there haven't been intuitive objective functions that depend\non the click probability and user engagement to directly optimize towards this.\nIn this work, we propose the In-Batch Balancing Regularization (IBBR) to\nmitigate the ranking disparity among subgroups. In particular, we develop a\ndifferentiable \\textit{normed Pairwise Ranking Fairness} (nPRF) and leverage\nthe T-statistics on top of nPRF over subgroups as a regularization to improve\nfairness. Empirical results with the BERT-based neural rankers on the MS MARCO\nPassage Retrieval dataset with the human-annotated non-gendered queries\nbenchmark \\citep{rekabsaz2020neural} show that our IBBR method with nPRF\nachieves significantly less bias with minimal degradation in ranking\nperformance compared with the baseline.",
        "pdf_link": "https://arxiv.org/pdf/2205.09240v1.pdf"
    },
    {
        "title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
        "authors": [
            "Danilo Ribeiro",
            "Shen Wang",
            "Xiaofei Ma",
            "Rui Dong",
            "Xiaokai Wei",
            "Henry Zhu",
            "Xinchi Chen",
            "Zhiheng Huang",
            "Peng Xu",
            "Andrew Arnold",
            "Dan Roth"
        ],
        "published": "2022-05-18T21:52:11Z",
        "summary": "Large language models have achieved high performance on various question\nanswering (QA) benchmarks, but the explainability of their output remains\nelusive. Structured explanations, called entailment trees, were recently\nsuggested as a way to explain and inspect a QA system's answer. In order to\nbetter generate such entailment trees, we propose an architecture called\nIterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a\ngiven hypothesis by systematically generating a step-by-step explanation from\ntextual premises. The IRGR model iteratively searches for suitable premises,\nconstructing a single entailment step at a time. Contrary to previous\napproaches, our method combines generation steps and retrieval of premises,\nallowing the model to leverage intermediate conclusions, and mitigating the\ninput size limit of baseline encoder-decoder models. We conduct experiments\nusing the EntailmentBank dataset, where we outperform existing benchmarks on\npremise retrieval and entailment tree generation, with around 300% gain in\noverall correctness.",
        "pdf_link": "https://arxiv.org/pdf/2205.09224v2.pdf"
    },
    {
        "title": "LeRaC: Learning Rate Curriculum",
        "authors": [
            "Florinel-Alin Croitoru",
            "Nicolae-Catalin Ristea",
            "Radu Tudor Ionescu",
            "Nicu Sebe"
        ],
        "published": "2022-05-18T18:57:36Z",
        "summary": "Most curriculum learning methods require an approach to sort the data samples\nby difficulty, which is often cumbersome to perform. In this work, we propose a\nnovel curriculum learning approach termed Learning Rate Curriculum (LeRaC),\nwhich leverages the use of a different learning rate for each layer of a neural\nnetwork to create a data-free curriculum during the initial training epochs.\nMore specifically, LeRaC assigns higher learning rates to neural layers closer\nto the input, gradually decreasing the learning rates as the layers are placed\nfarther away from the input. The learning rates increase at various paces\nduring the first training iterations, until they all reach the same value. From\nthis point on, the neural model is trained as usual. This creates a model-level\ncurriculum learning strategy that does not require sorting the examples by\ndifficulty and is compatible with any neural network, generating higher\nperformance levels regardless of the architecture. We conduct comprehensive\nexperiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100,\nTiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D)\ndomains, considering various convolutional (ResNet-18, Wide-ResNet-50,\nDenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr)\narchitectures, comparing our approach with the conventional training regime.\nMoreover, we also compare with Curriculum by Smoothing (CBS), a\nstate-of-the-art data-free curriculum learning approach. Unlike CBS, our\nperformance improvements over the standard training regime are consistent\nacross all datasets and models. Furthermore, we significantly surpass CBS in\nterms of training time (there is no additional cost over the standard training\nregime for LeRaC).",
        "pdf_link": "https://arxiv.org/pdf/2205.09180v2.pdf"
    },
    {
        "title": "Masked Autoencoders As Spatiotemporal Learners",
        "authors": [
            "Christoph Feichtenhofer",
            "Haoqi Fan",
            "Yanghao Li",
            "Kaiming He"
        ],
        "published": "2022-05-18T17:59:59Z",
        "summary": "This paper studies a conceptually simple extension of Masked Autoencoders\n(MAE) to spatiotemporal representation learning from videos. We randomly mask\nout spacetime patches in videos and learn an autoencoder to reconstruct them in\npixels. Interestingly, we show that our MAE method can learn strong\nrepresentations with almost no inductive bias on spacetime (only except for\npatch and positional embeddings), and spacetime-agnostic random masking\nperforms the best. We observe that the optimal masking ratio is as high as 90%\n(vs. 75% on images), supporting the hypothesis that this ratio is related to\ninformation redundancy of the data. A high masking ratio leads to a large\nspeedup, e.g., > 4x in wall-clock time or even more. We report competitive\nresults on several challenging video datasets using vanilla Vision\nTransformers. We observe that MAE can outperform supervised pre-training by\nlarge margins. We further report encouraging results of training on real-world,\nuncurated Instagram data. Our study suggests that the general framework of\nmasked autoencoding (BERT, MAE, etc.) can be a unified methodology for\nrepresentation learning with minimal domain knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2205.09113v2.pdf"
    },
    {
        "title": "Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator",
        "authors": [
            "Guangzhi Sun",
            "Chao Zhang",
            "Philip C Woodland"
        ],
        "published": "2022-05-18T16:40:50Z",
        "summary": "Contextual knowledge is essential for reducing speech recognition errors on\nhigh-valued long-tail words. This paper proposes a novel tree-constrained\npointer generator (TCPGen) component that enables end-to-end ASR models to bias\ntowards a list of long-tail words obtained using external contextual\ninformation. With only a small overhead in memory use and computation cost,\nTCPGen can structure thousands of biasing words efficiently into a symbolic\nprefix-tree and creates a neural shortcut between the tree and the final ASR\noutput to facilitate the recognition of the biasing words. To enhance TCPGen,\nwe further propose a novel minimum biasing word error (MBWE) loss that directly\noptimises biasing word errors during training, along with a biasing-word-driven\nlanguage model discounting (BLMD) method during the test. All contextual ASR\nsystems were evaluated on the public Librispeech audiobook corpus and the data\nfrom the dialogue state tracking challenges (DSTC) with the biasing lists\nextracted from the dialogue-system ontology. Consistent word error rate (WER)\nreductions were achieved with TCPGen, which were particularly significant on\nthe biasing words with around 40\\% relative reductions in the recognition error\nrates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved\nmore significant WER reductions on the biasing words. TCPGen also achieved\nzero-shot learning of words not in the audio training set with large WER\nreductions on the out-of-vocabulary words in the biasing list.",
        "pdf_link": "https://arxiv.org/pdf/2205.09058v2.pdf"
    },
    {
        "title": "GPoeT-2: A GPT-2 Based Poem Generator",
        "authors": [
            "Kai-Ling Lo",
            "Rami Ariss",
            "Philipp Kurz"
        ],
        "published": "2022-05-18T10:25:12Z",
        "summary": "This project aims to produce the next volume of machine-generated poetry, a\ncomplex art form that can be structured and unstructured, and carries depth in\nthe meaning between the lines. GPoeT-2 is based on fine-tuning a state of the\nart natural language model (i.e. GPT-2) to generate limericks, typically\nhumorous structured poems consisting of five lines with a AABBA rhyming scheme.\nWith a two-stage generation system utilizing both forward and reverse language\nmodeling, GPoeT-2 is capable of freely generating limericks in diverse topics\nwhile following the rhyming structure without any seed phrase or a posteriori\nconstraints.Based on the automated generation process, we explore a wide\nvariety of evaluation metrics to quantify \"good poetry,\" including syntactical\ncorrectness, lexical diversity, and subject continuity. Finally, we present a\ncollection of 94 categorized limericks that rank highly on the explored \"good\npoetry\" metrics to provoke human creativity.",
        "pdf_link": "https://arxiv.org/pdf/2205.08847v1.pdf"
    },
    {
        "title": "Evaluation of Transfer Learning for Polish with a Text-to-Text Model",
        "authors": [
            "Aleksandra Chrabrowa",
            "\u0141ukasz Dragan",
            "Karol Grzegorczyk",
            "Dariusz Kajtoch",
            "Miko\u0142aj Koszowski",
            "Robert Mroczkowski",
            "Piotr Rybak"
        ],
        "published": "2022-05-18T09:17:14Z",
        "summary": "We introduce a new benchmark for assessing the quality of text-to-text models\nfor Polish. The benchmark consists of diverse tasks and datasets: KLEJ\nbenchmark adapted for text-to-text, en-pl translation, summarization, and\nquestion answering. In particular, since summarization and question answering\nlack benchmark datasets for the Polish language, we describe their construction\nand make them publicly available. Additionally, we present plT5 - a\ngeneral-purpose text-to-text model for Polish that can be fine-tuned on various\nNatural Language Processing (NLP) tasks with a single training objective.\nUnsupervised denoising pre-training is performed efficiently by initializing\nthe model weights with a multi-lingual T5 (mT5) counterpart. We evaluate the\nperformance of plT5, mT5, Polish BART (plBART), and Polish GPT-2 (papuGaPT2).\nThe plT5 scores top on all of these tasks except summarization, where plBART is\nbest. In general (except for summarization), the larger the model, the better\nthe results. The encoder-decoder architectures prove to be better than the\ndecoder-only equivalent.",
        "pdf_link": "https://arxiv.org/pdf/2205.08808v1.pdf"
    },
    {
        "title": "PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking",
        "authors": [
            "Yixuan Qiao",
            "Hao Chen",
            "Jun Wang",
            "Yongquan Lai",
            "Tuozhen Liu",
            "Xianbin Ye",
            "Xin Tang",
            "Rui Fang",
            "Peng Gao",
            "Wenfeng Xie",
            "Guotong Xie"
        ],
        "published": "2022-05-18T04:38:15Z",
        "summary": "This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.11245v3.pdf"
    },
    {
        "title": "ViralBERT: A User Focused BERT-Based Approach to Virality Prediction",
        "authors": [
            "Rikaz Rameez",
            "Hossein A. Rahmani",
            "Emine Yilmaz"
        ],
        "published": "2022-05-17T21:40:24Z",
        "summary": "Recently, Twitter has become the social network of choice for sharing and\nspreading information to a multitude of users through posts called 'tweets'.\nUsers can easily re-share these posts to other users through 'retweets', which\nallow information to cascade to many more users, increasing its outreach.\nClearly, being able to know the extent to which a post can be retweeted has\ngreat value in advertising, influencing and other such campaigns. In this paper\nwe propose ViralBERT, which can be used to predict the virality of tweets using\ncontent- and user-based features. We employ a method of concatenating numerical\nfeatures such as hashtags and follower numbers to tweet text, and utilise two\nBERT modules: one for semantic representation of the combined text and\nnumerical features, and another module purely for sentiment analysis of text,\nas both the information within text and it's ability to elicit an emotional\nresponse play a part in retweet proneness. We collect a dataset of 330k tweets\nto train ViralBERT and validate the efficacy of our model using baselines from\ncurrent studies in this field. Our experiments show that our approach\noutperforms these baselines, with a 13% increase in both F1 Score and Accuracy\ncompared to the best performing baseline method. We then undergo an ablation\nstudy to investigate the importance of chosen features, finding that text\nsentiment and follower counts, and to a lesser extent mentions and following\ncounts, are the strongest features for the model, and that hashtag counts are\ndetrimental to the model.",
        "pdf_link": "https://arxiv.org/pdf/2206.10298v1.pdf"
    },
    {
        "title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars",
        "authors": [
            "Fangzhou Hong",
            "Mingyuan Zhang",
            "Liang Pan",
            "Zhongang Cai",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "published": "2022-05-17T17:59:19Z",
        "summary": "3D avatar creation plays a crucial role in the digital age. However, the\nwhole production process is prohibitively time-consuming and labor-intensive.\nTo democratize this technology to a larger audience, we propose AvatarCLIP, a\nzero-shot text-driven framework for 3D avatar generation and animation. Unlike\nprofessional software that requires expert knowledge, AvatarCLIP empowers\nlayman users to customize a 3D avatar with the desired shape and texture, and\ndrive the avatar with the described motions using solely natural languages. Our\nkey insight is to take advantage of the powerful vision-language model CLIP for\nsupervising neural human generation, in terms of 3D geometry, texture and\nanimation. Specifically, driven by natural language descriptions, we initialize\n3D human geometry generation with a shape VAE network. Based on the generated\n3D human shapes, a volume rendering model is utilized to further facilitate\ngeometry sculpting and texture generation. Moreover, by leveraging the priors\nlearned in the motion VAE, a CLIP-guided reference-based motion synthesis\nmethod is proposed for the animation of the generated 3D avatar. Extensive\nqualitative and quantitative experiments validate the effectiveness and\ngeneralizability of AvatarCLIP on a wide range of avatars. Remarkably,\nAvatarCLIP can generate unseen 3D avatars with novel animations, achieving\nsuperior zero-shot capability.",
        "pdf_link": "https://arxiv.org/pdf/2205.08535v1.pdf"
    },
    {
        "title": "Feature Aggregation in Zero-Shot Cross-Lingual Transfer Using Multilingual BERT",
        "authors": [
            "Beiduo Chen",
            "Wu Guo",
            "Quan Liu",
            "Kun Tao"
        ],
        "published": "2022-05-17T17:12:19Z",
        "summary": "Multilingual BERT (mBERT), a language model pre-trained on large multilingual\ncorpora, has impressive zero-shot cross-lingual transfer capabilities and\nperforms surprisingly well on zero-shot POS tagging and Named Entity\nRecognition (NER), as well as on cross-lingual model transfer. At present, the\nmainstream methods to solve the cross-lingual downstream tasks are always using\nthe last transformer layer's output of mBERT as the representation of\nlinguistic information. In this work, we explore the complementary property of\nlower layers to the last transformer layer of mBERT. A feature aggregation\nmodule based on an attention mechanism is proposed to fuse the information\ncontained in different layers of mBERT. The experiments are conducted on four\nzero-shot cross-lingual transfer datasets, and the proposed method obtains\nperformance improvements on key multilingual benchmark tasks XNLI (+1.5 %),\nPAWS-X (+2.4 %), NER (+1.2 F1), and POS (+1.5 F1). Through the analysis of the\nexperimental results, we prove that the layers before the last layer of mBERT\ncan provide extra useful information for cross-lingual downstream tasks and\nexplore the interpretability of mBERT empirically.",
        "pdf_link": "https://arxiv.org/pdf/2205.08497v1.pdf"
    },
    {
        "title": "SKILL: Structured Knowledge Infusion for Large Language Models",
        "authors": [
            "Fedor Moiseev",
            "Zhe Dong",
            "Enrique Alfonseca",
            "Martin Jaggi"
        ],
        "published": "2022-05-17T09:12:22Z",
        "summary": "Large language models (LLMs) have demonstrated human-level performance on a\nvast spectrum of natural language tasks. However, it is largely unexplored\nwhether they can better internalize knowledge from a structured data, such as a\nknowledge graph, or from text. In this work, we propose a method to infuse\nstructured knowledge into LLMs, by directly training T5 models on factual\ntriples of knowledge graphs (KGs). We show that models pre-trained on Wikidata\nKG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The\nmodels pre-trained on factual triples compare competitively with the ones on\nnatural language sentences that contain the same knowledge. Trained on a\nsmaller size KG, WikiMovies, we saw 3x improvement of exact match score on\nMetaQA task compared to T5 baseline. The proposed method has an advantage that\nno alignment between the knowledge graph and text corpus is required in\ncurating training data. This makes our method particularly useful when working\nwith industry-scale knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2205.08184v1.pdf"
    },
    {
        "title": "SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation",
        "authors": [
            "Sameer Khurana",
            "Antoine Laurent",
            "James Glass"
        ],
        "published": "2022-05-17T08:58:48Z",
        "summary": "We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level\nCross-Lingual Speech Representation learning framework. Unlike previous works\non speech representation learning, which learns multilingual contextual speech\nembedding at the resolution of an acoustic frame (10-20ms), this work focuses\non learning multimodal (speech-text) multilingual speech embedding at the\nresolution of a sentence (5-10s) such that the embedding vector space is\nsemantically aligned across different languages. We combine state-of-the-art\nmultilingual acoustic frame-level speech representation learning model XLS-R\nwith the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an\nutterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we\ntrain SAMU-XLSR with only multilingual transcribed speech data, cross-lingual\nspeech-text and speech-speech associations emerge in its learned representation\nspace. To substantiate our claims, we use SAMU-XLSR speech encoder in\ncombination with a pre-trained LaBSE text sentence encoder for cross-lingual\nspeech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual\nspeech-to-speech translation retrieval. We highlight these applications by\nperforming several cross-lingual text and speech translation retrieval tasks\nacross several datasets.",
        "pdf_link": "https://arxiv.org/pdf/2205.08180v1.pdf"
    },
    {
        "title": "SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection",
        "authors": [
            "Prabhav Singh",
            "Ridam Srivastava",
            "K. P. S. Rana",
            "Vineet Kumar"
        ],
        "published": "2022-05-17T07:51:55Z",
        "summary": "Fake News Detection (FND) is an essential field in natural language\nprocessing that aims to identify and check the truthfulness of major claims in\na news article to decide the news veracity. FND finds its uses in preventing\nsocial, political and national damage caused due to misrepresentation of facts\nwhich may harm a certain section of society. Further, with the explosive rise\nin fake news dissemination over social media, including images and text, it has\nbecome imperative to identify fake news faster and more accurately. To solve\nthis problem, this work investigates a novel multimodal stacked ensemble-based\napproach (SEMIFND) to fake news detection. Focus is also kept on ensuring\nfaster performance with fewer parameters. Moreover, to improve multimodal\nperformance, a deep unimodal analysis is done on the image modality to identify\nNasNet Mobile as the most appropriate model for the task. For text, an ensemble\nof BERT and ELECTRA is used. The approach was evaluated on two datasets:\nTwitter MediaEval and Weibo Corpus. The suggested framework offered accuracies\nof 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These\nreported metrics are found to be superior when compared to similar recent\nworks. Further, we also report a reduction in the number of parameters used in\ntraining when compared to recent relevant works. SEMI-FND offers an overall\nparameter reduction of at least 20% with unimodal parametric reduction on text\nbeing 60%. Therefore, based on the investigations presented, it is concluded\nthat the application of a stacked ensembling significantly improves FND over\nother approaches while also improving speed.",
        "pdf_link": "https://arxiv.org/pdf/2205.08159v1.pdf"
    },
    {
        "title": "Harnessing Multilingual Resources to Question Answering in Arabic",
        "authors": [
            "Khalid Alnajjar",
            "Mika H\u00e4m\u00e4l\u00e4inen"
        ],
        "published": "2022-05-16T23:28:01Z",
        "summary": "The goal of the paper is to predict answers to questions given a passage of\nQur'an. The answers are always found in the passage, so the task of the model\nis to predict where an answer starts and where it ends. As the initial data set\nis rather small for training, we make use of multilingual BERT so that we can\naugment the training data by using data available for languages other than\nArabic. Furthermore, we crawl a large Arabic corpus that is domain specific to\nreligious discourse. Our approach consists of two steps, first we train a BERT\nmodel to predict a set of possible answers in a passage. Finally, we use\nanother BERT based model to rank the candidate answers produced by the first\nBERT model.",
        "pdf_link": "https://arxiv.org/pdf/2205.08024v1.pdf"
    },
    {
        "title": "The Primacy Bias in Deep Reinforcement Learning",
        "authors": [
            "Evgenii Nikishin",
            "Max Schwarzer",
            "Pierluca D'Oro",
            "Pierre-Luc Bacon",
            "Aaron Courville"
        ],
        "published": "2022-05-16T16:48:36Z",
        "summary": "This work identifies a common flaw of deep reinforcement learning (RL)\nalgorithms: a tendency to rely on early interactions and ignore useful evidence\nencountered later. Because of training on progressively growing datasets, deep\nRL agents incur a risk of overfitting to earlier experiences, negatively\naffecting the rest of the learning process. Inspired by cognitive science, we\nrefer to this effect as the primacy bias. Through a series of experiments, we\ndissect the algorithmic aspects of deep RL that exacerbate this bias. We then\npropose a simple yet generally-applicable mechanism that tackles the primacy\nbias by periodically resetting a part of the agent. We apply this mechanism to\nalgorithms in both discrete (Atari 100k) and continuous action (DeepMind\nControl Suite) domains, consistently improving their performance.",
        "pdf_link": "https://arxiv.org/pdf/2205.07802v1.pdf"
    },
    {
        "title": "Qualitative Differences Between Evolutionary Strategies and Reinforcement Learning Methods for Control of Autonomous Agents",
        "authors": [
            "Nicola Milano",
            "Stefano Nolfi"
        ],
        "published": "2022-05-16T11:51:36Z",
        "summary": "In this paper we analyze the qualitative differences between evolutionary\nstrategies and reinforcement learning algorithms by focusing on two popular\nstate-of-the-art algorithms: the OpenAI-ES evolutionary strategy and the\nProximal Policy Optimization (PPO) reinforcement learning algorithm -- the most\nsimilar methods of the two families. We analyze how the methods differ with\nrespect to: (i) general efficacy, (ii) ability to cope with sparse rewards,\n(iii) propensity/capacity to discover minimal solutions, (iv) dependency on\nreward shaping, and (v) ability to cope with variations of the environmental\nconditions. The analysis of the performance and of the behavioral strategies\ndisplayed by the agents trained with the two methods on benchmark problems\nenable us to demonstrate qualitative differences which were not identified in\nprevious studies, to identify the relative weakness of the two methods, and to\npropose ways to ameliorate some of those weakness. We show that the\ncharacteristics of the reward function has a strong impact which vary\nqualitatively not only for the OpenAI-ES and the PPO but also for alternative\nreinforcement learning algorithms, thus demonstrating the importance of\noptimizing the characteristic of the reward function to the algorithm used.",
        "pdf_link": "https://arxiv.org/pdf/2205.07592v1.pdf"
    },
    {
        "title": "Chemical transformer compression for accelerating both training and inference of molecular modeling",
        "authors": [
            "Yi Yu",
            "Karl Borjesson"
        ],
        "published": "2022-05-16T11:38:31Z",
        "summary": "Transformer models have been developed in molecular science with excellent\nperformance in applications including quantitative structure-activity\nrelationship (QSAR) and virtual screening (VS). Compared with other types of\nmodels, however, they are large, which results in a high hardware requirement\nto abridge time for both training and inference processes. In this work,\ncross-layer parameter sharing (CLPS), and knowledge distillation (KD) are used\nto reduce the sizes of transformers in molecular science. Both methods not only\nhave competitive QSAR predictive performance as compared to the original BERT\nmodel, but also are more parameter efficient. Furthermore, by integrating CLPS\nand KD into a two-state chemical network, we introduce a new deep lite chemical\ntransformer model, DeLiCaTe. DeLiCaTe captures general-domains as well as\ntask-specific knowledge, which lead to a 4x faster rate of both training and\ninference due to a 10- and 3-times reduction of the number of parameters and\nlayers, respectively. Meanwhile, it achieves comparable performance in QSAR and\nVS modeling. Moreover, we anticipate that the model compression strategy\nprovides a pathway to the creation of effective generative transformer models\nfor organic drug and material design.",
        "pdf_link": "https://arxiv.org/pdf/2205.07582v1.pdf"
    },
    {
        "title": "Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data",
        "authors": [
            "Dominik Stammbach",
            "Maria Antoniak",
            "Elliott Ash"
        ],
        "published": "2022-05-16T10:08:11Z",
        "summary": "This paper shows how to use large-scale pre-trained language models to\nextract character roles from narrative texts without training data. Queried\nwith a zero-shot question-answering prompt, GPT-3 can identify the hero,\nvillain, and victim in diverse domains: newspaper articles, movie plot\nsummaries, and political speeches.",
        "pdf_link": "https://arxiv.org/pdf/2205.07557v2.pdf"
    },
    {
        "title": "The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues",
        "authors": [
            "Ana\u00efs Tack",
            "Chris Piech"
        ],
        "published": "2022-05-16T09:36:30Z",
        "summary": "How can we test whether state-of-the-art generative models, such as Blender\nand GPT-3, are good AI teachers, capable of replying to a student in an\neducational dialogue? Designing an AI teacher test is challenging: although\nevaluation methods are much-needed, there is no off-the-shelf solution to\nmeasuring pedagogical ability. This paper reports on a first attempt at an AI\nteacher test. We built a solution around the insight that you can run\nconversational agents in parallel to human teachers in real-world dialogues,\nsimulate how different agents would respond to a student, and compare these\ncounterpart responses in terms of three abilities: speak like a teacher,\nunderstand a student, help a student. Our method builds on the reliability of\ncomparative judgments in education and uses a probabilistic model and Bayesian\nsampling to infer estimates of pedagogical ability. We find that, even though\nconversational agents (Blender in particular) perform well on conversational\nuptake, they are quantifiably worse than real teachers on several pedagogical\ndimensions, especially with regard to helpfulness (Blender: {\\Delta} ability =\n-0.75; GPT-3: {\\Delta} ability = -0.93).",
        "pdf_link": "https://arxiv.org/pdf/2205.07540v1.pdf"
    },
    {
        "title": "What GPT Knows About Who is Who",
        "authors": [
            "Xiaohan Yang",
            "Eduardo Peynetti",
            "Vasco Meerman",
            "Chris Tanner"
        ],
        "published": "2022-05-16T00:59:37Z",
        "summary": "Coreference resolution -- which is a crucial task for understanding discourse\nand language at large -- has yet to witness widespread benefits from large\nlanguage models (LLMs). Moreover, coreference resolution systems largely rely\non supervised labels, which are highly expensive and difficult to annotate,\nthus making it ripe for prompt engineering. In this paper, we introduce a\nQA-based prompt-engineering method and discern \\textit{generative}, pre-trained\nLLMs' abilities and limitations toward the task of coreference resolution. Our\nexperiments show that GPT-2 and GPT-Neo can return valid answers, but that\ntheir capabilities to identify coreferent mentions are limited and\nprompt-sensitive, leading to inconsistent results.",
        "pdf_link": "https://arxiv.org/pdf/2205.07407v1.pdf"
    },
    {
        "title": "Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines",
        "authors": [
            "Cheng Zhang",
            "Hao Zhang",
            "Jie Wang"
        ],
        "published": "2022-05-15T21:53:45Z",
        "summary": "We present a system called TP3 to perform a downstream task of transformers\non generating question-answer pairs (QAPs) from a given article. TP3 first\nfinetunes pretrained transformers on QAP datasets, then uses a preprocessing\npipeline to select appropriate answers, feeds the relevant sentences and the\nanswer to the finetuned transformer to generate candidate QAPs, and finally\nuses a postprocessing pipeline to filter inadequate QAPs. In particular, using\npretrained T5 models as transformers and the SQuAD dataset as the finetruning\ndataset, we show that TP3 generates satisfactory number of QAPs with high\nqualities on the Gaokao-EN dataset.",
        "pdf_link": "https://arxiv.org/pdf/2205.07387v1.pdf"
    },
    {
        "title": "Transkimmer: Transformer Learns to Layer-wise Skim",
        "authors": [
            "Yue Guan",
            "Zhengyi Li",
            "Jingwen Leng",
            "Zhouhan Lin",
            "Minyi Guo"
        ],
        "published": "2022-05-15T16:23:30Z",
        "summary": "Transformer architecture has become the de-facto model for many machine\nlearning tasks from natural language processing and computer vision. As such,\nimproving its computational efficiency becomes paramount. One of the major\ncomputational inefficiency of Transformer-based models is that they spend the\nidentical amount of computation throughout all layers. Prior works have\nproposed to augment the Transformer model with the capability of skimming\ntokens to improve its computational efficiency. However, they suffer from not\nhaving effectual and end-to-end optimization of the discrete skimming\npredictor. To address the above limitations, we propose the Transkimmer\narchitecture, which learns to identify hidden state tokens that are not\nrequired by each layer. The skimmed tokens are then forwarded directly to the\nfinal output, thus reducing the computation of the successive layers. The key\nidea in Transkimmer is to add a parameterized predictor before each layer that\nlearns to make the skimming decision. We also propose to adopt\nreparameterization trick and add skim loss for the end-to-end training of\nTranskimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark\ncompared with vanilla BERT-base baseline with less than 1% accuracy\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2205.07324v1.pdf"
    },
    {
        "title": "TiBERT: Tibetan Pre-trained Language Model",
        "authors": [
            "Yuan Sun",
            "Sisi Liu",
            "Junjie Deng",
            "Xiaobing Zhao"
        ],
        "published": "2022-05-15T14:45:08Z",
        "summary": "The pre-trained language model is trained on large-scale unlabeled text and\ncan achieve state-of-the-art results in many different downstream tasks.\nHowever, the current pre-trained language model is mainly concentrated in the\nChinese and English fields. For low resource language such as Tibetan, there is\nlack of a monolingual pre-trained model. To promote the development of Tibetan\nnatural language processing tasks, this paper collects the large-scale training\ndata from Tibetan websites and constructs a vocabulary that can cover 99.95$\\%$\nof the words in the corpus by using Sentencepiece. Then, we train the Tibetan\nmonolingual pre-trained language model named TiBERT on the data and vocabulary.\nFinally, we apply TiBERT to the downstream tasks of text classification and\nquestion generation, and compare it with classic models and multilingual\npre-trained models, the experimental results show that TiBERT can achieve the\nbest performance. Our model is published in http://tibert.cmli-nlp.com/",
        "pdf_link": "https://arxiv.org/pdf/2205.07303v1.pdf"
    },
    {
        "title": "Classifiers are Better Experts for Controllable Text Generation",
        "authors": [
            "Askhat Sitdikov",
            "Nikita Balagansky",
            "Daniil Gavrilov",
            "Alexander Markov"
        ],
        "published": "2022-05-15T12:58:35Z",
        "summary": "This paper proposes a simple method for controllable text generation based on\nweighting logits with a free-form classifier, namely CAIF sampling. Using an\narbitrary text classifier, we adjust a small part of a language model's logits\nand guide text generation towards or away from classifier prediction. We\nexperimented with toxicity avoidance and sentiment control tasks and showed\nthat the proposed method significantly outperforms recent PPLM, GeDi, and\nDExperts on PPL and task accuracy metrics based on the external classifier of\ngenerated texts. In addition, compared to other approaches, it is easier to\nimplement and tune and has significantly fewer restrictions and requirements.",
        "pdf_link": "https://arxiv.org/pdf/2205.07276v3.pdf"
    },
    {
        "title": "Topic Modelling on Consumer Financial Protection Bureau Data: An Approach Using BERT Based Embeddings",
        "authors": [
            "Vasudeva Raju Sangaraju",
            "Bharath Kumar Bolla",
            "Deepak Kumar Nayak",
            "Jyothsna Kh"
        ],
        "published": "2022-05-15T11:14:47Z",
        "summary": "Customers' reviews and comments are important for businesses to understand\nusers' sentiment about the products and services. However, this data needs to\nbe analyzed to assess the sentiment associated with topics/aspects to provide\nefficient customer assistance. LDA and LSA fail to capture the semantic\nrelationship and are not specific to any domain. In this study, we evaluate\nBERTopic, a novel method that generates topics using sentence embeddings on\nConsumer Financial Protection Bureau (CFPB) data. Our work shows that BERTopic\nis flexible and yet provides meaningful and diverse topics compared to LDA and\nLSA. Furthermore, domain-specific pre-trained embeddings (FinBERT) yield even\nbetter topics. We evaluated the topics on coherence score (c_v) and UMass.",
        "pdf_link": "https://arxiv.org/pdf/2205.07259v1.pdf"
    },
    {
        "title": "Discovering Latent Concepts Learned in BERT",
        "authors": [
            "Fahim Dalvi",
            "Abdul Rafae Khan",
            "Firoj Alam",
            "Nadir Durrani",
            "Jia Xu",
            "Hassan Sajjad"
        ],
        "published": "2022-05-15T09:45:34Z",
        "summary": "A large number of studies that analyze deep neural network models and their\nability to encode various linguistic and non-linguistic concepts provide an\ninterpretation of the inner mechanics of these models. The scope of the\nanalyses is limited to pre-defined concepts that reinforce the traditional\nlinguistic knowledge and do not reflect on how novel concepts are learned by\nthe model. We address this limitation by discovering and analyzing latent\nconcepts learned in neural network models in an unsupervised fashion and\nprovide interpretations from the model's perspective. In this work, we study:\ni) what latent concepts exist in the pre-trained BERT model, ii) how the\ndiscovered latent concepts align or diverge from classical linguistic hierarchy\nand iii) how the latent concepts evolve across layers. Our findings show: i) a\nmodel learns novel concepts (e.g. animal categories and demographic groups),\nwhich do not strictly adhere to any pre-defined categorization (e.g. POS,\nsemantic tags), ii) several latent concepts are based on multiple properties\nwhich may include semantics, syntax, and morphology, iii) the lower layers in\nthe model dominate in learning shallow lexical concepts while the higher layers\nlearn semantic relations and iv) the discovered latent concepts highlight\npotential biases learned in the model. We also release a novel BERT ConceptNet\ndataset (BCN) consisting of 174 concept labels and 1M annotated instances.",
        "pdf_link": "https://arxiv.org/pdf/2205.07237v1.pdf"
    },
    {
        "title": "Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT",
        "authors": [
            "Bowen Shi",
            "Abdelrahman Mohamed",
            "Wei-Ning Hsu"
        ],
        "published": "2022-05-15T04:48:41Z",
        "summary": "This paper investigates self-supervised pre-training for audio-visual speaker\nrepresentation learning where a visual stream showing the speaker's mouth area\nis used alongside speech as inputs. Our study focuses on the Audio-Visual\nHidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose\naudio-visual speech pre-training framework. We conducted extensive experiments\nprobing the effectiveness of pre-training and visual modality. Experimental\nresults suggest that AV-HuBERT generalizes decently to speaker related\ndownstream tasks, improving label efficiency by roughly ten fold for both\naudio-only and audio-visual speaker verification. We also show that\nincorporating visual information, even just the lip area, greatly improves the\nperformance and noise robustness, reducing EER by 38% in the clean condition\nand 75% in noisy conditions.",
        "pdf_link": "https://arxiv.org/pdf/2205.07180v2.pdf"
    },
    {
        "title": "Naturalistic Causal Probing for Morpho-Syntax",
        "authors": [
            "Afra Amini",
            "Tiago Pimentel",
            "Clara Meister",
            "Ryan Cotterell"
        ],
        "published": "2022-05-14T11:47:58Z",
        "summary": "Probing has become a go-to methodology for interpreting and analyzing deep\nneural models in natural language processing. However, there is still a lack of\nunderstanding of the limitations and weaknesses of various types of probes. In\nthis work, we suggest a strategy for input-level intervention on naturalistic\nsentences. Using our approach, we intervene on the morpho-syntactic features of\na sentence, while keeping the rest of the sentence unchanged. Such an\nintervention allows us to causally probe pre-trained models. We apply our\nnaturalistic causal probing framework to analyze the effects of grammatical\ngender and number on contextualized representations extracted from three\npre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and\nGPT-2. Our experiments suggest that naturalistic interventions lead to stable\nestimates of the causal effects of various linguistic properties. Moreover, our\nexperiments demonstrate the importance of naturalistic causal probing when\nanalyzing pre-trained models.",
        "pdf_link": "https://arxiv.org/pdf/2205.07043v2.pdf"
    },
    {
        "title": "Fake News Quick Detection on Dynamic Heterogeneous Information Networks",
        "authors": [
            "Jin Ho Go",
            "Alina Sari",
            "Jiaojiao Jiang",
            "Shuiqiao Yang",
            "Sanjay Jha"
        ],
        "published": "2022-05-14T11:23:25Z",
        "summary": "The spread of fake news has caused great harm to society in recent years. So\nthe quick detection of fake news has become an important task. Some current\ndetection methods often model news articles and other related components as a\nstatic heterogeneous information network (HIN) and use expensive\nmessage-passing algorithms. However, in the real-world, quickly identifying\nfake news is of great significance and the network may vary over time in terms\nof dynamic nodes and edges. Therefore, in this paper, we propose a novel\nDynamic Heterogeneous Graph Neural Network (DHGNN) for fake news quick\ndetection. More specifically, we first implement BERT and fine-tuned BERT to\nget a semantic representation of the news article contents and author profiles\nand convert it into graph data. Then, we construct the heterogeneous\nnews-author graph to reflect contextual information and relationships.\nAdditionally, we adapt ideas from personalized PageRank propagation and dynamic\npropagation to heterogeneous networks in order to reduce the time complexity of\nback-propagating through many nodes during training. Experiments on three\nreal-world fake news datasets show that DHGNN can outperform other GNN-based\nmodels in terms of both effectiveness and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2205.07039v1.pdf"
    },
    {
        "title": "RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL",
        "authors": [
            "Jiexing Qi",
            "Jingyao Tang",
            "Ziwei He",
            "Xiangpeng Wan",
            "Yu Cheng",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Quanshi Zhang",
            "Zhouhan Lin"
        ],
        "published": "2022-05-14T06:27:40Z",
        "summary": "Relational structures such as schema linking and schema encoding have been\nvalidated as a key component to qualitatively translating natural language into\nSQL queries. However, introducing these structural relations comes with prices:\nthey often result in a specialized model structure, which largely prohibits\nusing large pretrained models in text-to-SQL. To address this problem, we\npropose RASAT: a Transformer seq2seq architecture augmented with relation-aware\nself-attention that could leverage a variety of relational structures while\ninheriting the pretrained parameters from the T5 model effectively. Our model\ncan incorporate almost all types of existing relations in the literature, and\nin addition, we propose introducing co-reference relations for the multi-turn\nscenario. Experimental results on three widely used text-to-SQL datasets,\ncovering both single-turn and multi-turn scenarios, have shown that RASAT could\nachieve state-of-the-art results across all three benchmarks (75.5% EX on\nSpider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).",
        "pdf_link": "https://arxiv.org/pdf/2205.06983v2.pdf"
    },
    {
        "title": "Unified Distributed Environment",
        "authors": [
            "Woong Gyu La",
            "Sunil Muralidhara",
            "Lingjie Kong",
            "Pratik Nichat"
        ],
        "published": "2022-05-14T02:27:35Z",
        "summary": "We propose Unified Distributed Environment (UDE), an environment\nvirtualization toolkit for reinforcement learning research. UDE is designed to\nintegrate environments built on any simulation platform such as Gazebo, Unity,\nUnreal, and OpenAI Gym. Through environment virtualization, UDE enables\noffloading the environment for execution on a remote machine while still\nmaintaining a unified interface. The UDE interface is designed to support\nmulti-agent by default. With environment virtualization and its interface\ndesign, the agent policies can be trained in multiple machines for a\nmulti-agent environment. Furthermore, UDE supports integration with existing\nmajor RL toolkits for researchers to leverage the benefits. This paper\ndiscusses the components of UDE and its design decisions.",
        "pdf_link": "https://arxiv.org/pdf/2205.06946v1.pdf"
    },
    {
        "title": "Bootstrapping Text Anonymization Models with Distant Supervision",
        "authors": [
            "Anthi Papadopoulou",
            "Pierre Lison",
            "Lilja \u00d8vrelid",
            "Ildik\u00f3 Pil\u00e1n"
        ],
        "published": "2022-05-13T21:10:14Z",
        "summary": "We propose a novel method to bootstrap text anonymization models based on\ndistant supervision. Instead of requiring manually labeled training data, the\napproach relies on a knowledge graph expressing the background information\nassumed to be publicly available about various individuals. This knowledge\ngraph is employed to automatically annotate text documents including personal\ndata about a subset of those individuals. More precisely, the method determines\nwhich text spans ought to be masked in order to guarantee $k$-anonymity,\nassuming an adversary with access to both the text documents and the background\ninformation expressed in the knowledge graph. The resulting collection of\nlabeled documents is then used as training data to fine-tune a pre-trained\nlanguage model for text anonymization. We illustrate this approach using a\nknowledge graph extracted from Wikidata and short biographical texts from\nWikipedia. Evaluation results with a RoBERTa-based model and a manually\nannotated collection of 553 summaries showcase the potential of the approach,\nbut also unveil a number of issues that may arise if the knowledge graph is\nnoisy or incomplete. The results also illustrate that, contrary to most\nsequence labeling problems, the text anonymization task may admit several\nalternative solutions.",
        "pdf_link": "https://arxiv.org/pdf/2205.06895v1.pdf"
    },
    {
        "title": "PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain",
        "authors": [
            "Thiago Santos",
            "Amara Tariq",
            "Susmita Das",
            "Kavyasree Vayalpati",
            "Geoffrey H. Smith",
            "Hari Trivedi",
            "Imon Banerjee"
        ],
        "published": "2022-05-13T20:42:07Z",
        "summary": "Pathology text mining is a challenging task given the reporting variability\nand constant new findings in cancer sub-type definitions. However, successful\ntext mining of a large pathology database can play a critical role to advance\n'big data' cancer research like similarity-based treatment selection, case\nidentification, prognostication, surveillance, clinical trial screening, risk\nstratification, and many others. While there is a growing interest in\ndeveloping language models for more specific clinical domains, no\npathology-specific language space exist to support the rapid data-mining\ndevelopment in pathology space. In literature, a few approaches fine-tuned\ngeneral transformer models on specialized corpora while maintaining the\noriginal tokenizer, but in fields requiring specialized terminology, these\nmodels often fail to perform adequately. We propose PathologyBERT - a\npre-trained masked language model which was trained on 347,173 histopathology\nspecimen reports and publicly released in the Huggingface repository. Our\ncomprehensive experiments demonstrate that pre-training of transformer model on\npathology corpora yields performance improvements on Natural Language\nUnderstanding (NLU) and Breast Cancer Diagnose Classification when compared to\nnonspecific language models.",
        "pdf_link": "https://arxiv.org/pdf/2205.06885v1.pdf"
    },
    {
        "title": "A Study of the Attention Abnormality in Trojaned BERTs",
        "authors": [
            "Weimin Lyu",
            "Songzhu Zheng",
            "Tengfei Ma",
            "Chao Chen"
        ],
        "published": "2022-05-13T16:48:37Z",
        "summary": "Trojan attacks raise serious security concerns. In this paper, we investigate\nthe underlying mechanism of Trojaned BERT models. We observe the attention\nfocus drifting behavior of Trojaned models, i.e., when encountering an poisoned\ninput, the trigger token hijacks the attention focus regardless of the context.\nWe provide a thorough qualitative and quantitative analysis of this phenomenon,\nrevealing insights into the Trojan mechanism. Based on the observation, we\npropose an attention-based Trojan detector to distinguish Trojaned models from\nclean ones. To the best of our knowledge, this is the first paper to analyze\nthe Trojan mechanism and to develop a Trojan detector based on the\ntransformer's attention.",
        "pdf_link": "https://arxiv.org/pdf/2205.08305v2.pdf"
    },
    {
        "title": "Controlling Translation Formality Using Pre-trained Multilingual Language Models",
        "authors": [
            "Elijah Rippeth",
            "Sweta Agrawal",
            "Marine Carpuat"
        ],
        "published": "2022-05-13T13:47:28Z",
        "summary": "This paper describes the University of Maryland's submission to the Special\nTask on Formality Control for Spoken Language Translation at \\iwslt, which\nevaluates translation from English into 6 languages with diverse grammatical\nformality markers. We investigate to what extent this problem can be addressed\nwith a \\textit{single multilingual model}, simultaneously controlling its\noutput for target language and formality. Results show that this strategy can\napproach the translation quality and formality control achieved by dedicated\ntranslation models. However, the nature of the underlying pre-trained language\nmodel and of the finetuning samples greatly impact results.",
        "pdf_link": "https://arxiv.org/pdf/2205.06644v1.pdf"
    },
    {
        "title": "Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes",
        "authors": [
            "Antonis Maronikolakis",
            "Philip Baader",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2022-05-13T13:13:46Z",
        "summary": "To tackle the rising phenomenon of hate speech, efforts have been made\ntowards data curation and analysis. When it comes to analysis of bias, previous\nwork has focused predominantly on race. In our work, we further investigate\nbias in hate speech datasets along racial, gender and intersectional axes. We\nidentify strong bias against African American English (AAE), masculine and\nAAE+Masculine tweets, which are annotated as disproportionately more hateful\nand offensive than from other demographics. We provide evidence that BERT-based\nmodels propagate this bias and show that balancing the training data for these\nprotected attributes can lead to fairer models with regards to gender, but not\nrace.",
        "pdf_link": "https://arxiv.org/pdf/2205.06621v2.pdf"
    },
    {
        "title": "Weakly Supervised Text Classification using Supervision Signals from a Language Model",
        "authors": [
            "Ziqian Zeng",
            "Weimin Ni",
            "Tianqing Fang",
            "Xiang Li",
            "Xinran Zhao",
            "Yangqiu Song"
        ],
        "published": "2022-05-13T12:57:15Z",
        "summary": "Solving text classification in a weakly supervised manner is important for\nreal-world applications where human annotations are scarce. In this paper, we\npropose to query a masked language model with cloze style prompts to obtain\nsupervision signals. We design a prompt which combines the document itself and\n\"this article is talking about [MASK].\" A masked language model can generate\nwords for the [MASK] token. The generated words which summarize the content of\na document can be utilized as supervision signals. We propose a latent variable\nmodel to learn a word distribution learner which associates generated words to\npre-defined categories and a document classifier simultaneously without using\nany annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and\nUCINews, shows that our method can outperform baselines by 2%, 4%, and 3%.",
        "pdf_link": "https://arxiv.org/pdf/2205.06604v1.pdf"
    },
    {
        "title": "Improving Contextual Representation with Gloss Regularized Pre-training",
        "authors": [
            "Yu Lin",
            "Zhecheng An",
            "Peihao Wu",
            "Zejun Ma"
        ],
        "published": "2022-05-13T12:50:32Z",
        "summary": "Though achieving impressive results on many NLP tasks, the BERT-like masked\nlanguage models (MLM) encounter the discrepancy between pre-training and\ninference. In light of this gap, we investigate the contextual representation\nof pre-training and inference from the perspective of word probability\ndistribution. We discover that BERT risks neglecting the contextual word\nsimilarity in pre-training. To tackle this issue, we propose an auxiliary gloss\nregularizer module to BERT pre-training (GR-BERT), to enhance word semantic\nsimilarity. By predicting masked words and aligning contextual embeddings to\ncorresponding glosses simultaneously, the word similarity can be explicitly\nmodeled. We design two architectures for GR-BERT and evaluate our model in\ndownstream tasks. Experimental results show that the gloss regularizer benefits\nBERT in word-level and sentence-level semantic representation. The GR-BERT\nachieves new state-of-the-art in lexical substitution task and greatly promotes\nBERT sentence representation in both unsupervised and supervised STS tasks.",
        "pdf_link": "https://arxiv.org/pdf/2205.06603v1.pdf"
    },
    {
        "title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation",
        "authors": [
            "Long Phan",
            "Hieu Tran",
            "Hieu Nguyen",
            "Trieu H. Trinh"
        ],
        "published": "2022-05-13T06:08:35Z",
        "summary": "We present ViT5, a pretrained Transformer-based encoder-decoder model for the\nVietnamese language. With T5-style self-supervised pretraining, ViT5 is trained\non a large corpus of high-quality and diverse Vietnamese texts. We benchmark\nViT5 on two downstream text generation tasks, Abstractive Text Summarization\nand Named Entity Recognition. Although Abstractive Text Summarization has been\nwidely studied for the English language thanks to its rich and large source of\ndata, there has been minimal research into the same task in Vietnamese, a much\nlower resource language. In this work, we perform exhaustive experiments on\nboth Vietnamese Abstractive Summarization and Named Entity Recognition,\nvalidating the performance of ViT5 against many other pretrained\nTransformer-based encoder-decoder models. Our experiments show that ViT5\nsignificantly outperforms existing models and achieves state-of-the-art results\non Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5\nis competitive against previous best results from pretrained encoder-based\nTransformer models. Further analysis shows the importance of context length\nduring the self-supervised pretraining on downstream performance across\ndifferent settings.",
        "pdf_link": "https://arxiv.org/pdf/2205.06457v2.pdf"
    },
    {
        "title": "TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages",
        "authors": [
            "Zihan Zhao",
            "Lu Chen",
            "Ruisheng Cao",
            "Hongshen Xu",
            "Xingyu Chen",
            "Kai Yu"
        ],
        "published": "2022-05-13T03:21:09Z",
        "summary": "Recently, the structural reading comprehension (SRC) task on web pages has\nattracted increasing research interests. Although previous SRC work has\nleveraged extra information such as HTML tags or XPaths, the informative\ntopology of web pages is not effectively exploited. In this work, we propose a\nTopological Information Enhanced model (TIE), which transforms the token-level\ntask into a tag-level task by introducing a two-stage process (i.e. node\nlocating and answer refining). Based on that, TIE integrates Graph Attention\nNetwork (GAT) and Pre-trained Language Model (PLM) to leverage the topological\ninformation of both logical structures and spatial structures. Experimental\nresults demonstrate that our model outperforms strong baselines and achieves\nstate-of-the-art performances on the web-based SRC benchmark WebSRC at the time\nof writing. The code of TIE will be publicly available at\nhttps://github.com/X-LANCE/TIE.",
        "pdf_link": "https://arxiv.org/pdf/2205.06435v1.pdf"
    },
    {
        "title": "Localized Vision-Language Matching for Open-vocabulary Object Detection",
        "authors": [
            "Maria A. Bravo",
            "Sudhanshu Mittal",
            "Thomas Brox"
        ],
        "published": "2022-05-12T15:34:37Z",
        "summary": "In this work, we propose an open-vocabulary object detection method that,\nbased on image-caption pairs, learns to detect novel object classes along with\na given set of known classes. It is a two-stage training approach that first\nuses a location-guided image-caption matching technique to learn class labels\nfor both novel and known classes in a weakly-supervised manner and second\nspecializes the model for the object detection task using known class\nannotations. We show that a simple language model fits better than a large\ncontextualized language model for detecting novel objects. Moreover, we\nintroduce a consistency-regularization technique to better exploit\nimage-caption pair information. Our method compares favorably to existing\nopen-vocabulary detection approaches while being data-efficient. Source code is\navailable at https://github.com/lmb-freiburg/locov .",
        "pdf_link": "https://arxiv.org/pdf/2205.06160v2.pdf"
    },
    {
        "title": "Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?",
        "authors": [
            "Lukas Thoma",
            "Benjamin Roth"
        ],
        "published": "2022-05-12T15:19:54Z",
        "summary": "In recent years, deep neural language models have made strong progress in\nvarious NLP tasks. This work explores one facet of the question whether\nstate-of-the-art NLP models exhibit elementary mechanisms known from human\ncognition. The exploration is focused on a relatively primitive mechanism for\nwhich there is a lot of evidence from various psycholinguistic experiments with\ninfants. The computation of \"abstract sameness relations\" is assumed to play an\nimportant role in human language acquisition and processing, especially in\nlearning more complex grammar rules. In order to investigate this mechanism in\nBERT and other pre-trained language models (PLMs), the experiment designs from\nstudies with infants were taken as the starting point. On this basis, we\ndesigned experimental settings in which each element from the original studies\nwas mapped to a component of language models. Even though the task in our\nexperiments was relatively simple, the results suggest that the cognitive\nfaculty of computing abstract sameness relations is stronger in infants than in\nall investigated PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2205.06149v1.pdf"
    },
    {
        "title": "Efficient and Training-Free Control of Language Generation",
        "authors": [
            "Shangda Wu",
            "Maosong Sun"
        ],
        "published": "2022-05-12T11:48:11Z",
        "summary": "In recent years, there has been a growing interest in the development of\nlanguage models capable of generating text with controllable attributes. While\nseveral approaches have been proposed, many of these methods require\ncondition-specific data or significant computational resources. In this study,\nwe propose a novel method called Gamma Sampling, which enables controllable\nlanguage generation without the need for any training data and maintains a fast\ngeneration speed. Gamma Sampling incorporates attribute-related information\ninto the sampling process, effectively guiding the language model to produce\ntext with desired attributes. Our experimental results demonstrate that Gamma\nSampling, when applied to GPT2, outperforms representative baselines in terms\nof diversity, attribute relevance, and overall quality of the generated\nsamples.",
        "pdf_link": "https://arxiv.org/pdf/2205.06036v5.pdf"
    },
    {
        "title": "Towards Answering Open-ended Ethical Quandary Questions",
        "authors": [
            "Yejin Bang",
            "Nayeon Lee",
            "Tiezheng Yu",
            "Leila Khalatbari",
            "Yan Xu",
            "Samuel Cahyawijaya",
            "Dan Su",
            "Bryan Wilie",
            "Romain Barraud",
            "Elham J. Barezi",
            "Andrea Madotto",
            "Hayden Kee",
            "Pascale Fung"
        ],
        "published": "2022-05-12T09:52:59Z",
        "summary": "Considerable advancements have been made in various NLP tasks based on the\nimpressive power of large language models (LLMs) and many NLP applications are\ndeployed in our daily lives. In this work, we challenge the capability of LLMs\nwith the new task of Ethical Quandary Generative Question Answering. Ethical\nquandary questions are more challenging to address because multiple conflicting\nanswers may exist to a single quandary. We explore the current capability of\nLLMs in providing an answer with a deliberative exchange of different\nperspectives to an ethical quandary, in the approach of Socratic philosophy,\ninstead of providing a closed answer like an oracle. We propose a model that\nsearches for different ethical principles applicable to the ethical quandary\nand generates an answer conditioned on the chosen principles through\nprompt-based few-shot learning. We also discuss the remaining challenges and\nethical issues involved in this task and suggest the direction toward\ndeveloping responsible NLP systems by incorporating human values explicitly.",
        "pdf_link": "https://arxiv.org/pdf/2205.05989v3.pdf"
    },
    {
        "title": "SimCPSR: Simple Contrastive Learning for Paper Submission Recommendation System",
        "authors": [
            "Duc H. Le",
            "Tram T. Doan",
            "Son T. Huynh",
            "Binh T. Nguyen"
        ],
        "published": "2022-05-12T08:08:22Z",
        "summary": "The recommendation system plays a vital role in many areas, especially\nacademic fields, to support researchers in submitting and increasing the\nacceptance of their work through the conference or journal selection process.\nThis study proposes a transformer-based model using transfer learning as an\nefficient approach for the paper submission recommendation system. By combining\nessential information (such as the title, the abstract, and the list of\nkeywords) with the aims and scopes of journals, the model can recommend the Top\nK journals that maximize the acceptance of the paper. Our model had developed\nthrough two states: (i) Fine-tuning the pre-trained language model (LM) with a\nsimple contrastive learning framework. We utilized a simple supervised\ncontrastive objective to fine-tune all parameters, encouraging the LM to learn\nthe document representation effectively. (ii) The fine-tuned LM was then\ntrained on different combinations of the features for the downstream task. This\nstudy suggests a more advanced method for enhancing the efficiency of the paper\nsubmission recommendation system compared to previous approaches when we\nrespectively achieve 0.5173, 0.8097, 0.8862, 0.9496 for Top 1, 3, 5, and 10\naccuracies on the test set for combining the title, abstract, and keywords as\ninput features. Incorporating the journals' aims and scopes, our model shows an\nexciting result by getting 0.5194, 0.8112, 0.8866, and 0.9496 respective to Top\n1, 3, 5, and 10.",
        "pdf_link": "https://arxiv.org/pdf/2205.05940v1.pdf"
    },
    {
        "title": "NER-MQMRC: Formulating Named Entity Recognition as Multi Question Machine Reading Comprehension",
        "authors": [
            "Anubhav Shrimal",
            "Avi Jain",
            "Kartik Mehta",
            "Promod Yenigalla"
        ],
        "published": "2022-05-12T06:54:03Z",
        "summary": "NER has been traditionally formulated as a sequence labeling task. However,\nthere has been recent trend in posing NER as a machine reading comprehension\ntask (Wang et al., 2020; Mengge et al., 2020), where entity name (or other\ninformation) is considered as a question, text as the context and entity value\nin text as answer snippet. These works consider MRC based on a single question\n(entity) at a time. We propose posing NER as a multi-question MRC task, where\nmultiple questions (one question per entity) are considered at the same time\nfor a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC)\narchitecture for this formulation. NER-MQMRC architecture considers all\nentities as input to BERT for learning token embeddings with self-attention and\nleverages BERT-based entity representation for further improving these token\nembeddings for NER task. Evaluation on three NER datasets show that our\nproposed architecture leads to average 2.5 times faster training and 2.3 times\nfaster inference as compared to NER-SQMRC framework based models by considering\nall entities together in a single pass. Further, we show that our model\nperformance does not degrade compared to single-question based MRC (NER-SQMRC)\n(Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for\nAE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this\narchitecture primarily to solve large scale e-commerce attribute (or entity)\nextraction from unstructured text of a magnitude of 50k+ attributes to be\nextracted on a scalable production environment with high performance and\noptimised training and inference runtimes.",
        "pdf_link": "https://arxiv.org/pdf/2205.05904v1.pdf"
    },
    {
        "title": "AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling",
        "authors": [
            "Haoqin Tu",
            "Zhongliang Yang",
            "Jinshuai Yang",
            "Yongfeng Huang"
        ],
        "published": "2022-05-12T03:22:07Z",
        "summary": "Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in\nachieving representation learning and generation for natural language at the\nsame time. Nevertheless, existing VAE-based language models either employ\nelementary RNNs, which is not powerful to handle complex works in the\nmulti-task situation, or fine-tunes two pre-trained language models (PLMs) for\nany downstream task, which is a huge drain on resources. In this paper, we\npropose the first VAE framework empowered with adaptive GPT-2s (AdaVAE).\nDifferent from existing systems, we unify both the encoder\\&decoder of the VAE\nmodel using GPT-2s with adaptive parameter-efficient components, and further\nintroduce Latent Attention operation to better construct latent space from\ntransformer models. Experiments from multiple dimensions validate that AdaVAE\nis competent to effectively organize language in three related tasks (language\nmodeling, representation modeling and guided text generation) even with less\nthan $15\\%$ activated parameters in training. Our code is available at\n\\url{https://github.com/ImKeTT/AdaVAE}.",
        "pdf_link": "https://arxiv.org/pdf/2205.05862v3.pdf"
    },
    {
        "title": "AppTek's Submission to the IWSLT 2022 Isometric Spoken Language Translation Task",
        "authors": [
            "Patrick Wilken",
            "Evgeny Matusov"
        ],
        "published": "2022-05-12T00:02:24Z",
        "summary": "To participate in the Isometric Spoken Language Translation Task of the IWSLT\n2022 evaluation, constrained condition, AppTek developed neural\nTransformer-based systems for English-to-German with various mechanisms of\nlength control, ranging from source-side and target-side pseudo-tokens to\nencoding of remaining length in characters that replaces positional encoding.\nWe further increased translation length compliance by sentence-level selection\nof length-compliant hypotheses from different system variants, as well as\nrescoring of N-best candidates from a single system. Length-compliant\nback-translated and forward-translated synthetic data, as well as other\nparallel data variants derived from the original MuST-C training corpus were\nimportant for a good quality/desired length trade-off. Our experimental results\nshow that length compliance levels above 90% can be reached while minimizing\nlosses in MT quality as measured in BERT and BLEU scores.",
        "pdf_link": "https://arxiv.org/pdf/2205.05807v1.pdf"
    },
    {
        "title": "A time-varying study of Chinese investor sentiment, stock market liquidity and volatility: Based on deep learning BERT model and TVP-VAR model",
        "authors": [
            "Chenrui Zhang",
            "Xinyi Wu",
            "Hailu Deng",
            "Huiwei Zhang"
        ],
        "published": "2022-05-11T18:16:26Z",
        "summary": "Based on the commentary data of the Shenzhen Stock Index bar on the EastMoney\nwebsite from January 1, 2018 to December 31, 2019. This paper extracts the\nembedded investor sentiment by using a deep learning BERT model and\ninvestigates the time-varying linkage between investment sentiment, stock\nmarket liquidity and volatility using a TVP-VAR model. The results show that\nthe impact of investor sentiment on stock market liquidity and volatility is\nstronger. Although the inverse effect is relatively small, it is more\npronounced with the state of the stock market. In all cases, the response is\nmore pronounced in the short term than in the medium to long term, and the\nimpact is asymmetric, with shocks stronger when the market is in a downward\nspiral.",
        "pdf_link": "https://arxiv.org/pdf/2205.05719v2.pdf"
    },
    {
        "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
        "authors": [
            "Katherine M. Collins",
            "Catherine Wong",
            "Jiahai Feng",
            "Megan Wei",
            "Joshua B. Tenenbaum"
        ],
        "published": "2022-05-11T18:14:33Z",
        "summary": "Human language offers a powerful window into our thoughts -- we tell stories,\ngive explanations, and express our beliefs and goals through words. Abundant\nevidence also suggests that language plays a developmental role in structuring\nour learning. Here, we ask: how much of human-like thinking can be captured by\nlearning statistical patterns in language alone? We first contribute a new\nchallenge benchmark for comparing humans and distributional large language\nmodels (LLMs). Our benchmark contains two problem-solving domains (planning and\nexplanation generation) and is designed to require generalization to new,\nout-of-distribution problems expressed in language. We find that humans are far\nmore robust than LLMs on this benchmark. Next, we propose a hybrid\nParse-and-Solve model, which augments distributional LLMs with a structured\nsymbolic reasoning module. We find that this model shows more robust adaptation\nto out-of-distribution planning problems, demonstrating the promise of hybrid\nAI models for more human-like reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2205.05718v1.pdf"
    },
    {
        "title": "Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes",
        "authors": [
            "Hang Dong",
            "V\u00edctor Su\u00e1rez-Paniagua",
            "Huayu Zhang",
            "Minhong Wang",
            "Arlene Casey",
            "Emma Davidson",
            "Jiaoyan Chen",
            "Beatrice Alex",
            "William Whiteley",
            "Honghan Wu"
        ],
        "published": "2022-05-11T17:38:24Z",
        "summary": "Computational text phenotyping is the practice of identifying patients with\ncertain disorders and traits from clinical notes. Rare diseases are challenging\nto be identified due to few cases available for machine learning and the need\nfor data annotation from domain experts. We propose a method using ontologies\nand weak supervision, with recent pre-trained contextual representations from\nBi-directional Transformers (e.g. BERT). The ontology-based framework includes\ntwo steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking\nmentions to concepts in Unified Medical Language System (UMLS), with a Named\nEntity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with\ncustomised rules and contextual mention representation; (ii) UMLS-to-ORDO,\nmatching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology\n(ORDO). The weakly supervised approach is proposed to learn a phenotype\nconfirmation model to improve Text-to-UMLS linking, without annotated data from\ndomain experts. We evaluated the approach on three clinical datasets, MIMIC-III\ndischarge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging\nreports from two institutions in the US and the UK, with annotations. The\nimprovements in the precision were pronounced (by over 30% to 50% absolute\nscore for Text-to-UMLS linking), with almost no loss of recall compared to the\nexisting NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and\nNHS Tayside were consistent with the discharge summaries. The overall pipeline\nprocessing clinical notes can extract rare disease cases, mostly uncaptured in\nstructured data (manually assigned ICD codes). We discuss the usefulness of the\nweak supervision approach and propose directions for future studies.",
        "pdf_link": "https://arxiv.org/pdf/2205.05656v5.pdf"
    },
    {
        "title": "Aggregating Pairwise Semantic Differences for Few-Shot Claim Veracity Classification",
        "authors": [
            "Xia Zeng",
            "Arkaitz Zubiaga"
        ],
        "published": "2022-05-11T17:23:37Z",
        "summary": "As part of an automated fact-checking pipeline, the claim veracity\nclassification task consists in determining if a claim is supported by an\nassociated piece of evidence. The complexity of gathering labelled\nclaim-evidence pairs leads to a scarcity of datasets, particularly when dealing\nwith new domains. In this paper, we introduce SEED, a novel vector-based method\nto few-shot claim veracity classification that aggregates pairwise semantic\ndifferences for claim-evidence pairs. We build on the hypothesis that we can\nsimulate class representative vectors that capture average semantic differences\nfor claim-evidence pairs in a class, which can then be used for classification\nof new instances. We compare the performance of our method with competitive\nbaselines including fine-tuned BERT/RoBERTa models, as well as the\nstate-of-the-art few-shot veracity classification method that leverages\nlanguage model perplexity. Experiments conducted on the FEVER and SCIFACT\ndatasets show consistent improvements over competitive baselines in few-shot\nsettings. Our code is available.",
        "pdf_link": "https://arxiv.org/pdf/2205.05646v1.pdf"
    },
    {
        "title": "Clinical Prompt Learning with Frozen Language Models",
        "authors": [
            "Niall Taylor",
            "Yi Zhang",
            "Dan Joyce",
            "Alejo Nevado-Holgado",
            "Andrey Kormilitzin"
        ],
        "published": "2022-05-11T14:25:13Z",
        "summary": "Prompt learning is a new paradigm in the Natural Language Processing (NLP)\nfield which has shown impressive performance on a number of natural language\ntasks with common benchmarking text datasets in full, few-shot, and zero-shot\ntrain-evaluation setups. Recently, it has even been observed that large but\nfrozen pre-trained language models (PLMs) with prompt learning outperform\nsmaller but fine-tuned models. However, as with many recent NLP trends, the\nperformance of even the largest PLMs such as GPT-3 do not perform well on\nspecialized domains (e.g. medical text), and the common practice to achieve\nState of the Art (SoTA) results still consists of pre-training and fine-tuning\nthe PLMs on downstream tasks. The reliance on fine-tuning large PLMs is\nproblematic in clinical settings where data is often held in non-GPU\nenvironments, and more resource efficient methods of training specialized\ndomain models is crucial. We investigated the viability of prompt learning on\nclinically meaningful decision tasks and directly compared with more\ntraditional fine-tuning methods. Results are partially in line with the prompt\nlearning literature, with prompt learning able to match or improve on\ntraditional fine-tuning with substantially fewer trainable parameters and\nrequiring less training data. We argue that prompt learning therefore provides\nlower computational resource costs applicable to clinical settings, that can\nserve as an alternative to fine-tuning ever increasing in size PLMs.\nComplementary code to reproduce experiments presented in this work can be found\nat: https://github.com/NtaylorOX/Public_Clinical_Prompt.",
        "pdf_link": "https://arxiv.org/pdf/2205.05535v1.pdf"
    },
    {
        "title": "Towards the Generation of Musical Explanations with GPT-3",
        "authors": [
            "Stephen James Krol",
            "Maria Teresa Llano",
            "Jon McCormack"
        ],
        "published": "2022-05-11T13:04:54Z",
        "summary": "Open AI's language model, GPT-3, has shown great potential for many NLP\ntasks, with applications in many different domains. In this work we carry out a\nfirst study on GPT-3's capability to communicate musical decisions through\ntextual explanations when prompted with a textual representation of a piece of\nmusic. Enabling a dialogue in human-AI music partnerships is an important step\ntowards more engaging and creative human-AI interactions. Our results show that\nGPT-3 lacks the necessary intelligence to really understand musical decisions.\nA major barrier to reach a better performance is the lack of data that includes\nexplanations of the creative process carried out by artists for musical pieces.\nWe believe such a resource would aid the understanding and collaboration with\nAI music systems.",
        "pdf_link": "https://arxiv.org/pdf/2206.08264v1.pdf"
    },
    {
        "title": "Query-Based Keyphrase Extraction from Long Documents",
        "authors": [
            "Martin Docekal",
            "Pavel Smrz"
        ],
        "published": "2022-05-11T10:29:30Z",
        "summary": "Transformer-based architectures in natural language processing force input\nsize limits that can be problematic when long documents need to be processed.\nThis paper overcomes this issue for keyphrase extraction by chunking the long\ndocuments while keeping a global context as a query defining the topic for\nwhich relevant keyphrases should be extracted. The developed system employs a\npre-trained BERT model and adapts it to estimate the probability that a given\ntext span forms a keyphrase. We experimented using various context sizes on two\npopular datasets, Inspec and SemEval, and a large novel dataset. The presented\nresults show that a shorter context with a query overcomes a longer one without\nthe query on long documents.",
        "pdf_link": "https://arxiv.org/pdf/2205.05391v1.pdf"
    },
    {
        "title": "Towards Unified Prompt Tuning for Few-shot Text Classification",
        "authors": [
            "Jianing Wang",
            "Chengyu Wang",
            "Fuli Luo",
            "Chuanqi Tan",
            "Minghui Qiu",
            "Fei Yang",
            "Qiuhui Shi",
            "Songfang Huang",
            "Ming Gao"
        ],
        "published": "2022-05-11T07:40:45Z",
        "summary": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language\nModels (PLMs) on few-shot text classification by employing task-specific\nprompts. Yet, PLMs are unfamiliar with prompt-style expressions during\npre-training, which limits the few-shot learning performance on downstream\ntasks. It would be desirable if the models can acquire some prompting knowledge\nbefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning\n(UPT) framework, leading to better few-shot text classification for BERT-style\nmodels by explicitly capturing prompting semantics from non-target NLP\ndatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for\njoint prompt learning across different NLP tasks, forcing PLMs to capture\ntask-invariant prompting knowledge. We further design a self-supervised task\nnamed Knowledge-enhanced Selective Masked Language Modeling to improve the\nPLM's generalization abilities for accurate adaptation to previously unseen\ntasks. After multi-task learning across multiple tasks, the PLM can be better\nprompt-tuned towards any dissimilar target tasks in low-resourced settings.\nExperiments over a variety of NLP tasks show that UPT consistently outperforms\nstate-of-the-arts for prompt-based fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2205.05313v1.pdf"
    },
    {
        "title": "Reducing Activation Recomputation in Large Transformer Models",
        "authors": [
            "Vijay Korthikanti",
            "Jared Casper",
            "Sangkug Lym",
            "Lawrence McAfee",
            "Michael Andersch",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2022-05-10T22:40:17Z",
        "summary": "Training large transformer models is one of the most important computational\nchallenges of modern AI. In this paper, we show how to significantly accelerate\ntraining of large transformer models by reducing activation recomputation.\nActivation recomputation is commonly used to work around memory capacity\nconstraints. Rather than storing activations for backpropagation, they are\ntraditionally recomputed, which saves memory but adds redundant compute. In\nthis work, we show most of this redundant compute is unnecessary because we can\nreduce memory consumption sufficiently without it. We present two novel yet\nvery simple techniques: sequence parallelism and selective activation\nrecomputation. In conjunction with tensor parallelism, these techniques almost\neliminate the need to recompute activations. We evaluate our approach on\nlanguage models up to one trillion parameters in scale and show that our method\nreduces activation memory by 5x, while reducing execution time overhead from\nactivation recomputation by over 90%. For example, when training a 530B\nparameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops\nUtilization of 54.2%, which is 29% faster than the 42.1% we achieve using\nrecomputation. Our implementation will be available in both Megatron-LM and\nNeMo-Megatron.",
        "pdf_link": "https://arxiv.org/pdf/2205.05198v1.pdf"
    },
    {
        "title": "UL2: Unifying Language Learning Paradigms",
        "authors": [
            "Yi Tay",
            "Mostafa Dehghani",
            "Vinh Q. Tran",
            "Xavier Garcia",
            "Jason Wei",
            "Xuezhi Wang",
            "Hyung Won Chung",
            "Siamak Shakeri",
            "Dara Bahri",
            "Tal Schuster",
            "Huaixiu Steven Zheng",
            "Denny Zhou",
            "Neil Houlsby",
            "Donald Metzler"
        ],
        "published": "2022-05-10T19:32:20Z",
        "summary": "Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.",
        "pdf_link": "https://arxiv.org/pdf/2205.05131v3.pdf"
    },
    {
        "title": "Extracting Latent Steering Vectors from Pretrained Language Models",
        "authors": [
            "Nishant Subramani",
            "Nivedita Suresh",
            "Matthew E. Peters"
        ],
        "published": "2022-05-10T19:04:37Z",
        "summary": "Prior work on controllable text generation has focused on learning how to\ncontrol language models through trainable decoding, smart-prompt design, or\nfine-tuning based on a desired objective. We hypothesize that the information\nneeded to steer the model to generate a target sentence is already encoded\nwithin the model. Accordingly, we explore a different approach altogether:\nextracting latent vectors directly from pretrained language model decoders\nwithout fine-tuning. Experiments show that there exist steering vectors, which,\nwhen added to the hidden states of the language model, generate a target\nsentence nearly perfectly (> 99 BLEU) for English sentences from a variety of\ndomains. We show that vector arithmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with performance comparable to models\ntailored to this task. We find that distances between steering vectors reflect\nsentence similarity when evaluated on a textual similarity benchmark (STS-B),\noutperforming pooled hidden states of models. Finally, we present an analysis\nof the intrinsic properties of the steering vectors. Taken together, our\nresults suggest that frozen LMs can be effectively controlled through their\nlatent steering space.",
        "pdf_link": "https://arxiv.org/pdf/2205.05124v1.pdf"
    },
    {
        "title": "Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words",
        "authors": [
            "Kaitlyn Zhou",
            "Kawin Ethayarajh",
            "Dallas Card",
            "Dan Jurafsky"
        ],
        "published": "2022-05-10T18:00:06Z",
        "summary": "Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,\nQA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in\nwhich word similarities estimated by cosine over BERT embeddings are\nunderstated and trace this effect to training data frequency. We find that\nrelative to human judgements, cosine similarity underestimates the similarity\nof frequent words with other instances of the same word or other words across\ncontexts, even after controlling for polysemy and other factors. We conjecture\nthat this underestimation of similarity for high frequency words is due to\ndifferences in the representational geometry of high and low frequency words\nand provide a formal argument for the two-dimensional case.",
        "pdf_link": "https://arxiv.org/pdf/2205.05092v1.pdf"
    },
    {
        "title": "Symphony Generation with Permutation Invariant Language Model",
        "authors": [
            "Jiafeng Liu",
            "Yuanliang Dong",
            "Zehua Cheng",
            "Xinran Zhang",
            "Xiaobing Li",
            "Feng Yu",
            "Maosong Sun"
        ],
        "published": "2022-05-10T13:08:49Z",
        "summary": "In this work, we propose a permutation invariant language model, SymphonyNet,\nas a solution for symbolic symphony music generation. We propose a novel\nMulti-track Multi-instrument Repeatable (MMR) representation for symphonic\nmusic and model the music sequence using a Transformer-based auto-regressive\nlanguage model with specific 3-D positional embedding. To overcome length\noverflow when modeling extra-long symphony tokens, we also propose a modified\nByte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel\nlinear transformer decoder architecture as a backbone. Meanwhile, we train the\ndecoder to learn automatic orchestration as a joint task by masking instrument\ninformation from the input. We also introduce a large-scale symbolic symphony\ndataset for the advance of symphony generation research. Empirical results show\nthat the proposed approach can generate coherent, novel, complex and harmonious\nsymphony as a pioneer solution for multi-track multi-instrument symbolic music\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2205.05448v2.pdf"
    },
    {
        "title": "Ratatouille: A tool for Novel Recipe Generation",
        "authors": [
            "Mansi Goel",
            "Pallab Chakraborty",
            "Vijay Ponnaganti",
            "Minnet Khan",
            "Sritanaya Tatipamala",
            "Aakanksha Saini",
            "Ganesh Bagler"
        ],
        "published": "2022-05-10T11:20:19Z",
        "summary": "Due to availability of a large amount of cooking recipes online, there is a\ngrowing interest in using this as data to create novel recipes. Novel Recipe\nGeneration is a problem in the field of Natural Language Processing in which\nour main interest is to generate realistic, novel cooking recipes. To come up\nwith such novel recipes, we trained various Deep Learning models such as LSTMs\nand GPT-2 with a large amount of recipe data. We present Ratatouille\n(https://cosylab.iiitd.edu.in/ratatouille2/), a web based application to\ngenerate novel recipes.",
        "pdf_link": "https://arxiv.org/pdf/2206.08267v1.pdf"
    },
    {
        "title": "The Importance of Context in Very Low Resource Language Modeling",
        "authors": [
            "Lukas Edman",
            "Antonio Toral",
            "Gertjan van Noord"
        ],
        "published": "2022-05-10T11:19:56Z",
        "summary": "This paper investigates very low resource language model pretraining, when\nless than 100 thousand sentences are available. We find that, in very low\nresource scenarios, statistical n-gram language models outperform\nstate-of-the-art neural models. Our experiments show that this is mainly due to\nthe focus of the former on a local context. As such, we introduce three methods\nto improve a neural model's performance in the low-resource setting, finding\nthat limiting the model's self-attention is the most effective one, improving\non downstream tasks such as NLI and POS tagging by up to 5% for the languages\nwe test on: English, Hindi, and Turkish.",
        "pdf_link": "https://arxiv.org/pdf/2205.04810v1.pdf"
    },
    {
        "title": "Deep learning based Chinese text sentiment mining and stock market correlation research",
        "authors": [
            "Chenrui Zhang"
        ],
        "published": "2022-05-10T08:35:33Z",
        "summary": "We explore how to crawl financial forum data such as stock bars and combine\nthem with deep learning models for sentiment analysis. In this paper, we will\nuse the BERT model to train against the financial corpus and predict the SZSE\nComponent Index, and find that applying the BERT model to the financial corpus\nthrough the maximum information coefficient comparison study. The obtained\nsentiment features will be able to reflect the fluctuations in the stock market\nand help to improve the prediction accuracy effectively. Meanwhile, this paper\ncombines deep learning with financial text, in further exploring the mechanism\nof investor sentiment on stock market through deep learning method, which will\nbe beneficial for national regulators and policy departments to develop more\nreasonable policy guidelines for maintaining the stability of stock market.",
        "pdf_link": "https://arxiv.org/pdf/2205.04743v1.pdf"
    },
    {
        "title": "From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective",
        "authors": [
            "Thibault Formal",
            "Carlos Lassance",
            "Benjamin Piwowarski",
            "St\u00e9phane Clinchant"
        ],
        "published": "2022-05-10T08:08:43Z",
        "summary": "Neural retrievers based on dense representations combined with Approximate\nNearest Neighbors search have recently received a lot of attention, owing their\nsuccess to distillation and/or better sampling of examples for training --\nwhile still relying on the same backbone architecture. In the meantime, sparse\nrepresentation learning fueled by traditional inverted indexing techniques has\nseen a growing interest, inheriting from desirable IR priors such as explicit\nlexical matching. While some architectural variants have been proposed, a\nlesser effort has been put in the training of such models. In this work, we\nbuild on SPLADE -- a sparse expansion-based retriever -- and show to which\nextent it is able to benefit from the same training improvements as dense\nmodels, by studying the effect of distillation, hard-negative mining as well as\nthe Pre-trained Language Model initialization. We furthermore study the link\nbetween effectiveness and efficiency, on in-domain and zero-shot settings,\nleading to state-of-the-art results in both scenarios for sufficiently\nexpressive models.",
        "pdf_link": "https://arxiv.org/pdf/2205.04733v2.pdf"
    },
    {
        "title": "Long Document Re-ranking with Modular Re-ranker",
        "authors": [
            "Luyu Gao",
            "Jamie Callan"
        ],
        "published": "2022-05-09T13:44:02Z",
        "summary": "Long document re-ranking has been a challenging problem for neural re-rankers\nbased on deep language models like BERT. Early work breaks the documents into\nshort passage-like chunks. These chunks are independently mapped to scalar\nscores or latent vectors, which are then pooled into a final relevance score.\nThese encode-and-pool methods however inevitably introduce an information\nbottleneck: the low dimension representations. In this paper, we propose\ninstead to model full query-to-document interaction, leveraging the attention\noperation and modular Transformer re-ranker framework. First, document chunks\nare encoded independently with an encoder module. An interaction module then\nencodes the query and performs joint attention from the query to all document\nchunk representations. We demonstrate that the model can use this new degree of\nfreedom to aggregate important information from the entire document. Our\nexperiments show that this design produces effective re-ranking on two\nclassical IR collections Robust04 and ClueWeb09, and a large-scale supervised\ncollection MS-MARCO document ranking.",
        "pdf_link": "https://arxiv.org/pdf/2205.04275v2.pdf"
    },
    {
        "title": "LayoutXLM vs. GNN: An Empirical Evaluation of Relation Extraction for Documents",
        "authors": [
            "Herv\u00e9 D\u00e9jean",
            "St\u00e9phane Clinchant",
            "Jean-Luc Meunier"
        ],
        "published": "2022-05-09T13:36:09Z",
        "summary": "This paper investigates the Relation Extraction task in documents by\nbenchmarking two different neural network models: a multi-modal language model\n(LayoutXLM) and a Graph Neural Network: Edge Convolution Network (ECN). For\nthis benchmark, we use the XFUND dataset, released along with LayoutXLM. While\nboth models reach similar results, they both exhibit very different\ncharacteristics. This raises the question on how to integrate various\nmodalities in a neural network: by merging all modalities thanks to additional\npretraining (LayoutXLM), or in a cascaded way (ECN). We conclude by discussing\nsome methodological issues that must be considered for new datasets and task\ndefinition in the domain of Information Extraction with complex documents.",
        "pdf_link": "https://arxiv.org/pdf/2206.10304v1.pdf"
    },
    {
        "title": "Multi-segment preserving sampling for deep manifold sampler",
        "authors": [
            "Daniel Berenberg",
            "Jae Hyeon Lee",
            "Simon Kelow",
            "Ji Won Park",
            "Andrew Watkins",
            "Vladimir Gligorijevi\u0107",
            "Richard Bonneau",
            "Stephen Ra",
            "Kyunghyun Cho"
        ],
        "published": "2022-05-09T13:19:41Z",
        "summary": "Deep generative modeling for biological sequences presents a unique challenge\nin reconciling the bias-variance trade-off between explicit biological insight\nand model flexibility. The deep manifold sampler was recently proposed as a\nmeans to iteratively sample variable-length protein sequences by exploiting the\ngradients from a function predictor. We introduce an alternative approach to\nthis guided sampling procedure, multi-segment preserving sampling, that enables\nthe direct inclusion of domain-specific knowledge by designating preserved and\nnon-preserved segments along the input sequence, thereby restricting variation\nto only select regions. We present its effectiveness in the context of antibody\ndesign by training two models: a deep manifold sampler and a GPT-2 language\nmodel on nearly six million heavy chain sequences annotated with the IGHV1-18\ngene. During sampling, we restrict variation to only the\ncomplementarity-determining region 3 (CDR3) of the input. We obtain log\nprobability scores from a GPT-2 model for each sampled CDR3 and demonstrate\nthat multi-segment preserving sampling generates reasonable designs while\nmaintaining the desired, preserved regions.",
        "pdf_link": "https://arxiv.org/pdf/2205.04259v1.pdf"
    },
    {
        "title": "Research on the correlation between text emotion mining and stock market based on deep learning",
        "authors": [
            "Chenrui Zhang"
        ],
        "published": "2022-05-09T12:51:16Z",
        "summary": "This paper discusses how to crawl the data of financial forums such as stock\nbar, and conduct emotional analysis combined with the in-depth learning model.\nThis paper will use the Bert model to train the financial corpus and predict\nthe Shenzhen stock index. Through the comparative study of the maximal\ninformation coefficient (MIC), it is found that the emotional characteristics\nobtained by applying the BERT model to the financial corpus can be reflected in\nthe fluctuation of the stock market, which is conducive to effectively improve\nthe prediction accuracy. At the same time, this paper combines in-depth\nlearning with financial texts to further explore the impact mechanism of\ninvestor sentiment on the stock market through in-depth learning, which will\nhelp the national regulatory authorities and policy departments to formulate\nmore reasonable policies and guidelines for maintaining the stability of the\nstock market.",
        "pdf_link": "https://arxiv.org/pdf/2205.06675v1.pdf"
    },
    {
        "title": "A Dataset and BERT-based Models for Targeted Sentiment Analysis on Turkish Texts",
        "authors": [
            "M. Melih Mutlu",
            "Arzucan \u00d6zg\u00fcr"
        ],
        "published": "2022-05-09T10:57:39Z",
        "summary": "Targeted Sentiment Analysis aims to extract sentiment towards a particular\ntarget from a given text. It is a field that is attracting attention due to the\nincreasing accessibility of the Internet, which leads people to generate an\nenormous amount of data. Sentiment analysis, which in general requires\nannotated data for training, is a well-researched area for widely studied\nlanguages such as English. For low-resource languages such as Turkish, there is\na lack of such annotated data. We present an annotated Turkish dataset suitable\nfor targeted sentiment analysis. We also propose BERT-based models with\ndifferent architectures to accomplish the task of targeted sentiment analysis.\nThe results demonstrate that the proposed models outperform the traditional\nsentiment analysis models for the targeted sentiment analysis task.",
        "pdf_link": "https://arxiv.org/pdf/2205.04185v1.pdf"
    },
    {
        "title": "A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank",
        "authors": [
            "Dan Malkin",
            "Tomasz Limisiewicz",
            "Gabriel Stanovsky"
        ],
        "published": "2022-05-09T07:32:50Z",
        "summary": "We show that the choice of pretraining languages affects downstream\ncross-lingual transfer for BERT-based models. We inspect zero-shot performance\nin balanced data conditions to mitigate data size confounds, classifying\npretraining languages that improve downstream performance as donors, and\nlanguages that are improved in zero-shot performance as recipients. We develop\na method of quadratic time complexity in the number of languages to estimate\nthese relations, instead of an exponential exhaustive computation of all\npossible combinations. We find that our method is effective on a diverse set of\nlanguages spanning different linguistic features and two downstream tasks. Our\nfindings can inform developers of large-scale multilingual language models in\nchoosing better pretraining configurations.",
        "pdf_link": "https://arxiv.org/pdf/2205.04086v1.pdf"
    },
    {
        "title": "Automated Evaluation for Student Argumentative Writing: A Survey",
        "authors": [
            "Xinyu Wang",
            "Yohan Lee",
            "Juneyoung Park"
        ],
        "published": "2022-05-09T07:27:59Z",
        "summary": "This paper surveys and organizes research works in an under-studied area,\nwhich we call automated evaluation for student argumentative writing. Unlike\ntraditional automated writing evaluation that focuses on holistic essay\nscoring, this field is more specific: it focuses on evaluating argumentative\nessays and offers specific feedback, including argumentation structures,\nargument strength trait score, etc. The focused and detailed evaluation is\nuseful for helping students acquire important argumentation skill. In this\npaper we organize existing works around tasks, data and methods. We further\nexperiment with BERT on representative datasets, aiming to provide up-to-date\nbaselines for this field.",
        "pdf_link": "https://arxiv.org/pdf/2205.04083v1.pdf"
    },
    {
        "title": "Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning",
        "authors": [
            "Fei Wang",
            "Zhewei Xu",
            "Pedro Szekely",
            "Muhao Chen"
        ],
        "published": "2022-05-08T23:37:27Z",
        "summary": "Controlled table-to-text generation seeks to generate natural language\ndescriptions for highlighted subparts of a table. Previous SOTA systems still\nemploy a sequence-to-sequence generation method, which merely captures the\ntable as a linear structure and is brittle when table layouts change. We seek\nto go beyond this paradigm by (1) effectively expressing the relations of\ncontent pieces in the table, and (2) making our model robust to\ncontent-invariant structural transformations. Accordingly, we propose an\nequivariance learning framework, which encodes tables with a structure-aware\nself-attention mechanism. This prunes the full self-attention structure into an\norder-invariant graph attention that captures the connected graph structure of\ncells belonging to the same row or column, and it differentiates between\nrelevant cells and irrelevant cells from the structural perspective. Our\nframework also modifies the positional encoding mechanism to preserve the\nrelative position of tokens in the same cell but enforce position invariance\namong different cells. Our technology is free to be plugged into existing\ntable-to-text generation models, and has improved T5-based models to offer\nbetter performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,\nwe preserve promising performance, while previous SOTA systems, even with\ntransformation-based data augmentation, have seen significant performance\ndrops. Our code is available at https://github.com/luka-group/Lattice.",
        "pdf_link": "https://arxiv.org/pdf/2205.03972v1.pdf"
    },
    {
        "title": "Multimodal Semi-Supervised Learning for Text Recognition",
        "authors": [
            "Aviad Aberdam",
            "Roy Ganz",
            "Shai Mazor",
            "Ron Litman"
        ],
        "published": "2022-05-08T13:55:30Z",
        "summary": "Until recently, the number of public real-world text images was insufficient\nfor training scene text recognizers. Therefore, most modern training methods\nrely on synthetic data and operate in a fully supervised manner. Nevertheless,\nthe amount of public real-world text images has increased significantly lately,\nincluding a great deal of unlabeled data. Leveraging these resources requires\nsemi-supervised approaches; however, the few existing methods do not account\nfor vision-language multimodality structure and therefore suboptimal for\nstate-of-the-art multimodal architectures. To bridge this gap, we present\nsemi-supervised learning for multimodal text recognizers (SemiMTR) that\nleverages unlabeled data at each modality training phase. Notably, our method\nrefrains from extra training stages and maintains the current three-stage\nmultimodal training procedure. Our algorithm starts by pretraining the vision\nmodel through a single-stage training that unifies self-supervised learning\nwith supervised training. More specifically, we extend an existing visual\nrepresentation learning algorithm and propose the first contrastive-based\nmethod for scene text recognition. After pretraining the language model on a\ntext corpus, we fine-tune the entire network via a sequential, character-level,\nconsistency regularization between weakly and strongly augmented views of text\nimages. In a novel setup, consistency is enforced on each modality separately.\nExtensive experiments validate that our method outperforms the current training\nschemes and achieves state-of-the-art results on multiple scene text\nrecognition benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2205.03873v1.pdf"
    },
    {
        "title": "On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation",
        "authors": [
            "Yongjie Wang",
            "Chuan Wang",
            "Ruobing Li",
            "Hui Lin"
        ],
        "published": "2022-05-08T10:36:54Z",
        "summary": "In recent years, pre-trained models have become dominant in most natural\nlanguage processing (NLP) tasks. However, in the area of Automated Essay\nScoring (AES), pre-trained models such as BERT have not been properly used to\noutperform other deep learning models such as LSTM. In this paper, we introduce\na novel multi-scale essay representation for BERT that can be jointly learned.\nWe also employ multiple losses and transfer learning from out-of-domain essays\nto further improve the performance. Experiment results show that our approach\nderives much benefit from joint learning of multi-scale essay representation\nand obtains almost the state-of-the-art result among all deep learning models\nin the ASAP task. Our multi-scale essay representation also generalizes well to\nCommonLit Readability Prize data set, which suggests that the novel text\nrepresentation proposed in this paper may be a new and effective choice for\nlong-text tasks.",
        "pdf_link": "https://arxiv.org/pdf/2205.03835v2.pdf"
    },
    {
        "title": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence",
        "authors": [
            "Myeongjun Jang",
            "Frank Mtumbuka",
            "Thomas Lukasiewicz"
        ],
        "published": "2022-05-08T08:37:36Z",
        "summary": "The logical negation property (LNP), which implies generating different\npredictions for semantically opposite inputs, is an important property that a\ntrustworthy language model must satisfy. However, much recent evidence shows\nthat large-size pre-trained language models (PLMs) do not satisfy this\nproperty. In this paper, we perform experiments using probing tasks to assess\nPLM's LNP understanding. Unlike previous studies that only examined negation\nexpressions, we expand the boundary of the investigation to lexical semantics.\nThrough experiments, we observe that PLMs violate the LNP frequently. To\nalleviate the issue, we propose a novel intermediate training task, names\nmeaning-matching, designed to directly learn a meaning-text correspondence,\ninstead of relying on the distributional hypothesis. Through multiple\nexperiments, we find that the task enables PLMs to learn lexical semantic\ninformation. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm\nthat it is a safe intermediate task that guarantees a similar or better\nperformance of downstream tasks. Finally, we observe that our proposed approach\noutperforms our previous counterparts despite its time and resource efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2205.03815v1.pdf"
    },
    {
        "title": "Context-Aware Abbreviation Expansion Using Large Language Models",
        "authors": [
            "Shanqing Cai",
            "Subhashini Venugopalan",
            "Katrin Tomanek",
            "Ajit Narayanan",
            "Meredith Ringel Morris",
            "Michael P. Brenner"
        ],
        "published": "2022-05-08T03:02:53Z",
        "summary": "Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.",
        "pdf_link": "https://arxiv.org/pdf/2205.03767v3.pdf"
    },
    {
        "title": "AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury",
        "authors": [
            "Chengsheng Mao",
            "Liang Yao",
            "Yuan Luo"
        ],
        "published": "2022-05-07T18:04:31Z",
        "summary": "Acute kidney injury (AKI) is a common clinical syndrome characterized by a\nsudden episode of kidney failure or kidney damage within a few hours or a few\ndays. Accurate early prediction of AKI for patients in ICU who are more likely\nthan others to have AKI can enable timely interventions, and reduce the\ncomplications of AKI. Much of the clinical information relevant to AKI is\ncaptured in clinical notes that are largely unstructured text and requires\nadvanced natural language processing (NLP) for useful information extraction.\nOn the other hand, pre-trained contextual language models such as Bidirectional\nEncoder Representations from Transformers (BERT) have improved performances for\nmany NLP tasks in general domain recently. However, few have explored BERT on\ndisease-specific medical domain tasks such as AKI early prediction. In this\npaper, we try to apply BERT to specific diseases and present an AKI\ndomain-specific pre-trained language model based on BERT (AKI-BERT) that could\nbe used to mine the clinical notes for early prediction of AKI. AKI-BERT is a\nBERT model pre-trained on the clinical notes of patients having risks for AKI.\nOur experiments on Medical Information Mart for Intensive Care III (MIMIC-III)\ndataset demonstrate that AKI-BERT can yield performance improvements for early\nAKI prediction, thus expanding the utility of the BERT model from general\nclinical domain to disease-specific domain.",
        "pdf_link": "https://arxiv.org/pdf/2205.03695v1.pdf"
    },
    {
        "title": "Vector Representations of Idioms in Conversational Systems",
        "authors": [
            "Tosin Adewumi",
            "Foteini Liwicki",
            "Marcus Liwicki"
        ],
        "published": "2022-05-07T14:50:05Z",
        "summary": "We demonstrate, in this study, that an open-domain conversational system\ntrained on idioms or figurative language generates more fitting responses to\nprompts containing idioms. Idioms are part of everyday speech in many\nlanguages, across many cultures, but they pose a great challenge for many\nNatural Language Processing (NLP) systems that involve tasks such as\nInformation Retrieval (IR) and Machine Translation (MT), besides conversational\nAI. We utilize the Potential Idiomatic Expression (PIE)-English idioms corpus\nfor the two tasks that we investigate: classification and conversation\ngeneration. We achieve state-of-the-art (SoTA) result of 98% macro F1 score on\nthe classification task by using the SoTA T5 model. We experiment with three\ninstances of the SoTA dialogue model, Dialogue Generative Pre-trained\nTransformer (DialoGPT), for conversation generation. Their performances are\nevaluated using the automatic metric perplexity and human evaluation. The\nresults show that the model trained on the idiom corpus generates more fitting\nresponses to prompts containing idioms 71.9% of the time, compared to a similar\nmodel not trained on the idioms corpus. We contribute the model checkpoint/demo\nand code on the HuggingFace hub for public access.",
        "pdf_link": "https://arxiv.org/pdf/2205.03666v1.pdf"
    },
    {
        "title": "Improving Downstream Task Performance by Treating Numbers as Entities",
        "authors": [
            "Dhanasekar Sundararaman",
            "Vivek Subramanian",
            "Guoyin Wang",
            "Liyan Xu",
            "Lawrence Carin"
        ],
        "published": "2022-05-07T05:22:43Z",
        "summary": "Numbers are essential components of text, like any other word tokens, from\nwhich natural language processing (NLP) models are built and deployed. Though\nnumbers are typically not accounted for distinctly in most NLP tasks, there is\nstill an underlying amount of numeracy already exhibited by NLP models. In this\nwork, we attempt to tap this potential of state-of-the-art NLP models and\ntransfer their ability to boost performance in related tasks. Our proposed\nclassification of numbers into entities helps NLP models perform well on\nseveral tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on\nquestion answering using joint embeddings, outperforming the BERT and RoBERTa\nbaseline classification.",
        "pdf_link": "https://arxiv.org/pdf/2205.03559v2.pdf"
    },
    {
        "title": "When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",
        "authors": [
            "Sebastian Schuster",
            "Tal Linzen"
        ],
        "published": "2022-05-06T20:49:27Z",
        "summary": "Understanding longer narratives or participating in conversations requires\ntracking of discourse entities that have been mentioned. Indefinite noun\nphrases (NPs), such as 'a dog', frequently introduce discourse entities but\nthis behavior is modulated by sentential operators such as negation. For\nexample, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse\nentity due to the presence of negation. In this work, we adapt the\npsycholinguistic assessment of language models paradigm to higher-level\nlinguistic phenomena and introduce an English evaluation suite that targets the\nknowledge of the interactions between sentential operators and indefinite NPs.\nWe use this evaluation suite for a fine-grained investigation of the entity\ntracking abilities of the Transformer-based models GPT-2 and GPT-3. We find\nthat while the models are to a certain extent sensitive to the interactions we\ninvestigate, they are all challenged by the presence of multiple NPs and their\nbehavior is not systematic, which suggests that even models at the scale of\nGPT-3 do not fully acquire basic entity tracking abilities.",
        "pdf_link": "https://arxiv.org/pdf/2205.03472v1.pdf"
    },
    {
        "title": "A Data Cartography based MixUp for Pre-trained Language Models",
        "authors": [
            "Seo Yeon Park",
            "Cornelia Caragea"
        ],
        "published": "2022-05-06T17:59:19Z",
        "summary": "MixUp is a data augmentation strategy where additional samples are generated\nduring training by combining random pairs of training samples and their labels.\nHowever, selecting random pairs is not potentially an optimal choice. In this\nwork, we propose TDMixUp, a novel MixUp strategy that leverages Training\nDynamics and allows more informative samples to be combined for generating new\ndata samples. Our proposed TDMixUp first measures confidence, variability,\n(Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al.,\n2020) to identify the characteristics of training samples (e.g., as\neasy-to-learn or ambiguous samples), and then interpolates these characterized\nsamples. We empirically validate that our method not only achieves competitive\nperformance using a smaller subset of the training data compared with strong\nbaselines, but also yields lower expected calibration error on the pre-trained\nlanguage model, BERT, on both in-domain and out-of-domain settings in a wide\nrange of NLP tasks. We publicly release our code.",
        "pdf_link": "https://arxiv.org/pdf/2205.03403v1.pdf"
    },
    {
        "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
        "authors": [
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2022-05-06T17:57:58Z",
        "summary": "Does prompting a large language model (LLM) like GPT-3 with explanations\nimprove in-context learning? We study this question on two NLP tasks that\ninvolve reasoning over text, namely question answering and natural language\ninference. We test the performance of four LLMs on three textual reasoning\ndatasets using prompts that include explanations in multiple different styles.\nFor these tasks, we find that including explanations in the prompts for OPT,\nGPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to\nmoderate accuracy improvements over standard few-show learning. However,\ntext-davinci-002 is able to benefit more substantially.\n  We further show that explanations generated by the LLMs may not entail the\nmodels' predictions nor be factually grounded in the input, even on simple\ntasks with extractive explanations. However, these flawed explanations can\nstill be useful as a way to verify LLMs' predictions post-hoc. Through analysis\nin our three settings, we show that explanations judged by humans to be\ngood--logically consistent with the input and the prediction--more likely\ncooccur with accurate predictions. Following these observations, we train\ncalibrators using automatically extracted scores that assess the reliability of\nexplanations, allowing us to improve performance post-hoc across all of our\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2205.03401v2.pdf"
    },
    {
        "title": "Prompt Distribution Learning",
        "authors": [
            "Yuning Lu",
            "Jianzhuang Liu",
            "Yonggang Zhang",
            "Yajing Liu",
            "Xinmei Tian"
        ],
        "published": "2022-05-06T16:22:36Z",
        "summary": "We present prompt distribution learning for effectively adapting a\npre-trained vision-language model to address downstream recognition tasks. Our\nmethod not only learns low-bias prompts from a few samples but also captures\nthe distribution of diverse prompts to handle the varying visual\nrepresentations. In this way, we provide high-quality task-related content for\nfacilitating recognition. This prompt distribution learning is realized by an\nefficient approach that learns the output embeddings of prompts instead of the\ninput embeddings. Thus, we can employ a Gaussian distribution to model them\neffectively and derive a surrogate loss for efficient training. Extensive\nexperiments on 12 datasets demonstrate that our method consistently and\nsignificantly outperforms existing methods. For example, with 1 sample per\ncategory, it relatively improves the average result by 9.1% compared to\nhuman-crafted prompts.",
        "pdf_link": "https://arxiv.org/pdf/2205.03340v1.pdf"
    },
    {
        "title": "RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour",
        "authors": [
            "Erick Mendez Guzman",
            "Viktor Schlegel",
            "Riza Batista-Navarro"
        ],
        "published": "2022-05-05T14:43:31Z",
        "summary": "Forced labour is the most common type of modern slavery, and it is\nincreasingly gaining the attention of the research and social community. Recent\nstudies suggest that artificial intelligence (AI) holds immense potential for\naugmenting anti-slavery action. However, AI tools need to be developed\ntransparently in cooperation with different stakeholders. Such tools are\ncontingent on the availability and access to domain-specific data, which are\nscarce due to the near-invisible nature of forced labour. To the best of our\nknowledge, this paper presents the first openly accessible English corpus\nannotated for multi-class and multi-label forced labour detection. The corpus\nconsists of 989 news articles retrieved from specialised data sources and\nannotated according to risk indicators defined by the International Labour\nOrganization (ILO). Each news article was annotated for two aspects: (1)\nindicators of forced labour as classification labels and (2) snippets of the\ntext that justify labelling decisions. We hope that our data set can help\npromote research on explainability for multi-class and multi-label text\nclassification. In this work, we explain our process for collecting the data\nunderpinning the proposed corpus, describe our annotation guidelines and\npresent some statistical analysis of its content. Finally, we summarise the\nresults of baseline experiments based on different variants of the\nBidirectional Encoder Representation from Transformer (BERT) model.",
        "pdf_link": "https://arxiv.org/pdf/2205.02684v1.pdf"
    },
    {
        "title": "Language Models Can See: Plugging Visual Controls in Text Generation",
        "authors": [
            "Yixuan Su",
            "Tian Lan",
            "Yahui Liu",
            "Fangyu Liu",
            "Dani Yogatama",
            "Yan Wang",
            "Lingpeng Kong",
            "Nigel Collier"
        ],
        "published": "2022-05-05T13:56:18Z",
        "summary": "Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.",
        "pdf_link": "https://arxiv.org/pdf/2205.02655v2.pdf"
    },
    {
        "title": "Exploiting Global and Local Hierarchies for Hierarchical Text Classification",
        "authors": [
            "Ting Jiang",
            "Deqing Wang",
            "Leilei Sun",
            "Zhongzhi Chen",
            "Fuzhen Zhuang",
            "Qinghong Yang"
        ],
        "published": "2022-05-05T12:48:41Z",
        "summary": "Hierarchical text classification aims to leverage label hierarchy in\nmulti-label text classification. Existing methods encode label hierarchy in a\nglobal view, where label hierarchy is treated as the static hierarchical\nstructure containing all labels. Since global hierarchy is static and\nirrelevant to text samples, it makes these methods hard to exploit hierarchical\ninformation. Contrary to global hierarchy, local hierarchy as a structured\nlabels hierarchy corresponding to each text sample. It is dynamic and relevant\nto text samples, which is ignored in previous methods. To exploit global and\nlocal hierarchies,we propose Hierarchy-guided BERT with Global and Local\nhierarchies (HBGL), which utilizes the large-scale parameters and prior\nlanguage knowledge of BERT to model both global and local\nhierarchies.Moreover,HBGL avoids the intentional fusion of semantic and\nhierarchical modules by directly modeling semantic and hierarchical information\nwith BERT.Compared with the state-of-the-art method HGCLR,our method achieves\nsignificant improvement on three benchmark datasets.",
        "pdf_link": "https://arxiv.org/pdf/2205.02613v3.pdf"
    },
    {
        "title": "FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework",
        "authors": [
            "Guozheng Li",
            "Xu Chen",
            "Peng Wang",
            "Jiafeng Xie",
            "Qiqing Luo"
        ],
        "published": "2022-05-05T07:59:51Z",
        "summary": "Recent work for extracting relations from texts has achieved excellent\nperformance. However, most existing methods pay less attention to the\nefficiency, making it still challenging to quickly extract relations from\nmassive or streaming text data in realistic scenarios. The main efficiency\nbottleneck is that these methods use a Transformer-based pre-trained language\nmodel for encoding, which heavily affects the training speed and inference\nspeed. To address this issue, we propose a fast relation extraction model\n(FastRE) based on convolutional encoder and improved cascade binary tagging\nframework. Compared to previous work, FastRE employs several innovations to\nimprove efficiency while also keeping promising performance. Concretely, FastRE\nadopts a novel convolutional encoder architecture combined with dilated\nconvolution, gated unit and residual connection, which significantly reduces\nthe computation cost of training and inference, while maintaining the\nsatisfactory performance. Moreover, to improve the cascade binary tagging\nframework, FastRE first introduces a type-relation mapping mechanism to\naccelerate tagging efficiency and alleviate relation redundancy, and then\nutilizes a position-dependent adaptive thresholding strategy to obtain higher\ntagging accuracy and better model generalization. Experimental results\ndemonstrate that FastRE is well balanced between efficiency and performance,\nand achieves 3-10x training speed, 7-15x inference speed faster, and 1/100\nparameters compared to the state-of-the-art models, while the performance is\nstill competitive.",
        "pdf_link": "https://arxiv.org/pdf/2205.02490v2.pdf"
    },
    {
        "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers",
        "authors": [
            "Ninareh Mehrabi",
            "Ahmad Beirami",
            "Fred Morstatter",
            "Aram Galstyan"
        ],
        "published": "2022-05-05T01:48:39Z",
        "summary": "Warning: this paper contains content that maybe offensive or upsetting.\nRecent research in Natural Language Processing (NLP) has advanced the\ndevelopment of various toxicity detection models with the intention of\nidentifying and mitigating toxic language from existing systems. Despite the\nabundance of research in this area, less attention has been given to\nadversarial attacks that force the system to generate toxic language and the\ndefense against them. Existing work to generate such attacks is either based on\nhuman-generated attacks which is costly and not scalable or, in case of\nautomatic attacks, the attack vector does not conform to human-like language,\nwhich can be detected using a language model loss. In this work, we propose\nattacks against conversational agents that are imperceptible, i.e., they fit\nthe conversation in terms of coherency, relevancy, and fluency, while they are\neffective and scalable, i.e., they can automatically trigger the system into\ngenerating toxic language. We then propose a defense mechanism against such\nattacks which not only mitigates the attack but also attempts to maintain the\nconversational flow. Through automatic and human evaluations, we show that our\ndefense is effective at avoiding toxic language generation even against\nimperceptible toxicity triggers while the generated language fits the\nconversation in terms of coherency and relevancy. Lastly, we establish the\ngeneralizability of such a defense mechanism on language generation models\nbeyond conversational agents.",
        "pdf_link": "https://arxiv.org/pdf/2205.02392v1.pdf"
    },
    {
        "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
        "authors": [
            "Ryan Smith",
            "Jason A. Fries",
            "Braden Hancock",
            "Stephen H. Bach"
        ],
        "published": "2022-05-04T20:42:40Z",
        "summary": "We propose a new strategy for applying large pre-trained language models to\nnovel tasks when labeled training data is limited. Rather than apply the model\nin a typical zero-shot or few-shot fashion, we treat the model as the basis for\nlabeling functions in a weak supervision framework. To create a classifier, we\nfirst prompt the model to answer multiple distinct queries about an example and\ndefine how the possible responses should be mapped to votes for labels and\nabstentions. We then denoise these noisy label sources using the Snorkel system\nand train an end classifier with the resulting training data. Our experimental\nevaluation shows that prompting large language models within a weak supervision\nframework can provide significant gains in accuracy. On the WRENCH weak\nsupervision benchmark, this approach can significantly improve over zero-shot\nperformance, an average 19.5% reduction in errors. We also find that this\napproach produces classifiers with comparable or superior accuracy to those\ntrained from hand-engineered rules.",
        "pdf_link": "https://arxiv.org/pdf/2205.02318v1.pdf"
    },
    {
        "title": "Using virtual edges to extract keywords from texts modeled as complex networks",
        "authors": [
            "Jorge A. V. Tohalino",
            "Thiago C. Silva",
            "Diego R. Amancio"
        ],
        "published": "2022-05-04T16:43:03Z",
        "summary": "Detecting keywords in texts is important for many text mining applications.\nGraph-based methods have been commonly used to automatically find the key\nconcepts in texts, however, relevant information provided by embeddings has not\nbeen widely used to enrich the graph structure. Here we modeled texts\nco-occurrence networks, where nodes are words and edges are established either\nby contextual or semantical similarity. We compared two embedding approaches --\nWord2vec and BERT -- to check whether edges created via word embeddings can\nimprove the quality of the keyword extraction method. We found that, in fact,\nthe use of virtual edges can improve the discriminability of co-occurrence\nnetworks. The best performance was obtained when we considered low percentages\nof addition of virtual (embedding) edges. A comparative analysis of structural\nand dynamical network metrics revealed the degree, PageRank, and accessibility\nare the metrics displaying the best performance in the model enriched with\nvirtual edges.",
        "pdf_link": "https://arxiv.org/pdf/2205.02172v1.pdf"
    },
    {
        "title": "Provably Confidential Language Modelling",
        "authors": [
            "Xuandong Zhao",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "published": "2022-05-04T02:33:45Z",
        "summary": "Large language models are shown to memorize privacy information such as\nsocial security numbers in training data. Given the sheer scale of the training\ncorpus, it is challenging to screen and filter these privacy data, either\nmanually or automatically. In this paper, we propose Confidentially Redacted\nTraining (CRT), a method to train language generation models while protecting\nthe confidential segments. We borrow ideas from differential privacy (which\nsolves a related but distinct problem) and show that our method is able to\nprovably prevent unintended memorization by randomizing parts of the training\nprocess. Moreover, we show that redaction with an approximately correct\nscreening policy amplifies the confidentiality guarantee. We implement the\nmethod for both LSTM and GPT language models. Our experimental results show\nthat the models trained by CRT obtain almost the same perplexity while\npreserving strong confidentiality.",
        "pdf_link": "https://arxiv.org/pdf/2205.01863v2.pdf"
    },
    {
        "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology",
        "authors": [
            "Yacine Jernite",
            "Huu Nguyen",
            "Stella Biderman",
            "Anna Rogers",
            "Maraim Masoud",
            "Valentin Danchev",
            "Samson Tan",
            "Alexandra Sasha Luccioni",
            "Nishant Subramani",
            "G\u00e9rard Dupont",
            "Jesse Dodge",
            "Kyle Lo",
            "Zeerak Talat",
            "Isaac Johnson",
            "Dragomir Radev",
            "Somaieh Nikpoor",
            "J\u00f6rg Frohberg",
            "Aaron Gokaslan",
            "Peter Henderson",
            "Rishi Bommasani",
            "Margaret Mitchell"
        ],
        "published": "2022-05-04T00:44:35Z",
        "summary": "The recent emergence and adoption of Machine Learning technology, and\nspecifically of Large Language Models, has drawn attention to the need for\nsystematic and transparent management of language data. This work proposes an\napproach to global language data governance that attempts to organize data\nmanagement amongst stakeholders, values, and rights. Our proposal is informed\nby prior work on distributed governance that accounts for human values and\ngrounded by an international research collaboration that brings together\nresearchers and practitioners from 60 countries. The framework we present is a\nmulti-party international governance structure focused on language data, and\nincorporating technical and organizational tools needed to support its work.",
        "pdf_link": "https://arxiv.org/pdf/2206.03216v2.pdf"
    },
    {
        "title": "Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency",
        "authors": [
            "I\u00f1igo L\u00f3pez-Riob\u00f3o Botana",
            "Ver\u00f3nica Bol\u00f3n-Canedo",
            "Bertha Guijarro-Berdi\u00f1as",
            "Amparo Alonso-Betanzos"
        ],
        "published": "2022-05-03T20:04:32Z",
        "summary": "There are many contexts in which dyadic data are present. Social networks are\na well-known example. In these contexts, pairs of elements are linked building\na network that reflects interactions. Explaining why these relationships are\nestablished is essential to obtain transparency, an increasingly important\nnotion. These explanations are often presented using text, thanks to the spread\nof the natural language understanding tasks. Our aim is to represent and\nexplain pairs established by any agent (e.g., a recommender system or a paid\npromotion mechanism), so that text-based personalisation is taken into account.\nWe have focused on the TripAdvisor platform, considering the applicability to\nother dyadic data contexts. The items are a subset of users and restaurants and\nthe interactions the reviews posted by these users. We propose the PTER\n(Personalised TExt-based Reviews) model. We predict, from the available reviews\nfor a given restaurant, those that fit to the specific user interactions. PTER\nleverages the BERT (Bidirectional Encoders Representations from Transformers)\ntransformer-encoder model. We customised a deep neural network following the\nfeature-based approach, presenting a LTR (Learning To Rank) downstream task. We\ncarried out several comparisons of our proposal with a random baseline and\nother models of the state of the art, following the EXTRA (EXplanaTion RAnking)\nbenchmark. Our method outperforms other collaborative filtering proposals.",
        "pdf_link": "https://arxiv.org/pdf/2205.01759v2.pdf"
    },
    {
        "title": "Mixed-effects transformers for hierarchical adaptation",
        "authors": [
            "Julia White",
            "Noah Goodman",
            "Robert Hawkins"
        ],
        "published": "2022-05-03T19:34:15Z",
        "summary": "Language use differs dramatically from context to context. To some degree,\nmodern language models like GPT-3 are able to account for such variance by\nconditioning on a string of previous input text, or prompt. Yet prompting is\nineffective when contexts are sparse, out-of-sample, or extra-textual; for\ninstance, accounting for when and where the text was produced or who produced\nit. In this paper, we introduce the mixed-effects transformer (MET), a novel\napproach for learning hierarchically-structured prefixes -- lightweight modules\nprepended to the input -- to account for structured variation. Specifically, we\nshow how the popular class of mixed-effects models may be extended to\ntransformer-based architectures using a regularized prefix-tuning procedure\nwith dropout. We evaluate this approach on several domain-adaptation\nbenchmarks, finding that it efficiently adapts to novel contexts with minimal\ndata while still effectively generalizing to unseen contexts.",
        "pdf_link": "https://arxiv.org/pdf/2205.01749v2.pdf"
    },
    {
        "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training",
        "authors": [
            "Mingda Chen",
            "Jingfei Du",
            "Ramakanth Pasunuru",
            "Todor Mihaylov",
            "Srini Iyer",
            "Veselin Stoyanov",
            "Zornitsa Kozareva"
        ],
        "published": "2022-05-03T18:01:07Z",
        "summary": "Self-supervised pretraining has made few-shot learning possible for many NLP\ntasks. But the pretraining objectives are not typically adapted specifically\nfor in-context few-shot learning. In this paper, we propose to use\nself-supervision in an intermediate training stage between pretraining and\ndownstream few-shot usage with the goal to teach the model to perform\nin-context few shot learning. We propose and evaluate four self-supervised\nobjectives on two benchmarks. We find that the intermediate self-supervision\nstage produces models that outperform strong baselines. Ablation study shows\nthat several factors affect the downstream performance, such as the amount of\ntraining data and the diversity of the self-supervised objectives.\nHuman-annotated cross-task supervision and self-supervision are complementary.\nQualitative analysis suggests that the self-supervised-trained models are\nbetter at following task requirements.",
        "pdf_link": "https://arxiv.org/pdf/2205.01703v2.pdf"
    },
    {
        "title": "Efficient Fine-Tuning of BERT Models on the Edge",
        "authors": [
            "Danilo Vucetic",
            "Mohammadreza Tayaranian",
            "Maryam Ziaeefard",
            "James J. Clark",
            "Brett H. Meyer",
            "Warren J. Gross"
        ],
        "published": "2022-05-03T14:51:53Z",
        "summary": "Resource-constrained devices are increasingly the deployment targets of\nmachine learning applications. Static models, however, do not always suffice\nfor dynamic environments. On-device training of models allows for quick\nadaptability to new scenarios. With the increasing size of deep neural\nnetworks, as noted with the likes of BERT and other natural language processing\nmodels, comes increased resource requirements, namely memory, computation,\nenergy, and time. Furthermore, training is far more resource intensive than\ninference. Resource-constrained on-device learning is thus doubly difficult,\nespecially with large BERT-like models. By reducing the memory usage of\nfine-tuning, pre-trained BERT models can become efficient enough to fine-tune\non resource-constrained devices. We propose Freeze And Reconfigure (FAR), a\nmemory-efficient training regime for BERT-like models that reduces the memory\nusage of activation maps during fine-tuning by avoiding unnecessary parameter\nupdates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset\nby 30%, and time spent on memory operations by 47%. More broadly, reductions in\nmetric performance on the GLUE and SQuAD datasets are around 1% on average.",
        "pdf_link": "https://arxiv.org/pdf/2205.01541v1.pdf"
    },
    {
        "title": "Finding patterns in Knowledge Attribution for Transformers",
        "authors": [
            "Jeevesh Juneja",
            "Ritu Agarwal"
        ],
        "published": "2022-05-03T08:30:51Z",
        "summary": "We analyze the Knowledge Neurons framework for the attribution of factual and\nrelational knowledge to particular neurons in the transformer network. We use a\n12-layer multi-lingual BERT model for our experiments. Our study reveals\nvarious interesting phenomena. We observe that mostly factual knowledge can be\nattributed to middle and higher layers of the network($\\ge 6$). Further\nanalysis reveals that the middle layers($6-9$) are mostly responsible for\nrelational information, which is further refined into actual factual knowledge\nor the \"correct answer\" in the last few layers($10-12$). Our experiments also\nshow that the model handles prompts in different languages, but representing\nthe same fact, similarly, providing further evidence for effectiveness of\nmulti-lingual pre-training. Applying the attribution scheme for grammatical\nknowledge, we find that grammatical knowledge is far more dispersed among the\nneurons than factual knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2205.01366v2.pdf"
    },
    {
        "title": "Predicting Issue Types with seBERT",
        "authors": [
            "Alexander Trautsch",
            "Steffen Herbold"
        ],
        "published": "2022-05-03T06:47:13Z",
        "summary": "Pre-trained transformer models are the current state-of-the-art for natural\nlanguage models processing. seBERT is such a model, that was developed based on\nthe BERT architecture, but trained from scratch with software engineering data.\nWe fine-tuned this model for the NLBSE challenge for the task of issue type\nprediction. Our model dominates the baseline fastText for all three issue types\nin both recall and precisio} to achieve an overall F1-score of 85.7%, which is\nan increase of 4.1% over the baseline.",
        "pdf_link": "https://arxiv.org/pdf/2205.01335v1.pdf"
    },
    {
        "title": "Contrastive Learning for Prompt-Based Few-Shot Language Learners",
        "authors": [
            "Yiren Jian",
            "Chongyang Gao",
            "Soroush Vosoughi"
        ],
        "published": "2022-05-03T04:56:45Z",
        "summary": "The impressive performance of GPT-3 using natural language prompts and\nin-context learning has inspired work on better fine-tuning of moderately-sized\nmodels under this paradigm. Following this line of work, we present a\ncontrastive learning framework that clusters inputs from the same class for\nbetter generality of models trained with only limited examples. Specifically,\nwe propose a supervised contrastive framework that clusters inputs from the\nsame class under different augmented \"views\" and repel the ones from different\nclasses. We create different \"views\" of an example by appending it with\ndifferent language prompts and contextual demonstrations. Combining a\ncontrastive loss with the standard masked language modeling (MLM) loss in\nprompt-based few-shot learners, the experimental results show that our method\ncan improve over the state-of-the-art methods in a diverse set of 15 language\ntasks. Our framework makes minimal assumptions on the task or the base model,\nand can be applied to many recent methods with little modification. The code\nwill be made available at: https://github.com/yiren-jian/LM-SupCon.",
        "pdf_link": "https://arxiv.org/pdf/2205.01308v1.pdf"
    },
    {
        "title": "Embedding Hallucination for Few-Shot Language Fine-tuning",
        "authors": [
            "Yiren Jian",
            "Chongyang Gao",
            "Soroush Vosoughi"
        ],
        "published": "2022-05-03T04:55:50Z",
        "summary": "Few-shot language learners adapt knowledge from a pre-trained model to\nrecognize novel classes from a few-labeled sentences. In such settings,\nfine-tuning a pre-trained language model can cause severe over-fitting. In this\npaper, we propose an Embedding Hallucination (EmbedHalluc) method, which\ngenerates auxiliary embedding-label pairs to expand the fine-tuning dataset.\nThe hallucinator is trained by playing an adversarial game with the\ndiscriminator, such that the hallucinated embedding is indiscriminative to the\nreal ones in the fine-tuning dataset. By training with the extended dataset,\nthe language learner effectively learns from the diverse hallucinated\nembeddings to overcome the over-fitting issue. Experiments demonstrate that our\nproposed method is effective in a wide range of language tasks, outperforming\ncurrent fine-tuning methods. Further, we show that EmbedHalluc outperforms\nother methods that address this over-fitting problem, such as common data\naugmentation, semi-supervised pseudo-labeling, and regularization. The code\nwill be made available at: https://github.com/yiren-jian/EmbedHalluc.",
        "pdf_link": "https://arxiv.org/pdf/2205.01307v1.pdf"
    },
    {
        "title": "SemAttack: Natural Textual Attacks via Different Semantic Spaces",
        "authors": [
            "Boxin Wang",
            "Chejian Xu",
            "Xiangyu Liu",
            "Yu Cheng",
            "Bo Li"
        ],
        "published": "2022-05-03T03:44:03Z",
        "summary": "Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.",
        "pdf_link": "https://arxiv.org/pdf/2205.01287v3.pdf"
    },
    {
        "title": "OPT: Open Pre-trained Transformer Language Models",
        "authors": [
            "Susan Zhang",
            "Stephen Roller",
            "Naman Goyal",
            "Mikel Artetxe",
            "Moya Chen",
            "Shuohui Chen",
            "Christopher Dewan",
            "Mona Diab",
            "Xian Li",
            "Xi Victoria Lin",
            "Todor Mihaylov",
            "Myle Ott",
            "Sam Shleifer",
            "Kurt Shuster",
            "Daniel Simig",
            "Punit Singh Koura",
            "Anjali Sridhar",
            "Tianlu Wang",
            "Luke Zettlemoyer"
        ],
        "published": "2022-05-02T17:49:50Z",
        "summary": "Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2205.01068v4.pdf"
    },
    {
        "title": "BERTops: Studying BERT Representations under a Topological Lens",
        "authors": [
            "Jatin Chauhan",
            "Manohar Kaul"
        ],
        "published": "2022-05-02T14:56:17Z",
        "summary": "Proposing scoring functions to effectively understand, analyze and learn\nvarious properties of high dimensional hidden representations of large-scale\ntransformer models like BERT can be a challenging task. In this work, we\nexplore a new direction by studying the topological features of BERT hidden\nrepresentations using persistent homology (PH). We propose a novel scoring\nfunction named \"persistence scoring function (PSF)\" which: (i) accurately\ncaptures the homology of the high-dimensional hidden representations and\ncorrelates well with the test set accuracy of a wide range of datasets and\noutperforms existing scoring metrics, (ii) captures interesting post\nfine-tuning \"per-class\" level properties from both qualitative and quantitative\nviewpoints, (iii) is more stable to perturbations as compared to the baseline\nfunctions, which makes it a very robust proxy, and (iv) finally, also serves as\na predictor of the attack success rates for a wide category of black-box and\nwhite-box adversarial attack methods. Our extensive correlation experiments\ndemonstrate the practical utility of PSF on various NLP tasks relevant to BERT.",
        "pdf_link": "https://arxiv.org/pdf/2205.00953v2.pdf"
    },
    {
        "title": "CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning",
        "authors": [
            "Chenyu Sun",
            "Hangwei Qian",
            "Chunyan Miao"
        ],
        "published": "2022-05-02T14:42:05Z",
        "summary": "In reinforcement learning (RL), it is challenging to learn directly from\nhigh-dimensional observations, where data augmentation has recently been shown\nto remedy this via encoding invariances from raw pixels. Nevertheless, we\nempirically find that not all samples are equally important and hence simply\ninjecting more augmented inputs may instead cause instability in Q-learning. In\nthis paper, we approach this problem systematically by developing a\nmodel-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which\ncan fully exploit sample importance and improve learning efficiency in a\nself-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF\nis capable of prioritizing the experience replay, selecting the most\ninformative augmented inputs, and more importantly regularizing the Q-function\nas well as the encoder to concentrate more on under-learned data. Moreover, it\nencourages the agent to explore with a curiosity-based reward. As a result, the\nagent can focus on more informative samples and learn representation\ninvariances more efficiently, with significantly reduced augmented inputs. We\napply CCLF to several base RL algorithms and evaluate on the DeepMind Control\nSuite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior\nsample efficiency and learning performances compared with other\nstate-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2205.00943v2.pdf"
    },
    {
        "title": "Entity-aware Transformers for Entity Search",
        "authors": [
            "Emma J. Gerritse",
            "Faegheh Hasibi",
            "Arjen P. de Vries"
        ],
        "published": "2022-05-02T11:53:59Z",
        "summary": "Pre-trained language models such as BERT have been a key ingredient to\nachieve state-of-the-art results on a variety of tasks in natural language\nprocessing and, more recently, also in information retrieval.Recent research\neven claims that BERT is able to capture factual knowledge about entity\nrelations and properties, the information that is commonly obtained from\nknowledge graphs. This paper investigates the following question: Do BERT-based\nentity retrieval models benefit from additional entity information stored in\nknowledge graphs? To address this research question, we map entity embeddings\ninto the same input space as a pre-trained BERT model and inject these entity\nembeddings into the BERT model. This entity-enriched language model is then\nemployed on the entity retrieval task. We show that the entity-enriched BERT\nmodel improves effectiveness on entity-oriented queries over a regular BERT\nmodel, establishing a new state-of-the-art result for the entity retrieval\ntask, with substantial improvements for complex natural language queries and\nqueries requesting a list of entities with a certain property. Additionally, we\nshow that the entity information provided by our entity-enriched model\nparticularly helps queries related to less popular entities. Last, we observe\nempirically that the entity-enriched BERT models enable fine-tuning on limited\ntraining data, which otherwise would not be feasible due to the known\ninstabilities of BERT in few-sample fine-tuning, thereby contributing to\ndata-efficient training of BERT for entity search.",
        "pdf_link": "https://arxiv.org/pdf/2205.00820v1.pdf"
    },
    {
        "title": "Seeding Diversity into AI Art",
        "authors": [
            "Marvin Zammit",
            "Antonios Liapis",
            "Georgios N. Yannakakis"
        ],
        "published": "2022-05-02T10:40:52Z",
        "summary": "This paper argues that generative art driven by conformance to a visual\nand/or semantic corpus lacks the necessary criteria to be considered creative.\nAmong several issues identified in the literature, we focus on the fact that\ngenerative adversarial networks (GANs) that create a single image, in a vacuum,\nlack a concept of novelty regarding how their product differs from previously\ncreated ones. We envision that an algorithm that combines the novelty\npreservation mechanisms in evolutionary algorithms with the power of GANs can\ndeliberately guide its creative process towards output that is both good and\nnovel. In this paper, we use recent advances in image generation based on\nsemantic prompts using OpenAI's CLIP model, interrupting the GAN's iterative\nprocess with short cycles of evolutionary divergent search. The results of\nevolution are then used to continue the GAN's iterative process; we hypothesise\nthat this intervention will lead to more novel outputs. Testing our hypothesis\nusing novelty search with local competition, a quality-diversity evolutionary\nalgorithm that can increase visual diversity while maintaining quality in the\nform of adherence to the semantic prompt, we explore how different notions of\nvisual diversity can affect both the process and the product of the algorithm.\nResults show that even a simplistic measure of visual diversity can help\ncounter a drift towards similar images caused by the GAN. This first experiment\nopens a new direction for introducing higher intentionality and a more nuanced\ndrive for GANs.",
        "pdf_link": "https://arxiv.org/pdf/2205.00804v1.pdf"
    },
    {
        "title": "Improving Students' Academic Performance with AI and Semantic Technologies",
        "authors": [
            "Yixin Cheng"
        ],
        "published": "2022-05-02T06:11:24Z",
        "summary": "Artificial intelligence and semantic technologies are evolving and have been\napplied in various research areas, including the education domain. Higher\nEducation institutions strive to improve students' academic performance. Early\nintervention to at-risk students and a reasonable curriculum is vital for\nstudents' success. Prior research opted for deploying traditional machine\nlearning models to predict students' performance. In terms of curriculum\nsemantic analysis, after conducting a comprehensive systematic review regarding\nthe use of semantic technologies in the Computer Science curriculum, a major\nfinding of the study is that technologies used to measure similarity have\nlimitations in terms of accuracy and ambiguity in the representation of\nconcepts, courses, etc. To fill these gaps, in this study, three\nimplementations were developed, that is, to predict students' performance using\nmarks from the previous semester, to model a course representation in a\nsemantic way and compute the similarity, and to identify the prerequisite\nbetween two similar courses. Regarding performance prediction, we used the\ncombination of Genetic Algorithm and Long-Short Term Memory (LSTM) on a dataset\nfrom a Brazilian university containing 248730 records. As for similarity\nmeasurement, we deployed BERT to encode the sentences and used cosine\nsimilarity to obtain the distance between courses. With respect to prerequisite\nidentification, TextRazor was applied to extract concepts from course\ndescription, followed by employing SemRefD to measure the degree of\nprerequisite between two concepts. The outcomes of this study can be summarized\nas: (i) a breakthrough result improves Manrique's work by 2.5% in terms of\naccuracy in dropout prediction; (ii) uncover the similarity between courses\nbased on course description; (iii) identify the prerequisite over three\ncompulsory courses of School of Computing at ANU.",
        "pdf_link": "https://arxiv.org/pdf/2206.03213v2.pdf"
    },
    {
        "title": "Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection",
        "authors": [
            "Angelica Chen",
            "Vicky Zayats",
            "Daniel D. Walker",
            "Dirk Padfield"
        ],
        "published": "2022-05-02T02:13:24Z",
        "summary": "In modern interactive speech-based systems, speech is consumed and\ntranscribed incrementally prior to having disfluencies removed. This\npost-processing step is crucial for producing clean transcripts and high\nperformance on downstream tasks (e.g. machine translation). However, most\ncurrent state-of-the-art NLP models such as the Transformer operate\nnon-incrementally, potentially causing unacceptable delays. We propose a\nstreaming BERT-based sequence tagging model that, combined with a novel\ntraining objective, is capable of detecting disfluencies in real-time while\nbalancing accuracy and latency. This is accomplished by training the model to\ndecide whether to immediately output a prediction for the current input or to\nwait for further context. Essentially, the model learns to dynamically size its\nlookahead window. Our results demonstrate that our model produces comparably\naccurate predictions and does so sooner than our baselines, with lower flicker.\nFurthermore, the model attains state-of-the-art latency and stability scores\nwhen compared with recent work on incremental disfluency detection.",
        "pdf_link": "https://arxiv.org/pdf/2205.00620v1.pdf"
    },
    {
        "title": "Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning",
        "authors": [
            "Angelo Ziletti",
            "Alan Akbik",
            "Christoph Berns",
            "Thomas Herold",
            "Marion Legler",
            "Martina Viell"
        ],
        "published": "2022-05-01T22:49:28Z",
        "summary": "Medical coding (MC) is an essential pre-requisite for reliable data retrieval\nand reporting. Given a free-text reported term (RT) such as \"pain of right\nthigh to the knee\", the task is to identify the matching lowest-level term\n(LLT) - in this case \"unilateral leg pain\" - from a very large and continuously\ngrowing repository of standardized medical terms. However, automating this task\nis challenging due to a large number of LLT codes (as of writing over 80,000),\nlimited availability of training data for long tail/emerging classes, and the\ngeneral high accuracy demands of the medical domain. With this paper, we\nintroduce the MC task, discuss its challenges, and present a novel approach\ncalled xTARS that combines traditional BERT-based classification with a recent\nzero/few-shot learning approach (TARS). We present extensive experiments that\nshow that our combined approach outperforms strong baselines, especially in the\nfew-shot regime. The approach is developed and deployed at Bayer, live since\nNovember 2021. As we believe our approach potentially promising beyond MC, and\nto ensure reproducibility, we release the code to the research community.",
        "pdf_link": "https://arxiv.org/pdf/2206.02662v1.pdf"
    },
    {
        "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression",
        "authors": [
            "Ning Wang",
            "Han Liu",
            "Diego Klabjan"
        ],
        "published": "2022-05-01T19:49:15Z",
        "summary": "We develop an abstractive summarization framework independent of labeled data\nfor multiple heterogeneous documents. Unlike existing multi-document\nsummarization methods, our framework processes documents telling different\nstories instead of documents on the same topic. We also enhance an existing\nsentence fusion method with a uni-directional language model to prioritize\nfused sentences with higher sentence probability with the goal of increasing\nreadability. Lastly, we construct a total of twelve dataset variations based on\nCNN/Daily Mail and the NewsRoom datasets, where each document group contains a\nlarge and diverse collection of documents to evaluate the performance of our\nmodel in comparison with other baseline systems. Our experiments demonstrate\nthat our framework outperforms current state-of-the-art methods in this more\ngeneric setting.",
        "pdf_link": "https://arxiv.org/pdf/2205.00548v1.pdf"
    },
    {
        "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",
        "authors": [
            "Ehud Karpas",
            "Omri Abend",
            "Yonatan Belinkov",
            "Barak Lenz",
            "Opher Lieber",
            "Nir Ratner",
            "Yoav Shoham",
            "Hofit Bata",
            "Yoav Levine",
            "Kevin Leyton-Brown",
            "Dor Muhlgay",
            "Noam Rozen",
            "Erez Schwartz",
            "Gal Shachaf",
            "Shai Shalev-Shwartz",
            "Amnon Shashua",
            "Moshe Tenenholtz"
        ],
        "published": "2022-05-01T11:01:28Z",
        "summary": "Huge language models (LMs) have ushered in a new era for AI, serving as a\ngateway to natural-language-based knowledge tasks. Although an essential\nelement of modern AI, LMs are also inherently limited in a number of ways. We\ndiscuss these limitations and how they can be avoided by adopting a systems\napproach. Conceptualizing the challenge as one that involves knowledge and\nreasoning in addition to linguistic processing, we define a flexible\narchitecture with multiple neural models, complemented by discrete knowledge\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'\nMRKL system implementation.",
        "pdf_link": "https://arxiv.org/pdf/2205.00445v1.pdf"
    },
    {
        "title": "ELQA: A Corpus of Metalinguistic Questions and Answers about English",
        "authors": [
            "Shabnam Behzad",
            "Keisuke Sakaguchi",
            "Nathan Schneider",
            "Amir Zeldes"
        ],
        "published": "2022-05-01T04:29:50Z",
        "summary": "We present ELQA, a corpus of questions and answers in and about the English\nlanguage. Collected from two online forums, the >70k questions (from English\nlearners and others) cover wide-ranging topics including grammar, meaning,\nfluency, and etymology. The answers include descriptions of general properties\nof English vocabulary and grammar as well as explanations about specific\n(correct and incorrect) usage examples. Unlike most NLP datasets, this corpus\nis metalinguistic -- it consists of language about language. As such, it can\nfacilitate investigations of the metalinguistic capabilities of NLU models, as\nwell as educational applications in the language learning domain. To study\nthis, we define a free-form question answering task on our dataset and conduct\nevaluations on multiple LLMs (Large Language Models) to analyze their capacity\nto generate metalinguistic answers.",
        "pdf_link": "https://arxiv.org/pdf/2205.00395v2.pdf"
    },
    {
        "title": "Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF",
        "authors": [
            "Haoming Guo",
            "Tianyi Huang",
            "Huixuan Huang",
            "Mingyue Fan",
            "Gerald Friedland"
        ],
        "published": "2022-05-01T01:48:48Z",
        "summary": "The sharing of fake news and conspiracy theories on social media has\nwide-spread negative effects. By designing and applying different machine\nlearning models, researchers have made progress in detecting fake news from\ntext. However, existing research places a heavy emphasis on general,\ncommon-sense fake news, while in reality fake news often involves rapidly\nchanging topics and domain-specific vocabulary. In this paper, we present our\nmethods and results for three fake news detection tasks at MediaEval benchmark\n2021 that specifically involve COVID-19 related topics. We experiment with a\ngroup of text-based models including Support Vector Machines, Random Forest,\nBERT, and RoBERTa. We find that a pre-trained transformer yields the best\nvalidation results, but a randomly initialized transformer with smart design\ncan also be trained to reach accuracies close to that of the pre-trained\ntransformer.",
        "pdf_link": "https://arxiv.org/pdf/2205.00377v1.pdf"
    },
    {
        "title": "LayoutBERT: Masked Language Layout Model for Object Insertion",
        "authors": [
            "Kerem Turgutlu",
            "Sanat Sharma",
            "Jayant Kumar"
        ],
        "published": "2022-04-30T21:35:38Z",
        "summary": "Image compositing is one of the most fundamental steps in creative workflows.\nIt involves taking objects/parts of several images to create a new image,\ncalled a composite. Currently, this process is done manually by creating\naccurate masks of objects to be inserted and carefully blending them with the\ntarget scene or images, usually with the help of tools such as Photoshop or\nGIMP. While there have been several works on automatic selection of objects for\ncreating masks, the problem of object placement within an image with the\ncorrect position, scale, and harmony remains a difficult problem with limited\nexploration. Automatic object insertion in images or designs is a difficult\nproblem as it requires understanding of the scene geometry and the color\nharmony between objects. We propose LayoutBERT for the object insertion task.\nIt uses a novel self-supervised masked language model objective and\nbidirectional multi-head self-attention. It outperforms previous layout-based\nlikelihood models and shows favorable properties in terms of model capacity. We\ndemonstrate the effectiveness of our approach for object insertion in the image\ncompositing setting and other settings like documents and design templates. We\nfurther demonstrate the usefulness of the learned representations for\nlayout-based retrieval tasks. We provide both qualitative and quantitative\nevaluations on datasets from diverse domains like COCO, PublayNet, and two new\ndatasets which we call Image Layouts and Template Layouts. Image Layouts which\nconsists of 5.8 million images with layout annotations is the largest image\nlayout dataset to our knowledge. We also share ablation study results on the\neffect of dataset size, model size and class sample size for this task.",
        "pdf_link": "https://arxiv.org/pdf/2205.00347v1.pdf"
    },
    {
        "title": "Engineering flexible machine learning systems by traversing functionally-invariant paths",
        "authors": [
            "Guruprasad Raghavan",
            "Bahey Tharwat",
            "Surya Narayanan Hari",
            "Dhruvil Satani",
            "Matt Thomson"
        ],
        "published": "2022-04-30T19:44:56Z",
        "summary": "Transformers have emerged as the state of the art neural network architecture\nfor natural language processing and computer vision. In the foundation model\nparadigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained\non self-supervised tasks such as word or image masking, and then, adapted\nthrough fine-tuning for downstream user applications including instruction\nfollowing and Question Answering. While many approaches have been developed for\nmodel fine-tuning including low-rank weight update strategies (eg. LoRA),\nunderlying mathematical principles that enable network adaptation without\nknowledge loss remain poorly understood. Here, we introduce a differential\ngeometry framework, functionally invariant paths (FIP), that provides flexible\nand continuous adaptation of neural networks for a range of machine learning\ngoals and network sparsification objectives. We conceptualize the weight space\nof a neural network as a curved Riemannian manifold equipped with a metric\ntensor whose spectrum defines low rank subspaces in weight space that\naccommodate network adaptation without loss of prior knowledge. We formalize\nadaptation as movement along a geodesic path in weight space while searching\nfor networks that accommodate secondary objectives. With modest computational\nresources, the FIP algorithm achieves comparable to state of the art\nperformance on continual learning and sparsification tasks for language models\n(BERT), vision transformers (ViT, DeIT), and the CNNs. Broadly, we\nconceptualize a neural network as a mathematical object that can be iteratively\ntransformed into distinct configurations by the path-sampling algorithm to\ndefine a sub-manifold of weight space that can be harnessed to achieve user\ngoals.",
        "pdf_link": "https://arxiv.org/pdf/2205.00334v4.pdf"
    },
    {
        "title": "HateCheckHIn: Evaluating Hindi Hate Speech Detection Models",
        "authors": [
            "Mithun Das",
            "Punyajoy Saha",
            "Binny Mathew",
            "Animesh Mukherjee"
        ],
        "published": "2022-04-30T19:09:09Z",
        "summary": "Due to the sheer volume of online hate, the AI and NLP communities have\nstarted building models to detect such hateful content. Recently, multilingual\nhate is a major emerging challenge for automated detection where code-mixing or\nmore than one language have been used for conversation in social media.\nTypically, hate speech detection models are evaluated by measuring their\nperformance on the held-out test data using metrics such as accuracy and\nF1-score. While these metrics are useful, it becomes difficult to identify\nusing them where the model is failing, and how to resolve it. To enable more\ntargeted diagnostic insights of such multilingual hate speech models, we\nintroduce a set of functionalities for the purpose of evaluation. We have been\ninspired to design this kind of functionalities based on real-world\nconversation on social media. Considering Hindi as a base language, we craft\ntest cases for each functionality. We name our evaluation dataset HateCheckHIn.\nTo illustrate the utility of these functionalities , we test state-of-the-art\ntransformer based m-BERT model and the Perspective API.",
        "pdf_link": "https://arxiv.org/pdf/2205.00328v1.pdf"
    },
    {
        "title": "Visualizing and Explaining Language Models",
        "authors": [
            "Adrian M. P. Bra\u015foveanu",
            "R\u0103zvan Andonie"
        ],
        "published": "2022-04-30T17:23:33Z",
        "summary": "During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.",
        "pdf_link": "https://arxiv.org/pdf/2205.10238v1.pdf"
    },
    {
        "title": "StorSeismic: A new paradigm in deep learning for seismic processing",
        "authors": [
            "Randy Harsuko",
            "Tariq Alkhalifah"
        ],
        "published": "2022-04-30T09:55:00Z",
        "summary": "Machine learned tasks on seismic data are often trained sequentially and\nseparately, even though they utilize the same features (i.e. geometrical) of\nthe data. We present StorSeismic, as a framework for seismic data processing,\nwhich consists of neural network pre-training and fine-tuning procedures. We,\nspecifically, utilize a neural network as a preprocessing model to store\nseismic data features of a particular dataset for any downstream tasks. After\npre-training, the resulting model can be utilized later, through a fine-tuning\nprocedure, to perform tasks using limited additional training. Used often in\nNatural Language Processing (NLP) and lately in vision tasks, BERT\n(Bidirectional Encoder Representations from Transformer), a form of a\nTransformer model, provides an optimal platform for this framework. The\nattention mechanism of BERT, applied here on a sequence of traces within the\nshot gather, is able to capture and store key geometrical features of the\nseismic data. We pre-train StorSeismic on field data, along with synthetically\ngenerated ones, in the self-supervised step. Then, we use the labeled synthetic\ndata to fine-tune the pre-trained network in a supervised fashion to perform\nvarious seismic processing tasks, like denoising, velocity estimation, first\narrival picking, and NMO. Finally, the fine-tuned model is used to obtain\nsatisfactory inference results on the field data.",
        "pdf_link": "https://arxiv.org/pdf/2205.00222v1.pdf"
    },
    {
        "title": "Self-Programming Artificial Intelligence Using Code-Generating Language Models",
        "authors": [
            "Alex Sheng",
            "Shankar Padmanabhan"
        ],
        "published": "2022-04-30T05:44:34Z",
        "summary": "Recent progress in large-scale language models has enabled breakthroughs in\npreviously intractable computer programming tasks. Prior work in meta-learning\nand neural architecture search has led to substantial successes across various\ntask domains, spawning myriad approaches for algorithmically optimizing the\ndesign and learning dynamics of deep learning models. At the intersection of\nthese research areas, we implement a code-generating language model with the\nability to modify its own source code. Self-programming AI algorithms have been\nof interest since the dawn of AI itself. Although various theoretical\nformulations of generalized self-programming AI have been posed, no such system\nhas been successfully implemented to date under real-world computational\nconstraints. Applying AI-based code generation to AI itself, we develop and\nexperimentally validate the first practical implementation of a\nself-programming AI system. We empirically show that a self-programming AI\nimplemented using a code generation model can successfully modify its own\nsource code to improve performance and program sub-models to perform auxiliary\ntasks. Our model can self-modify various properties including model\narchitecture, computational capacity, and learning dynamics.",
        "pdf_link": "https://arxiv.org/pdf/2205.00167v2.pdf"
    },
    {
        "title": "To Know by the Company Words Keep and What Else Lies in the Vicinity",
        "authors": [
            "Jake Ryland Williams",
            "Hunter Scott Heidenreich"
        ],
        "published": "2022-04-30T03:47:48Z",
        "summary": "The development of state-of-the-art (SOTA) Natural Language Processing (NLP)\nsystems has steadily been establishing new techniques to absorb the statistics\nof linguistic data. These techniques often trace well-known constructs from\ntraditional theories, and we study these connections to close gaps around key\nNLP methods as a means to orient future work. For this, we introduce an\nanalytic model of the statistics learned by seminal algorithms (including GloVe\nand Word2Vec), and derive insights for systems that use these algorithms and\nthe statistics of co-occurrence, in general. In this work, we derive -- to the\nbest of our knowledge -- the first known solution to Word2Vec's\nsoftmax-optimized, skip-gram algorithm. This result presents exciting potential\nfor future development as a direct solution to a deep learning (DL) language\nmodel's (LM's) matrix factorization. However, we use the solution to\ndemonstrate a seemingly-universal existence of a property that word vectors\nexhibit and which allows for the prophylactic discernment of biases in data --\nprior to their absorption by DL models. To qualify our work, we conduct an\nanalysis of independence, i.e., on the density of statistical dependencies in\nco-occurrence models, which in turn renders insights on the distributional\nhypothesis' partial fulfillment by co-occurrence statistics.",
        "pdf_link": "https://arxiv.org/pdf/2205.00148v1.pdf"
    },
    {
        "title": "Training Naturalized Semantic Parsers with Very Little Data",
        "authors": [
            "Subendhu Rongali",
            "Konstantine Arkoudas",
            "Melanie Rubino",
            "Wael Hamza"
        ],
        "published": "2022-04-29T17:14:54Z",
        "summary": "Semantic parsing is an important NLP problem, particularly for voice\nassistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic\nparsers are seq2seq architectures based on large language models that have been\npretrained on vast amounts of text. To better leverage that pretraining, recent\nwork has explored a reformulation of semantic parsing whereby the output\nsequences are themselves natural language sentences, but in a controlled\nfragment of natural language. This approach delivers strong results,\nparticularly for few-shot semantic parsing, which is of key importance in\npractice and the focus of our paper. We push this line of work forward by\nintroducing an automated methodology that delivers very significant additional\nimprovements by utilizing modest amounts of unannotated data, which is\ntypically easy to obtain. Our method is based on a novel synthesis of four\ntechniques: joint training with auxiliary unsupervised tasks; constrained\ndecoding; self-training; and paraphrasing. We show that this method delivers\nnew SOTA few-shot performance on the Overnight dataset, particularly in very\nlow-resource settings, and very compelling few-shot results on a new semantic\nparsing dataset.",
        "pdf_link": "https://arxiv.org/pdf/2204.14243v2.pdf"
    },
    {
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
        "authors": [
            "Jean-Baptiste Alayrac",
            "Jeff Donahue",
            "Pauline Luc",
            "Antoine Miech",
            "Iain Barr",
            "Yana Hasson",
            "Karel Lenc",
            "Arthur Mensch",
            "Katie Millican",
            "Malcolm Reynolds",
            "Roman Ring",
            "Eliza Rutherford",
            "Serkan Cabi",
            "Tengda Han",
            "Zhitao Gong",
            "Sina Samangooei",
            "Marianne Monteiro",
            "Jacob Menick",
            "Sebastian Borgeaud",
            "Andrew Brock",
            "Aida Nematzadeh",
            "Sahand Sharifzadeh",
            "Mikolaj Binkowski",
            "Ricardo Barreira",
            "Oriol Vinyals",
            "Andrew Zisserman",
            "Karen Simonyan"
        ],
        "published": "2022-04-29T16:29:01Z",
        "summary": "Building models that can be rapidly adapted to novel tasks using only a\nhandful of annotated examples is an open challenge for multimodal machine\nlearning research. We introduce Flamingo, a family of Visual Language Models\n(VLM) with this ability. We propose key architectural innovations to: (i)\nbridge powerful pretrained vision-only and language-only models, (ii) handle\nsequences of arbitrarily interleaved visual and textual data, and (iii)\nseamlessly ingest images or videos as inputs. Thanks to their flexibility,\nFlamingo models can be trained on large-scale multimodal web corpora containing\narbitrarily interleaved text and images, which is key to endow them with\nin-context few-shot learning capabilities. We perform a thorough evaluation of\nour models, exploring and measuring their ability to rapidly adapt to a variety\nof image and video tasks. These include open-ended tasks such as visual\nquestion-answering, where the model is prompted with a question which it has to\nanswer; captioning tasks, which evaluate the ability to describe a scene or an\nevent; and close-ended tasks such as multiple-choice visual question-answering.\nFor tasks lying anywhere on this spectrum, a single Flamingo model can achieve\na new state of the art with few-shot learning, simply by prompting the model\nwith task-specific examples. On numerous benchmarks, Flamingo outperforms\nmodels fine-tuned on thousands of times more task-specific data.",
        "pdf_link": "https://arxiv.org/pdf/2204.14198v2.pdf"
    },
    {
        "title": "Training Language Models with Language Feedback",
        "authors": [
            "J\u00e9r\u00e9my Scheurer",
            "Jon Ander Campos",
            "Jun Shern Chan",
            "Angelica Chen",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2022-04-29T15:06:58Z",
        "summary": "Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level summarization\nability.",
        "pdf_link": "https://arxiv.org/pdf/2204.14146v4.pdf"
    },
    {
        "title": "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining",
        "authors": [
            "Yuting Gao",
            "Jinfeng Liu",
            "Zihan Xu",
            "Jun Zhang",
            "Ke Li",
            "Rongrong Ji",
            "Chunhua Shen"
        ],
        "published": "2022-04-29T13:38:42Z",
        "summary": "Large-scale vision-language pre-training has achieved promising results on\ndownstream tasks. Existing methods highly rely on the assumption that the\nimage-text pairs crawled from the Internet are in perfect one-to-one\ncorrespondence. However, in real scenarios, this assumption can be difficult to\nhold: the text description, obtained by crawling the affiliated metadata of the\nimage, often suffers from the semantic mismatch and the mutual compatibility.\nTo address these issues, we introduce PyramidCLIP, which constructs an input\npyramid with different semantic levels for each modality, and aligns visual\nelements and linguistic elements in the form of hierarchy via peer-level\nsemantics alignment and cross-level relation alignment. Furthermore, we soften\nthe loss of negative samples (unpaired samples) so as to weaken the strict\nconstraint during the pre-training stage, thus mitigating the risk of forcing\nthe model to distinguish compatible negative pairs. Experiments on five\ndownstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In\nparticular, with the same amount of 15 million pre-training image-text pairs,\nPyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by\n10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder\nrespectively. When scaling to larger datasets, PyramidCLIP achieves the\nstate-of-the-art results on several downstream tasks. In particular, the\nresults of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that\nof CLIP using 400M data on ImageNet zero-shot classification task,\nsignificantly improving the data efficiency of CLIP.",
        "pdf_link": "https://arxiv.org/pdf/2204.14095v2.pdf"
    },
    {
        "title": "C3-STISR: Scene Text Image Super-resolution with Triple Clues",
        "authors": [
            "Minyi Zhao",
            "Miao Wang",
            "Fan Bai",
            "Bingjia Li",
            "Jie Wang",
            "Shuigeng Zhou"
        ],
        "published": "2022-04-29T12:39:51Z",
        "summary": "Scene text image super-resolution (STISR) has been regarded as an important\npre-processing task for text recognition from low-resolution scene text images.\nMost recent approaches use the recognizer's feedback as clues to guide\nsuper-resolution. However, directly using recognition clue has two problems: 1)\nCompatibility. It is in the form of probability distribution, has an obvious\nmodal gap with STISR - a pixel-level task; 2) Inaccuracy. it usually contains\nwrong information, thus will mislead the main task and degrade super-resolution\nperformance. In this paper, we present a novel method C3-STISR that jointly\nexploits the recognizer's feedback, visual and linguistical information as\nclues to guide super-resolution. Here, visual clue is from the images of texts\npredicted by the recognizer, which is informative and more compatible with the\nSTISR task; while linguistical clue is generated by a pre-trained\ncharacter-level language model, which is able to correct the predicted texts.\nWe design effective extraction and fusion mechanisms for the triple cross-modal\nclues to generate a comprehensive and unified guidance for super-resolution.\nExtensive experiments on TextZoom show that C3-STISR outperforms the SOTA\nmethods in fidelity and recognition performance. Code is available in\nhttps://github.com/zhaominyiz/C3-STISR.",
        "pdf_link": "https://arxiv.org/pdf/2204.14044v1.pdf"
    },
    {
        "title": "ExaASC: A General Target-Based Stance Detection Corpus in Arabic Language",
        "authors": [
            "Mohammad Mehdi Jaziriyan",
            "Ahmad Akbari",
            "Hamed Karbasi"
        ],
        "published": "2022-04-29T10:03:51Z",
        "summary": "Target-based Stance Detection is the task of finding a stance toward a\ntarget. Twitter is one of the primary sources of political discussions in\nsocial media and one of the best resources to analyze Stance toward entities.\nThis work proposes a new method toward Target-based Stance detection by using\nthe stance of replies toward a most important and arguing target in source\ntweet. This target is detected with respect to the source tweet itself and not\nlimited to a set of pre-defined targets which is the usual approach of the\ncurrent state-of-the-art methods. Our proposed new attitude resulted in a new\ncorpus called ExaASC for the Arabic Language, one of the low resource languages\nin this field. In the end, we used BERT to evaluate our corpus and reached a\n70.69 Macro F-score. This shows that our data and model can work in a general\nTarget-base Stance Detection system. The corpus is publicly available1.",
        "pdf_link": "https://arxiv.org/pdf/2204.13979v1.pdf"
    },
    {
        "title": "QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance",
        "authors": [
            "Xiaoqiang Wang",
            "Bang Liu",
            "Siliang Tang",
            "Lingfei Wu"
        ],
        "published": "2022-04-29T07:39:53Z",
        "summary": "Existing metrics for assessing question generation not only require costly\nhuman reference but also fail to take into account the input context of\ngeneration, rendering the lack of deep understanding of the relevance between\nthe generated questions and input contexts. As a result, they may wrongly\npenalize a legitimate and reasonable candidate question when it (i) involves\ncomplicated reasoning with the context or (ii) can be grounded by multiple\nevidences in the context. In this paper, we propose $\\textbf{QRelScore}$, a\ncontext-aware $\\underline{\\textbf{Rel}}$evance evaluation metric for\n$\\underline{\\textbf{Q}}$uestion Generation. Based on off-the-shelf language\nmodels such as BERT and GPT2, QRelScore employs both word-level hierarchical\nmatching and sentence-level prompt-based generation to cope with the\ncomplicated reasoning and diverse generation from multiple evidences,\nrespectively. Compared with existing metrics, our experiments demonstrate that\nQRelScore is able to achieve a higher correlation with human judgments while\nbeing much more robust to adversarial samples.",
        "pdf_link": "https://arxiv.org/pdf/2204.13921v1.pdf"
    },
    {
        "title": "Czech Dataset for Cross-lingual Subjectivity Classification",
        "authors": [
            "Pavel P\u0159ib\u00e1\u0148",
            "Josef Steinberger"
        ],
        "published": "2022-04-29T07:31:46Z",
        "summary": "In this paper, we introduce a new Czech subjectivity dataset of 10k manually\nannotated subjective and objective sentences from movie reviews and\ndescriptions. Our prime motivation is to provide a reliable dataset that can be\nused with the existing English dataset as a benchmark to test the ability of\npre-trained multilingual models to transfer knowledge between Czech and English\nand vice versa. Two annotators annotated the dataset reaching 0.83 of the\nCohen's \\k{appa} inter-annotator agreement. To the best of our knowledge, this\nis the first subjectivity dataset for the Czech language. We also created an\nadditional dataset that consists of 200k automatically labeled sentences. Both\ndatasets are freely available for research purposes. Furthermore, we fine-tune\nfive pre-trained BERT-like models to set a monolingual baseline for the new\ndataset and we achieve 93.56% of accuracy. We fine-tune models on the existing\nEnglish dataset for which we obtained results that are on par with the current\nstate-of-the-art results. Finally, we perform zero-shot cross-lingual\nsubjectivity classification between Czech and English to verify the usability\nof our dataset as the cross-lingual benchmark. We compare and discuss the\ncross-lingual and monolingual results and the ability of multilingual models to\ntransfer knowledge between languages.",
        "pdf_link": "https://arxiv.org/pdf/2204.13915v1.pdf"
    },
    {
        "title": "OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision",
        "authors": [
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Xian Li",
            "Xin Luna Dong",
            "Jingbo Shang",
            "Christos Faloutsos",
            "Jiawei Han"
        ],
        "published": "2022-04-29T04:16:04Z",
        "summary": "Automatic extraction of product attributes from their textual descriptions is\nessential for online shopper experience. One inherent challenge of this task is\nthe emerging nature of e-commerce products -- we see new types of products with\ntheir unique set of new attributes constantly. Most prior works on this matter\nmine new values for a set of known attributes but cannot handle new attributes\nthat arose from constantly changing data. In this work, we study the attribute\nmining problem in an open-world setting to extract novel attributes and their\nvalues. Instead of providing comprehensive training data, the user only needs\nto provide a few examples for a few known attribute types as weak supervision.\nWe propose a principled framework that first generates attribute value\ncandidates and then groups them into clusters of attributes. The candidate\ngeneration step probes a pre-trained language model to extract phrases from\nproduct titles. Then, an attribute-aware fine-tuning method optimizes a\nmultitask objective and shapes the language model representation to be\nattribute-discriminative. Finally, we discover new attributes and values\nthrough the self-ensemble of our framework, which handles the open-world\nchallenge. We run extensive experiments on a large distantly annotated\ndevelopment set and a gold standard human-annotated test set that we collected.\nOur model significantly outperforms strong baselines and can generalize to\nunseen attributes and product types.",
        "pdf_link": "https://arxiv.org/pdf/2204.13874v1.pdf"
    },
    {
        "title": "Inferring Implicit Relations in Complex Questions with Language Models",
        "authors": [
            "Uri Katz",
            "Mor Geva",
            "Jonathan Berant"
        ],
        "published": "2022-04-28T21:00:54Z",
        "summary": "A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the challenge in\nimplicit reasoning questions does not stem from the need to plan a reasoning\nstrategy alone, but to do it while also retrieving and reasoning over relevant\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2204.13778v2.pdf"
    },
    {
        "title": "Process-BERT: A Framework for Representation Learning on Educational Process Data",
        "authors": [
            "Alexander Scarlatos",
            "Christopher Brinton",
            "Andrew Lan"
        ],
        "published": "2022-04-28T16:07:28Z",
        "summary": "Educational process data, i.e., logs of detailed student activities in\ncomputerized or online learning platforms, has the potential to offer deep\ninsights into how students learn. One can use process data for many downstream\ntasks such as learning outcome prediction and automatically delivering\npersonalized intervention. However, analyzing process data is challenging since\nthe specific format of process data varies a lot depending on different\nlearning/testing scenarios. In this paper, we propose a framework for learning\nrepresentations of educational process data that is applicable across many\ndifferent learning scenarios. Our framework consists of a pre-training step\nthat uses BERT-type objectives to learn representations from sequential process\ndata and a fine-tuning step that further adjusts these representations on\ndownstream prediction tasks. We apply our framework to the 2019 nation's report\ncard data mining competition dataset that consists of student problem-solving\nprocess data and detail the specific models we use in this scenario. We conduct\nboth quantitative and qualitative experiments to show that our framework\nresults in process data representations that are both predictive and\ninformative.",
        "pdf_link": "https://arxiv.org/pdf/2204.13607v1.pdf"
    },
    {
        "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers",
        "authors": [
            "Ming Ding",
            "Wendi Zheng",
            "Wenyi Hong",
            "Jie Tang"
        ],
        "published": "2022-04-28T15:51:11Z",
        "summary": "The development of the transformer-based text-to-image models are impeded by\nits slow generation and complexity for high-resolution images. In this work, we\nput forward a solution based on hierarchical transformers and local parallel\nauto-regressive generation. We pretrain a 6B-parameter transformer with a\nsimple and flexible self-supervised task, Cross-modal general language model\n(CogLM), and finetune it for fast super-resolution. The new text-to-image\nsystem, CogView2, shows very competitive generation compared to concurrent\nstate-of-the-art DALL-E-2, and naturally supports interactive text-guided\nediting on images.",
        "pdf_link": "https://arxiv.org/pdf/2204.14217v2.pdf"
    },
    {
        "title": "RobBERTje: a Distilled Dutch BERT Model",
        "authors": [
            "Pieter Delobelle",
            "Thomas Winters",
            "Bettina Berendt"
        ],
        "published": "2022-04-28T14:02:13Z",
        "summary": "Pre-trained large-scale language models such as BERT have gained a lot of\nattention thanks to their outstanding performance on a wide range of natural\nlanguage tasks. However, due to their large number of parameters, they are\nresource-intensive both to deploy and to fine-tune. Researchers have created\nseveral methods for distilling language models into smaller ones to increase\nefficiency, with a small performance trade-off. In this paper, we create\nseveral different distilled versions of the state-of-the-art Dutch RobBERT\nmodel and call them RobBERTje. The distillations differ in their distillation\ncorpus, namely whether or not they are shuffled and whether they are merged\nwith subsequent sentences. We found that the performance of the models using\nthe shuffled versus non-shuffled datasets is similar for most tasks and that\nrandomly merging subsequent sentences in a corpus creates models that train\nfaster and perform better on tasks with long sequences. Upon comparing\ndistillation architectures, we found that the larger DistilBERT architecture\nworked significantly better than the Bort hyperparametrization. Interestingly,\nwe also found that the distilled models exhibit less gender-stereotypical bias\nthan its teacher model. Since smaller architectures decrease the time to\nfine-tune, these models allow for more efficient training and more lightweight\ndeployment of many Dutch downstream language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.13511v1.pdf"
    },
    {
        "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model",
        "authors": [
            "Seongjin Shin",
            "Sang-Woo Lee",
            "Hwijeen Ahn",
            "Sungdong Kim",
            "HyoungSeok Kim",
            "Boseop Kim",
            "Kyunghyun Cho",
            "Gichang Lee",
            "Woomyoung Park",
            "Jung-Woo Ha",
            "Nako Sung"
        ],
        "published": "2022-04-28T13:59:54Z",
        "summary": "Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.13509v2.pdf"
    },
    {
        "title": "Post-Training Dialogue Summarization using Pseudo-Paraphrasing",
        "authors": [
            "Qi Jia",
            "Yizhu Liu",
            "Haifeng Tang",
            "Kenny Q. Zhu"
        ],
        "published": "2022-04-28T13:42:19Z",
        "summary": "Previous dialogue summarization techniques adapt large language models\npretrained on the narrative text by injecting dialogue-specific features into\nthe models. These features either require additional knowledge to recognize or\nmake the resulting models harder to tune. To bridge the format gap between\ndialogues and narrative summaries in dialogue summarization tasks, we propose\nto post-train pretrained language models (PLMs) to rephrase from dialogue to\nnarratives. After that, the model is fine-tuned for dialogue summarization as\nusual. Comprehensive experiments show that our approach significantly improves\nvanilla PLMs on dialogue summarization and outperforms other SOTA models by the\nsummary quality and implementation costs.",
        "pdf_link": "https://arxiv.org/pdf/2204.13498v1.pdf"
    },
    {
        "title": "A Survey on Sentence Embedding Models Performance for Patent Analysis",
        "authors": [
            "Hamid Bekamiri",
            "Daniel S. Hain",
            "Roman Jurowetzki"
        ],
        "published": "2022-04-28T12:04:42Z",
        "summary": "Patent data is an important source of knowledge for innovation research,\nwhile the technological similarity between pairs of patents is a key enabling\nindicator for patent analysis. Recently researchers have been using patent\nvector space models based on different NLP embeddings models to calculate the\ntechnological similarity between pairs of patents to help better understand\ninnovations, patent landscaping, technology mapping, and patent quality\nevaluation. More often than not, Text Embedding is a vital precursor to patent\nanalysis tasks. A pertinent question then arises: How should we measure and\nevaluate the accuracy of these embeddings? To the best of our knowledge, there\nis no comprehensive survey that builds a clear delineation of embedding models'\nperformance for calculating patent similarity indicators. Therefore, in this\nstudy, we provide an overview of the accuracy of these algorithms based on\npatent classification performance and propose a standard library and dataset\nfor assessing the accuracy of embeddings models based on PatentSBERTa approach.\nIn a detailed discussion, we report the performance of the top 3 algorithms at\nsection, class, and subclass levels. The results based on the first claim of\npatents show that PatentSBERTa, Bert-for-patents, and TF-IDF Weighted Word\nEmbeddings have the best accuracy for computing sentence embeddings at the\nsubclass level. According to the first results, the performance of the models\nin different classes varies, which shows researchers in patent analysis can\nutilize the results of this study to choose the best proper model based on the\nspecific section of patent data they used.",
        "pdf_link": "https://arxiv.org/pdf/2206.02690v3.pdf"
    },
    {
        "title": "HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification",
        "authors": [
            "Zihan Wang",
            "Peiyi Wang",
            "Tianyu Liu",
            "Binghuai Lin",
            "Yunbo Cao",
            "Zhifang Sui",
            "Houfeng Wang"
        ],
        "published": "2022-04-28T11:22:49Z",
        "summary": "Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex label hierarchy. Recently, the\npretrained language models (PLM)have been widely adopted in HTC through a\nfine-tuning paradigm. However, in this paradigm, there exists a huge gap\nbetween the classification tasks with sophisticated label hierarchy and the\nmasked language model (MLM) pretraining tasks of PLMs and thus the potentials\nof PLMs can not be fully tapped. To bridge the gap, in this paper, we propose\nHPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label\nMLM perspective. Specifically, we construct a dynamic virtual template and\nlabel words that take the form of soft prompts to fuse the label hierarchy\nknowledge and introduce a zero-bounded multi-label cross entropy loss to\nharmonize the objectives of HTC and MLM. Extensive experiments show HPT\nachieves state-of-the-art performances on 3 popular HTC datasets and is adept\nat handling the imbalance and low resource situations. Our code is available at\nhttps://github.com/wzh9969/HPT.",
        "pdf_link": "https://arxiv.org/pdf/2204.13413v2.pdf"
    },
    {
        "title": "Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation",
        "authors": [
            "Kexin Yang",
            "Dayiheng Liu",
            "Wenqiang Lei",
            "Baosong Yang",
            "Mingfeng Xue",
            "Boxing Chen",
            "Jun Xie"
        ],
        "published": "2022-04-28T09:09:45Z",
        "summary": "Attribute-based Controlled Text Generation (CTG) refers to generating\nsentences that satisfy desirable attributes (e.g., emotions and topics).\nExisting works often utilize fine-tuning or resort to extra attribute\nclassifiers, yet suffer from storage and inference time increases. To address\nthese concerns, we explore attribute-based CTG in a prompt-based manner. In\nshort, the proposed Tailor represents each attribute as a pre-trained\ncontinuous vector (i.e., single-attribute prompt) and guides the generation of\na fixed PLM switch to a pre-specified attribute. We experimentally find that\nthese prompts can be simply concatenated as a whole to multi-attribute CTG\nwithout any re-training, yet raises problems of fluency decrease and position\nsensitivity. To this end, Tailor provides a multi-attribute prompt mask and a\nre-indexing position-ids sequence to bridge the gap between the training (one\nprompt for each task) and testing stage (concatenating more than one prompt).\nTo further enhance such single-attribute prompt combinations, Tailor also\nintroduces a trainable prompt connector, which can be concatenated with any two\nsingle-attribute prompts to multi-attribute text generation. Experiments on 11\nattribute-specific generation tasks demonstrate strong performances of Tailor\non both single-attribute and multi-attribute CTG, with 0.08\\% training\nparameters of a GPT-2.",
        "pdf_link": "https://arxiv.org/pdf/2204.13362v1.pdf"
    },
    {
        "title": "BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information",
        "authors": [
            "Jiexin Wang",
            "Adam Jatowt",
            "Masatoshi Yoshikawa",
            "Yi Cai"
        ],
        "published": "2022-04-27T16:20:09Z",
        "summary": "Time is an important aspect of documents and is used in a range of NLP and IR\ntasks. In this work, we investigate methods for incorporating temporal\ninformation during pre-training to further improve the performance on\ntime-related tasks. Compared with common pre-trained language models like BERT\nwhich utilize synchronic document collections (e.g., BookCorpus and Wikipedia)\nas the training corpora, we use long-span temporal news article collection for\nbuilding word representations. We introduce BiTimeBERT, a novel language\nrepresentation model trained on a temporal collection of news articles via two\nnew pre-training tasks, which harnesses two distinct temporal signals to\nconstruct time-aware language representations. The experimental results show\nthat BiTimeBERT consistently outperforms BERT and other existing pre-trained\nmodels with substantial gains on different downstream NLP tasks and\napplications for which time is of importance (e.g., the accuracy improvement\nover BERT is 155\\% on the event time estimation task).",
        "pdf_link": "https://arxiv.org/pdf/2204.13032v4.pdf"
    },
    {
        "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation",
        "authors": [
            "Wei Chen",
            "Yeyun Gong",
            "Song Wang",
            "Bolun Yao",
            "Weizhen Qi",
            "Zhongyu Wei",
            "Xiaowu Hu",
            "Bartuer Zhou",
            "Yi Mao",
            "Weizhu Chen",
            "Biao Cheng",
            "Nan Duan"
        ],
        "published": "2022-04-27T16:18:15Z",
        "summary": "Dialog response generation in open domain is an important research topic\nwhere the main challenge is to generate relevant and diverse responses. In this\npaper, we propose a new dialog pre-training framework called DialogVED, which\nintroduces continuous latent variables into the enhanced encoder-decoder\npre-training framework to increase the relevance and diversity of responses.\nWith the help of a large dialog corpus (Reddit), we pre-train the model using\nthe following 4 tasks adopted in language models (LMs) and variational\nautoencoders (VAEs): 1) masked language model; 2) response generation; 3)\nbag-of-words prediction; and 4) KL divergence reduction. We also add additional\nparameters to model the turn structure in dialogs to improve the performance of\nthe pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and\nDSTC7-AVSD benchmarks for response generation. Experimental results show that\nour model achieves the new state-of-the-art results on all these datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.13031v2.pdf"
    },
    {
        "title": "An End-to-End Dialogue Summarization System for Sales Calls",
        "authors": [
            "Abedelkadir Asi",
            "Song Wang",
            "Roy Eisenstadt",
            "Dean Geckt",
            "Yarin Kuper",
            "Yi Mao",
            "Royi Ronen"
        ],
        "published": "2022-04-27T14:02:50Z",
        "summary": "Summarizing sales calls is a routine task performed manually by salespeople.\nWe present a production system which combines generative models fine-tuned for\ncustomer-agent setting, with a human-in-the-loop user experience for an\ninteractive summary curation process. We address challenging aspects of\ndialogue summarization task in a real-world setting including long input\ndialogues, content validation, lack of labeled data and quality evaluation. We\nshow how GPT-3 can be leveraged as an offline data labeler to handle training\ndata scarcity and accommodate privacy constraints in an industrial setting.\nExperiments show significant improvements by our models in tackling the\nsummarization and content validation tasks on public datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.12951v2.pdf"
    },
    {
        "title": "RigoBERTa: A State-of-the-Art Language Model For Spanish",
        "authors": [
            "Alejandro Vaca Serrano",
            "Guillem Garcia Subies",
            "Helena Montoro Zamorano",
            "Nuria Aldama Garcia",
            "Doaa Samy",
            "David Betancur Sanchez",
            "Antonio Moreno Sandoval",
            "Marta Guerrero Nieto",
            "Alvaro Barbero Jimenez"
        ],
        "published": "2022-04-27T11:53:25Z",
        "summary": "This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over a well-curated corpus formed up from different\nsubcorpora with key features. It follows the DeBERTa architecture, which has\nseveral advantages over other architectures of similar size as BERT or RoBERTa.\nRigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.",
        "pdf_link": "https://arxiv.org/pdf/2205.10233v3.pdf"
    },
    {
        "title": "SkillSpan: Hard and Soft Skill Extraction from English Job Postings",
        "authors": [
            "Mike Zhang",
            "Kristian N\u00f8rgaard Jensen",
            "Sif Dam Sonniks",
            "Barbara Plank"
        ],
        "published": "2022-04-27T10:07:36Z",
        "summary": "Skill Extraction (SE) is an important and widely-studied task useful to gain\ninsights into labor market dynamics. However, there is a lacuna of datasets and\nannotation guidelines; available datasets are few and contain crowd-sourced\nlabels on the span-level or labels from a predefined skill inventory. To\naddress this gap, we introduce SKILLSPAN, a novel SE dataset consisting of\n14.5K sentences and over 12.5K annotated spans. We release its respective\nguidelines created over three different sources annotated for hard and soft\nskills by domain experts. We introduce a BERT baseline (Devlin et al., 2019).\nTo improve upon this baseline, we experiment with language models that are\noptimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous\npre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et\nal., 2020), and multi-task learning (Caruana, 1997). Our results show that the\ndomain-adapted models significantly outperform their non-adapted counterparts,\nand single-task outperforms multi-task learning.",
        "pdf_link": "https://arxiv.org/pdf/2204.12811v1.pdf"
    },
    {
        "title": "Probing Simile Knowledge from Pre-trained Language Models",
        "authors": [
            "Weijie Chen",
            "Yongzhu Chang",
            "Rongsheng Zhang",
            "Jiashu Pu",
            "Guandan Chen",
            "Le Zhang",
            "Yadong Xi",
            "Yijiang Chen",
            "Chang Su"
        ],
        "published": "2022-04-27T09:55:40Z",
        "summary": "Simile interpretation (SI) and simile generation (SG) are challenging tasks\nfor NLP because models require adequate world knowledge to produce predictions.\nPrevious works have employed many hand-crafted resources to bring\nknowledge-related into models, which is time-consuming and labor-intensive. In\nrecent years, pre-trained language models (PLMs) based approaches have become\nthe de-facto standard in NLP since they learn generic knowledge from a large\ncorpus. The knowledge embedded in PLMs may be useful for SI and SG tasks.\nNevertheless, there are few works to explore it. In this paper, we probe simile\nknowledge from PLMs to solve the SI and SG tasks in the unified framework of\nsimile triple completion for the first time. The backbone of our framework is\nto construct masked sentences with manual patterns and then predict the\ncandidate words in the masked position. In this framework, we adopt a secondary\ntraining process (Adjective-Noun mask Training) with the masked language model\n(MLM) loss to enhance the prediction diversity of candidate words in the masked\nposition. Moreover, pattern ensemble (PE) and pattern search (PS) are applied\nto improve the quality of predicted words. Finally, automatic and human\nevaluations demonstrate the effectiveness of our framework in both SI and SG\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.12807v1.pdf"
    },
    {
        "title": "Modern Baselines for SPARQL Semantic Parsing",
        "authors": [
            "Debayan Banerjee",
            "Pranav Ajit Nair",
            "Jivat Neet Kaur",
            "Ricardo Usbeck",
            "Chris Biemann"
        ],
        "published": "2022-04-27T09:26:59Z",
        "summary": "In this work, we focus on the task of generating SPARQL queries from natural\nlanguage questions, which can then be executed on Knowledge Graphs (KGs). We\nassume that gold entity and relations have been provided, and the remaining\ntask is to arrange them in the right order along with SPARQL vocabulary, and\ninput tokens to produce the correct SPARQL query. Pre-trained Language Models\n(PLMs) have not been explored in depth on this task so far, so we experiment\nwith BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,\nlooking for new baselines in the PLM era for this task, on DBpedia and Wikidata\nKGs. We show that T5 requires special input tokenisation, but produces state of\nthe art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms\ntask-specific models from previous works. Moreover, the methods enable semantic\nparsing for questions where a part of the input needs to be copied to the\noutput query, thus enabling a new paradigm in KG semantic parsing.",
        "pdf_link": "https://arxiv.org/pdf/2204.12793v3.pdf"
    },
    {
        "title": "Multimodal Transformer-based Model for Buchwald-Hartwig and Suzuki-Miyaura Reaction Yield Prediction",
        "authors": [
            "Shimaa Baraka",
            "Ahmed M. El Kerdawy"
        ],
        "published": "2022-04-27T07:28:27Z",
        "summary": "Predicting the yield percentage of a chemical reaction is useful in many\naspects such as reducing wet-lab experimentation by giving the priority to the\nreactions with a high predicted yield. In this work we investigated the use of\nmultiple type inputs to predict chemical reaction yield. We used simplified\nmolecular-input line-entry system (SMILES) as well as calculated chemical\ndescriptors as model inputs. The model consists of a pre-trained bidirectional\ntransformer-based encoder (BERT) and a multi-layer perceptron (MLP) with a\nregression head to predict the yield. We experimented on two high throughput\nexperimentation (HTE) datasets for Buchwald-Hartwig and Suzuki-Miyaura\nreactions. The experiments show improvements in the prediction on both datasets\ncompared to systems using only SMILES or chemical descriptors as input. We also\ntested the model's performance on out-of-sample dataset splits of\nBuchwald-Hartwig and achieved comparable results with the state-of-the-art. In\naddition to predicting the yield, we demonstrated the model's ability to\nsuggest the optimum (highest yield) reaction conditions. The model was able to\nsuggest conditions that achieves 94% of the optimum reported yields. This\nproves the model to be useful in achieving the best results in the wet lab\nwithout expensive experimentation.",
        "pdf_link": "https://arxiv.org/pdf/2204.14062v1.pdf"
    },
    {
        "title": "UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus",
        "authors": [
            "Thilini Wijesiriwardene",
            "Vinh Nguyen",
            "Goonmeet Bajaj",
            "Hong Yung Yip",
            "Vishesh Javangula",
            "Yuqing Mao",
            "Kin Wah Fung",
            "Srinivasan Parthasarathy",
            "Amit P. Sheth",
            "Olivier Bodenreider"
        ],
        "published": "2022-04-27T06:03:24Z",
        "summary": "The UMLS Metathesaurus integrates more than 200 biomedical source\nvocabularies. During the Metathesaurus construction process, synonymous terms\nare clustered into concepts by human editors, assisted by lexical similarity\nalgorithms. This process is error-prone and time-consuming. Recently, a deep\nlearning model (LexLM) has been developed for the UMLS Vocabulary Alignment\n(UVA) task. This work introduces UBERT, a BERT-based language model, pretrained\non UMLS terms via a supervised Synonymy Prediction (SP) task replacing the\noriginal Next Sentence Prediction (NSP) task. The effectiveness of UBERT for\nUMLS Metathesaurus construction process is evaluated using the UMLS Vocabulary\nAlignment (UVA) task. We show that UBERT outperforms the LexLM, as well as\nbiomedical BERT-based models. Key to the performance of UBERT are the synonymy\nprediction task specifically developed for UBERT, the tight alignment of\ntraining data to the UVA task, and the similarity of the models used for\npretrained UBERT.",
        "pdf_link": "https://arxiv.org/pdf/2204.12716v1.pdf"
    },
    {
        "title": "Better Query Graph Selection for Knowledge Base Question Answering",
        "authors": [
            "Yonghui Jia",
            "Wenliang Chen"
        ],
        "published": "2022-04-27T01:53:06Z",
        "summary": "This paper presents a novel approach based on semantic parsing to improve the\nperformance of Knowledge Base Question Answering (KBQA). Specifically, we focus\non how to select an optimal query graph from a candidate set so as to retrieve\nthe answer from knowledge base (KB). In our approach, we first propose to\nlinearize the query graph into a sequence, which is used to form a sequence\npair with the question. It allows us to use mature sequence modeling, such as\nBERT, to encode the sequence pair. Then we use a ranking method to sort\ncandidate query graphs. In contrast to the previous studies, our approach can\nefficiently model semantic interactions between the graph and the question as\nwell as rank the candidate graphs from a global view. The experimental results\nshow that our system achieves the top performance on ComplexQuestions and the\nsecond best performance on WebQuestions.",
        "pdf_link": "https://arxiv.org/pdf/2204.12662v1.pdf"
    },
    {
        "title": "Parkinson's disease diagnostics using AI and natural language knowledge transfer",
        "authors": [
            "Maurycy Chronowski",
            "Maciej Klaczynski",
            "Malgorzata Dec-Cwiek",
            "Karolina Porebska"
        ],
        "published": "2022-04-26T19:39:29Z",
        "summary": "In this work, the issue of Parkinson's disease (PD) diagnostics using\nnon-invasive antemortem techniques was tackled. A deep learning approach for\nclassification of raw speech recordings in patients with diagnosed PD was\nproposed. The core of proposed method is an audio classifier using knowledge\ntransfer from a pretrained natural language model, namely \\textit{wav2vec 2.0}.\nMethod was tested on a group of 38 PD patients and 10 healthy persons above the\nage of 50. A dataset of speech recordings acquired using a smartphone recorder\nwas constructed and the recordings were label as PD/non-PD with severity of the\ndisease additionally rated using Hoehn-Yahr scale. The audio recordings were\ncut into 2141 samples that include sentences, syllables, vowels and sustained\nphonation. The classifier scores up to 97.92\\% of cross-validated accuracy.\nAdditionally, paper presents results of a human-level performance assessment\nquestionnaire, which was consulted with the neurology professionals",
        "pdf_link": "https://arxiv.org/pdf/2204.12559v1.pdf"
    },
    {
        "title": "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval",
        "authors": [
            "Yuying Ge",
            "Yixiao Ge",
            "Xihui Liu",
            "Alex Jinpeng Wang",
            "Jianping Wu",
            "Ying Shan",
            "Xiaohu Qie",
            "Ping Luo"
        ],
        "published": "2022-04-26T16:06:31Z",
        "summary": "Dominant pre-training work for video-text retrieval mainly adopt the\n\"dual-encoder\" architectures to enable efficient retrieval, where two separate\nencoders are used to contrast global video and text representations, but ignore\ndetailed local semantics. The recent success of image BERT pre-training with\nmasked visual modeling that promotes the learning of local visual context,\nmotivates a possible solution to address the above limitation. In this work, we\nfor the first time investigate masked visual modeling in video-text\npre-training with the \"dual-encoder\" architecture. We perform Masked visual\nmodeling with Injected LanguagE Semantics (MILES) by employing an extra\nsnapshot video encoder as an evolving \"tokenizer\" to produce reconstruction\ntargets for masked video patch prediction. Given the corrupted video, the video\nencoder is trained to recover text-aligned features of the masked patches via\nreasoning with the visible regions along the spatial and temporal dimensions,\nwhich enhances the discriminativeness of local visual features and the\nfine-grained cross-modality alignment. Our method outperforms state-of-the-art\nmethods for text-to-video retrieval on four datasets with both zero-shot and\nfine-tune evaluation protocols. Our approach also surpasses the baseline models\nsignificantly on zero-shot action recognition, which can be cast as\nvideo-to-text retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2204.12408v1.pdf"
    },
    {
        "title": "Science Checker: Extractive-Boolean Question Answering For Scientific Fact Checking",
        "authors": [
            "Lo\u00efc Rakotoson",
            "Charles Letaillieur",
            "Sylvain Massip",
            "Fr\u00e9jus Laleye"
        ],
        "published": "2022-04-26T12:35:23Z",
        "summary": "With the explosive growth of scientific publications, making the synthesis of\nscientific knowledge and fact checking becomes an increasingly complex task. In\nthis paper, we propose a multi-task approach for verifying the scientific\nquestions based on a joint reasoning from facts and evidence in research\narticles. We propose an intelligent combination of (1) an automatic information\nsummarization and (2) a Boolean Question Answering which allows to generate an\nanswer to a scientific question from only extracts obtained after\nsummarization. Thus on a given topic, our proposed approach conducts structured\ncontent modeling based on paper abstracts to answer a scientific question while\nhighlighting texts from paper that discuss the topic. We based our final system\non an end-to-end Extractive Question Answering (EQA) combined with a three\noutputs classification model to perform in-depth semantic understanding of a\nquestion to illustrate the aggregation of multiple responses. With our light\nand fast proposed architecture, we achieved an average error rate of 4% and a\nF1-score of 95.6%. Our results are supported via experiments with two QA models\n(BERT, RoBERTa) over 3 Million Open Access (OA) articles in the medical and\nhealth domains on Europe PMC.",
        "pdf_link": "https://arxiv.org/pdf/2204.12263v2.pdf"
    },
    {
        "title": "You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas",
        "authors": [
            "Haoran Li",
            "Yangqiu Song",
            "Lixin Fan"
        ],
        "published": "2022-04-26T09:36:18Z",
        "summary": "Social chatbots, also known as chit-chat chatbots, evolve rapidly with large\npretrained language models. Despite the huge progress, privacy concerns have\narisen recently: training data of large language models can be extracted via\nmodel inversion attacks. On the other hand, the datasets used for training\nchatbots contain many private conversations between two individuals. In this\nwork, we further investigate the privacy leakage of the hidden states of\nchatbots trained by language modeling which has not been well studied yet. We\nshow that speakers' personas can be inferred through a simple neural network\nwith high accuracy. To this end, we propose effective defense objectives to\nprotect persona leakage from hidden states. We conduct extensive experiments to\ndemonstrate that our proposed defense objectives can greatly reduce the attack\naccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve\nlanguage models' powerful generation ability.",
        "pdf_link": "https://arxiv.org/pdf/2205.10228v1.pdf"
    },
    {
        "title": "GypSum: Learning Hybrid Representations for Code Summarization",
        "authors": [
            "Yu Wang",
            "Yu Dong",
            "Xuesong Lu",
            "Aoying Zhou"
        ],
        "published": "2022-04-26T07:44:49Z",
        "summary": "Code summarization with deep learning has been widely studied in recent\nyears. Current deep learning models for code summarization generally follow the\nprinciple in neural machine translation and adopt the encoder-decoder\nframework, where the encoder learns the semantic representations from source\ncode and the decoder transforms the learnt representations into human-readable\ntext that describes the functionality of code snippets. Despite they achieve\nthe new state-of-the-art performance, we notice that current models often\neither generate less fluent summaries, or fail to capture the core\nfunctionality, since they usually focus on a single type of code\nrepresentations. As such we propose GypSum, a new deep learning model that\nlearns hybrid representations using graph attention neural networks and a\npre-trained programming and natural language model. We introduce particular\nedges related to the control flow of a code snippet into the abstract syntax\ntree for graph construction, and design two encoders to learn from the graph\nand the token sequence of source code, respectively. We modify the\nencoder-decoder sublayer in the Transformer's decoder to fuse the\nrepresentations and propose a dual-copy mechanism to facilitate summary\ngeneration. Experimental results demonstrate the superior performance of GypSum\nover existing code summarization models.",
        "pdf_link": "https://arxiv.org/pdf/2204.12916v1.pdf"
    },
    {
        "title": "Approach to Predicting News -- A Precise Multi-LSTM Network With BERT",
        "authors": [
            "Chia-Lin Chen",
            "Pei-Yu Huang",
            "Yi-Ting Huang",
            "Chun Lin"
        ],
        "published": "2022-04-26T06:14:01Z",
        "summary": "Varieties of Democracy (V-Dem) is a new approach to conceptualizing and\nmeasuring democracy and politics. It has information for 200 countries and is\none of the biggest databases for political science. According to the V-Dem\nannual democracy report 2019, Taiwan is one of the two countries that got\ndisseminated false information from foreign governments the most. It also shows\nthat the \"made-up news\" has caused a great deal of confusion in Taiwanese\nsociety and has serious impacts on global stability. Although there are several\napplications helping distinguish the false information, we found out that the\npre-processing of categorizing the news is still done by human labor. However,\nhuman labor may cause mistakes and cannot work for a long time. The growing\ndemands for automatic machines in the near decades show that while the machine\ncan do as good as humans or even better, using machines can reduce humans'\nburden and cut down costs. Therefore, in this work, we build a predictive model\nto classify the category of news. The corpora we used contains 28358 news and\n200 news scraped from the online newspaper Liberty Times Net (LTN) website and\nincludes 8 categories: Technology, Entertainment, Fashion, Politics, Sports,\nInternational, Finance, and Health. At first, we use Bidirectional Encoder\nRepresentations from Transformers (BERT) for word embeddings which transform\neach Chinese character into a (1,768) vector. Then, we use a Long Short-Term\nMemory (LSTM) layer to transform word embeddings into sentence embeddings and\nadd another LSTM layer to transform them into document embeddings. Each\ndocument embedding is an input for the final predicting model, which contains\ntwo Dense layers and one Activation layer. And each document embedding is\ntransformed into 1 vector with 8 real numbers, then the highest one will\ncorrespond to the 8 news categories with up to 99% accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2204.12093v1.pdf"
    },
    {
        "title": "Pretraining Chinese BERT for Detecting Word Insertion and Deletion Errors",
        "authors": [
            "Cong Zhou",
            "Yong Dai",
            "Duyu Tang",
            "Enbo Zhao",
            "Zhangyin Feng",
            "Li Kuang",
            "Shuming Shi"
        ],
        "published": "2022-04-26T03:19:36Z",
        "summary": "Chinese BERT models achieve remarkable progress in dealing with grammatical\nerrors of word substitution. However, they fail to handle word insertion and\ndeletion because BERT assumes the existence of a word at each position. To\naddress this, we present a simple and effective Chinese pretrained model. The\nbasic idea is to enable the model to determine whether a word exists at a\nparticular position. We achieve this by introducing a special token\n\\texttt{[null]}, the prediction of which stands for the non-existence of a\nword. In the training stage, we design pretraining tasks such that the model\nlearns to predict \\texttt{[null]} and real words jointly given the surrounding\ncontext. In the inference stage, the model readily detects whether a word\nshould be inserted or deleted with the standard masked language modeling\nfunction. We further create an evaluation dataset to foster research on word\ninsertion and deletion. It includes human-annotated corrections for 7,726\nerroneous sentences. Results show that existing Chinese BERT performs poorly on\ndetecting insertion and deletion errors. Our approach significantly improves\nthe F1 scores from 24.1\\% to 78.1\\% for word insertion and from 26.5\\% to\n68.5\\% for word deletion, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2204.12052v1.pdf"
    },
    {
        "title": "C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval",
        "authors": [
            "Eugene Yang",
            "Suraj Nair",
            "Ramraj Chandradevan",
            "Rebecca Iglesias-Flores",
            "Douglas W. Oard"
        ],
        "published": "2022-04-25T23:12:05Z",
        "summary": "Pretrained language models have improved effectiveness on numerous tasks,\nincluding ad-hoc retrieval. Recent work has shown that continuing to pretrain a\nlanguage model with auxiliary objectives before fine-tuning on the retrieval\ntask can further improve retrieval effectiveness. Unlike monolingual retrieval,\ndesigning an appropriate auxiliary task for cross-language mappings is\nchallenging. To address this challenge, we use comparable Wikipedia articles in\ndifferent languages to further pretrain off-the-shelf multilingual pretrained\nmodels before fine-tuning on the retrieval task. We show that our approach\nyields improvements in retrieval effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2204.11989v1.pdf"
    },
    {
        "title": "Crystal Transformer: Self-learning neural language model for Generative and Tinkering Design of Materials",
        "authors": [
            "Lai Wei",
            "Qinyang Li",
            "Yuqi Song",
            "Stanislav Stefanov",
            "Edirisuriya M. D. Siriwardane",
            "Fanglin Chen",
            "Jianjun Hu"
        ],
        "published": "2022-04-25T20:20:26Z",
        "summary": "Self-supervised neural language models have recently achieved unprecedented\nsuccess, from natural language processing to learning the languages of\nbiological sequences and organic molecules. These models have demonstrated\nsuperior performance in the generation, structure classification, and\nfunctional predictions for proteins and molecules with learned representations.\nHowever, most of the masking-based pre-trained language models are not designed\nfor generative design, and their black-box nature makes it difficult to\ninterpret their design logic. Here we propose BLMM Crystal Transformer, a\nneural network based probabilistic generative model for generative and\ntinkering design of inorganic materials. Our model is built on the blank\nfilling language model for text generation and has demonstrated unique\nadvantages in learning the \"materials grammars\" together with high-quality\ngeneration, interpretability, and data efficiency. It can generate chemically\nvalid materials compositions with as high as 89.7\\% charge neutrality and\n84.8\\% balanced electronegativity, which are more than 4 and 8 times higher\ncompared to a pseudo random sampling baseline. The probabilistic generation\nprocess of BLMM allows it to recommend tinkering operations based on learned\nmaterials chemistry and makes it useful for materials doping. Combined with the\nTCSP crysal structure prediction algorithm, We have applied our model to\ndiscover a set of new materials as validated using DFT calculations. Our work\nthus brings the unsupervised transformer language models based generative\nartificial intelligence to inorganic materials. A user-friendly web app has\nbeen developed for computational materials doping and can be accessed freely at\n\\url{www.materialsatlas.org/blmtinker}.",
        "pdf_link": "https://arxiv.org/pdf/2204.11953v1.pdf"
    },
    {
        "title": "Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks",
        "authors": [
            "Navid Rezaei",
            "Marek Z. Reformat"
        ],
        "published": "2022-04-25T18:56:55Z",
        "summary": "Pre-trained language models have shown excellent results in few-shot learning\nscenarios using in-context learning. Although it is impressive, the size of\nlanguage models can be prohibitive to make them usable in on-device\napplications, such as sensors or smartphones. With smaller language models,\ntask-specific data annotation is needed to fine-tune the language model for a\nspecific purpose. However, data annotation can have a substantial financial and\ntime burden for small research groups, startups, and even companies. In this\npaper, we analyze different prompt-based fine-tuning techniques to improve\nresults on both language and multimodal causal transformer models. To evaluate\nour results, we use a dataset focusing on visual commonsense reasoning in time.\nOur results show that by simple model-agnostic prompt-based fine-tuning,\ncomparable results can be reached by only using 35%-40% of the fine-tuning\ntraining dataset. The proposed approaches result in significant time and\nfinancial savings. As the proposed methods make minimal architectural\nassumptions, other researchers can use the results in their transformer models\nwith minimal adaptations. We plan to release the source code freely to make it\neasier for the community to use and contribute to our work.",
        "pdf_link": "https://arxiv.org/pdf/2204.11922v1.pdf"
    },
    {
        "title": "The Causal News Corpus: Annotating Causal Relations in Event Sentences from News",
        "authors": [
            "Fiona Anting Tan",
            "Ali H\u00fcrriyeto\u011flu",
            "Tommaso Caselli",
            "Nelleke Oostdijk",
            "Tadashi Nomoto",
            "Hansi Hettiarachchi",
            "Iqra Ameer",
            "Onur Uca",
            "Farhana Ferdousi Liza",
            "Tiancheng Hu"
        ],
        "published": "2022-04-25T15:14:07Z",
        "summary": "Despite the importance of understanding causality, corpora addressing causal\nrelations are limited. There is a discrepancy between existing annotation\nguidelines of event causality and conventional causality corpora that focus\nmore on linguistics. Many guidelines restrict themselves to include only\nexplicit relations or clause-based arguments. Therefore, we propose an\nannotation schema for event causality that addresses these concerns. We\nannotated 3,559 event sentences from protest event news with labels on whether\nit contains causal relations or not. Our corpus is known as the Causal News\nCorpus (CNC). A neural network built upon a state-of-the-art pre-trained\nlanguage model performed well with 81.20% F1 score on test set, and 83.46% in\n5-folds cross-validation. CNC is transferable across two external corpora:\nCausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of\nthese external datasets for training, we achieved up to approximately 64% F1 on\nthe CNC test set without additional fine-tuning. CNC also served as an\neffective training and pre-training dataset for the two external corpora.\nLastly, we demonstrate the difficulty of our task to the layman in a\ncrowd-sourced annotation exercise. Our annotated corpus is publicly available,\nproviding a valuable resource for causal text mining researchers.",
        "pdf_link": "https://arxiv.org/pdf/2204.11714v1.pdf"
    },
    {
        "title": "Which Discriminator for Cooperative Text Generation?",
        "authors": [
            "Antoine Chaffin",
            "Thomas Scialom",
            "Sylvain Lamprier",
            "Jacopo Staiano",
            "Benjamin Piwowarski",
            "Ewa Kijak",
            "Vincent Claveau"
        ],
        "published": "2022-04-25T12:16:02Z",
        "summary": "Language models generate texts by successively predicting probability\ndistributions for next tokens given past ones. A growing field of interest\ntries to leverage external information in the decoding process so that the\ngenerated texts have desired properties, such as being more natural, non toxic,\nfaithful, or having a specific writing style. A solution is to use a classifier\nat each generation step, resulting in a cooperative environment where the\nclassifier guides the decoding of the language model distribution towards\nrelevant texts for the task at hand. In this paper, we examine three families\nof (transformer-based) discriminators for this specific task of cooperative\ndecoding: bidirectional, left-to-right and generative ones. We evaluate the\npros and cons of these different types of discriminators for cooperative\ngeneration, exploring respective accuracy on classification tasks along with\ntheir impact on the resulting sample quality and computational performances. We\nalso provide the code of a batched implementation of the powerful cooperative\ndecoding strategy used for our experiments, the Monte Carlo Tree Search,\nworking with each discriminator for Natural Language Generation.",
        "pdf_link": "https://arxiv.org/pdf/2204.11586v1.pdf"
    },
    {
        "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
        "authors": [
            "Stephanie Brandl",
            "Oliver Eberle",
            "Jonas Pilot",
            "Anders S\u00f8gaard"
        ],
        "published": "2022-04-25T08:23:13Z",
        "summary": "Learned self-attention functions in state-of-the-art NLP models often\ncorrelate with human attention. We investigate whether self-attention in\nlarge-scale pre-trained language models is as predictive of human eye fixation\npatterns during task-reading as classical cognitive models of human attention.\nWe compare attention functions across two task-specific reading datasets for\nsentiment analysis and relation extraction. We find the predictiveness of\nlarge-scale pre-trained self-attention for human attention depends on `what is\nin the tail', e.g., the syntactic nature of rare contexts. Further, we observe\nthat task-specific fine-tuning does not increase the correlation with human\ntask-specific reading. Through an input reduction experiment we give\ncomplementary insights on the sparsity and fidelity trade-off, showing that\nlower-entropy attention vectors are more faithful.",
        "pdf_link": "https://arxiv.org/pdf/2205.10226v1.pdf"
    },
    {
        "title": "ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference",
        "authors": [
            "Kai Hui",
            "Honglei Zhuang",
            "Tao Chen",
            "Zhen Qin",
            "Jing Lu",
            "Dara Bahri",
            "Ji Ma",
            "Jai Prakash Gupta",
            "Cicero Nogueira dos Santos",
            "Yi Tay",
            "Don Metzler"
        ],
        "published": "2022-04-25T06:26:29Z",
        "summary": "State-of-the-art neural models typically encode document-query pairs using\ncross-attention for re-ranking. To this end, models generally utilize an\nencoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach.\nThese paradigms, however, are not without flaws, i.e., running the model on all\nquery-document pairs at inference-time incurs a significant computational cost.\nThis paper proposes a new training and inference paradigm for re-ranking. We\npropose to finetune a pretrained encoder-decoder model using in the form of\ndocument to query generation. Subsequently, we show that this encoder-decoder\narchitecture can be decomposed into a decoder-only language model during\ninference. This results in significant inference time speedups since the\ndecoder-only architecture only needs to learn to interpret static encoder\nembeddings during inference. Our experiments show that this new paradigm\nachieves results that are comparable to the more expensive cross-attention\nranking approaches while being up to 6.8X faster. We believe this work paves\nthe way for more efficient neural rankers that leverage large pretrained\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2204.11458v1.pdf"
    },
    {
        "title": "Twitter-Based Gender Recognition Using Transformers",
        "authors": [
            "Zahra Movahedi Nia",
            "Ali Ahmadi",
            "Bruce Mellado",
            "Jianhong Wu",
            "James Orbinski",
            "Ali Agary",
            "Jude Dzevela Kong"
        ],
        "published": "2022-04-24T19:58:42Z",
        "summary": "Social media contains useful information about people and the society that\ncould help advance research in many different areas (e.g. by applying opinion\nmining, emotion/sentiment analysis, and statistical analysis) such as business\nand finance, health, socio-economic inequality and gender vulnerability. User\ndemographics provide rich information that could help study the subject\nfurther. However, user demographics such as gender are considered private and\nare not freely available. In this study, we propose a model based on\ntransformers to predict the user's gender from their images and tweets. We\nfine-tune a model based on Vision Transformers (ViT) to stratify female and\nmale images. Next, we fine-tune another model based on Bidirectional Encoders\nRepresentations from Transformers (BERT) to recognize the user's gender by\ntheir tweets. This is highly beneficial, because not all users provide an image\nthat indicates their gender. The gender of such users could be detected form\ntheir tweets. The combination model improves the accuracy of image and text\nclassification models by 6.98% and 4.43%, respectively. This shows that the\nimage and text classification models are capable of complementing each other by\nproviding additional information to one another. We apply our method to the\nPAN-2018 dataset, and obtain an accuracy of 85.52%.",
        "pdf_link": "https://arxiv.org/pdf/2205.06801v1.pdf"
    },
    {
        "title": "Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training",
        "authors": [
            "Yuanxin Liu",
            "Fandong Meng",
            "Zheng Lin",
            "Peng Fu",
            "Yanan Cao",
            "Weiping Wang",
            "Jie Zhou"
        ],
        "published": "2022-04-24T08:42:47Z",
        "summary": "Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained\nlanguage models (PLMs) like BERT contain matching subnetworks that have similar\ntransfer learning performance as the original PLM. These subnetworks are found\nusing magnitude-based pruning. In this paper, we find that the BERT subnetworks\nhave even more potential than these studies have shown. Firstly, we discover\nthat the success of magnitude pruning can be attributed to the preserved\npre-training performance, which correlates with the downstream transferability.\nInspired by this, we propose to directly optimize the subnetwork structure\ntowards the pre-training objectives, which can better preserve the pre-training\nperformance. Specifically, we train binary masks over model weights on the\npre-training tasks, with the aim of preserving the universal transferability of\nthe subnetwork, which is agnostic to any specific downstream tasks. We then\nfine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The\nresults show that, compared with magnitude pruning, mask training can\neffectively find BERT subnetworks with improved overall performance on\ndownstream tasks. Moreover, our method is also more efficient in searching\nsubnetworks and more advantageous when fine-tuning within a certain range of\ndata scarcity. Our code is available at https://github.com/llyx97/TAMT.",
        "pdf_link": "https://arxiv.org/pdf/2204.11218v2.pdf"
    },
    {
        "title": "Locally Aggregated Feature Attribution on Natural Language Model Understanding",
        "authors": [
            "Sheng Zhang",
            "Jin Wang",
            "Haitao Jiang",
            "Rui Song"
        ],
        "published": "2022-04-22T18:59:27Z",
        "summary": "With the growing popularity of deep-learning models, model understanding\nbecomes more important. Much effort has been devoted to demystify deep neural\nnetworks for better interpretability. Some feature attribution methods have\nshown promising results in computer vision, especially the gradient-based\nmethods where effectively smoothing the gradients with reference data is key to\na robust and faithful result. However, direct application of these\ngradient-based methods to NLP tasks is not trivial due to the fact that the\ninput consists of discrete tokens and the \"reference\" tokens are not explicitly\ndefined. In this work, we propose Locally Aggregated Feature Attribution\n(LAFA), a novel gradient-based feature attribution method for NLP models.\nInstead of relying on obscure reference tokens, it smooths gradients by\naggregating similar reference texts derived from language model embeddings. For\nevaluation purpose, we also design experiments on different NLP tasks including\nEntity Recognition and Sentiment Analysis on public datasets as well as key\nfeature detection on a constructed Amazon catalogue dataset. The superior\nperformance of the proposed method is demonstrated through experiments.",
        "pdf_link": "https://arxiv.org/pdf/2204.10893v2.pdf"
    },
    {
        "title": "Exploiting Session Information in BERT-based Session-aware Sequential Recommendation",
        "authors": [
            "Jinseok Seol",
            "Youngrok Ko",
            "Sang-goo Lee"
        ],
        "published": "2022-04-22T17:58:10Z",
        "summary": "In recommendation systems, utilizing the user interaction history as\nsequential information has resulted in great performance improvement. However,\nin many online services, user interactions are commonly grouped by sessions\nthat presumably share preferences, which requires a different approach from\nordinary sequence representation techniques. To this end, sequence\nrepresentation models with a hierarchical structure or various viewpoints have\nbeen developed but with a rather complex network structure. In this paper, we\npropose three methods to improve recommendation performance by exploiting\nsession information while minimizing additional parameters in a BERT-based\nsequential recommendation model: using session tokens, adding session segment\nembeddings, and a time-aware self-attention. We demonstrate the feasibility of\nthe proposed methods through experiments on widely used recommendation\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.10851v3.pdf"
    },
    {
        "title": "Reward Reports for Reinforcement Learning",
        "authors": [
            "Thomas Krendl Gilbert",
            "Nathan Lambert",
            "Sarah Dean",
            "Tom Zick",
            "Aaron Snoswell"
        ],
        "published": "2022-04-22T16:53:39Z",
        "summary": "Building systems that are good for society in the face of complex societal\neffects requires a dynamic approach. Recent approaches to machine learning (ML)\ndocumentation have demonstrated the promise of discursive frameworks for\ndeliberation about these complexities. However, these developments have been\ngrounded in a static ML paradigm, leaving the role of feedback and\npost-deployment performance unexamined. Meanwhile, recent work in reinforcement\nlearning has shown that the effects of feedback and optimization objectives on\nsystem behavior can be wide-ranging and unpredictable. In this paper we sketch\na framework for documenting deployed and iteratively updated learning systems,\nwhich we call Reward Reports. Taking inspiration from various contributions to\nthe technical literature on reinforcement learning, we outline Reward Reports\nas living documents that track updates to design choices and assumptions behind\nwhat a particular automated system is optimizing for. They are intended to\ntrack dynamic phenomena arising from system deployment, rather than merely\nstatic properties of models or data. After presenting the elements of a Reward\nReport, we discuss a concrete example: Meta's BlenderBot 3 chatbot. Several\nothers for game-playing (DeepMind's MuZero), content recommendation\n(MovieLens), and traffic control (Project Flow) are included in the appendix.",
        "pdf_link": "https://arxiv.org/pdf/2204.10817v3.pdf"
    },
    {
        "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
        "authors": [
            "Stephanie C. Y. Chan",
            "Adam Santoro",
            "Andrew K. Lampinen",
            "Jane X. Wang",
            "Aaditya Singh",
            "Pierre H. Richemond",
            "Jay McClelland",
            "Felix Hill"
        ],
        "published": "2022-04-22T16:10:50Z",
        "summary": "Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2205.05055v6.pdf"
    },
    {
        "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues",
        "authors": [
            "Gustavo Penha",
            "Claudia Hauff"
        ],
        "published": "2022-04-22T08:15:15Z",
        "summary": "Ranking responses for a given dialogue context is a popular benchmark in\nwhich the setup is to re-rank the ground-truth response over a limited set of\n$n$ responses, where $n$ is typically 10. The predominance of this setup in\nconversation response ranking has lead to a great deal of attention to building\nneural re-rankers, while the first-stage retrieval step has been overlooked.\nSince the correct answer is always available in the candidate list of $n$\nresponses, this artificial evaluation setup assumes that there is a first-stage\nretrieval step which is always able to rank the correct response in its top-$n$\nlist. In this paper we focus on the more realistic task of full-rank retrieval\nof responses, where $n$ can be up to millions of responses. We investigate both\ndialogue context and response expansion techniques for sparse retrieval, as\nwell as zero-shot and fine-tuned dense retrieval approaches. Our findings based\non three different information-seeking dialogue datasets reveal that a learned\nresponse expansion technique is a solid baseline for sparse retrieval. We find\nthe best performing method overall to be dense retrieval with intermediate\ntraining, i.e. a step after the language model pre-training where sentence\nrepresentations are learned, followed by fine-tuning on the target\nconversational data. We also investigate the intriguing phenomena that harder\nnegatives sampling techniques lead to worse results for the fine-tuned dense\nretrieval models. The code and datasets are available at\nhttps://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2204.10558v1.pdf"
    },
    {
        "title": "KALA: Knowledge-Augmented Language Model Adaptation",
        "authors": [
            "Minki Kang",
            "Jinheon Baek",
            "Sung Ju Hwang"
        ],
        "published": "2022-04-22T08:11:59Z",
        "summary": "Pre-trained language models (PLMs) have achieved remarkable success on\nvarious natural language understanding tasks. Simple fine-tuning of PLMs, on\nthe other hand, might be suboptimal for domain-specific tasks because they\ncannot possibly cover knowledge from all domains. While adaptive pre-training\nof PLMs can help them obtain domain-specific knowledge, it requires a large\ntraining cost. Moreover, adaptive pre-training can harm the PLM's performance\non the downstream task by causing catastrophic forgetting of its general\nknowledge. To overcome such limitations of adaptive pre-training for PLM\nadaption, we propose a novel domain adaption framework for PLMs coined as\nKnowledge-Augmented Language model Adaptation (KALA), which modulates the\nintermediate hidden representations of PLMs with domain knowledge, consisting\nof entities and their relational facts. We validate the performance of our KALA\non question answering and named entity recognition tasks on multiple datasets\nacross various domains. The results show that, despite being computationally\nefficient, our KALA largely outperforms adaptive pre-training. Code is\navailable at: https://github.com/Nardien/KALA/.",
        "pdf_link": "https://arxiv.org/pdf/2204.10555v2.pdf"
    },
    {
        "title": "Taygete at SemEval-2022 Task 4: RoBERTa based models for detecting Patronising and Condescending Language",
        "authors": [
            "Jayant Chhillar"
        ],
        "published": "2022-04-22T06:11:47Z",
        "summary": "This work describes the development of different models to detect patronising\nand condescending language within extracts of news articles as part of the\nSemEval 2022 competition (Task-4). This work explores different models based on\nthe pre-trained RoBERTa language model coupled with LSTM and CNN layers. The\nbest models achieved 15$^{th}$ rank with an F1-score of 0.5924 for subtask-A\nand 12$^{th}$ in subtask-B with a macro-F1 score of 0.3763.",
        "pdf_link": "https://arxiv.org/pdf/2204.10519v1.pdf"
    },
    {
        "title": "WaBERT: A Low-resource End-to-end Model for Spoken Language Understanding and Speech-to-BERT Alignment",
        "authors": [
            "Lin Yao",
            "Jianfei Song",
            "Ruizhuo Xu",
            "Yingfang Yang",
            "Zijian Chen",
            "Yafeng Deng"
        ],
        "published": "2022-04-22T02:14:40Z",
        "summary": "Historically lower-level tasks such as automatic speech recognition (ASR) and\nspeaker identification are the main focus in the speech field. Interest has\nbeen growing in higher-level spoken language understanding (SLU) tasks\nrecently, like sentiment analysis (SA). However, improving performances on SLU\ntasks remains a big challenge. Basically, there are two main methods for SLU\ntasks: (1) Two-stage method, which uses a speech model to transfer speech to\ntext, then uses a language model to get the results of downstream tasks; (2)\nOne-stage method, which just fine-tunes a pre-trained speech model to fit in\nthe downstream tasks. The first method loses emotional cues such as intonation,\nand causes recognition errors during ASR process, and the second one lacks\nnecessary language knowledge. In this paper, we propose the Wave BERT (WaBERT),\na novel end-to-end model combining the speech model and the language model for\nSLU tasks. WaBERT is based on the pre-trained speech and language model, hence\ntraining from scratch is not needed. We also set most parameters of WaBERT\nfrozen during training. By introducing WaBERT, audio-specific information and\nlanguage knowledge are integrated in the short-time and low-resource training\nprocess to improve results on the dev dataset of SLUE SA tasks by 1.15% of\nrecall score and 0.82% of F1 score. Additionally, we modify the serial\nContinuous Integrate-and-Fire (CIF) mechanism to achieve the monotonic\nalignment between the speech and text modalities.",
        "pdf_link": "https://arxiv.org/pdf/2204.10461v1.pdf"
    },
    {
        "title": "Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires",
        "authors": [
            "Thong Nguyen",
            "Andrew Yates",
            "Ayah Zirikly",
            "Bart Desmet",
            "Arman Cohan"
        ],
        "published": "2022-04-21T22:57:11Z",
        "summary": "Automated methods have been widely used to identify and analyze mental health\nconditions (e.g., depression) from various sources of information, including\nsocial media. Yet, deployment of such models in real-world healthcare\napplications faces challenges including poor out-of-domain generalization and\nlack of trust in black box models. In this work, we propose approaches for\ndepression detection that are constrained to different degrees by the presence\nof symptoms described in PHQ9, a questionnaire used by clinicians in the\ndepression screening process. In dataset-transfer experiments on three social\nmedia datasets, we find that grounding the model in PHQ9's symptoms\nsubstantially improves its ability to generalize to out-of-distribution data\ncompared to a standard BERT-based approach. Furthermore, this approach can\nstill perform competitively on in-domain data. These results and our\nqualitative analyses suggest that grounding model predictions in\nclinically-relevant symptoms can improve generalizability while producing a\nmodel that is easier to inspect.",
        "pdf_link": "https://arxiv.org/pdf/2204.10432v1.pdf"
    },
    {
        "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings",
        "authors": [
            "Yung-Sung Chuang",
            "Rumen Dangovski",
            "Hongyin Luo",
            "Yang Zhang",
            "Shiyu Chang",
            "Marin Solja\u010di\u0107",
            "Shang-Wen Li",
            "Wen-tau Yih",
            "Yoon Kim",
            "James Glass"
        ],
        "published": "2022-04-21T17:32:01Z",
        "summary": "We propose DiffCSE, an unsupervised contrastive learning framework for\nlearning sentence embeddings. DiffCSE learns sentence embeddings that are\nsensitive to the difference between the original sentence and an edited\nsentence, where the edited sentence is obtained by stochastically masking out\nthe original sentence and then sampling from a masked language model. We show\nthat DiffSCE is an instance of equivariant contrastive learning (Dangovski et\nal., 2021), which generalizes contrastive learning and learns representations\nthat are insensitive to certain types of augmentations and sensitive to other\n\"harmful\" types of augmentations. Our experiments show that DiffCSE achieves\nstate-of-the-art results among unsupervised sentence representation learning\nmethods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic\ntextual similarity tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.10298v1.pdf"
    },
    {
        "title": "Revisiting Gaussian mixture critics in off-policy reinforcement learning: a sample-based approach",
        "authors": [
            "Bobak Shahriari",
            "Abbas Abdolmaleki",
            "Arunkumar Byravan",
            "Abe Friesen",
            "Siqi Liu",
            "Jost Tobias Springenberg",
            "Nicolas Heess",
            "Matt Hoffman",
            "Martin Riedmiller"
        ],
        "published": "2022-04-21T16:44:47Z",
        "summary": "Actor-critic algorithms that make use of distributional policy evaluation\nhave frequently been shown to outperform their non-distributional counterparts\non many challenging control tasks. Examples of this behavior include the D4PG\nand DMPO algorithms as compared to DDPG and MPO, respectively [Barth-Maron et\nal., 2018; Hoffman et al., 2020]. However, both agents rely on the C51 critic\nfor value estimation.One major drawback of the C51 approach is its requirement\nof prior knowledge about the minimum andmaximum values a policy can attain as\nwell as the number of bins used, which fixes the resolution ofthe\ndistributional estimate. While the DeepMind control suite of tasks utilizes\nstandardized rewards and episode lengths, thus enabling the entire suite to be\nsolved with a single setting of these hyperparameters, this is often not the\ncase. This paper revisits a natural alternative that removes this requirement,\nnamelya mixture of Gaussians, and a simple sample-based loss function to train\nit in an off-policy regime. We empirically evaluate its performance on a broad\nrange of continuous control tasks and demonstrate that it eliminates the need\nfor these distributional hyperparameters and achieves state-of-the-art\nperformance on a variety of challenging tasks (e.g. the humanoid, dog,\nquadruped, and manipulator domains). Finallywe provide an implementation in the\nAcme agent repository.",
        "pdf_link": "https://arxiv.org/pdf/2204.10256v2.pdf"
    },
    {
        "title": "Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad"
        ],
        "published": "2022-04-21T04:26:51Z",
        "summary": "Recent work incorporates pre-trained word embeddings such as BERT embeddings\ninto Neural Topic Models (NTMs), generating highly coherent topics. However,\nwith high-quality contextualized document representations, do we really need\nsophisticated neural models to obtain coherent and interpretable topics? In\nthis paper, we conduct thorough experiments showing that directly clustering\nhigh-quality sentence embeddings with an appropriate word selecting method can\ngenerate more coherent and diverse topics than NTMs, achieving also higher\nefficiency and simplicity.",
        "pdf_link": "https://arxiv.org/pdf/2204.09874v1.pdf"
    },
    {
        "title": "Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing",
        "authors": [
            "Benedikt Boecking",
            "Naoto Usuyama",
            "Shruthi Bannur",
            "Daniel C. Castro",
            "Anton Schwaighofer",
            "Stephanie Hyland",
            "Maria Wetscherek",
            "Tristan Naumann",
            "Aditya Nori",
            "Javier Alvarez-Valle",
            "Hoifung Poon",
            "Ozan Oktay"
        ],
        "published": "2022-04-21T00:04:35Z",
        "summary": "Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.",
        "pdf_link": "https://arxiv.org/pdf/2204.09817v4.pdf"
    },
    {
        "title": "When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes",
        "authors": [
            "Mycal Tucker",
            "Tiwalayo Eisape",
            "Peng Qian",
            "Roger Levy",
            "Julie Shah"
        ],
        "published": "2022-04-20T18:09:36Z",
        "summary": "Recent causal probing literature reveals when language models and syntactic\nprobes use similar representations. Such techniques may yield \"false negative\"\ncausality results: models may use representations of syntax, but probes may\nhave learned to use redundant encodings of the same syntactic information. We\ndemonstrate that models do encode syntactic information redundantly and\nintroduce a new probe design that guides probes to consider all syntactic\ninformation present in embeddings. Using these probes, we find evidence for the\nuse of syntax in models where prior methods did not, allowing us to boost model\nperformance by injecting syntactic information into representations.",
        "pdf_link": "https://arxiv.org/pdf/2204.09722v1.pdf"
    },
    {
        "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models",
        "authors": [
            "Richard Plant",
            "Valerio Giuffrida",
            "Dimitra Gkatzia"
        ],
        "published": "2022-04-20T11:12:53Z",
        "summary": "Large scale adoption of large language models has introduced a new era of\nconvenient knowledge transfer for a slew of natural language processing tasks.\nHowever, these models also run the risk of undermining user trust by exposing\nunwanted information about the data subjects, which may be extracted by a\nmalicious party, e.g. through adversarial attacks. We present an empirical\ninvestigation into the extent of the personal information encoded into\npre-trained representations by a range of popular models, and we show a\npositive correlation between the complexity of a model, the amount of data used\nin pre-training, and data leakage. In this paper, we present the first wide\ncoverage evaluation and comparison of some of the most popular\nprivacy-preserving algorithms, on a large, multi-lingual dataset on sentiment\nanalysis annotated with demographic information (location, age and gender). The\nresults show since larger and more complex models are more prone to leaking\nprivate information, use of privacy-preserving methods is highly desirable. We\nalso find that highly privacy-preserving technologies like differential privacy\n(DP) can have serious model utility effects, which can be ameliorated using\nhybrid or metric-DP techniques.",
        "pdf_link": "https://arxiv.org/pdf/2204.09391v1.pdf"
    },
    {
        "title": "Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification",
        "authors": [
            "Dawei Zhu",
            "Michael A. Hedderich",
            "Fangzhou Zhai",
            "David Ifeoluwa Adelani",
            "Dietrich Klakow"
        ],
        "published": "2022-04-20T10:24:19Z",
        "summary": "Incorrect labels in training data occur when human annotators make mistakes\nor when the data is generated via weak or distant supervision. It has been\nshown that complex noise-handling techniques - by modeling, cleaning or\nfiltering the noisy instances - are required to prevent models from fitting\nthis label noise. However, we show in this work that, for text classification\ntasks with modern NLP models like BERT, over a variety of noise types, existing\nnoisehandling methods do not always improve its performance, and may even\ndeteriorate it, suggesting the need for further investigation. We also back our\nobservations with a comprehensive analysis.",
        "pdf_link": "https://arxiv.org/pdf/2204.09371v1.pdf"
    },
    {
        "title": "Towards Arabic Sentence Simplification via Classification and Generative Approaches",
        "authors": [
            "Nouran Khallaf",
            "Serge Sharoff"
        ],
        "published": "2022-04-20T08:17:33Z",
        "summary": "This paper presents an attempt to build a Modern Standard Arabic (MSA)\nsentence-level simplification system. We experimented with sentence\nsimplification using two approaches: (i) a classification approach leading to\nlexical simplification pipelines which use Arabic-BERT, a pre-trained\ncontextualised model, as well as a model of fastText word embeddings; and (ii)\na generative approach, a Seq2Seq technique by applying a multilingual\nText-to-Text Transfer Transformer mT5. We developed our training corpus by\naligning the original and simplified sentences from the internationally\nacclaimed Arabic novel \"Saaq al-Bambuu\". We evaluate effectiveness of these\nmethods by comparing the generated simple sentences to the target simple\nsentences using the BERTScore evaluation metric. The simple sentences produced\nby the mT5 model achieve P 0.72, R 0.68 and F-1 0.70 via BERTScore, while,\ncombining Arabic-BERT and fastText achieves P 0.97, R 0.97 and F-1 0.97. In\naddition, we report a manual error analysis for these experiments.\n\\url{https://github.com/Nouran-Khallaf/Lexical_Simplification}",
        "pdf_link": "https://arxiv.org/pdf/2204.09292v1.pdf"
    },
    {
        "title": "On the Representation Collapse of Sparse Mixture of Experts",
        "authors": [
            "Zewen Chi",
            "Li Dong",
            "Shaohan Huang",
            "Damai Dai",
            "Shuming Ma",
            "Barun Patra",
            "Saksham Singhal",
            "Payal Bajaj",
            "Xia Song",
            "Xian-Ling Mao",
            "Heyan Huang",
            "Furu Wei"
        ],
        "published": "2022-04-20T01:40:19Z",
        "summary": "Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational overhead. It employs the routing mechanism to distribute\ninput tokens to the best-matched experts according to their hidden\nrepresentations. However, learning such a routing mechanism encourages token\nclustering around expert centroids, implying a trend toward representation\ncollapse. In this work, we propose to estimate the routing scores between\ntokens and experts on a low-dimensional hypersphere. We conduct extensive\nexperiments on cross-lingual language model pre-training and fine-tuning on\ndownstream tasks. Experimental results across seven multilingual benchmarks\nshow that our method achieves consistent gains. We also present a comprehensive\nanalysis on the representation and routing behaviors of our models. Our method\nalleviates the representation collapse issue and achieves more consistent\nrouting than the baseline mixture-of-experts methods.",
        "pdf_link": "https://arxiv.org/pdf/2204.09179v3.pdf"
    },
    {
        "title": "DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation",
        "authors": [
            "Md Rashad Al Hasan Rony",
            "Ricardo Usbeck",
            "Jens Lehmann"
        ],
        "published": "2022-04-19T22:26:18Z",
        "summary": "Task-oriented dialogue generation is challenging since the underlying\nknowledge is often dynamic and effectively incorporating knowledge into the\nlearning process is hard. It is particularly challenging to generate both\nhuman-like and informative responses in this setting. Recent research primarily\nfocused on various knowledge distillation methods where the underlying\nrelationship between the facts in a knowledge base is not effectively captured.\nIn this paper, we go one step further and demonstrate how the structural\ninformation of a knowledge graph can improve the system's inference\ncapabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue\nsystem that effectively incorporates knowledge into a language model. Our\nproposed system views relational knowledge as a knowledge graph and introduces\n(1) a structure-aware knowledge embedding technique, and (2) a knowledge\ngraph-weighted attention masking strategy to facilitate the system selecting\nrelevant information during the dialogue generation. An empirical evaluation\ndemonstrates the effectiveness of DialoKG over state-of-the-art methods on\nseveral standard benchmark datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.09149v1.pdf"
    },
    {
        "title": "What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment",
        "authors": [
            "Matthew Finlayson",
            "Kyle Richardson",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "published": "2022-04-19T22:11:47Z",
        "summary": "The instruction learning paradigm -- where a model learns to perform new\ntasks from task descriptions alone -- has become popular in general-purpose\nmodel research. The capabilities of large transformer models as instruction\nlearners, however, remain poorly understood. We use a controlled synthetic\nenvironment to characterize such capabilities. Specifically, we use the task of\ndeciding whether a given string matches a regular expression (viewed as an\ninstruction) to identify properties of tasks, instructions, and instances that\nmake instruction learning challenging. For instance, we find that our model, a\nfine-tuned T5-based text2text transformer, struggles with large regular\nlanguages, suggesting that less precise instructions are challenging for\nmodels. Additionally, instruction executions that require tracking longer\ncontexts of prior steps are also more difficult. We use our findings to\nsystematically construct a challenging instruction learning dataset, which we\ncall Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to\ncorrectly interpret only 65.6% of test instructions (with at least 90%\naccuracy), and 11%-24% of the instructions in out-of-distribution\ngeneralization settings. We propose Hard RegSet as a challenging instruction\nlearning task, and a controlled environment for studying instruction learning.",
        "pdf_link": "https://arxiv.org/pdf/2204.09148v2.pdf"
    },
    {
        "title": "ALBETO and DistilBETO: Lightweight Spanish Language Models",
        "authors": [
            "Jos\u00e9 Ca\u00f1ete",
            "Sebasti\u00e1n Donoso",
            "Felipe Bravo-Marquez",
            "Andr\u00e9s Carvallo",
            "Vladimir Araujo"
        ],
        "published": "2022-04-19T22:07:34Z",
        "summary": "In recent years there have been considerable advances in pre-trained language\nmodels, where non-English language versions have also been made available. Due\nto their increasing use, many lightweight versions of these models (with\nreduced parameters) have also been released to speed up training and inference\ntimes. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for\nlanguages other than English are still scarce. In this paper we present ALBETO\nand DistilBETO, which are versions of ALBERT and DistilBERT pre-trained\nexclusively on Spanish corpora. We train several versions of ALBETO ranging\nfrom 5M to 223M parameters and one of DistilBETO with 67M parameters. We\nevaluate our models in the GLUES benchmark that includes various natural\nlanguage understanding tasks in Spanish. The results show that our lightweight\nmodels achieve competitive results to those of BETO (Spanish-BERT) despite\nhaving fewer parameters. More specifically, our larger ALBETO model outperforms\nall other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.\nHowever, BETO remains unbeaten for POS and NER. As a further contribution, all\nmodels are publicly available to the community for future research.",
        "pdf_link": "https://arxiv.org/pdf/2204.09145v2.pdf"
    },
    {
        "title": "Optimize_Prime@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil",
        "authors": [
            "Shantanu Patankar",
            "Omkar Gokhale",
            "Onkar Litake",
            "Aditya Mandke",
            "Dipali Kadam"
        ],
        "published": "2022-04-19T18:55:18Z",
        "summary": "This paper tries to address the problem of abusive comment detection in\nlow-resource indic languages. Abusive comments are statements that are\noffensive to a person or a group of people. These comments are targeted toward\nindividuals belonging to specific ethnicities, genders, caste, race, sexuality,\netc. Abusive Comment Detection is a significant problem, especially with the\nrecent rise in social media users. This paper presents the approach used by our\nteam - Optimize_Prime, in the ACL 2022 shared task \"Abusive Comment Detection\nin Tamil.\" This task detects and classifies YouTube comments in Tamil and\nTamil- English Codemixed format into multiple categories. We have used three\nmethods to optimize our results: Ensemble models, Recurrent Neural Networks,\nand Transformers. In the Tamil data, MuRIL and XLM-RoBERTA were our best\nperforming models with a macro-averaged f1 score of 0.43. Furthermore, for the\nCode-mixed data, MuRIL and M-BERT provided sub-lime results, with a\nmacro-averaged f1 score of 0.45.",
        "pdf_link": "https://arxiv.org/pdf/2204.09675v1.pdf"
    },
    {
        "title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex",
        "authors": [
            "Immanuel Trummer"
        ],
        "published": "2022-04-19T15:19:35Z",
        "summary": "CodexDB is an SQL processing engine whose internals can be customized via\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\ndecomposes complex SQL queries into a series of simple processing steps,\ndescribed in natural language. Processing steps are enriched with user-provided\ninstructions and descriptions of database properties. Codex translates the\nresulting text into query processing code. An early prototype of CodexDB is\nable to generate correct code for a majority of queries of the WikiSQL\nbenchmark and can be customized in various ways.",
        "pdf_link": "https://arxiv.org/pdf/2204.08941v1.pdf"
    },
    {
        "title": "Impact of Tokenization on Language Models: An Analysis for Turkish",
        "authors": [
            "Cagri Toraman",
            "Eyup Halit Yilmaz",
            "Furkan \u015eahinu\u00e7",
            "Oguzhan Ozcelik"
        ],
        "published": "2022-04-19T12:01:46Z",
        "summary": "Tokenization is an important text preprocessing step to prepare input tokens\nfor deep language models. WordPiece and BPE are de facto methods employed by\nimportant models, such as BERT and GPT. However, the impact of tokenization can\nbe different for morphologically rich languages, such as Turkic languages,\nwhere many words can be generated by adding prefixes and suffixes. We compare\nfive tokenizers at different granularity levels, i.e. their outputs vary from\nsmallest pieces of characters to the surface form of words, including a\nMorphological-level tokenizer. We train these tokenizers and pretrain\nmedium-sized language models using RoBERTa pretraining procedure on the Turkish\nsplit of the OSCAR corpus. We then fine-tune our models on six downstream\ntasks. Our experiments, supported by statistical tests, reveal that\nMorphological-level tokenizer has challenging performance with de facto\ntokenizers. Furthermore, we find that increasing the vocabulary size improves\nthe performance of Morphological and Word-level tokenizers more than that of de\nfacto tokenizers. The ratio of the number of vocabulary parameters to the total\nnumber of model parameters can be empirically chosen as 20% for de facto\ntokenizers and 40% for other tokenizers to obtain a reasonable trade-off\nbetween model size and performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.08832v1.pdf"
    },
    {
        "title": "Probing for the Usage of Grammatical Number",
        "authors": [
            "Karim Lasri",
            "Tiago Pimentel",
            "Alessandro Lenci",
            "Thierry Poibeau",
            "Ryan Cotterell"
        ],
        "published": "2022-04-19T11:59:52Z",
        "summary": "A central quest of probing is to uncover how pre-trained models encode a\nlinguistic property within their representations. An encoding, however, might\nbe spurious-i.e., the model might not rely on it when making predictions. In\nthis paper, we try to find encodings that the model actually uses, introducing\na usage-based probing setup. We first choose a behavioral task which cannot be\nsolved without using the linguistic property. Then, we attempt to remove the\nproperty by intervening on the model's representations. We contend that, if an\nencoding is used by the model, its removal should harm the performance on the\nchosen behavioral task. As a case study, we focus on how BERT encodes\ngrammatical number, and on how it uses this encoding to solve the number\nagreement task. Experimentally, we find that BERT relies on a linear encoding\nof grammatical number to produce the correct behavioral output. We also find\nthat BERT uses a separate encoding of grammatical number for nouns and verbs.\nFinally, we identify in which layers information about grammatical number is\ntransferred from a noun to its head verb.",
        "pdf_link": "https://arxiv.org/pdf/2204.08831v3.pdf"
    },
    {
        "title": "Multimodal Hate Speech Detection from Bengali Memes and Texts",
        "authors": [
            "Md. Rezaul Karim",
            "Sumon Kanti Dey",
            "Tanhim Islam",
            "Md. Shajalal",
            "Bharathi Raja Chakravarthi"
        ],
        "published": "2022-04-19T11:15:25Z",
        "summary": "Numerous machine learning (ML) and deep learning (DL)-based approaches have\nbeen proposed to utilize textual data from social media for anti-social\nbehavior analysis like cyberbullying, fake news detection, and identification\nof hate speech mainly for highly-resourced languages such as English. However,\ndespite having a lot of diversity and millions of native speakers, some\nlanguages like Bengali are under-resourced, which is due to a lack of\ncomputational resources for natural language processing (NLP). Similar to other\nlanguages, Bengali social media contents also include images along with texts\n(e.g., multimodal memes are posted by embedding short texts into images on\nFacebook). Therefore, only the textual data is not enough to judge them since\nimages might give extra context to make a proper judgement. This paper is about\nhate speech detection from multimodal Bengali memes and texts. We prepared the\nonly multimodal hate speech dataset for-a-kind of problem for Bengali, which we\nuse to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM\nwith word embeddings, ConvNets + pre-trained language models, e.g., monolingual\nBangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly\nanalyze textual and visual information for hate speech detection. Conv-LSTM and\nXLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and\n0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1\nscores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +\nDenseNet-161 performed the best, yielding an F1 score of 0.83. Our study\nsuggests that text modality is most useful for hate speech detection, while\nmemes are moderately useful.",
        "pdf_link": "https://arxiv.org/pdf/2204.10196v3.pdf"
    },
    {
        "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks",
        "authors": [
            "Ziyang Luo",
            "Yadong Xi",
            "Jing Ma",
            "Zhiwei Yang",
            "Xiaoxi Mao",
            "Changjie Fan",
            "Rongsheng Zhang"
        ],
        "published": "2022-04-19T06:12:48Z",
        "summary": "Since 2017, the Transformer-based models play critical roles in various\ndownstream Natural Language Processing tasks. However, a common limitation of\nthe attention mechanism utilized in Transformer Encoder is that it cannot\nautomatically capture the information of word order, so explicit position\nembeddings are generally required to be fed into the target model. In contrast,\nTransformer Decoder with the causal attention masks is naturally sensitive to\nthe word order. In this work, we focus on improving the position encoding\nability of BERT with the causal attention masks. Furthermore, we propose a new\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\nExperimental results show that (1) the causal attention mask is effective for\nBERT on the language understanding tasks; (2) our DecBERT model without\nposition embeddings achieve comparable performance on the GLUE benchmark; and\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\nachieves better overall performance than the baseline systems when pre-training\nwith the same amount of computational resources.",
        "pdf_link": "https://arxiv.org/pdf/2204.08688v1.pdf"
    },
    {
        "title": "Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi",
        "authors": [
            "Abhishek Velankar",
            "Hrushikesh Patil",
            "Raviraj Joshi"
        ],
        "published": "2022-04-19T05:07:58Z",
        "summary": "Transformers are the most eminent architectures used for a vast range of\nNatural Language Processing tasks. These models are pre-trained over a large\ntext corpus and are meant to serve state-of-the-art results over tasks like\ntext classification. In this work, we conduct a comparative study between\nmonolingual and multilingual BERT models. We focus on the Marathi language and\nevaluate the models on the datasets for hate speech detection, sentiment\nanalysis and simple text classification in Marathi. We use standard\nmultilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with\nMahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We\nfurther show that Marathi monolingual models outperform the multilingual BERT\nvariants on five different downstream fine-tuning experiments. We also evaluate\nsentence embeddings from these models by freezing the BERT encoder layers. We\nshow that monolingual MahaBERT based models provide rich representations as\ncompared to sentence embeddings from multi-lingual counterparts. However, we\nobserve that these embeddings are not generic enough and do not work well on\nout of domain social media datasets. We consider two Marathi hate speech\ndatasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification\ndataset L3Cube-MahaSent, and Marathi Headline, Articles classification\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.08669v1.pdf"
    },
    {
        "title": "LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation",
        "authors": [
            "Qingyu Chen",
            "Jingcheng Du",
            "Alexis Allot",
            "Zhiyong Lu"
        ],
        "published": "2022-04-19T04:03:45Z",
        "summary": "The rapid growth of biomedical literature poses a significant challenge for\ncuration and interpretation. This has become more evident during the COVID-19\npandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,\nhas accumulated over 180,000 articles with millions of accesses. Approximately\n10,000 new articles are added to LitCovid every month. A main curation task in\nLitCovid is topic annotation where an article is assigned with up to eight\ntopics, e.g., Treatment and Diagnosis. The annotated topics have been widely\nused both in LitCovid (e.g., accounting for ~18% of total uses) and downstream\nstudies such as network generation. However, it has been a primary curation\nbottleneck due to the nature of the task and the rapid literature growth. This\nstudy proposes LITMC-BERT, a transformer-based multi-label classification\nmethod in biomedical literature. It uses a shared transformer backbone for all\nthe labels while also captures label-specific features and the correlations\nbetween label pairs. We compare LITMC-BERT with three baseline models on two\ndatasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the\ncurrent best results, respectively, and only requires ~18% of the inference\ntime than the Binary BERT baseline. The related datasets and models are\navailable via https://github.com/ncbi/ml-transformer.",
        "pdf_link": "https://arxiv.org/pdf/2204.08649v1.pdf"
    },
    {
        "title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
        "authors": [
            "Charlie Snell",
            "Mengjiao Yang",
            "Justin Fu",
            "Yi Su",
            "Sergey Levine"
        ],
        "published": "2022-04-18T17:23:11Z",
        "summary": "Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.10198v2.pdf"
    },
    {
        "title": "Zero-shot Entity and Tweet Characterization with Designed Conditional Prompts and Contexts",
        "authors": [
            "Sharath Srivatsa",
            "Tushar Mohan",
            "Kumari Neha",
            "Nishchay Malakar",
            "Ponnurangam Kumaraguru",
            "Srinath Srinivasa"
        ],
        "published": "2022-04-18T17:01:49Z",
        "summary": "Online news and social media have been the de facto mediums to disseminate\ninformation globally from the beginning of the last decade. However, bias in\ncontent and purpose of intentions are not regulated, and managing bias is the\nresponsibility of content consumers. In this regard, understanding the stances\nand biases of news sources towards specific entities becomes important. To\naddress this problem, we use pretrained language models, which have been shown\nto bring about good results with no task-specific training or few-shot\ntraining. In this work, we approach the problem of characterizing Named\nEntities and Tweets as an open-ended text classification and open-ended fact\nprobing problem.We evaluate the zero-shot language model capabilities of\nGenerative Pretrained Transformer 2 (GPT-2) to characterize Entities and Tweets\nsubjectively with human psychology-inspired and logical conditional prefixes\nand contexts. First, we fine-tune the GPT-2 model on a sufficiently large news\ncorpus and evaluate subjective characterization of popular entities in the\ncorpus by priming with prefixes. Second, we fine-tune GPT-2 with a Tweets\ncorpus from a few popular hashtags and evaluate characterizing tweets by\npriming the language model with prefixes, questions, and contextual synopsis\nprompts. Entity characterization results were positive across measures and\nhuman evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2204.08405v1.pdf"
    },
    {
        "title": "L3Cube-HingCorpus and HingBERT: A Code Mixed Hindi-English Dataset and BERT Language Models",
        "authors": [
            "Ravindra Nayak",
            "Raviraj Joshi"
        ],
        "published": "2022-04-18T16:49:59Z",
        "summary": "Code-switching occurs when more than one language is mixed in a given\nsentence or a conversation. This phenomenon is more prominent on social media\nplatforms and its adoption is increasing over time. Therefore code-mixed NLP\nhas been extensively studied in the literature. As pre-trained\ntransformer-based architectures are gaining popularity, we observe that real\ncode-mixing data are scarce to pre-train large language models. We present\nL3Cube-HingCorpus, the first large-scale real Hindi-English code mixed data in\na Roman script. It consists of 52.93M sentences and 1.04B tokens, scraped from\nTwitter. We further present HingBERT, HingMBERT, HingRoBERTa, and HingGPT. The\nBERT models have been pre-trained on codemixed HingCorpus using masked language\nmodelling objectives. We show the effectiveness of these BERT models on the\nsubsequent downstream tasks like code-mixed sentiment analysis, POS tagging,\nNER, and LID from the GLUECoS benchmark. The HingGPT is a GPT2 based generative\ntransformer model capable of generating full tweets. We also release\nL3Cube-HingLID Corpus, the largest code-mixed Hindi-English language\nidentification(LID) dataset and HingBERT-LID, a production-quality LID model to\nfacilitate capturing of more code-mixed data using the process outlined in this\nwork. The dataset and models are available at\nhttps://github.com/l3cube-pune/code-mixed-nlp .",
        "pdf_link": "https://arxiv.org/pdf/2204.08398v1.pdf"
    },
    {
        "title": "UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language",
        "authors": [
            "David Koleczek",
            "Alex Scarlatos",
            "Siddha Karakare",
            "Preshma Linet Pereira"
        ],
        "published": "2022-04-18T13:22:10Z",
        "summary": "Patronizing and condescending language (PCL) is everywhere, but rarely is the\nfocus on its use by media towards vulnerable communities. Accurately detecting\nPCL of this form is a difficult task due to limited labeled data and how subtle\nit can be. In this paper, we describe our system for detecting such language\nwhich was submitted to SemEval 2022 Task 4: Patronizing and Condescending\nLanguage Detection. Our approach uses an ensemble of pre-trained language\nmodels, data augmentation, and optimizing the threshold for detection.\nExperimental results on the evaluation dataset released by the competition\nhosts show that our work is reliably able to detect PCL, achieving an F1 score\nof 55.47% on the binary classification task and a macro F1 score of 36.25% on\nthe fine-grained, multi-label detection task.",
        "pdf_link": "https://arxiv.org/pdf/2204.08304v1.pdf"
    },
    {
        "title": "Ingredient Extraction from Text in the Recipe Domain",
        "authors": [
            "Arkin Dharawat",
            "Chris Doan"
        ],
        "published": "2022-04-18T02:54:54Z",
        "summary": "In recent years, there has been an increase in the number of devices with\nvirtual assistants (e.g: Siri, Google Home, Alexa) in our living rooms and\nkitchens. As a result of this, these devices receive several queries about\nrecipes. All these queries will contain terms relating to a \"recipe-domain\"\ni.e: they will contain dish-names, ingredients, cooking times, dietary\npreferences etc. Extracting these recipe-relevant aspects from the query thus\nbecomes important when it comes to addressing the user's information need. Our\nproject focuses on extracting ingredients from such plain-text user utterances.\nOur best performing model was a fine-tuned BERT which achieved an F1-score of\n$95.01$. We have released all our code in a GitHub repository.",
        "pdf_link": "https://arxiv.org/pdf/2204.08137v1.pdf"
    },
    {
        "title": "Pathologies of Pre-trained Language Models in Few-shot Fine-tuning",
        "authors": [
            "Hanjie Chen",
            "Guoqing Zheng",
            "Ahmed Hassan Awadallah",
            "Yangfeng Ji"
        ],
        "published": "2022-04-17T15:55:18Z",
        "summary": "Although adapting pre-trained language models with few examples has shown\npromising performance on text classification, there is a lack of understanding\nof where the performance gain comes from. In this work, we propose to answer\nthis question by interpreting the adaptation behavior using post-hoc\nexplanations from model predictions. By modeling feature statistics of\nexplanations, we discover that (1) without fine-tuning, pre-trained models\n(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although\nfew-shot fine-tuning can mitigate the prediction bias and demonstrate promising\nprediction performance, our analysis shows models gain performance improvement\nby capturing non-task-related features (e.g. stop words) or shallow data\npatterns (e.g. lexical overlaps). These observations alert that pursuing model\nperformance with fewer examples may incur pathological prediction behavior,\nwhich requires further sanity check on model predictions and careful design in\nmodel evaluations in few-shot fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2204.08039v1.pdf"
    },
    {
        "title": "Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base",
        "authors": [
            "Cunxiang Wang",
            "Fuli Luo",
            "Yanyang Li",
            "Runxin Xu",
            "Fei Huang",
            "Yue Zhang"
        ],
        "published": "2022-04-17T12:33:34Z",
        "summary": "Pre-trained language models (PLMs) like BERT have made significant progress\nin various downstream NLP tasks. However, by asking models to do cloze-style\ntests, recent work finds that PLMs are short in acquiring knowledge from\nunstructured text. To understand the internal behaviour of PLMs in retrieving\nknowledge, we first define knowledge-baring (K-B) tokens and knowledge-free\n(K-F) tokens for unstructured text and ask professional annotators to label\nsome samples manually. Then, we find that PLMs are more likely to give wrong\npredictions on K-B tokens and attend less attention to those tokens inside the\nself-attention module. Based on these observations, we develop two solutions to\nhelp the model learn more knowledge from unstructured text in a fully\nself-supervised manner. Experiments on knowledge-intensive tasks show the\neffectiveness of the proposed methods. To our best knowledge, we are the first\nto explore fully self-supervised learning of knowledge in continual\npre-training.",
        "pdf_link": "https://arxiv.org/pdf/2204.07994v2.pdf"
    },
    {
        "title": "nigam@COLIEE-22: Legal Case Retrieval and Entailment using Cascading of Lexical and Semantic-based models",
        "authors": [
            "Shubham Kumar Nigam",
            "Navansh Goel"
        ],
        "published": "2022-04-16T18:10:02Z",
        "summary": "This paper describes our submission to the Competition on Legal Information\nExtraction/Entailment 2022 (COLIEE-2022) workshop on case law competition for\ntasks 1 and 2. Task 1 is a legal case retrieval task, which involves reading a\nnew case and extracting supporting cases from the provided case law corpus to\nsupport the decision. Task 2 is the legal case entailment task, which involves\nthe identification of a paragraph from existing cases that entails the decision\nin a relevant case. We employed the neural models Sentence-BERT and Sent2Vec\nfor semantic understanding and the traditional retrieval model BM25 for exact\nmatching in both tasks. As a result, our team (\"nigam\") ranked 5th among all\nthe teams in Tasks 1 and 2. Experimental results indicate that the traditional\nretrieval model BM25 still outperforms neural network-based models.",
        "pdf_link": "https://arxiv.org/pdf/2204.07853v1.pdf"
    },
    {
        "title": "A Contrastive Cross-Channel Data Augmentation Framework for Aspect-based Sentiment Analysis",
        "authors": [
            "Bing Wang",
            "Liang Ding",
            "Qihuang Zhong",
            "Ximing Li",
            "Dacheng Tao"
        ],
        "published": "2022-04-16T16:05:58Z",
        "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask, which focuses on detecting the sentiment polarity towards the aspect in a\nsentence. However, it is always sensitive to the multi-aspect challenge, where\nfeatures of multiple aspects in a sentence will affect each other. To mitigate\nthis issue, we design a novel training framework, called Contrastive\nCross-Channel Data Augmentation (C3 DA), which leverages an in-domain generator\nto construct more multi-aspect samples and then boosts the robustness of ABSA\nmodels via contrastive learning on these generated data. In practice, given a\ngenerative pretrained language model and some limited ABSA labeled data, we\nfirst employ some parameter-efficient approaches to perform the in-domain\nfine-tuning. Then, the obtained in-domain generator is used to generate the\nsynthetic sentences from two channels, i.e., Aspect Augmentation Channel and\nPolarity Augmentation Channel, which generate the sentence condition on a given\naspect and polarity respectively. Specifically, our C3 DA performs the sentence\ngeneration in a cross-channel manner to obtain more sentences, and proposes an\nEntropy-Minimization Filter to filter low-quality generated samples. Extensive\nexperiments show that our C3 DA can outperform those baselines without any\naugmentations by about 1% on accuracy and Macro- F1. Code and data are released\nin https://github.com/wangbing1416/C3DA.",
        "pdf_link": "https://arxiv.org/pdf/2204.07832v2.pdf"
    },
    {
        "title": "Contrastive Learning with Hard Negative Entities for Entity Set Expansion",
        "authors": [
            "Yinghui Li",
            "Yangning Li",
            "Yuxin He",
            "Tianyu Yu",
            "Ying Shen",
            "Hai-Tao Zheng"
        ],
        "published": "2022-04-16T12:26:42Z",
        "summary": "Entity Set Expansion (ESE) is a promising task which aims to expand entities\nof the target semantic class described by a small seed entity set. Various NLP\nand IR applications will benefit from ESE due to its ability to discover\nknowledge. Although previous ESE methods have achieved great progress, most of\nthem still lack the ability to handle hard negative entities (i.e., entities\nthat are difficult to distinguish from the target entities), since two entities\nmay or may not belong to the same semantic class based on different granularity\nlevels we analyze on. To address this challenge, we devise an entity-level\nmasked language model with contrastive learning to refine the representation of\nentities. In addition, we propose the ProbExpan, a novel probabilistic ESE\nframework utilizing the entity representation obtained by the aforementioned\nlanguage model to expand entities. Extensive experiments and detailed analyses\non three datasets show that our method outperforms previous state-of-the-art\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2204.07789v2.pdf"
    },
    {
        "title": "WordAlchemy: A transformer-based Reverse Dictionary",
        "authors": [
            "Dr. Sunil B. Mane",
            "Harshal Patil",
            "Kanhaiya Madaswar",
            "Pranav Sadavarte"
        ],
        "published": "2022-04-16T11:41:48Z",
        "summary": "A reverse dictionary takes a target word's description as input and returns\nthe words that fit the description. Reverse Dictionaries are useful for new\nlanguage learners, anomia patients, and for solving common tip-of-the-tongue\nproblems (lethologica). Currently, there does not exist any Reverse Dictionary\nprovider with support for any Indian Language. We present a novel open-source\ncross-lingual reverse dictionary system with support for Indian languages. In\nthis paper, we propose a transformer-based deep learning approach to tackle the\nlimitations faced by the existing systems using the mT5 model. This\narchitecture uses the Translation Language Modeling (TLM) technique, rather\nthan the conventional BERT's Masked Language Modeling (MLM) technique.",
        "pdf_link": "https://arxiv.org/pdf/2204.10181v1.pdf"
    },
    {
        "title": "SimpleBERT: A Pre-trained Model That Learns to Generate Simple Words",
        "authors": [
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2022-04-16T11:28:01Z",
        "summary": "Pre-trained models are widely used in the tasks of natural language\nprocessing nowadays. However, in the specific field of text simplification, the\nresearch on improving pre-trained models is still blank. In this work, we\npropose a continued pre-training method for text simplification. Specifically,\nwe propose a new masked language modeling (MLM) mechanism, which does not\nrandomly mask words but only masks simple words. The new mechanism can make the\nmodel learn to generate simple words. We use a small-scale simple text dataset\nfor continued pre-training and employ two methods to identify simple words from\nthe texts. We choose BERT, a representative pre-trained model, and continue\npre-training it using our proposed method. Finally, we obtain SimpleBERT, which\nsurpasses BERT in both lexical simplification and sentence simplification tasks\nand has achieved state-of-the-art results on multiple datasets. What's more,\nSimpleBERT can replace BERT in existing simplification models without\nmodification.",
        "pdf_link": "https://arxiv.org/pdf/2204.07779v1.pdf"
    },
    {
        "title": "Probing Script Knowledge from Pre-Trained Models",
        "authors": [
            "Zijian Jin",
            "Xingyu Zhang",
            "Mo Yu",
            "Lifu Huang"
        ],
        "published": "2022-04-16T05:13:39Z",
        "summary": "Script knowledge is critical for humans to understand the broad daily tasks\nand routine activities in the world. Recently researchers have explored the\nlarge-scale pre-trained language models (PLMs) to perform various script\nrelated tasks, such as story generation, temporal ordering of event, future\nevent prediction and so on. However, it's still not well studied in terms of\nhow well the PLMs capture the script knowledge. To answer this question, we\ndesign three probing tasks: inclusive sub-event selection, starting sub-event\nselection and temporal ordering to investigate the capabilities of PLMs with\nand without fine-tuning. The three probing tasks can be further used to\nautomatically induce a script for each main event given all the possible\nsub-events. Taking BERT as a case study, by analyzing its performance on script\ninduction as well as each individual probing task, we conclude that the\nstereotypical temporal knowledge among the sub-events is well captured in BERT,\nhowever the inclusive or starting sub-event knowledge is barely encoded.",
        "pdf_link": "https://arxiv.org/pdf/2204.10176v1.pdf"
    },
    {
        "title": "BLCU-ICALL at SemEval-2022 Task 1: Cross-Attention Multitasking Framework for Definition Modeling",
        "authors": [
            "Cunliang Kong",
            "Yujie Wang",
            "Ruining Chong",
            "Liner Yang",
            "Hengyuan Zhang",
            "Erhong Yang",
            "Yaping Huang"
        ],
        "published": "2022-04-16T02:33:28Z",
        "summary": "This paper describes the BLCU-ICALL system used in the SemEval-2022 Task 1\nComparing Dictionaries and Word Embeddings, the Definition Modeling subtrack,\nachieving 1st on Italian, 2nd on Spanish and Russian, and 3rd on English and\nFrench. We propose a transformer-based multitasking framework to explore the\ntask. The framework integrates multiple embedding architectures through the\ncross-attention mechanism, and captures the structure of glosses through a\nmasking language model objective. Additionally, we also investigate a simple\nbut effective model ensembling strategy to further improve the robustness. The\nevaluation results show the effectiveness of our solution. We release our code\nat: https://github.com/blcuicall/SemEval2022-Task1-DM.",
        "pdf_link": "https://arxiv.org/pdf/2204.07701v1.pdf"
    },
    {
        "title": "DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling",
        "authors": [
            "Lahari Poddar",
            "Peiyao Wang",
            "Julia Reinspach"
        ],
        "published": "2022-04-15T23:39:41Z",
        "summary": "Retrieval-based conversational systems learn to rank response candidates for\na given dialogue context by computing the similarity between their vector\nrepresentations. However, training on a single textual form of the multi-turn\ncontext limits the ability of a model to learn representations that generalize\nto natural perturbations seen during inference. In this paper we propose a\nframework that incorporates augmented versions of a dialogue context into the\nlearning objective. We utilize contrastive learning as an auxiliary objective\nto learn robust dialogue context representations that are invariant to\nperturbations injected through the augmentation method. We experiment with four\nbenchmark dialogue datasets and demonstrate that our framework combines well\nwith existing augmentation methods and can significantly improve over baseline\nBERT-based ranking architectures. Furthermore, we propose a novel data\naugmentation method, ConMix, that adds token level perturbations through\nstochastic mixing of tokens from other contexts in the batch. We show that our\nproposed augmentation method outperforms previous data augmentation approaches,\nand provides dialogue representations that are more robust to common\nperturbations seen during inference.",
        "pdf_link": "https://arxiv.org/pdf/2204.07679v1.pdf"
    },
    {
        "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation",
        "authors": [
            "Simiao Zuo",
            "Qingru Zhang",
            "Chen Liang",
            "Pengcheng He",
            "Tuo Zhao",
            "Weizhu Chen"
        ],
        "published": "2022-04-15T23:19:37Z",
        "summary": "Pre-trained language models have demonstrated superior performance in various\nnatural language processing tasks. However, these models usually contain\nhundreds of millions of parameters, which limits their practicality because of\nlatency requirements in real-world applications. Existing methods train small\ncompressed models via knowledge distillation. However, performance of these\nsmall models drops significantly compared with the pre-trained models due to\ntheir reduced model capacity. We propose MoEBERT, which uses a\nMixture-of-Experts structure to increase model capacity and inference speed. We\ninitialize MoEBERT by adapting the feed-forward neural networks in a\npre-trained model into multiple experts. As such, representation power of the\npre-trained model is largely retained. During inference, only one of the\nexperts is activated, such that speed can be improved. We also propose a\nlayer-wise distillation method to train MoEBERT. We validate the efficiency and\neffectiveness of MoEBERT on natural language understanding and question\nanswering tasks. Results show that the proposed method outperforms existing\ntask-specific distillation algorithms. For example, our method outperforms\nprevious approaches by over 2% on the MNLI (mismatched) dataset. Our code is\npublicly available at https://github.com/SimiaoZuo/MoEBERT.",
        "pdf_link": "https://arxiv.org/pdf/2204.07675v2.pdf"
    },
    {
        "title": "CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation",
        "authors": [
            "Md Akmal Haidar",
            "Mehdi Rezagholizadeh",
            "Abbas Ghaddar",
            "Khalil Bibi",
            "Philippe Langlais",
            "Pascal Poupart"
        ],
        "published": "2022-04-15T23:16:37Z",
        "summary": "Knowledge distillation (KD) is an efficient framework for compressing\nlarge-scale pre-trained language models. Recent years have seen a surge of\nresearch aiming to improve KD by leveraging Contrastive Learning, Intermediate\nLayer Distillation, Data Augmentation, and Adversarial Training. In this work,\nwe propose a learning based data augmentation technique tailored for knowledge\ndistillation, called CILDA. To the best of our knowledge, this is the first\ntime that intermediate layer representations of the main task are used in\nimproving the quality of augmented samples. More precisely, we introduce an\naugmentation technique for KD based on intermediate layer matching using\ncontrastive loss to improve masked adversarial data augmentation. CILDA\noutperforms existing state-of-the-art KD approaches on the GLUE benchmark, as\nwell as in an out-of-domain evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2204.07674v1.pdf"
    },
    {
        "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
        "authors": [
            "Weiyan Shi",
            "Ryan Shea",
            "Si Chen",
            "Chiyuan Zhang",
            "Ruoxi Jia",
            "Zhou Yu"
        ],
        "published": "2022-04-15T22:36:55Z",
        "summary": "Protecting large language models from privacy leakage is becoming\nincreasingly crucial with their wide adoption in real-world products. Yet\napplying differential privacy (DP), a canonical notion with provable privacy\nguarantees for machine learning models, to those models remains challenging due\nto the trade-off between model utility and privacy loss. Utilizing the fact\nthat sensitive information in language data tends to be sparse, Shi et al.\n(2021) formalized a DP notion extension called Selective Differential Privacy\n(SDP) to protect only the sensitive tokens defined by a policy function.\nHowever, their algorithm only works for RNN-based models. In this paper, we\ndevelop a novel framework, Just Fine-tune Twice (JFT), that achieves SDP for\nstate-of-the-art large transformer-based models. Our method is easy to\nimplement: it first fine-tunes the model with redacted in-domain data, and then\nfine-tunes it again with the original in-domain data using a private training\nmechanism. Furthermore, we study the scenario of imperfect implementation of\npolicy functions that misses sensitive tokens and develop systematic methods to\nhandle it. Experiments show that our method achieves strong utility compared to\nprevious baselines. We also analyze the SDP privacy guarantee empirically with\nthe canary insertion attack.",
        "pdf_link": "https://arxiv.org/pdf/2204.07667v3.pdf"
    },
    {
        "title": "Semantic Structure based Query Graph Prediction for Question Answering over Knowledge Graph",
        "authors": [
            "Mingchen Li",
            "Shihao Ji"
        ],
        "published": "2022-04-15T20:35:00Z",
        "summary": "Building query graphs from natural language questions is an important step in\ncomplex question answering over knowledge graph (Complex KGQA). In general, a\nquestion can be correctly answered if its query graph is built correctly and\nthe right answer is then retrieved by issuing the query graph against the KG.\nTherefore, this paper focuses on query graph generation from natural language\nquestions. Existing approaches for query graph generation ignore the semantic\nstructure of a question, resulting in a large number of noisy query graph\ncandidates that undermine prediction accuracies. In this paper, we define six\nsemantic structures from common questions in KGQA and develop a novel\nStructure-BERT to predict the semantic structure of a question. By doing so, we\ncan first filter out noisy candidate query graphs, and then rank the remaining\ncandidates with a BERT-based ranking model. Extensive experiments on two\npopular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the\neffectiveness of our method as compared to state-of-the-arts.",
        "pdf_link": "https://arxiv.org/pdf/2204.10194v6.pdf"
    },
    {
        "title": "Evaluation Benchmarks for Spanish Sentence Representations",
        "authors": [
            "Vladimir Araujo",
            "Andr\u00e9s Carvallo",
            "Souvik Kundu",
            "Jos\u00e9 Ca\u00f1ete",
            "Marcelo Mendoza",
            "Robert E. Mercer",
            "Felipe Bravo-Marquez",
            "Marie-Francine Moens",
            "Alvaro Soto"
        ],
        "published": "2022-04-15T17:53:05Z",
        "summary": "Due to the success of pre-trained language models, versions of languages\nother than English have been released in recent years. This fact implies the\nneed for resources to evaluate these models. In the case of Spanish, there are\nfew ways to systematically assess the models' quality. In this paper, we narrow\nthe gap by building two evaluation benchmarks. Inspired by previous work\n(Conneau and Kiela, 2018; Chen et al., 2019), we introduce Spanish SentEval and\nSpanish DiscoEval, aiming to assess the capabilities of stand-alone and\ndiscourse-aware sentence representations, respectively. Our benchmarks include\nconsiderable pre-existing and newly constructed datasets that address different\ntasks from various domains. In addition, we evaluate and analyze the most\nrecent pre-trained Spanish language models to exhibit their capabilities and\nlimitations. As an example, we discover that for the case of discourse\nevaluation tasks, mBERT, a language model trained on multiple languages,\nusually provides a richer latent representation than models trained only with\ndocuments in Spanish. We hope our contribution will motivate a fairer, more\ncomparable, and less cumbersome way to evaluate future Spanish language models.",
        "pdf_link": "https://arxiv.org/pdf/2204.07571v1.pdf"
    },
    {
        "title": "Improving Passage Retrieval with Zero-Shot Question Generation",
        "authors": [
            "Devendra Singh Sachan",
            "Mike Lewis",
            "Mandar Joshi",
            "Armen Aghajanyan",
            "Wen-tau Yih",
            "Joelle Pineau",
            "Luke Zettlemoyer"
        ],
        "published": "2022-04-15T14:51:41Z",
        "summary": "We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.",
        "pdf_link": "https://arxiv.org/pdf/2204.07496v4.pdf"
    },
    {
        "title": "Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models",
        "authors": [
            "Philip Feldman",
            "Aaron Dant",
            "James R. Foulds",
            "Shemei Pan"
        ],
        "published": "2022-04-15T14:33:58Z",
        "summary": "Text analysis of social media for sentiment, topic analysis, and other\nanalysis depends initially on the selection of keywords and phrases that will\nbe used to create the research corpora. However, keywords that researchers\nchoose may occur infrequently, leading to errors that arise from using small\nsamples. In this paper, we use the capacity for memorization, interpolation,\nand extrapolation of Transformer Language Models such as the GPT series to\nlearn the linguistic behaviors of a subgroup within larger corpora of Yelp\nreviews. We then use prompt-based queries to generate synthetic text that can\nbe analyzed to produce insights into specific opinions held by the populations\nthat the models were trained on. Once learned, more specific sentiment queries\ncan be made of the model with high levels of accuracy when compared to\ntraditional keyword searches. We show that even in cases where a specific\nkeyphrase is limited or not present at all in the training corpora, the GPT is\nable to accurately generate large volumes of text that have the correct\nsentiment.",
        "pdf_link": "https://arxiv.org/pdf/2204.07483v2.pdf"
    },
    {
        "title": "Improving Pre-trained Language Models with Syntactic Dependency Prediction Task for Chinese Semantic Error Recognition",
        "authors": [
            "Bo Sun",
            "Baoxin Wang",
            "Wanxiang Che",
            "Dayong Wu",
            "Zhigang Chen",
            "Ting Liu"
        ],
        "published": "2022-04-15T13:55:32Z",
        "summary": "Existing Chinese text error detection mainly focuses on spelling and simple\ngrammatical errors. These errors have been studied extensively and are\nrelatively simple for humans. On the contrary, Chinese semantic errors are\nunderstudied and more complex that humans cannot easily recognize. The task of\nthis paper is Chinese Semantic Error Recognition (CSER), a binary\nclassification task to determine whether a sentence contains semantic errors.\nThe current research has no effective method to solve this task. In this paper,\nwe inherit the model structure of BERT and design several syntax-related\npre-training tasks so that the model can learn syntactic knowledge. Our\npre-training tasks consider both the directionality of the dependency structure\nand the diversity of the dependency relationship. Due to the lack of a\npublished dataset for CSER, we build a high-quality dataset for CSER for the\nfirst time named Corpus of Chinese Linguistic Semantic Acceptability (CoCLSA).\nThe experimental results on the CoCLSA show that our methods outperform\nuniversal pre-trained models and syntax-infused models.",
        "pdf_link": "https://arxiv.org/pdf/2204.07464v1.pdf"
    },
    {
        "title": "mGPT: Few-Shot Learners Go Multilingual",
        "authors": [
            "Oleh Shliazhko",
            "Alena Fenogenova",
            "Maria Tikhonova",
            "Vladislav Mikhailov",
            "Anastasia Kozlova",
            "Tatiana Shavrina"
        ],
        "published": "2022-04-15T13:02:33Z",
        "summary": "Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2204.07580v2.pdf"
    },
    {
        "title": "ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language",
        "authors": [
            "Tosin Adewumi",
            "Lama Alkhaled",
            "Hamam Mokayed",
            "Foteini Liwicki",
            "Marcus Liwicki"
        ],
        "published": "2022-04-15T12:00:25Z",
        "summary": "This paper describes the system used by the Machine Learning Group of LTU in\nsubtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language\n(PCL) Detection. Our system consists of finetuning a pretrained\nText-to-Text-Transfer Transformer (T5) and innovatively reducing its\nout-of-class predictions. The main contributions of this paper are 1) the\ndescription of the implementation details of the T5 model we used, 2) analysis\nof the successes & struggles of the model in this task, and 3) ablation studies\nbeyond the official submission to ascertain the relative importance of data\nsplit. Our model achieves an F1 score of 0.5452 on the official test set.",
        "pdf_link": "https://arxiv.org/pdf/2204.07432v2.pdf"
    },
    {
        "title": "Is Surprisal in Issue Trackers Actionable?",
        "authors": [
            "James Caddy",
            "Markus Wagner",
            "Christoph Treude",
            "Earl T. Barr",
            "Miltiadis Allamanis"
        ],
        "published": "2022-04-15T07:49:40Z",
        "summary": "Background. From information theory, surprisal is a measurement of how\nunexpected an event is. Statistical language models provide a probabilistic\napproximation of natural languages, and because surprisal is constructed with\nthe probability of an event occuring, it is therefore possible to determine the\nsurprisal associated with English sentences. The issues and pull requests of\nsoftware repository issue trackers give insight into the development process\nand likely contain the surprising events of this process.\n  Objective. Prior works have identified that unusual events in software\nrepositories are of interest to developers, and use simple code metrics-based\nmethods for detecting them. In this study we will propose a new method for\nunusual event detection in software repositories using surprisal. With the\nability to find surprising issues and pull requests, we intend to further\nanalyse them to determine if they actually hold importance in a repository, or\nif they pose a significant challenge to address. If it is possible to find bad\nsurprises early, or before they cause additional troubles, it is plausible that\neffort, cost and time will be saved as a result.\n  Method. After extracting the issues and pull requests from 5000 of the most\npopular software repositories on GitHub, we will train a language model to\nrepresent these issues. We will measure their perceived importance in the\nrepository, measure their resolution difficulty using several analogues,\nmeasure the surprisal of each, and finally generate inferential statistics to\ndescribe any correlations.",
        "pdf_link": "https://arxiv.org/pdf/2204.07363v1.pdf"
    },
    {
        "title": "XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding",
        "authors": [
            "Chan-Jan Hsu",
            "Hung-yi Lee",
            "Yu Tsao"
        ],
        "published": "2022-04-15T03:44:00Z",
        "summary": "Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.",
        "pdf_link": "https://arxiv.org/pdf/2204.07316v3.pdf"
    },
    {
        "title": "Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning",
        "authors": [
            "Feilong Chen",
            "Xiuyi Chen",
            "Shuang Xu",
            "Bo Xu"
        ],
        "published": "2022-04-15T02:36:52Z",
        "summary": "Visual Dialog is a challenging vision-language task since the visual dialog\nagent needs to answer a series of questions after reasoning over both the image\ncontent and dialog history. Though existing methods try to deal with the\ncross-modal understanding in visual dialog, they are still not enough in\nranking candidate answers based on their understanding of visual and textual\ncontexts. In this paper, we analyze the cross-modal understanding in visual\ndialog based on the vision-language pre-training model VD-BERT and propose a\nnovel approach to improve the cross-modal understanding for visual dialog,\nnamed ICMU. ICMU enhances cross-modal understanding by distinguishing different\npulled inputs (i.e. pulled images, questions or answers) based on four-way\ncontrastive learning. In addition, ICMU exploits the single-turn visual\nquestion answering to enhance the visual dialog model's cross-modal\nunderstanding to handle a multi-turn visually-grounded conversation.\nExperiments show that the proposed approach improves the visual dialog model's\ncross-modal understanding and brings satisfactory gain to the VisDial dataset.",
        "pdf_link": "https://arxiv.org/pdf/2204.07302v1.pdf"
    },
    {
        "title": "Identifying and Measuring Token-Level Sentiment Bias in Pre-trained Language Models with Prompts",
        "authors": [
            "Apoorv Garg",
            "Deval Srivastava",
            "Zhiyang Xu",
            "Lifu Huang"
        ],
        "published": "2022-04-15T02:01:31Z",
        "summary": "Due to the superior performance, large-scale pre-trained language models\n(PLMs) have been widely adopted in many aspects of human society. However, we\nstill lack effective tools to understand the potential bias embedded in the\nblack-box models. Recent advances in prompt tuning show the possibility to\nexplore the internal mechanism of the PLMs. In this work, we propose two\ntoken-level sentiment tests: Sentiment Association Test (SAT) and Sentiment\nShift Test (SST) which utilize the prompt as a probe to detect the latent bias\nin the PLMs. Our experiments on the collection of sentiment datasets show that\nboth SAT and SST can identify sentiment bias in PLMs and SST is able to\nquantify the bias. The results also suggest that fine-tuning can possibly\naugment the existing bias in PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2204.07289v1.pdf"
    },
    {
        "title": "Analysing similarities between legal court documents using natural language processing approaches based on Transformers",
        "authors": [
            "Raphael Souza de Oliveira",
            "Erick Giovani Sperandio Nascimento"
        ],
        "published": "2022-04-14T18:25:56Z",
        "summary": "Recent advances in Artificial Intelligence (AI) have leveraged promising\nresults in solving complex problems in the area of Natural Language Processing\n(NLP), being an important tool to help in the expeditious resolution of\njudicial proceedings in the legal area. In this context, this work targets the\nproblem of detecting the degree of similarity between judicial documents that\ncan be achieved in the inference group, by applying six NLP techniques based on\nthe transformers architecture to a case study of legal proceedings in the\nBrazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2\nand RoBERTa, were pre-trained using a general purpose corpora of the Brazilian\nPortuguese language, and then were fine-tuned and specialised for the legal\nsector using 210,000 legal proceedings. Vector representations of each legal\ndocument were calculated based on their embeddings, which were used to cluster\nthe lawsuits, calculating the quality of each model based on the cosine of the\ndistance between the elements of the group to its centroid. We noticed that\nmodels based on transformers presented better performance when compared to\nprevious traditional NLP techniques, with the RoBERTa model specialised for the\nBrazilian Portuguese language presenting the best results. This methodology can\nbe also applied to other case studies for different languages, making it\npossible to advance in the current state of the art in the area of NLP applied\nto the legal sector.",
        "pdf_link": "https://arxiv.org/pdf/2204.07182v3.pdf"
    },
    {
        "title": "Label Semantic Aware Pre-training for Few-shot Text Classification",
        "authors": [
            "Aaron Mueller",
            "Jason Krone",
            "Salvatore Romeo",
            "Saab Mansour",
            "Elman Mansimov",
            "Yi Zhang",
            "Dan Roth"
        ],
        "published": "2022-04-14T17:33:34Z",
        "summary": "In text classification tasks, useful information is encoded in the label\nnames. Label semantic aware systems have leveraged this information for\nimproved text classification performance during fine-tuning and prediction.\nHowever, use of label-semantics during pre-training has not been extensively\nexplored. We therefore propose Label Semantic Aware Pre-training (LSAP) to\nimprove the generalization and data efficiency of text classification systems.\nLSAP incorporates label semantics into pre-trained generative models (T5 in our\ncase) by performing secondary pre-training on labeled sentences from a variety\nof domains. As domain-general pre-training requires large amounts of data, we\ndevelop a filtering and labeling pipeline to automatically create\nsentence-label pairs from unlabeled text. We perform experiments on intent\n(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP\nobtains significant accuracy improvements over state-of-the-art models for\nfew-shot text classification while maintaining performance comparable to state\nof the art in high-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2204.07128v2.pdf"
    },
    {
        "title": "DeiT III: Revenge of the ViT",
        "authors": [
            "Hugo Touvron",
            "Matthieu Cord",
            "Herv\u00e9 J\u00e9gou"
        ],
        "published": "2022-04-14T17:13:44Z",
        "summary": "A Vision Transformer (ViT) is a simple neural architecture amenable to serve\nseveral computer vision tasks. It has limited built-in architectural priors, in\ncontrast to more recent architectures that incorporate priors either about the\ninput data or of specific tasks. Recent works show that ViTs benefit from\nself-supervised pre-training, in particular BerT-like pre-training like BeiT.\nIn this paper, we revisit the supervised training of ViTs. Our procedure builds\nupon and simplifies a recipe introduced for training ResNet-50. It includes a\nnew simple data-augmentation procedure with only 3 augmentations, closer to the\npractice in self-supervised learning. Our evaluations on Image classification\n(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning\nand semantic segmentation show that our procedure outperforms by a large margin\nprevious fully supervised training recipes for ViT. It also reveals that the\nperformance of our ViT trained with supervision is comparable to that of more\nrecent architectures. Our results could serve as better baselines for recent\nself-supervised approaches demonstrated on ViT.",
        "pdf_link": "https://arxiv.org/pdf/2204.07118v1.pdf"
    },
    {
        "title": "Generative power of a protein language model trained on multiple sequence alignments",
        "authors": [
            "Damiano Sgarbossa",
            "Umberto Lupo",
            "Anne-Florence Bitbol"
        ],
        "published": "2022-04-14T16:59:05Z",
        "summary": "Computational models starting from large ensembles of evolutionarily related\nprotein sequences capture a representation of protein families and learn\nconstraints associated to protein structure and function. They thus open the\npossibility for generating novel sequences belonging to protein families.\nProtein language models trained on multiple sequence alignments, such as MSA\nTransformer, are highly attractive candidates to this end. We propose and test\nan iterative method that directly employs the masked language modeling\nobjective to generate sequences using MSA Transformer. We demonstrate that the\nresulting sequences score as well as natural sequences, for homology,\ncoevolution and structure-based measures. For large protein families, our\nsynthetic sequences have similar or better properties compared to sequences\ngenerated by Potts models, including experimentally-validated ones. Moreover,\nfor small protein families, our generation method based on MSA Transformer\noutperforms Potts models. Our method also more accurately reproduces the\nhigher-order statistics and the distribution of sequences in sequence space of\nnatural data than Potts models. MSA Transformer is thus a strong candidate for\nprotein sequence generation and protein design.",
        "pdf_link": "https://arxiv.org/pdf/2204.07110v2.pdf"
    },
    {
        "title": "Composite Code Sparse Autoencoders for first stage retrieval",
        "authors": [
            "Carlos Lassance",
            "Thibault Formal",
            "Stephane Clinchant"
        ],
        "published": "2022-04-14T15:20:46Z",
        "summary": "We propose a Composite Code Sparse Autoencoder (CCSA) approach for\nApproximate Nearest Neighbor (ANN) search of document representations based on\nSiamese-BERT models. In Information Retrieval (IR), the ranking pipeline is\ngenerally decomposed in two stages: the first stage focus on retrieving a\ncandidate set from the whole collection. The second stage re-ranks the\ncandidate set by relying on more complex models. Recently, Siamese-BERT models\nhave been used as first stage ranker to replace or complement the traditional\nbag-of-word models. However, indexing and searching a large document collection\nrequire efficient similarity search on dense vectors and this is why ANN\ntechniques come into play. Since composite codes are naturally sparse, we first\nshow how CCSA can learn efficient parallel inverted index thanks to an\nuniformity regularizer. Second, CCSA can be used as a binary quantization\nmethod and we propose to combine it with the recent graph based ANN techniques.\nOur experiments on MSMARCO dataset reveal that CCSA outperforms IVF with\nproduct quantization. Furthermore, CCSA binary quantization is beneficial for\nthe index size, and memory usage for the graph-based HNSW method, while\nmaintaining a good level of recall and MRR. Third, we compare with recent\nsupervised quantization methods for image retrieval and find that CCSA is able\nto outperform them.",
        "pdf_link": "https://arxiv.org/pdf/2204.07023v1.pdf"
    },
    {
        "title": "Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model",
        "authors": [
            "Carina Negreanu",
            "Alperen Karaoglu",
            "Jack Williams",
            "Shuang Chen",
            "Daniel Fabian",
            "Andrew Gordon",
            "Chin-Yew Lin"
        ],
        "published": "2022-04-14T15:11:52Z",
        "summary": "Row completion is the task of augmenting a given table of text and numbers\nwith additional, relevant rows. The task divides into two steps: subject\nsuggestion, the task of populating the main column; and gap filling, the task\nof populating the remaining columns. We present state-of-the-art results for\nsubject suggestion and gap filling measured on a standard benchmark\n(WikiTables). Our idea is to solve this task by harmoniously combining\nknowledge base table interpretation and free text generation. We interpret the\ntable using the knowledge base to suggest new rows and generate metadata like\nheaders through property linking. To improve candidate diversity, we synthesize\nadditional rows using free text generation via GPT-3, and crucially, we exploit\nthe metadata we interpret to produce better prompts for text generation.\nFinally, we verify that the additional synthesized content can be linked to the\nknowledge base or a trusted web source such as Wikipedia.",
        "pdf_link": "https://arxiv.org/pdf/2204.07014v1.pdf"
    },
    {
        "title": "Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task",
        "authors": [
            "Karim Lasri",
            "Alessandro Lenci",
            "Thierry Poibeau"
        ],
        "published": "2022-04-14T11:33:15Z",
        "summary": "Although transformer-based Neural Language Models demonstrate impressive\nperformance on a variety of tasks, their generalization abilities are not well\nunderstood. They have been shown to perform strongly on subject-verb number\nagreement in a wide array of settings, suggesting that they learned to track\nsyntactic dependencies during their training even without explicit supervision.\nIn this paper, we examine the extent to which BERT is able to perform\nlexically-independent subject-verb number agreement (NA) on targeted syntactic\ntemplates. To do so, we disrupt the lexical patterns found in naturally\noccurring stimuli for each targeted structure in a novel fine-grained analysis\nof BERT's behavior. Our results on nonce sentences suggest that the model\ngeneralizes well for simple templates, but fails to perform\nlexically-independent syntactic generalization when as little as one attractor\nis present.",
        "pdf_link": "https://arxiv.org/pdf/2204.06889v1.pdf"
    },
    {
        "title": "Multi-label topic classification for COVID-19 literature with Bioformer",
        "authors": [
            "Li Fang",
            "Kai Wang"
        ],
        "published": "2022-04-14T05:24:54Z",
        "summary": "We describe Bioformer team's participation in the multi-label topic\nclassification task for COVID-19 literature (track 5 of BioCreative VII). Topic\nclassification is performed using different BERT models (BioBERT, PubMedBERT,\nand Bioformer). We formulate the topic classification task as a sentence pair\nclassification problem, where the title is the first sentence, and the abstract\nis the second sentence. Our results show that Bioformer outperforms BioBERT and\nPubMedBERT in this task. Compared to the baseline results, our best model\nincreased micro, macro, and instance-based F1 score by 8.8%, 15.5%, 7.4%,\nrespectively. Bioformer achieved the highest micro F1 and macro F1 scores in\nthis challenge. In post-challenge experiments, we found that pretraining of\nBioformer on COVID-19 articles further improves the performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.06758v1.pdf"
    },
    {
        "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
        "authors": [
            "Sid Black",
            "Stella Biderman",
            "Eric Hallahan",
            "Quentin Anthony",
            "Leo Gao",
            "Laurence Golding",
            "Horace He",
            "Connor Leahy",
            "Kyle McDonell",
            "Jason Phang",
            "Michael Pieler",
            "USVSN Sai Prashanth",
            "Shivanshu Purohit",
            "Laria Reynolds",
            "Jonathan Tow",
            "Ben Wang",
            "Samuel Weinbach"
        ],
        "published": "2022-04-14T04:00:27Z",
        "summary": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}'s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.",
        "pdf_link": "https://arxiv.org/pdf/2204.06745v1.pdf"
    },
    {
        "title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
        "authors": [
            "Anthony Colas",
            "Mehrdad Alvandipour",
            "Daisy Zhe Wang"
        ],
        "published": "2022-04-13T23:53:37Z",
        "summary": "Recent improvements in KG-to-text generation are due to additional auxiliary\npre-training tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-training\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show our models are competitive while involving fewer\nparameters and no additional pre-training tasks. By formulating the problem as\na framework, we can interchange the various proposed components and begin\ninterpreting KG-to-text generative models based on the topological and type\ninformation found in a graph.",
        "pdf_link": "https://arxiv.org/pdf/2204.06674v4.pdf"
    },
    {
        "title": "CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing",
        "authors": [
            "Chen Liang",
            "Pengcheng He",
            "Yelong Shen",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "2022-04-13T19:54:51Z",
        "summary": "Model ensemble is a popular approach to produce a low-variance and\nwell-generalized model. However, it induces large memory and inference costs,\nwhich are often not affordable for real-world deployment. Existing work has\nresorted to sharing weights among models. However, when increasing the\nproportion of the shared weights, the resulting models tend to be similar, and\nthe benefits of using model ensemble diminish. To retain ensemble benefits\nwhile maintaining a low memory cost, we propose a consistency-regularized\nensemble learning approach based on perturbed models, named CAMERO.\nSpecifically, we share the weights of bottom layers across all models and apply\ndifferent perturbations to the hidden representations for different models,\nwhich can effectively promote the model diversity. Meanwhile, we apply a\nprediction consistency regularizer across the perturbed models to control the\nvariance due to the model diversity. Our experiments using large language\nmodels demonstrate that CAMERO significantly improves the generalization\nperformance of the ensemble model. Specifically, CAMERO outperforms the\nstandard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a\nsignificantly smaller model size (114.2M vs. 880.6M).",
        "pdf_link": "https://arxiv.org/pdf/2204.06625v2.pdf"
    },
    {
        "title": "Scalable Training of Language Models using JAX pjit and TPUv4",
        "authors": [
            "Joanna Yoo",
            "Kuba Perlin",
            "Siddhartha Rao Kamalakara",
            "Jo\u00e3o G. M. Ara\u00fajo"
        ],
        "published": "2022-04-13T17:08:58Z",
        "summary": "Modern large language models require distributed training strategies due to\ntheir size. The challenges of efficiently and robustly training them are met\nwith rapid developments on both software and hardware frontiers. In this\ntechnical report, we explore challenges and design decisions associated with\ndeveloping a scalable training framework, and present a quantitative analysis\nof efficiency improvements coming from adopting new software and hardware\nsolutions.",
        "pdf_link": "https://arxiv.org/pdf/2204.06514v1.pdf"
    },
    {
        "title": "Building Markovian Generative Architectures over Pretrained LM Backbones for Efficient Task-Oriented Dialog Systems",
        "authors": [
            "Hong Liu",
            "Yucheng Cai",
            "Zhijian Ou",
            "Yi Huang",
            "Junlan Feng"
        ],
        "published": "2022-04-13T15:21:34Z",
        "summary": "Recently, Transformer based pretrained language models (PLMs), such as GPT2\nand T5, have been leveraged to build generative task-oriented dialog (TOD)\nsystems. A drawback of existing PLM-based models is their non-Markov\narchitectures across turns, i.e., the whole history is used as the conditioning\ninput at each turn. First, this brings inefficiencies in memory and\ncomputation. Furthermore, using the whole history increases model complexity\nand may hurt the training efficiency, especially when facing small amounts of\nlabeled training data (the low-resource setting). In this paper, motivated by\nthe observation that dialog states could be viewed as Markov states, we propose\nto build Markovian Generative Architectures (MGA) over PLM backbones for\nefficient TOD systems. Experiments on MultiWOZ2.1 show that in the\nrich-resource setting, the proposed Markov models reduce memory and time costs\nwithout performance degradation; in the low-resource setting, the training\nefficiency of the Markov models is more significant.",
        "pdf_link": "https://arxiv.org/pdf/2204.06452v2.pdf"
    },
    {
        "title": "Local Feature Swapping for Generalization in Reinforcement Learning",
        "authors": [
            "David Bertoin",
            "Emmanuel Rachelson"
        ],
        "published": "2022-04-13T13:12:51Z",
        "summary": "Over the past few years, the acceleration of computing resources and research\nin deep learning has led to significant practical successes in a range of\ntasks, including in particular in computer vision. Building on these advances,\nreinforcement learning has also seen a leap forward with the emergence of\nagents capable of making decisions directly from visual observations. Despite\nthese successes, the over-parametrization of neural architectures leads to\nmemorization of the data used during training and thus to a lack of\ngeneralization. Reinforcement learning agents based on visual inputs also\nsuffer from this phenomenon by erroneously correlating rewards with unrelated\nvisual features such as background elements. To alleviate this problem, we\nintroduce a new regularization technique consisting of channel-consistent local\npermutations (CLOP) of the feature maps. The proposed permutations induce\nrobustness to spatial correlations and help prevent overfitting behaviors in\nRL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained\nwith the CLOP method exhibit robustness to visual changes and better\ngeneralization properties than agents trained using other state-of-the-art\nregularization techniques. We also demonstrate the effectiveness of CLOP as a\ngeneral regularization technique in supervised learning.",
        "pdf_link": "https://arxiv.org/pdf/2204.06355v1.pdf"
    },
    {
        "title": "HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition",
        "authors": [
            "Ji Won Yoon",
            "Beom Jun Woo",
            "Nam Soo Kim"
        ],
        "published": "2022-04-13T12:11:44Z",
        "summary": "Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)\nand wav2vec 2.0, has brought significant improvements in automatic speech\nrecognition (ASR). However, these models usually require an expensive\ncomputational cost to achieve outstanding performance, slowing down the\ninference speed. To improve the model efficiency, we propose an early exit\nscheme for ASR, namely HuBERT-EE, that allows the model to stop the inference\ndynamically. In HuBERT-EE, multiple early exit branches are added at the\nintermediate layers, and each branch is used to decide whether a prediction can\nbe exited early. Experimental results on the LibriSpeech dataset show that\nHuBERT-EE can accelerate the inference of a large-scale HuBERT model while\nsimultaneously balancing the trade-off between the word error rate (WER)\nperformance and the latency.",
        "pdf_link": "https://arxiv.org/pdf/2204.06328v1.pdf"
    },
    {
        "title": "Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification",
        "authors": [
            "Han Wang",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2022-04-13T11:15:52Z",
        "summary": "Prompt-based learning (i.e., prompting) is an emerging paradigm for\nexploiting knowledge learned by a pretrained language model. In this paper, we\npropose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method\nto automatically select label mappings for few-shot text classification with\nprompting. Our method exploits one-to-many label mappings and a\nstatistics-based algorithm to select label mappings given a prompt template.\nOur experiments demonstrate that AMuLaP achieves competitive performance on the\nGLUE benchmark without human effort or external resources.",
        "pdf_link": "https://arxiv.org/pdf/2204.06305v2.pdf"
    },
    {
        "title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
        "authors": [
            "Zeming Chen",
            "Qiyue Gao"
        ],
        "published": "2022-04-13T10:32:03Z",
        "summary": "In the age of large transformer language models, linguistic evaluation play\nan important role in diagnosing models' abilities and limitations on natural\nlanguage understanding. However, current evaluation methods show some\nsignificant shortcomings. In particular, they do not provide insight into how\nwell a language model captures distinct linguistic skills essential for\nlanguage understanding and reasoning. Thus they fail to effectively map out the\naspects of language understanding that remain challenging to existing models,\nwhich makes it hard to discover potential limitations in models and datasets.\nIn this paper, we introduce Curriculum as a new format of NLI benchmark for\nevaluation of broad-coverage linguistic phenomena. Curriculum contains a\ncollection of datasets that covers 36 types of major linguistic phenomena and\nan evaluation procedure for diagnosing how well a language model captures\nreasoning skills for distinct types of linguistic phenomena. We show that this\nlinguistic-phenomena-driven benchmark can serve as an effective tool for\ndiagnosing model behavior and verifying model learning quality. In addition,\nour experiments provide insight into the limitation of existing benchmark\ndatasets and state-of-the-art models that may encourage future research on\nre-designing datasets, model architectures, and learning objectives.",
        "pdf_link": "https://arxiv.org/pdf/2204.06283v2.pdf"
    },
    {
        "title": "TangoBERT: Reducing Inference Cost by using Cascaded Architecture",
        "authors": [
            "Jonathan Mamou",
            "Oren Pereg",
            "Moshe Wasserblat",
            "Roy Schwartz"
        ],
        "published": "2022-04-13T09:45:08Z",
        "summary": "The remarkable success of large transformer-based models such as BERT,\nRoBERTa and XLNet in many NLP tasks comes with a large increase in monetary and\nenvironmental cost due to their high computational load and energy consumption.\nIn order to reduce this computational load in inference time, we present\nTangoBERT, a cascaded model architecture in which instances are first processed\nby an efficient but less accurate first tier model, and only part of those\ninstances are additionally processed by a less efficient but more accurate\nsecond tier model. The decision of whether to apply the second tier model is\nbased on a confidence score produced by the first tier model. Our simple method\nhas several appealing practical advantages compared to standard cascading\napproaches based on multi-layered transformer models. First, it enables higher\nspeedup gains (average lower latency). Second, it takes advantage of batch size\noptimization for cascading, which increases the relative inference cost\nreductions. We report TangoBERT inference CPU speedup on four text\nclassification GLUE tasks and on one reading comprehension task. Experimental\nresults show that TangoBERT outperforms efficient early exit baseline models;\non the the SST-2 task, it achieves an accuracy of 93.9% with a CPU speedup of\n8.2x.",
        "pdf_link": "https://arxiv.org/pdf/2204.06271v1.pdf"
    },
    {
        "title": "IIITDWD-ShankarB@ Dravidian-CodeMixi-HASOC2021: mBERT based model for identification of offensive content in south Indian languages",
        "authors": [
            "Shankar Biradar",
            "Sunil Saumya"
        ],
        "published": "2022-04-13T06:24:57Z",
        "summary": "In recent years, there has been a lot of focus on offensive content. The\namount of offensive content generated by social media is increasing at an\nalarming rate. This created a greater need to address this issue than ever\nbefore. To address these issues, the organizers of \"Dravidian-Code Mixed\nHASOC-2020\" have created two challenges. Task 1 involves identifying offensive\ncontent in Malayalam data, whereas Task 2 includes Malayalam and Tamil Code\nMixed Sentences. Our team participated in Task 2. In our suggested model, we\nexperiment with multilingual BERT to extract features, and three different\nclassifiers are used on extracted features. Our model received a weighted F1\nscore of 0.70 for Malayalam data and was ranked fifth; we also received a\nweighted F1 score of 0.573 for Tamil Code Mixed data and were ranked eleventh.",
        "pdf_link": "https://arxiv.org/pdf/2204.10195v1.pdf"
    },
    {
        "title": "HIT at SemEval-2022 Task 2: Pre-trained Language Model for Idioms Detection",
        "authors": [
            "Zheng Chu",
            "Ziqing Yang",
            "Yiming Cui",
            "Zhigang Chen",
            "Ming Liu"
        ],
        "published": "2022-04-13T02:45:04Z",
        "summary": "The same multi-word expressions may have different meanings in different\nsentences. They can be mainly divided into two categories, which are literal\nmeaning and idiomatic meaning. Non-contextual-based methods perform poorly on\nthis problem, and we need contextual embedding to understand the idiomatic\nmeaning of multi-word expressions correctly. We use a pre-trained language\nmodel, which can provide a context-aware sentence embedding, to detect whether\nmulti-word expression in the sentence is idiomatic usage.",
        "pdf_link": "https://arxiv.org/pdf/2204.06145v1.pdf"
    },
    {
        "title": "Impossible Triangle: What's Next for Pre-trained Language Models?",
        "authors": [
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2022-04-13T01:28:18Z",
        "summary": "Recent development of large-scale pre-trained language models (PLM) have\nsignificantly improved the capability of models in various NLP tasks, in terms\nof performance after task-specific fine-tuning and zero-shot / few-shot\nlearning. However, many of such models come with a dauntingly huge size that\nfew institutions can afford to pre-train, fine-tune or even deploy, while\nmoderate-sized models usually lack strong generalized few-shot learning\ncapabilities. In this paper, we first elaborate the current obstacles of using\nPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)\nstate-of-the-art few-shot learning capability, and 3) state-of-the-art\nfine-tuning capability. We argue that all existing PLM models lack one or more\nproperties from the Impossible Triangle. To remedy these missing properties of\nPLMs, various techniques have been proposed, such as knowledge distillation,\ndata augmentation and prompt learning, which inevitably brings additional work\nto the application of PLMs in real scenarios. We then offer insights into\nfuture research directions of PLMs to achieve the Impossible Triangle, and\nbreak down the task into several key phases.",
        "pdf_link": "https://arxiv.org/pdf/2204.06130v2.pdf"
    },
    {
        "title": "L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models",
        "authors": [
            "Parth Patil",
            "Aparna Ranade",
            "Maithili Sabane",
            "Onkar Litake",
            "Raviraj Joshi"
        ],
        "published": "2022-04-12T18:32:15Z",
        "summary": "Named Entity Recognition (NER) is a basic NLP task and finds major\napplications in conversational and search systems. It helps us identify key\nentities in a sentence used for the downstream application. NER or similar slot\nfilling systems for popular languages have been heavily used in commercial\napplications. In this work, we focus on Marathi, an Indian language, spoken\nprominently by the people of Maharashtra state. Marathi is a low resource\nlanguage and still lacks useful NER resources. We present L3Cube-MahaNER, the\nfirst major gold standard named entity recognition dataset in Marathi. We also\ndescribe the manual annotation guidelines followed during the process. In the\nend, we benchmark the dataset on different CNN, LSTM, and Transformer based\nmodels like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides\nthe best performance among all the models. The data and models are available at\nhttps://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2204.06029v1.pdf"
    },
    {
        "title": "Mining Logical Event Schemas From Pre-Trained Language Models",
        "authors": [
            "Lane Lawley",
            "Lenhart Schubert"
        ],
        "published": "2022-04-12T16:41:18Z",
        "summary": "We present NESL (the Neuro-Episodic Schema Learner), an event schema learning\nsystem that combines large language models, FrameNet parsing, a powerful\nlogical representation of language, and a set of simple behavioral schemas\nmeant to bootstrap the learning process. In lieu of a pre-made corpus of\nstories, our dataset is a continuous feed of \"situation samples\" from a\npre-trained language model, which are then parsed into FrameNet frames, mapped\ninto simple behavioral schemas, and combined and generalized into complex,\nhierarchical schemas for a variety of everyday scenarios. We show that careful\nsampling from the language model can help emphasize stereotypical properties of\nsituations and de-emphasize irrelevant details, and that the resulting schemas\nspecify situations more comprehensively than those learned by other systems.",
        "pdf_link": "https://arxiv.org/pdf/2204.05939v1.pdf"
    },
    {
        "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
        "authors": [
            "Thomas Wang",
            "Adam Roberts",
            "Daniel Hesslow",
            "Teven Le Scao",
            "Hyung Won Chung",
            "Iz Beltagy",
            "Julien Launay",
            "Colin Raffel"
        ],
        "published": "2022-04-12T14:19:49Z",
        "summary": "Large pretrained Transformer language models have been shown to exhibit\nzero-shot generalization, i.e. they can perform a wide variety of tasks that\nthey were not explicitly trained on. However, the architectures and pretraining\nobjectives used across state-of-the-art models differ significantly, and there\nhas been limited systematic comparison of these factors. In this work, we\npresent a large-scale evaluation of modeling choices and their impact on\nzero-shot generalization. In particular, we focus on text-to-text models and\nexperiment with three model architectures (causal/non-causal decoder-only and\nencoder-decoder), trained with two different pretraining objectives\n(autoregressive and masked language modeling), and evaluated with and without\nmultitask prompted finetuning. We train models with over 5 billion parameters\nfor more than 170 billion tokens, thereby increasing the likelihood that our\nconclusions will transfer to even larger scales. Our experiments show that\ncausal decoder-only models trained on an autoregressive language modeling\nobjective exhibit the strongest zero-shot generalization after purely\nunsupervised pretraining. However, models with non-causal visibility on their\ninput trained with a masked language modeling objective followed by multitask\nfinetuning perform the best among our experiments. We therefore consider the\nadaptation of pretrained models across architectures and objectives. We find\nthat pretrained non-causal decoder models can be adapted into performant\ngenerative causal decoder models, using autoregressive language modeling as a\ndownstream task. Furthermore, we find that pretrained causal decoder models can\nbe efficiently adapted into non-causal decoder models, ultimately achieving\ncompetitive performance after multitask finetuning. Code and checkpoints are\navailable at https://github.com/bigscience-workshop/architecture-objective.",
        "pdf_link": "https://arxiv.org/pdf/2204.05832v1.pdf"
    },
    {
        "title": "MuCoT: Multilingual Contrastive Training for Question-Answering in Low-resource Languages",
        "authors": [
            "Gokul Karthik Kumar",
            "Abhishek Singh Gehlot",
            "Sahal Shaji Mullappilly",
            "Karthik Nandakumar"
        ],
        "published": "2022-04-12T13:52:54Z",
        "summary": "Accuracy of English-language Question Answering (QA) systems has improved\nsignificantly in recent years with the advent of Transformer-based models\n(e.g., BERT). These models are pre-trained in a self-supervised fashion with a\nlarge English text corpus and further fine-tuned with a massive English QA\ndataset (e.g., SQuAD). However, QA datasets on such a scale are not available\nfor most of the other languages. Multi-lingual BERT-based models (mBERT) are\noften used to transfer knowledge from high-resource languages to low-resource\nlanguages. Since these models are pre-trained with huge text corpora containing\nmultiple languages, they typically learn language-agnostic embeddings for\ntokens from different languages. However, directly training an mBERT-based QA\nsystem for low-resource languages is challenging due to the paucity of training\ndata. In this work, we augment the QA samples of the target language using\ntranslation and transliteration into other languages and use the augmented data\nto fine-tune an mBERT-based QA model, which is already pre-trained in English.\nExperiments on the Google ChAII dataset show that fine-tuning the mBERT model\nwith translations from the same language family boosts the question-answering\nperformance, whereas the performance degrades in the case of cross-language\nfamilies. We further show that introducing a contrastive loss between the\ntranslated question-context feature pairs during the fine-tuning process,\nprevents such degradation with cross-lingual family translations and leads to\nmarginal improvement. The code for this work is available at\nhttps://github.com/gokulkarthik/mucot.",
        "pdf_link": "https://arxiv.org/pdf/2204.05814v1.pdf"
    },
    {
        "title": "Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change",
        "authors": [
            "Mario Giulianelli",
            "Andrey Kutuzov",
            "Lidia Pivovarova"
        ],
        "published": "2022-04-12T11:20:42Z",
        "summary": "Morphological and syntactic changes in word usage (as captured, e.g., by\ngrammatical profiles) have been shown to be good predictors of a word's meaning\nchange. In this work, we explore whether large pre-trained contextualised\nlanguage models, a common tool for lexical semantic change detection, are\nsensitive to such morphosyntactic changes. To this end, we first compare the\nperformance of grammatical profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, covering 7 languages, and then combine\nthe two approaches in ensembles to assess their complementarity. Our results\nshow that ensembling grammatical profiles with XLM-R improves semantic change\ndetection performance for most datasets and languages. This indicates that\nlanguage models do not fully cover the fine-grained morphological and syntactic\nsignals that are explicitly represented in grammatical profiles.\n  An interesting exception are the test sets where the time spans under\nanalysis are much longer than the time gap between them (for example,\ncentury-long spans with a one-year gap between them). Morphosyntactic change is\nslow so grammatical profiles do not detect in such cases. In contrast, language\nmodels, thanks to their access to lexical information, are able to detect fast\ntopical changes.",
        "pdf_link": "https://arxiv.org/pdf/2204.05717v1.pdf"
    },
    {
        "title": "Solving Price Per Unit Problem Around the World: Formulating Fact Extraction as Question Answering",
        "authors": [
            "Tarik Arici",
            "Kushal Kumar",
            "Hayreddin \u00c7eker",
            "Anoop S V K K Saladi",
            "Ismail Tutar"
        ],
        "published": "2022-04-12T06:43:48Z",
        "summary": "Price Per Unit (PPU) is an essential information for consumers shopping on\ne-commerce websites when comparing products. Finding total quantity in a\nproduct is required for computing PPU, which is not always provided by the\nsellers. To predict total quantity, all relevant quantities given in a product\nattributes such as title, description and image need to be inferred correctly.\nWe formulate this problem as a question-answering (QA) task rather than named\nentity recognition (NER) task for fact extraction. In our QA approach, we first\npredict the unit of measure (UoM) type (e.g., volume, weight or count), that\nformulates the desired question (e.g., \"What is the total volume?\") and then\nuse this question to find all the relevant answers. Our model architecture\nconsists of two subnetworks for the two subtasks: a classifier to predict UoM\ntype (or the question) and an extractor to extract the relevant quantities. We\nuse a deep character-level CNN architecture for both subtasks, which enables\n(1) easy expansion to new stores with similar alphabets, (2) multi-span\nanswering due to its span-image architecture and (3) easy deployment by keeping\nmodel-inference latency low. Our QA approach outperforms rule-based methods by\n34.4% in precision and also BERT-based fact extraction approach in all stores\nglobally, with largest precision lift of 10.6% in the US store.",
        "pdf_link": "https://arxiv.org/pdf/2204.05555v1.pdf"
    },
    {
        "title": "Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection",
        "authors": [
            "Hariharan RamakrishnaIyer LekshmiAmmal",
            "Manikandan Ravikiran",
            "Gayathri Nisha",
            "Navyasree Balamuralidhar",
            "Adithya Madhusoodanan",
            "Anand Kumar Madasamy",
            "Bharathi Raja Chakravarthi"
        ],
        "published": "2022-04-12T02:38:54Z",
        "summary": "Hope Speech Detection, a task of recognizing positive expressions, has made\nsignificant strides recently. However, much of the current works focus on model\ndevelopment without considering the issue of inherent imbalance in the data.\nOur work revisits this issue in hope-speech detection by introducing focal\nloss, data augmentation, and pre-processing strategies. Accordingly, we find\nthat introducing focal loss as part of Multilingual-BERT's (M-BERT) training\nprocess mitigates the effect of class imbalance and improves overall F1-Macro\nby 0.11. At the same time, contextual and back-translation-based word\naugmentation with M-BERT improves results by 0.10 over baseline despite\nimbalance. Finally, we show that overlapping word removal based on\npre-processing, though simple, improves F1-Macro by 0.28. In due process, we\npresent detailed studies depicting various behaviors of each of these\nstrategies and summarize key findings from our empirical results for those\ninterested in getting the most out of M-BERT for hope speech detection under\nreal-world conditions of data imbalance.",
        "pdf_link": "https://arxiv.org/pdf/2204.05488v1.pdf"
    },
    {
        "title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors",
        "authors": [
            "Anubrata Das",
            "Chitrank Gupta",
            "Venelin Kovatchev",
            "Matthew Lease",
            "Junyi Jessy Li"
        ],
        "published": "2022-04-11T22:08:45Z",
        "summary": "We present ProtoTEx, a novel white-box NLP classification architecture based\non prototype networks. ProtoTEx faithfully explains model decisions based on\nprototype tensors that encode latent clusters of training examples. At\ninference time, classification decisions are based on the distances between the\ninput text and the prototype tensors, explained via the training examples most\nsimilar to the most influential prototypes. We also describe a novel\ninterleaved training algorithm that effectively handles classes characterized\nby the absence of indicative features. On a propaganda detection task, ProtoTEx\naccuracy matches BART-large and exceeds BERT-large with the added benefit of\nproviding faithful explanations. A user study also shows that prototype-based\nexplanations help non-experts to better recognize propaganda in online news.",
        "pdf_link": "https://arxiv.org/pdf/2204.05426v2.pdf"
    },
    {
        "title": "Doctor XAvIer: Explainable Diagnosis on Physician-Patient Dialogues and XAI Evaluation",
        "authors": [
            "Hillary Ngai",
            "Frank Rudzicz"
        ],
        "published": "2022-04-11T18:38:22Z",
        "summary": "We introduce Doctor XAvIer, a BERT-based diagnostic system that extracts\nrelevant clinical data from transcribed patient-doctor dialogues and explains\npredictions using feature attribution methods. We present a novel performance\nplot and evaluation metric for feature attribution methods: Feature Attribution\nDropping (FAD) curve and its Normalized Area Under the Curve (N-AUC). FAD curve\nanalysis shows that integrated gradients outperforms Shapley values in\nexplaining diagnosis classification. Doctor XAvIer outperforms the baseline\nwith 0.97 F1-score in named entity recognition and symptom pertinence\nclassification and 0.91 F1-score in diagnosis classification.",
        "pdf_link": "https://arxiv.org/pdf/2204.10178v2.pdf"
    },
    {
        "title": "A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis",
        "authors": [
            "Ehsan Hosseini-Asl",
            "Wenhao Liu",
            "Caiming Xiong"
        ],
        "published": "2022-04-11T18:31:53Z",
        "summary": "Sentiment analysis is an important task in natural language processing. In\nrecent works, pre-trained language models are often used to achieve\nstate-of-the-art results, especially when training data is scarce. It is common\nto fine-tune on the downstream task, usually by adding task-specific layers on\ntop of the model. In this paper, we focus on aspect-based sentiment analysis,\nwhich involves extracting aspect term, category, and predicting their\ncorresponding polarities. In particular, we are interested in few-shot\nsettings. We propose to reformulate the extraction and prediction tasks into\nthe sequence generation task, using a generative language model with\nunidirectional attention (GPT2 is used unless stated otherwise). This way, the\nmodel learns to accomplish the tasks via language generation without the need\nof training task-specific layers. Our evaluation results on the single-task\npolarity prediction show that our approach outperforms the previous\nstate-of-the-art (based on BERT) on average performance by a large margins in\nfew-shot and full-shot settings. More importantly, our generative approach\nsignificantly reduces the model variance caused by low-resource data. We\nfurther demonstrate that the proposed generative language model can handle\njoint and multi-task settings, unlike previous work. We observe that the\nproposed sequence generation method achieves further improved performances on\npolarity prediction when the model is trained via joint and multi-task\nsettings. Further evaluation on similar sentiment analysis datasets, SST-2,\nSST- and OOS intent detection validates the superiority and noise robustness of\ngenerative language model in few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2204.05356v1.pdf"
    },
    {
        "title": "GDC- Generalized Distribution Calibration for Few-Shot Learning",
        "authors": [
            "Shakti Kumar",
            "Hussain Zaidi"
        ],
        "published": "2022-04-11T16:22:53Z",
        "summary": "Few shot learning is an important problem in machine learning as large\nlabelled datasets take considerable time and effort to assemble. Most few-shot\nlearning algorithms suffer from one of two limitations- they either require the\ndesign of sophisticated models and loss functions, thus hampering\ninterpretability; or employ statistical techniques but make assumptions that\nmay not hold across different datasets or features. Developing on recent work\nin extrapolating distributions of small sample classes from the most similar\nlarger classes, we propose a Generalized sampling method that learns to\nestimate few-shot distributions for classification as weighted random variables\nof all large classes. We use a form of covariance shrinkage to provide\nrobustness against singular covariances due to overparameterized features or\nsmall datasets. We show that our sampled points are close to few-shot classes\neven in cases when there are no similar large classes in the training set. Our\nmethod works with arbitrary off-the-shelf feature extractors and outperforms\nexisting state-of-the-art on miniImagenet, CUB and Stanford Dogs datasets by 3%\nto 5% on 5way-1shot and 5way-5shot tasks and by 1% in challenging cross domain\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.05230v1.pdf"
    },
    {
        "title": "Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems",
        "authors": [
            "Vishal Sunder",
            "Eric Fosler-Lussier",
            "Samuel Thomas",
            "Hong-Kwang J. Kuo",
            "Brian Kingsbury"
        ],
        "published": "2022-04-11T15:24:25Z",
        "summary": "Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have\nbeen primarily due to effective pretraining of speech representations. One such\npretraining paradigm is the distillation of semantic knowledge from\nstate-of-the-art text-based models like BERT to speech encoder neural networks.\nThis work is a step towards doing the same in a much more efficient and\nfine-grained manner where we align speech embeddings and BERT embeddings on a\ntoken-by-token basis. We introduce a simple yet novel technique that uses a\ncross-modal attention mechanism to extract token-level contextual embeddings\nfrom a speech encoder such that these can be directly compared and aligned with\nBERT based contextual embeddings. This alignment is performed using a novel\ntokenwise contrastive loss. Fine-tuning such a pretrained model to perform\nintent recognition using speech directly yields state-of-the-art performance on\ntwo widely used SLU datasets. Our model improves further when fine-tuned with\nadditional regularization using SpecAugment especially when speech is noisy,\ngiving an absolute improvement as high as 8% over previous results.",
        "pdf_link": "https://arxiv.org/pdf/2204.05188v2.pdf"
    },
    {
        "title": "Uniform Complexity for Text Generation",
        "authors": [
            "Joseph Marvin Imperial",
            "Harish Tayyar Madabushi"
        ],
        "published": "2022-04-11T15:19:47Z",
        "summary": "Large language models (LLMs) have shown promising results in a wide array of\ngenerative NLP tasks, such as summarization and machine translation. In the\ncontext of narrative generation, however, existing models still do not capture\nfactors that contribute to producing consistent text. For instance, it is\nlogical that a piece of text or a story should be uniformly readable throughout\nand that this form of complexity should be controllable. As such, if the\ncomplexity of an input text prompt is rated first-grade reading level in the\nFlesch Reading Ease test, then the generated text continuing the plot should\nalso be within this range of complexity. With this in mind, we introduce\nUniform Complexity for Text Generation (UCTG), a new benchmark test which\nraises the challenge of making generative models observe uniform linguistic\nproperties with respect to prompts. We experiment with over 150+ linguistically\nand cognitively motivated features for evaluating text complexity in humans and\ngenerative models. From our results, we find that models such as GPT-2 struggle\nto preserve the complexity of input prompts used in its generations, even if\nfinetuned with professionally written texts.",
        "pdf_link": "https://arxiv.org/pdf/2204.05185v3.pdf"
    },
    {
        "title": "Team \u00daFAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models",
        "authors": [
            "Sunit Bhattacharya",
            "Rishu Kumar",
            "Ondrej Bojar"
        ],
        "published": "2022-04-11T10:43:34Z",
        "summary": "Eye-Tracking data is a very useful source of information to study cognition\nand especially language comprehension in humans. In this paper, we describe our\nsystems for the CMCL 2022 shared task on predicting eye-tracking information.\nWe describe our experiments with pretrained models like BERT and XLM and the\ndifferent ways in which we used those representations to predict four\neye-tracking features. Along with analysing the effect of using two different\nkinds of pretrained multilingual language models and different ways of pooling\nthe tokenlevel representations, we also explore how contextual information\naffects the performance of the systems. Finally, we also explore if factors\nlike augmenting linguistic information affect the predictions. Our submissions\nachieved an average MAE of 5.72 and ranked 5th in the shared task. The average\nMAE showed further reduction to 5.25 in post task evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2204.04998v1.pdf"
    },
    {
        "title": "JORLDY: a fully customizable open source framework for reinforcement learning",
        "authors": [
            "Kyushik Min",
            "Hyunho Lee",
            "Kwansu Shin",
            "Taehak Lee",
            "Hojoon Lee",
            "Jinwon Choi",
            "Sungho Son"
        ],
        "published": "2022-04-11T06:28:27Z",
        "summary": "Recently, Reinforcement Learning (RL) has been actively researched in both\nacademic and industrial fields. However, there exist only a few RL frameworks\nwhich are developed for researchers or students who want to study RL. In\nresponse, we propose an open-source RL framework \"Join Our Reinforcement\nLearning framework for Developing Yours\" (JORLDY). JORLDY provides more than 20\nwidely used RL algorithms which are implemented with Pytorch. Also, JORLDY\nsupports multiple RL environments which include OpenAI gym, Unity ML-Agents,\nMujoco, Super Mario Bros and Procgen. Moreover, the algorithmic components such\nas agent, network, environment can be freely customized, so that the users can\neasily modify and append algorithmic components. We expect that JORLDY will\nsupport various RL research and contribute further advance the field of RL. The\nsource code of JORLDY is provided on the following Github:\nhttps://github.com/kakaoenterprise/JORLDY",
        "pdf_link": "https://arxiv.org/pdf/2204.04892v1.pdf"
    },
    {
        "title": "Adapting BigScience Multilingual Model to Unseen Languages",
        "authors": [
            "Zheng-Xin Yong",
            "Vassilina Nikoulina"
        ],
        "published": "2022-04-11T05:32:14Z",
        "summary": "We benchmark different strategies of adding new languages (German and Korean)\ninto the BigScience's pretrained multilingual language model with 1.3 billion\nparameters that currently supports 13 languages. We investigate the factors\nthat affect the language adaptability of the model and the trade-offs between\ncomputational costs and expected performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.04873v1.pdf"
    },
    {
        "title": "Fake news detection using parallel BERT deep neural networks",
        "authors": [
            "Mahmood Farokhian",
            "Vahid Rafe",
            "Hadi Veisi"
        ],
        "published": "2022-04-10T23:16:00Z",
        "summary": "Fake news is a growing challenge for social networks and media. Detection of\nfake news always has been a problem for many years, but after the evolution of\nsocial networks and increasing speed of news dissemination in recent years has\nbeen considered again. There are several approaches to solving this problem,\none of which is to detect fake news based on its text style using deep neural\nnetworks. In recent years, one of the most used forms of deep neural networks\nfor natural language processing is transfer learning with transformers. BERT is\none of the most promising transformers who outperforms other models in many NLP\nbenchmarks. This article, we introduce MWPBert, which uses two parallel BERT\nnetworks to perform veracity detection on full-text news articles. One of the\nBERT networks encodes news headline, and another encodes news body. Since the\ninput length of the BERT network is limited and constant and the news body is\nusually a long text, we cannot fed the whole news text into the BERT.\nTherefore, using the MaxWorth algorithm, we selected the part of the news text\nthat is more valuable for fact-checking, and fed it into the BERT network.\nFinally, we encode the output of the two BERT networks to an output network to\nclassify the news. The experiment results showed that the proposed model\noutperformed previous models in terms of accuracy and other performance\nmeasures.",
        "pdf_link": "https://arxiv.org/pdf/2204.04793v2.pdf"
    },
    {
        "title": "Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts",
        "authors": [
            "Saadullah Amin",
            "Noon Pokaratsiri Goldstein",
            "Morgan Kelly Wixted",
            "Alejandro Garc\u00eda-Rudolph",
            "Catalina Mart\u00ednez-Costa",
            "G\u00fcnter Neumann"
        ],
        "published": "2022-04-10T21:46:52Z",
        "summary": "Despite the advances in digital healthcare systems offering curated\nstructured knowledge, much of the critical information still lies in large\nvolumes of unlabeled and unstructured clinical texts. These texts, which often\ncontain protected health information (PHI), are exposed to information\nextraction tools for downstream applications, risking patient identification.\nExisting works in de-identification rely on using large-scale annotated corpora\nin English, which often are not suitable in real-world multilingual settings.\nPre-trained language models (LM) have shown great potential for cross-lingual\ntransfer in low-resource settings. In this work, we empirically show the\nfew-shot cross-lingual transfer property of LMs for named entity recognition\n(NER) and apply it to solve a low-resource and real-world challenge of\ncode-mixed (Spanish-Catalan) clinical notes de-identification in the stroke\ndomain. We annotate a gold evaluation dataset to assess few-shot setting\nperformance where we only use a few hundred labeled examples for training. Our\nmodel improves the zero-shot F1-score from 73.7% to 91.2% on the gold\nevaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)\nfrom the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual\ntarget corpus. When generalized to an out-of-sample test set, the best model\nachieves a human-evaluation F1-score of 97.2%.",
        "pdf_link": "https://arxiv.org/pdf/2204.04775v1.pdf"
    },
    {
        "title": "Breaking Character: Are Subwords Good Enough for MRLs After All?",
        "authors": [
            "Omri Keren",
            "Tal Avinari",
            "Reut Tsarfaty",
            "Omer Levy"
        ],
        "published": "2022-04-10T18:54:43Z",
        "summary": "Large pretrained language models (PLMs) typically tokenize the input string\ninto contiguous subwords before any pretraining or inference. However, previous\nstudies have claimed that this form of subword tokenization is inadequate for\nprocessing morphologically-rich languages (MRLs). We revisit this hypothesis by\npretraining a BERT-style masked language model over character sequences instead\nof word-pieces. We compare the resulting model, dubbed TavBERT, against\ncontemporary PLMs based on subwords for three highly complex and ambiguous MRLs\n(Hebrew, Turkish, and Arabic), testing them on both morphological and semantic\ntasks. Our results show, for all tested languages, that while TavBERT obtains\nmild improvements on surface-level tasks \\`a la POS tagging and full\nmorphological disambiguation, subword-based PLMs achieve significantly higher\nperformance on semantic tasks, such as named entity recognition and extractive\nquestion answering. These results showcase and (re)confirm the potential of\nsubword tokenization as a reasonable modeling assumption for many languages,\nincluding MRLs.",
        "pdf_link": "https://arxiv.org/pdf/2204.04748v1.pdf"
    },
    {
        "title": "Pushing on Personality Detection from Verbal Behavior: A Transformer Meets Text Contours of Psycholinguistic Features",
        "authors": [
            "Elma Kerz",
            "Yu Qiao",
            "Sourabh Zanwar",
            "Daniel Wiechmann"
        ],
        "published": "2022-04-10T08:08:46Z",
        "summary": "Research at the intersection of personality psychology, computer science, and\nlinguistics has recently focused increasingly on modeling and predicting\npersonality from language use. We report two major improvements in predicting\npersonality traits from text data: (1) to our knowledge, the most comprehensive\nset of theory-based psycholinguistic features and (2) hybrid models that\nintegrate a pre-trained Transformer Language Model BERT and Bidirectional Long\nShort-Term Memory (BLSTM) networks trained on within-text distributions ('text\ncontours') of psycholinguistic features. We experiment with BLSTM models (with\nand without Attention) and with two techniques for applying pre-trained\nlanguage representations from the transformer model - 'feature-based' and\n'fine-tuning'. We evaluate the performance of the models we built on two\nbenchmark datasets that target the two dominant theoretical models of\npersonality: the Big Five Essay dataset and the MBTI Kaggle dataset. Our\nresults are encouraging as our models outperform existing work on the same\ndatasets. More specifically, our models achieve improvement in classification\naccuracy by 2.9% on the Essay dataset and 8.28% on the Kaggle MBTI dataset. In\naddition, we perform ablation experiments to quantify the impact of different\ncategories of psycholinguistic features in the respective personality\nprediction models.",
        "pdf_link": "https://arxiv.org/pdf/2204.04629v1.pdf"
    },
    {
        "title": "Efficient Extraction of Pathologies from C-Spine Radiology Reports using Multi-Task Learning",
        "authors": [
            "Arijit Sehanobish",
            "Nathaniel Brown",
            "Ishita Daga",
            "Jayashri Pawar",
            "Danielle Torres",
            "Anasuya Das",
            "Murray Becker",
            "Richard Herzog",
            "Benjamin Odry",
            "Ron Vianu"
        ],
        "published": "2022-04-09T20:29:48Z",
        "summary": "Pretrained Transformer based models finetuned on domain specific corpora have\nchanged the landscape of NLP. Generally, if one has multiple tasks on a given\ndataset, one may finetune different models or use task specific adapters. In\nthis work, we show that a multi-task model can beat or achieve the performance\nof multiple BERT-based models finetuned on various tasks and various task\nspecific adapter augmented BERT-based models. We validate our method on our\ninternal radiologist's report dataset on cervical spine. We hypothesize that\nthe tasks are semantically close and related and thus multitask learners are\npowerful classifiers. Our work opens the scope of using our method to\nradiologist's reports on various body parts.",
        "pdf_link": "https://arxiv.org/pdf/2204.04544v1.pdf"
    },
    {
        "title": "Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model",
        "authors": [
            "Usman Naseem",
            "Byoung Chan Lee",
            "Matloob Khushi",
            "Jinman Kim",
            "Adam G. Dunn"
        ],
        "published": "2022-04-09T18:01:18Z",
        "summary": "A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.04521v1.pdf"
    },
    {
        "title": "MR-iNet Gym: Framework for Edge Deployment of Deep Reinforcement Learning on Embedded Software Defined Radio",
        "authors": [
            "Jithin Jagannath",
            "Kian Hamedani",
            "Collin Farquhar",
            "Keyvan Ramezanpour",
            "Anu Jagannath"
        ],
        "published": "2022-04-09T16:28:43Z",
        "summary": "Dynamic resource allocation plays a critical role in the next generation of\nintelligent wireless communication systems. Machine learning has been leveraged\nas a powerful tool to make strides in this domain. In most cases, the progress\nhas been limited to simulations due to the challenging nature of hardware\ndeployment of these solutions. In this paper, for the first time, we design and\ndeploy deep reinforcement learning (DRL)-based power control agents on the GPU\nembedded software defined radios (SDRs). To this end, we propose an end-to-end\nframework (MR-iNet Gym) where the simulation suite and the embedded SDR\ndevelopment work cohesively to overcome real-world implementation hurdles. To\nprove feasibility, we consider the problem of distributed power control for\ncode-division multiple access (DS-CDMA)-based LPI/D transceivers. We first\nbuild a DS-CDMA ns3 module that interacts with the OpenAI Gym environment.\nNext, we train the power control DRL agents in this ns3-gym simulation\nenvironment in a scenario that replicates our hardware testbed. Next, for edge\n(embedded on-device) deployment, the trained models are optimized for real-time\noperation without loss of performance. Hardware-based evaluation verifies the\nefficiency of DRL agents over traditional distributed constrained power control\n(DCPC) algorithm. More significantly, as the primary goal, this is the first\nwork that has established the feasibility of deploying DRL to provide optimized\ndistributed resource allocation for next-generation of GPU-embedded radios.",
        "pdf_link": "https://arxiv.org/pdf/2204.04507v1.pdf"
    },
    {
        "title": "IDPG: An Instance-Dependent Prompt Generation Method",
        "authors": [
            "Zhuofeng Wu",
            "Sinong Wang",
            "Jiatao Gu",
            "Rui Hou",
            "Yuxiao Dong",
            "V. G. Vinod Vydiswaran",
            "Hao Ma"
        ],
        "published": "2022-04-09T15:45:27Z",
        "summary": "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a\ntask-specific prompt in each input instance during the model training stage. It\nfreezes the pre-trained language model and only optimizes a few task-specific\nprompts. In this paper, we propose a conditional prompt generation method to\ngenerate prompts for each input instance, referred to as the Instance-Dependent\nPrompt Generation (IDPG). Unlike traditional prompt tuning methods that use a\nfixed prompt, IDPG introduces a lightweight and trainable component to generate\nprompts based on each input sentence. Extensive experiments on ten natural\nlanguage understanding (NLU) tasks show that the proposed strategy consistently\noutperforms various prompt tuning baselines and is on par with other efficient\ntransfer learning methods such as Compacter while tuning far fewer model\nparameters.",
        "pdf_link": "https://arxiv.org/pdf/2204.04497v1.pdf"
    },
    {
        "title": "FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers",
        "authors": [
            "Dezhou Shen"
        ],
        "published": "2022-04-09T14:03:28Z",
        "summary": "The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.",
        "pdf_link": "https://arxiv.org/pdf/2204.04477v1.pdf"
    },
    {
        "title": "Modeling Multi-Granularity Hierarchical Features for Relation Extraction",
        "authors": [
            "Xinnian Liang",
            "Shuangzhi Wu",
            "Mu Li",
            "Zhoujun Li"
        ],
        "published": "2022-04-09T09:44:05Z",
        "summary": "Relation extraction is a key task in Natural Language Processing (NLP), which\naims to extract relations between entity pairs from given texts. Recently,\nrelation extraction (RE) has achieved remarkable progress with the development\nof deep neural networks. Most existing research focuses on constructing\nexplicit structured features using external knowledge such as knowledge graph\nand dependency tree. In this paper, we propose a novel method to extract\nmulti-granularity features based solely on the original input sentences. We\nshow that effective structured features can be attained even without external\nknowledge. Three kinds of features based on the input sentences are fully\nexploited, which are in entity mention level, segment level, and sentence\nlevel. All the three are jointly and hierarchically modeled. We evaluate our\nmethod on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred\nRevisited. To verify the effectiveness, we apply our method to different\nencoders such as LSTM and BERT. Experimental results show that our method\nsignificantly outperforms existing state-of-the-art models that even use\nexternal knowledge. Extensive analyses demonstrate that the performance of our\nmodel is contributed by the capture of multi-granularity features and the model\nof their hierarchical structure. Code and data are available at\n\\url{https://github.com/xnliang98/sms}.",
        "pdf_link": "https://arxiv.org/pdf/2204.04437v1.pdf"
    },
    {
        "title": "Should we tweet this? Generative response modeling for predicting reception of public health messaging on Twitter",
        "authors": [
            "Abraham Sanders",
            "Debjani Ray-Majumder",
            "John S. Erickson",
            "Kristin P. Bennett"
        ],
        "published": "2022-04-09T01:56:46Z",
        "summary": "The way people respond to messaging from public health organizations on\nsocial media can provide insight into public perceptions on critical health\nissues, especially during a global crisis such as COVID-19. It could be\nvaluable for high-impact organizations such as the US Centers for Disease\nControl and Prevention (CDC) or the World Health Organization (WHO) to\nunderstand how these perceptions impact reception of messaging on health policy\nrecommendations. We collect two datasets of public health messages and their\nresponses from Twitter relating to COVID-19 and Vaccines, and introduce a\npredictive method which can be used to explore the potential reception of such\nmessages. Specifically, we harness a generative model (GPT-2) to directly\npredict probable future responses and demonstrate how it can be used to\noptimize expected reception of important health guidance. Finally, we introduce\na novel evaluation scheme with extensive statistical testing which allows us to\nconclude that our models capture the semantics and sentiment found in actual\npublic health responses.",
        "pdf_link": "https://arxiv.org/pdf/2204.04353v2.pdf"
    },
    {
        "title": "Show, Don't Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue",
        "authors": [
            "Raghav Gupta",
            "Harrison Lee",
            "Jeffrey Zhao",
            "Abhinav Rastogi",
            "Yuan Cao",
            "Yonghui Wu"
        ],
        "published": "2022-04-08T23:27:18Z",
        "summary": "Building universal dialogue systems that operate across multiple domains/APIs\nand generalize to new ones with minimal overhead is a critical challenge.\nRecent works have leveraged natural language descriptions of schema elements to\nenable such systems; however, descriptions only indirectly convey schema\nsemantics. In this work, we propose Show, Don't Tell, which prompts seq2seq\nmodels with a labeled example dialogue to show the semantics of schema elements\nrather than tell the model through descriptions. While requiring similar effort\nfrom service developers as generating descriptions, we show that using short\nexamples as schema representations with large language models results in\nstate-of-the-art performance on two popular dialogue state tracking benchmarks\ndesigned to measure zero-shot generalization - the Schema-Guided Dialogue\ndataset and the MultiWOZ leave-one-out benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2204.04327v2.pdf"
    },
    {
        "title": "MMTAfrica: Multilingual Machine Translation for African Languages",
        "authors": [
            "Chris C. Emezue",
            "Bonaventure F. P. Dossou"
        ],
        "published": "2022-04-08T21:42:44Z",
        "summary": "In this paper, we focus on the task of multilingual machine translation for\nAfrican languages and describe our contribution in the 2021 WMT Shared Task:\nLarge-Scale Multilingual Machine Translation. We introduce MMTAfrica, the first\nmany-to-many multilingual translation system for six African languages: Fon\n(fon), Igbo (ibo), Kinyarwanda (kin), Swahili/Kiswahili (swa), Xhosa (xho), and\nYoruba (yor) and two non-African languages: English (eng) and French (fra). For\nmultilingual translation concerning African languages, we introduce a novel\nbacktranslation and reconstruction objective, BT\\&REC, inspired by the random\nonline back translation and T5 modeling framework respectively, to effectively\nleverage monolingual data. Additionally, we report improvements from MMTAfrica\nover the FLORES 101 benchmarks (spBLEU gains ranging from $+0.58$ in Swahili to\nFrench to $+19.46$ in French to Xhosa). We release our dataset and code source\nat https://github.com/edaiofficial/mmtafrica.",
        "pdf_link": "https://arxiv.org/pdf/2204.04306v1.pdf"
    },
    {
        "title": "Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models",
        "authors": [
            "Patrick Huber",
            "Giuseppe Carenini"
        ],
        "published": "2022-04-08T20:42:08Z",
        "summary": "With a growing number of BERTology work analyzing different components of\npre-trained language models, we extend this line of research through an\nin-depth analysis of discourse information in pre-trained and fine-tuned\nlanguage models. We move beyond prior work along three dimensions: First, we\ndescribe a novel approach to infer discourse structures from arbitrarily long\ndocuments. Second, we propose a new type of analysis to explore where and how\naccurately intrinsic discourse is captured in the BERT and BART models.\nFinally, we assess how similar the generated structures are to a variety of\nbaselines as well as their distribution within and between models.",
        "pdf_link": "https://arxiv.org/pdf/2204.04289v1.pdf"
    },
    {
        "title": "BioRED: A Rich Biomedical Relation Extraction Dataset",
        "authors": [
            "Ling Luo",
            "Po-Ting Lai",
            "Chih-Hsuan Wei",
            "Cecilia N Arighi",
            "Zhiyong Lu"
        ],
        "published": "2022-04-08T19:23:49Z",
        "summary": "Automated relation extraction (RE) from biomedical literature is critical for\nmany downstream text mining applications in both research and real-world\nsettings. However, most existing benchmarking datasets for bio-medical RE only\nfocus on relations of a single type (e.g., protein-protein interactions) at the\nsentence level, greatly limiting the development of RE systems in biomedicine.\nIn this work, we first review commonly used named entity recognition (NER) and\nRE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus\nwith multiple entity types (e.g., gene/protein, disease, chemical) and relation\npairs (e.g., gene-disease; chemical-chemical) at the document level, on a set\nof 600 PubMed abstracts. Further, we label each relation as describing either a\nnovel finding or previously known background knowledge, enabling automated\nalgorithms to differentiate between novel and background information. We assess\nthe utility of BioRED by benchmarking several existing state-of-the-art\nmethods, including BERT-based models, on the NER and RE tasks. Our results show\nthat while existing approaches can reach high performance on the NER task\n(F-score of 89.3%), there is much room for improvement for the RE task,\nespecially when extracting novel relations (F-score of 47.7%). Our experiments\nalso demonstrate that such a rich dataset can successfully facilitate the\ndevelopment of more accurate, efficient, and robust RE systems for biomedicine.\nThe BioRED dataset and annotation guideline are freely available at\nhttps://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/.",
        "pdf_link": "https://arxiv.org/pdf/2204.04263v2.pdf"
    },
    {
        "title": "Contextual Representation Learning beyond Masked Language Modeling",
        "authors": [
            "Zhiyi Fu",
            "Wangchunshu Zhou",
            "Jingjing Xu",
            "Hao Zhou",
            "Lei Li"
        ],
        "published": "2022-04-08T16:18:06Z",
        "summary": "How do masked language models (MLMs) such as BERT learn contextual\nrepresentations? In this work, we analyze the learning dynamics of MLMs. We\nfind that MLMs adopt sampled embeddings as anchors to estimate and inject\ncontextual semantics to representations, which limits the efficiency and\neffectiveness of MLMs. To address these issues, we propose TACO, a simple yet\neffective representation learning approach to directly model global semantics.\nTACO extracts and aligns contextual semantics hidden in contextualized\nrepresentations to encourage models to attend global semantics when generating\ncontextualized representations. Experiments on the GLUE benchmark show that\nTACO achieves up to 5x speedup and up to 1.2 points average improvement over\nexisting MLMs. The code is available at https://github.com/FUZHIYI/TACO.",
        "pdf_link": "https://arxiv.org/pdf/2204.04163v1.pdf"
    },
    {
        "title": "Fair and Argumentative Language Modeling for Computational Argumentation",
        "authors": [
            "Carolin Holtermann",
            "Anne Lauscher",
            "Simone Paolo Ponzetto"
        ],
        "published": "2022-04-08T12:23:46Z",
        "summary": "Although much work in NLP has focused on measuring and mitigating\nstereotypical bias in semantic spaces, research addressing bias in\ncomputational argumentation is still in its infancy. In this paper, we address\nthis research gap and conduct a thorough investigation of bias in argumentative\nlanguage models. To this end, we introduce ABBA, a novel resource for bias\nmeasurement specifically tailored to argumentation. We employ our resource to\nassess the effect of argumentative fine-tuning and debiasing on the intrinsic\nbias found in transformer-based language models using a lightweight\nadapter-based approach that is more sustainable and parameter-efficient than\nfull fine-tuning. Finally, we analyze the potential impact of language model\ndebiasing on the performance in argument quality prediction, a downstream task\nof computational argumentation. Our results show that we are able to\nsuccessfully and sustainably remove bias in general and argumentative language\nmodels while preserving (and sometimes improving) model performance in\ndownstream tasks. We make all experimental code and data available at\nhttps://github.com/umanlp/FairArgumentativeLM.",
        "pdf_link": "https://arxiv.org/pdf/2204.04026v1.pdf"
    },
    {
        "title": "Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation",
        "authors": [
            "Shumpei Inoue",
            "Tsungwei Liu",
            "Nguyen Hong Son",
            "Minh-Tien Nguyen"
        ],
        "published": "2022-04-08T09:32:18Z",
        "summary": "This paper introduces a model for incomplete utterance restoration (IUR)\ncalled JET (\\textbf{J}oint learning token \\textbf{E}xtraction and \\textbf{T}ext\ngeneration). Different from prior studies that only work on extraction or\nabstraction datasets, we design a simple but effective model, working for both\nscenarios of IUR. Our design simulates the nature of IUR, where omitted tokens\nfrom the context contribute to restoration. From this, we construct a Picker\nthat identifies the omitted tokens. To support the picker, we design two label\ncreation methods (soft and hard labels), which can work in cases of no\nannotation data for the omitted tokens. The restoration is done by using a\nGenerator with the help of the Picker on joint learning. Promising results on\nfour benchmark datasets in extraction and abstraction scenarios show that our\nmodel is better than the pretrained T5 and non-generative language model\nmethods in both rich and limited training data settings.\\footnote{The code is\navailable at \\url{https://github.com/shumpei19/JET}}",
        "pdf_link": "https://arxiv.org/pdf/2204.03958v3.pdf"
    },
    {
        "title": "RuBioRoBERTa: a pre-trained biomedical language model for Russian language biomedical text mining",
        "authors": [
            "Alexander Yalunin",
            "Alexander Nesterov",
            "Dmitriy Umerenkov"
        ],
        "published": "2022-04-08T09:18:59Z",
        "summary": "This paper presents several BERT-based models for Russian language biomedical\ntext mining (RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus\nof freely available texts in the Russian biomedical domain. With this\npre-training, our models demonstrate state-of-the-art results on RuMedBench -\nRussian medical language understanding benchmark that covers a diverse set of\ntasks, including text classification, question answering, natural language\ninference, and named entity recognition.",
        "pdf_link": "https://arxiv.org/pdf/2204.03951v1.pdf"
    },
    {
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model",
        "authors": [
            "Hongyi Yuan",
            "Zheng Yuan",
            "Ruyi Gan",
            "Jiaxing Zhang",
            "Yutao Xie",
            "Sheng Yu"
        ],
        "published": "2022-04-08T08:07:42Z",
        "summary": "Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2204.03905v2.pdf"
    },
    {
        "title": "Towards Semi-Supervised Learning of Automatic Post-Editing: Data-Synthesis by Infilling Mask with Erroneous Tokens",
        "authors": [
            "WonKee Lee",
            "Seong-Hwan Heo",
            "Baikjin Jung",
            "Jong-Hyeok Lee"
        ],
        "published": "2022-04-08T07:48:57Z",
        "summary": "Semi-supervised learning that leverages synthetic training data has been\nwidely adopted in the field of Automatic post-editing (APE) to overcome the\nlack of human-annotated training data. In that context, data-synthesis methods\nto create high-quality synthetic data have also received much attention.\nConsidering that APE takes machine-translation outputs containing translation\nerrors as input, we propose a noising-based data-synthesis method that uses a\nmask language model to create noisy texts through substituting masked tokens\nwith erroneous tokens, yet following the error-quantity statistics appearing in\ngenuine APE data. In addition, we propose corpus interleaving, which is to\ncombine two separate synthetic data by taking only advantageous samples, to\nfurther enhance the quality of the synthetic data created with our noising\nmethod. Experimental results reveal that using the synthetic data created with\nour approach results in significant improvements in APE performance upon using\nother synthetic data created with different existing data-synthesis methods.",
        "pdf_link": "https://arxiv.org/pdf/2204.03896v1.pdf"
    },
    {
        "title": "Infusing Knowledge from Wikipedia to Enhance Stance Detection",
        "authors": [
            "Zihao He",
            "Negar Mokhberian",
            "Kristina Lerman"
        ],
        "published": "2022-04-08T04:49:55Z",
        "summary": "Stance detection infers a text author's attitude towards a target. This is\nchallenging when the model lacks background knowledge about the target. Here,\nwe show how background knowledge from Wikipedia can help enhance the\nperformance on stance detection. We introduce Wikipedia Stance Detection BERT\n(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on\nthree benchmark datasets covering social media discussions and online debates\nindicate that our model significantly outperforms the state-of-the-art methods\non target-specific stance detection, cross-target stance detection, and\nzero/few-shot stance detection.",
        "pdf_link": "https://arxiv.org/pdf/2204.03839v1.pdf"
    },
    {
        "title": "Q-learning with online random forests",
        "authors": [
            "Joosung Min",
            "Lloyd T. Elliott"
        ],
        "published": "2022-04-07T23:00:39Z",
        "summary": "$Q$-learning is the most fundamental model-free reinforcement learning\nalgorithm. Deployment of $Q$-learning requires approximation of the\nstate-action value function (also known as the $Q$-function). In this work, we\nprovide online random forests as $Q$-function approximators and propose a novel\nmethod wherein the random forest is grown as learning proceeds (through\nexpanding forests). We demonstrate improved performance of our methods over\nstate-of-the-art Deep $Q$-Networks in two OpenAI gyms (`blackjack' and\n`inverted pendulum') but not in the `lunar lander' gym. We suspect that the\nresilience to overfitting enjoyed by random forests recommends our method for\ncommon tasks that do not require a strong representation of the problem domain.\nWe show that expanding forests (in which the number of trees increases as data\ncomes in) improve performance, suggesting that expanding forests are viable for\nother applications of online random forests beyond the reinforcement learning\nsetting.",
        "pdf_link": "https://arxiv.org/pdf/2204.03771v1.pdf"
    },
    {
        "title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
        "authors": [
            "Wanyu Du",
            "Zae Myung Kim",
            "Vipul Raheja",
            "Dhruv Kumar",
            "Dongyeop Kang"
        ],
        "published": "2022-04-07T18:33:10Z",
        "summary": "Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.",
        "pdf_link": "https://arxiv.org/pdf/2204.03685v2.pdf"
    },
    {
        "title": "Testing the limits of natural language models for predicting human language judgments",
        "authors": [
            "Tal Golan",
            "Matthew Siegelman",
            "Nikolaus Kriegeskorte",
            "Christopher Baldassano"
        ],
        "published": "2022-04-07T17:12:57Z",
        "summary": "Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.",
        "pdf_link": "https://arxiv.org/pdf/2204.03592v3.pdf"
    },
    {
        "title": "BERTuit: Understanding Spanish language in Twitter through a native transformer",
        "authors": [
            "Javier Huertas-Tato",
            "Alejandro Martin",
            "David Camacho"
        ],
        "published": "2022-04-07T14:28:51Z",
        "summary": "The appearance of complex attention-based language models such as BERT,\nRoberta or GPT-3 has allowed to address highly complex tasks in a plethora of\nscenarios. However, when applied to specific domains, these models encounter\nconsiderable difficulties. This is the case of Social Networks such as Twitter,\nan ever-changing stream of information written with informal and complex\nlanguage, where each message requires careful evaluation to be understood even\nby humans given the important role that context plays. Addressing tasks in this\ndomain through Natural Language Processing involves severe challenges. When\npowerful state-of-the-art multilingual language models are applied to this\nscenario, language specific nuances use to get lost in translation. To face\nthese challenges we present \\textbf{BERTuit}, the larger transformer proposed\nso far for Spanish language, pre-trained on a massive dataset of 230M Spanish\ntweets using RoBERTa optimization. Our motivation is to provide a powerful\nresource to better understand Spanish Twitter and to be used on applications\nfocused on this social network, with special emphasis on solutions devoted to\ntackle the spreading of misinformation in this platform. BERTuit is evaluated\non several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very\ncompetitive multilingual transformers. The utility of our approach is shown\nwith applications, in this case: a zero-shot methodology to visualize groups of\nhoaxes and profiling authors spreading disinformation.\n  Misinformation spreads wildly on platforms such as Twitter in languages other\nthan English, meaning performance of transformers may suffer when transferred\noutside English speaking communities.",
        "pdf_link": "https://arxiv.org/pdf/2204.03465v2.pdf"
    },
    {
        "title": "MAESTRO: Matched Speech Text Representations through Modality Matching",
        "authors": [
            "Zhehuai Chen",
            "Yu Zhang",
            "Andrew Rosenberg",
            "Bhuvana Ramabhadran",
            "Pedro Moreno",
            "Ankur Bapna",
            "Heiga Zen"
        ],
        "published": "2022-04-07T12:48:16Z",
        "summary": "We present Maestro, a self-supervised training method to unify\nrepresentations learnt from speech and text modalities. Self-supervised\nlearning from speech signals aims to learn the latent structure inherent in the\nsignal, while self-supervised learning from text attempts to capture lexical\ninformation. Learning aligned representations from unpaired speech and text\nsequences is a challenging task. Previous work either implicitly enforced the\nrepresentations learnt from these two modalities to be aligned in the latent\nspace through multitasking and parameter sharing or explicitly through\nconversion of modalities via speech synthesis. While the former suffers from\ninterference between the two modalities, the latter introduces additional\ncomplexity. In this paper, we propose Maestro, a novel algorithm to learn\nunified representations from both these modalities simultaneously that can\ntransfer to diverse downstream tasks such as Automated Speech Recognition (ASR)\nand Speech Translation (ST). Maestro learns unified representations through\nsequence alignment, duration prediction and matching embeddings in the learned\nspace through an aligned masked-language model loss. We establish a new\nstate-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative\nreduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)\nand 21 languages to English multilingual ST on CoVoST 2 with an improvement of\n2.8 BLEU averaged over 21 languages.",
        "pdf_link": "https://arxiv.org/pdf/2204.03409v2.pdf"
    },
    {
        "title": "Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation",
        "authors": [
            "Ngo Quang Huy",
            "Tu Minh Phuong",
            "Ngo Xuan Bach"
        ],
        "published": "2022-04-07T09:43:51Z",
        "summary": "An ultimate goal of artificial intelligence is to build computer systems that\ncan understand human languages. Understanding commonsense knowledge about the\nworld expressed in text is one of the foundational and challenging problems to\ncreate such intelligent systems. As a step towards this goal, we present in\nthis paper ALMEn, an Autoencoding Language Model based Ensemble learning method\nfor commonsense validation and explanation. By ensembling several advanced\npre-trained language models including RoBERTa, DeBERTa, and ELECTRA with\nSiamese neural networks, our method can distinguish natural language statements\nthat are against commonsense (validation subtask) and correctly identify the\nreason for making against commonsense (explanation selection subtask).\nExperimental results on the benchmark dataset of SemEval-2020 Task 4 show that\nour method outperforms state-of-the-art models, reaching 97.9% and 95.4%\naccuracies on the validation and explanation selection subtasks, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2204.03324v1.pdf"
    },
    {
        "title": "Towards Automatic Construction of Filipino WordNet: Word Sense Induction and Synset Induction Using Sentence Embeddings",
        "authors": [
            "Dan John Velasco",
            "Axel Alba",
            "Trisha Gail Pelagio",
            "Bryce Anthony Ramirez",
            "Unisse Chua",
            "Briane Paul Samson",
            "Jan Christian Blaise Cruz",
            "Charibeth Cheng"
        ],
        "published": "2022-04-07T06:50:37Z",
        "summary": "Wordnets are indispensable tools for various natural language processing\napplications. Unfortunately, wordnets get outdated, and producing or updating\nwordnets can be slow and costly in terms of time and resources. This problem\nintensifies for low-resource languages. This study proposes a method for word\nsense induction and synset induction using only two linguistic resources,\nnamely, an unlabeled corpus and a sentence embeddings-based language model. The\nresulting sense inventory and synonym sets can be used in automatically\ncreating a wordnet. We applied this method on a corpus of Filipino text. The\nsense inventory and synsets were evaluated by matching them with the sense\ninventory of the machine translated Princeton WordNet, as well as comparing the\nsynsets to the Filipino WordNet. This study empirically shows that the 30% of\nthe induced word senses are valid and 40% of the induced synsets are valid in\nwhich 20% are novel synsets.",
        "pdf_link": "https://arxiv.org/pdf/2204.03251v3.pdf"
    },
    {
        "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators",
        "authors": [
            "Yu Meng",
            "Chenyan Xiong",
            "Payal Bajaj",
            "Saurabh Tiwary",
            "Paul Bennett",
            "Jiawei Han",
            "Xia Song"
        ],
        "published": "2022-04-07T06:19:06Z",
        "summary": "We present a new framework AMOS that pretrains text encoders with an\nAdversarial learning curriculum via a Mixture Of Signals from multiple\nauxiliary generators. Following ELECTRA-style pretraining, the main encoder is\ntrained as a discriminator to detect replaced tokens generated by auxiliary\nmasked language models (MLMs). Different from ELECTRA which trains one MLM as\nthe generator, we jointly train multiple MLMs of different sizes to provide\ntraining signals at various levels of difficulty. To push the discriminator to\nlearn better with challenging replaced tokens, we learn mixture weights over\nthe auxiliary MLMs' outputs to maximize the discriminator loss by\nbackpropagating the gradient from the discriminator via Gumbel-Softmax. For\nbetter pretraining efficiency, we propose a way to assemble multiple MLMs into\none unified auxiliary model. AMOS outperforms ELECTRA and recent\nstate-of-the-art pretrained models by about 1 point on the GLUE benchmark for\nBERT base-sized models.",
        "pdf_link": "https://arxiv.org/pdf/2204.03243v1.pdf"
    },
    {
        "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning",
        "authors": [
            "Zheng Li",
            "Soroush Ghodrati",
            "Amir Yazdanbakhsh",
            "Hadi Esmaeilzadeh",
            "Mingu Kang"
        ],
        "published": "2022-04-07T05:31:13Z",
        "summary": "Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (<0.2% degradation)",
        "pdf_link": "https://arxiv.org/pdf/2204.03227v3.pdf"
    },
    {
        "title": "Knowledge Infused Decoding",
        "authors": [
            "Ruibo Liu",
            "Guoqing Zheng",
            "Shashank Gupta",
            "Radhika Gaonkar",
            "Chongyang Gao",
            "Soroush Vosoughi",
            "Milad Shokouhi",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2022-04-06T20:58:32Z",
        "summary": "Pre-trained language models (LMs) have been shown to memorize a substantial\namount of knowledge from the pre-training corpora; however, they are still\nlimited in recalling factually correct knowledge given a certain context.\nHence, they tend to suffer from counterfactual or hallucinatory generation when\nused in knowledge-intensive natural language generation (NLG) tasks. Recent\nremedies to this problem focus on modifying either the pre-training or task\nfine-tuning objectives to incorporate knowledge, which normally require\nadditional costly training or architecture modification of LMs for practical\napplications. We present Knowledge Infused Decoding (KID) -- a novel decoding\nalgorithm for generative LMs, which dynamically infuses external knowledge into\neach step of the LM decoding. Specifically, we maintain a local knowledge\nmemory based on the current context, interacting with a dynamically created\nexternal knowledge trie, and continuously update the local memory as a\nknowledge-aware constraint to guide decoding via reinforcement learning. On six\ndiverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)\narmed with KID outperform many task-optimized state-of-the-art models, and show\nparticularly strong performance in few-shot scenarios over seven related\nknowledge-infusion techniques. Human evaluation confirms KID's ability to\ngenerate more relevant and factual language for the input context when compared\nwith multiple baselines. Finally, KID also alleviates exposure bias and\nprovides stable generation quality when generating longer sequences. Code for\nKID is available at https://github.com/microsoft/KID.",
        "pdf_link": "https://arxiv.org/pdf/2204.03084v1.pdf"
    },
    {
        "title": "Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding",
        "authors": [
            "Shanshan Wang",
            "Zhumin Chen",
            "Zhaochun Ren",
            "Huasheng Liang",
            "Qiang Yan",
            "Pengjie Ren"
        ],
        "published": "2022-04-06T16:22:02Z",
        "summary": "Pre-trained language models (PLM) have demonstrated their effectiveness for a\nbroad range of information retrieval and natural language processing tasks. As\nthe core part of PLM, multi-head self-attention is appealing for its ability to\njointly attend to information from different positions. However, researchers\nhave found that PLM always exhibits fixed attention patterns regardless of the\ninput (e.g., excessively paying attention to [CLS] or [SEP]), which we argue\nmight neglect important information in the other positions. In this work, we\npropose a simple yet effective attention guiding mechanism to improve the\nperformance of PLM by encouraging attention towards the established goals.\nSpecifically, we propose two kinds of attention guiding methods, i.e., map\ndiscrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).\nThe former definitely encourages the diversity among multiple self-attention\nheads to jointly attend to information from different representation subspaces,\nwhile the latter encourages self-attention to attend to as many different\npositions of the input as possible. We conduct experiments with multiple\ngeneral pre-trained models (i.e., BERT, ALBERT, and Roberta) and\ndomain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and\nSciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and\nCross-genre-IR). Extensive experimental results demonstrate that our proposed\nMDG and PDG bring stable performance improvements on all datasets with high\nefficiency and low cost.",
        "pdf_link": "https://arxiv.org/pdf/2204.02922v1.pdf"
    },
    {
        "title": "drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM",
        "authors": [
            "Dylan Phelps"
        ],
        "published": "2022-04-06T13:32:37Z",
        "summary": "This paper describes our system for SemEval-2022 Task 2 Multilingual\nIdiomaticity Detection and Sentence Embedding sub-task B. We modify a standard\nBERT sentence transformer by adding embeddings for each idioms, which are\ncreated using BERTRAM and a small number of contexts. We show that this\ntechnique increases the quality of idiom representations and leads to better\nperformance on the task. We also perform analysis on our final results and show\nthat the quality of the produced idiom embeddings is highly sensitive to the\nquality of the input contexts.",
        "pdf_link": "https://arxiv.org/pdf/2204.02821v3.pdf"
    },
    {
        "title": "SecureBERT: A Domain-Specific Language Model for Cybersecurity",
        "authors": [
            "Ehsan Aghaei",
            "Xi Niu",
            "Waseem Shadid",
            "Ehab Al-Shaer"
        ],
        "published": "2022-04-06T09:17:21Z",
        "summary": "Natural Language Processing (NLP) has recently gained wide attention in\ncybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber\nautomation. Increased connection and automation have revolutionized the world's\neconomic and cultural infrastructures, while they have introduced risks in\nterms of cyber attacks. CTI is information that helps cybersecurity analysts\nmake intelligent security decisions, that is often delivered in the form of\nnatural language text, which must be transformed to machine readable format\nthrough an automated procedure before it can be used for automated security\nmeasures.\n  This paper proposes SecureBERT, a cybersecurity language model capable of\ncapturing text connotations in cybersecurity text (e.g., CTI) and therefore\nsuccessful in automation for many critical cybersecurity tasks that would\notherwise rely on human expertise and time-consuming manual efforts. SecureBERT\nhas been trained using a large corpus of cybersecurity text.To make SecureBERT\neffective not just in retaining general English understanding, but also when\napplied to text with cybersecurity implications, we developed a customized\ntokenizer as well as a method to alter pre-trained weights. The SecureBERT is\nevaluated using the standard Masked Language Model (MLM) test as well as two\nadditional standard NLP tasks. Our evaluation studies show that\nSecureBERT\\footnote{\\url{https://github.com/ehsanaghaei/SecureBERT}}\noutperforms existing similar models, confirming its capability for solving\ncrucial NLP tasks in cybersecurity.",
        "pdf_link": "https://arxiv.org/pdf/2204.02685v3.pdf"
    },
    {
        "title": "Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis of BERT Classifiers and Weak Supervision",
        "authors": [
            "Duygu Ider",
            "Stefan Lessmann"
        ],
        "published": "2022-04-06T07:45:05Z",
        "summary": "Anticipating price developments in financial markets is a topic of continued\ninterest in forecasting. Funneled by advancements in deep learning and natural\nlanguage processing (NLP) together with the availability of vast amounts of\ntextual data in form of news articles, social media postings, etc., an\nincreasing number of studies incorporate text-based predictors in forecasting\nmodels. We contribute to this literature by introducing weak learning, a\nrecently proposed NLP approach to address the problem that text data is\nunlabeled. Without a dependent variable, it is not possible to finetune\npretrained NLP models on a custom corpus. We confirm that finetuning using weak\nlabels enhances the predictive value of text-based features and raises forecast\naccuracy in the context of predicting cryptocurrency returns. More\nfundamentally, the modeling paradigm we present, weak labeling domain-specific\ntext and finetuning pretrained NLP models, is universally applicable in\n(financial) forecasting and unlocks new ways to leverage text data.",
        "pdf_link": "https://arxiv.org/pdf/2204.05781v3.pdf"
    },
    {
        "title": "DAGAM: Data Augmentation with Generation And Modification",
        "authors": [
            "Byeong-Cheol Jo",
            "Tak-Sung Heo",
            "Yeongjoon Park",
            "Yongmin Yoo",
            "Won Ik Cho",
            "Kyungsun Kim"
        ],
        "published": "2022-04-06T07:20:45Z",
        "summary": "Text classification is a representative downstream task of natural language\nprocessing, and has exhibited excellent performance since the advent of\npre-trained language models based on Transformer architecture. However, in\npre-trained language models, under-fitting often occurs due to the size of the\nmodel being very large compared to the amount of available training data. Along\nwith significant importance of data collection in modern machine learning\nparadigm, studies have been actively conducted for natural language data\naugmentation. In light of this, we introduce three data augmentation schemes\nthat help reduce underfitting problems of large-scale language models.\nPrimarily we use a generation model for data augmentation, which is defined as\nData Augmentation with Generation (DAG). Next, we augment data using text\nmodification techniques such as corruption and word order change (Data\nAugmentation with Modification, DAM). Finally, we propose Data Augmentation\nwith Generation And Modification (DAGAM), which combines DAG and DAM techniques\nfor a boosted performance. We conduct data augmentation for six benchmark\ndatasets of text classification task, and verify the usefulness of DAG, DAM,\nand DAGAM through BERT-based fine-tuning and evaluation, deriving better\nresults compared to the performance with original datasets.",
        "pdf_link": "https://arxiv.org/pdf/2204.02633v1.pdf"
    },
    {
        "title": "Structure-aware Protein Self-supervised Learning",
        "authors": [
            "Can Chen",
            "Jingbo Zhou",
            "Fan Wang",
            "Xue Liu",
            "Dejing Dou"
        ],
        "published": "2022-04-06T02:18:41Z",
        "summary": "Protein representation learning methods have shown great potential to yield\nuseful representation for many downstream tasks, especially on protein\nclassification. Moreover, a few recent studies have shown great promise in\naddressing insufficient labels of proteins with self-supervised learning\nmethods. However, existing protein language models are usually pretrained on\nprotein sequences without considering the important protein structural\ninformation. To this end, we propose a novel structure-aware protein\nself-supervised learning method to effectively capture structural information\nof proteins. In particular, a well-designed graph neural network (GNN) model is\npretrained to preserve the protein structural information with self-supervised\ntasks from a pairwise residue distance perspective and a dihedral angle\nperspective, respectively. Furthermore, we propose to leverage the available\nprotein language model pretrained on protein sequences to enhance the\nself-supervised learning. Specifically, we identify the relation between the\nsequential information in the protein language model and the structural\ninformation in the specially designed GNN model via a novel pseudo bi-level\noptimization scheme. Experiments on several supervised downstream tasks verify\nthe effectiveness of our proposed method.The code of the proposed method is\navailable in \\url{https://github.com/GGchen1997/STEPS_Bioinformatics}.",
        "pdf_link": "https://arxiv.org/pdf/2204.04213v4.pdf"
    },
    {
        "title": "An Exploratory Study on Code Attention in BERT",
        "authors": [
            "Rishab Sharma",
            "Fuxiang Chen",
            "Fatemeh Fard",
            "David Lo"
        ],
        "published": "2022-04-05T21:23:10Z",
        "summary": "Many recent models in software engineering introduced deep neural models\nbased on the Transformer architecture or use transformer-based Pre-trained\nLanguage Models (PLM) trained on code. Although these models achieve the state\nof the arts results in many downstream tasks such as code summarization and bug\ndetection, they are based on Transformer and PLM, which are mainly studied in\nthe Natural Language Processing (NLP) field. The current studies rely on the\nreasoning and practices from NLP for these models in code, despite the\ndifferences between natural languages and programming languages. There is also\nlimited literature on explaining how code is modeled.\n  Here, we investigate the attention behavior of PLM on code and compare it\nwith natural language. We pre-trained BERT, a Transformer based PLM, on code\nand explored what kind of information it learns, both semantic and syntactic.\nWe run several experiments to analyze the attention values of code constructs\non each other and what BERT learns in each layer. Our analyses show that BERT\npays more attention to syntactic entities, specifically identifiers and\nseparators, in contrast to the most attended token [CLS] in NLP. This\nobservation motivated us to leverage identifiers to represent the code sequence\ninstead of the [CLS] token when used for code clone detection. Our results show\nthat employing embeddings from identifiers increases the performance of BERT by\n605% and 4% F1-score in its lower layers and the upper layers, respectively.\nWhen identifiers' embeddings are used in CodeBERT, a code-based PLM, the\nperformance is improved by 21-24% in the F1-score of clone detection. The\nfindings can benefit the research community by using code-specific\nrepresentations instead of applying the common embeddings used in NLP, and open\nnew directions for developing smaller models with similar performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.10200v1.pdf"
    },
    {
        "title": "LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition",
        "authors": [
            "Rishab Sharma",
            "Fuxiang Chen",
            "Fatemeh Fard"
        ],
        "published": "2022-04-05T20:53:06Z",
        "summary": "Code comment generation is the task of generating a high-level natural\nlanguage description for a given code method or function. Although researchers\nhave been studying multiple ways to generate code comments automatically,\nprevious work mainly considers representing a code token in its entirety\nsemantics form only (e.g., a language model is used to learn the semantics of a\ncode token), and additional code properties such as the tree structure of a\ncode are included as an auxiliary input to the model. There are two\nlimitations: 1) Learning the code token in its entirety form may not be able to\ncapture information succinctly in source code, and 2) The code token does not\ncontain additional syntactic information, inherently important in programming\nlanguages.\n  In this paper, we present LAnguage Model and Named Entity Recognition\n(LAMNER), a code comment generator capable of encoding code constructs\neffectively and capturing the structural property of a code token. A\ncharacter-level language model is used to learn the semantic representation to\nencode a code token. For the structural property of a token, a Named Entity\nRecognition model is trained to learn the different types of code tokens. These\nrepresentations are then fed into an encoder-decoder architecture to generate\ncode comments. We evaluate the generated comments from LAMNER and other\nbaselines on a popular Java dataset with four commonly used metrics. Our\nresults show that LAMNER is effective and improves over the best baseline model\nin BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,\n18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,\nwe fused LAMNER's code representation with the baseline models, and the fused\nmodels consistently showed improvement over the non-fused models. The human\nevaluation further shows that LAMNER produces high-quality code comments.",
        "pdf_link": "https://arxiv.org/pdf/2204.09654v1.pdf"
    },
    {
        "title": "On the Effectiveness of Pretrained Models for API Learning",
        "authors": [
            "Mohammad Abdul Hadi",
            "Imam Nur Bani Yusuf",
            "Ferdian Thung",
            "Kien Gia Luong",
            "Jiang Lingxiao",
            "Fatemeh H. Fard",
            "David Lo"
        ],
        "published": "2022-04-05T20:33:24Z",
        "summary": "Developers frequently use APIs to implement certain functionalities, such as\nparsing Excel Files, reading and writing text files line by line, etc.\nDevelopers can greatly benefit from automatic API usage sequence generation\nbased on natural language queries for building applications in a faster and\ncleaner manner. Existing approaches utilize information retrieval models to\nsearch for matching API sequences given a query or use RNN-based\nencoder-decoder to generate API sequences. As it stands, the first approach\ntreats queries and API names as bags of words. It lacks deep comprehension of\nthe semantics of the queries. The latter approach adapts a neural language\nmodel to encode a user query into a fixed-length context vector and generate\nAPI sequences from the context vector.\n  We want to understand the effectiveness of recent Pre-trained Transformer\nbased Models (PTMs) for the API learning task. These PTMs are trained on large\nnatural language corpora in an unsupervised manner to retain contextual\nknowledge about the language and have found success in solving similar Natural\nLanguage Processing (NLP) problems. However, the applicability of PTMs has not\nyet been explored for the API sequence generation task. We use a dataset that\ncontains 7 million annotations collected from GitHub to evaluate the PTMs\nempirically. This dataset was also used to assess previous approaches. Based on\nour results, PTMs generate more accurate API sequences and outperform other\nrelated methods by around 11%. We have also identified two different\ntokenization approaches that can contribute to a significant boost in PTMs'\nperformance for the API sequence generation task.",
        "pdf_link": "https://arxiv.org/pdf/2204.03498v1.pdf"
    },
    {
        "title": "PaLM: Scaling Language Modeling with Pathways",
        "authors": [
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Jacob Devlin",
            "Maarten Bosma",
            "Gaurav Mishra",
            "Adam Roberts",
            "Paul Barham",
            "Hyung Won Chung",
            "Charles Sutton",
            "Sebastian Gehrmann",
            "Parker Schuh",
            "Kensen Shi",
            "Sasha Tsvyashchenko",
            "Joshua Maynez",
            "Abhishek Rao",
            "Parker Barnes",
            "Yi Tay",
            "Noam Shazeer",
            "Vinodkumar Prabhakaran",
            "Emily Reif",
            "Nan Du",
            "Ben Hutchinson",
            "Reiner Pope",
            "James Bradbury",
            "Jacob Austin",
            "Michael Isard",
            "Guy Gur-Ari",
            "Pengcheng Yin",
            "Toju Duke",
            "Anselm Levskaya",
            "Sanjay Ghemawat",
            "Sunipa Dev",
            "Henryk Michalewski",
            "Xavier Garcia",
            "Vedant Misra",
            "Kevin Robinson",
            "Liam Fedus",
            "Denny Zhou",
            "Daphne Ippolito",
            "David Luan",
            "Hyeontaek Lim",
            "Barret Zoph",
            "Alexander Spiridonov",
            "Ryan Sepassi",
            "David Dohan",
            "Shivani Agrawal",
            "Mark Omernick",
            "Andrew M. Dai",
            "Thanumalayan Sankaranarayana Pillai",
            "Marie Pellat",
            "Aitor Lewkowycz",
            "Erica Moreira",
            "Rewon Child",
            "Oleksandr Polozov",
            "Katherine Lee",
            "Zongwei Zhou",
            "Xuezhi Wang",
            "Brennan Saeta",
            "Mark Diaz",
            "Orhan Firat",
            "Michele Catasta",
            "Jason Wei",
            "Kathy Meier-Hellstern",
            "Douglas Eck",
            "Jeff Dean",
            "Slav Petrov",
            "Noah Fiedel"
        ],
        "published": "2022-04-05T16:11:45Z",
        "summary": "Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.",
        "pdf_link": "https://arxiv.org/pdf/2204.02311v5.pdf"
    },
    {
        "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
        "authors": [
            "Robert Litschko",
            "Ivan Vuli\u0107",
            "Goran Glava\u0161"
        ],
        "published": "2022-04-05T15:44:27Z",
        "summary": "State-of-the-art neural (re)rankers are notoriously data-hungry which --\ngiven the lack of large-scale training data in languages other than English --\nmakes them rarely used in multilingual and cross-lingual retrieval settings.\nCurrent approaches therefore commonly transfer rankers trained on English data\nto other languages and cross-lingual setups by means of multilingual encoders:\nthey fine-tune all parameters of pretrained massively multilingual Transformers\n(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy\nthem in the target language(s). In this work, we show that two\nparameter-efficient approaches to cross-lingual transfer, namely Sparse\nFine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more\neffective zero-shot transfer to multilingual and cross-lingual retrieval tasks.\nWe first train language adapters (or SFTMs) via Masked Language Modelling and\nthen train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping\nall other parameters fixed. At inference, this modular design allows us to\ncompose the ranker by applying the (re)ranking adapter (or SFTM) trained with\nsource language data together with the language adapter (or SFTM) of a target\nlanguage. We carry out a large scale evaluation on the CLEF-2003 and HC4\nbenchmarks and additionally, as another contribution, extend the former with\nqueries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed\nparameter-efficient methods outperform standard zero-shot transfer with full\nMMT fine-tuning, while being more modular and reducing training times. The\ngains are particularly pronounced for low-resource languages, where our\napproaches also substantially outperform the competitive machine\ntranslation-based rankers.",
        "pdf_link": "https://arxiv.org/pdf/2204.02292v2.pdf"
    },
    {
        "title": "Abstractive summarization of hospitalisation histories with transformer networks",
        "authors": [
            "Alexander Yalunin",
            "Dmitriy Umerenkov",
            "Vladimir Kokh"
        ],
        "published": "2022-04-05T13:38:39Z",
        "summary": "In this paper we present a novel approach to abstractive summarization of\npatient hospitalisation histories. We applied an encoder-decoder framework with\nLongformer neural network as an encoder and BERT as a decoder. Our experiments\nshow improved quality on some summarization tasks compared with\npointer-generator networks. We also conducted a study with experienced\nphysicians evaluating the results of our model in comparison with PGN baseline\nand human-generated abstracts, which showed the effectiveness of our model.",
        "pdf_link": "https://arxiv.org/pdf/2204.02208v1.pdf"
    },
    {
        "title": "Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture for Complex NER",
        "authors": [
            "Amit Pandey",
            "Swayatta Daw",
            "Vikram Pudi"
        ],
        "published": "2022-04-05T12:58:57Z",
        "summary": "We investigate the task of complex NER for the English language. The task is\nnon-trivial due to the semantic ambiguity of the textual structure and the\nrarity of occurrence of such entities in the prevalent literature. Using\npre-trained language models such as BERT, we obtain a competitive performance\non this task. We qualitatively analyze the performance of multiple\narchitectures for this task. All our models are able to outperform the baseline\nby a significant margin. Our best performing model beats the baseline F1-score\nby over 9%.",
        "pdf_link": "https://arxiv.org/pdf/2204.02173v1.pdf"
    },
    {
        "title": "SemanticCAP: Chromatin Accessibility Prediction Enhanced by Features Learning from a Language Model",
        "authors": [
            "Yikang Zhang",
            "Xiaomin Chu",
            "Yelu Jiang",
            "Hongjie Wu",
            "Lijun Quan"
        ],
        "published": "2022-04-05T11:47:58Z",
        "summary": "A large number of inorganic and organic compounds are able to bind DNA and\nform complexes, among which drug-related molecules are important. Chromatin\naccessibility changes not only directly affects drug-DNA interactions, but also\npromote or inhibit the expression of critical genes associated with drug\nresistance by affecting the DNA binding capacity of TFs and transcriptional\nregulators. However, Biological experimental techniques for measuring it are\nexpensive and time consuming. In recent years, several kinds of computational\nmethods have been proposed to identify accessible regions of the genome.\nExisting computational models mostly ignore the contextual information of bases\nin gene sequences. To address these issues, we proposed a new solution named\nSemanticCAP. It introduces a gene language model which models the context of\ngene sequences, thus being able to provide an effective representation of a\ncertain site in gene sequences. Basically, we merge the features provided by\nthe gene language model into our chromatin accessibility model. During the\nprocess, we designed some methods to make feature fusion smoother. Compared\nwith other systems under public benchmarks, our model proved to have better\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2204.02130v2.pdf"
    },
    {
        "title": "How Different are Pre-trained Transformers for Text Ranking?",
        "authors": [
            "David Rau",
            "Jaap Kamps"
        ],
        "published": "2022-04-05T10:48:52Z",
        "summary": "In recent years, large pre-trained transformers have led to substantial gains\nin performance over traditional retrieval models and feedback approaches.\nHowever, these results are primarily based on the MS Marco/TREC Deep Learning\nTrack setup, with its very particular setup, and our understanding of why and\nhow these models work better is fragmented at best. We analyze effective\nBERT-based cross-encoders versus traditional BM25 ranking for the passage\nretrieval task where the largest gains have been observed, and investigate two\nmain questions. On the one hand, what is similar? To what extent does the\nneural ranker already encompass the capacity of traditional rankers? Is the\ngain in performance due to a better ranking of the same documents (prioritizing\nprecision)? On the other hand, what is different? Can it retrieve effectively\ndocuments missed by traditional systems (prioritizing recall)? We discover\nsubstantial differences in the notion of relevance identifying strengths and\nweaknesses of BERT that may inspire research for future improvement. Our\nresults contribute to our understanding of (black-box) neural rankers relative\nto (well-understood) traditional rankers, help understand the particular\nexperimental setting of MS-Marco-based test collections.",
        "pdf_link": "https://arxiv.org/pdf/2204.07233v1.pdf"
    },
    {
        "title": "A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition",
        "authors": [
            "Ye-Qian Du",
            "Jie Zhang",
            "Qiu-Shi Zhu",
            "Li-Rong Dai",
            "Ming-Hui Wu",
            "Xin Fang",
            "Zhou-Wang Yang"
        ],
        "published": "2022-04-05T07:02:53Z",
        "summary": "Unpaired data has shown to be beneficial for low-resource automatic speech\nrecognition~(ASR), which can be involved in the design of hybrid models with\nmulti-task training or language model dependent pre-training. In this work, we\nleverage unpaired data to train a general sequence-to-sequence model. Unpaired\nspeech and text are used in the form of data pairs by generating the\ncorresponding missing parts in prior to model training. Inspired by the\ncomplementarity of speech-PseudoLabel pair and SynthesizedAudio-text pair in\nboth acoustic features and linguistic features, we propose a complementary\njoint training~(CJT) method that trains a model alternatively with two data\npairs. Furthermore, label masking for pseudo-labels and gradient restriction\nfor synthesized audio are proposed to further cope with the deviations from\nreal data, termed as CJT++. Experimental results show that compared to\nspeech-only training, the proposed basic CJT achieves great performance\nimprovements on clean/other test sets, and the CJT++ re-training yields further\nperformance enhancements. It is also apparent that the proposed method\noutperforms the wav2vec2.0 model with the same model size and beam size,\nparticularly in extreme low-resource cases.",
        "pdf_link": "https://arxiv.org/pdf/2204.02023v1.pdf"
    },
    {
        "title": "Data Augmentation for Intent Classification with Off-the-shelf Large Language Models",
        "authors": [
            "Gaurav Sahu",
            "Pau Rodriguez",
            "Issam H. Laradji",
            "Parmida Atighehchian",
            "David Vazquez",
            "Dzmitry Bahdanau"
        ],
        "published": "2022-04-05T03:29:26Z",
        "summary": "Data augmentation is a widely employed technique to alleviate the problem of\ndata scarcity. In this work, we propose a prompting-based approach to generate\nlabelled training data for intent classification with off-the-shelf language\nmodels (LMs) such as GPT-3. An advantage of this method is that no\ntask-specific LM-fine-tuning for data generation is required; hence the method\nrequires no hyper-parameter tuning and is applicable even when the available\ntraining data is very scarce. We evaluate the proposed method in a few-shot\nsetting on four diverse intent classification tasks. We find that GPT-generated\ndata significantly boosts the performance of intent classifiers when intents in\nconsideration are sufficiently distinct from each other. In tasks with\nsemantically close intents, we observe that the generated data is less helpful.\nOur analysis shows that this is because GPT often generates utterances that\nbelong to a closely-related intent instead of the desired one. We present\npreliminary evidence that a prompting-based GPT classifier could be helpful in\nfiltering the generated data to enhance its quality.",
        "pdf_link": "https://arxiv.org/pdf/2204.01959v1.pdf"
    },
    {
        "title": "Applying Automatic Text Summarization for Fake News Detection",
        "authors": [
            "Philipp Hartl",
            "Udo Kruschwitz"
        ],
        "published": "2022-04-04T21:00:55Z",
        "summary": "The distribution of fake news is not a new but a rapidly growing problem. The\nshift to news consumption via social media has been one of the drivers for the\nspread of misleading and deliberately wrong information, as in addition to it\nof easy use there is rarely any veracity monitoring. Due to the harmful effects\nof such fake news on society, the detection of these has become increasingly\nimportant. We present an approach to the problem that combines the power of\ntransformer-based language models while simultaneously addressing one of their\ninherent problems. Our framework, CMTR-BERT, combines multiple text\nrepresentations, with the goal of circumventing sequential limits and related\nloss of information the underlying transformer architecture typically suffers\nfrom. Additionally, it enables the incorporation of contextual information.\nExtensive experiments on two very different, publicly available datasets\ndemonstrates that our approach is able to set new state-of-the-art performance\nbenchmarks. Apart from the benefit of using automatic text summarization\ntechniques we also find that the incorporation of contextual information\ncontributes to performance gains.",
        "pdf_link": "https://arxiv.org/pdf/2204.01841v1.pdf"
    },
    {
        "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
        "authors": [
            "Michael Ahn",
            "Anthony Brohan",
            "Noah Brown",
            "Yevgen Chebotar",
            "Omar Cortes",
            "Byron David",
            "Chelsea Finn",
            "Chuyuan Fu",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Alex Herzog",
            "Daniel Ho",
            "Jasmine Hsu",
            "Julian Ibarz",
            "Brian Ichter",
            "Alex Irpan",
            "Eric Jang",
            "Rosario Jauregui Ruano",
            "Kyle Jeffrey",
            "Sally Jesmonth",
            "Nikhil J Joshi",
            "Ryan Julian",
            "Dmitry Kalashnikov",
            "Yuheng Kuang",
            "Kuang-Huei Lee",
            "Sergey Levine",
            "Yao Lu",
            "Linda Luu",
            "Carolina Parada",
            "Peter Pastor",
            "Jornell Quiambao",
            "Kanishka Rao",
            "Jarek Rettinghouse",
            "Diego Reyes",
            "Pierre Sermanet",
            "Nicolas Sievers",
            "Clayton Tan",
            "Alexander Toshev",
            "Vincent Vanhoucke",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Sichun Xu",
            "Mengyuan Yan",
            "Andy Zeng"
        ],
        "published": "2022-04-04T17:57:11Z",
        "summary": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2204.01691v2.pdf"
    },
    {
        "title": "A Machine With Human-Like Memory Systems",
        "authors": [
            "Taewoon Kim",
            "Michael Cochez",
            "Vincent Francois-Lavet",
            "Mark Neerincx",
            "Piek Vossen"
        ],
        "published": "2022-04-04T16:05:53Z",
        "summary": "Inspired by the cognitive science theory, we explicitly model an agent with\nboth semantic and episodic memory systems, and show that it is better than\nhaving just one of the two memory systems. In order to show this, we have\ndesigned and released our own challenging environment, \"the Room\", compatible\nwith OpenAI Gym, where an agent has to properly learn how to encode, store, and\nretrieve memories to maximize its rewards. The Room environment allows for a\nhybrid intelligence setup where machines and humans can collaborate. We show\nthat two agents collaborating with each other results in better performance\nthan one agent acting alone. We have open-sourced our code and models at\nhttps://github.com/tae898/explicit-memory.",
        "pdf_link": "https://arxiv.org/pdf/2204.01611v1.pdf"
    },
    {
        "title": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
        "authors": [
            "Serra Sinem Tekiroglu",
            "Helena Bonaldi",
            "Margherita Fanton",
            "Marco Guerini"
        ],
        "published": "2022-04-04T12:44:47Z",
        "summary": "In this work, we present an extensive study on the use of pre-trained\nlanguage models for the task of automatic Counter Narrative (CN) generation to\nfight online hate speech in English. We first present a comparative study to\ndetermine whether there is a particular Language Model (or class of LMs) and a\nparticular decoding mechanism that are the most appropriate to generate CNs.\nFindings show that autoregressive models combined with stochastic decodings are\nthe most promising. We then investigate how an LM performs in generating a CN\nwith regard to an unseen target of hate. We find out that a key element for\nsuccessful `out of target' experiments is not an overall similarity with the\ntraining data but the presence of a specific subset of training data, i.e. a\ntarget that shares some commonalities with the test target that can be defined\na-priori. We finally introduce the idea of a pipeline based on the addition of\nan automatic post-editing step to refine generated CNs.",
        "pdf_link": "https://arxiv.org/pdf/2204.01440v1.pdf"
    },
    {
        "title": "Aligned Weight Regularizers for Pruning Pretrained Neural Networks",
        "authors": [
            "James O' Neill",
            "Sourav Dutta",
            "Haytham Assem"
        ],
        "published": "2022-04-04T11:06:42Z",
        "summary": "While various avenues of research have been explored for iterative pruning,\nlittle is known what effect pruning has on zero-shot test performance and its\npotential implications on the choice of pruning criteria. This pruning setup is\nparticularly important for cross-lingual models that implicitly learn alignment\nbetween language representations during pretraining, which if distorted via\npruning, not only leads to poorer performance on language data used for\nretraining but also on zero-shot languages that are evaluated.\n  In this work, we show that there is a clear performance discrepancy in\nmagnitude-based pruning when comparing standard supervised learning to the\nzero-shot setting. From this finding, we propose two weight regularizers that\naim to maximize the alignment between units of pruned and unpruned networks to\nmitigate alignment distortion in pruned cross-lingual models and perform well\nfor both non zero-shot and zero-shot settings.\n  We provide experimental results on cross-lingual tasks for the zero-shot\nsetting using XLM-RoBERTa$_{\\mathrm{Base}}$, where we also find that pruning\nhas varying degrees of representational degradation depending on the language\ncorresponding to the zero-shot test set. This is also the first study that\nfocuses on cross-lingual language model compression.",
        "pdf_link": "https://arxiv.org/pdf/2204.01385v2.pdf"
    },
    {
        "title": "Interpretable Saliency Maps And Self-Supervised Learning For Generalized Zero Shot Medical Image Classification",
        "authors": [
            "Dwarikanath Mahapatra"
        ],
        "published": "2022-04-04T09:30:08Z",
        "summary": "In many real world medical image classification settings we do not have\naccess to samples of all possible disease classes, while a robust system is\nexpected to give high performance in recognizing novel test data. We propose a\ngeneralized zero shot learning (GZSL) method that uses self supervised learning\n(SSL) for: 1) selecting anchor vectors of different disease classes; and 2)\ntraining a feature generator. Our approach does not require class attribute\nvectors which are available for natural images but not for medical images. SSL\nensures that the anchor vectors are representative of each class. SSL is also\nused to generate synthetic features of unseen classes. Using a simpler\narchitecture, our method matches a state of the art SSL based GZSL method for\nnatural images and outperforms all methods for medical images. Our method is\nadaptable enough to accommodate class attribute vectors when they are available\nfor natural images.",
        "pdf_link": "https://arxiv.org/pdf/2204.01728v2.pdf"
    },
    {
        "title": "Into-TTS : Intonation Template Based Prosody Control System",
        "authors": [
            "Jihwan Lee",
            "Joun Yeop Lee",
            "Heejin Choi",
            "Seongkyu Mun",
            "Sangjun Park",
            "Jae-Sung Bae",
            "Chanwoo Kim"
        ],
        "published": "2022-04-04T06:37:19Z",
        "summary": "Intonations play an important role in delivering the intention of a speaker.\nHowever, current end-to-end TTS systems often fail to model proper intonations.\nTo alleviate this problem, we propose a novel, intuitive method to synthesize\nspeech in different intonations using predefined intonation templates. Prior to\nTTS model training, speech data are grouped into intonation templates in an\nunsupervised manner. Two proposed modules are added to the end-to-end TTS\nframework: an intonation predictor and an intonation encoder. The intonation\npredictor recommends a suitable intonation template to the given text. The\nintonation encoder, attached to the text encoder output, synthesizes speech\nabiding the requested intonation template. Main contributions of our paper are:\n(a) an easy-to-use intonation control system covering a wide range of users;\n(b) better performance in wrapping speech in a requested intonation with\nimproved objective and subjective evaluation; and (c) incorporating a\npre-trained language model for intonation modelling. Audio samples are\navailable at https://srtts.github.io/IntoTTS.",
        "pdf_link": "https://arxiv.org/pdf/2204.01271v2.pdf"
    },
    {
        "title": "Graph Enhanced BERT for Query Understanding",
        "authors": [
            "Juanhui Li",
            "Yao Ma",
            "Wei Zeng",
            "Suqi Cheng",
            "Jiliang Tang",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published": "2022-04-03T16:50:30Z",
        "summary": "Query understanding plays a key role in exploring users' search intents and\nfacilitating users to locate their most desired information. However, it is\ninherently challenging since it needs to capture semantic information from\nshort and ambiguous queries and often requires massive task-specific labeled\ndata. In recent years, pre-trained language models (PLMs) have advanced various\nnatural language processing tasks because they can extract general semantic\ninformation from large-scale corpora. Therefore, there are unprecedented\nopportunities to adopt PLMs for query understanding. However, there is a gap\nbetween the goal of query understanding and existing pre-training strategies --\nthe goal of query understanding is to boost search performance while existing\nstrategies rarely consider this goal. Thus, directly applying them to query\nunderstanding is sub-optimal. On the other hand, search logs contain user\nclicks between queries and urls that provide rich users' search behavioral\ninformation on queries beyond their content. Therefore, in this paper, we aim\nto fill this gap by exploring search logs. In particular, to incorporate search\nlogs into pre-training, we first construct a query graph where nodes are\nqueries and two queries are connected if they lead to clicks on the same urls.\nThen we propose a novel graph-enhanced pre-training framework, GE-BERT, which\ncan leverage both query content and the query graph. In other words, GE-BERT\ncan capture both the semantic information and the users' search behavioral\ninformation of queries. Extensive experiments on various query understanding\ntasks have demonstrated the effectiveness of the proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2204.06522v2.pdf"
    },
    {
        "title": "POS-BERT: Point Cloud One-Stage BERT Pre-Training",
        "authors": [
            "Kexue Fu",
            "Peng Gao",
            "ShaoLei Liu",
            "Renrui Zhang",
            "Yu Qiao",
            "Manning Wang"
        ],
        "published": "2022-04-03T04:49:39Z",
        "summary": "Recently, the pre-training paradigm combining Transformer and masked language\nmodeling has achieved tremendous success in NLP, images, and point clouds, such\nas BERT. However, directly extending BERT from NLP to point clouds requires\ntraining a fixed discrete Variational AutoEncoder (dVAE) before pre-training,\nwhich results in a complex two-stage method called Point-BERT. Inspired by BERT\nand MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point\nclouds. Specifically, we use the mask patch modeling (MPM) task to perform\npoint cloud pre-training, which aims to recover masked patches information\nunder the supervision of the corresponding tokenizer output. Unlike Point-BERT,\nits tokenizer is extra-trained and frozen. We propose to use the dynamically\nupdated momentum encoder as the tokenizer, which is updated and outputs the\ndynamic supervision signal along with the training process. Further, in order\nto learn high-level semantic representation, we combine contrastive learning to\nmaximize the class token consistency between different transformation point\nclouds. Extensive experiments have demonstrated that POS-BERT can extract\nhigh-quality pre-training features and promote downstream tasks to improve\nperformance. Using the pre-training model without any fine-tuning to extract\nfeatures and train linear SVM on ModelNet40, POS-BERT achieves the\nstate-of-the-art classification accuracy, which exceeds Point-BERT by 3.5\\%. In\naddition, our approach has significantly improved many downstream tasks, such\nas fine-tuned classification, few-shot classification, part segmentation. The\ncode and trained-models will be available at:\n\\url{https://github.com/fukexue/POS-BERT}.",
        "pdf_link": "https://arxiv.org/pdf/2204.00989v1.pdf"
    },
    {
        "title": "BERT-Assisted Semantic Annotation Correction for Emotion-Related Questions",
        "authors": [
            "Abe Kazemzadeh"
        ],
        "published": "2022-04-02T18:00:49Z",
        "summary": "Annotated data have traditionally been used to provide the input for training\na supervised machine learning (ML) model. However, current pre-trained ML\nmodels for natural language processing (NLP) contain embedded linguistic\ninformation that can be used to inform the annotation process. We use the BERT\nneural language model to feed information back into an annotation task that\ninvolves semantic labelling of dialog behavior in a question-asking game called\nEmotion Twenty Questions (EMO20Q). First we describe the background of BERT,\nthe EMO20Q data, and assisted annotation tasks. Then we describe the methods\nfor fine-tuning BERT for the purpose of checking the annotated labels. To do\nthis, we use the paraphrase task as a way to check that all utterances with the\nsame annotation label are classified as paraphrases of each other. We show this\nmethod to be an effective way to assess and revise annotations of textual user\ndata with complex, utterance-level semantic labels.",
        "pdf_link": "https://arxiv.org/pdf/2204.00916v1.pdf"
    },
    {
        "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
        "authors": [
            "Pei Ke",
            "Hao Zhou",
            "Yankai Lin",
            "Peng Li",
            "Jie Zhou",
            "Xiaoyan Zhu",
            "Minlie Huang"
        ],
        "published": "2022-04-02T13:42:49Z",
        "summary": "Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.",
        "pdf_link": "https://arxiv.org/pdf/2204.00862v2.pdf"
    },
    {
        "title": "Efficient comparison of sentence embeddings",
        "authors": [
            "Spyros Zoupanos",
            "Stratis Kolovos",
            "Athanasios Kanavos",
            "Orestis Papadimitriou",
            "Manolis Maragoudakis"
        ],
        "published": "2022-04-02T09:08:34Z",
        "summary": "The domain of natural language processing (NLP), which has greatly evolved\nover the last years, has highly benefited from the recent developments in word\nand sentence embeddings. Such embeddings enable the transformation of complex\nNLP tasks, like semantic similarity or Question and Answering (Q&A), into much\nsimpler to perform vector comparisons. However, such a problem transformation\nraises new challenges like the efficient comparison of embeddings and their\nmanipulation. In this work, we will discuss about various word and sentence\nembeddings algorithms, we will select a sentence embedding algorithm, BERT, as\nour algorithm of choice and we will evaluate the performance of two vector\ncomparison approaches, FAISS and Elasticsearch, in the specific problem of\nsentence embeddings. According to the results, FAISS outperforms Elasticsearch\nwhen used in a centralized environment with only one node, especially when big\ndatasets are included.",
        "pdf_link": "https://arxiv.org/pdf/2204.00820v2.pdf"
    },
    {
        "title": "CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos",
        "authors": [
            "Shengyao Zhuang",
            "Guido Zuccon"
        ],
        "published": "2022-04-01T23:02:50Z",
        "summary": "Current dense retrievers are not robust to out-of-domain and outlier queries,\ni.e. their effectiveness on these queries is much poorer than what one would\nexpect. In this paper, we consider a specific instance of such queries: queries\nthat contain typos. We show that a small character level perturbation in\nqueries (as caused by typos) highly impacts the effectiveness of dense\nretrievers. We then demonstrate that the root cause of this resides in the\ninput tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\nqueries with typos, while still being as performant as previous methods on\nqueries without typos. For this, we use CharacterBERT as the backbone encoder\nand an efficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers. Code, experimental results and dataset\nare made available at https://github.com/ielab/CharacterBERT-DR.",
        "pdf_link": "https://arxiv.org/pdf/2204.00716v2.pdf"
    },
    {
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
        "authors": [
            "Andy Zeng",
            "Maria Attarian",
            "Brian Ichter",
            "Krzysztof Choromanski",
            "Adrian Wong",
            "Stefan Welker",
            "Federico Tombari",
            "Aveek Purohit",
            "Michael Ryoo",
            "Vikas Sindhwani",
            "Johnny Lee",
            "Vincent Vanhoucke",
            "Pete Florence"
        ],
        "published": "2022-04-01T17:43:13Z",
        "summary": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.",
        "pdf_link": "https://arxiv.org/pdf/2204.00598v2.pdf"
    },
    {
        "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training",
        "authors": [
            "Tri Dao",
            "Beidi Chen",
            "Nimit Sohoni",
            "Arjun Desai",
            "Michael Poli",
            "Jessica Grogan",
            "Alexander Liu",
            "Aniruddh Rao",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "published": "2022-04-01T17:37:29Z",
        "summary": "Large neural networks excel in many domains, but they are expensive to train\nand fine-tune. A popular approach to reduce their compute or memory\nrequirements is to replace dense weight matrices with structured ones (e.g.,\nsparse, low-rank, Fourier transform). These methods have not seen widespread\nadoption (1) in end-to-end training due to unfavorable efficiency--quality\ntradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable\nalgorithms to approximate a given dense weight matrix. To address these issues,\nwe propose a class of matrices (Monarch) that is hardware-efficient (they are\nparameterized as products of two block-diagonal matrices for better hardware\nutilization) and expressive (they can represent many commonly used transforms).\nSurprisingly, the problem of approximating a dense weight matrix with a Monarch\nmatrix, though nonconvex, has an analytical optimal solution. These properties\nof Monarch matrices unlock new ways to train and fine-tune sparse and dense\nmodels. We empirically validate that Monarch can achieve favorable\naccuracy-efficiency tradeoffs in several end-to-end sparse training\napplications: speeding up ViT and GPT-2 training on ImageNet classification and\nWikitext-103 language modeling by 2x with comparable model quality, and\nreducing the error on PDE solving and MRI reconstruction tasks by 40%. In\nsparse-to-dense training, with a simple technique called \"reverse\nsparsification,\" Monarch matrices serve as a useful intermediate representation\nto speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The\nsame technique brings 23% faster BERT pretraining than even the very optimized\nimplementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse\nfine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds\nup BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2204.00595v1.pdf"
    },
    {
        "title": "Evaluation of Fake News Detection with Knowledge-Enhanced Language Models",
        "authors": [
            "Chenxi Whitehouse",
            "Tillman Weyde",
            "Pranava Madhyastha",
            "Nikos Komninos"
        ],
        "published": "2022-04-01T14:14:46Z",
        "summary": "Recent advances in fake news detection have exploited the success of\nlarge-scale pre-trained language models (PLMs). The predominant\nstate-of-the-art approaches are based on fine-tuning PLMs on labelled fake news\ndatasets. However, large-scale PLMs are generally not trained on structured\nfactual data and hence may not possess priors that are grounded in factually\naccurate knowledge. The use of existing knowledge bases (KBs) with rich\nhuman-curated factual information has thus the potential to make fake news\ndetection more effective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detection. We study several\nstate-of-the-art approaches for knowledge integration, mostly using Wikidata as\nKB, on two popular fake news datasets - LIAR, a politics-based dataset, and\nCOVID-19, a dataset of messages posted on social media relating to the COVID-19\npandemic. Our experiments show that knowledge-enhanced models can significantly\nimprove fake news detection on LIAR where the KB is relevant and up-to-date.\nThe mixed results on COVID-19 highlight the reliance on stylistic features and\nthe importance of domain-specific and current KBs.",
        "pdf_link": "https://arxiv.org/pdf/2204.00458v2.pdf"
    },
    {
        "title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition",
        "authors": [
            "Gerasimos Chatzoudis",
            "Manos Plitsis",
            "Spyridoula Stamouli",
            "Athanasia-Lida Dimou",
            "Athanasios Katsamanis",
            "Vassilis Katsouros"
        ],
        "published": "2022-04-01T14:05:02Z",
        "summary": "Aphasia is a common speech and language disorder, typically caused by a brain\ninjury or a stroke, that affects millions of people worldwide. Detecting and\nassessing Aphasia in patients is a difficult, time-consuming process, and\nnumerous attempts to automate it have been made, the most successful using\nmachine learning models trained on aphasic speech data. Like in many medical\napplications, aphasic speech data is scarce and the problem is exacerbated in\nso-called \"low resource\" languages, which are, for this task, most languages\nexcluding English. We attempt to leverage available data in English and achieve\nzero-shot aphasia detection in low-resource languages such as Greek and French,\nby using language-agnostic linguistic features. Current cross-lingual aphasia\ndetection approaches rely on manually extracted transcripts. We propose an\nend-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models\nthat share cross-lingual speech representations and are fine-tuned for our\ndesired low-resource languages. To further boost our ASR model's performance,\nwe also combine it with a language model. We show that our ASR-based end-to-end\npipeline offers comparable results to previous setups using human-annotated\ntranscripts.",
        "pdf_link": "https://arxiv.org/pdf/2204.00448v1.pdf"
    },
    {
        "title": "Cyberbullying detection across social media platforms via platform-aware adversarial encoding",
        "authors": [
            "Peiling Yi",
            "Arkaitz Zubiaga"
        ],
        "published": "2022-04-01T10:25:46Z",
        "summary": "Despite the increasing interest in cyberbullying detection, existing efforts\nhave largely been limited to experiments on a single platform and their\ngeneralisability across different social media platforms have received less\nattention. We propose XP-CB, a novel cross-platform framework based on\nTransformers and adversarial learning. XP-CB can enhance a Transformer\nleveraging unlabelled data from the source and target platforms to come up with\na common representation while preventing platform-specific training. To\nvalidate our proposed framework, we experiment on cyberbullying datasets from\nthree different platforms through six cross-platform configurations, showing\nits effectiveness with both BERT and RoBERTa as the underlying Transformer\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2204.00334v1.pdf"
    },
    {
        "title": "Feature Structure Distillation with Centered Kernel Alignment in BERT Transferring",
        "authors": [
            "Hee-Jun Jung",
            "Doyeon Kim",
            "Seung-Hoon Na",
            "Kangil Kim"
        ],
        "published": "2022-04-01T10:10:27Z",
        "summary": "Knowledge distillation is an approach to transfer information on\nrepresentations from a teacher to a student by reducing their difference. A\nchallenge of this approach is to reduce the flexibility of the student's\nrepresentations inducing inaccurate learning of the teacher's knowledge. To\nresolve it in transferring, we investigate distillation of structures of\nrepresentations specified to three types: intra-feature, local inter-feature,\nglobal inter-feature structures. To transfer them, we introduce feature\nstructure distillation methods based on the Centered Kernel Alignment, which\nassigns a consistent value to similar features structures and reveals more\ninformative relations. In particular, a memory-augmented transfer method with\nclustering is implemented for the global structures. The methods are\nempirically analyzed on the nine tasks for language understanding of the GLUE\ndataset with Bidirectional Encoder Representations from Transformers (BERT),\nwhich is a representative neural language model. In the results, the proposed\nmethods effectively transfer the three types of structures and improve\nperformance compared to state-of-the-art distillation methods. Indeed, the code\nfor the methods is available in https://github.com/maroo-sky/FSD.",
        "pdf_link": "https://arxiv.org/pdf/2204.08922v3.pdf"
    },
    {
        "title": "Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization",
        "authors": [
            "Georgios Katsimpras",
            "Georgios Paliouras"
        ],
        "published": "2022-04-01T08:45:39Z",
        "summary": "Clinical trials offer a fundamental opportunity to discover new treatments\nand advance the medical knowledge. However, the uncertainty of the outcome of a\ntrial can lead to unforeseen costs and setbacks. In this study, we propose a\nnew method to predict the effectiveness of an intervention in a clinical trial.\nOur method relies on generating an informative summary from multiple documents\navailable in the literature about the intervention under study. Specifically,\nour method first gathers all the abstracts of PubMed articles related to the\nintervention. Then, an evidence sentence, which conveys information about the\neffectiveness of the intervention, is extracted automatically from each\nabstract. Based on the set of evidence sentences extracted from the abstracts,\na short summary about the intervention is constructed. Finally, the produced\nsummaries are used to train a BERT-based classifier, in order to infer the\neffectiveness of an intervention. To evaluate our proposed method, we introduce\na new dataset which is a collection of clinical trials together with their\nassociated PubMed articles. Our experiments, demonstrate the effectiveness of\nproducing short informative summaries and using them to predict the\neffectiveness of an intervention.",
        "pdf_link": "https://arxiv.org/pdf/2204.00290v1.pdf"
    },
    {
        "title": "Syntax-informed Question Answering with Heterogeneous Graph Transformer",
        "authors": [
            "Fangyi Zhu",
            "Lok You Tan",
            "See-Kiong Ng",
            "St\u00e9phane Bressan"
        ],
        "published": "2022-04-01T07:48:03Z",
        "summary": "Large neural language models are steadily contributing state-of-the-art\nperformance to question answering and other natural language and information\nprocessing tasks. These models are expensive to train. We propose to evaluate\nwhether such pre-trained models can benefit from the addition of explicit\nlinguistics information without requiring retraining from scratch.\n  We present a linguistics-informed question answering approach that extends\nand fine-tunes a pre-trained transformer-based neural language model with\nsymbolic knowledge encoded with a heterogeneous graph transformer. We\nillustrate the approach by the addition of syntactic information in the form of\ndependency and constituency graphic structures connecting tokens and virtual\nvertices.\n  A comparative empirical performance evaluation with BERT as its baseline and\nwith Stanford Question Answering Dataset demonstrates the competitiveness of\nthe proposed approach. We argue, in conclusion and in the light of further\nresults of preliminary experiments, that the approach is extensible to further\nlinguistics information including semantics and pragmatics.",
        "pdf_link": "https://arxiv.org/pdf/2204.09655v2.pdf"
    },
    {
        "title": "NC-DRE: Leveraging Non-entity Clue Information for Document-level Relation Extraction",
        "authors": [
            "Liang Zhang",
            "Yidong Cheng"
        ],
        "published": "2022-04-01T07:30:26Z",
        "summary": "Document-level relation extraction (RE), which requires reasoning on multiple\nentities in different sentences to identify complex inter-sentence relations,\nis more challenging than sentence-level RE. To extract the complex\ninter-sentence relations, previous studies usually employ graph neural networks\n(GNN) to perform inference upon heterogeneous document-graphs. Despite their\ngreat successes, these graph-based methods, which normally only consider the\nwords within the mentions in the process of building graphs and reasoning, tend\nto ignore the non-entity clue words that are not in the mentions but provide\nimportant clue information for relation reasoning. To alleviate this problem,\nwe treat graph-based document-level RE models as an encoder-decoder framework,\nwhich typically uses a pre-trained language model as the encoder and a GNN\nmodel as the decoder, and propose a novel graph-based model NC-DRE that\nintroduces decoder-to-encoder attention mechanism to leverage Non-entity Clue\ninformation for Document-level Relation Extraction.",
        "pdf_link": "https://arxiv.org/pdf/2204.00255v1.pdf"
    },
    {
        "title": "Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems",
        "authors": [
            "Takuma Udagawa",
            "Masayuki Suzuki",
            "Gakuto Kurata",
            "Nobuyasu Itoh",
            "George Saon"
        ],
        "published": "2022-04-01T05:20:55Z",
        "summary": "Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been\nsuccessfully applied to ASR N-best rescoring. However, whether or how they can\nbenefit competitive, near state-of-the-art ASR systems remains unexplored. In\nthis study, we incorporate LLM rescoring into one of the most competitive ASR\nbaselines: the Conformer-Transducer model. We demonstrate that consistent\nimprovement is achieved by the LLM's bidirectionality, pretraining, in-domain\nfinetuning and context augmentation. Furthermore, our lexical analysis sheds\nlight on how each of these components may be contributing to the ASR\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2204.00212v2.pdf"
    },
    {
        "title": "A Baseline Readability Model for Cebuano",
        "authors": [
            "Lloyd Lois Antonie Reyes",
            "Michael Antonio Iba\u00f1ez",
            "Ranz Sapinit",
            "Mohammed Hussien",
            "Joseph Marvin Imperial"
        ],
        "published": "2022-03-31T17:49:11Z",
        "summary": "In this study, we developed the first baseline readability model for the\nCebuano language. Cebuano is the second most-used native language in the\nPhilippines with about 27.5 million speakers. As the baseline, we extracted\ntraditional or surface-based features, syllable patterns based from Cebuano's\ndocumented orthography, and neural embeddings from the multilingual BERT model.\nResults show that the use of the first two handcrafted linguistic features\nobtained the best performance trained on an optimized Random Forest model with\napproximately 87% across all metrics. The feature sets and algorithm used also\nis similar to previous results in readability assessment for the Filipino\nlanguage showing potential of crosslingual application. To encourage more work\nfor readability assessment in Philippine languages such as Cebuano, we\nopen-sourced both code and data.",
        "pdf_link": "https://arxiv.org/pdf/2203.17225v3.pdf"
    },
    {
        "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech",
        "authors": [
            "Guangyan Zhang",
            "Kaitao Song",
            "Xu Tan",
            "Daxin Tan",
            "Yuzi Yan",
            "Yanqing Liu",
            "Gang Wang",
            "Wei Zhou",
            "Tao Qin",
            "Tan Lee",
            "Sheng Zhao"
        ],
        "published": "2022-03-31T17:12:26Z",
        "summary": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT",
        "pdf_link": "https://arxiv.org/pdf/2203.17190v3.pdf"
    },
    {
        "title": "Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$",
        "authors": [
            "Adam Roberts",
            "Hyung Won Chung",
            "Anselm Levskaya",
            "Gaurav Mishra",
            "James Bradbury",
            "Daniel Andor",
            "Sharan Narang",
            "Brian Lester",
            "Colin Gaffney",
            "Afroz Mohiuddin",
            "Curtis Hawthorne",
            "Aitor Lewkowycz",
            "Alex Salcianu",
            "Marc van Zee",
            "Jacob Austin",
            "Sebastian Goodman",
            "Livio Baldini Soares",
            "Haitang Hu",
            "Sasha Tsvyashchenko",
            "Aakanksha Chowdhery",
            "Jasmijn Bastings",
            "Jannis Bulian",
            "Xavier Garcia",
            "Jianmo Ni",
            "Andrew Chen",
            "Kathleen Kenealy",
            "Jonathan H. Clark",
            "Stephan Lee",
            "Dan Garrette",
            "James Lee-Thorp",
            "Colin Raffel",
            "Noam Shazeer",
            "Marvin Ritter",
            "Maarten Bosma",
            "Alexandre Passos",
            "Jeremy Maitin-Shepard",
            "Noah Fiedel",
            "Mark Omernick",
            "Brennan Saeta",
            "Ryan Sepassi",
            "Alexander Spiridonov",
            "Joshua Newlan",
            "Andrea Gesmundo"
        ],
        "published": "2022-03-31T17:12:13Z",
        "summary": "Recent neural network-based language models have benefited greatly from\nscaling up the size of training datasets and the number of parameters in the\nmodels themselves. Scaling can be complicated due to various factors including\nthe need to distribute computation on supercomputer clusters (e.g., TPUs),\nprevent bottlenecks when infeeding data, and ensure reproducible results. In\nthis work, we present two software libraries that ease these issues:\n$\\texttt{t5x}$ simplifies the process of building and training large language\nmodels at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a\ntask-based API for simple creation of fast and reproducible training data and\nevaluation pipelines. These open-source libraries have been used to train\nmodels with hundreds of billions of parameters on datasets with multiple\nterabytes of training data.\n  Along with the libraries, we release configurations and instructions for\nT5-like encoder-decoder models as well as GPT-like decoder-only architectures.\n  $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at\nhttps://github.com/google-research/t5x and https://github.com/google/seqio,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2203.17189v1.pdf"
    },
    {
        "title": "Scaling Language Model Size in Cross-Device Federated Learning",
        "authors": [
            "Jae Hun Ro",
            "Theresa Breiner",
            "Lara McConnaughey",
            "Mingqing Chen",
            "Ananda Theertha Suresh",
            "Shankar Kumar",
            "Rajiv Mathews"
        ],
        "published": "2022-03-31T15:51:53Z",
        "summary": "Most studies in cross-device federated learning focus on small models, due to\nthe server-client communication and on-device computation bottlenecks. In this\nwork, we leverage various techniques for mitigating these bottlenecks to train\nlarger language models in cross-device federated learning. With systematic\napplications of partial model training, quantization, efficient transfer\nlearning, and communication-efficient optimizers, we are able to train a $21$M\nparameter Transformer and $20.2$M parameter Conformer that achieve the same or\nbetter perplexity as that of a similarly sized LSTM with $\\sim10\\times$ smaller\nclient-to-server communication cost and $11\\%$ lower perplexity than smaller\nLSTMs commonly studied in literature.",
        "pdf_link": "https://arxiv.org/pdf/2204.09715v2.pdf"
    },
    {
        "title": "PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model",
        "authors": [
            "Fei Mi",
            "Yitong Li",
            "Yulong Zeng",
            "Jingyan Zhou",
            "Yasheng Wang",
            "Chuanfei Xu",
            "Lifeng Shang",
            "Xin Jiang",
            "Shiqi Zhao",
            "Qun Liu"
        ],
        "published": "2022-03-31T15:09:12Z",
        "summary": "In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain\ndialogue generation model based on a large pre-trained language model (PLM)\nPANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue\nmodels trained over a massive amount of dialogue data from scratch, we aim to\nbuild a powerful dialogue model with relatively fewer data and computation\ncosts by inheriting valuable language capabilities and knowledge from PLMs. To\nthis end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been\nproven well-performed on a variety of Chinese natural language tasks. We\ninvestigate different aspects of responses generated by PanGu-Bot, including\nresponse quality, knowledge, and safety. We show that PanGu-Bot outperforms\nstate-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA\n(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.\nWe also demonstrate that PanGu-Bot can be easily deployed to generate emotional\nresponses without further training. Throughout our empirical analysis, we also\npoint out that the PanGu-Bot response quality, knowledge correctness, and\nsafety are still far from perfect, and further explorations are indispensable\nto building reliable and smart dialogue systems. Our model and code will be\navailable at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot\nsoon.",
        "pdf_link": "https://arxiv.org/pdf/2203.17090v3.pdf"
    },
    {
        "title": "PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations",
        "authors": [
            "Lodagala V S V Durga Prasad",
            "Sreyan Ghosh",
            "S. Umesh"
        ],
        "published": "2022-03-31T11:34:58Z",
        "summary": "While self-supervised speech representation learning (SSL) models serve a\nvariety of downstream tasks, these models have been observed to overfit to the\ndomain from which the unlabelled data originates. To alleviate this issue, we\npropose PADA (Pruning Assisted Domain Adaptation) and zero out redundant\nweights from models pre-trained on large amounts of out-of-domain (OOD) data.\nIntuitively, this helps to make space for the target-domain ASR finetuning. The\nredundant weights can be identified through various pruning strategies which\nhave been discussed in detail as a part of this work. Specifically, we\ninvestigate the effect of the recently discovered Task-Agnostic and Task-Aware\npruning on PADA and propose a new pruning paradigm based on the latter, which\nwe call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial\npruning mask from a well fine-tuned OOD model, which makes it starkly different\nfrom the rest of the pruning strategies discussed in the paper. Our proposed\nCD-TAW methodology achieves up to 20.6% relative WER improvement over our\nbaseline when fine-tuned on a 2-hour subset of Switchboard data without\nlanguage model (LM) decoding. Furthermore, we conduct a detailed analysis to\nhighlight the key design choices of our proposed method.",
        "pdf_link": "https://arxiv.org/pdf/2203.16965v4.pdf"
    },
    {
        "title": "Generative Pre-Trained Transformers for Biologically Inspired Design",
        "authors": [
            "Qihao Zhu",
            "Xinyu Zhang",
            "Jianxi Luo"
        ],
        "published": "2022-03-31T11:13:22Z",
        "summary": "Biological systems in nature have evolved for millions of years to adapt and\nsurvive the environment. Many features they developed can be inspirational and\nbeneficial for solving technical problems in modern industries. This leads to a\nnovel form of design-by-analogy called bio-inspired design (BID). Although BID\nas a design method has been proven beneficial, the gap between biology and\nengineering continuously hinders designers from effectively applying the\nmethod. Therefore, we explore the recent advance of artificial intelligence\n(AI) for a computational approach to bridge the gap. This paper proposes a\ngenerative design approach based on the pre-trained language model (PLM) to\nautomatically retrieve and map biological analogy and generate BID in the form\nof natural language. The latest generative pre-trained transformer, namely\nGPT-3, is used as the base PLM. Three types of design concept generators are\nidentified and fine-tuned from the PLM according to the looseness of the\nproblem space representation. Machine evaluators are also fine-tuned to assess\nthe correlation between the domains within the generated BID concepts. The\napproach is then tested via a case study in which the fine-tuned models are\napplied to generate and evaluate light-weighted flying car concepts inspired by\nnature. The results show our approach can generate BID concepts with good\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2204.09714v1.pdf"
    },
    {
        "title": "Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?",
        "authors": [
            "Marina Sedinkina",
            "Martin Schmitt",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2022-03-31T09:59:08Z",
        "summary": "The practical success of much of NLP depends on the availability of training\ndata. However, in real-world scenarios, training data is often scarce, not\nleast because many application domains are restricted and specific. In this\nwork, we compare different methods to handle this problem and provide\nguidelines for building NLP applications when there is only a small amount of\nlabeled training data available for a specific domain. While transfer learning\nwith pre-trained language models outperforms other methods across tasks,\nalternatives do not perform much worse while requiring much less computational\neffort, thus significantly reducing monetary and environmental cost. We examine\nthe performance tradeoffs of several such alternatives, including models that\ncan be trained up to 175K times faster and do not require a single GPU.",
        "pdf_link": "https://arxiv.org/pdf/2203.16926v1.pdf"
    },
    {
        "title": "A Character-level Span-based Model for Mandarin Prosodic Structure Prediction",
        "authors": [
            "Xueyuan Chen",
            "Changhe Song",
            "Yixuan Zhou",
            "Zhiyong Wu",
            "Changbin Chen",
            "Zhongqin Wu",
            "Helen Meng"
        ],
        "published": "2022-03-31T09:47:08Z",
        "summary": "The accuracy of prosodic structure prediction is crucial to the naturalness\nof synthesized speech in Mandarin text-to-speech system, but now is limited by\nwidely-used sequence-to-sequence framework and error accumulation from previous\nword segmentation results. In this paper, we propose a span-based Mandarin\nprosodic structure prediction model to obtain an optimal prosodic structure\ntree, which can be converted to corresponding prosodic label sequence. Instead\nof the prerequisite for word segmentation, rich linguistic features are\nprovided by Chinese character-level BERT and sent to encoder with\nself-attention architecture. On top of this, span representation and label\nscoring are used to describe all possible prosodic structure trees, of which\neach tree has its corresponding score. To find the optimal tree with the\nhighest score for a given sentence, a bottom-up CKY-style algorithm is further\nused. The proposed method can predict prosodic labels of different levels at\nthe same time and accomplish the process directly from Chinese characters in an\nend-to-end manner. Experiment results on two real-world datasets demonstrate\nthe excellent performance of our span-based method over all\nsequence-to-sequence baseline approaches.",
        "pdf_link": "https://arxiv.org/pdf/2203.16922v1.pdf"
    },
    {
        "title": "Leveraging pre-trained language models for conversational information seeking from text",
        "authors": [
            "Patrizio Bellan",
            "Mauro Dragoni",
            "Chiara Ghidini"
        ],
        "published": "2022-03-31T09:00:46Z",
        "summary": "Recent advances in Natural Language Processing, and in particular on the\nconstruction of very large pre-trained language representation models, is\nopening up new perspectives on the construction of conversational information\nseeking (CIS) systems. In this paper we investigate the usage of in-context\nlearning and pre-trained language representation models to address the problem\nof information extraction from process description documents, in an incremental\nquestion and answering oriented fashion. In particular we investigate the usage\nof the native GPT-3 (Generative Pre-trained Transformer 3) model, together with\ntwo in-context learning customizations that inject conceptual definitions and a\nlimited number of samples in a few shot-learning fashion. The results highlight\nthe potential of the approach and the usefulness of the in-context learning\ncustomizations, which can substantially contribute to address the \"training\ndata challenge\" of deep learning based NLP techniques the BPM field. It also\nhighlight the challenge posed by control flow relations for which further\ntraining needs to be devised.",
        "pdf_link": "https://arxiv.org/pdf/2204.03542v1.pdf"
    },
    {
        "title": "ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices",
        "authors": [
            "Srishti Mehra",
            "Robert Louka",
            "Yixun Zhang"
        ],
        "published": "2022-03-31T04:22:44Z",
        "summary": "Environmental, Social, and Governance (ESG) are non-financial factors that\nare garnering attention from investors as they increasingly look to apply these\nas part of their analysis to identify material risks and growth opportunities.\nSome of this attention is also driven by clients who, now more aware than ever,\nare demanding for their money to be managed and invested responsibly. As the\ninterest in ESG grows, so does the need for investors to have access to\nconsumable ESG information. Since most of it is in text form in reports,\ndisclosures, press releases, and 10-Q filings, we see a need for sophisticated\nNLP techniques for classification tasks for ESG text. We hypothesize that an\nESG domain-specific pre-trained model will help with such and study building of\nthe same in this paper. We explored doing this by fine-tuning BERTs pre-trained\nweights using ESG specific text and then further fine-tuning the model for a\nclassification task. We were able to achieve accuracy better than the original\nBERT and baseline models in environment-specific classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.16788v1.pdf"
    },
    {
        "title": "An Empirical Study of Language Model Integration for Transducer based Speech Recognition",
        "authors": [
            "Huahuan Zheng",
            "Keyu An",
            "Zhijian Ou",
            "Chen Huang",
            "Ke Ding",
            "Guanglu Wan"
        ],
        "published": "2022-03-31T03:33:50Z",
        "summary": "Utilizing text-only data with an external language model (ELM) in end-to-end\nRNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class\nof methods such as density ratio (DR) and internal language model estimation\n(ILME) have been developed, outperforming the classic shallow fusion (SF)\nmethod. The basic idea behind these methods is that RNN-T posterior should\nfirst subtract the implicitly learned internal language model (ILM) prior, in\norder to integrate the ELM. While recent studies suggest that RNN-T only learns\nsome low-order language model information, the DR method uses a well-trained\nneural language model with full context, which may be inappropriate for the\nestimation of ILM and deteriorate the integration performance. Based on the DR\nmethod, we propose a low-order density ratio method (LODR) by replacing the\nestimation with a low-order weak language model. Extensive empirical\nexperiments are conducted on both in-domain and cross-domain scenarios on\nEnglish LibriSpeech & Tedlium-2 and Chinese WenetSpeech & AISHELL-1 datasets.\nIt is shown that LODR consistently outperforms SF in all tasks, while\nperforming generally close to ILME and better than DR in most tests.",
        "pdf_link": "https://arxiv.org/pdf/2203.16776v4.pdf"
    },
    {
        "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
        "authors": [
            "Kai-Wei Chang",
            "Wei-Cheng Tseng",
            "Shang-Wen Li",
            "Hung-yi Lee"
        ],
        "published": "2022-03-31T03:26:55Z",
        "summary": "Speech representations learned from Self-supervised learning (SSL) models can\nbenefit various speech processing tasks. However, utilizing SSL representations\nusually requires fine-tuning the pre-trained models or designing task-specific\ndownstream models and loss functions, causing much memory usage and human\nlabor. Recently, prompting in Natural Language Processing (NLP) has been found\nto be an efficient technique to leverage pre-trained language models (LMs).\nSpecifically, prompt tuning optimizes a limited number of task-specific\nparameters with a fixed pre-trained model; as a result, only a small set of\nparameters is needed to be stored for each task. Prompt tuning improves\ncomputation and memory efficiency by leveraging the pre-trained LM's prediction\nability. Nevertheless, such a paradigm is little studied in the speech\ncommunity. We report in this paper the first exploration of the prompt tuning\nparadigm for speech processing tasks based on Generative Spoken Language Model\n(GSLM). Experiment results show that the prompt tuning technique achieves\ncompetitive performance in speech classification tasks with fewer trainable\nparameters than fine-tuning specialized downstream models. We further study the\ntechnique in challenging sequence generation tasks. Prompt tuning also\ndemonstrates its potential, while the limitation and possible research\ndirections are discussed in this paper. The source code is available on\nhttps://github.com/ga642381/SpeechPrompt.",
        "pdf_link": "https://arxiv.org/pdf/2203.16773v3.pdf"
    },
    {
        "title": "CREATE: A Benchmark for Chinese Short Video Retrieval and Title Generation",
        "authors": [
            "Ziqi Zhang",
            "Yuxin Chen",
            "Zongyang Ma",
            "Zhongang Qi",
            "Chunfeng Yuan",
            "Bing Li",
            "Ying Shan",
            "Weiming Hu"
        ],
        "published": "2022-03-31T02:39:18Z",
        "summary": "Previous works of video captioning aim to objectively describe the video's\nactual content, which lacks subjective and attractive expression, limiting its\npractical application scenarios. Video titling is intended to achieve this\ngoal, but there is a lack of a proper benchmark. In this paper, we propose to\nCREATE, the first large-scale Chinese shoRt vidEo retrievAl and Title\ngEneration benchmark, to facilitate research and application in video titling\nand video retrieval in Chinese. CREATE consists of a high-quality labeled 210K\ndataset and two large-scale 3M/10M pre-training datasets, covering 51\ncategories, 50K+ tags, 537K manually annotated titles and captions, and 10M+\nshort videos. Based on CREATE, we propose a novel model ALWIG which combines\nvideo retrieval and video titling tasks to achieve the purpose of multi-modal\nALignment WIth Generation with the help of video tags and a GPT pre-trained\nmodel. CREATE opens new directions for facilitating future research and\napplications on video titling and video retrieval in the field of Chinese short\nvideos.",
        "pdf_link": "https://arxiv.org/pdf/2203.16763v1.pdf"
    },
    {
        "title": "Reproducibility Issues for BERT-based Evaluation Metrics",
        "authors": [
            "Yanran Chen",
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022-03-30T20:35:37Z",
        "summary": "Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).",
        "pdf_link": "https://arxiv.org/pdf/2204.00004v3.pdf"
    },
    {
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
        "authors": [
            "Adi Haviv",
            "Ori Ram",
            "Ofir Press",
            "Peter Izsak",
            "Omer Levy"
        ],
        "published": "2022-03-30T19:37:07Z",
        "summary": "Causal transformer language models (LMs), such as GPT-3, typically require\nsome form of positional encoding, such as positional embeddings. However, we\nshow that LMs without any explicit positional encoding are still competitive\nwith standard models, and that this phenomenon is robust across different\ndatasets, model sizes, and sequence lengths. Probing experiments reveal that\nsuch models acquire an implicit notion of absolute positions throughout the\nnetwork, effectively compensating for the missing information. We conjecture\nthat causal attention enables the model to infer the number of predecessors\nthat each token can attend to, thereby approximating its absolute position. Our\nfindings indicate that causal LMs might derive positional awareness not only\nfrom the explicit positioning mechanism, but also from the effects of the\ncausal mask.",
        "pdf_link": "https://arxiv.org/pdf/2203.16634v2.pdf"
    },
    {
        "title": "Improving Speech Recognition for Indic Languages using Language Model",
        "authors": [
            "Ankur Dhuriya",
            "Harveen Singh Chadha",
            "Anirudh Gupta",
            "Priyanshi Shah",
            "Neeraj Chhimwal",
            "Rishabh Gaur",
            "Vivek Raghavan"
        ],
        "published": "2022-03-30T18:22:12Z",
        "summary": "We study the effect of applying a language model (LM) on the output of\nAutomatic Speech Recognition (ASR) systems for Indic languages. We fine-tune\nwav2vec $2.0$ models for $18$ Indic languages and adjust the results with\nlanguage models trained on text derived from a variety of sources. Our findings\ndemonstrate that the average Character Error Rate (CER) decreases by over $28$\n\\% and the average Word Error Rate (WER) decreases by about $36$ \\% after\ndecoding with LM. We show that a large LM may not provide a substantial\nimprovement as compared to a diverse one. We also demonstrate that high quality\ntranscriptions can be obtained on domain-specific data without retraining the\nASR model and show results on biomedical domain.",
        "pdf_link": "https://arxiv.org/pdf/2203.16595v3.pdf"
    },
    {
        "title": "PromptDet: Towards Open-vocabulary Detection using Uncurated Images",
        "authors": [
            "Chengjian Feng",
            "Yujie Zhong",
            "Zequn Jie",
            "Xiangxiang Chu",
            "Haibing Ren",
            "Xiaolin Wei",
            "Weidi Xie",
            "Lin Ma"
        ],
        "published": "2022-03-30T17:50:21Z",
        "summary": "The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector, where\nthe class-agnostic object proposals are classified with a text encoder from\npre-trained visual-language model; (ii) To pair the visual latent space (of RPN\nbox proposals) with that of the pre-trained text encoder, we propose the idea\nof regional prompt learning to align the textual embedding space with regional\nvisual object features; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online resource\nvia a novel self-training framework, which allows to train the proposed\ndetector on a large corpus of noisy uncurated web images. Lastly, (iv) to\nevaluate our proposed detector, termed as PromptDet, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset. PromptDet shows\nsuperior performance over existing approaches with fewer additional training\nimages and zero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.",
        "pdf_link": "https://arxiv.org/pdf/2203.16513v2.pdf"
    },
    {
        "title": "Position-based Prompting for Health Outcome Generation",
        "authors": [
            "M. Abaho",
            "D. Bollegala",
            "P. Williamson",
            "S. Dodd"
        ],
        "published": "2022-03-30T16:44:04Z",
        "summary": "Probing Pre-trained Language Models (PLMs) using prompts has indirectly\nimplied that language models (LMs) can be treated as knowledge bases. To this\nend, this phenomena has been effective especially when these LMs are fine-tuned\ntowards not just data of a specific domain, but also to the style or linguistic\npattern of the prompts themselves. We observe that, satisfying a particular\nlinguistic pattern in prompts is an unsustainable constraint that unnecessarily\nlengthens the probing task, especially because, they are often manually\ndesigned and the range of possible prompt template patterns can vary depending\non the prompting objective and domain. We therefore explore an idea of using a\nposition-attention mechanism to capture positional information of each word in\na prompt relative to the mask to be filled, hence avoiding the need to\nre-construct prompts when the prompts linguistic pattern changes. Using our\napproach, we demonstrate the ability of eliciting answers to rare prompt\ntemplates (in a case study on health outcome generation) such as Postfix and\nMixed patterns whose missing information is respectively at the start and in\nmultiple random places of the prompt. More so, using various biomedical PLMs,\nour approach consistently outperforms a baseline in which the default mask\nlanguage model (MLM) representation is used to predict masked tokens.",
        "pdf_link": "https://arxiv.org/pdf/2204.03489v1.pdf"
    },
    {
        "title": "Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis",
        "authors": [
            "Kai Zhang",
            "Kun Zhang",
            "Mengdi Zhang",
            "Hongke Zhao",
            "Qi Liu",
            "Wei Wu",
            "Enhong Chen"
        ],
        "published": "2022-03-30T14:48:46Z",
        "summary": "Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a\nspecific aspect in the given sentence. While pre-trained language models such\nas BERT have achieved great success, incorporating dynamic semantic changes\ninto ABSA remains challenging. To this end, in this paper, we propose to\naddress this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method\ndesigned to learn dynamic aspect-oriented semantics for ABSA. Specifically, we\nfirst take the Stack-BERT layers as a primary encoder to grasp the overall\nsemantic of the sentence and then fine-tune it by incorporating a lightweight\nDynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention\nto a small region of the sentences at each step and re-weigh the vitally\nimportant words for better aspect-aware sentiment understanding. Finally,\nexperimental results on three benchmark datasets demonstrate the effectiveness\nand the rationality of our proposed model and provide good interpretable\ninsights for future semantic modeling.",
        "pdf_link": "https://arxiv.org/pdf/2203.16369v2.pdf"
    },
    {
        "title": "Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval",
        "authors": [
            "Wenshen Xu",
            "Mieradilijiang Maimaiti",
            "Yuanhang Zheng",
            "Xin Tang",
            "Ji Zhang"
        ],
        "published": "2022-03-30T10:13:57Z",
        "summary": "Contrastive learning (CL) has become a ubiquitous approach for several\nnatural language processing (NLP) downstream tasks, especially for question\nanswering (QA). However, the major challenge, how to efficiently train the\nknowledge retrieval model in an unsupervised manner, is still unresolved.\nRecently the commonly used methods are composed of CL and masked language model\n(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also\nneglects extraction of the internal info from the query. To optimize the CL\nhardly obtain internal information from the original query, we introduce a\njoint training method by combining CL and Auto-MLM for self-supervised\nmulti-lingual knowledge retrieval. First, we acquire the fixed dimensional\nsentence vector. Then, mask some words among the original sentences with random\nstrategy. Finally, we generate a new token representation for predicting the\nmasked tokens. Experimental results show that our proposed approach\nconsistently outperforms all the previous SOTA methods on both AliExpress $\\&$\nLAZADA service corpus and openly available corpora in 8 languages.",
        "pdf_link": "https://arxiv.org/pdf/2203.16187v1.pdf"
    },
    {
        "title": "Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling",
        "authors": [
            "Elena \u00c1lvarez-Mellado",
            "Constantine Lignos"
        ],
        "published": "2022-03-30T09:46:51Z",
        "summary": "This work presents a new resource for borrowing identification and analyzes\nthe performance and errors of several models on this task. We introduce a new\nannotated corpus of Spanish newswire rich in unassimilated lexical borrowings\n-- words from one language that are introduced into another without\northographic adaptation -- and use it to evaluate how several sequence labeling\nmodels (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus\ncontains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and\ntopic-varied than previous corpora available for this task. Our results show\nthat a BiLSTM-CRF model fed with subword embeddings along with either\nTransformer-based embeddings pretrained on codeswitched data or a combination\nof contextualized word embeddings outperforms results obtained by a\nmultilingual BERT-based model.",
        "pdf_link": "https://arxiv.org/pdf/2203.16169v1.pdf"
    },
    {
        "title": "An Iterative Co-Training Transductive Framework for Zero Shot Learning",
        "authors": [
            "Bo Liu",
            "Lihua Hu",
            "Qiulei Dong",
            "Zhanyi Hu"
        ],
        "published": "2022-03-30T04:08:44Z",
        "summary": "In zero-shot learning (ZSL) community, it is generally recognized that\ntransductive learning performs better than inductive one as the unseen-class\nsamples are also used in its training stage. How to generate pseudo labels for\nunseen-class samples and how to use such usually noisy pseudo labels are two\ncritical issues in transductive learning. In this work, we introduce an\niterative co-training framework which contains two different base ZSL models\nand an exchanging module. At each iteration, the two different ZSL models are\nco-trained to separately predict pseudo labels for the unseen-class samples,\nand the exchanging module exchanges the predicted pseudo labels, then the\nexchanged pseudo-labeled samples are added into the training sets for the next\niteration. By such, our framework can gradually boost the ZSL performance by\nfully exploiting the potential complementarity of the two models'\nclassification capabilities. In addition, our co-training framework is also\napplied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector\nis proposed to pick out the most likely unseen-class samples before class-level\nclassification to alleviate the bias problem in GZSL. Extensive experiments on\nthree benchmarks show that our proposed methods could significantly outperform\nabout $31$ state-of-the-art ones.",
        "pdf_link": "https://arxiv.org/pdf/2203.16041v1.pdf"
    },
    {
        "title": "Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization",
        "authors": [
            "Evelina Bakhturina",
            "Yang Zhang",
            "Boris Ginsburg"
        ],
        "published": "2022-03-29T21:34:35Z",
        "summary": "Text normalization (TN) systems in production are largely rule-based using\nweighted finite-state transducers (WFST). However, WFST-based systems struggle\nwith ambiguous input when the normalized form is context-dependent. On the\nother hand, neural text normalization systems can take context into account but\nthey suffer from unrecoverable errors and require labeled normalization\ndatasets, which are hard to collect. We propose a new hybrid approach that\ncombines the benefits of rule-based and neural systems. First, a\nnon-deterministic WFST outputs all normalization candidates, and then a neural\nlanguage model picks the best one -- similar to shallow fusion for automatic\nspeech recognition. While the WFST prevents unrecoverable errors, the language\nmodel resolves contextual ambiguity. The approach is easy to extend and we show\nit is effective. It achieves comparable or better results than existing\nstate-of-the-art TN models.",
        "pdf_link": "https://arxiv.org/pdf/2203.15917v1.pdf"
    },
    {
        "title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
        "authors": [
            "Heting Gao",
            "Junrui Ni",
            "Kaizhi Qian",
            "Yang Zhang",
            "Shiyu Chang",
            "Mark Hasegawa-Johnson"
        ],
        "published": "2022-03-29T19:08:55Z",
        "summary": "Large-scale auto-regressive language models pretrained on massive text have\ndemonstrated their impressive ability to perform new natural language tasks\nwith only a few text examples, without the need for fine-tuning. Recent studies\nfurther show that such a few-shot learning ability can be extended to the\ntext-image setting by training an encoder to encode the images into embeddings\nfunctioning like the text embeddings of the language model. Interested in\nexploring the possibility of transferring the few-shot learning ability to the\naudio-text setting, we propose a novel speech understanding framework,\nWavPrompt, where we finetune a wav2vec model to generate a sequence of audio\nembeddings understood by the language model. We show that WavPrompt is a\nfew-shot learner that can perform speech understanding tasks better than a\nnaive text baseline. We conduct detailed ablation studies on different\ncomponents and hyperparameters to empirically identify the best model\nconfiguration. In addition, we conduct a non-speech understanding experiment to\nshow WavPrompt can extract more information than just the transcriptions. Code\nis available at https://github.com/Hertin/WavPrompt",
        "pdf_link": "https://arxiv.org/pdf/2203.15863v2.pdf"
    },
    {
        "title": "LinkBERT: Pretraining Language Models with Document Links",
        "authors": [
            "Michihiro Yasunaga",
            "Jure Leskovec",
            "Percy Liang"
        ],
        "published": "2022-03-29T18:01:24Z",
        "summary": "Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.",
        "pdf_link": "https://arxiv.org/pdf/2203.15827v1.pdf"
    },
    {
        "title": "Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting",
        "authors": [
            "Gabriel Orlanski"
        ],
        "published": "2022-03-29T17:04:17Z",
        "summary": "Large language models have shown that impressive zero-shot performance can be\nachieved through natural language prompts (Radford et al., 2019; Brown et al.,\n2020; Sanh et al., 2021). Creating an effective prompt, however, requires\nsignificant trial and error. That \\textit{prompts} the question: how do the\nqualities of a prompt effects its performance? To this end, we collect and\nstandardize prompts from a diverse range of tasks for use with tasks they were\nnot designed for. We then evaluate these prompts across fixed multiple choice\ndatasets for a quantitative analysis of how certain attributes of a prompt\naffect performance. We find that including the choices and using prompts not\nused during pre-training provide significant improvements. All experiments and\ncode can be found https://github.com/gabeorlanski/zero-shot-cross-task.",
        "pdf_link": "https://arxiv.org/pdf/2203.15754v1.pdf"
    },
    {
        "title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT",
        "authors": [
            "Rui Wang",
            "Qibing Bai",
            "Junyi Ao",
            "Long Zhou",
            "Zhixiang Xiong",
            "Zhihua Wei",
            "Yu Zhang",
            "Tom Ko",
            "Haizhou Li"
        ],
        "published": "2022-03-29T14:20:55Z",
        "summary": "Self-supervised speech representation learning has shown promising results in\nvarious speech processing tasks. However, the pre-trained models, e.g., HuBERT,\nare storage-intensive Transformers, limiting their scope of applications under\nlow-resource settings. To this end, we propose LightHuBERT, a once-for-all\nTransformer compression framework, to find the desired architectures\nautomatically by pruning structured parameters. More precisely, we create a\nTransformer-based supernet that is nested with thousands of weight-sharing\nsubnets and design a two-stage distillation strategy to leverage the\ncontextualized latent representations from HuBERT. Experiments on automatic\nspeech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT\nenables over $10^9$ architectures concerning the embedding dimension, attention\ndimension, head number, feed-forward network ratio, and network depth.\nLightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with\nthe HuBERT size, achieves comparable performance to the teacher model in most\ntasks with a reduction of 29% parameters, and obtains a $3.5\\times$ compression\nratio in three SUPERB tasks, e.g., automatic speaker verification, keyword\nspotting, and intent classification, with a slight accuracy loss. The code and\npre-trained models are available at\nhttps://github.com/mechanicalsea/lighthubert.",
        "pdf_link": "https://arxiv.org/pdf/2203.15610v2.pdf"
    },
    {
        "title": "Cross-Media Scientific Research Achievements Retrieval Based on Deep Language Model",
        "authors": [
            "Benzhi Wang",
            "Meiyu Liang",
            "Feifei Kou",
            "Mingying Xu"
        ],
        "published": "2022-03-29T14:04:53Z",
        "summary": "Science and technology big data contain a lot of cross-media\ninformation.There are images and texts in the scientific paper.The s ingle\nmodal search method cannot well meet the needs of scientific researchers.This\npaper proposes a cross-media scientific research achievements retrieval method\nbased on deep language model (CARDL).It achieves a unified cross-media semantic\nrepresentation by learning the semantic association between different modal\ndata, and is applied to the generation of text semantic vector of scientific\nresearch achievements, and then cross-media retrieval is realized through\nsemantic similarity matching between different modal data.Experimental results\nshow that the proposed CARDL method achieves better cross-modal retrieval\nperformance than existing methods. Key words science and technology big data ;\ncross-media retrieval; cross-media semantic association learning; deep language\nmodel; semantic similarity",
        "pdf_link": "https://arxiv.org/pdf/2203.15595v1.pdf"
    },
    {
        "title": "Training Compute-Optimal Large Language Models",
        "authors": [
            "Jordan Hoffmann",
            "Sebastian Borgeaud",
            "Arthur Mensch",
            "Elena Buchatskaya",
            "Trevor Cai",
            "Eliza Rutherford",
            "Diego de Las Casas",
            "Lisa Anne Hendricks",
            "Johannes Welbl",
            "Aidan Clark",
            "Tom Hennigan",
            "Eric Noland",
            "Katie Millican",
            "George van den Driessche",
            "Bogdan Damoc",
            "Aurelia Guy",
            "Simon Osindero",
            "Karen Simonyan",
            "Erich Elsen",
            "Jack W. Rae",
            "Oriol Vinyals",
            "Laurent Sifre"
        ],
        "published": "2022-03-29T13:38:03Z",
        "summary": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "pdf_link": "https://arxiv.org/pdf/2203.15556v1.pdf"
    },
    {
        "title": "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit",
        "authors": [
            "Binbin Zhang",
            "Di Wu",
            "Zhendong Peng",
            "Xingchen Song",
            "Zhuoyuan Yao",
            "Hang Lv",
            "Lei Xie",
            "Chao Yang",
            "Fuping Pan",
            "Jianwei Niu"
        ],
        "published": "2022-03-29T11:54:34Z",
        "summary": "Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10\\% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.",
        "pdf_link": "https://arxiv.org/pdf/2203.15455v2.pdf"
    },
    {
        "title": "mc-BEiT: Multi-choice Discretization for Image BERT Pre-training",
        "authors": [
            "Xiaotong Li",
            "Yixiao Ge",
            "Kun Yi",
            "Zixuan Hu",
            "Ying Shan",
            "Ling-Yu Duan"
        ],
        "published": "2022-03-29T09:08:18Z",
        "summary": "Image BERT pre-training with masked image modeling (MIM) becomes a popular\npractice to cope with self-supervised representation learning. A seminal work,\nBEiT, casts MIM as a classification task with a visual vocabulary, tokenizing\nthe continuous visual signals into discrete vision tokens using a pre-learned\ndVAE. Despite a feasible solution, the improper discretization hinders further\nimprovements of image pre-training. Since image discretization has no\nground-truth answers, we believe that the masked patch should not be assigned\nwith a unique token id even if a better tokenizer can be obtained. In this\nwork, we introduce an improved BERT-style image pre-training method, namely\nmc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice\ntraining objectives. Specifically, the multi-choice supervision for the masked\nimage patches is formed by the soft probability vectors of the discrete token\nids, which are predicted by the off-the-shelf image tokenizer and further\nrefined by high-level inter-patch perceptions resorting to the observation that\nsimilar patches should share their choices. Extensive experiments on\nclassification, segmentation, and detection tasks demonstrate the superiority\nof our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning\naccuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object\ndetection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic\nsegmentation, outperforming the competitive counterparts. The code will be\navailable at https://github.com/lixiaotong97/mc-BEiT.",
        "pdf_link": "https://arxiv.org/pdf/2203.15371v4.pdf"
    },
    {
        "title": "Improving Persian Relation Extraction Models by Data Augmentation",
        "authors": [
            "Moein Salimi Sartakhti",
            "Romina Etezadi",
            "Mehrnoush Shamsfard"
        ],
        "published": "2022-03-29T08:08:47Z",
        "summary": "Relation extraction that is the task of predicting semantic relation type\nbetween entities in a sentence or document is an important task in natural\nlanguage processing. Although there are many researches and datasets for\nEnglish, Persian suffers from sufficient researches and comprehensive datasets.\nThe only available Persian dataset for this task is PERLEX, which is a Persian\nexpert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we\npresent our augmented dataset and the results and findings of our system,\nparticipated in the Persian relation Extraction shared task of NSURL 2021\nworkshop. We use PERLEX as the base dataset and enhance it by applying some\ntext preprocessing steps and by increasing its size via data augmentation\ntechniques to improve the generalization and robustness of applied models. We\nthen employ two different models including ParsBERT and multilingual BERT for\nrelation extraction on the augmented PERLEX dataset. Our best model obtained\n64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of\nMacro-F1 on the test set of PERLEX.",
        "pdf_link": "https://arxiv.org/pdf/2203.15323v1.pdf"
    },
    {
        "title": "A Fast Post-Training Pruning Framework for Transformers",
        "authors": [
            "Woosuk Kwon",
            "Sehoon Kim",
            "Michael W. Mahoney",
            "Joseph Hassoun",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "published": "2022-03-29T07:41:11Z",
        "summary": "Pruning is an effective way to reduce the huge inference cost of Transformer\nmodels. However, prior work on pruning Transformers requires retraining the\nmodels. This can add high training cost and high complexity to model\ndeployment, making it difficult to use in many practical situations. To address\nthis, we propose a fast post-training pruning framework for Transformers that\ndoes not require any retraining. Given a resource constraint and a sample\ndataset, our framework automatically prunes the Transformer model using\nstructured sparsity methods. To retain high accuracy without retraining, we\nintroduce three novel techniques: (i) a lightweight mask search algorithm that\nfinds which heads and filters to prune based on the Fisher information; (ii)\nmask rearrangement that complements the search algorithm; and (iii) mask tuning\nthat reconstructs the output activations for each layer. We apply our method to\nBERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD\nbenchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x\nspeedup in inference latency, while maintaining < 1% loss in accuracy.\nImportantly, our framework prunes Transformers in less than 3 minutes on a\nsingle GPU, which is over two orders of magnitude faster than existing pruning\napproaches that retrain the models.",
        "pdf_link": "https://arxiv.org/pdf/2204.09656v2.pdf"
    },
    {
        "title": "Worldwide city transport typology prediction with sentence-BERT based supervised learning via Wikipedia",
        "authors": [
            "Srushti Rath",
            "Joseph Y. J. Chow"
        ],
        "published": "2022-03-29T00:09:55Z",
        "summary": "An overwhelming majority of the world's human population lives in urban areas\nand cities. Understanding a city's transportation typology is immensely\nvaluable for planners and policy makers whose decisions can potentially impact\nmillions of city residents. Despite the value of understanding a city's\ntypology, labeled data (city and it's typology) is scarce, and spans at most a\nfew hundred cities in the current transportation literature. To break this\nbarrier, we propose a supervised machine learning approach to predict a city's\ntypology given the information in its Wikipedia page. Our method leverages\nrecent breakthroughs in natural language processing, namely sentence-BERT, and\nshows how the text-based information from Wikipedia can be effectively used as\na data source for city typology prediction tasks that can be applied to over\n2000 cities worldwide. We propose a novel method for low-dimensional city\nrepresentation using a city's Wikipedia page, which makes supervised learning\nof city typology labels tractable even with a few hundred labeled samples.\nThese features are used with labeled city samples to train binary classifiers\n(logistic regression) for four different city typologies: (i) congestion, (ii)\nauto-heavy, (iii) transit-heavy, and (iv) bike-friendly cities resulting in\nreasonably high AUC scores of 0.87, 0.86, 0.61 and 0.94 respectively. Our\napproach provides sufficient flexibility for incorporating additional variables\nin the city typology models and can be applied to study other city typologies\nas well. Our findings can assist a diverse group of stakeholders in\ntransportation and urban planning fields, and opens up new opportunities for\nusing text-based information from Wikipedia (or similar platforms) as data\nsources in such fields.",
        "pdf_link": "https://arxiv.org/pdf/2204.05193v1.pdf"
    },
    {
        "title": "Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model",
        "authors": [
            "Yu Du",
            "Fangyun Wei",
            "Zihe Zhang",
            "Miaojing Shi",
            "Yue Gao",
            "Guoqi Li"
        ],
        "published": "2022-03-28T17:50:26Z",
        "summary": "Recently, vision-language pre-training shows great potential in\nopen-vocabulary object detection, where detectors trained on base classes are\ndevised for detecting new classes. The class text embedding is firstly\ngenerated by feeding prompts to the text encoder of a pre-trained\nvision-language model. It is then used as the region classifier to supervise\nthe training of a detector. The key element that leads to the success of this\nmodel is the proper prompt, which requires careful words tuning and ingenious\ndesign. To avoid laborious prompt engineering, there are some prompt\nrepresentation learning methods being proposed for the image classification\ntask, which however can only be sub-optimal solutions when applied to the\ndetection task. In this paper, we introduce a novel method, detection prompt\n(DetPro), to learn continuous prompt representations for open-vocabulary object\ndetection based on the pre-trained vision-language model. Different from the\nprevious classification-oriented methods, DetPro has two highlights: 1) a\nbackground interpretation scheme to include the proposals in image background\ninto the prompt training; 2) a context grading scheme to separate proposals in\nimage foreground for tailored prompt training. We assemble DetPro with ViLD, a\nrecent state-of-the-art open-world object detector, and conduct experiments on\nthe LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365\ndatasets. Experimental results show that our DetPro outperforms the baseline\nViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the\nnovel classes of LVIS. Code and models are available at\nhttps://github.com/dyabel/detpro.",
        "pdf_link": "https://arxiv.org/pdf/2203.14940v1.pdf"
    },
    {
        "title": "Hierarchical Transformer Model for Scientific Named Entity Recognition",
        "authors": [
            "Urchade Zaratiana",
            "Pierre Holat",
            "Nadi Tomeh",
            "Thierry Charnois"
        ],
        "published": "2022-03-28T12:59:06Z",
        "summary": "The task of Named Entity Recognition (NER) is an important component of many\nnatural language processing systems, such as relation extraction and knowledge\ngraph construction. In this work, we present a simple and effective approach\nfor Named Entity Recognition. The main idea of our approach is to encode the\ninput subword sequence with a pre-trained transformer such as BERT, and then,\ninstead of directly classifying the word labels, another layer of transformer\nis added to the subword representation to better encode the word-level\ninteraction. We evaluate our approach on three benchmark datasets for\nscientific NER, particularly in the computer science and biomedical domains.\nExperimental results show that our model outperforms the current\nstate-of-the-art on SciERC and TDM datasets without requiring external\nresources or specific data augmentation. Code is available at\n\\url{https://github.com/urchade/HNER}.",
        "pdf_link": "https://arxiv.org/pdf/2203.14710v1.pdf"
    },
    {
        "title": "ANNA: Enhanced Language Representation for Question Answering",
        "authors": [
            "Changwook Jun",
            "Hansol Jang",
            "Myoseop Sim",
            "Hyun Kim",
            "Jooyoung Choi",
            "Kyungkoo Min",
            "Kyunghoon Bae"
        ],
        "published": "2022-03-28T05:26:52Z",
        "summary": "Pre-trained language models have brought significant improvements in\nperformance in a variety of natural language processing tasks. Most existing\nmodels performing state-of-the-art results have shown their approaches in the\nseparate perspectives of data processing, pre-training tasks, neural network\nmodeling, or fine-tuning. In this paper, we demonstrate how the approaches\naffect performance individually, and that the language model performs the best\nresults on a specific question answering task when those approaches are jointly\nconsidered in pre-training models. In particular, we propose an extended\npre-training task, and a new neighbor-aware mechanism that attends neighboring\ntokens more to capture the richness of context for pre-training language\nmodeling. Our best model achieves new state-of-the-art results of 95.7\\% F1 and\n90.6\\% EM on SQuAD 1.1 and also outperforms existing pre-trained language\nmodels such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2203.14507v2.pdf"
    },
    {
        "title": "STaR: Bootstrapping Reasoning With Reasoning",
        "authors": [
            "Eric Zelikman",
            "Yuhuai Wu",
            "Jesse Mu",
            "Noah D. Goodman"
        ],
        "published": "2022-03-28T03:12:15Z",
        "summary": "Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2203.14465v2.pdf"
    },
    {
        "title": "Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection",
        "authors": [
            "Xin Huang",
            "Ashish Khetan",
            "Rene Bidart",
            "Zohar Karnin"
        ],
        "published": "2022-03-27T19:52:01Z",
        "summary": "Transformer-based language models such as BERT have achieved the\nstate-of-the-art performance on various NLP tasks, but are computationally\nprohibitive. A recent line of works use various heuristics to successively\nshorten sequence length while transforming tokens through encoders, in tasks\nsuch as classification and ranking that require a single token embedding for\nprediction. We present a novel solution to this problem, called Pyramid-BERT\nwhere we replace previously used heuristics with a {\\em core-set} based token\nselection method justified by theoretical results. The core-set based token\nselection technique allows us to avoid expensive pre-training, gives a\nspace-efficient fine tuning, and thus makes it suitable to handle longer\nsequence lengths. We provide extensive experiments establishing advantages of\npyramid BERT over several baselines and existing works on the GLUE benchmarks\nand Long Range Arena datasets.",
        "pdf_link": "https://arxiv.org/pdf/2203.14380v1.pdf"
    },
    {
        "title": "Example-based Hypernetworks for Out-of-Distribution Generalization",
        "authors": [
            "Tomer Volk",
            "Eyal Ben-David",
            "Ohad Amosy",
            "Gal Chechik",
            "Roi Reichart"
        ],
        "published": "2022-03-27T11:10:10Z",
        "summary": "As Natural Language Processing (NLP) algorithms continually achieve new\nmilestones, out-of-distribution generalization remains a significant challenge.\nThis paper addresses the issue of multi-source adaptation for unfamiliar\ndomains: We leverage labeled data from multiple source domains to generalize to\nunknown target domains at training. Our innovative framework employs\nexample-based Hypernetwork adaptation: a T5 encoder-decoder initially generates\na unique signature from an input example, embedding it within the source\ndomains' semantic space. This signature is subsequently utilized by a\nHypernetwork to generate the task classifier's weights. We evaluated our method\nacross two tasks - sentiment classification and natural language inference - in\n29 adaptation scenarios, where it outpaced established algorithms. In an\nadvanced version, the signature also enriches the input example's\nrepresentation. We also compare our finetuned architecture to few-shot GPT-3,\ndemonstrating its effectiveness in essential use cases. To our knowledge, this\nmarks the first application of Hypernetworks to the adaptation for unknown\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2203.14276v3.pdf"
    },
    {
        "title": "Text Adversarial Purification as Defense against Adversarial Attacks",
        "authors": [
            "Linyang Li",
            "Demin Song",
            "Xipeng Qiu"
        ],
        "published": "2022-03-27T04:41:55Z",
        "summary": "Adversarial purification is a successful defense mechanism against\nadversarial attacks without requiring knowledge of the form of the incoming\nattack. Generally, adversarial purification aims to remove the adversarial\nperturbations therefore can make correct predictions based on the recovered\nclean samples. Despite the success of adversarial purification in the computer\nvision field that incorporates generative models such as energy-based models\nand diffusion models, using purification as a defense strategy against textual\nadversarial attacks is rarely explored. In this work, we introduce a novel\nadversarial purification method that focuses on defending against textual\nadversarial attacks. With the help of language models, we can inject noise by\nmasking input texts and reconstructing the masked texts based on the masked\nlanguage models. In this way, we construct an adversarial purification process\nfor textual models against the most widely used word-substitution adversarial\nattacks. We test our proposed adversarial purification method on several strong\nadversarial attack methods including Textfooler and BERT-Attack and\nexperimental results indicate that the purification algorithm can successfully\ndefend against strong word-substitution attacks.",
        "pdf_link": "https://arxiv.org/pdf/2203.14207v2.pdf"
    },
    {
        "title": "A Roadmap for Big Model",
        "authors": [
            "Sha Yuan",
            "Hanyu Zhao",
            "Shuai Zhao",
            "Jiahong Leng",
            "Yangxiao Liang",
            "Xiaozhi Wang",
            "Jifan Yu",
            "Xin Lv",
            "Zhou Shao",
            "Jiaao He",
            "Yankai Lin",
            "Xu Han",
            "Zhenghao Liu",
            "Ning Ding",
            "Yongming Rao",
            "Yizhao Gao",
            "Liang Zhang",
            "Ming Ding",
            "Cong Fang",
            "Yisen Wang",
            "Mingsheng Long",
            "Jing Zhang",
            "Yinpeng Dong",
            "Tianyu Pang",
            "Peng Cui",
            "Lingxiao Huang",
            "Zheng Liang",
            "Huawei Shen",
            "Hui Zhang",
            "Quanshi Zhang",
            "Qingxiu Dong",
            "Zhixing Tan",
            "Mingxuan Wang",
            "Shuo Wang",
            "Long Zhou",
            "Haoran Li",
            "Junwei Bao",
            "Yingwei Pan",
            "Weinan Zhang",
            "Zhou Yu",
            "Rui Yan",
            "Chence Shi",
            "Minghao Xu",
            "Zuobai Zhang",
            "Guoqiang Wang",
            "Xiang Pan",
            "Mengjie Li",
            "Xiaoyu Chu",
            "Zijun Yao",
            "Fangwei Zhu",
            "Shulin Cao",
            "Weicheng Xue",
            "Zixuan Ma",
            "Zhengyan Zhang",
            "Shengding Hu",
            "Yujia Qin",
            "Chaojun Xiao",
            "Zheni Zeng",
            "Ganqu Cui",
            "Weize Chen",
            "Weilin Zhao",
            "Yuan Yao",
            "Peng Li",
            "Wenzhao Zheng",
            "Wenliang Zhao",
            "Ziyi Wang",
            "Borui Zhang",
            "Nanyi Fei",
            "Anwen Hu",
            "Zenan Ling",
            "Haoyang Li",
            "Boxi Cao",
            "Xianpei Han",
            "Weidong Zhan",
            "Baobao Chang",
            "Hao Sun",
            "Jiawen Deng",
            "Chujie Zheng",
            "Juanzi Li",
            "Lei Hou",
            "Xigang Cao",
            "Jidong Zhai",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jiwen Lu",
            "Zhiwu Lu",
            "Qin Jin",
            "Ruihua Song",
            "Ji-Rong Wen",
            "Zhouchen Lin",
            "Liwei Wang",
            "Hang Su",
            "Jun Zhu",
            "Zhifang Sui",
            "Jiajun Zhang",
            "Yang Liu",
            "Xiaodong He",
            "Minlie Huang",
            "Jian Tang",
            "Jie Tang"
        ],
        "published": "2022-03-26T15:38:00Z",
        "summary": "With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&Interpretability, Commonsense Reasoning, Reliability&Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.",
        "pdf_link": "https://arxiv.org/pdf/2203.14101v4.pdf"
    },
    {
        "title": "Autoregressive Linguistic Steganography Based on BERT and Consistency Coding",
        "authors": [
            "Xiaoyan Zheng",
            "Hanzhou Wu"
        ],
        "published": "2022-03-26T02:36:55Z",
        "summary": "Linguistic steganography (LS) conceals the presence of communication by\nembedding secret information into a text. How to generate a high-quality text\ncarrying secret information is a key problem. With the widespread application\nof deep learning in natural language processing, recent algorithms use a\nlanguage model (LM) to generate the steganographic text, which provides a\nhigher payload compared with many previous arts. However, the security still\nneeds to be enhanced. To tackle with this problem, we propose a novel\nautoregressive LS algorithm based on BERT and consistency coding, which\nachieves a better trade-off between embedding payload and system security. In\nthe proposed work, based on the introduction of the masked LM, given a text, we\nuse consistency coding to make up for the shortcomings of block coding used in\nthe previous work so that we can encode arbitrary-size candidate token set and\ntake advantages of the probability distribution for information hiding. The\nmasked positions to be embedded are filled with tokens determined by an\nautoregressive manner to enhance the connection between contexts and therefore\nmaintain the quality of the text. Experimental results have shown that,\ncompared with related works, the proposed work improves the fluency of the\nsteganographic text while guaranteeing security, and also increases the\nembedding payload to a certain extent.",
        "pdf_link": "https://arxiv.org/pdf/2203.13972v1.pdf"
    },
    {
        "title": "A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data",
        "authors": [
            "Christopher Meaney",
            "Wali Hakimpour",
            "Sumeet Kalia",
            "Rahim Moineddin"
        ],
        "published": "2022-03-25T19:42:03Z",
        "summary": "Objective: To comparatively evaluate several transformer model architectures\nat identifying protected health information (PHI) in the i2b2/UTHealth 2014\nclinical text de-identification challenge corpus.\n  Methods: The i2b2/UTHealth 2014 corpus contains N=1304 clinical notes\nobtained from N=296 patients. Using a transfer learning framework, we fine-tune\nseveral transformer model architectures on the corpus, including: BERT-base,\nBERT-large, ROBERTA-base, ROBERTA-large, ALBERT-base and ALBERT-xxlarge. During\nfine-tuning we vary the following model hyper-parameters: batch size, number\ntraining epochs, learning rate and weight decay. We fine tune models on a\ntraining data set, we evaluate and select optimally performing models on an\nindependent validation dataset, and lastly assess generalization performance on\na held-out test dataset. We assess model performance in terms of accuracy,\nprecision (positive predictive value), recall (sensitivity) and F1 score\n(harmonic mean of precision and recall). We are interested in overall model\nperformance (PHI identified vs. PHI not identified), as well as PHI-specific\nmodel performance.\n  Results: We observe that the ROBERTA-large models perform best at identifying\nPHI in the i2b2/UTHealth 2014 corpus, achieving >99% overall accuracy and 96.7%\nrecall/precision on the heldout test corpus. Performance was good across many\nPHI classes; however, accuracy/precision/recall decreased for identification of\nthe following entity classes: professions, organizations, ages, and certain\nlocations.\n  Conclusions: Transformers are a promising model class/architecture for\nclinical text de-identification. With minimal hyper-parameter tuning\ntransformers afford researchers/clinicians the opportunity to obtain (near)\nstate-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2204.07056v1.pdf"
    },
    {
        "title": "L3Cube-MahaHate: A Tweet-based Marathi Hate Speech Detection Dataset and BERT models",
        "authors": [
            "Abhishek Velankar",
            "Hrushikesh Patil",
            "Amol Gore",
            "Shubham Salunke",
            "Raviraj Joshi"
        ],
        "published": "2022-03-25T17:00:33Z",
        "summary": "Social media platforms are used by a large number of people prominently to\nexpress their thoughts and opinions. However, these platforms have contributed\nto a substantial amount of hateful and abusive content as well. Therefore, it\nis important to curb the spread of hate speech on these platforms. In India,\nMarathi is one of the most popular languages used by a wide audience. In this\nwork, we present L3Cube-MahaHate, the first major Hate Speech Dataset in\nMarathi. The dataset is curated from Twitter, annotated manually. Our dataset\nconsists of over 25000 distinct tweets labeled into four major classes i.e\nhate, offensive, profane, and not. We present the approaches used for\ncollecting and annotating the data and the challenges faced during the process.\nFinally, we present baseline classification results using deep learning models\nbased on CNN, LSTM, and Transformers. We explore mono-lingual and multi-lingual\nvariants of BERT like MahaBERT, IndicBERT, mBERT, and xlm-RoBERTa and show that\nmono-lingual models perform better than their multi-lingual counterparts. The\nMahaBERT model provides the best results on L3Cube-MahaHate Corpus. The data\nand models are available at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2203.13778v2.pdf"
    },
    {
        "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations",
        "authors": [
            "Hanlin Tang",
            "Xipeng Zhang",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2022-03-25T07:27:18Z",
        "summary": "Recently, pre-trained Transformer based language models, such as BERT, have\nshown great superiority over the traditional methods in many Natural Language\nProcessing (NLP) tasks. However, the computational cost for deploying these\nmodels is prohibitive on resource-restricted devices. One method to alleviate\nthis computation overhead is to quantize the original model into fewer bits\nrepresentation, and previous work has proved that we can at most quantize both\nweights and activations of BERT into 8-bits, without degrading its performance.\nIn this work, we propose MKQ-BERT, which further improves the compression level\nand uses 4-bits for quantization. In MKQ-BERT, we propose a novel way for\ncomputing the gradient of the quantization scale, combined with an advanced\ndistillation strategy. On the one hand, we prove that MKQ-BERT outperforms the\nexisting BERT quantization methods for achieving a higher accuracy under the\nsame compression level. On the other hand, we are the first work that\nsuccessfully deploys the 4-bits BERT and achieves an end-to-end speedup for\ninference. Our results suggest that we could achieve 5.3x of bits reduction\nwithout degrading the model accuracy, and the inference speed of one int4 layer\nis 15x faster than a float32 layer in Transformer based model.",
        "pdf_link": "https://arxiv.org/pdf/2203.13483v1.pdf"
    },
    {
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
        "authors": [
            "Erik Nijkamp",
            "Bo Pang",
            "Hiroaki Hayashi",
            "Lifu Tu",
            "Huan Wang",
            "Yingbo Zhou",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "published": "2022-03-25T06:55:15Z",
        "summary": "Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.",
        "pdf_link": "https://arxiv.org/pdf/2203.13474v5.pdf"
    },
    {
        "title": "Predicting Clinical Intent from Free Text Electronic Health Records",
        "authors": [
            "Kawsar Noor",
            "Katherine Smith",
            "Julia Bennett",
            "Jade OConnell",
            "Jessica Fisk",
            "Monika Hunt",
            "Gary Philippo",
            "Teresa Xu",
            "Simon Knight",
            "Luis Romao",
            "Richard JB Dobson",
            "Wai Keong Wong"
        ],
        "published": "2022-03-25T04:27:00Z",
        "summary": "After a patient consultation, a clinician determines the steps in the\nmanagement of the patient. A clinician may for example request to see the\npatient again or refer them to a specialist. Whilst most clinicians will record\ntheir intent as \"next steps\" in the patient's clinical notes, in some cases the\nclinician may forget to indicate their intent as an order or request, e.g.\nfailure to place the follow-up order. This consequently results in patients\nbecoming lost-to-follow up and may in some cases lead to adverse consequences.\nIn this paper we train a machine learning model to detect a clinician's intent\nto follow up with a patient from the patient's clinical notes. Annotators\nsystematically identified 22 possible types of clinical intent and annotated\n3000 Bariatric clinical notes. The annotation process revealed a class\nimbalance in the labeled data and we found that there was only sufficient\nlabeled data to train 11 out of the 22 intents. We used the data to train a\nBERT based multilabel classification model and reported the following average\naccuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,\nmacro-f1: 0.90.",
        "pdf_link": "https://arxiv.org/pdf/2204.09594v1.pdf"
    },
    {
        "title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers",
        "authors": [
            "Arthur Bucker",
            "Luis Figueredo",
            "Sami Haddadin",
            "Ashish Kapoor",
            "Shuang Ma",
            "Rogerio Bonatti"
        ],
        "published": "2022-03-25T01:36:56Z",
        "summary": "Natural language is the most intuitive medium for us to interact with other\npeople when expressing commands and instructions. However, using language is\nseldom an easy task when humans need to express their intent towards robots,\nsince most of the current language interfaces require rigid templates with a\nstatic set of action targets and commands. In this work, we provide a flexible\nlanguage-based interface for human-robot collaboration, which allows a user to\nreshape existing trajectories for an autonomous agent. We take advantage of\nrecent advancements in the field of large language models (BERT and CLIP) to\nencode the user command, and then combine these features with trajectory\ninformation using multi-modal attention transformers. We train the model using\nimitation learning over a dataset containing robot trajectories modified by\nlanguage commands, and treat the trajectory generation process as a sequence\nprediction problem, analogously to how language generation architectures\noperate. We evaluate the system in multiple simulated trajectory scenarios, and\nshow a significant performance increase of our model over baseline approaches.\nIn addition, our real-world experiments with a robot arm show that users\nsignificantly prefer our natural language interface over traditional methods\nsuch as kinesthetic teaching or cost-function programming. Our study shows how\nthe field of robotics can take advantage of large pre-trained language models\ntowards creating more intuitive interfaces between robots and machines. Project\nwebpage: https://arthurfenderbucker.github.io/NL_trajectory_reshaper/",
        "pdf_link": "https://arxiv.org/pdf/2203.13411v1.pdf"
    },
    {
        "title": "GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models",
        "authors": [
            "Changye Li",
            "David Knopman",
            "Weizhe Xu",
            "Trevor Cohen",
            "Serguei Pakhomov"
        ],
        "published": "2022-03-25T00:25:42Z",
        "summary": "Deep learning (DL) techniques involving fine-tuning large numbers of model\nparameters have delivered impressive performance on the task of discriminating\nbetween language produced by cognitively healthy individuals, and those with\nAlzheimer's disease (AD). However, questions remain about their ability to\ngeneralize beyond the small reference sets that are publicly available for\nresearch. As an alternative to fitting model parameters directly, we propose a\nnovel method by which a Transformer DL model (GPT-2) pre-trained on general\nEnglish text is paired with an artificially degraded version of itself (GPT-D),\nto compute the ratio between these two models' \\textit{perplexities} on\nlanguage from cognitively healthy and impaired individuals. This technique\napproaches state-of-the-art performance on text data from a widely used \"Cookie\nTheft\" picture description task, and unlike established alternatives also\ngeneralizes well to spontaneous conversations. Furthermore, GPT-D generates\ntext with characteristics known to be associated with AD, demonstrating the\ninduction of dementia-related linguistic anomalies. Our study is a step toward\nbetter understanding of the relationships between the inner workings of\ngenerative neural language models, the language that they produce, and the\ndeleterious effects of dementia on human speech and language characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2203.13397v1.pdf"
    },
    {
        "title": "Remember and Forget Experience Replay for Multi-Agent Reinforcement Learning",
        "authors": [
            "Pascal Weber",
            "Daniel W\u00e4lchli",
            "Mustafa Zeqiri",
            "Petros Koumoutsakos"
        ],
        "published": "2022-03-24T19:59:43Z",
        "summary": "We present the extension of the Remember and Forget for Experience Replay\n(ReF-ER) algorithm to Multi-Agent Reinforcement Learning (MARL). ReF-ER was\nshown to outperform state of the art algorithms for continuous control in\nproblems ranging from the OpenAI Gym to complex fluid flows. In MARL, the\ndependencies between the agents are included in the state-value estimator and\nthe environment dynamics are modeled via the importance weights used by ReF-ER.\nIn collaborative environments, we find the best performance when the value is\nestimated using individual rewards and we ignore the effects of other actions\non the transition map. We benchmark the performance of ReF-ER MARL on the\nStanford Intelligent Systems Laboratory (SISL) environments. We find that\nemploying a single feed-forward neural network for the policy and the value\nfunction in ReF-ER MARL, outperforms state of the art algorithms that rely on\ncomplex neural network architectures.",
        "pdf_link": "https://arxiv.org/pdf/2203.13319v3.pdf"
    },
    {
        "title": "Mix and Match: Learning-free Controllable Text Generation using Energy Language Models",
        "authors": [
            "Fatemehsadat Mireshghallah",
            "Kartik Goyal",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2022-03-24T18:52:09Z",
        "summary": "Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.",
        "pdf_link": "https://arxiv.org/pdf/2203.13299v2.pdf"
    },
    {
        "title": "Token Dropping for Efficient BERT Pretraining",
        "authors": [
            "Le Hou",
            "Richard Yuanzhe Pang",
            "Tianyi Zhou",
            "Yuexin Wu",
            "Xinying Song",
            "Xiaodan Song",
            "Denny Zhou"
        ],
        "published": "2022-03-24T17:50:46Z",
        "summary": "Transformer-based models generally allocate the same amount of computation\nfor each token in a given sequence. We develop a simple but effective \"token\ndropping\" method to accelerate the pretraining of transformer models, such as\nBERT, without degrading its performance on downstream tasks. In short, we drop\nunimportant tokens starting from an intermediate layer in the model to make the\nmodel focus on important tokens; the dropped tokens are later picked up by the\nlast layer of the model so that the model still produces full-length sequences.\nWe leverage the already built-in masked language modeling (MLM) loss to\nidentify unimportant tokens with practically no computational overhead. In our\nexperiments, this simple approach reduces the pretraining cost of BERT by 25%\nwhile achieving similar overall fine-tuning performance on standard downstream\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.13240v1.pdf"
    },
    {
        "title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion",
        "authors": [
            "Kurt Shuster",
            "Mojtaba Komeili",
            "Leonard Adolphs",
            "Stephen Roller",
            "Arthur Szlam",
            "Jason Weston"
        ],
        "published": "2022-03-24T17:31:26Z",
        "summary": "Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine->Knowledge->Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2203.13224v2.pdf"
    },
    {
        "title": "Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",
        "authors": [
            "I\u00f1igo Urteaga",
            "Moulay-Za\u00efdane Dra\u00efdia",
            "Tomer Lancewicki",
            "Shahram Khadivi"
        ],
        "published": "2022-03-24T16:12:21Z",
        "summary": "We design and evaluate a Bayesian optimization framework for resource\nefficient pre-training of Transformer-based language models (TLMs). TLM\npre-training requires high computational resources and introduces many\nunresolved design choices, such as selecting its pre-training hyperparameters.\nWe propose a multi-armed bandit framework for the sequential selection of TLM\npre-training hyperparameters, aimed at optimizing language model performance,\nin a resource efficient manner. We design a Thompson sampling algorithm, with a\nsurrogate Gaussian process reward model of the Masked Language Model (MLM)\npre-training objective, for its sequential minimization. Instead of MLM\npre-training with fixed masking probabilities, the proposed Gaussian\nprocess-based Thompson sampling (GP-TS) accelerates pre-training by\nsequentially selecting masking hyperparameters that improve performance. We\nempirically demonstrate how GP-TS pre-trains language models efficiently, i.e.,\nit achieves lower MLM loss in fewer epochs, across a variety of settings. In\naddition, GP-TS pre-trained TLMs attain competitive downstream performance,\nwhile avoiding expensive hyperparameter grid search. GP-TS provides an\ninteractive framework for efficient and optimized TLM pre-training that, by\ncircumventing costly hyperparameter selection, enables substantial\ncomputational savings.",
        "pdf_link": "https://arxiv.org/pdf/2203.13151v2.pdf"
    },
    {
        "title": "minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models",
        "authors": [
            "Kanishka Misra"
        ],
        "published": "2022-03-24T15:11:06Z",
        "summary": "We present minicons, an open source library that provides a standard API for\nresearchers interested in conducting behavioral and representational analyses\nof transformer-based language models (LMs). Specifically, minicons enables\nresearchers to apply analysis methods at two levels: (1) at the prediction\nlevel -- by providing functions to efficiently extract word/sentence level\nprobabilities; and (2) at the representational level -- by also facilitating\nefficient extraction of word/phrase level vectors from one or more layers. In\nthis paper, we describe the library and apply it to two motivating case\nstudies: One focusing on the learning dynamics of the BERT architecture on\nrelative grammatical judgments, and the other on benchmarking 23 different LMs\non zero-shot abductive reasoning. minicons is available at\nhttps://github.com/kanishkamisra/minicons",
        "pdf_link": "https://arxiv.org/pdf/2203.13112v1.pdf"
    },
    {
        "title": "Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction",
        "authors": [
            "Maksym Tarnavskyi",
            "Artem Chernodub",
            "Kostiantyn Omelianchuk"
        ],
        "published": "2022-03-24T13:18:36Z",
        "summary": "In this paper, we investigate improvements to the GEC sequence tagging\narchitecture with a focus on ensembling of recent cutting-edge\nTransformer-based encoders in Large configurations. We encourage ensembling\nmodels by majority votes on span-level edits because this approach is tolerant\nto the model architecture and vocabulary size. Our best ensemble achieves a new\nSOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without\npre-training on synthetic datasets. In addition, we perform knowledge\ndistillation with a trained ensemble to generate new synthetic training\ndatasets, \"Troy-Blogs\" and \"Troy-1BW\". Our best single sequence tagging model\nthat is pretrained on the generated Troy-datasets in combination with the\npublicly available synthetic PIE dataset achieves a near-SOTA (To the best of\nour knowledge, our best single model gives way only to much heavier T5 model\nresult with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,\nand trained models are publicly available).",
        "pdf_link": "https://arxiv.org/pdf/2203.13064v1.pdf"
    },
    {
        "title": "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory",
        "authors": [
            "Li Siyao",
            "Weijiang Yu",
            "Tianpei Gu",
            "Chunze Lin",
            "Quan Wang",
            "Chen Qian",
            "Chen Change Loy",
            "Ziwei Liu"
        ],
        "published": "2022-03-24T13:06:43Z",
        "summary": "Driving 3D characters to dance following a piece of music is highly\nchallenging due to the spatial constraints applied to poses by choreography\nnorms. In addition, the generated dance sequence also needs to maintain\ntemporal coherency with different music genres. To tackle these challenges, we\npropose a novel music-to-dance framework, Bailando, with two powerful\ncomponents: 1) a choreographic memory that learns to summarize meaningful\ndancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic\nGenerative Pre-trained Transformer (GPT) that composes these units to a fluent\ndance coherent to the music. With the learned choreographic memory, dance\ngeneration is realized on the quantized units that meet high choreography\nstandards, such that the generated dancing sequences are confined within the\nspatial constraints. To achieve synchronized alignment between diverse motion\ntempos and music beats, we introduce an actor-critic-based reinforcement\nlearning scheme to the GPT with a newly-designed beat-align reward function.\nExtensive experiments on the standard benchmark demonstrate that our proposed\nframework achieves state-of-the-art performance both qualitatively and\nquantitatively. Notably, the learned choreographic memory is shown to discover\nhuman-interpretable dancing-style poses in an unsupervised manner.",
        "pdf_link": "https://arxiv.org/pdf/2203.13055v2.pdf"
    },
    {
        "title": "mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling",
        "authors": [
            "Seong-Hwan Heo",
            "WonKee Lee",
            "Jong-Hyeok Lee"
        ],
        "published": "2022-03-24T09:04:52Z",
        "summary": "Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.",
        "pdf_link": "https://arxiv.org/pdf/2203.12940v2.pdf"
    },
    {
        "title": "Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition",
        "authors": [
            "Onkar Litake",
            "Maithili Sabane",
            "Parth Patil",
            "Aparna Ranade",
            "Raviraj Joshi"
        ],
        "published": "2022-03-24T07:50:41Z",
        "summary": "Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.",
        "pdf_link": "https://arxiv.org/pdf/2203.12907v1.pdf"
    },
    {
        "title": "Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",
        "authors": [
            "Hanjie Chen",
            "Yangfeng Ji"
        ],
        "published": "2022-03-23T20:04:14Z",
        "summary": "Neural language models show vulnerability to adversarial examples which are\nsemantically similar to their original counterparts with a few words replaced\nby their synonyms. A common way to improve model robustness is adversarial\ntraining which follows two steps-collecting adversarial examples by attacking a\ntarget model, and fine-tuning the model on the augmented dataset with these\nadversarial examples. The objective of traditional adversarial training is to\nmake a model produce the same correct predictions on an original/adversarial\nexample pair. However, the consistency between model decision-makings on two\nsimilar texts is ignored. We argue that a robust model should behave\nconsistently on original/adversarial example pairs, that is making the same\npredictions (what) based on the same reasons (how) which can be reflected by\nconsistent interpretations. In this work, we propose a novel feature-level\nadversarial training method named FLAT. FLAT aims at improving model robustness\nin terms of both predictions and interpretations. FLAT incorporates variational\nword masks in neural networks to learn global word importance and play as a\nbottleneck teaching the model to make predictions based on important words.\nFLAT explicitly shoots at the vulnerability problem caused by the mismatch\nbetween model understandings on the replaced words and their synonyms in\noriginal/adversarial example pairs by regularizing the corresponding global\nword importance scores. Experiments show the effectiveness of FLAT in improving\nthe robustness with respect to both predictions and interpretations of four\nneural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks\non four text classification tasks. The models trained via FLAT also show better\nrobustness than baseline models on unforeseen adversarial examples across\ndifferent attacks.",
        "pdf_link": "https://arxiv.org/pdf/2203.12709v1.pdf"
    },
    {
        "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
        "authors": [
            "Umang Gupta",
            "Jwala Dhamala",
            "Varun Kumar",
            "Apurv Verma",
            "Yada Pruksachatkun",
            "Satyapriya Krishna",
            "Rahul Gupta",
            "Kai-Wei Chang",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "published": "2022-03-23T17:34:35Z",
        "summary": "Language models excel at generating coherent text, and model compression\ntechniques such as knowledge distillation have enabled their use in\nresource-constrained settings. However, these models can be biased in multiple\nways, including the unfounded association of male and female genders with\ngender-neutral professions. Therefore, knowledge distillation without any\nfairness constraints may preserve or exaggerate the teacher model's biases onto\nthe distilled model. To this end, we present a novel approach to mitigate\ngender disparity in text generation by learning a fair model during knowledge\ndistillation. We propose two modifications to the base knowledge distillation\nbased on counterfactual role reversal$\\unicode{x2014}$modifying teacher\nprobabilities and augmenting the training set. We evaluate gender polarity\nacross professions in open-ended text generated from the resulting distilled\nand finetuned GPT$\\unicode{x2012}$2 models and demonstrate a substantial\nreduction in gender disparity with only a minor compromise in utility. Finally,\nwe observe that language models that reduce gender polarity in language\ngeneration do not improve embedding fairness or downstream classification\nfairness.",
        "pdf_link": "https://arxiv.org/pdf/2203.12574v1.pdf"
    },
    {
        "title": "Prompt-based System for Personality and Interpersonal Reactivity Prediction",
        "authors": [
            "Bin Li",
            "Yixuan Weng"
        ],
        "published": "2022-03-23T15:22:34Z",
        "summary": "This paper describes our proposed method for the Workshop on Computational\nApproaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) 2022\nshared task on Personality Prediction (PER) and Reactivity Index Prediction\n(IRI). In this paper, we adopt the prompt-based learning method with the\npre-trained language model to accomplish these tasks. Specifically, the prompt\nis designed to provide knowledge of the extra personalized information for\nenhancing the pre-trained model. Data augmentation and model ensemble are\nadopted for obtaining better results. Moreover, we also provided the online\nsoftware demonstration and the codes of the software for further research.",
        "pdf_link": "https://arxiv.org/pdf/2203.12481v3.pdf"
    },
    {
        "title": "Input-specific Attention Subnetworks for Adversarial Detection",
        "authors": [
            "Emil Biju",
            "Anirudh Sriram",
            "Pratyush Kumar",
            "Mitesh M Khapra"
        ],
        "published": "2022-03-23T09:46:41Z",
        "summary": "Self-attention heads are characteristic of Transformer models and have been\nwell studied for interpretability and pruning. In this work, we demonstrate an\naltogether different utility of attention heads, namely for adversarial\ndetection. Specifically, we propose a method to construct input-specific\nattention subnetworks (IAS) from which we extract three features to\ndiscriminate between authentic and adversarial inputs. The resultant detector\nsignificantly improves (by over 7.5%) the state-of-the-art adversarial\ndetection accuracy for the BERT encoder on 10 NLU datasets with 11 different\nadversarial attack types. We also demonstrate that our method (a) is more\naccurate for larger models which are likely to have more spurious correlations\nand thus vulnerable to adversarial attack, and (b) performs well even with\nmodest training sets of adversarial examples.",
        "pdf_link": "https://arxiv.org/pdf/2203.12298v1.pdf"
    },
    {
        "title": "Visual Prompt Tuning",
        "authors": [
            "Menglin Jia",
            "Luming Tang",
            "Bor-Chun Chen",
            "Claire Cardie",
            "Serge Belongie",
            "Bharath Hariharan",
            "Ser-Nam Lim"
        ],
        "published": "2022-03-23T01:17:16Z",
        "summary": "The current modus operandi in adapting pre-trained models involves updating\nall the backbone parameters, ie, full fine-tuning. This paper introduces Visual\nPrompt Tuning (VPT) as an efficient and effective alternative to full\nfine-tuning for large-scale Transformer models in vision. Taking inspiration\nfrom recent advances in efficiently tuning large language models, VPT\nintroduces only a small amount (less than 1% of model parameters) of trainable\nparameters in the input space while keeping the model backbone frozen. Via\nextensive experiments on a wide variety of downstream recognition tasks, we\nshow that VPT achieves significant performance gains compared to other\nparameter efficient tuning protocols. Most importantly, VPT even outperforms\nfull fine-tuning in many cases across model capacities and training data\nscales, while reducing per-task storage cost.",
        "pdf_link": "https://arxiv.org/pdf/2203.12119v2.pdf"
    },
    {
        "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
        "authors": [
            "Tianyu Hua",
            "Yonglong Tian",
            "Sucheng Ren",
            "Michalis Raptis",
            "Hang Zhao",
            "Leonid Sigal"
        ],
        "published": "2022-03-22T21:28:55Z",
        "summary": "Inspired by the success of self-supervised autoregressive representation\nlearning in natural language (GPT and its variants), and advances in recent\nvisual architecture design with Vision Transformers (ViTs), in this paper, we\nexplore the effect various design choices have on the success of applying such\ntraining strategies for visual feature learning. Specifically, we introduce a\nnovel strategy that we call Random Segments with Autoregressive Coding\n(RandSAC). In RandSAC, we group patch representations (image tokens) into\nhierarchically arranged segments; within each segment, tokens are predicted in\nparallel, similar to BERT, while across segment predictions are sequential,\nsimilar to GPT. We illustrate that randomized serialization of the segments\nsignificantly improves the performance and results in distribution over\nspatially-long (across-segments) and -short (within-segment) predictions which\nare effective for feature learning. We illustrate the pertinence of these\ndesign choices and explore alternatives on a number of datasets (e.g., CIFAR10,\nCIFAR100, ImageNet). While our pre-training strategy works with a vanilla\nTransformer, we also propose a conceptually simple, but highly effective,\naddition to the decoder that allows learnable skip-connections to encoder$'$s\nfeature layers, which further improves the performance.",
        "pdf_link": "https://arxiv.org/pdf/2203.12054v2.pdf"
    },
    {
        "title": "Transformer based ensemble for emotion detection",
        "authors": [
            "Aditya Kane",
            "Shantanu Patankar",
            "Sahil Khose",
            "Neeraja Kirtane"
        ],
        "published": "2022-03-22T17:11:18Z",
        "summary": "Detecting emotions in languages is important to accomplish a complete\ninteraction between humans and machines. This paper describes our contribution\nto the WASSA 2022 shared task which handles this crucial task of emotion\ndetection. We have to identify the following emotions: sadness, surprise,\nneutral, anger, fear, disgust, joy based on a given essay text. We are using an\nensemble of ELECTRA and BERT models to tackle this problem achieving an F1\nscore of $62.76\\%$. Our codebase (https://bit.ly/WASSA_shared_task) and our\nWandB project (https://wandb.ai/acl_wassa_pictxmanipal/acl_wassa) is publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2203.11899v2.pdf"
    },
    {
        "title": "Under the Hood of Transformer Networks for Trajectory Forecasting",
        "authors": [
            "Luca Franco",
            "Leonardo Placidi",
            "Francesco Giuliari",
            "Irtiza Hasan",
            "Marco Cristani",
            "Fabio Galasso"
        ],
        "published": "2022-03-22T16:56:05Z",
        "summary": "Transformer Networks have established themselves as the de-facto\nstate-of-the-art for trajectory forecasting but there is currently no\nsystematic study on their capability to model the motion patterns of people,\nwithout interactions with other individuals nor the social context. This paper\nproposes the first in-depth study of Transformer Networks (TF) and\nBidirectional Transformers (BERT) for the forecasting of the individual motion\nof people, without bells and whistles. We conduct an exhaustive evaluation of\ninput/output representations, problem formulations and sequence modeling,\nincluding a novel analysis of their capability to predict multi-modal futures.\nOut of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are\ntop performers in predicting individual motions, definitely overcoming RNNs and\nLSTMs. Furthermore, they remain within a narrow margin wrt more complex\ntechniques, which include both social interactions and scene contexts. Source\ncode will be released for all conducted experiments.",
        "pdf_link": "https://arxiv.org/pdf/2203.11878v1.pdf"
    },
    {
        "title": "Open-Vocabulary DETR with Conditional Matching",
        "authors": [
            "Yuhang Zang",
            "Wei Li",
            "Kaiyang Zhou",
            "Chen Huang",
            "Chen Change Loy"
        ],
        "published": "2022-03-22T16:54:52Z",
        "summary": "Open-vocabulary object detection, which is concerned with the problem of\ndetecting novel objects guided by natural language, has gained increasing\nattention from the community. Ideally, we would like to extend an\nopen-vocabulary detector such that it can produce bounding box predictions\nbased on user inputs in form of either natural language or exemplar image. This\noffers great flexibility and user experience for human-computer interaction. To\nthis end, we propose a novel open-vocabulary detector based on DETR -- hence\nthe name OV-DETR -- which, once trained, can detect any object given its class\nname or an exemplar image. The biggest challenge of turning DETR into an\nopen-vocabulary detector is that it is impossible to calculate the\nclassification cost matrix of novel classes without access to their labeled\nimages. To overcome this challenge, we formulate the learning objective as a\nbinary matching one between input queries (class name or exemplar image) and\nthe corresponding objects, which learns useful correspondence to generalize to\nunseen queries during testing. For training, we choose to condition the\nTransformer decoder on the input embeddings obtained from a pre-trained\nvision-language model like CLIP, in order to enable matching for both text and\nimage queries. With extensive experiments on LVIS and COCO datasets, we\ndemonstrate that our OV-DETR -- the first end-to-end Transformer-based\nopen-vocabulary detector -- achieves non-trivial improvements over current\nstate of the arts.",
        "pdf_link": "https://arxiv.org/pdf/2203.11876v2.pdf"
    },
    {
        "title": "Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments",
        "authors": [
            "Antonis Maronikolakis",
            "Axel Wisiorek",
            "Leah Nann",
            "Haris Jabbar",
            "Sahana Udupa",
            "Hinrich Schuetze"
        ],
        "published": "2022-03-22T14:24:56Z",
        "summary": "Building on current work on multilingual hate speech (e.g., Ousidhoum et al.\n(2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present\nXTREMESPEECH, a new hate speech dataset containing 20,297 social media passages\nfrom Brazil, Germany, India and Kenya. The key novelty is that we directly\ninvolve the affected communities in collecting and annotating the data - as\nopposed to giving companies and governments control over defining and\ncombatting hate speech. This inclusive approach results in datasets more\nrepresentative of actually occurring online speech and is likely to facilitate\nthe removal of the social media content that marginalized communities view as\ncausing the most harm. Based on XTREMESPEECH, we establish novel tasks with\naccompanying baselines, provide evidence that cross-country training is\ngenerally not feasible due to cultural differences between countries and\nperform an interpretability analysis of BERT's predictions.",
        "pdf_link": "https://arxiv.org/pdf/2203.11764v1.pdf"
    },
    {
        "title": "BERT-ASC: Implicit Aspect Representation Learning through Auxiliary-Sentence Construction for Sentiment Analysis",
        "authors": [
            "Murtadha Ahmed",
            "Shengfeng Pan",
            "Jianlin Su",
            "Xinxin Cao",
            "Wenze Zhang",
            "Bo Wen",
            "Yunfeng Liu"
        ],
        "published": "2022-03-22T13:12:27Z",
        "summary": "Aspect-based sentiment analysis (ABSA) task aim at associating a piece of\ntext with a set of aspects and meanwhile infer their respective sentimental\npolarities. The state-of-the-art approaches are built upon fine-tuning of\nvarious pre-trained language models. They commonly attempt to learn\naspect-specific representation from the corpus. Unfortunately, the aspect is\noften expressed implicitly through a set of representatives and thus renders\nimplicit mapping process unattainable unless sufficient labeled examples are\navailable. However, high-quality labeled examples may not be readily available\nin real-world scenarios. In this paper, we propose to jointly address aspect\ncategorization and aspect-based sentiment subtasks in a unified framework.\nSpecifically, we first introduce a simple but effective mechanism to construct\nan auxiliary-sentence for the implicit aspect based on the semantic information\nin the corpus. Then, we encourage BERT to learn the aspect-specific\nrepresentation in response to the automatically constructed auxiliary-sentence\ninstead of the aspect itself. Finally, we empirically evaluate the performance\nof the proposed solution by a comparative study on real benchmark datasets for\nboth ABSA and Targeted-ABSA tasks. Our extensive experiments show that it\nconsistently achieves state-of-the-art performance in terms of aspect\ncategorization and aspect-based sentiment across all datasets and the\nimprovement margins are considerable. The code of BERT-ASC is available in\nGitHub: https://github.com/amurtadha/BERT-ASC.",
        "pdf_link": "https://arxiv.org/pdf/2203.11702v2.pdf"
    },
    {
        "title": "Are You Misinformed? A Study of Covid-Related Fake News in Bengali on Facebook",
        "authors": [
            "Protik Bose Pranto",
            "Syed Zami-Ul-Haque Navid",
            "Protik Dey",
            "Gias Uddin",
            "Anindya Iqbal"
        ],
        "published": "2022-03-22T12:41:42Z",
        "summary": "Our opinions and views of life can be shaped by how we perceive the opinions\nof others on social media like Facebook. This dependence has increased during\nCOVID-19 periods when we have fewer means to connect with others. However, fake\nnews related to COVID-19 has become a significant problem on Facebook. Bengali\nis the seventh most spoken language worldwide, yet we are aware of no previous\nresearch that studied the prevalence of COVID-19 related fake news in Bengali\non Facebook. In this paper, we develop machine learning models to detect fake\nnews in Bengali automatically. The best performing model is BERT, with an\nF1-score of 0.97. We apply BERT on all Facebook Bengali posts related to\nCOVID-19. We find 10 topics in the COVID-19 Bengali fake news grouped into\nthree categories: System (e.g., medical system), belief (e.g., religious\nrituals), and social (e.g., scientific awareness).",
        "pdf_link": "https://arxiv.org/pdf/2203.11669v1.pdf"
    },
    {
        "title": "Factual Consistency of Multilingual Pretrained Language Models",
        "authors": [
            "Constanza Fierro",
            "Anders S\u00f8gaard"
        ],
        "published": "2022-03-22T09:15:53Z",
        "summary": "Pretrained language models can be queried for factual knowledge, with\npotential applications in knowledge base acquisition and tasks that require\ninference. However, for that, we need to know how reliable this knowledge is,\nand recent work has shown that monolingual English language models lack\nconsistency when predicting factual knowledge, that is, they fill-in-the-blank\ndifferently for paraphrases describing the same fact. In this paper, we extend\nthe analysis of consistency to a multilingual setting. We introduce a resource,\nmParaRel, and investigate (i) whether multilingual language models such as\nmBERT and XLM-R are more consistent than their monolingual counterparts; and\n(ii) if such models are equally consistent across languages. We find that mBERT\nis as inconsistent as English BERT in English paraphrases, but that both mBERT\nand XLM-R exhibit a high degree of inconsistency in English and even more so\nfor all the other 45 languages.",
        "pdf_link": "https://arxiv.org/pdf/2203.11552v1.pdf"
    },
    {
        "title": "VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension",
        "authors": [
            "Kiet Van Nguyen",
            "Son Quoc Tran",
            "Luan Thanh Nguyen",
            "Tin Van Huynh",
            "Son T. Luu",
            "Ngan Luu-Thuy Nguyen"
        ],
        "published": "2022-03-22T00:44:41Z",
        "summary": "One of the emerging research trends in natural language understanding is\nmachine reading comprehension (MRC) which is the task to find answers to human\nquestions based on textual data. Existing Vietnamese datasets for MRC research\nconcentrate solely on answerable questions. However, in reality, questions can\nbe unanswerable for which the correct answer is not stated in the given textual\ndata. To address the weakness, we provide the research community with a\nbenchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question\nanswering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a\nbenchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on\nVietnamese Language and Speech Processing (VLSP 2021). This task attracted 77\nparticipant teams from 34 universities and other organizations. In this\narticle, we present details of the organization of the challenge, an overview\nof the methods employed by shared-task participants, and the results. The\nhighest performances are 77.24% in F1-score and 67.43% in Exact Match on the\nprivate test set. The Vietnamese MRC systems proposed by the top 3 teams use\nXLM-RoBERTa, a powerful pre-trained language model based on the transformer\narchitecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further\nexplore the Vietnamese machine reading comprehension task and related tasks\nsuch as question answering, question generation, and natural language\ninference.",
        "pdf_link": "https://arxiv.org/pdf/2203.11400v3.pdf"
    },
    {
        "title": "Towards Textual Out-of-Domain Detection without In-Domain Labels",
        "authors": [
            "Di Jin",
            "Shuyang Gao",
            "Seokhwan Kim",
            "Yang Liu",
            "Dilek Hakkani-Tur"
        ],
        "published": "2022-03-22T00:11:46Z",
        "summary": "In many real-world settings, machine learning models need to identify user\ninputs that are out-of-domain (OOD) so as to avoid performing wrong actions.\nThis work focuses on a challenging case of OOD detection, where no labels for\nin-domain data are accessible (e.g., no intent labels for the intent\nclassification task). To this end, we first evaluate different language model\nbased approaches that predict likelihood for a sequence of tokens. Furthermore,\nwe propose a novel representation learning based method by combining\nunsupervised clustering and contrastive learning so that better data\nrepresentations for OOD detection can be learned. Through extensive\nexperiments, we demonstrate that this method can significantly outperform\nlikelihood-based methods and can be even competitive to the state-of-the-art\nsupervised approaches with label information.",
        "pdf_link": "https://arxiv.org/pdf/2203.11396v1.pdf"
    },
    {
        "title": "Enhancing Speech Recognition Decoding via Layer Aggregation",
        "authors": [
            "Tomer Wullach",
            "Shlomo E. Chazan"
        ],
        "published": "2022-03-21T20:28:06Z",
        "summary": "Recently proposed speech recognition systems are designed to predict using\nrepresentations generated by their top layers, employing greedy decoding which\nisolates each timestep from the rest of the sequence. Aiming for improved\nperformance, a beam search algorithm is frequently utilized and a language\nmodel is incorporated to assist with ranking the top candidates. In this work,\nwe experiment with several speech recognition models and find that logits\npredicted using the top layers may hamper beam search from achieving optimal\nresults. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield\nhighly confident predictions, and hypothesize that the predictions are based on\nlocal information and may not take full advantage of the information encoded in\nintermediate layers. To this end, we perform a layer analysis to reveal and\nvisualize how predictions evolve throughout the inference flow. We then propose\na prediction method that aggregates the top M layers, potentially leveraging\nuseful information encoded in intermediate layers and relaxing model\nconfidence. We showcase the effectiveness of our approach via beam search\ndecoding, conducting our experiments on Librispeech test and dev sets and\nachieving WER, and CER reduction of up to 10% and 22%, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2203.11325v2.pdf"
    },
    {
        "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization",
        "authors": [
            "Zheng Li",
            "Zijian Wang",
            "Ming Tan",
            "Ramesh Nallapati",
            "Parminder Bhatia",
            "Andrew Arnold",
            "Bing Xiang",
            "Dan Roth"
        ],
        "published": "2022-03-21T18:04:25Z",
        "summary": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve\nstate-of-the-art performance on many generative NLP tasks. However, such models\npose a great challenge in resource-constrained scenarios owing to their large\nmemory requirements and high latency. To alleviate this issue, we propose to\njointly distill and quantize the model, where knowledge is transferred from the\nfull-precision teacher model to the quantized and distilled low-precision\nstudent model. Empirical analyses show that, despite the challenging nature of\ngenerative tasks, we were able to achieve a 16.5x model footprint compression\nratio with little performance drop relative to the full-precision counterparts\non multiple summarization and QA datasets. We further pushed the limit of\ncompression ratio to 27.7x and presented the performance-efficiency trade-off\nfor generative tasks using pre-trained models. To the best of our knowledge,\nthis is the first work aiming to effectively distill and quantize\nsequence-to-sequence pre-trained models for language generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.11239v1.pdf"
    },
    {
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "authors": [
            "Xuezhi Wang",
            "Jason Wei",
            "Dale Schuurmans",
            "Quoc Le",
            "Ed Chi",
            "Sharan Narang",
            "Aakanksha Chowdhery",
            "Denny Zhou"
        ],
        "published": "2022-03-21T17:48:52Z",
        "summary": "Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).",
        "pdf_link": "https://arxiv.org/pdf/2203.11171v4.pdf"
    },
    {
        "title": "Teaching language models to support answers with verified quotes",
        "authors": [
            "Jacob Menick",
            "Maja Trebacz",
            "Vladimir Mikulik",
            "John Aslanides",
            "Francis Song",
            "Martin Chadwick",
            "Mia Glaese",
            "Susannah Young",
            "Lucy Campbell-Gillingham",
            "Geoffrey Irving",
            "Nat McAleese"
        ],
        "published": "2022-03-21T17:26:29Z",
        "summary": "Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.",
        "pdf_link": "https://arxiv.org/pdf/2203.11147v1.pdf"
    },
    {
        "title": "Towards Explainable Evaluation Metrics for Natural Language Generation",
        "authors": [
            "Christoph Leiter",
            "Piyawat Lertvittayakumjorn",
            "Marina Fomicheva",
            "Wei Zhao",
            "Yang Gao",
            "Steffen Eger"
        ],
        "published": "2022-03-21T17:05:54Z",
        "summary": "Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics (such as BERTScore or MoverScore) are based on black-box\nlanguage models such as BERT or XLM-R. They often achieve strong correlations\nwith human judgments, but recent research indicates that the lower-quality\nclassical metrics remain dominant, one of the potential reasons being that\ntheir decision processes are transparent. To foster more widespread acceptance\nof the novel high-quality metrics, explainability thus becomes crucial. In this\nconcept paper, we identify key properties and propose key goals of explainable\nmachine translation evaluation metrics. We also provide a synthesizing overview\nover recent approaches for explainable machine translation metrics and discuss\nhow they relate to those goals and properties. Further, we conduct own novel\nexperiments, which (among others) find that current adversarial NLP techniques\nare unsuitable for automatically identifying limitations of high-quality\nblack-box evaluation metrics, as they are not meaning-preserving. Finally, we\nprovide a vision of future approaches to explainable evaluation metrics and\ntheir evaluation. We hope that our work can help catalyze and guide future\nresearch on explainable evaluation metrics and, mediately, also contribute to\nbetter and more transparent text generation systems.",
        "pdf_link": "https://arxiv.org/pdf/2203.11131v1.pdf"
    },
    {
        "title": "AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive Summarization",
        "authors": [
            "Moussa Kamal Eddine",
            "Nadi Tomeh",
            "Nizar Habash",
            "Joseph Le Roux",
            "Michalis Vazirgiannis"
        ],
        "published": "2022-03-21T13:11:41Z",
        "summary": "Like most natural language understanding and generation tasks,\nstate-of-the-art models for summarization are transformer-based\nsequence-to-sequence architectures that are pretrained on large corpora. While\nmost existing models focused on English, Arabic remained understudied. In this\npaper we propose AraBART, the first Arabic model in which the encoder and the\ndecoder are pretrained end-to-end, based on BART. We show that AraBART achieves\nthe best performance on multiple abstractive summarization datasets,\noutperforming strong baselines including a pretrained Arabic BERT-based model\nand multilingual mBART and mT5 models.",
        "pdf_link": "https://arxiv.org/pdf/2203.10945v1.pdf"
    },
    {
        "title": "Neural Token Segmentation for High Token-Internal Complexity",
        "authors": [
            "Idan Brusilovsky",
            "Reut Tsarfaty"
        ],
        "published": "2022-03-21T10:07:17Z",
        "summary": "Tokenizing raw texts into word units is an essential pre-processing step for\ncritical tasks in the NLP pipeline such as tagging, parsing, named entity\nrecognition, and more. For most languages, this tokenization step\nstraightforward. However, for languages with high token-internal complexity,\nfurther token-to-word segmentation is required. Previous canonical segmentation\nstudies were based on character-level frameworks, with no contextualised\nrepresentation involved. Contextualized vectors a la BERT show remarkable\nresults in many applications, but were not shown to improve performance on\nlinguistic segmentation per se. Here we propose a novel neural segmentation\nmodel which combines the best of both worlds, contextualised token\nrepresentation and char-level decoding, which is particularly effective for\nlanguages with high token-internal complexity and extreme morphological\nambiguity. Our model shows substantial improvements in segmentation accuracy on\nHebrew and Arabic compared to the state-of-the-art, and leads to further\nimprovements on downstream tasks such as Part-of-Speech Tagging, Dependency\nParsing and Named-Entity Recognition, over existing pipelines. When comparing\nour segmentation-first pipeline with joint segmentation and labeling in the\nsame settings, we show that, contrary to pre-neural studies, the pipeline\nperformance is superior.",
        "pdf_link": "https://arxiv.org/pdf/2203.10845v1.pdf"
    },
    {
        "title": "Multitask Neuroevolution for Reinforcement Learning with Long and Short Episodes",
        "authors": [
            "Nick Zhang",
            "Abhishek Gupta",
            "Zefeng Chen",
            "Yew-Soon Ong"
        ],
        "published": "2022-03-21T10:06:16Z",
        "summary": "Studies have shown evolution strategies (ES) to be a promising approach for\nreinforcement learning (RL) with deep neural networks. However, the issue of\nhigh sample complexity persists in applications of ES to deep RL over long\nhorizons. This paper is the first to address the shortcoming of today's methods\nvia a novel neuroevolutionary multitasking (NuEMT) algorithm, designed to\ntransfer information from a set of auxiliary tasks (of short episode length) to\nthe target (full length) RL task at hand. The auxiliary tasks, extracted from\nthe target, allow an agent to update and quickly evaluate policies on shorter\ntime horizons. The evolved skills are then transferred to guide the longer and\nharder task towards an optimal policy. We demonstrate that the NuEMT algorithm\nachieves data-efficient evolutionary RL, reducing expensive agent-environment\ninteraction data requirements. Our key algorithmic contribution in this setting\nis to introduce, for the first time, a multitask skills transfer mechanism\nbased on the statistical importance sampling technique. In addition, an\nadaptive resource allocation strategy is utilized to assign computational\nresources to auxiliary tasks based on their gleaned usefulness. Experiments on\na range of continuous control tasks from the OpenAI Gym confirm that our\nproposed algorithm is efficient compared to recent ES baselines.",
        "pdf_link": "https://arxiv.org/pdf/2203.10844v3.pdf"
    },
    {
        "title": "TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural Language Processing",
        "authors": [
            "Mucheng Ren",
            "Heyan Huang",
            "Yuxiang Zhou",
            "Qianwen Cao",
            "Yuan Bu",
            "Yang Gao"
        ],
        "published": "2022-03-21T09:59:54Z",
        "summary": "Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy\nthat has spread and been applied worldwide. The unique TCM diagnosis and\ntreatment system requires a comprehensive analysis of a patient's symptoms\nhidden in the clinical record written in free text. Prior studies have shown\nthat this system can be informationized and intelligentized with the aid of\nartificial intelligence (AI) technology, such as natural language processing\n(NLP). However, existing datasets are not of sufficient quality nor quantity to\nsupport the further development of data-driven AI technology in TCM. Therefore,\nin this paper, we focus on the core task of the TCM diagnosis and treatment\nsystem -- syndrome differentiation (SD) -- and we introduce the first public\nlarge-scale dataset for SD, called TCM-SD. Our dataset contains 54,152\nreal-world clinical records covering 148 syndromes. Furthermore, we collect a\nlarge-scale unlabelled textual corpus in the field of TCM and propose a\ndomain-specific pre-trained language model, called ZY-BERT. We conducted\nexperiments using deep neural networks to establish a strong performance\nbaseline, reveal various challenges in SD, and prove the potential of\ndomain-specific pre-trained language model. Our study and analysis reveal\nopportunities for incorporating computer science and linguistics knowledge to\nexplore the empirical validity of TCM theories.",
        "pdf_link": "https://arxiv.org/pdf/2203.10839v2.pdf"
    },
    {
        "title": "Semantic Similarity Computing for Scientific Academic Conferences fused with domain features",
        "authors": [
            "Runyu Yu",
            "Yawen Li",
            "Ang Li"
        ],
        "published": "2022-03-21T04:55:21Z",
        "summary": "Aiming at the problem that the current general-purpose semantic text\nsimilarity calculation methods are difficult to use the semantic information of\nscientific academic conference data, a semantic similarity calculation\nalgorithm for scientific academic conferences by fusion with domain features is\nproposed. First, the domain feature information of the conference is obtained\nthrough entity recognition and keyword extraction, and it is input into the\nBERT network as a feature and the conference information. The structure of the\nSiamese network is used to solve the anisotropy problem of BERT. The output of\nthe network is pooled and normalized, and finally the cosine similarity is used\nto calculate the similarity between the two sessions. Experimental results show\nthat the SBFD algorithm has achieved good results on different data sets, and\nthe Spearman correlation coefficient has a certain improvement compared with\nthe comparison algorithm.",
        "pdf_link": "https://arxiv.org/pdf/2203.12593v1.pdf"
    },
    {
        "title": "An Intellectual Property Entity Recognition Method Based on Transformer and Technological Word Information",
        "authors": [
            "Yuhui Wang",
            "Junping Du",
            "Yingxia Shao"
        ],
        "published": "2022-03-21T03:28:37Z",
        "summary": "Patent texts contain a large amount of entity information. Through named\nentity recognition, intellectual property entity information containing key\ninformation can be extracted from it, helping researchers to understand the\npatent content faster. Therefore, it is difficult for existing named entity\nextraction methods to make full use of the semantic information at the word\nlevel brought about by professional vocabulary changes. This paper proposes a\nmethod for extracting intellectual property entities based on Transformer and\ntechnical word information , and provides accurate word vector representation\nin combination with the BERT language method. In the process of word vector\ngeneration, the technical word information extracted by IDCNN is added to\nimprove the understanding of intellectual property entities Representation\nability. Finally, the Transformer encoder that introduces relative position\nencoding is used to learn the deep semantic information of the text from the\nsequence of word vectors, and realize entity label prediction. Experimental\nresults on public datasets and annotated patent datasets show that the method\nimproves the accuracy of entity recognition.",
        "pdf_link": "https://arxiv.org/pdf/2203.10717v1.pdf"
    },
    {
        "title": "Compression of Generative Pre-trained Language Models via Quantization",
        "authors": [
            "Chaofan Tao",
            "Lu Hou",
            "Wei Zhang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Ping Luo",
            "Ngai Wong"
        ],
        "published": "2022-03-21T02:11:35Z",
        "summary": "The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2203.10705v2.pdf"
    },
    {
        "title": "Better Language Model with Hypernym Class Prediction",
        "authors": [
            "He Bai",
            "Tong Wang",
            "Alessandro Sordoni",
            "Peng Shi"
        ],
        "published": "2022-03-21T01:16:44Z",
        "summary": "Class-based language models (LMs) have been long devised to address context\nsparsity in $n$-gram LMs. In this study, we revisit this approach in the\ncontext of neural LMs. We hypothesize that class-based prediction leads to an\nimplicit context aggregation for similar words and thus can improve\ngeneralization for rare words. We map words that have a common WordNet hypernym\nto the same class and train large neural LMs by gradually annealing from\npredicting the class to token prediction during training. Empirically, this\ncurriculum learning strategy consistently improves perplexity over various\nlarge, highly-performant state-of-the-art Transformer-based models on two\ndatasets, WikiText-103 and Arxiv. Our analysis shows that the performance\nimprovement is achieved without sacrificing performance on rare words. Finally,\nwe document other attempts that failed to yield empirical gains, and discuss\nfuture directions for the adoption of class-based LMs on a larger scale.",
        "pdf_link": "https://arxiv.org/pdf/2203.10692v1.pdf"
    },
    {
        "title": "Mitigating Gender Bias in Machine Translation through Adversarial Learning",
        "authors": [
            "Eve Fleisig",
            "Christiane Fellbaum"
        ],
        "published": "2022-03-20T23:35:09Z",
        "summary": "Machine translation and other NLP systems often contain significant biases\nregarding sensitive attributes, such as gender or race, that worsen system\nperformance and perpetuate harmful stereotypes. Recent preliminary research\nsuggests that adversarial learning can be used as part of a model-agnostic bias\nmitigation method that requires no data modifications. However, adapting this\nstrategy for machine translation and other modern NLP domains requires (1)\nrestructuring training objectives in the context of fine-tuning pretrained\nlarge language models and (2) developing measures for gender or other protected\nvariables for tasks in which these attributes must be deduced from the data\nitself.\n  We present an adversarial learning framework that addresses these challenges\nto mitigate gender bias in seq2seq machine translation. Our framework improves\nthe disparity in translation quality for sentences with male vs. female\nentities by 86% for English-German translation and 91% for English-French\ntranslation, with minimal effect on translation quality. The results suggest\nthat adversarial learning is a promising technique for mitigating gender bias\nin machine translation.",
        "pdf_link": "https://arxiv.org/pdf/2203.10675v1.pdf"
    },
    {
        "title": "Immersive Text Game and Personality Classification",
        "authors": [
            "Wanshui Li",
            "Yifan Bai",
            "Jiaxuan Lu",
            "Kexin Yi"
        ],
        "published": "2022-03-20T18:37:03Z",
        "summary": "We designed and built a game called \\textit{Immersive Text Game}, which\nallows the player to choose a story and a character, and interact with other\ncharacters in the story in an immersive manner of dialogues. The game is based\non several latest models, including text generation language model, information\nextraction model, commonsense reasoning model, and psychology evaluation model.\nIn the past, similar text games usually let players choose from limited actions\ninstead of answering on their own, and not every time what characters said are\ndetermined by the player. Through the combination of these models and elaborate\ngame mechanics and modes, the player will find some novel experiences as driven\nthrough the storyline.",
        "pdf_link": "https://arxiv.org/pdf/2203.10621v1.pdf"
    },
    {
        "title": "Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation",
        "authors": [
            "Zongyang Ma",
            "Guan Luo",
            "Jin Gao",
            "Liang Li",
            "Yuxin Chen",
            "Shaoru Wang",
            "Congxuan Zhang",
            "Weiming Hu"
        ],
        "published": "2022-03-20T16:31:49Z",
        "summary": "Open-vocabulary object detection aims to detect novel object categories\nbeyond the training set.\n  The advanced open-vocabulary two-stage detectors employ instance-level\nvisual-to-visual knowledge distillation to align the visual space of the\ndetector with the semantic space of the Pre-trained Visual-Language Model\n(PVLM).\n  However, in the more efficient one-stage detector, the absence of\nclass-agnostic object proposals hinders the knowledge distillation on unseen\nobjects, leading to severe performance degradation.\n  In this paper, we propose a hierarchical visual-language knowledge\ndistillation method, i.e., HierKD, for open-vocabulary one-stage detection.\n  Specifically, a global-level knowledge distillation is explored to transfer\nthe knowledge of unseen categories from the PVLM to the detector.\n  Moreover, we combine the proposed global-level knowledge distillation and the\ncommon instance-level knowledge distillation to learn the knowledge of seen and\nunseen categories simultaneously.\n  Extensive experiments on MS-COCO show that our method significantly surpasses\nthe previous best one-stage detector with 11.9\\% and 6.7\\% $AP_{50}$ gains\nunder the zero-shot detection and generalized zero-shot detection settings, and\nreduces the $AP_{50}$ performance gap from 14\\% to 7.3\\% compared to the best\ntwo-stage detector.",
        "pdf_link": "https://arxiv.org/pdf/2203.10593v1.pdf"
    },
    {
        "title": "Cluster & Tune: Boost Cold Start Performance in Text Classification",
        "authors": [
            "Eyal Shnarch",
            "Ariel Gera",
            "Alon Halfon",
            "Lena Dankin",
            "Leshem Choshen",
            "Ranit Aharonov",
            "Noam Slonim"
        ],
        "published": "2022-03-20T15:29:34Z",
        "summary": "In real-world scenarios, a text classification task often begins with a cold\nstart, when labeled data is scarce. In such cases, the common practice of\nfine-tuning pre-trained models, such as BERT, for a target classification task,\nis prone to produce poor performance. We suggest a method to boost the\nperformance of such models by adding an intermediate unsupervised\nclassification task, between the pre-training and fine-tuning phases. As such\nan intermediate task, we perform clustering and train the pre-trained model on\npredicting the cluster labels. We test this hypothesis on various data sets,\nand show that this additional classification phase can significantly improve\nperformance, mainly for topical classification tasks, when the number of\nlabeled instances available for fine-tuning is only a couple of dozen to a few\nhundred.",
        "pdf_link": "https://arxiv.org/pdf/2203.10581v1.pdf"
    },
    {
        "title": "g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin",
        "authors": [
            "Yi-Chang Chen",
            "Yu-Chuan Chang",
            "Yen-Cheng Chang",
            "Yi-Ren Yeh"
        ],
        "published": "2022-03-20T02:28:25Z",
        "summary": "Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2203.10430v5.pdf"
    },
    {
        "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
        "authors": [
            "Ahmed Alajrami",
            "Nikolaos Aletras"
        ],
        "published": "2022-03-20T00:02:10Z",
        "summary": "Several pre-training objectives, such as masked language modeling (MLM), have\nbeen proposed to pre-train language models (e.g. BERT) with the aim of learning\nbetter language representations. However, to the best of our knowledge, no\nprevious work so far has investigated how different pre-training objectives\naffect what BERT learns about linguistics properties. We hypothesize that\nlinguistically motivated objectives such as MLM should help BERT to acquire\nbetter linguistic knowledge compared to other non-linguistically motivated\nobjectives that are not intuitive or hard for humans to guess the association\nbetween the input and the label to be predicted. To this end, we pre-train BERT\nwith two linguistically motivated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic characteristics encoded in the\nrepresentation of the resulting models. We find strong evidence that there are\nonly small differences in probing performance between the representations\nlearned by the two different types of objectives. These surprising results\nquestion the dominant narrative of linguistically informed pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2203.10415v1.pdf"
    },
    {
        "title": "On Robust Prefix-Tuning for Text Classification",
        "authors": [
            "Zonghan Yang",
            "Yang Liu"
        ],
        "published": "2022-03-19T18:52:47Z",
        "summary": "Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2203.10378v1.pdf"
    },
    {
        "title": "Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense",
        "authors": [
            "Thai Le",
            "Jooyoung Lee",
            "Kevin Yen",
            "Yifan Hu",
            "Dongwon Lee"
        ],
        "published": "2022-03-19T16:00:01Z",
        "summary": "We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K\nhuman-written text perturbations in the wild and leverages them for realistic\nadversarial attack. Unlike existing character-based attacks which often\ndeductively hypothesize a set of manipulation strategies, our work is grounded\non actual observations from real-world texts. We find that adversarial texts\ngenerated by ANTHRO achieve the best trade-off between (1) attack success rate,\n(2) semantic preservation of the original text, and (3) stealthiness--i.e.\nindistinguishable from human writings hence harder to be flagged as suspicious.\nSpecifically, our attacks accomplished around 83% and 91% attack success rates\non BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger\nbaseline with an increase of 50% and 40% in terms of semantic preservation and\nstealthiness when evaluated by both layperson and professional human workers.\nANTHRO can further enhance a BERT classifier's performance in understanding\ndifferent variations of human-written toxic texts via adversarial training when\ncompared to the Perspective API.",
        "pdf_link": "https://arxiv.org/pdf/2203.10346v1.pdf"
    },
    {
        "title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge",
        "authors": [
            "Ganesh Jawahar",
            "Muhammad Abdul-Mageed",
            "Laks V. S. Lakshmanan"
        ],
        "published": "2022-03-19T15:35:59Z",
        "summary": "In this work, we focus on the problem of distinguishing a human written news\narticle from a news article that is created by manipulating entities in a human\nwritten news article (e.g., replacing entities with factually incorrect\nentities). Such manipulated articles can mislead the reader by posing as a\nhuman written news article. We propose a neural network based detector that\ndetects manipulated news articles by reasoning about the facts mentioned in the\narticle. Our proposed detector exploits factual knowledge via graph\nconvolutional neural network along with the textual information in the news\narticle. We also create challenging datasets for this task by considering\nvarious strategies to generate the new replacement entity (e.g., entity\ngeneration from GPT-2). In all the settings, our proposed model either matches\nor outperforms the state-of-the-art detector in terms of accuracy. Our code and\ndata are available at https://github.com/UBC-NLP/manipulated_entity_detection.",
        "pdf_link": "https://arxiv.org/pdf/2203.10343v1.pdf"
    },
    {
        "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
        "authors": [
            "Jiayi Wang",
            "Rongzhou Bao",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2022-03-19T14:06:46Z",
        "summary": "Recently, the problem of robustness of pre-trained language models (PrLMs)\nhas received increasing research interest. Latest studies on adversarial\nattacks achieve high attack success rates against PrLMs, claiming that PrLMs\nare not robust. However, we find that the adversarial samples that PrLMs fail\nare mostly non-natural and do not appear in reality. We question the validity\nof current evaluation of robustness of PrLMs based on these non-natural\nadversarial samples and propose an anomaly detector to evaluate the robustness\nof PrLMs with more natural adversarial samples. We also investigate two\napplications of the anomaly detector: (1) In data augmentation, we employ the\nanomaly detector to force generating augmented data that are distinguished as\nnon-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply\nthe anomaly detector to a defense framework to enhance the robustness of PrLMs.\nIt can be used to defend all types of attacks and achieves higher accuracy on\nboth adversarial samples and compliant samples than other defense frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2203.11199v1.pdf"
    },
    {
        "title": "Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging",
        "authors": [
            "Houquan Zhou",
            "Yang Li",
            "Zhenghua Li",
            "Min Zhang"
        ],
        "published": "2022-03-19T12:33:38Z",
        "summary": "In recent years, large-scale pre-trained language models (PLMs) have made\nextraordinary progress in most NLP tasks. But, in the unsupervised POS tagging\ntask, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA)\nperformance. The recent SOTA performance is yielded by a Guassian HMM variant\nproposed by He et al. (2018). However, as a generative model, HMM makes very\nstrong independence assumptions, making it very challenging to incorporate\ncontexualized word representations from PLMs. In this work, we for the first\ntime propose a neural conditional random field autoencoder (CRF-AE) model for\nunsupervised POS tagging. The discriminative encoder of CRF-AE can\nstraightforwardly incorporate ELMo word representations. Moreover, inspired by\nfeature-rich HMM, we reintroduce hand-crafted features into the decoder of\nCRF-AE. Finally, experiments clearly show that our model outperforms previous\nstate-of-the-art models by a large margin on Penn Treebank and multilingual\nUniversal Dependencies treebank v2.0.",
        "pdf_link": "https://arxiv.org/pdf/2203.10315v1.pdf"
    },
    {
        "title": "Dependency-based Mixture Language Models",
        "authors": [
            "Zhixian Yang",
            "Xiaojun Wan"
        ],
        "published": "2022-03-19T06:28:30Z",
        "summary": "Various models have been proposed to incorporate knowledge of syntactic\nstructures into neural language models. However, previous works have relied\nheavily on elaborate components for a specific language model, usually\nrecurrent neural network (RNN), which makes themselves unwieldy in practice to\nfit into other neural language models, such as Transformer and GPT-2. In this\npaper, we introduce the Dependency-based Mixture Language Models. In detail, we\nfirst train neural language models with a novel dependency modeling objective\nto learn the probability distribution of future dependent tokens given context.\nWe then formulate the next-token probability by mixing the previous dependency\nmodeling probability distributions with self-attention. Extensive experiments\nand human evaluations show that our method can be easily and effectively\napplied to different neural language models while improving neural text\ngeneration on various tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.10256v1.pdf"
    },
    {
        "title": "Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators",
        "authors": [
            "Keenan Jones",
            "Jason R. C. Nurse",
            "Shujun Li"
        ],
        "published": "2022-03-18T09:19:14Z",
        "summary": "Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.",
        "pdf_link": "https://arxiv.org/pdf/2203.09813v1.pdf"
    },
    {
        "title": "Three things everyone should know about Vision Transformers",
        "authors": [
            "Hugo Touvron",
            "Matthieu Cord",
            "Alaaeldin El-Nouby",
            "Jakob Verbeek",
            "Herv\u00e9 J\u00e9gou"
        ],
        "published": "2022-03-18T08:23:03Z",
        "summary": "After their initial success in natural language processing, transformer\narchitectures have rapidly gained traction in computer vision, providing\nstate-of-the-art results for tasks such as image classification, detection,\nsegmentation, and video analysis. We offer three insights based on simple and\neasy to implement variants of vision transformers. (1) The residual layers of\nvision transformers, which are usually processed sequentially, can to some\nextent be processed efficiently in parallel without noticeably affecting the\naccuracy. (2) Fine-tuning the weights of the attention layers is sufficient to\nadapt vision transformers to a higher resolution and to other classification\ntasks. This saves compute, reduces the peak memory consumption at fine-tuning\ntime, and allows sharing the majority of weights across tasks. (3) Adding\nMLP-based patch pre-processing layers improves Bert-like self-supervised\ntraining based on patch masking. We evaluate the impact of these design choices\nusing the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test\nset. Transfer performance is measured across six smaller datasets.",
        "pdf_link": "https://arxiv.org/pdf/2203.09795v1.pdf"
    },
    {
        "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information",
        "authors": [
            "Qian Ruan",
            "Malte Ostendorff",
            "Georg Rehm"
        ],
        "published": "2022-03-17T21:49:26Z",
        "summary": "Transformer-based language models usually treat texts as linear sequences.\nHowever, most texts also have an inherent hierarchical structure, i.e., parts\nof a text can be identified using their position in this hierarchy. In\naddition, section titles usually indicate the common topic of their respective\nsentences. We propose a novel approach to formulate, extract, encode and inject\nhierarchical structure information explicitly into an extractive summarization\nmodel based on a pre-trained, encoder-only Transformer language model\n(HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on\nPubMed and arXiv substantially. Using various experimental settings on three\ndatasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model\noutperforms a strong baseline collectively, which differs from our model only\nin that the hierarchical structure information is not injected. It is also\nobserved that the more conspicuous hierarchical structure the dataset has, the\nlarger improvements our method gains. The ablation study demonstrates that the\nhierarchical position information is the main contributor to our model's SOTA\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2203.09629v1.pdf"
    },
    {
        "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
        "authors": [
            "Thomas Hartvigsen",
            "Saadia Gabriel",
            "Hamid Palangi",
            "Maarten Sap",
            "Dipankar Ray",
            "Ece Kamar"
        ],
        "published": "2022-03-17T17:57:56Z",
        "summary": "Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.",
        "pdf_link": "https://arxiv.org/pdf/2203.09509v4.pdf"
    },
    {
        "title": "Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models",
        "authors": [
            "Aaron Mueller",
            "Robert Frank",
            "Tal Linzen",
            "Luheng Wang",
            "Sebastian Schuster"
        ],
        "published": "2022-03-17T15:46:53Z",
        "summary": "Relations between words are governed by hierarchical structure rather than\nlinear ordering. Sequence-to-sequence (seq2seq) models, despite their success\nin downstream NLP applications, often fail to generalize in a\nhierarchy-sensitive manner when performing syntactic transformations - for\nexample, transforming declarative sentences into questions. However, syntactic\nevaluations of seq2seq models have only observed models that were not\npre-trained on natural language data before being trained to perform syntactic\ntransformations, in spite of the fact that pre-training has been found to\ninduce hierarchical linguistic generalizations in language models; in other\nwords, the syntactic capabilities of seq2seq models may have been greatly\nunderstated. We address this gap using the pre-trained seq2seq models T5 and\nBART, as well as their multilingual variants mT5 and mBART. We evaluate whether\nthey generalize hierarchically on two transformations in two languages:\nquestion formation and passivization in English and German. We find that\npre-trained seq2seq models generalize hierarchically when performing syntactic\ntransformations, whereas models trained from scratch on syntactic\ntransformations do not. This result presents evidence for the learnability of\nhierarchical syntactic information from non-annotated natural language text\nwhile also demonstrating that seq2seq models are capable of syntactic\ngeneralization, though only after exposure to much more language data than\nhuman learners receive.",
        "pdf_link": "https://arxiv.org/pdf/2203.09397v1.pdf"
    },
    {
        "title": "Finding Structural Knowledge in Multimodal-BERT",
        "authors": [
            "Victor Milewski",
            "Miryam de Lhoneux",
            "Marie-Francine Moens"
        ],
        "published": "2022-03-17T13:20:01Z",
        "summary": "In this work, we investigate the knowledge learned in the embeddings of\nmultimodal-BERT models. More specifically, we probe their capabilities of\nstoring the grammatical structure of linguistic data and the structure learned\nover objects in visual data. To reach that goal, we first make the inherent\nstructure of language and visuals explicit by a dependency parse of the\nsentences that describe the image and by the dependencies between the object\nregions in the image, respectively. We call this explicit visual structure the\n\\textit{scene tree}, that is based on the dependency tree of the language\ndescription. Extensive probing experiments show that the multimodal-BERT models\ndo not encode these scene trees.Code available at\n\\url{https://github.com/VSJMilewski/multimodal-probes}.",
        "pdf_link": "https://arxiv.org/pdf/2203.09306v1.pdf"
    },
    {
        "title": "Universal Conditional Masked Language Pre-training for Neural Machine Translation",
        "authors": [
            "Pengfei Li",
            "Liangyou Li",
            "Meng Zhang",
            "Minghao Wu",
            "Qun Liu"
        ],
        "published": "2022-03-17T10:00:33Z",
        "summary": "Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching & masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.",
        "pdf_link": "https://arxiv.org/pdf/2203.09210v3.pdf"
    },
    {
        "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists",
        "authors": [
            "Giuseppe Attanasio",
            "Debora Nozza",
            "Dirk Hovy",
            "Elena Baralis"
        ],
        "published": "2022-03-17T09:29:50Z",
        "summary": "Natural Language Processing (NLP) models risk overfitting to specific terms\nin the training data, thereby reducing their performance, fairness, and\ngeneralizability. E.g., neural hate speech detection models are strongly\ninfluenced by identity terms like gay, or women, resulting in false positives,\nsevere unintended bias, and lower performance. Most mitigation techniques use\nlists of identity terms or samples from the target domain during training.\nHowever, this approach requires a-priori knowledge and introduces further bias\nif important terms are neglected. Instead, we propose a knowledge-free\nEntropy-based Attention Regularization (EAR) to discourage overfitting to\ntraining-specific terms. An additional objective function penalizes tokens with\nlow self-attention entropy. We fine-tune BERT via EAR: the resulting model\nmatches or exceeds state-of-the-art performance for hate speech classification\nand bias metrics on three benchmark corpora in English and Italian. EAR also\nreveals overfitting terms, i.e., terms most likely to induce bias, to help\nidentify their effect on the model, task, and predictions.",
        "pdf_link": "https://arxiv.org/pdf/2203.09192v1.pdf"
    },
    {
        "title": "Multilingual Detection of Personal Employment Status on Twitter",
        "authors": [
            "Manuel Tonneau",
            "Dhaval Adjodah",
            "Jo\u00e3o Palotti",
            "Nir Grinberg",
            "Samuel Fraiberger"
        ],
        "published": "2022-03-17T08:55:18Z",
        "summary": "Detecting disclosures of individuals' employment status on social media can\nprovide valuable information to match job seekers with suitable vacancies,\noffer social protection, or measure labor market flows. However, identifying\nsuch personal disclosures is a challenging task due to their rarity in a sea of\nsocial media content and the variety of linguistic forms used to describe them.\nHere, we examine three Active Learning (AL) strategies in real-world settings\nof extreme class imbalance, and identify five types of disclosures about\nindividuals' employment status (e.g. job loss) in three languages using\nBERT-based classification models. Our findings show that, even under extreme\nimbalance settings, a small number of AL iterations is sufficient to obtain\nlarge and significant gains in precision, recall, and diversity of results\ncompared to a supervised baseline with the same number of labels. We also find\nthat no AL strategy consistently outperforms the rest. Qualitative analysis\nsuggests that AL helps focus the attention mechanism of BERT on core terms and\nadjust the boundaries of semantic expansion, highlighting the importance of\ninterpretable models to provide greater control and visibility into this\ndynamic learning process.",
        "pdf_link": "https://arxiv.org/pdf/2203.09178v1.pdf"
    },
    {
        "title": "RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction",
        "authors": [
            "Yew Ken Chia",
            "Lidong Bing",
            "Soujanya Poria",
            "Luo Si"
        ],
        "published": "2022-03-17T05:55:14Z",
        "summary": "Despite the importance of relation extraction in building and representing\nknowledge, less research is focused on generalizing to unseen relations types.\nWe introduce the task setting of Zero-Shot Relation Triplet Extraction\n(ZeroRTE) to encourage further research in low-resource relation extraction\nmethods. Given an input sentence, each extracted triplet consists of the head\nentity, relation label, and tail entity where the relation label is not seen at\nthe training stage. To solve ZeroRTE, we propose to synthesize relation\nexamples by prompting language models to generate structured texts. Concretely,\nwe unify language model prompts and structured text approaches to design a\nstructured prompt template for generating synthetic relation samples when\nconditioning on relation label prompts (RelationPrompt). To overcome the\nlimitation for extracting multiple relation triplets in a sentence, we design a\nnovel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL\ndatasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot\nrelation classification. Our code and data are available at\ngithub.com/declare-lab/RelationPrompt.",
        "pdf_link": "https://arxiv.org/pdf/2203.09101v1.pdf"
    },
    {
        "title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT",
        "authors": [
            "Jing Zhao",
            "Yifan Wang",
            "Junwei Bao",
            "Youzheng Wu",
            "Xiaodong He"
        ],
        "published": "2022-03-17T03:33:47Z",
        "summary": "Transformer-based pre-trained models, such as BERT, have shown extraordinary\nsuccess in achieving state-of-the-art results in many natural language\nprocessing applications. However, deploying these models can be prohibitively\ncostly, as the standard self-attention mechanism of the Transformer suffers\nfrom quadratic computational cost in the input sequence length. To confront\nthis, we propose FCA, a fine- and coarse-granularity hybrid self-attention that\nreduces the computation cost through progressively shortening the computational\nsequence length in self-attention. Specifically, FCA conducts an\nattention-based scoring strategy to determine the informativeness of tokens at\neach layer. Then, the informative tokens serve as the fine-granularity\ncomputing units in self-attention and the uninformative tokens are replaced\nwith one or several clusters as the coarse-granularity computing units in\nself-attention. Experiments on GLUE and RACE datasets show that BERT with FCA\nachieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We\nshow that FCA offers a significantly better trade-off between accuracy and\nFLOPs compared to prior methods.",
        "pdf_link": "https://arxiv.org/pdf/2203.09055v1.pdf"
    },
    {
        "title": "Triangular Transfer: Freezing the Pivot for Triangular Machine Translation",
        "authors": [
            "Meng Zhang",
            "Liangyou Li",
            "Qun Liu"
        ],
        "published": "2022-03-17T02:00:40Z",
        "summary": "Triangular machine translation is a special case of low-resource machine\ntranslation where the language pair of interest has limited parallel data, but\nboth languages have abundant parallel data with a pivot language. Naturally,\nthe key to triangular machine translation is the successful exploitation of\nsuch auxiliary data. In this work, we propose a transfer-learning-based\napproach that utilizes all types of auxiliary data. As we train auxiliary\nsource-pivot and pivot-target translation models, we initialize some parameters\nof the pivot side with a pre-trained language model and freeze them to\nencourage both translation models to work in the same pivot language space, so\nthat they can be smoothly transferred to the source-target translation model.\nExperiments show that our approach can outperform previous ones.",
        "pdf_link": "https://arxiv.org/pdf/2203.09027v1.pdf"
    },
    {
        "title": "AdapLeR: Speeding up Inference by Adaptive Length Reduction",
        "authors": [
            "Ali Modarressi",
            "Hosein Mohebbi",
            "Mohammad Taher Pilehvar"
        ],
        "published": "2022-03-16T23:41:38Z",
        "summary": "Pre-trained language models have shown stellar performance in various\ndownstream tasks. But, this usually comes at the cost of high latency and\ncomputation, hindering their usage in resource-limited settings. In this work,\nwe propose a novel approach for reducing the computational cost of BERT with\nminimal loss in downstream performance. Our method dynamically eliminates less\ncontributing tokens through layers, resulting in shorter lengths and\nconsequently lower computational cost. To determine the importance of each\ntoken representation, we train a Contribution Predictor for each layer using a\ngradient-based saliency method. Our experiments on several diverse\nclassification tasks show speedups up to 22x during inference time without much\nsacrifice in performance. We also validate the quality of the selected tokens\nin our method using human annotations in the ERASER benchmark. In comparison to\nother widely used strategies for selecting important tokens, such as saliency\nand attention, our proposed method has a significantly lower false positive\nrate in generating rationales. Our code is freely available at\nhttps://github.com/amodaresi/AdapLeR .",
        "pdf_link": "https://arxiv.org/pdf/2203.08991v1.pdf"
    },
    {
        "title": "Label Semantics for Few Shot Named Entity Recognition",
        "authors": [
            "Jie Ma",
            "Miguel Ballesteros",
            "Srikanth Doss",
            "Rishita Anubhai",
            "Sunil Mallya",
            "Yaser Al-Onaizan",
            "Dan Roth"
        ],
        "published": "2022-03-16T23:21:05Z",
        "summary": "We study the problem of few shot learning for named entity recognition.\nSpecifically, we leverage the semantic information in the names of the labels\nas a way of giving the model additional signal and enriched priors. We propose\na neural architecture that consists of two BERT encoders, one to encode the\ndocument and its tokens and another one to encode each of the labels in natural\nlanguage format. Our model learns to match the representations of named\nentities computed by the first encoder with label representations computed by\nthe second encoder. The label semantics signal is shown to support improved\nstate-of-the-art results in multiple few shot NER benchmarks and on-par\nperformance in standard benchmarks. Our model is especially effective in low\nresource settings.",
        "pdf_link": "https://arxiv.org/pdf/2203.08985v1.pdf"
    },
    {
        "title": "CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals",
        "authors": [
            "Scott Novotney",
            "Sreeparna Mukherjee",
            "Zeeshan Ahmed",
            "Andreas Stolcke"
        ],
        "published": "2022-03-16T17:37:28Z",
        "summary": "We propose a framework to modularize the training of neural language models\nthat use diverse forms of sentence-external context (including metadata) by\neliminating the need to jointly train sentence-external and within-sentence\nencoders. Our approach, contextual universal embeddings (CUE), trains LMs on\none set of context, such as date and author, and adapts to novel metadata\ntypes, such as article title, or previous sentence. The model consists of a\npretrained neural sentence LM, a BERT-based context encoder, and a masked\ntransformer decoder that estimates LM probabilities using sentence-internal and\nsentence-external information. When context or metadata are unavailable, our\nmodel learns to combine contextual and sentence-internal information using\nnoisy oracle unigram embeddings as a proxy. Real contextual information can be\nintroduced later and used to adapt a small number of parameters that map\ncontextual data into the decoder's embedding space. We validate the CUE\nframework on a NYTimes text corpus with multiple metadata types, for which the\nLM perplexity can be lowered from 36.6 to 27.4 by conditioning on context.\nBootstrapping a contextual LM with only a subset of the context/metadata during\ntraining retains 85\\% of the achievable gain. Training the model initially with\nproxy context retains 67% of the perplexity gain after adapting to real\ncontext. Furthermore, we can swap one type of pretrained sentence LM for\nanother without retraining the context encoders, by only adapting the decoder\nmodel. Overall, we obtain a modular framework that allows incremental, scalable\ntraining of context-enhanced LMs.",
        "pdf_link": "https://arxiv.org/pdf/2203.08774v1.pdf"
    },
    {
        "title": "Multi-Stage Prompting for Knowledgeable Dialogue Generation",
        "authors": [
            "Zihan Liu",
            "Mostofa Patwary",
            "Ryan Prenger",
            "Shrimai Prabhumoye",
            "Wei Ping",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2022-03-16T16:53:43Z",
        "summary": "Existing knowledge-grounded dialogue systems typically use finetuned versions\nof a pretrained language model (LM) and large-scale knowledge bases. These\nmodels typically fail to generalize on topics outside of the knowledge base,\nand require maintaining separate potentially large checkpoints each time\nfinetuning is needed. In this paper, we aim to address these limitations by\nleveraging the inherent knowledge stored in the pretrained LM as well as its\npowerful generation ability. We propose a multi-stage prompting approach to\ngenerate knowledgeable responses from a single pretrained LM. We first prompt\nthe LM to generate knowledge based on the dialogue context. Then, we further\nprompt it to generate responses based on the dialogue context and the\npreviously generated knowledge. Results show that our knowledge generator\noutperforms the state-of-the-art retrieval-based model by 5.8% when combining\nknowledge relevance and correctness. In addition, our multi-stage prompting\noutperforms the finetuning-based dialogue model in terms of response\nknowledgeability and engagement by up to 10% and 5%, respectively. Furthermore,\nwe scale our model up to 530 billion parameters and show that larger LMs\nimprove the generation correctness score by up to 10%, and response relevance,\nknowledgeability and engagement by up to 10%. Our code is available at:\nhttps://github.com/NVIDIA/Megatron-LM.",
        "pdf_link": "https://arxiv.org/pdf/2203.08745v1.pdf"
    },
    {
        "title": "In-Context Learning for Few-Shot Dialogue State Tracking",
        "authors": [
            "Yushi Hu",
            "Chia-Hsuan Lee",
            "Tianbao Xie",
            "Tao Yu",
            "Noah A. Smith",
            "Mari Ostendorf"
        ],
        "published": "2022-03-16T11:58:24Z",
        "summary": "Collecting and annotating task-oriented dialogues is time-consuming and\ncostly; thus, zero and few shot learning could greatly benefit dialogue state\ntracking (DST). In this work, we propose an in-context learning (ICL) framework\nfor zero-shot and few-shot learning DST, where a large pre-trained language\nmodel (LM) takes a test instance and a few exemplars as input, and directly\ndecodes the dialogue state without any parameter updates. To better leverage a\ntabular domain description in the LM prompt, we reformulate DST into a\ntext-to-SQL problem. We also propose a novel approach to retrieve annotated\ndialogues as exemplars. Empirical results on MultiWOZ show that our method\nIC-DST substantially outperforms previous fine-tuned state-of-the-art models in\nfew-shot settings. In addition, we test IC-DST in zero-shot settings, in which\nthe model only takes a fixed task instruction as input, finding that it\noutperforms previous zero-shot methods by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2203.08568v3.pdf"
    },
    {
        "title": "Linking Theories and Methods in Cognitive Sciences via Joint Embedding of the Scientific Literature: The Example of Cognitive Control",
        "authors": [
            "Morteza Ansarinia",
            "Paul Schrater",
            "Pedro Cardoso-Leite"
        ],
        "published": "2022-03-16T11:03:09Z",
        "summary": "Traditionally, theory and practice of Cognitive Control are linked via\nliterature reviews by human domain experts. This approach, however, is\ninadequate to track the ever-growing literature. It may also be biased, and\nyield redundancies and confusion.\n  Here we present an alternative approach. We performed automated text analyses\non a large body of scientific texts to create a joint representation of tasks\nand constructs. More specifically, 385,705 scientific abstracts were first\nmapped into an embedding space using a transformers-based language model.\nDocument embeddings were then used to identify a task-construct graph embedding\nthat grounds constructs on tasks and supports nuanced meaning of the constructs\nby taking advantage of constrained random walks in the graph.\n  This joint task-construct graph embedding, can be queried to generate task\nbatteries targeting specific constructs, may reveal knowledge gaps in the\nliterature, and inspire new tasks and novel hypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2203.11016v2.pdf"
    },
    {
        "title": "Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding",
        "authors": [
            "Haojun Jiang",
            "Yuanze Lin",
            "Dongchen Han",
            "Shiji Song",
            "Gao Huang"
        ],
        "published": "2022-03-16T09:17:41Z",
        "summary": "Visual grounding, i.e., localizing objects in images according to natural\nlanguage queries, is an important topic in visual language understanding. The\nmost effective approaches for this task are based on deep learning, which\ngenerally require expensive manually labeled image-query or patch-query pairs.\nTo eliminate the heavy dependence on human annotations, we present a novel\nmethod, named Pseudo-Q, to automatically generate pseudo language queries for\nsupervised training. Our method leverages an off-the-shelf object detector to\nidentify visual objects from unlabeled images, and then language queries for\nthese objects are obtained in an unsupervised fashion with a pseudo-query\ngeneration module. Then, we design a task-related query prompt module to\nspecifically tailor generated pseudo language queries for visual grounding\ntasks. Further, in order to fully capture the contextual relationships between\nimages and language queries, we develop a visual-language model equipped with\nmulti-level cross-modality attention mechanism. Extensive experimental results\ndemonstrate that our method has two notable benefits: (1) it can reduce human\nannotation costs significantly, e.g., 31% on RefCOCO without degrading original\nmodel's performance under the fully supervised setting, and (2) without bells\nand whistles, it achieves superior or comparable performance compared to\nstate-of-the-art weakly-supervised visual grounding methods on all the five\ndatasets we have experimented. Code is available at\nhttps://github.com/LeapLabTHU/Pseudo-Q.",
        "pdf_link": "https://arxiv.org/pdf/2203.08481v2.pdf"
    },
    {
        "title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model",
        "authors": [
            "Antoine Nzeyimana",
            "Andre Niyongabo Rubungo"
        ],
        "published": "2022-03-16T08:36:14Z",
        "summary": "Pre-trained language models such as BERT have been successful at tackling\nmany natural language processing tasks. However, the unsupervised sub-word\ntokenization methods commonly used in these models (e.g., byte-pair encoding -\nBPE) are sub-optimal at handling morphologically rich languages. Even given a\nmorphological analyzer, naive sequencing of morphemes into a standard BERT\narchitecture is inefficient at capturing morphological compositionality and\nexpressing word-relative syntactic regularities. We address these challenges by\nproposing a simple yet effective two-tier BERT architecture that leverages a\nmorphological analyzer and explicitly represents morphological\ncompositionality. Despite the success of BERT, most of its evaluations have\nbeen conducted on high-resource languages, obscuring its applicability on\nlow-resource languages. We evaluate our proposed method on the low-resource\nmorphologically rich Kinyarwanda language, naming the proposed model\narchitecture KinyaBERT. A robust set of experimental results reveal that\nKinyaBERT outperforms solid baselines by 2% in F1 score on a named entity\nrecognition task and by 4.3% in average score of a machine-translated GLUE\nbenchmark. KinyaBERT fine-tuning has better convergence and achieves more\nrobust results on multiple tasks even in the presence of translation noise.",
        "pdf_link": "https://arxiv.org/pdf/2203.08459v2.pdf"
    },
    {
        "title": "Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure",
        "authors": [
            "Yuan Chai",
            "Yaobo Liang",
            "Nan Duan"
        ],
        "published": "2022-03-16T07:09:35Z",
        "summary": "Multilingual pre-trained language models, such as mBERT and XLM-R, have shown\nimpressive cross-lingual ability. Surprisingly, both of them use multilingual\nmasked language model (MLM) without any cross-lingual supervision or aligned\ndata. Despite the encouraging results, we still lack a clear understanding of\nwhy cross-lingual ability could emerge from multilingual MLM. In our work, we\nargue that cross-language ability comes from the commonality between languages.\nSpecifically, we study three language properties: constituent order,\ncomposition and word co-occurrence. First, we create an artificial language by\nmodifying property in source language. Then we study the contribution of\nmodified property through the change of cross-language transfer results on\ntarget language. We conduct experiments on six languages and two cross-lingual\nNLP tasks (textual entailment, sentence retrieval). Our main conclusion is that\nthe contribution of constituent order and word co-occurrence is limited, while\nthe composition is more crucial to the success of cross-linguistic transfer.",
        "pdf_link": "https://arxiv.org/pdf/2203.08430v1.pdf"
    },
    {
        "title": "Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again",
        "authors": [
            "Bernal Jim\u00e9nez Guti\u00e9rrez",
            "Nikolas McNeal",
            "Clay Washington",
            "You Chen",
            "Lang Li",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2022-03-16T05:56:08Z",
        "summary": "The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for application\ndomains such as biomedicine, which feature high and diverse demands of language\ntechnologies but also high data annotation costs. In this paper, we present the\nfirst systematic and comprehensive study to compare the few-shot performance of\nGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on\ntwo highly representative biomedical information extraction tasks, named entity\nrecognition and relation extraction. We follow the true few-shot setting to\navoid overestimating models' few-shot performance by model selection over a\nlarge validation set. We also optimize GPT-3's performance with known\ntechniques such as contextual calibration and dynamic in-context example\nretrieval. However, our results show that GPT-3 still significantly\nunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3\nin-context learning also yields smaller gains in accuracy when more training\ndata becomes available. Our in-depth analyses further reveal issues of the\nin-context learning setting that may be detrimental to information extraction\ntasks in general. Given the high cost of experimenting with GPT-3, we hope our\nstudy provides guidance for biomedical researchers and practitioners towards\nmore promising directions such as fine-tuning small PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2203.08410v3.pdf"
    },
    {
        "title": "Data Contamination: From Memorization to Exploitation",
        "authors": [
            "Inbal Magar",
            "Roy Schwartz"
        ],
        "published": "2022-03-15T20:37:16Z",
        "summary": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "pdf_link": "https://arxiv.org/pdf/2203.08242v1.pdf"
    },
    {
        "title": "Representation Learning for Resource-Constrained Keyphrase Generation",
        "authors": [
            "Di Wu",
            "Wasi Uddin Ahmad",
            "Sunipa Dev",
            "Kai-Wei Chang"
        ],
        "published": "2022-03-15T17:48:04Z",
        "summary": "State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with limited\nannotated data. To overcome this challenge, we design a data-oriented approach\nthat first identifies salient information using retrieval-based corpus-level\nstatistics, and then learns a task-specific intermediate representation based\non a pre-trained language model using large-scale unlabeled documents. We\nintroduce salient span recovery and salient span prediction as denoising\ntraining objectives that condense the intra-article and inter-article knowledge\nessential for keyphrase generation. Through experiments on multiple keyphrase\ngeneration benchmarks, we show the effectiveness of the proposed approach for\nfacilitating low-resource keyphrase generation and zero-shot domain adaptation.\nOur method especially benefits the generation of absent keyphrases, approaching\nthe performance of models trained with large training sets.",
        "pdf_link": "https://arxiv.org/pdf/2203.08118v3.pdf"
    },
    {
        "title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
        "authors": [
            "Nitarshan Rajkumar",
            "Raymond Li",
            "Dzmitry Bahdanau"
        ],
        "published": "2022-03-15T17:23:53Z",
        "summary": "We perform an empirical evaluation of Text-to-SQL capabilities of the Codex\nlanguage model. We find that, without any finetuning, Codex is a strong\nbaseline on the Spider benchmark; we also analyze the failure modes of Codex in\nthis setting. Furthermore, we demonstrate on the GeoQuery and Scholar\nbenchmarks that a small number of in-domain examples provided in the prompt\nenables Codex to perform better than state-of-the-art models finetuned on such\nfew-shot examples.",
        "pdf_link": "https://arxiv.org/pdf/2204.00498v1.pdf"
    },
    {
        "title": "Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns",
        "authors": [
            "Daniel Wiechmann",
            "Yu Qiao",
            "Elma Kerz",
            "Justus Mattern"
        ],
        "published": "2022-03-15T17:13:45Z",
        "summary": "There is a growing interest in the combined use of NLP and machine learning\nmethods to predict gaze patterns during naturalistic reading. While promising\nresults have been obtained through the use of transformer-based language\nmodels, little work has been undertaken to relate the performance of such\nmodels to general text characteristics. In this paper we report on experiments\nwith two eye-tracking corpora of naturalistic reading and two language models\n(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of\nfeatures for predicting human reading behavior that fall into five categories\n(syntactic complexity, lexical richness, register-based multiword combinations,\nreadability and psycholinguistic word properties). Our experiments show that\nboth the features included and the architecture of the transformer-based\nlanguage models play a role in predicting multiple eye-tracking measures during\nnaturalistic reading. We also report the results of experiments aimed at\ndetermining the relative importance of features from different groups using\nSP-LIME.",
        "pdf_link": "https://arxiv.org/pdf/2203.08085v1.pdf"
    },
    {
        "title": "Training a Tokenizer for Free with Private Federated Learning",
        "authors": [
            "Eugene Bagdasaryan",
            "Congzheng Song",
            "Rogier van Dalen",
            "Matt Seigel",
            "\u00c1ine Cahill"
        ],
        "published": "2022-03-15T14:29:39Z",
        "summary": "Federated learning with differential privacy, i.e. private federated learning\n(PFL), makes it possible to train models on private data distributed across\nusers' devices without harming privacy. PFL is efficient for models, such as\nneural networks, that have a fixed number of parameters, and thus a\nfixed-dimensional gradient vector. Such models include neural-net language\nmodels, but not tokenizers, the topic of this work. Training a tokenizer\nrequires frequencies of words from an unlimited vocabulary, and existing\nmethods for finding an unlimited vocabulary need a separate privacy budget.\n  A workaround is to train the tokenizer on publicly available data. However,\nin this paper we first show that a tokenizer trained on mismatched data results\nin worse model performance compared to a privacy-violating \"oracle\" tokenizer\nthat accesses user data, with perplexity increasing by 20%. We also show that\nsub-word tokenizers are better suited to the federated context than word-level\nones, since they can encode new words, though with more tokens per word.\n  Second, we propose a novel method to obtain a tokenizer without using any\nadditional privacy budget. During private federated learning of the language\nmodel, we sample from the model, train a new tokenizer on the sampled\nsequences, and update the model embeddings. We then continue private federated\nlearning, and obtain performance within 1% of the \"oracle\" tokenizer. Since\nthis process trains the tokenizer only indirectly on private data, we can use\nthe \"postprocessing guarantee\" of differential privacy and thus use no\nadditional privacy budget.",
        "pdf_link": "https://arxiv.org/pdf/2203.09943v1.pdf"
    },
    {
        "title": "Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost",
        "authors": [
            "Lihu Chen",
            "Ga\u00ebl Varoquaux",
            "Fabian M. Suchanek"
        ],
        "published": "2022-03-15T13:11:07Z",
        "summary": "State-of-the-art NLP systems represent inputs with word embeddings, but these\nare brittle when faced with Out-of-Vocabulary (OOV) words. To address this\nissue, we follow the principle of mimick-like models to generate vectors for\nunseen words, by learning the behavior of pre-trained embeddings using only the\nsurface form of words. We present a simple contrastive learning framework,\nLOVE, which extends the word representation of an existing pre-trained language\nmodel (such as BERT), and makes it robust to OOV with few additional\nparameters. Extensive evaluations demonstrate that our lightweight model\nachieves similar or even better performances than prior competitors, both on\noriginal datasets and on corrupted variants. Moreover, it can be used in a\nplug-and-play fashion with FastText and BERT, where it significantly improves\ntheir robustness.",
        "pdf_link": "https://arxiv.org/pdf/2203.07860v2.pdf"
    },
    {
        "title": "Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs",
        "authors": [
            "Taichi Iki",
            "Akiko Aizawa"
        ],
        "published": "2022-03-15T12:32:28Z",
        "summary": "Pre-trained Transformers are good foundations for unified multi-task models\nowing to their task-agnostic representation. Pre-trained Transformers are often\ncombined with text-to-text framework to execute multiple tasks by a single\nmodel. Performing a task through a graphical user interface (GUI) is another\ncandidate to accommodate various tasks, including multi-step tasks with vision\nand language inputs. However, few papers combine pre-trained Transformers with\nperforming through GUI. To fill this gap, we explore a framework in which a\nmodel performs a task by manipulating the GUI implemented with web pages in\nmultiple steps. We develop task pages with and without page transitions and\npropose a BERT extension for the framework. We jointly trained our BERT\nextension with those task pages, and made the following observations. (1) The\nmodel learned to use both task pages with and without page transition. (2) In\nfour out of five tasks without page transitions, the model performs greater\nthan 75% of the performance of the original BERT, which does not use browsers.\n(3) The model did not generalize effectively on unseen tasks. These results\nsuggest that we can fine-tune BERTs to multi-step tasks through GUIs, and there\nis room for improvement in their generalizability. Code will be available\nonline.",
        "pdf_link": "https://arxiv.org/pdf/2203.07828v1.pdf"
    },
    {
        "title": "The Ghost in the Machine has an American accent: value conflict in GPT-3",
        "authors": [
            "Rebecca L Johnson",
            "Giada Pistilli",
            "Natalia Men\u00e9dez-Gonz\u00e1lez",
            "Leslye Denisse Dias Duran",
            "Enrico Panai",
            "Julija Kalpokiene",
            "Donald Jay Bertulfo"
        ],
        "published": "2022-03-15T11:06:54Z",
        "summary": "The alignment problem in the context of large language models must consider\nthe plurality of human values in our world. Whilst there are many resonant and\noverlapping values amongst the world's cultures, there are also many\nconflicting, yet equally valid, values. It is important to observe which\ncultural values a model exhibits, particularly when there is a value conflict\nbetween input prompts and generated outputs. We discuss how the co-creation of\nlanguage and cultural value impacts large language models (LLMs). We explore\nthe constitution of the training data for GPT-3 and compare that to the world's\nlanguage and internet access demographics, as well as to reported statistical\nprofiles of dominant values in some Nation-states. We stress tested GPT-3 with\na range of value-rich texts representing several languages and nations;\nincluding some with values orthogonal to dominant US public opinion as reported\nby the World Values Survey. We observed when values embedded in the input text\nwere mutated in the generated outputs and noted when these conflicting values\nwere more aligned with reported dominant US values. Our discussion of these\nresults uses a moral value pluralism (MVP) lens to better understand these\nvalue mutations. Finally, we provide recommendations for how our work may\ncontribute to other current work in the field.",
        "pdf_link": "https://arxiv.org/pdf/2203.07785v1.pdf"
    },
    {
        "title": "UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL",
        "authors": [
            "Longxu Dou",
            "Yan Gao",
            "Mingyang Pan",
            "Dingzirui Wang",
            "Wanxiang Che",
            "Dechen Zhan",
            "Jian-Guang Lou"
        ],
        "published": "2022-03-15T11:02:55Z",
        "summary": "Existing text-to-SQL semantic parsers are typically designed for particular\nsettings such as handling queries that span multiple tables, domains or turns\nwhich makes them ineffective when applied to different settings. We present\nUniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits\nfrom directly using an off-the-shelf language model architecture and\ndemonstrates consistently high performance under different settings.\nSpecifically, UniSAr extends existing autoregressive language models to\nincorporate three non-invasive extensions to make them structure-aware: (1)\nadding structure mark to encode database schema, conversation context, and\ntheir relationships; (2) constrained decoding to decode well structured SQL for\na given database schema; and (3) SQL completion to complete potential missing\nJOIN relationships in SQL based on database schema. On seven well-known\ntext-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr\ndemonstrates highly comparable or better performance to the most advanced\nspecifically-designed text-to-SQL models. Importantly, our UniSAr is\nnon-invasive, such that other core model advances in text-to-SQL can also adopt\nour extensions to further enhance performance.",
        "pdf_link": "https://arxiv.org/pdf/2203.07781v2.pdf"
    },
    {
        "title": "Evaluating BERT-based Pre-training Language Models for Detecting Misinformation",
        "authors": [
            "Rini Anggrainingsih",
            "Ghulam Mubashar Hassan",
            "Amitava Datta"
        ],
        "published": "2022-03-15T08:54:36Z",
        "summary": "It is challenging to control the quality of online information due to the\nlack of supervision over all the information posted online. Manual checking is\nalmost impossible given the vast number of posts made on online media and how\nquickly they spread. Therefore, there is a need for automated rumour detection\ntechniques to limit the adverse effects of spreading misinformation. Previous\nstudies mainly focused on finding and extracting the significant features of\ntext data. However, extracting features is time-consuming and not a highly\neffective process. This study proposes the BERT- based pre-trained language\nmodels to encode text data into vectors and utilise neural network models to\nclassify these vectors to detect misinformation. Furthermore, different\nlanguage models (LM) ' performance with different trainable parameters was\ncompared. The proposed technique is tested on different short and long text\ndatasets. The result of the proposed technique has been compared with the\nstate-of-the-art techniques on the same datasets. The results show that the\nproposed technique performs better than the state-of-the-art techniques. We\nalso tested the proposed technique by combining the datasets. The results\ndemonstrated that the large data training and testing size considerably\nimproves the technique's performance.",
        "pdf_link": "https://arxiv.org/pdf/2203.07731v1.pdf"
    },
    {
        "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
        "authors": [
            "Shuai Lu",
            "Nan Duan",
            "Hojae Han",
            "Daya Guo",
            "Seung-won Hwang",
            "Alexey Svyatkovskiy"
        ],
        "published": "2022-03-15T08:25:08Z",
        "summary": "Code completion, which aims to predict the following code token(s) according\nto the code context, can improve the productivity of software development.\nRecent work has proved that statistical language modeling with transformers can\ngreatly improve the performance in the code completion task via learning from\nlarge-scale source code datasets. However, current approaches focus only on\ncode context within the file or project, i.e. internal context. Our distinction\nis utilizing \"external\" context, inspired by human behaviors of copying from\nthe related code snippets when writing code. Specifically, we propose a\nretrieval-augmented code completion framework, leveraging both lexical copying\nand referring to code with similar semantics by retrieval. We adopt a\nstage-wise training approach that combines a source code retriever and an\nauto-regressive language model for programming language. We evaluate our\napproach in the code completion task in Python and Java programming languages,\nachieving a state-of-the-art performance on CodeXGLUE benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2203.07722v1.pdf"
    },
    {
        "title": "Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation",
        "authors": [
            "Xuandong Zhao",
            "Zhiguo Yu",
            "Ming Wu",
            "Lei Li"
        ],
        "published": "2022-03-15T07:05:43Z",
        "summary": "How to learn highly compact yet effective sentence representation?\nPre-trained language models have been effective in many NLP tasks. However,\nthese models are often huge and produce large sentence embeddings. Moreover,\nthere is a big performance gap between large and small models. In this paper,\nwe propose Homomorphic Projective Distillation (HPD) to learn compressed\nsentence embeddings. Our method augments a small Transformer encoder model with\nlearnable projection layers to produce compact representations while mimicking\na large pre-trained language model to retain the sentence representation\nquality. We evaluate our method with different model sizes on both semantic\ntextual similarity (STS) and semantic retrieval (SR) tasks. Experiments show\nthat our method achieves 2.7-4.5 points performance gain on STS tasks compared\nwith previous best representations of the same size. In SR tasks, our method\nimproves retrieval speed (8.2$\\times$) and memory usage (8.0$\\times$) compared\nwith state-of-the-art large models.",
        "pdf_link": "https://arxiv.org/pdf/2203.07687v1.pdf"
    },
    {
        "title": "Do Language Models Plagiarize?",
        "authors": [
            "Jooyoung Lee",
            "Thai Le",
            "Jinghui Chen",
            "Dongwon Lee"
        ],
        "published": "2022-03-15T03:11:11Z",
        "summary": "Past literature has illustrated that language models (LMs) often memorize\nparts of training instances and reproduce them in natural language generation\n(NLG) processes. However, it is unclear to what extent LMs \"reuse\" a training\ncorpus. For instance, models can generate paraphrased sentences that are\ncontextually similar to training samples. In this work, therefore, we study\nthree types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2\ngenerated texts, in comparison to its training data, and further analyze the\nplagiarism patterns of fine-tuned LMs with domain-specific corpora which are\nextensively used in practice. Our results suggest that (1) three types of\nplagiarism widely exist in LMs beyond memorization, (2) both size and decoding\nmethods of LMs are strongly associated with the degrees of plagiarism they\nexhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus\nsimilarity and homogeneity. Given that a majority of LMs' training data is\nscraped from the Web without informing content owners, their reiteration of\nwords, phrases, and even core ideas from training sets into generated texts has\nethical implications. Their patterns are likely to exacerbate as both the size\nof LMs and their training data increase, raising concerns about\nindiscriminately pursuing larger models with larger training corpora.\nPlagiarized content can also contain individuals' personal and sensitive\ninformation. These findings overall cast doubt on the practicality of current\nLMs in mission-critical writing tasks and urge more discussions around the\nobserved phenomena. Data and source code are available at\nhttps://github.com/Brit7777/LM-plagiarism.",
        "pdf_link": "https://arxiv.org/pdf/2203.07618v2.pdf"
    },
    {
        "title": "Long Document Summarization with Top-down and Bottom-up Inference",
        "authors": [
            "Bo Pang",
            "Erik Nijkamp",
            "Wojciech Kry\u015bci\u0144ski",
            "Silvio Savarese",
            "Yingbo Zhou",
            "Caiming Xiong"
        ],
        "published": "2022-03-15T01:24:51Z",
        "summary": "Text summarization aims to condense long documents and retain key\ninformation. Critical to the success of a summarization model is the faithful\ninference of latent representations of words or tokens in the source documents.\nMost recent models infer the latent representations with a transformer encoder,\nwhich is purely bottom-up. Also, self-attention-based inference models face the\nchallenge of quadratic complexity with respect to sequence length. We propose a\nprincipled inference framework to improve summarization models on these two\naspects. Our framework assumes a hierarchical latent structure of a document\nwhere the top-level captures the long range dependency at a coarser time scale\nand the bottom token level preserves the details. Critically, this hierarchical\nstructure enables token representations to be updated in both a bottom-up and\ntop-down manner. In the bottom-up pass, token representations are inferred with\nlocal self-attention to leverage its efficiency. Top-down correction is then\napplied to allow tokens to capture long-range dependency. We demonstrate the\neffectiveness of the proposed framework on a diverse set of summarization\ndatasets, including narrative, conversational, scientific documents and news.\nOur model achieves (1) competitive or better performance on short documents\nwith higher memory and compute efficiency, compared to full attention\ntransformers, and (2) state-of-the-art performance on a wide range of long\ndocument summarization benchmarks, compared to recent efficient transformers.\nWe also show that our model can summarize an entire book and achieve\ncompetitive performance using $0.27\\%$ parameters (464M vs. 175B) and much less\ntraining data, compared to a recent GPT-3-based model. These results indicate\nthe general applicability and benefits of the proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2203.07586v1.pdf"
    },
    {
        "title": "Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering",
        "authors": [
            "Man Luo",
            "Kazuma Hashimoto",
            "Semih Yavuz",
            "Zhiwei Liu",
            "Chitta Baral",
            "Yingbo Zhou"
        ],
        "published": "2022-03-14T22:07:52Z",
        "summary": "While both extractive and generative readers have been successfully applied\nto the Question Answering (QA) task, little attention has been paid toward the\nsystematic comparison of them. Characterizing the strengths and weaknesses of\nthe two readers is crucial not only for making a more informed reader selection\nin practice but also for developing a deeper understanding to foster further\nresearch on improving readers in a principled manner. Motivated by this goal,\nwe make the first attempt to systematically study the comparison of extractive\nand generative readers for question answering. To be aligned with the\nstate-of-the-art, we explore nine transformer-based large pre-trained language\nmodels (PrLMs) as backbone architectures. Furthermore, we organize our findings\nunder two main categories: (1) keeping the architecture invariant, and (2)\nvarying the underlying PrLMs. Among several interesting findings, it is\nimportant to highlight that (1) the generative readers perform better in long\ncontext QA, (2) the extractive readers perform better in short context while\nalso showing better out-of-domain generalization, and (3) the encoder of\nencoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and\noutperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also\nstudy the effect of multi-task learning on the two types of readers varying the\nunderlying PrLMs and perform qualitative and quantitative diagnosis to provide\nfurther insights into future directions in modeling better readers.",
        "pdf_link": "https://arxiv.org/pdf/2203.07522v1.pdf"
    },
    {
        "title": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer",
        "authors": [
            "Woojeong Jin",
            "Dong-Ho Lee",
            "Chenguang Zhu",
            "Jay Pujara",
            "Xiang Ren"
        ],
        "published": "2022-03-14T22:02:40Z",
        "summary": "Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.",
        "pdf_link": "https://arxiv.org/pdf/2203.07519v2.pdf"
    },
    {
        "title": "Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations",
        "authors": [
            "Robert Wolfe",
            "Aylin Caliskan"
        ],
        "published": "2022-03-14T21:42:13Z",
        "summary": "We examine the effects of contrastive visual semantic pretraining by\ncomparing the geometry and semantic properties of contextualized English\nlanguage representations formed by GPT-2 and CLIP, a zero-shot multimodal image\nclassifier which adapts the GPT-2 architecture to encode image captions. We\nfind that contrastive visual semantic pretraining significantly mitigates the\nanisotropy found in contextualized word embeddings from GPT-2, such that the\nintra-layer self-similarity (mean pairwise cosine similarity) of CLIP word\nembeddings is under .25 in all layers, compared to greater than .95 in the top\nlayer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic\nintrinsic evaluation tasks, and achieve a new corpus-based state of the art for\nthe RG65 evaluation, at .88. CLIP also forms fine-grained semantic\nrepresentations of sentences, and obtains Spearman's rho = .73 on the\nSemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning,\ncompared to no greater than rho = .45 in any layer of GPT-2. Finally,\nintra-layer self-similarity of CLIP sentence embeddings decreases as the layer\nindex increases, finishing at .25 in the top layer, while the self-similarity\nof GPT-2 sentence embeddings formed using the EOS token increases\nlayer-over-layer and never falls below .97. Our results indicate that high\nanisotropy is not an inevitable consequence of contextualization, and that\nvisual semantic pretraining is beneficial not only for ordering visual\nrepresentations, but also for encoding useful semantic representations of\nlanguage, both on the word level and the sentence level.",
        "pdf_link": "https://arxiv.org/pdf/2203.07511v1.pdf"
    },
    {
        "title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models",
        "authors": [
            "Robert Wolfe",
            "Aylin Caliskan"
        ],
        "published": "2022-03-14T21:29:38Z",
        "summary": "VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation\ntask for contextualized word embeddings (CWEs). VAST uses valence, the\nassociation of a word with pleasantness, to measure the correspondence of\nword-level LM semantics with widely used human judgments, and examines the\neffects of contextualization, tokenization, and LM-specific geometry. Because\nprior research has found that CWEs from GPT-2 perform poorly on other intrinsic\nevaluations, we select GPT-2 as our primary subject, and include results\nshowing that VAST is useful for 7 other LMs, and can be used in 7 languages.\nGPT-2 results show that the semantics of a word incorporate the semantics of\ncontext in layers closer to model output, such that VAST scores diverge between\nour contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.\nWe also show that multiply tokenized words are not semantically encoded until\nlayer 8, where they achieve Pearson's rho of .46, indicating the presence of an\nencoding process for multiply tokenized words which differs from that of singly\ntokenized words, for which rho is highest in layer 0. We find that a few\nneurons with values having greater magnitude than the rest mask word-level\nsemantics in GPT-2's top layer, but that word-level semantics can be recovered\nby nullifying non-semantic principal components: Pearson's rho in the top layer\nimproves from .32 to .76. After isolating semantics, we show the utility of\nVAST for understanding LM semantics via improvements over related work on four\nword similarity tasks, with a score of .50 on SimLex-999, better than the\nprevious best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,\nwhich compare differences in word embedding associations between groups of\nwords, exhibit more stereotype-congruent biases after isolating semantics,\nindicating that non-semantic structures in LMs also mask biases.",
        "pdf_link": "https://arxiv.org/pdf/2203.07504v1.pdf"
    },
    {
        "title": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection",
        "authors": [
            "Jens Lemmens",
            "Jens Van Nooten",
            "Tim Kreutz",
            "Walter Daelemans"
        ],
        "published": "2022-03-14T17:55:32Z",
        "summary": "We present CoNTACT: a Dutch language model adapted to the domain of COVID-19\ntweets. The model was developed by continuing the pre-training phase of RobBERT\n(Delobelle, 2020) by using 2.8M Dutch COVID-19 related tweets posted in 2021.\nIn order to test the performance of the model and compare it to RobBERT, the\ntwo models were tested on two tasks: (1) binary vaccine hesitancy detection and\n(2) detection of arguments for vaccine hesitancy. For both tasks, not only\nTwitter but also Facebook data was used to show cross-genre performance. In our\nexperiments, CoNTACT showed statistically significant gains over RobBERT in all\nexperiments for task 1. For task 2, we observed substantial improvements in\nvirtually all classes in all experiments. An error analysis indicated that the\ndomain adaptation yielded better representations of domain-specific\nterminology, causing CoNTACT to make more accurate classification decisions.",
        "pdf_link": "https://arxiv.org/pdf/2203.07362v1.pdf"
    },
    {
        "title": "All in One: Exploring Unified Video-Language Pre-training",
        "authors": [
            "Alex Jinpeng Wang",
            "Yixiao Ge",
            "Rui Yan",
            "Yuying Ge",
            "Xudong Lin",
            "Guanyu Cai",
            "Jianping Wu",
            "Ying Shan",
            "Xiaohu Qie",
            "Mike Zheng Shou"
        ],
        "published": "2022-03-14T17:06:30Z",
        "summary": "Mainstream Video-Language Pre-training models \\cite{actbert,clipbert,violet}\nconsist of three parts, a video encoder, a text encoder, and a video-text\nfusion Transformer. They pursue better performance via utilizing heavier\nunimodal encoders or multimodal fusion Transformers, resulting in increased\nparameters with lower efficiency in downstream tasks. In this work, we for the\nfirst time introduce an end-to-end video-language model, namely\n\\textit{all-in-one Transformer}, that embeds raw video and textual signals into\njoint representations using a unified backbone architecture. We argue that the\nunique temporal information of video data turns out to be a key barrier\nhindering the design of a modality-agnostic Transformer. To overcome the\nchallenge, we introduce a novel and effective token rolling operation to encode\ntemporal representations from video clips in a non-parametric manner. The\ncareful design enables the representation learning of both video-text\nmultimodal inputs and unimodal inputs using a unified backbone model. Our\npre-trained all-in-one Transformer is transferred to various downstream\nvideo-text tasks after fine-tuning, including text-video retrieval,\nvideo-question answering, multiple choice and visual commonsense reasoning.\nState-of-the-art performances with the minimal model FLOPs on nine datasets\ndemonstrate the superiority of our method compared to the competitive\ncounterparts. The code and pretrained model have been released in\nhttps://github.com/showlab/all-in-one.",
        "pdf_link": "https://arxiv.org/pdf/2203.07303v1.pdf"
    },
    {
        "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
        "authors": [
            "Archiki Prasad",
            "Peter Hase",
            "Xiang Zhou",
            "Mohit Bansal"
        ],
        "published": "2022-03-14T16:54:46Z",
        "summary": "Providing natural language instructions in prompts is a useful new paradigm\nfor improving task performance of large language models in a zero-shot setting.\nRecent work has aimed to improve such prompts via manual rewriting or\ngradient-based tuning. However, manual rewriting is time-consuming and requires\nsubjective interpretation, while gradient-based tuning can be extremely\ncomputationally demanding for large models and may not be feasible for\nAPI-based models. In this work, we introduce Gradient-free Instructional Prompt\nSearch (GrIPS), a gradient-free, edit-based search approach for improving task\ninstructions for large language models. GrIPS takes in instructions designed\nfor humans and automatically returns an improved, edited prompt, while allowing\nfor API-based tuning. With InstructGPT models, GrIPS improves the average task\nperformance by up to 4.30 percentage points on eight classification tasks from\nthe Natural Instructions dataset (with similar improvements for OPT, BLOOM, and\nFLAN-T5). We see improvements for both instruction-only prompts and instruction\n+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and\npurely example-based prompts while controlling for the available compute and\ndata budget. Further, performance of GrIPS is comparable to select\ngradient-based tuning approaches. Qualitatively, we show our edits can simplify\ninstructions and at times make them incoherent but nonetheless improve\naccuracy. Our code is available at: https://github.com/archiki/GrIPS",
        "pdf_link": "https://arxiv.org/pdf/2203.07281v2.pdf"
    },
    {
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
        "authors": [
            "Eldar Kurtic",
            "Daniel Campos",
            "Tuan Nguyen",
            "Elias Frantar",
            "Mark Kurtz",
            "Benjamin Fineran",
            "Michael Goin",
            "Dan Alistarh"
        ],
        "published": "2022-03-14T16:40:31Z",
        "summary": "Transformer-based language models have become a key building block for\nnatural language processing. While these models are extremely accurate, they\ncan be too large and computationally intensive to run on standard deployments.\nA variety of compression methods, including distillation, quantization,\nstructured and unstructured pruning are known to decrease model size and\nincrease inference speed, with low accuracy loss. In this context, this paper's\ncontributions are two-fold. We perform an in-depth study of the\naccuracy-compression trade-off for unstructured weight pruning of BERT models.\nWe introduce Optimal BERT Surgeon (oBERT), an efficient and accurate weight\npruning method based on approximate second-order information, which we show to\nyield state-of-the-art results in both stages of language tasks: pre-training\nand fine-tuning. Specifically, oBERT extends existing work on unstructured\nsecond-order pruning by allowing for pruning blocks of weights, and by being\napplicable at the BERT scale. Second, we investigate the impact of this pruning\nmethod when compounding compression approaches to obtain highly compressed but\naccurate models for deployment on edge devices. These models significantly push\nboundaries of the current state-of-the-art sparse BERT models with respect to\nall metrics: model size, inference speed and task accuracy. For example,\nrelative to the dense BERT-base, we obtain 10x model size compression (in MB)\nwith < 1% accuracy drop, 10x CPU-inference speedup with < 2% accuracy drop, and\n29x CPU-inference speedup with < 7.5% accuracy drop. Our code, fully integrated\nwith Transformers and SparseML, is available at\nhttps://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.",
        "pdf_link": "https://arxiv.org/pdf/2203.07259v3.pdf"
    },
    {
        "title": "WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition",
        "authors": [
            "Renjie Zhou",
            "Qiang Hu",
            "Jian Wan",
            "Jilin Zhang",
            "Qiang Liu",
            "Tianxiang Hu",
            "Jianjun Li"
        ],
        "published": "2022-03-14T08:29:58Z",
        "summary": "Named Entity Recognition task is one of the core tasks of information\nextraction. Word ambiguity and word abbreviation are important reasons for the\nlow recognition rate of named entities. In this paper, we propose a novel named\nentity recognition model WCL-BBCD (Word Contrastive Learning with\nBERT-BiLSTM-CRF-DBpedia), which incorporates the idea of contrastive learning.\nThe model first trains the sentence pairs in the text, calculate similarity\nbetween sentence pairs, and fine-tunes BERT used for the named entity\nrecognition task according to the similarity, so as to alleviate word\nambiguity. Then, the fine-tuned BERT is combined with BiLSTM-CRF to perform the\nnamed entity recognition task. Finally, the recognition results are corrected\nin combination with prior knowledge such as knowledge graphs, so as to\nalleviate the low-recognition-rate problem caused by word abbreviations. The\nresults of experimentals conducted on the CoNLL-2003 English dataset and\nOntoNotes V5 English dataset show that our model outperforms other similar\nmodels on.",
        "pdf_link": "https://arxiv.org/pdf/2203.06925v5.pdf"
    },
    {
        "title": "PERT: Pre-training BERT with Permuted Language Model",
        "authors": [
            "Yiming Cui",
            "Ziqing Yang",
            "Ting Liu"
        ],
        "published": "2022-03-14T07:58:34Z",
        "summary": "Pre-trained Language Models (PLMs) have been widely used in various natural\nlanguage processing (NLP) tasks, owing to their powerful text representations\ntrained on large-scale corpora. In this paper, we propose a new PLM called PERT\nfor natural language understanding (NLU). PERT is an auto-encoding model (like\nBERT) trained with Permuted Language Model (PerLM). The formulation of the\nproposed PerLM is straightforward. We permute a proportion of the input text,\nand the training objective is to predict the position of the original token.\nMoreover, we also apply whole word masking and N-gram masking to improve the\nperformance of PERT. We carried out extensive experiments on both Chinese and\nEnglish NLU benchmarks. The experimental results show that PERT can bring\nimprovements over various comparable baselines on some of the tasks, while\nothers are not. These results indicate that developing more diverse\npre-training tasks is possible instead of masked language model variants.\nSeveral quantitative studies are carried out to better understand PERT, which\nmight help design PLMs in the future. Resources are available:\nhttps://github.com/ymcui/PERT",
        "pdf_link": "https://arxiv.org/pdf/2203.06906v1.pdf"
    },
    {
        "title": "Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study",
        "authors": [
            "Roelien C. Timmer",
            "David Liebowitz",
            "Surya Nepal",
            "Salil S. Kanhere"
        ],
        "published": "2022-03-14T00:17:34Z",
        "summary": "Each and every organisation releases information in a variety of forms\nranging from annual reports to legal proceedings. Such documents may contain\nsensitive information and releasing them openly may lead to the leakage of\nconfidential information. Detection of sentences that contain sensitive\ninformation in documents can help organisations prevent the leakage of valuable\nconfidential information. This is especially challenging when such sentences\ncontain a substantial amount of information or are paraphrased versions of\nknown sensitive content. Current approaches to sensitive information detection\nin such complex settings are based on keyword-based approaches or standard\nmachine learning models. In this paper, we wish to explore whether pre-trained\ntransformer models are well suited to detect complex sensitive information.\nPre-trained transformers are typically trained on an enormous amount of text\nand therefore readily learn grammar, structure and other linguistic features,\nmaking them particularly attractive for this task. Through our experiments on\nthe Monsanto trial data set, we observe that the fine-tuned Bidirectional\nEncoder Representations from Transformers (BERT) transformer model performs\nbetter than traditional models. We experimented with four different categories\nof documents in the Monsanto dataset and observed that BERT achieves better F2\nscores by 24.13\\% to 65.79\\% for GHOST, 30.14\\% to 54.88\\% for TOXIC, 39.22\\%\nfor CHEMI, 53.57\\% for REGUL compared to existing sensitive information\ndetection models.",
        "pdf_link": "https://arxiv.org/pdf/2203.06793v1.pdf"
    },
    {
        "title": "Pruned Graph Neural Network for Short Story Ordering",
        "authors": [
            "Melika Golestani",
            "Zeinab Borhanifard",
            "Farnaz Tahmasebian",
            "Heshaam Faili"
        ],
        "published": "2022-03-13T22:25:17Z",
        "summary": "Text coherence is a fundamental problem in natural language generation and\nunderstanding. Organizing sentences into an order that maximizes coherence is\nknown as sentence ordering. This paper is proposing a new approach based on the\ngraph neural network approach to encode a set of sentences and learn orderings\nof short stories. We propose a new method for constructing sentence-entity\ngraphs of short stories to create the edges between sentences and reduce noise\nin our graph by replacing the pronouns with their referring entities. We\nimprove the sentence ordering by introducing an aggregation method based on\nmajority voting of state-of-the-art methods and our proposed one. Our approach\nemploys a BERT-based model to learn semantic representations of the sentences.\nThe results demonstrate that the proposed method significantly outperforms\nexisting baselines on a corpus of short stories with a new state-of-the-art\nperformance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau)\nmetrics. More precisely, our method increases PMR and Tau criteria by more than\n5% and 4.3%, respectively. These outcomes highlight the benefit of forming the\nedges between sentences based on their cosine similarity. We also observe that\nreplacing pronouns with their referring entities effectively encodes sentences\nin sentence-entity graphs.",
        "pdf_link": "https://arxiv.org/pdf/2203.06778v1.pdf"
    },
    {
        "title": "Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video",
        "authors": [
            "Bin Li",
            "Yixuan Weng",
            "Bin Sun",
            "Shutao Li"
        ],
        "published": "2022-03-13T14:42:53Z",
        "summary": "The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.",
        "pdf_link": "https://arxiv.org/pdf/2203.06667v6.pdf"
    },
    {
        "title": "FiNER: Financial Numeric Entity Recognition for XBRL Tagging",
        "authors": [
            "Lefteris Loukas",
            "Manos Fergadiotis",
            "Ilias Chalkidis",
            "Eirini Spyropoulou",
            "Prodromos Malakasiotis",
            "Ion Androutsopoulos",
            "Georgios Paliouras"
        ],
        "published": "2022-03-12T16:43:57Z",
        "summary": "Publicly traded companies are required to submit periodic reports with\neXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging\nthe reports is tedious and costly. We, therefore, introduce XBRL tagging as a\nnew entity extraction task for the financial domain and release FiNER-139, a\ndataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction\ndatasets, FiNER-139 uses a much larger label set of 139 entity types. Most\nannotated tokens are numeric, with the correct tag per token depending mostly\non context, rather than the token itself. We show that subword fragmentation of\nnumeric expressions harms BERT's performance, allowing word-level BILSTMs to\nperform better. To improve BERT's performance, we propose two simple and\neffective solutions that replace numeric expressions with pseudo-tokens\nreflecting original token shapes and numeric magnitudes. We also experiment\nwith FIN-BERT, an existing BERT model for the financial domain, and release our\nown BERT (SEC-BERT), pre-trained on financial filings, which performs best.\nThrough data and error analysis, we finally identify possible limitations to\ninspire future work on XBRL tagging.",
        "pdf_link": "https://arxiv.org/pdf/2203.06482v2.pdf"
    },
    {
        "title": "Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice",
        "authors": [
            "Andreas Grivas",
            "Nikolay Bogoychev",
            "Adam Lopez"
        ],
        "published": "2022-03-12T15:34:54Z",
        "summary": "Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.",
        "pdf_link": "https://arxiv.org/pdf/2203.06462v2.pdf"
    },
    {
        "title": "BiBERT: Accurate Fully Binarized BERT",
        "authors": [
            "Haotong Qin",
            "Yifu Ding",
            "Mingyuan Zhang",
            "Qinghua Yan",
            "Aishan Liu",
            "Qingqing Dang",
            "Ziwei Liu",
            "Xianglong Liu"
        ],
        "published": "2022-03-12T09:46:13Z",
        "summary": "The large pre-trained BERT has achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks but is also computation and memory expensive.\nAs one of the powerful compression approaches, binarization extremely reduces\nthe computation and memory consumption by utilizing 1-bit parameters and\nbitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit\nweight, embedding, and activation) usually suffer a significant performance\ndrop, and there is rare study addressing this problem. In this paper, with the\ntheoretical justification and empirical analysis, we identify that the severe\nperformance drop can be mainly attributed to the information degradation and\noptimization direction mismatch respectively in the forward and backward\npropagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate\nthe performance bottlenecks. Specifically, BiBERT introduces an efficient\nBi-Attention structure for maximizing representation information statistically\nand a Direction-Matching Distillation (DMD) scheme to optimize the full\nbinarized BERT accurately. Extensive experiments show that BiBERT outperforms\nboth the straightforward baseline and existing state-of-the-art quantized BERTs\nwith ultra-low bit activations by convincing margins on the NLP benchmark. As\nthe first fully binarized BERT, our method yields impressive 56.3 times and\n31.2 times saving on FLOPs and model size, demonstrating the vast advantages\nand potential of the fully binarized BERT model in real-world\nresource-constrained scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2203.06390v1.pdf"
    },
    {
        "title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation",
        "authors": [
            "Wenliang Dai",
            "Lu Hou",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Pascale Fung"
        ],
        "published": "2022-03-12T09:33:37Z",
        "summary": "The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.06386v2.pdf"
    },
    {
        "title": "MarkBERT: Marking Word Boundaries Improves Chinese BERT",
        "authors": [
            "Linyang Li",
            "Yong Dai",
            "Duyu Tang",
            "Xipeng Qiu",
            "Zenglin Xu",
            "Shuming Shi"
        ],
        "published": "2022-03-12T08:43:06Z",
        "summary": "We present a Chinese BERT model dubbed MarkBERT that uses word information in\nthis work. Existing word-based BERT models regard words as basic units,\nhowever, due to the vocabulary limit of BERT, they only cover high-frequency\nwords and fall back to character level when encountering out-of-vocabulary\n(OOV) words. Different from existing works, MarkBERT keeps the vocabulary being\nChinese characters and inserts boundary markers between contiguous words. Such\ndesign enables the model to handle any words in the same way, no matter they\nare OOV words or not. Besides, our model has two additional benefits: first, it\nis convenient to add word-level learning objectives over markers, which is\ncomplementary to traditional character and sentence-level pretraining tasks;\nsecond, it can easily incorporate richer semantics such as POS tags of words by\nreplacing generic markers with POS tag-specific markers. With the simple\nmarkers insertion, MarkBERT can improve the performances of various downstream\ntasks including language understanding and sequence labeling. \\footnote{All the\ncodes and models will be made publicly available at\n\\url{https://github.com/daiyongya/markbert}}",
        "pdf_link": "https://arxiv.org/pdf/2203.06378v2.pdf"
    },
    {
        "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data",
        "authors": [
            "Yujia Qin",
            "Jiajie Zhang",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Peng Li",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2022-03-12T01:53:53Z",
        "summary": "Current pre-trained language models (PLM) are typically trained with static\ndata, ignoring that in real-world scenarios, streaming data of various sources\nmay continuously grow. This requires PLMs to integrate the information from all\nthe sources in a lifelong manner. Although this goal could be achieved by\nexhaustive pre-training on all the existing data, such a process is known to be\ncomputationally expensive. To this end, we propose ELLE, aiming at efficient\nlifelong pre-training for emerging data. Specifically, ELLE consists of (1)\nfunction preserved model expansion, which flexibly expands an existing PLM's\nwidth and depth to improve the efficiency of knowledge acquisition; and (2)\npre-trained domain prompts, which disentangle the versatile knowledge learned\nduring pre-training and stimulate the proper knowledge for downstream tasks. We\nexperiment ELLE with streaming data from 5 domains on BERT and GPT. The results\nshow the superiority of ELLE over various lifelong learning baselines in both\npre-training efficiency and downstream performances. The codes are publicly\navailable at https://github.com/thunlp/ELLE.",
        "pdf_link": "https://arxiv.org/pdf/2203.06311v2.pdf"
    },
    {
        "title": "verBERT: Automating Brazilian Case Law Document Multi-label Categorization Using BERT",
        "authors": [
            "Felipe R. Serras",
            "Marcelo Finger"
        ],
        "published": "2022-03-11T20:01:20Z",
        "summary": "In this work, we carried out a study about the use of attention-based\nalgorithms to automate the categorization of Brazilian case law documents. We\nused data from the Kollemata Project to produce two distinct datasets with\nadequate class systems. Then, we implemented a multi-class and multi-label\nversion of BERT and fine-tuned different BERT models with the produced\ndatasets. We evaluated several metrics, adopting the micro-averaged F1-Score as\nour main metric for which we obtained a performance value of F1-micro=0.72\ncorresponding to gains of 30 percent points over the tested statistical\nbaseline. In this work, we carried out a study about the use of attention-based\nalgorithms to automate the categorization of Brazilian case law documents. We\nused data from the \\textit{Kollemata} Project to produce two distinct datasets\nwith adequate class systems. Then, we implemented a multi-class and multi-label\nversion of BERT and fine-tuned different BERT models with the produced\ndatasets. We evaluated several metrics, adopting the micro-averaged F1-Score as\nour main metric for which we obtained a performance value of $\\langle\n\\mathcal{F}_1 \\rangle_{micro}=0.72$ corresponding to gains of 30 percent points\nover the tested statistical baseline.",
        "pdf_link": "https://arxiv.org/pdf/2203.06224v1.pdf"
    },
    {
        "title": "When classifying grammatical role, BERT doesn't care about word order... except when it matters",
        "authors": [
            "Isabel Papadimitriou",
            "Richard Futrell",
            "Kyle Mahowald"
        ],
        "published": "2022-03-11T19:00:15Z",
        "summary": "Because meaning can often be inferred from lexical semantics alone, word\norder is often a redundant cue in natural language. For example, the words\nchopped, chef, and onion are more likely used to convey \"The chef chopped the\nonion,\" not \"The onion chopped the chef.\" Recent work has shown large language\nmodels to be surprisingly word order invariant, but crucially has largely\nconsidered natural prototypical inputs, where compositional meaning mostly\nmatches lexical expectations. To overcome this confound, we probe grammatical\nrole representation in English BERT and GPT-2, on instances where lexical\nexpectations are not sufficient, and word order knowledge is necessary for\ncorrect classification. Such non-prototypical instances are naturally occurring\nEnglish sentences with inanimate subjects or animate objects, or sentences\nwhere we systematically swap the arguments to make sentences like \"The onion\nchopped the chef\". We find that, while early layer embeddings are largely\nlexical, word order is in fact crucial in defining the later-layer\nrepresentations of words in semantically non-prototypical positions. Our\nexperiments isolate the effect of word order on the contextualization process,\nand highlight how models use context in the uncommon, but critical, instances\nwhere it matters.",
        "pdf_link": "https://arxiv.org/pdf/2203.06204v1.pdf"
    },
    {
        "title": "Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers",
        "authors": [
            "Sahar Sadrizadeh",
            "Ljiljana Dolamic",
            "Pascal Frossard"
        ],
        "published": "2022-03-11T14:37:41Z",
        "summary": "Recently, it has been shown that, in spite of the significant performance of\ndeep neural networks in different fields, those are vulnerable to adversarial\nexamples. In this paper, we propose a gradient-based adversarial attack against\ntransformer-based text classifiers. The adversarial perturbation in our method\nis imposed to be block-sparse so that the resultant adversarial example differs\nfrom the original sentence in only a few words. Due to the discrete nature of\ntextual data, we perform gradient projection to find the minimizer of our\nproposed optimization problem. Experimental results demonstrate that, while our\nadversarial attack maintains the semantics of the sentence, it can reduce the\naccuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and\nYelp Reviews). Furthermore, the block-sparsity constraint of the proposed\noptimization problem results in small perturbations in the adversarial example.",
        "pdf_link": "https://arxiv.org/pdf/2203.05948v1.pdf"
    },
    {
        "title": "Are discrete units necessary for Spoken Language Modeling?",
        "authors": [
            "Tu Anh Nguyen",
            "Benoit Sagot",
            "Emmanuel Dupoux"
        ],
        "published": "2022-03-11T14:14:35Z",
        "summary": "Recent work in spoken language modeling shows the possibility of learning a\nlanguage unsupervisedly from raw audio without any text labels. The approach\nrelies first on transforming the audio into a sequence of discrete units (or\npseudo-text) and then training a language model directly on such pseudo-text.\nIs such a discrete bottleneck necessary, potentially introducing irreversible\nerrors in the encoding of the speech signal, or could we learn a language model\nwithout discrete units at all? In this work, we study the role of discrete\nversus continuous representations in spoken language modeling. We show that\ndiscretization is indeed essential for good results in spoken language\nmodeling. We show that discretization removes linguistically irrelevant\ninformation from the continuous features, helping to improve language modeling\nperformances. On the basis of this study, we train a language model on the\ndiscrete units of the HuBERT features, reaching new state-of-the-art results in\nthe lexical, syntactic and semantic metrics of the Zero Resource Speech\nChallenge 2021 (Track 1 - Speech Only).",
        "pdf_link": "https://arxiv.org/pdf/2203.05936v2.pdf"
    },
    {
        "title": "A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings",
        "authors": [
            "Haochen Tan",
            "Wei Shao",
            "Han Wu",
            "Ke Yang",
            "Linqi Song"
        ],
        "published": "2022-03-11T12:29:22Z",
        "summary": "Contrastive learning has shown great potential in unsupervised sentence\nembedding tasks, e.g., SimCSE. However, We find that these existing solutions\nare heavily affected by superficial features like the length of sentences or\nsyntactic structures. In this paper, we propose a semantics-aware contrastive\nlearning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT),\nwhich is able to exploit the pseudo-token space (i.e., latent semantic space)\nrepresentation of a sentence while eliminating the impact of superficial\nfeatures such as sentence length and syntax. Specifically, we introduce an\nadditional pseudo token embedding layer independent of the BERT encoder to map\neach sentence into a sequence of pseudo tokens in a fixed length. Leveraging\nthese pseudo sequences, we are able to construct same-length positive and\nnegative pairs based on the attention mechanism to perform contrastive\nlearning. In addition, we utilize both the gradient-updating and\nmomentum-updating encoders to encode instances while dynamically maintaining an\nadditional queue to store the representation of sentence embeddings, enhancing\nthe encoder's learning performance for negative examples. Experiments show that\nour model outperforms the state-of-the-art baselines on six standard semantic\ntextual similarity (STS) tasks. Furthermore, experiments on alignments and\nuniformity losses, as well as hard examples with different sentence lengths and\nsyntax, consistently verify the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2203.05877v1.pdf"
    },
    {
        "title": "DeepTrust: A Reliable Financial Knowledge Retrieval Framework For Explaining Extreme Pricing Anomalies",
        "authors": [
            "Pok Wah Chan"
        ],
        "published": "2022-03-11T06:29:22Z",
        "summary": "Extreme pricing anomalies may occur unexpectedly without a trivial cause, and\nequity traders typically experience a meticulous process to source disparate\ninformation and analyze its reliability before integrating it into the trusted\nknowledge base. We introduce DeepTrust, a reliable financial knowledge\nretrieval framework on Twitter to explain extreme price moves at speed, while\nensuring data veracity using state-of-the-art NLP techniques. Our proposed\nframework consists of three modules, specialized for anomaly detection,\ninformation retrieval and reliability assessment. The workflow starts with\nidentifying anomalous asset price changes using machine learning models trained\nwith historical pricing data, and retrieving correlated unstructured data from\nTwitter using enhanced queries with dynamic search conditions. DeepTrust\nextrapolates information reliability from tweet features, traces of generative\nlanguage model, argumentation structure, subjectivity and sentiment signals,\nand refine a concise collection of credible tweets for market insights. The\nframework is evaluated on two self-annotated financial anomalies, i.e., Twitter\nand Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms\nthe baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and\n18.88% on precision, respectively, proving its capability in screening\nunreliable information precisely. At the same time, information retrieval and\nreliability assessment modules are analyzed individually on their effectiveness\nand causes of limitations, with identified subjective and objective factors\nthat influence the performance. As a collaborative project with Refinitiv, this\nframework paves a promising path towards building a scalable commercial\nsolution that assists traders to reach investment decisions on pricing\nanomalies with authenticated knowledge from social media platforms in\nreal-time.",
        "pdf_link": "https://arxiv.org/pdf/2203.08144v1.pdf"
    },
    {
        "title": "Hierarchical BERT for Medical Document Understanding",
        "authors": [
            "Ning Zhang",
            "Maciej Jankowski"
        ],
        "published": "2022-03-11T03:50:03Z",
        "summary": "Medical document understanding has gained much attention recently. One\nrepresentative task is the International Classification of Disease (ICD)\ndiagnosis code assignment. Existing work adopts either RNN or CNN as the\nbackbone network because the vanilla BERT cannot handle well long documents\n(>2000 to kens). One issue shared across all these approaches is that they are\nover specific to the ICD code assignment task, losing generality to give the\nwhole document-level and sentence-level embedding. As a result, it is not\nstraight-forward to direct them to other downstream NLU tasks. Motivated by\nthese observations, we propose Medical Document BERT (MDBERT) for long medical\ndocument understanding tasks. MDBERT is not only effective in learning\nrepresentations at different levels of semantics but efficient in encoding long\ndocuments by leveraging a bottom-up hierarchical architecture. Compared to\nvanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%\non the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it\ncuts the computational complexity on self-attention modules to less than 1/100.\nOther than the ICD code assignment, we conduct a variety of other NLU tasks on\na large commercial dataset named as TrialTrove, to showcase MDBERT's strength\nin delivering different levels of semantics.",
        "pdf_link": "https://arxiv.org/pdf/2204.09600v1.pdf"
    },
    {
        "title": "Contextualized Sensorimotor Norms: multi-dimensional measures of sensorimotor strength for ambiguous English words, in context",
        "authors": [
            "Sean Trott",
            "Benjamin Bergen"
        ],
        "published": "2022-03-10T21:23:00Z",
        "summary": "Most large language models are trained on linguistic input alone, yet humans\nappear to ground their understanding of words in sensorimotor experience. A\nnatural solution is to augment LM representations with human judgments of a\nword's sensorimotor associations (e.g., the Lancaster Sensorimotor Norms), but\nthis raises another challenge: most words are ambiguous, and judgments of words\nin isolation fail to account for this multiplicity of meaning (e.g., \"wooden\ntable\" vs. \"data table\"). We attempted to address this problem by building a\nnew lexical resource of contextualized sensorimotor judgments for 112 English\nwords, each rated in four different contexts (448 sentences total). We show\nthat these ratings encode overlapping but distinct information from the\nLancaster Sensorimotor Norms, and that they also predict other measures of\ninterest (e.g., relatedness), above and beyond measures derived from BERT.\nBeyond shedding light on theoretical questions, we suggest that these ratings\ncould be of use as a \"challenge set\" for researchers building grounded language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2203.05648v1.pdf"
    },
    {
        "title": "A new approach to calculating BERTScore for automatic assessment of translation quality",
        "authors": [
            "A. A. Vetrov",
            "E. A. Gorn"
        ],
        "published": "2022-03-10T19:25:16Z",
        "summary": "The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -> Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.",
        "pdf_link": "https://arxiv.org/pdf/2203.05598v4.pdf"
    },
    {
        "title": "Semantic Norm Recognition and its application to Portuguese Law",
        "authors": [
            "Maria Duarte",
            "Pedro A. Santos",
            "Jo\u00e3o Dias",
            "Jorge Baptista"
        ],
        "published": "2022-03-10T15:28:05Z",
        "summary": "Being able to clearly interpret legal texts and fully understanding our\nrights, obligations and other legal norms has become progressively more\nimportant in the digital society. However, simply giving citizens access to the\nlaws is not enough, as there is a need to provide meaningful information that\ncater to their specific queries and needs. For this, it is necessary to extract\nthe relevant semantic information present in legal texts. Thus, we introduce\nthe SNR (Semantic Norm Recognition) system, an automatic semantic information\nextraction system trained on a domain-specific (legal) text corpus taken from\nPortuguese Consumer Law. The SNR system uses the Portuguese Bert (BERTimbau)\nand was trained on a legislative Portuguese corpus. We demonstrate how our\nsystem achieved good results (81.44\\% F1-score) on this domain-specific corpus,\ndespite existing noise, and how it can be used to improve downstream tasks such\nas information retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2203.05425v1.pdf"
    },
    {
        "title": "MVP: Multimodality-guided Visual Pre-training",
        "authors": [
            "Longhui Wei",
            "Lingxi Xie",
            "Wengang Zhou",
            "Houqiang Li",
            "Qi Tian"
        ],
        "published": "2022-03-10T06:11:20Z",
        "summary": "Recently, masked image modeling (MIM) has become a promising direction for\nvisual pre-training. In the context of vision transformers, MIM learns\neffective visual representation by aligning the token-level features with a\npre-defined space (e.g., BEIT used a d-VAE trained on a large image corpus as\nthe tokenizer). In this paper, we go one step further by introducing guidance\nfrom other modalities and validating that such additional knowledge leads to\nimpressive gains for visual pre-training. The proposed approach is named\nMultimodality-guided Visual Pre-training (MVP), in which we replace the\ntokenizer with the vision branch of CLIP, a vision-language model pre-trained\non 400 million image-text pairs. We demonstrate the effectiveness of MVP by\nperforming standard experiments, i.e., pre-training the ViT models on ImageNet\nand fine-tuning them on a series of downstream visual recognition tasks. In\nparticular, pre-training ViT-Base/16 for 300 epochs, MVP reports a 52.4% mIoU\non ADE20K, surpassing BEIT (the baseline and previous state-of-the-art) with an\nimpressive margin of 6.8%.",
        "pdf_link": "https://arxiv.org/pdf/2203.05175v1.pdf"
    },
    {
        "title": "Speciesist Language and Nonhuman Animal Bias in English Masked Language Models",
        "authors": [
            "Masashi Takeshita",
            "Rafal Rzepka",
            "Kenji Araki"
        ],
        "published": "2022-03-10T03:32:29Z",
        "summary": "Various existing studies have analyzed what social biases are inherited by\nNLP models. These biases may directly or indirectly harm people, therefore\nprevious studies have focused only on human attributes. However, until recently\nno research on social biases in NLP regarding nonhumans existed. In this paper,\nwe analyze biases to nonhuman animals, i.e. speciesist bias, inherent in\nEnglish Masked Language Models such as BERT. We analyzed speciesist bias\nagainst 46 animal names using template-based and corpus-extracted sentences\ncontaining speciesist (or non-speciesist) language. We found that pre-trained\nmasked language models tend to associate harmful words with nonhuman animals\nand have a bias toward using speciesist language for some nonhuman animal\nnames. Our code for reproducing the experiments will be made available on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2203.05140v3.pdf"
    },
    {
        "title": "Compilable Neural Code Generation with Compiler Feedback",
        "authors": [
            "Xin Wang",
            "Yasheng Wang",
            "Yao Wan",
            "Fei Mi",
            "Yitong Li",
            "Pingyi Zhou",
            "Jin Liu",
            "Hao Wu",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022-03-10T03:15:17Z",
        "summary": "Automatically generating compilable programs with (or without) natural\nlanguage descriptions has always been a touchstone problem for computational\nlinguistics and automated software engineering. Existing deep-learning\napproaches model code generation as text generation, either constrained by\ngrammar structures in decoder, or driven by pre-trained language models on\nlarge-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of\nthem account for compilability of the generated programs. To improve\ncompilability of the generated programs, this paper proposes COMPCODER, a\nthree-stage pipeline utilizing compiler feedback for compilable code\ngeneration, including language model fine-tuning, compilability reinforcement,\nand compilability discrimination. Comprehensive experiments on two code\ngeneration tasks demonstrate the effectiveness of our proposed approach,\nimproving the success rate of compilation from 44.18 to 89.18 in code\ncompletion on average and from 70.3 to 96.2 in text-to-code generation,\nrespectively, when comparing with the state-of-the-art CodeGPT.",
        "pdf_link": "https://arxiv.org/pdf/2203.05132v1.pdf"
    },
    {
        "title": "NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks",
        "authors": [
            "Fawaz Sammani",
            "Tanmoy Mukherjee",
            "Nikos Deligiannis"
        ],
        "published": "2022-03-09T22:57:15Z",
        "summary": "Natural language explanation (NLE) models aim at explaining the\ndecision-making process of a black box system via generating natural language\nsentences which are human-friendly, high-level and fine-grained. Current NLE\nmodels explain the decision-making process of a vision or vision-language model\n(a.k.a., task model), e.g., a VQA model, via a language model (a.k.a.,\nexplanation model), e.g., GPT. Other than the additional memory resources and\ninference time required by the task model, the task and explanation models are\ncompletely independent, which disassociates the explanation from the reasoning\nprocess made to predict the answer. We introduce NLX-GPT, a general, compact\nand faithful language model that can simultaneously predict an answer and\nexplain it. We first conduct pre-training on large scale data of image-caption\npairs for general understanding of images, and then formulate the answer as a\ntext prediction task along with the explanation. Without region proposals nor a\ntask model, our resulting overall framework attains better evaluation scores,\ncontains much less parameters and is 15$\\times$ faster than the current SoA\nmodel. We then address the problem of evaluating the explanations which can be\nin many times generic, data-biased and can come in several forms. We therefore\ndesign 2 new evaluation measures: (1) explain-predict and (2) retrieval-based\nattack, a self-evaluation framework that requires no labels. Code is at:\nhttps://github.com/fawazsammani/nlxgpt.",
        "pdf_link": "https://arxiv.org/pdf/2203.05081v1.pdf"
    },
    {
        "title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing",
        "authors": [
            "Sonish Sivarajkumar",
            "Yanshan Wang"
        ],
        "published": "2022-03-09T21:44:28Z",
        "summary": "Deep learning algorithms are dependent on the availability of large-scale\nannotated clinical text datasets. The lack of such publicly available datasets\nis the biggest bottleneck for the development of clinical Natural Language\nProcessing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep\nlearning models to classify instances from new classes of which no training\ndata have been seen before. Prompt-based learning is an emerging ZSL technique\nwhere we define task-based templates for NLP tasks. We developed a novel\nprompt-based clinical NLP framework called HealthPrompt and applied the\nparadigm of prompt-based learning on clinical texts. In this technique, rather\nthan fine-tuning a Pre-trained Language Model(PLM), the task definitions are\ntuned by defining a prompt template. We performed an in-depth analysis of\nHealthPrompt on six different PLMs in a no-data setting. Our experiments prove\nthat prompts effectively capture the context of clinical texts and perform\nremarkably well without any training data.",
        "pdf_link": "https://arxiv.org/pdf/2203.05061v1.pdf"
    },
    {
        "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition",
        "authors": [
            "W. Ronny Huang",
            "Cal Peyser",
            "Tara N. Sainath",
            "Ruoming Pang",
            "Trevor Strohman",
            "Shankar Kumar"
        ],
        "published": "2022-03-09T19:20:03Z",
        "summary": "Language model fusion helps smart assistants recognize words which are rare\nin acoustic data but abundant in text-only corpora (typed search logs).\nHowever, such corpora have properties that hinder downstream performance,\nincluding being (1) too large, (2) beset with domain-mismatched content, and\n(3) heavy-headed rather than heavy-tailed (excessively many duplicate search\nqueries such as \"weather\"). We show that three simple strategies for selecting\nlanguage modeling data can dramatically improve rare-word recognition without\nharming overall performance. First, to address the heavy-headedness, we\ndownsample the data according to a soft log function, which tunably reduces\nhigh frequency (head) sentences. Second, to encourage rare-word exposure, we\nexplicitly filter for words rare in the acoustic data. Finally, we tackle\ndomain-mismatch via perplexity-based contrastive selection, filtering for\nexamples matched to the target domain. We down-select a large corpus of web\nsearch queries by a factor of 53x and achieve better LM perplexities than\nwithout down-selection. When shallow-fused with a state-of-the-art, production\nspeech engine, our LM achieves WER reductions of up to 24% relative on\nrare-word sentences (without changing overall WER) compared to a baseline LM\ntrained on the raw corpus. These gains are further validated through favorable\nside-by-side evaluations on live voice search traffic.",
        "pdf_link": "https://arxiv.org/pdf/2203.05008v2.pdf"
    },
    {
        "title": "Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain",
        "authors": [
            "Zhe Zheng",
            "Xin-Zheng Lu",
            "Ke-Yin Chen",
            "Yu-Cheng Zhou",
            "Jia-Rui Lin"
        ],
        "published": "2022-03-09T14:10:55Z",
        "summary": "As an essential task for the architecture, engineering, and construction\n(AEC) industry, information retrieval (IR) from unstructured textual data based\non natural language processing (NLP) is gaining increasing attention. Although\nvarious deep learning (DL) models for IR tasks have been investigated in the\nAEC domain, it is still unclear how domain corpora and domain-specific\npretrained DL models can improve performance in various IR tasks. To this end,\nthis work systematically explores the impacts of domain corpora and various\ntransfer learning techniques on the performance of DL models for IR tasks and\nproposes a pretrained domain-specific language model for the AEC domain. First,\nboth in-domain and close-domain corpora are developed. Then, two types of\npretrained models, including traditional wording embedding models and\nBERT-based models, are pretrained based on various domain corpora and transfer\nlearning strategies. Finally, several widely used DL models for IR tasks are\nfurther trained and tested based on various configurations and pretrained\nmodels. The result shows that domain corpora have opposite effects on\ntraditional word embedding models for text classification and named entity\nrecognition tasks but can further improve the performance of BERT-based models\nin all tasks. Meanwhile, BERT-based models dramatically outperform traditional\nmethods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1\nscore, respectively. This research contributes to the body of knowledge in two\nways: 1) demonstrating the advantages of domain corpora and pretrained DL\nmodels and 2) opening the first domain-specific dataset and pretrained language\nmodel for the AEC domain, to the best of our knowledge. Thus, this work sheds\nlight on the adoption and application of pretrained models in the AEC domain.",
        "pdf_link": "https://arxiv.org/pdf/2203.04729v1.pdf"
    },
    {
        "title": "LEMON: LanguagE ModeL for Negative Sampling of Knowledge Graph Embeddings",
        "authors": [
            "Md Rashad Al Hasan Rony",
            "Mirza Mohtashim Alam",
            "Semab Ali",
            "Jens Lehmann",
            "Sahar Vahdati"
        ],
        "published": "2022-03-09T13:27:47Z",
        "summary": "Knowledge Graph Embedding models have become an important area of machine\nlearning.Those models provide a latent representation of entities and relations\nin a knowledge graph which can then be used in downstream machine learning\ntasks such as link prediction. The learning process of such models can be\nperformed by contrasting positive and negative triples. While all triples of a\nKG are considered positive, negative triples are usually not readily available.\nTherefore, the choice of the sampling method to obtain the negative triples\nplay a crucial role in the performance and effectiveness of Knowledge Graph\nEmbedding models. Most of the current methods fetch negative samples from a\nrandom distribution of entities in the underlying Knowledge Graph which also\noften includes meaningless triples. Other known methods use adversarial\ntechniques or generative neural networks which consequently reduce the\nefficiency of the process. In this paper, we propose an approach for generating\ninformative negative samples considering available complementary knowledge\nabout entities. Particularly, Pre-trained Language Models are used to form\nneighborhood clusters by utilizing the distances between entities to obtain\nrepresentations of symbolic entities via their textual information. Our\ncomprehensive evaluations demonstrate the effectiveness of the proposed\napproach on benchmark Knowledge Graphs with textual information for the link\nprediction task.",
        "pdf_link": "https://arxiv.org/pdf/2203.04703v3.pdf"
    },
    {
        "title": "Gym-saturation: an OpenAI Gym environment for saturation provers",
        "authors": [
            "Boris Shminke"
        ],
        "published": "2022-03-09T13:22:15Z",
        "summary": "`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL)\nagents capable of proving theorems. Currently, only theorems written in a\nformal language of the Thousands of Problems for Theorem Provers (TPTP) library\nin clausal normal form (CNF) are supported. `gym-saturation` implements the\n'given clause' algorithm (similar to the one used in Vampire and E Prover).\nBeing written in Python, `gym-saturation` was inspired by PyRes. In contrast to\nthe monolithic architecture of a typical Automated Theorem Prover (ATP),\n`gym-saturation` gives different agents opportunities to select clauses\nthemselves and train from their experience. Combined with a particular agent,\n`gym-saturation` can work as an ATP. Even with a non trained agent based on\nheuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF\nproblems from TPTP v7.5.0.",
        "pdf_link": "https://arxiv.org/pdf/2203.04699v1.pdf"
    },
    {
        "title": "Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing",
        "authors": [
            "Sonish Sivarajkumar",
            "Thomas Yu CHow Tam",
            "Haneef Ahamed Mohammad",
            "Samual Viggiano",
            "David Oniani",
            "Shyam Visweswaran",
            "Yanshan Wang"
        ],
        "published": "2022-03-08T21:20:19Z",
        "summary": "Alzheimer's Disease (AD) is the most common form of dementia in the United\nStates. Sleep is one of the lifestyle-related factors that has been shown\ncritical for optimal cognitive function in old age. However, there is a lack of\nresearch studying the association between sleep and AD incidence. A major\nbottleneck for conducting such research is that the traditional way to acquire\nsleep information is time-consuming, inefficient, non-scalable, and limited to\npatients' subjective experience. A gold standard dataset is created from manual\nannotation of 570 randomly sampled clinical note documents from the adSLEEP, a\ncorpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved\nfrom the University of Pittsburgh Medical Center (UPMC). We developed a\nrule-based Natural Language Processing (NLP) algorithm, machine learning\nmodels, and Large Language Model(LLM)-based NLP algorithms to automate the\nextraction of sleep-related concepts, including snoring, napping, sleep\nproblem, bad sleep quality, daytime sleepiness, night wakings, and sleep\nduration, from the gold standard dataset. Rule-based NLP algorithm achieved the\nbest performance of F1 across all sleep-related concepts. In terms of Positive\nPredictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime\nsleepiness and sleep duration, machine learning models: 0.95 and for napping,\n0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning\nachieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for\nsleep duration. The results show that the rule-based NLP algorithm consistently\nachieved the best performance for all sleep concepts. This study focused on the\nclinical notes of patients with AD, but could be extended to general sleep\ninformation extraction for other diseases.",
        "pdf_link": "https://arxiv.org/pdf/2204.09601v2.pdf"
    },
    {
        "title": "Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data",
        "authors": [
            "Minori Toyoda",
            "Kanata Suzuki",
            "Yoshihiko Hayashi",
            "Tetsuya Ogata"
        ],
        "published": "2022-03-08T17:39:16Z",
        "summary": "This study achieved bidirectional translation between descriptions and\nactions using small paired data from different modalities. The ability to\nmutually generate descriptions and actions is essential for robots to\ncollaborate with humans in their daily lives, which generally requires a large\ndataset that maintains comprehensive pairs of both modality data. However, a\npaired dataset is expensive to construct and difficult to collect. To address\nthis issue, this study proposes a two-stage training method for bidirectional\ntranslation. In the proposed method, we train recurrent autoencoders (RAEs) for\ndescriptions and actions with a large amount of non-paired data. Then, we\nfinetune the entire model to bind their intermediate representations using\nsmall paired data. Because the data used for pre-training do not require\npairing, behavior-only data or a large language corpus can be used. We\nexperimentally evaluated our method using a paired dataset consisting of\nmotion-captured actions and descriptions. The results showed that our method\nperformed well, even when the amount of paired data to train was small. The\nvisualization of the intermediate representations of each RAE showed that\nsimilar actions were encoded in a clustered position and the corresponding\nfeature vectors were well aligned.",
        "pdf_link": "https://arxiv.org/pdf/2203.04218v2.pdf"
    },
    {
        "title": "InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER",
        "authors": [
            "Liwen Wang",
            "Rumei Li",
            "Yang Yan",
            "Yuanmeng Yan",
            "Sirui Wang",
            "Wei Wu",
            "Weiran Xu"
        ],
        "published": "2022-03-08T07:56:36Z",
        "summary": "Recently, prompt-based methods have achieved significant performance in\nfew-shot learning scenarios by bridging the gap between language model\npre-training and fine-tuning for downstream tasks. However, existing prompt\ntemplates are mostly designed for sentence-level tasks and are inappropriate\nfor sequence labeling objectives. To address the above issue, we propose a\nmulti-task instruction-based generative framework, named InstructionNER, for\nlow-resource named entity recognition. Specifically, we reformulate the NER\ntask as a generation problem, which enriches source sentences with\ntask-specific instructions and answer options, then inferences the entities and\ntypes in natural language. We further propose two auxiliary tasks, including\nentity extraction and entity typing, which enable the model to capture more\nboundary information of entities and deepen the understanding of entity type\nsemantics, respectively. Experimental results show that our method consistently\noutperforms other baselines on five datasets in few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2203.03903v1.pdf"
    },
    {
        "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
        "authors": [
            "Zhengkun Zhang",
            "Wenya Guo",
            "Xiaojun Meng",
            "Yasheng Wang",
            "Yadao Wang",
            "Xin Jiang",
            "Qun Liu",
            "Zhenglu Yang"
        ],
        "published": "2022-03-08T06:51:33Z",
        "summary": "The workflow of pretraining and fine-tuning has emerged as a popular paradigm\nfor solving various NLP and V&L (Vision-and-Language) downstream tasks. With\nthe capacity of pretrained models growing rapidly, how to perform\nparameter-efficient fine-tuning has become fairly important for quick transfer\nlearning and deployment. In this paper, we design a novel unified\nparameter-efficient transfer learning framework that works effectively on both\npure language and V&L tasks. In particular, we use a shared hypernetwork that\ntakes trainable hyper-embeddings as input, and outputs weights for fine-tuning\ndifferent small modules in a pretrained language model, such as tuning the\nparameters inserted into multi-head attention blocks (i.e., prefix-tuning) and\nfeed-forward blocks (i.e., adapter-tuning). We define a set of embeddings\n(e.g., layer, block, task and visual embeddings) as the key components to\ncalculate hyper-embeddings, which thus can support both pure language and V&L\ntasks. Our proposed framework adds fewer trainable parameters in multi-task\nlearning while achieving superior performances and transfer ability compared to\nstate-of-the-art methods. Empirical results on the GLUE benchmark and multiple\nV&L tasks confirm the effectiveness of our framework on both textual and visual\nmodalities.",
        "pdf_link": "https://arxiv.org/pdf/2203.03878v1.pdf"
    },
    {
        "title": "Semantic-Preserving Linguistic Steganography by Pivot Translation and Semantic-Aware Bins Coding",
        "authors": [
            "Tianyu Yang",
            "Hanzhou Wu",
            "Biao Yi",
            "Guorui Feng",
            "Xinpeng Zhang"
        ],
        "published": "2022-03-08T01:35:05Z",
        "summary": "Linguistic steganography (LS) aims to embed secret information into a highly\nencoded text for covert communication. It can be roughly divided to two main\ncategories, i.e., modification based LS (MLS) and generation based LS (GLS).\nUnlike MLS that hides secret data by slightly modifying a given text without\nimpairing the meaning of the text, GLS uses a trained language model to\ndirectly generate a text carrying secret data. A common disadvantage for MLS\nmethods is that the embedding payload is very low, whose return is well\npreserving the semantic quality of the text. In contrast, GLS allows the data\nhider to embed a high payload, which has to pay the high price of\nuncontrollable semantics. In this paper, we propose a novel LS method to modify\na given text by pivoting it between two different languages and embed secret\ndata by applying a GLS-like information encoding strategy. Our purpose is to\nalter the expression of the given text, enabling a high payload to be embedded\nwhile keeping the semantic information unchanged. Experimental results have\nshown that the proposed work not only achieves a high embedding payload, but\nalso shows superior performance in maintaining the semantic consistency and\nresisting linguistic steganalysis.",
        "pdf_link": "https://arxiv.org/pdf/2203.03795v1.pdf"
    },
    {
        "title": "IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation",
        "authors": [
            "Gabriele Sarti",
            "Malvina Nissim"
        ],
        "published": "2022-03-07T22:39:01Z",
        "summary": "The T5 model and its unified text-to-text paradigm contributed in advancing\nthe state-of-the-art for many natural language processing tasks. While some\nmultilingual variants of the T5 model have recently been introduced, their\nperformances were found to provide suboptimal performances for languages other\nthan English if compared to monolingual variants. We are motivated by these\nfindings to introduce IT5, the first family of encoder-decoder transformer\nmodels pretrained specifically on Italian. We perform a thorough cleaning of a\nweb-crawled Italian corpus including more than 40 billion words and use it to\npretrain three IT5 models of different sizes. The performance of IT5 models and\ntheir multilingual counterparts is then evaluated on a broad range of natural\nlanguage understanding and generation benchmarks for Italian. We find the\nmonolingual IT5 models to provide the best scale-to-performance ratio across\ntested models, consistently outperforming their multilingual counterparts and\nsetting a new state-of-the-art for most Italian conditional language generation\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.03759v1.pdf"
    },
    {
        "title": "The Unsurprising Effectiveness of Pre-Trained Vision Models for Control",
        "authors": [
            "Simone Parisi",
            "Aravind Rajeswaran",
            "Senthil Purushwalkam",
            "Abhinav Gupta"
        ],
        "published": "2022-03-07T18:26:14Z",
        "summary": "Recent years have seen the emergence of pre-trained representations as a\npowerful abstraction for AI applications in computer vision, natural language,\nand speech. However, policy learning for control is still dominated by a\ntabula-rasa learning paradigm, with visuo-motor policies often trained from\nscratch using data from deployment environments. In this context, we revisit\nand study the role of pre-trained visual representations for control, and in\nparticular representations trained on large-scale computer vision datasets.\nThrough extensive empirical evaluation in diverse control domains (Habitat,\nDeepMind Control, Adroit, Franka Kitchen), we isolate and study the importance\nof different representation training methods, data augmentations, and feature\nhierarchies. Overall, we find that pre-trained visual representations can be\ncompetitive or even better than ground-truth state representations to train\ncontrol policies. This is in spite of using only out-of-domain data from\nstandard vision datasets, without any in-domain data from the deployment\nenvironments. Source code and more at\nhttps://sites.google.com/view/pvr-control.",
        "pdf_link": "https://arxiv.org/pdf/2203.03580v2.pdf"
    },
    {
        "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "authors": [
            "Greg Yang",
            "Edward J. Hu",
            "Igor Babuschkin",
            "Szymon Sidor",
            "Xiaodong Liu",
            "David Farhi",
            "Nick Ryder",
            "Jakub Pachocki",
            "Weizhu Chen",
            "Jianfeng Gao"
        ],
        "published": "2022-03-07T15:37:35Z",
        "summary": "Hyperparameter (HP) tuning in deep learning is an expensive process,\nprohibitively so for neural networks (NNs) with billions of parameters. We show\nthat, in the recently discovered Maximal Update Parametrization (muP), many\noptimal HPs remain stable even as model size changes. This leads to a new HP\ntuning paradigm we call muTransfer: parametrize the target model in muP, tune\nthe HP indirectly on a smaller model, and zero-shot transfer them to the\nfull-sized model, i.e., without directly tuning the latter at all. We verify\nmuTransfer on Transformer and ResNet. For example, 1) by transferring\npretraining HPs from a model of 13M parameters, we outperform published numbers\nof BERT-large (350M parameters), with a total tuning cost equivalent to\npretraining BERT-large once; 2) by transferring from 40M parameters, we\noutperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7%\nof total pretraining cost. A Pytorch implementation of our technique can be\nfound at github.com/microsoft/mup and installable via `pip install mup`.",
        "pdf_link": "https://arxiv.org/pdf/2203.03466v2.pdf"
    },
    {
        "title": "What Did You Say? Task-Oriented Dialog Datasets Are Not Conversational!?",
        "authors": [
            "Alice Shoshana Jakobovits",
            "Francesco Piccinno",
            "Yasemin Altun"
        ],
        "published": "2022-03-07T14:26:23Z",
        "summary": "High-quality datasets for task-oriented dialog are crucial for the\ndevelopment of virtual assistants. Yet three of the most relevant large scale\ndialog datasets suffer from one common flaw: the dialog state update can be\ntracked, to a great extent, by a model that only considers the current user\nutterance, ignoring the dialog history. In this work, we outline a taxonomy of\nconversational and contextual effects, which we use to examine MultiWOZ, SGD\nand SMCalFlow, among the most recent and widely used task-oriented dialog\ndatasets. We analyze the datasets in a model-independent fashion and\ncorroborate these findings experimentally using a strong text-to-text baseline\n(T5). We find that less than 4% of MultiWOZ's turns and 10% of SGD's turns are\nconversational, while SMCalFlow is not conversational at all in its current\nrelease: its dialog state tracking task can be reduced to single exchange\nsemantic parsing. We conclude by outlining desiderata for truly conversational\ndialog datasets.",
        "pdf_link": "https://arxiv.org/pdf/2203.03431v1.pdf"
    },
    {
        "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
        "authors": [
            "Dingkun Long",
            "Qiong Gao",
            "Kuan Zou",
            "Guangwei Xu",
            "Pengjun Xie",
            "Ruijie Guo",
            "Jian Xu",
            "Guanjun Jiang",
            "Luxi Xing",
            "Ping Yang"
        ],
        "published": "2022-03-07T13:20:46Z",
        "summary": "Passage retrieval is a fundamental task in information retrieval (IR)\nresearch, which has drawn much attention recently. In the English field, the\navailability of large-scale annotated dataset (e.g, MS MARCO) and the emergence\nof deep pre-trained language models (e.g, BERT) has resulted in a substantial\nimprovement of existing passage retrieval systems. However, in the Chinese\nfield, especially for specific domains, passage retrieval systems are still\nimmature due to quality-annotated dataset being limited by scale. Therefore, in\nthis paper, we present a novel multi-domain Chinese dataset for passage\nretrieval (Multi-CPR). The dataset is collected from three different domains,\nincluding E-commerce, Entertainment video and Medical. Each dataset contains\nmillions of passages and a certain amount of human annotated query-passage\nrelated pairs. We implement various representative passage retrieval methods as\nbaselines. We find that the performance of retrieval models trained on dataset\nfrom general domain will inevitably decrease on specific domain. Nevertheless,\na passage retrieval system built on in-domain annotated dataset can achieve\nsignificant improvement, which indeed demonstrates the necessity of domain\nlabeled data for further optimization. We hope the release of the Multi-CPR\ndataset could benchmark Chinese passage retrieval task in specific domain and\nalso make advances for future studies.",
        "pdf_link": "https://arxiv.org/pdf/2203.03367v2.pdf"
    },
    {
        "title": "Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models",
        "authors": [
            "Shengnan An",
            "Yifei Li",
            "Zeqi Lin",
            "Qian Liu",
            "Bei Chen",
            "Qiang Fu",
            "Weizhu Chen",
            "Nanning Zheng",
            "Jian-Guang Lou"
        ],
        "published": "2022-03-07T05:04:32Z",
        "summary": "Recently the prompt-tuning paradigm has attracted significant attention. By\nonly tuning continuous prompts with a frozen pre-trained language model (PLM),\nprompt-tuning takes a step towards deploying a shared frozen PLM to serve\nnumerous downstream tasks. Although prompt-tuning shows good performance on\ncertain natural language understanding (NLU) tasks, its effectiveness on\nnatural language generation (NLG) tasks is still under-explored. In this paper,\nwe argue that one of the factors hindering the development of prompt-tuning on\nNLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different\nfrom the pretraining corpus). For example, our preliminary exploration reveals\na large performance gap between prompt-tuning and fine-tuning when unfamiliar\ninputs occur frequently in NLG tasks. This motivates us to propose\ninput-tuning, which fine-tunes both the continuous prompts and the input\nrepresentations, leading to a more effective way to adapt unfamiliar inputs to\nfrozen PLMs. Our proposed input-tuning is conceptually simple and empirically\npowerful. Experimental results on seven NLG tasks demonstrate that input-tuning\nis significantly and consistently better than prompt-tuning. Furthermore, on\nthree of these tasks, input-tuning can achieve a comparable or even better\nperformance than fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2203.03131v1.pdf"
    },
    {
        "title": "Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos",
        "authors": [
            "Saghir Alfasly",
            "Jian Lu",
            "Chen Xu",
            "Yuru Zou"
        ],
        "published": "2022-03-06T17:31:06Z",
        "summary": "With the assumption that a video dataset is multimodality annotated in which\nauditory and visual modalities both are labeled or class-relevant, current\nmultimodal methods apply modality fusion or cross-modality attention. However,\neffectively leveraging the audio modality in vision-specific annotated videos\nfor action recognition is of particular challenge. To tackle this challenge, we\npropose a novel audio-visual framework that effectively leverages the audio\nmodality in any solely vision-specific annotated dataset. We adopt the language\nmodels (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)\nthat maps each video label to its most K-relevant audio labels in which SAVLD\nserves as a bridge between audio and video datasets. Then, SAVLD along with a\npretrained audio multi-label model are used to estimate the audio-visual\nmodality relevance during the training phase. Accordingly, a novel learnable\nirrelevant modality dropout (IMD) is proposed to completely drop out the\nirrelevant audio modality and fuse only the relevant modalities. Moreover, we\npresent a new two-stream video Transformer for efficiently modeling the visual\nmodalities. Results on several vision-specific annotated datasets including\nKinetics400 and UCF-101 validated our framework as it outperforms most relevant\naction recognition methods.",
        "pdf_link": "https://arxiv.org/pdf/2203.03014v2.pdf"
    },
    {
        "title": "Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation",
        "authors": [
            "Songming Zhang",
            "Yijin Liu",
            "Fandong Meng",
            "Yufeng Chen",
            "Jinan Xu",
            "Jian Liu",
            "Jie Zhou"
        ],
        "published": "2022-03-06T12:34:10Z",
        "summary": "Token-level adaptive training approaches can alleviate the token imbalance\nproblem and thus improve neural machine translation, through re-weighting the\nlosses of different target tokens based on specific statistical metrics (e.g.,\ntoken frequency or mutual information). Given that standard translation models\nmake predictions on the condition of previous target contexts, we argue that\nthe above statistical metrics ignore target context information and may assign\ninappropriate weights to target tokens. While one possible solution is to\ndirectly take target contexts into these statistical metrics, the\ntarget-context-aware statistical computing is extremely expensive, and the\ncorresponding storage overhead is unrealistic. To solve the above issues, we\npropose a target-context-aware metric, named conditional bilingual mutual\ninformation (CBMI), which makes it feasible to supplement target context\ninformation for statistical metrics. Particularly, our CBMI can be formalized\nas the log quotient of the translation model probability and language model\nprobability by decomposing the conditional joint distribution. Thus CBMI can be\nefficiently calculated during model training without any pre-specific\nstatistical calculations and large storage overhead. Furthermore, we propose an\neffective adaptive training approach based on both the token- and\nsentence-level CBMI. Experimental results on WMT14 English-German and WMT19\nChinese-English tasks show our approach can significantly outperform the\nTransformer baseline and other related methods.",
        "pdf_link": "https://arxiv.org/pdf/2203.02951v1.pdf"
    },
    {
        "title": "Graph Neural Network Enhanced Language Models for Efficient Multilingual Text Classification",
        "authors": [
            "Samujjwal Ghosh",
            "Subhadeep Maji",
            "Maunendra Sankar Desarkar"
        ],
        "published": "2022-03-06T09:05:42Z",
        "summary": "Online social media works as a source of various valuable and actionable\ninformation during disasters. These information might be available in multiple\nlanguages due to the nature of user generated content. An effective system to\nautomatically identify and categorize these actionable information should be\ncapable to handle multiple languages and under limited supervision. However,\nexisting works mostly focus on English language only with the assumption that\nsufficient labeled data is available. To overcome these challenges, we propose\na multilingual disaster related text classification system which is capable to\nwork under \\{mono, cross and multi\\} lingual scenarios and under limited\nsupervision. Our end-to-end trainable framework combines the versatility of\ngraph neural networks, by applying over the corpus, with the power of\ntransformer based large language models, over examples, with the help of\ncross-attention between the two. We evaluate our framework over total nine\nEnglish, Non-English and monolingual datasets in \\{mono, cross and multi\\}\nlingual classification scenarios. Our framework outperforms state-of-the-art\nmodels in disaster domain and multilingual BERT baseline in terms of Weighted\nF$_1$ score. We also show the generalizability of the proposed model under\nlimited supervision.",
        "pdf_link": "https://arxiv.org/pdf/2203.02912v1.pdf"
    },
    {
        "title": "Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents",
        "authors": [
            "Yicheng Zou",
            "Hongwei Liu",
            "Tao Gui",
            "Junzhe Wang",
            "Qi Zhang",
            "Meng Tang",
            "Haixiang Li",
            "Daniel Wang"
        ],
        "published": "2022-03-06T07:48:24Z",
        "summary": "Text semantic matching is a fundamental task that has been widely used in\nvarious scenarios, such as community question answering, information retrieval,\nand recommendation. Most state-of-the-art matching models, e.g., BERT, directly\nperform text comparison by processing each word uniformly. However, a query\nsentence generally comprises content that calls for different levels of\nmatching granularity. Specifically, keywords represent factual information such\nas action, entity, and event that should be strictly matched, while intents\nconvey abstract concepts and ideas that can be paraphrased into various\nexpressions. In this work, we propose a simple yet effective training strategy\nfor text semantic matching in a divide-and-conquer manner by disentangling\nkeywords from intents. Our approach can be easily combined with pre-trained\nlanguage models (PLM) without influencing their inference efficiency, achieving\nstable performance improvements against a wide range of PLMs on three\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2203.02898v1.pdf"
    },
    {
        "title": "Leveraging Pre-trained BERT for Audio Captioning",
        "authors": [
            "Xubo Liu",
            "Xinhao Mei",
            "Qiushi Huang",
            "Jianyuan Sun",
            "Jinzheng Zhao",
            "Haohe Liu",
            "Mark D. Plumbley",
            "Volkan K\u0131l\u0131\u00e7",
            "Wenwu Wang"
        ],
        "published": "2022-03-06T00:05:58Z",
        "summary": "Audio captioning aims at using natural language to describe the content of an\naudio clip. Existing audio captioning systems are generally based on an\nencoder-decoder architecture, in which acoustic information is extracted by an\naudio encoder and then a language decoder is used to generate the captions.\nTraining an audio captioning system often encounters the problem of data\nscarcity. Transferring knowledge from pre-trained audio models such as\nPre-trained Audio Neural Networks (PANNs) have recently emerged as a useful\nmethod to mitigate this issue. However, there is less attention on exploiting\npre-trained language models for the decoder, compared with the encoder. BERT is\na pre-trained language model that has been extensively used in Natural Language\nProcessing (NLP) tasks. Nevertheless, the potential of BERT as the language\ndecoder for audio captioning has not been investigated. In this study, we\ndemonstrate the efficacy of the pre-trained BERT model for audio captioning.\nSpecifically, we apply PANNs as the encoder and initialize the decoder from the\npublic pre-trained BERT models. We conduct an empirical study on the use of\nthese BERT models for the decoder in the audio captioning model. Our models\nachieve competitive results with the existing audio captioning methods on the\nAudioCaps dataset.",
        "pdf_link": "https://arxiv.org/pdf/2203.02838v2.pdf"
    },
    {
        "title": "Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation",
        "authors": [
            "Yicong Hong",
            "Zun Wang",
            "Qi Wu",
            "Stephen Gould"
        ],
        "published": "2022-03-05T14:56:14Z",
        "summary": "Most existing works in vision-and-language navigation (VLN) focus on either\ndiscrete or continuous environments, training agents that cannot generalize\nacross the two. The fundamental difference between the two setups is that\ndiscrete navigation assumes prior knowledge of the connectivity graph of the\nenvironment, so that the agent can effectively transfer the problem of\nnavigation with low-level controls to jumping from node to node with high-level\nactions by grounding to an image of a navigable direction. To bridge the\ndiscrete-to-continuous gap, we propose a predictor to generate a set of\ncandidate waypoints during navigation, so that agents designed with high-level\nactions can be transferred to and trained in continuous environments. We refine\nthe connectivity graph of Matterport3D to fit the continuous\nHabitat-Matterport3D, and train the waypoints predictor with the refined graphs\nto produce accessible waypoints at each time step. Moreover, we demonstrate\nthat the predicted waypoints can be augmented during training to diversify the\nviews and paths, and therefore enhance agent's generalization ability. Through\nextensive experiments we show that agents navigating in continuous environments\nwith predicted waypoints perform significantly better than agents using\nlow-level actions, which reduces the absolute discrete-to-continuous gap by\n11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent\nand 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple\nimitation learning objective, outperform previous methods by a large margin,\nachieving new state-of-the-art results on the testing environments of the\nR2R-CE and the RxR-CE datasets.",
        "pdf_link": "https://arxiv.org/pdf/2203.02764v1.pdf"
    },
    {
        "title": "Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models",
        "authors": [
            "Weiqi Sun",
            "Haidar Khan",
            "Nicolas Guenon des Mesnards",
            "Melanie Rubino",
            "Konstantine Arkoudas"
        ],
        "published": "2022-03-05T04:30:03Z",
        "summary": "Semantic parsing is a key NLP task that maps natural language to structured\nmeaning representations. As in many other NLP tasks, SOTA performance in\nsemantic parsing is now attained by fine-tuning a large pretrained language\nmodel (PLM). While effective, this approach is inefficient in the presence of\nmultiple downstream tasks, as a new set of values for all parameters of the PLM\nneeds to be stored for each task separately. Recent work has explored methods\nfor adapting PLMs to downstream tasks while keeping most (or all) of their\nparameters frozen. We examine two such promising techniques, prefix tuning and\nbias-term tuning, specifically on semantic parsing. We compare them against\neach other on two different semantic parsing datasets, and we also compare them\nagainst full and partial fine-tuning, both in few-shot and conventional data\nsettings. While prefix tuning is shown to do poorly for semantic parsing tasks\noff the shelf, we modify it by adding special token embeddings, which results\nin very strong performance without compromising parameter savings.",
        "pdf_link": "https://arxiv.org/pdf/2203.02652v1.pdf"
    },
    {
        "title": "Training language models to follow instructions with human feedback",
        "authors": [
            "Long Ouyang",
            "Jeff Wu",
            "Xu Jiang",
            "Diogo Almeida",
            "Carroll L. Wainwright",
            "Pamela Mishkin",
            "Chong Zhang",
            "Sandhini Agarwal",
            "Katarina Slama",
            "Alex Ray",
            "John Schulman",
            "Jacob Hilton",
            "Fraser Kelton",
            "Luke Miller",
            "Maddie Simens",
            "Amanda Askell",
            "Peter Welinder",
            "Paul Christiano",
            "Jan Leike",
            "Ryan Lowe"
        ],
        "published": "2022-03-04T07:04:42Z",
        "summary": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.",
        "pdf_link": "https://arxiv.org/pdf/2203.02155v1.pdf"
    },
    {
        "title": "Detecting Offensive Language on Social Networks: An End-to-end Detection Method based on Graph Attention Networks",
        "authors": [
            "Zhenxiong Miao",
            "Xingshu Chen",
            "Haizhou Wang",
            "Rui Tang",
            "Zhou Yang",
            "Wenyi Tang"
        ],
        "published": "2022-03-04T03:57:18Z",
        "summary": "The pervasiveness of offensive language on the social network has caused\nadverse effects on society, such as abusive behavior online. It is urgent to\ndetect offensive language and curb its spread. Existing research shows that\nmethods with community structure features effectively improve the performance\nof offensive language detection. However, the existing models deal with\ncommunity structure independently, which seriously affects the effectiveness of\ndetection models. In this paper, we propose an end-to-end method based on\ncommunity structure and text features for offensive language detection\n(CT-OLD). Specifically, the community structure features are directly captured\nby the graph attention network layer, and the text embeddings are taken from\nthe last hidden layer of BERT. Attention mechanisms and position encoding are\nused to fuse these features. Meanwhile, we add user opinion to the community\nstructure for representing user features. The user opinion is represented by\nuser historical behavior information, which outperforms that represented by\ntext information. Besides the above point, the distribution of users and tweets\nis unbalanced in the popular datasets, which limits the generalization ability\nof the model. To address this issue, we construct and release a dataset with\nreasonable user distribution. Our method outperforms baselines with the F1\nscore of 89.94%. The results show that the end-to-end model effectively learns\nthe potential information of community structure and text, and user historical\nbehavior information is more suitable for user opinion representation.",
        "pdf_link": "https://arxiv.org/pdf/2203.02123v1.pdf"
    },
    {
        "title": "LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models",
        "authors": [
            "Mojan Javaheripi",
            "Gustavo H. de Rosa",
            "Subhabrata Mukherjee",
            "Shital Shah",
            "Tomasz L. Religa",
            "Caio C. T. Mendes",
            "Sebastien Bubeck",
            "Farinaz Koushanfar",
            "Debadeepta Dey"
        ],
        "published": "2022-03-04T02:10:43Z",
        "summary": "The Transformer architecture is ubiquitously used as the building block of\nlarge-scale autoregressive language models. However, finding architectures with\nthe optimal trade-off between task performance (perplexity) and hardware\nconstraints like peak memory utilization and latency is non-trivial. This is\nexacerbated by the proliferation of various hardware. We leverage the somewhat\nsurprising empirical observation that the number of decoder parameters in\nautoregressive Transformers has a high rank correlation with task performance,\nirrespective of the architecture topology. This observation organically induces\na simple Neural Architecture Search (NAS) algorithm that uses decoder\nparameters as a proxy for perplexity without need for any model training. The\nsearch phase of our training-free algorithm, dubbed Lightweight Transformer\nSearch (LTS), can be run directly on target devices since it does not require\nGPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of\nperplexity versus any hardware performance cost. We evaluate LTS on diverse\ndevices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer\nbackbones: GPT-2 and Transformer-XL. Results show that the perplexity of\n16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster\nruntime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero\nand one-shot settings, LTS Pareto-frontier models achieve higher average\naccuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x\nlower latency. LTS extracts the Pareto-frontier in under 3 hours while running\non a commodity laptop. We effectively remove the carbon footprint of hundreds\nof GPU hours of training during search, offering a strong simple baseline for\nfuture NAS methods in autoregressive language modeling.",
        "pdf_link": "https://arxiv.org/pdf/2203.02094v2.pdf"
    },
    {
        "title": "Deep Lexical Hypothesis: Identifying personality structure in natural language",
        "authors": [
            "Andrew Cutler",
            "David M. Condon"
        ],
        "published": "2022-03-04T02:06:10Z",
        "summary": "Recent advances in natural language processing (NLP) have produced general\nmodels that can perform complex tasks such as summarizing long passages and\ntranslating across languages. Here, we introduce a method to extract adjective\nsimilarities from language models as done with survey-based ratings in\ntraditional psycholexical studies but using millions of times more text in a\nnatural setting. The correlational structure produced through this method is\nhighly similar to that of self- and other-ratings of 435 terms reported by\nSaucier and Goldberg (1996a). The first three unrotated factors produced using\nNLP are congruent with those in survey data, with coefficients of 0.89, 0.79,\nand 0.79. This structure is robust to many modeling decisions: adjective set,\nincluding those with 1,710 terms (Goldberg, 1982) and 18,000 terms (Allport &\nOdbert, 1936); the query used to extract correlations; and language model.\nNotably, Neuroticism and Openness are only weakly and inconsistently recovered.\nThis is a new source of signal that is closer to the original (semantic) vision\nof the Lexical Hypothesis. The method can be applied where surveys cannot: in\ndozens of languages simultaneously, with tens of thousands of items, on\nhistorical text, and at extremely large scale for little cost. The code is made\npublic to facilitate reproduction and fast iteration in new directions of\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2203.02092v1.pdf"
    },
    {
        "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models",
        "authors": [
            "Feng Li",
            "Hao Zhang",
            "Yi-Fan Zhang",
            "Shilong Liu",
            "Jian Guo",
            "Lionel M. Ni",
            "PengChuan Zhang",
            "Lei Zhang"
        ],
        "published": "2022-03-03T18:54:59Z",
        "summary": "This paper presents a comprehensive survey of vision-language (VL)\nintelligence from the perspective of time. This survey is inspired by the\nremarkable progress in both computer vision and natural language processing,\nand recent trends shifting from single modality processing to multiple modality\ncomprehension. We summarize the development in this field into three time\nperiods, namely task-specific methods, vision-language pre-training (VLP)\nmethods, and larger models empowered by large-scale weakly-labeled data. We\nfirst take some common VL tasks as examples to introduce the development of\ntask-specific methods. Then we focus on VLP methods and comprehensively review\nkey components of the model structures and training methods. After that, we\nshow how recent work utilizes large-scale raw image-text data to learn\nlanguage-aligned visual representations that generalize better on zero or few\nshot learning tasks. Finally, we discuss some potential future trends towards\nmodality cooperation, unified representation, and knowledge incorporation. We\nbelieve that this review will be of help for researchers and practitioners of\nAI and ML, especially those interested in computer vision and natural language\nprocessing.",
        "pdf_link": "https://arxiv.org/pdf/2203.01922v1.pdf"
    },
    {
        "title": "Improving Health Mentioning Classification of Tweets using Contrastive Adversarial Training",
        "authors": [
            "Pervaiz Iqbal Khan",
            "Shoaib Ahmed Siddiqui",
            "Imran Razzak",
            "Andreas Dengel",
            "Sheraz Ahmed"
        ],
        "published": "2022-03-03T18:20:51Z",
        "summary": "Health mentioning classification (HMC) classifies an input text as health\nmention or not. Figurative and non-health mention of disease words makes the\nclassification task challenging. Learning the context of the input text is the\nkey to this problem. The idea is to learn word representation by its\nsurrounding words and utilize emojis in the text to help improve the\nclassification results. In this paper, we improve the word representation of\nthe input text using adversarial training that acts as a regularizer during\nfine-tuning of the model. We generate adversarial examples by perturbing the\nembeddings of the model and then train the model on a pair of clean and\nadversarial examples. Additionally, we utilize contrastive loss that pushes a\npair of clean and perturbed examples close to each other and other examples\naway in the representation space. We train and evaluate the method on an\nextended version of the publicly available PHM2017 dataset. Experiments show an\nimprovement of 1.0% over BERT-Large baseline and 0.6% over RoBERTa-Large\nbaseline, whereas 5.8% over the state-of-the-art in terms of F1 score.\nFurthermore, we provide a brief analysis of the results by utilizing the power\nof explainable AI.",
        "pdf_link": "https://arxiv.org/pdf/2203.01895v1.pdf"
    },
    {
        "title": "Quantum Reinforcement Learning via Policy Iteration",
        "authors": [
            "El Amine Cherrat",
            "Iordanis Kerenidis",
            "Anupam Prakash"
        ],
        "published": "2022-03-03T18:08:17Z",
        "summary": "Quantum computing has shown the potential to substantially speed up machine\nlearning applications, in particular for supervised and unsupervised learning.\nReinforcement learning, on the other hand, has become essential for solving\nmany decision making problems and policy iteration methods remain the\nfoundation of such approaches. In this paper, we provide a general framework\nfor performing quantum reinforcement learning via policy iteration. We validate\nour framework by designing and analyzing: \\emph{quantum policy evaluation}\nmethods for infinite horizon discounted problems by building quantum states\nthat approximately encode the value function of a policy $\\pi$; and\n\\emph{quantum policy improvement} methods by post-processing measurement\noutcomes on these quantum states. Last, we study the theoretical and\nexperimental performance of our quantum algorithms on two environments from\nOpenAI's Gym.",
        "pdf_link": "https://arxiv.org/pdf/2203.01889v1.pdf"
    },
    {
        "title": "A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism",
        "authors": [
            "Rashid Khan",
            "M Shujah Islam",
            "Khadija Kanwal",
            "Mansoor Iqbal",
            "Md. Imran Hossain",
            "Zhongfu Ye"
        ],
        "published": "2022-03-03T09:47:59Z",
        "summary": "Image captioning is a fast-growing research field of computer vision and\nnatural language processing that involves creating text explanations for\nimages. This study aims to develop a system that uses a pre-trained\nconvolutional neural network (CNN) to extract features from an image,\nintegrates the features with an attention mechanism, and creates captions using\na recurrent neural network (RNN). To encode an image into a feature vector as\ngraphical attributes, we employed multiple pre-trained convolutional neural\nnetworks. Following that, a language model known as GRU is chosen as the\ndecoder to construct the descriptive sentence. In order to increase\nperformance, we merge the Bahdanau attention model with GRU to allow learning\nto be focused on a specific portion of the image. On the MSCOCO dataset, the\nexperimental results achieve competitive performance against state-of-the-art\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2203.01594v1.pdf"
    },
    {
        "title": "BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification",
        "authors": [
            "Yuanhong Chen",
            "Fengbei Liu",
            "Hu Wang",
            "Chong Wang",
            "Yu Tian",
            "Yuyuan Liu",
            "Gustavo Carneiro"
        ],
        "published": "2022-03-03T08:04:59Z",
        "summary": "Deep learning methods have shown outstanding classification accuracy in\nmedical imaging problems, which is largely attributed to the availability of\nlarge-scale datasets manually annotated with clean labels. However, given the\nhigh cost of such manual annotation, new medical imaging classification\nproblems may need to rely on machine-generated noisy labels extracted from\nradiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been\nmodelled from datasets with noisy labels, but their training procedure is in\ngeneral not robust to noisy-label samples, leading to sub-optimal models.\nFurthermore, CXR datasets are mostly multi-label, so current noisy-label\nlearning methods designed for multi-class problems cannot be easily adapted. In\nthis paper, we propose a new method designed for the noisy multi-label CXR\nlearning, which detects and smoothly re-labels samples from the dataset, which\nis then used to train common multi-label classifiers. The proposed method\noptimises a bag of multi-label descriptors (BoMD) to promote their similarity\nwith the semantic descriptors produced by BERT models from the multi-label\nimage annotation. Our experiments on diverse noisy multi-label training sets\nand clean testing sets show that our model has state-of-the-art accuracy and\nrobustness in many CXR multi-label classification benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2203.01937v5.pdf"
    },
    {
        "title": "Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking",
        "authors": [
            "Jamin Shin",
            "Hangyeol Yu",
            "Hyeongdon Moon",
            "Andrea Madotto",
            "Juneyoung Park"
        ],
        "published": "2022-03-03T07:54:09Z",
        "summary": "Annotating task-oriented dialogues is notorious for the expensive and\ndifficult data collection process. Few-shot dialogue state tracking (DST) is a\nrealistic solution to this problem. In this paper, we hypothesize that dialogue\nsummaries are essentially unstructured dialogue states; hence, we propose to\nreformulate dialogue state tracking as a dialogue summarization problem. To\nelaborate, we train a text-to-text language model with synthetic template-based\ndialogue summaries, generated by a set of rules from the dialogue states. Then,\nthe dialogue states can be recovered by inversely applying the summary\ngeneration rules. We empirically show that our method DS2 outperforms previous\nworks on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and\nmulti-domain settings. Our method also exhibits vast speedup during both\ntraining and inference as it can generate all states at once. Finally, based on\nour analysis, we discover that the naturalness of the summary templates plays a\nkey role for successful training.",
        "pdf_link": "https://arxiv.org/pdf/2203.01552v1.pdf"
    },
    {
        "title": "Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering",
        "authors": [
            "Soheil Esmaeilzadeh",
            "Brian Williams",
            "Davood Shamsi",
            "Onar Vikingstad"
        ],
        "published": "2022-03-02T18:24:10Z",
        "summary": "Teachers often conduct surveys in order to collect data from a predefined\ngroup of students to gain insights into topics of interest. When analyzing\nsurveys with open-ended textual responses, it is extremely time-consuming,\nlabor-intensive, and difficult to manually process all the responses into an\ninsightful and comprehensive report. In the analysis step, traditionally, the\nteacher has to read each of the responses and decide on how to group them in\norder to extract insightful information. Even though it is possible to group\nthe responses only using certain keywords, such an approach would be limited\nsince it not only fails to account for embedded contexts but also cannot detect\npolysemous words or phrases and semantics that are not expressible in single\nwords. In this work, we present a novel end-to-end context-aware framework that\nextracts, aggregates, and abbreviates embedded semantic patterns in\nopen-response survey data. Our framework relies on a pre-trained natural\nlanguage model in order to encode the textual data into semantic vectors. The\nencoded vectors then get clustered either into an optimally tuned number of\ngroups or into a set of groups with pre-specified titles. In the former case,\nthe clusters are then further analyzed to extract a representative set of\nkeywords or summary sentences that serve as the labels of the clusters. In our\nframework, for the designated clusters, we finally provide context-aware\nwordclouds that demonstrate the semantically prominent keywords within each\ngroup. Honoring user privacy, we have successfully built the on-device\nimplementation of our framework suitable for real-time analysis on mobile\ndevices and have tested it on a synthetic dataset. Our framework reduces the\ncosts at-scale by automating the process of extracting the most insightful\ninformation pieces from survey data.",
        "pdf_link": "https://arxiv.org/pdf/2203.01294v2.pdf"
    },
    {
        "title": "Integrating Contrastive Learning with Dynamic Models for Reinforcement Learning from Images",
        "authors": [
            "Bang You",
            "Oleg Arenz",
            "Youping Chen",
            "Jan Peters"
        ],
        "published": "2022-03-02T14:39:17Z",
        "summary": "Recent methods for reinforcement learning from images use auxiliary tasks to\nlearn image features that are used by the agent's policy or Q-function. In\nparticular, methods based on contrastive learning that induce linearity of the\nlatent dynamics or invariance to data augmentation have been shown to greatly\nimprove the sample efficiency of the reinforcement learning algorithm and the\ngeneralizability of the learned embedding. We further argue, that explicitly\nimproving Markovianity of the learned embedding is desirable and propose a\nself-supervised representation learning method which integrates contrastive\nlearning with dynamic models to synergistically combine these three objectives:\n(1) We maximize the InfoNCE bound on the mutual information between the state-\nand action-embedding and the embedding of the next state to induce a linearly\npredictive embedding without explicitly learning a linear transition model, (2)\nwe further improve Markovianity of the learned embedding by explicitly learning\na non-linear transition model using regression, and (3) we maximize the mutual\ninformation between the two nonlinear predictions of the next embeddings based\non the current action and two independent augmentations of the current state,\nwhich naturally induces transformation invariance not only for the state\nembedding, but also for the nonlinear transition model. Experimental evaluation\non the Deepmind control suite shows that our proposed method achieves higher\nsample efficiency and better generalization than state-of-art methods based on\ncontrastive learning or reconstruction.",
        "pdf_link": "https://arxiv.org/pdf/2203.01810v1.pdf"
    },
    {
        "title": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models",
        "authors": [
            "Ze-Feng Gao",
            "Peiyu Liu",
            "Wayne Xin Zhao",
            "Zhong-Yi Lu",
            "Ji-Rong Wen"
        ],
        "published": "2022-03-02T13:44:49Z",
        "summary": "Recently, Mixture-of-Experts (short as MoE) architecture has achieved\nremarkable success in increasing the model capacity of large-scale language\nmodels. However, MoE requires incorporating significantly more parameters than\nthe base model being extended. In this paper, we propose building a\nparameter-efficient MoE architecture by sharing information among experts. We\nadopt the matrix product operator (MPO, a tensor decomposition from quantum\nmany-body physics) to reconstruct the parameter matrix in the expert layer and\nincrease model capacity for pre-trained language models by sharing parameters\nof the central tensor (containing the core information) among different experts\nwhile enabling the specificity through the auxiliary tensors (complementing the\ncentral tensor) of different experts. To address the unbalanced optimization\nissue, we further design the gradient mask strategy for the MPO-based MoE\narchitecture. Extensive experiments based on T5 and GPT-2 show improved\nperformance and efficiency of the pre-trained language model (27.2x reduction\nin total parameters for the superior model performance, compared with the\nSwitch Transformers). Our code is publicly available at\nhttps://github.com/RUCAIBox/MPOE.",
        "pdf_link": "https://arxiv.org/pdf/2203.01104v4.pdf"
    },
    {
        "title": "Discontinuous Constituency and BERT: A Case Study of Dutch",
        "authors": [
            "Konstantinos Kogkalidis",
            "Gijs Wijnholds"
        ],
        "published": "2022-03-02T12:30:21Z",
        "summary": "In this paper, we set out to quantify the syntactic capacity of BERT in the\nevaluation regime of non-context free patterns, as occurring in Dutch. We\ndevise a test suite based on a mildly context-sensitive formalism, from which\nwe derive grammars that capture the linguistic phenomena of control verb\nnesting and verb raising. The grammars, paired with a small lexicon, provide us\nwith a large collection of naturalistic utterances, annotated with verb-subject\npairings, that serve as the evaluation test bed for an attention-based span\nselection probe. Our results, backed by extensive analysis, suggest that the\nmodels investigated fail in the implicit acquisition of the dependencies\nexamined.",
        "pdf_link": "https://arxiv.org/pdf/2203.01063v2.pdf"
    },
    {
        "title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers",
        "authors": [
            "Yun He",
            "Huaixiu Steven Zheng",
            "Yi Tay",
            "Jai Gupta",
            "Yu Du",
            "Vamsi Aribandi",
            "Zhe Zhao",
            "YaGuang Li",
            "Zhao Chen",
            "Donald Metzler",
            "Heng-Tze Cheng",
            "Ed H. Chi"
        ],
        "published": "2022-03-01T21:57:34Z",
        "summary": "Prompt-Tuning is a new paradigm for finetuning pre-trained language models in\na parameter-efficient way. Here, we explore the use of HyperNetworks to\ngenerate hyper-prompts: we propose HyperPrompt, a novel architecture for\nprompt-based task-conditioning of self-attention in Transformers. The\nhyper-prompts are end-to-end learnable via generation by a HyperNetwork.\nHyperPrompt allows the network to learn task-specific feature maps where the\nhyper-prompts serve as task global memories for the queries to attend to, at\nthe same time enabling flexible information sharing among tasks. We show that\nHyperPrompt is competitive against strong multi-task learning baselines with as\nfew as $0.14\\%$ of additional task-conditioning parameters, achieving great\nparameter and computational efficiency. Through extensive empirical\nexperiments, we demonstrate that HyperPrompt can achieve superior performances\nover strong T5 multi-task learning baselines and parameter-efficient adapter\nvariants including Prompt-Tuning and HyperFormer++ on Natural Language\nUnderstanding benchmarks of GLUE and SuperGLUE across many model sizes.",
        "pdf_link": "https://arxiv.org/pdf/2203.00759v2.pdf"
    },
    {
        "title": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
        "authors": [
            "Mohammad Akbari",
            "Amin Banitalebi-Dehkordi",
            "Yong Zhang"
        ],
        "published": "2022-03-01T21:21:27Z",
        "summary": "Building huge and highly capable language models has been a trend in the past\nyears. Despite their great performance, they incur high computational cost. A\ncommon solution is to apply model compression or choose light-weight\narchitectures, which often need a separate fixed-size model for each desirable\ncomputational budget, and may lose performance in case of heavy compression.\nThis paper proposes an effective dynamic inference approach, called E-LANG,\nwhich distributes the inference between large accurate Super-models and\nlight-weight Swift models. To this end, a decision making module routes the\ninputs to Super or Swift models based on the energy characteristics of the\nrepresentations in the latent space. This method is easily adoptable and\narchitecture agnostic. As such, it can be applied to black-box pre-trained\nmodels without a need for architectural manipulations, reassembling of modules,\nor re-training. Unlike existing methods that are only applicable to\nencoder-only backbones and classification tasks, our method also works for\nencoder-decoder structures and sequence-to-sequence tasks such as translation.\nThe E-LANG performance is verified through a set of experiments with T5 and\nBERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B\nwith an average computations speed-up of 3.3$\\times$ on GLUE and 2.9$\\times$ on\nSuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2$\\times$ less\ncomputations. Code and demo are available in the supplementary materials.",
        "pdf_link": "https://arxiv.org/pdf/2203.00748v1.pdf"
    },
    {
        "title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification",
        "authors": [
            "Yuting Nie",
            "Junhong Zhao",
            "Wei-Qiang Zhang",
            "Jinfeng Bai"
        ],
        "published": "2022-03-01T10:01:25Z",
        "summary": "Language identification is the task of automatically determining the identity\nof a language conveyed by a spoken segment. It has a profound impact on the\nmultilingual interoperability of an intelligent speech system. Despite language\nidentification attaining high accuracy on medium or long utterances(>3s), the\nperformance on short utterances (<=1s) is still far from satisfactory. We\npropose a BERT-based language identification system (BERT-LID) to improve\nlanguage identification performance, especially on short-duration speech\nsegments. We extend the original BERT model by taking the phonetic\nposteriorgrams (PPG) derived from the front-end phone recognizer as input. Then\nwe deployed the optimal deep classifier followed by it for language\nidentification. Our BERT-LID model can improve the baseline accuracy by about\n6.5% on long-segment identification and 19.9% on short-segment identification,\ndemonstrating our BERT-LID's effectiveness to language identification.",
        "pdf_link": "https://arxiv.org/pdf/2203.00328v3.pdf"
    },
    {
        "title": "\"Is Whole Word Masking Always Better for Chinese BERT?\": Probing on Chinese Grammatical Error Correction",
        "authors": [
            "Yong Dai",
            "Linyang Li",
            "Cong Zhou",
            "Zhangyin Feng",
            "Enbo Zhao",
            "Xipeng Qiu",
            "Piji Li",
            "Duyu Tang"
        ],
        "published": "2022-03-01T08:24:56Z",
        "summary": "Whole word masking (WWM), which masks all subwords corresponding to a word at\nonce, makes a better English BERT model. For the Chinese language, however,\nthere is no subword because each token is an atomic character. The meaning of a\nword in Chinese is different in that a word is a compositional unit consisting\nof multiple characters. Such difference motivates us to investigate whether WWM\nleads to better context understanding ability for Chinese BERT. To achieve\nthis, we introduce two probing tasks related to grammatical error correction\nand ask pretrained models to revise or insert tokens in a masked language\nmodeling manner. We construct a dataset including labels for 19,075 tokens in\n10,448 sentences. We train three Chinese BERT models with standard\ncharacter-level masking (CLM), WWM, and a combination of CLM and WWM,\nrespectively. Our major findings are as follows: First, when one character\nneeds to be inserted or replaced, the model trained with CLM performs the best.\nSecond, when more than one character needs to be handled, WWM is the key to\nbetter performance. Finally, when being fine-tuned on sentence-level downstream\ntasks, models trained with different masking strategies perform comparably.",
        "pdf_link": "https://arxiv.org/pdf/2203.00286v2.pdf"
    },
    {
        "title": "Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation",
        "authors": [
            "Xiang Hu",
            "Haitao Mi",
            "Liang Li",
            "Gerard de Melo"
        ],
        "published": "2022-03-01T07:54:44Z",
        "summary": "Recently CKY-based models show great potential in unsupervised grammar\ninduction thanks to their human-like encoding paradigm, which runs recursively\nand hierarchically, but requires $O(n^3)$ time-complexity. Recursive\nTransformer based on Differentiable Trees (R2D2) makes it possible to scale to\nlarge language model pre-training even with complex tree encoder by introducing\na heuristic pruning method. However, the rule-based pruning approach suffers\nfrom local optimum and slow inference issues. In this paper, we fix those\nissues in a unified method. We propose to use a top-down parser as a\nmodel-based pruning method, which also enables parallel encoding during\ninference. Typically, our parser casts parsing as a split point scoring task,\nwhich first scores all split points for a given sentence, and then recursively\nsplits a span into two by picking a split point with the highest score in the\ncurrent span. The reverse order of the splits is considered as the order of\npruning in R2D2 encoder. Beside the bi-directional language model loss, we also\noptimize the parser by minimizing the KL distance between tree probabilities\nfrom parser and R2D2. Our experiments show that our Fast-R2D2 improves\nperformance significantly in grammar induction and achieves competitive results\nin downstream classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2203.00281v3.pdf"
    },
    {
        "title": "Exploring and Adapting Chinese GPT to Pinyin Input Method",
        "authors": [
            "Minghuan Tan",
            "Yong Dai",
            "Duyu Tang",
            "Zhangyin Feng",
            "Guoping Huang",
            "Jing Jiang",
            "Jiwei Li",
            "Shuming Shi"
        ],
        "published": "2022-03-01T06:05:07Z",
        "summary": "While GPT has become the de-facto method for text generation tasks, its\napplication to pinyin input method remains unexplored. In this work, we make\nthe first exploration to leverage Chinese GPT for pinyin input method. We find\nthat a frozen GPT achieves state-of-the-art performance on perfect pinyin.\nHowever, the performance drops dramatically when the input includes abbreviated\npinyin. A reason is that an abbreviated pinyin can be mapped to many perfect\npinyin, which links to even larger number of Chinese characters. We mitigate\nthis issue with two strategies, including enriching the context with pinyin and\noptimizing the training process to help distinguish homophones. To further\nfacilitate the evaluation of pinyin input method, we create a dataset\nconsisting of 270K instances from 15 domains. Results show that our approach\nimproves performance on abbreviated pinyin across all domains. Model analysis\ndemonstrates that both strategies contribute to the performance boost.",
        "pdf_link": "https://arxiv.org/pdf/2203.00249v2.pdf"
    },
    {
        "title": "The impact of lexical and grammatical processing on generating code from natural language",
        "authors": [
            "Nathana\u00ebl Beau",
            "Beno\u00eet Crabb\u00e9"
        ],
        "published": "2022-02-28T17:23:30Z",
        "summary": "Considering the seq2seq architecture of TranX for natural language to code\ntranslation, we identify four key components of importance: grammatical\nconstraints, lexical preprocessing, input representations, and copy mechanisms.\nTo study the impact of these components, we use a state-of-the-art architecture\nthat relies on BERT encoder and a grammar-based decoder for which a\nformalization is provided. The paper highlights the importance of the lexical\nsubstitution component in the current natural language to code systems.",
        "pdf_link": "https://arxiv.org/pdf/2202.13972v2.pdf"
    },
    {
        "title": "Provably Efficient Convergence of Primal-Dual Actor-Critic with Nonlinear Function Approximation",
        "authors": [
            "Jing Dong",
            "Li Shen",
            "Yinggan Xu",
            "Baoxiang Wang"
        ],
        "published": "2022-02-28T15:16:23Z",
        "summary": "We study the convergence of the actor-critic algorithm with nonlinear\nfunction approximation under a nonconvex-nonconcave primal-dual formulation.\nStochastic gradient descent ascent is applied with an adaptive proximal term\nfor robust learning rates. We show the first efficient convergence result with\nprimal-dual actor-critic with a convergence rate of\n$\\mathcal{O}\\left(\\sqrt{\\frac{\\ln \\left(N d G^2 \\right)}{N}}\\right)$ under\nMarkovian sampling, where $G$ is the element-wise maximum of the gradient, $N$\nis the number of iterations, and $d$ is the dimension of the gradient. Our\nresult is presented with only the Polyak-\\L{}ojasiewicz condition for the dual\nvariables, which is easy to verify and applicable to a wide range of\nreinforcement learning (RL) scenarios. The algorithm and analysis are general\nenough to be applied to other RL settings, like multi-agent RL. Empirical\nresults on OpenAI Gym continuous control tasks corroborate our theoretical\nfindings.",
        "pdf_link": "https://arxiv.org/pdf/2202.13863v1.pdf"
    },
    {
        "title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
        "authors": [
            "Xing Wu",
            "Chaochen Gao",
            "Meng Lin",
            "Liangjun Zang",
            "Zhongyuan Wang",
            "Songlin Hu"
        ],
        "published": "2022-02-28T14:54:08Z",
        "summary": "Before entering the neural network, a token is generally converted to the\ncorresponding one-hot representation, which is a discrete distribution of the\nvocabulary. Smoothed representation is the probability of candidate tokens\nobtained from a pre-trained masked language model, which can be seen as a more\ninformative substitution to the one-hot representation. We propose an efficient\ndata augmentation method, termed text smoothing, by converting a sentence from\nits one-hot representation to a controllable smoothed representation. We\nevaluate text smoothing on different benchmarks in a low-resource regime.\nExperimental results show that text smoothing outperforms various mainstream\ndata augmentation methods by a substantial margin. Moreover, text smoothing can\nbe combined with those data augmentation methods to achieve better performance.",
        "pdf_link": "https://arxiv.org/pdf/2202.13840v1.pdf"
    },
    {
        "title": "Logical Fallacy Detection",
        "authors": [
            "Zhijing Jin",
            "Abhinav Lalwani",
            "Tejas Vaidhya",
            "Xiaoyu Shen",
            "Yiwen Ding",
            "Zhiheng Lyu",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Bernhard Sch\u00f6lkopf"
        ],
        "published": "2022-02-28T13:18:26Z",
        "summary": "Reasoning is central to human intelligence. However, fallacious arguments are\ncommon, and some exacerbate problems such as spreading misinformation about\nclimate change. In this paper, we propose the task of logical fallacy\ndetection, and provide a new dataset (Logic) of logical fallacies generally\nfound in text, together with an additional challenge set for detecting logical\nfallacies in climate change claims (LogicClimate). Detecting logical fallacies\nis a hard problem as the model must understand the underlying logical structure\nof the argument. We find that existing pretrained large language models perform\npoorly on this task. In contrast, we show that a simple structure-aware\nclassifier outperforms the best language model by 5.46% on Logic and 4.51% on\nLogicClimate. We encourage future work to explore this task as (a) it can serve\nas a new reasoning challenge for language models, and (b) it can have potential\napplications in tackling the spread of misinformation. Our dataset and code are\navailable at https://github.com/causalNLP/logical-fallacy",
        "pdf_link": "https://arxiv.org/pdf/2202.13758v3.pdf"
    },
    {
        "title": "Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation",
        "authors": [
            "Chulun Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Min Zhang",
            "Hongji Wang",
            "Jinsong Su"
        ],
        "published": "2022-02-28T10:24:22Z",
        "summary": "Most dominant neural machine translation (NMT) models are restricted to make\npredictions only according to the local context of preceding words in a\nleft-to-right manner. Although many previous studies try to incorporate global\ninformation into NMT models, there still exist limitations on how to\neffectively exploit bidirectional global context. In this paper, we propose a\nConfidence Based Bidirectional Global Context Aware (CBBGCA) training framework\nfor NMT, where the NMT model is jointly trained with an auxiliary conditional\nmasked language model (CMLM). The training consists of two stages: (1)\nmulti-task joint training; (2) confidence based knowledge distillation. At the\nfirst stage, by sharing encoder parameters, the NMT model is additionally\nsupervised by the signal from the CMLM decoder that contains bidirectional\nglobal contexts. Moreover, at the second stage, using the CMLM as teacher, we\nfurther pertinently incorporate bidirectional global context to the NMT model\non its unconfidently-predicted target words via knowledge distillation.\nExperimental results show that our proposed CBBGCA training framework\nsignificantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on\nthree large-scale translation datasets, namely WMT'14 English-to-German, WMT'19\nChinese-to-English and WMT'14 English-to-French, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2202.13663v3.pdf"
    },
    {
        "title": "Avalanche RL: a Continual Reinforcement Learning Library",
        "authors": [
            "Nicol\u00f2 Lucchesi",
            "Antonio Carta",
            "Vincenzo Lomonaco",
            "Davide Bacciu"
        ],
        "published": "2022-02-28T10:01:22Z",
        "summary": "Continual Reinforcement Learning (CRL) is a challenging setting where an\nagent learns to interact with an environment that is constantly changing over\ntime (the stream of experiences). In this paper, we describe Avalanche RL, a\nlibrary for Continual Reinforcement Learning which allows to easily train\nagents on a continuous stream of tasks. Avalanche RL is based on PyTorch and\nsupports any OpenAI Gym environment. Its design is based on Avalanche, one of\nthe more popular continual learning libraries, which allow us to reuse a large\nnumber of continual learning strategies and improve the interaction between\nreinforcement learning and continual learning researchers. Additionally, we\npropose Continual Habitat-Lab, a novel benchmark and a high-level library which\nenables the usage of the photorealistic simulator Habitat-Sim for CRL research.\nOverall, Avalanche RL attempts to unify under a common framework continual\nreinforcement learning applications, which we hope will foster the growth of\nthe field.",
        "pdf_link": "https://arxiv.org/pdf/2202.13657v2.pdf"
    },
    {
        "title": "Cross-Lingual Text Classification with Multilingual Distillation and Zero-Shot-Aware Training",
        "authors": [
            "Ziqing Yang",
            "Yiming Cui",
            "Zhigang Chen",
            "Shijin Wang"
        ],
        "published": "2022-02-28T09:51:32Z",
        "summary": "Multilingual pre-trained language models (MPLMs) not only can handle tasks in\ndifferent languages but also exhibit surprising zero-shot cross-lingual\ntransferability. However, MPLMs usually are not able to achieve comparable\nsupervised performance on rich-resource languages compared to the\nstate-of-the-art monolingual pre-trained models. In this paper, we aim to\nimprove the multilingual model's supervised and zero-shot performance\nsimultaneously only with the resources from supervised languages. Our approach\nis based on transferring knowledge from high-performance monolingual models\nwith a teacher-student framework. We let the multilingual model learn from\nmultiple monolingual models simultaneously. To exploit the model's\ncross-lingual transferability, we propose MBLM (multi-branch multilingual\nlanguage model), a model built on the MPLMs with multiple language branches.\nEach branch is a stack of transformers. MBLM is trained with the\nzero-shot-aware training strategy that encourages the model to learn from the\nmixture of zero-shot representations from all the branches. The results on two\ncross-lingual classification tasks show that, with only the task's supervised\ndata used, our method improves both the supervised and zero-shot performance of\nMPLMs.",
        "pdf_link": "https://arxiv.org/pdf/2202.13654v1.pdf"
    },
    {
        "title": "CINO: A Chinese Minority Pre-trained Language Model",
        "authors": [
            "Ziqing Yang",
            "Zihang Xu",
            "Yiming Cui",
            "Baoxin Wang",
            "Min Lin",
            "Dayong Wu",
            "Zhigang Chen"
        ],
        "published": "2022-02-28T06:02:06Z",
        "summary": "Multilingual pre-trained language models have shown impressive performance on\ncross-lingual tasks. It greatly facilitates the applications of natural\nlanguage processing on low-resource languages. However, there are still some\nlanguages that the current multilingual models do not perform well on. In this\npaper, we propose CINO (Chinese Minority Pre-trained Language Model), a\nmultilingual pre-trained language model for Chinese minority languages. It\ncovers Standard Chinese, Yue Chinese, and six other ethnic minority languages.\nTo evaluate the cross-lingual ability of the multilingual model on ethnic\nminority languages, we collect documents from Wikipedia and news websites, and\nconstruct two text classification datasets, WCM (Wiki-Chinese-Minority) and\nCMNews (Chinese-Minority-News). We show that CINO notably outperforms the\nbaselines on various classification tasks. The CINO model and the datasets are\npublicly available at http://cino.hfl-rc.com.",
        "pdf_link": "https://arxiv.org/pdf/2202.13558v2.pdf"
    },
    {
        "title": "Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks",
        "authors": [
            "Gechuan Zhang",
            "Paul Nulty",
            "David Lillis"
        ],
        "published": "2022-02-27T21:24:53Z",
        "summary": "The contextual word embedding model, BERT, has proved its ability on\ndownstream tasks with limited quantities of annotated data. BERT and its\nvariants help to reduce the burden of complex annotation work in many\ninterdisciplinary research areas, for example, legal argument mining in digital\nhumanities. Argument mining aims to develop text analysis tools that can\nautomatically retrieve arguments and identify relationships between\nargumentation clauses. Since argumentation is one of the key aspects of case\nlaw, argument mining tools for legal texts are applicable to both academic and\nnon-academic legal research. Domain-specific BERT variants (pre-trained with\ncorpora from a particular background) have also achieved strong performance in\nmany tasks. To our knowledge, previous machine learning studies of argument\nmining on judicial case law still heavily rely on statistical models. In this\npaper, we provide a broad study of both classic and contextual embedding models\nand their performance on practical case law from the European Court of Human\nRights (ECHR). During our study, we also explore a number of neural networks\nwhen being combined with different embeddings. Our experiments provide a\ncomprehensive overview of a variety of approaches to the legal argument mining\ntask. We conclude that domain pre-trained transformer models have great\npotential in this area, although traditional embeddings can also achieve strong\nperformance when combined with additional neural network layers.",
        "pdf_link": "https://arxiv.org/pdf/2202.13457v2.pdf"
    },
    {
        "title": "Controllable Natural Language Generation with Contrastive Prefixes",
        "authors": [
            "Jing Qian",
            "Li Dong",
            "Yelong Shen",
            "Furu Wei",
            "Weizhu Chen"
        ],
        "published": "2022-02-27T00:31:03Z",
        "summary": "To guide the generation of large pretrained language models (LM), previous\nwork has focused on directly fine-tuning the language model or utilizing an\nattribute discriminator. In this work, we propose a novel lightweight framework\nfor controllable GPT2 generation, which utilizes a set of small\nattribute-specific vectors, called prefixes, to steer natural language\ngeneration. Different from prefix-tuning, where each prefix is trained\nindependently, we take the relationship among prefixes into consideration and\ntrain multiple prefixes simultaneously. We propose a novel supervised method\nand also an unsupervised method to train the prefixes for single-aspect control\nwhile the combination of these two methods can achieve multi-aspect control.\nExperimental results on both single-aspect and multi-aspect control show that\nour methods can guide generation towards the desired attributes while keeping\nhigh linguistic quality.",
        "pdf_link": "https://arxiv.org/pdf/2202.13257v1.pdf"
    },
    {
        "title": "Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning",
        "authors": [
            "Seonghyeon Lee",
            "Dongha Lee",
            "Seongbo Jang",
            "Hwanjo Yu"
        ],
        "published": "2022-02-26T17:28:02Z",
        "summary": "Recently, finetuning a pretrained language model to capture the similarity\nbetween sentence embeddings has shown the state-of-the-art performance on the\nsemantic textual similarity (STS) task. However, the absence of an\ninterpretation method for the sentence similarity makes it difficult to explain\nthe model output. In this work, we explicitly describe the sentence distance as\nthe weighted sum of contextualized token distances on the basis of a\ntransportation problem, and then present the optimal transport-based distance\nmeasure, named RCMD; it identifies and leverages semantically-aligned token\npairs. In the end, we propose CLRCMD, a contrastive learning framework that\noptimizes RCMD of sentence pairs, which enhances the quality of sentence\nsimilarity and their interpretation. Extensive experiments demonstrate that our\nlearning framework outperforms other baselines on both STS and\ninterpretable-STS benchmarks, indicating that it computes effective sentence\nsimilarity and also provides interpretation consistent with human judgement.\nThe code and checkpoint are publicly available at\nhttps://github.com/sh0416/clrcmd.",
        "pdf_link": "https://arxiv.org/pdf/2202.13196v2.pdf"
    },
    {
        "title": "A Systematic Evaluation of Large Language Models of Code",
        "authors": [
            "Frank F. Xu",
            "Uri Alon",
            "Graham Neubig",
            "Vincent J. Hellendoorn"
        ],
        "published": "2022-02-26T15:53:55Z",
        "summary": "Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.",
        "pdf_link": "https://arxiv.org/pdf/2202.13169v3.pdf"
    },
    {
        "title": "Multi-Level Contrastive Learning for Cross-Lingual Alignment",
        "authors": [
            "Beiduo Chen",
            "Wu Guo",
            "Bin Gu",
            "Quan Liu",
            "Yongchao Wang"
        ],
        "published": "2022-02-26T07:14:20Z",
        "summary": "Cross-language pre-trained models such as multilingual BERT (mBERT) have\nachieved significant performance in various cross-lingual downstream NLP tasks.\nThis paper proposes a multi-level contrastive learning (ML-CTL) framework to\nfurther improve the cross-lingual ability of pre-trained models. The proposed\nmethod uses translated parallel data to encourage the model to generate similar\nsemantic embeddings for different languages. However, unlike the sentence-level\nalignment used in most previous studies, in this paper, we explicitly integrate\nthe word-level information of each pair of parallel sentences into contrastive\nlearning. Moreover, cross-zero noise contrastive estimation (CZ-NCE) loss is\nproposed to alleviate the impact of the floating-point error in the training\nprocess with a small batch size. The proposed method significantly improves the\ncross-lingual transfer ability of our basic model (mBERT) and outperforms on\nmultiple zero-shot cross-lingual downstream tasks compared to the same-size\nmodels in the Xtreme benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2202.13083v1.pdf"
    },
    {
        "title": "Bi-directional Joint Neural Networks for Intent Classification and Slot Filling",
        "authors": [
            "Soyeon Caren Han",
            "Siqu Long",
            "Huichun Li",
            "Henry Weld",
            "Josiah Poon"
        ],
        "published": "2022-02-26T06:35:21Z",
        "summary": "Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks proceeded independently.\nHowever, more recently joint models for intent classification and slot filling\nhave achieved state-of-the-art performance, and have proved that there exists a\nstrong relationship between the two tasks. In this paper, we propose a\nbi-directional joint model for intent classification and slot filling, which\nincludes a multi-stage hierarchical process via BERT and bi-directional joint\nnatural language understanding mechanisms, including intent2slot and\nslot2intent, to obtain mutual performance enhancement between intent\nclassification and slot filling. The evaluations show that our model achieves\nstate-of-the-art results on intent classification accuracy, slot filling F1,\nand significantly improves sentence-level semantic frame accuracy when applied\nto publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).",
        "pdf_link": "https://arxiv.org/pdf/2202.13079v1.pdf"
    },
    {
        "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
        "authors": [
            "Chujie Zheng",
            "Sahand Sabour",
            "Jiaxin Wen",
            "Zheng Zhang",
            "Minlie Huang"
        ],
        "published": "2022-02-26T03:17:08Z",
        "summary": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.13047v3.pdf"
    },
    {
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "authors": [
            "Sewon Min",
            "Xinxi Lyu",
            "Ari Holtzman",
            "Mikel Artetxe",
            "Mike Lewis",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published": "2022-02-25T17:25:19Z",
        "summary": "Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.",
        "pdf_link": "https://arxiv.org/pdf/2202.12837v2.pdf"
    },
    {
        "title": "Exploring Multi-Modal Representations for Ambiguity Detection & Coreference Resolution in the SIMMC 2.0 Challenge",
        "authors": [
            "Javier Chiyah-Garcia",
            "Alessandro Suglia",
            "Jos\u00e9 Lopes",
            "Arash Eshghi",
            "Helen Hastie"
        ],
        "published": "2022-02-25T12:10:02Z",
        "summary": "Anaphoric expressions, such as pronouns and referential descriptions, are\nsituated with respect to the linguistic context of prior turns, as well as, the\nimmediate visual environment. However, a speaker's referential descriptions do\nnot always uniquely identify the referent, leading to ambiguities in need of\nresolution through subsequent clarificational exchanges. Thus, effective\nAmbiguity Detection and Coreference Resolution are key to task success in\nConversational AI. In this paper, we present models for these two tasks as part\nof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT\nand LXMERT based models, compare them to a number of baselines and provide\nablation experiments. Our results show that (1) language models are able to\nexploit correlations in the data to detect ambiguity; and (2) unimodal\ncoreference resolution models can avoid the need for a vision component,\nthrough the use of smart object representations.",
        "pdf_link": "https://arxiv.org/pdf/2202.12645v2.pdf"
    },
    {
        "title": "TrimBERT: Tailoring BERT for Trade-offs",
        "authors": [
            "Sharath Nittur Sridhar",
            "Anthony Sarah",
            "Sairam Sundaresan"
        ],
        "published": "2022-02-24T23:06:29Z",
        "summary": "Models based on BERT have been extremely successful in solving a variety of\nnatural language processing (NLP) tasks. Unfortunately, many of these large\nmodels require a great deal of computational resources and/or time for\npre-training and fine-tuning which limits wider adoptability. While\nself-attention layers have been well-studied, a strong justification for\ninclusion of the intermediate layers which follow them remains missing in the\nliterature. In this work, we show that reducing the number of intermediate\nlayers in BERT-Base results in minimal fine-tuning accuracy loss of downstream\ntasks while significantly decreasing model size and training time. We further\nmitigate two key bottlenecks, by replacing all softmax operations in the\nself-attention layers with a computationally simpler alternative and removing\nhalf of all layernorm operations. This further decreases the training time\nwhile maintaining a high level of fine-tuning accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2202.12411v1.pdf"
    },
    {
        "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
        "authors": [
            "Zhengxuan Wu",
            "Alex Tamkin",
            "Isabel Papadimitriou"
        ],
        "published": "2022-02-24T19:00:39Z",
        "summary": "When we transfer a pretrained language model to a new language, there are\nmany axes of variation that change at once. To disentangle the impact of\ndifferent factors like syntactic similarity and vocabulary similarity, we\npropose a set of controlled transfer studies: we systematically transform the\nlanguage of the GLUE benchmark, altering one axis of crosslingual variation at\na time, and then measure the resulting drops in a pretrained model's downstream\nperformance. We find that models can largely recover from syntactic-style\nshifts, but cannot recover from vocabulary misalignment and embedding matrix\nre-initialization, even with continued pretraining on 15 million tokens. %On\nthe other hand, transferring to a dataset with an unaligned vocabulary is\nextremely hard to recover from in the low-data regime. Moreover, good-quality\ntokenizers in the transfer language do not make vocabulary alignment easier.\nOur experiments provide insights into the factors of cross-lingual transfer\nthat researchers should most focus on when designing language transfer\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2202.12312v2.pdf"
    },
    {
        "title": "Capturing Failures of Large Language Models via Human Cognitive Biases",
        "authors": [
            "Erik Jones",
            "Jacob Steinhardt"
        ],
        "published": "2022-02-24T18:58:52Z",
        "summary": "Large language models generate complex, open-ended outputs: instead of\noutputting a class label they write summaries, generate dialogue, or produce\nworking code. In order to asses the reliability of these open-ended generation\nsystems, we aim to identify qualitative categories of erroneous behavior,\nbeyond identifying individual errors. To hypothesize and test for such\nqualitative errors, we draw inspiration from human cognitive biases --\nsystematic patterns of deviation from rational judgement. Specifically, we use\ncognitive biases as motivation to (i) generate hypotheses for problems that\nmodels may have, and (ii) develop experiments that elicit these problems. Using\ncode generation as a case study, we find that OpenAI's Codex errs predictably\nbased on how the input prompt is framed, adjusts outputs towards anchors, and\nis biased towards outputs that mimic frequent training examples. We then use\nour framework to elicit high-impact errors such as incorrectly deleting files.\nOur results indicate that experimental methodology from cognitive science can\nhelp characterize how machine learning systems behave.",
        "pdf_link": "https://arxiv.org/pdf/2202.12299v2.pdf"
    },
    {
        "title": "Probing BERT's priors with serial reproduction chains",
        "authors": [
            "Takateru Yamakoshi",
            "Thomas L. Griffiths",
            "Robert D. Hawkins"
        ],
        "published": "2022-02-24T17:42:28Z",
        "summary": "Sampling is a promising bottom-up method for exposing what generative models\nhave learned about language, but it remains unclear how to generate\nrepresentative samples from popular masked language models (MLMs) like BERT.\nThe MLM objective yields a dependency network with no guarantee of consistent\nconditional distributions, posing a problem for naive approaches. Drawing from\ntheories of iterated learning in cognitive science, we explore the use of\nserial reproduction chains to sample from BERT's priors. In particular, we\nobserve that a unique and consistent estimator of the ground-truth joint\ndistribution is given by a Generative Stochastic Network (GSN) sampler, which\nrandomly selects which token to mask and reconstruct on each step. We show that\nthe lexical and syntactic statistics of sentences from GSN chains closely match\nthe ground-truth corpus distribution and perform better than other methods in a\nlarge corpus of naturalness judgments. Our findings establish a firmer\ntheoretical foundation for bottom-up probing and highlight richer deviations\nfrom human priors.",
        "pdf_link": "https://arxiv.org/pdf/2202.12226v2.pdf"
    },
    {
        "title": "Ask2Mask: Guided Data Selection for Masked Speech Modeling",
        "authors": [
            "Murali Karthick Baskar",
            "Andrew Rosenberg",
            "Bhuvana Ramabhadran",
            "Yu Zhang",
            "Pedro Moreno"
        ],
        "published": "2022-02-24T17:34:54Z",
        "summary": "Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn\nrepresentations over speech frames which are randomly masked within an\nutterance. While these methods improve performance of Automatic Speech\nRecognition (ASR) systems, they have one major limitation. They treat all\nunsupervised speech samples with equal weight, which hinders learning as not\nall samples have relevant information to learn meaningful representations. In\nthis work, we address this limitation. We propose ask2mask (ATM), a novel\napproach to focus on specific samples during MSM pre-training. ATM employs an\nexternal ASR model or \\textit{scorer} to weight unsupervised input samples in\ntwo different ways: 1) A fine-grained data selection is performed by masking\nover the highly confident input frames as chosen by the scorer. This allows the\nmodel to learn meaningful representations. 2) ATM is further extended to focus\nat utterance-level by weighting the final MSM loss with the utterance-level\nconfidence score. We conduct fine-tuning experiments on two well-benchmarked\ncorpora: LibriSpeech (matching the pre-training data) and Commonvoice,\nTED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results\nsubstantiate the efficacy of ATM on significantly improving the recognition\nperformance under mismatched conditions (up to 11.6\\% relative over published\nresults and upto 4.46\\% relative over our internal baseline) while still\nyielding modest improvements under matched conditions.",
        "pdf_link": "https://arxiv.org/pdf/2202.12719v1.pdf"
    },
    {
        "title": "BERTVision -- A Parameter-Efficient Approach for Question Answering",
        "authors": [
            "Siduo Jiang",
            "Cristopher Benge",
            "William Casey King"
        ],
        "published": "2022-02-24T17:16:25Z",
        "summary": "We present a highly parameter efficient approach for Question Answering that\nsignificantly reduces the need for extended BERT fine-tuning. Our method uses\ninformation from the hidden state activations of each BERT transformer layer,\nwhich is discarded during typical BERT inference. Our best model achieves\nmaximal BERT performance at a fraction of the training time and GPU or TPU\nexpense. Performance is further improved by ensembling our model with BERTs\npredictions. Furthermore, we find that near optimal performance can be achieved\nfor QA span annotation using less training data. Our experiments show that this\napproach works well not only for span annotation, but also for classification,\nsuggesting that it may be extensible to a wider range of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.12210v1.pdf"
    },
    {
        "title": "Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words",
        "authors": [
            "Zhangyin Feng",
            "Duyu Tang",
            "Cong Zhou",
            "Junwei Liao",
            "Shuangzhi Wu",
            "Xiaocheng Feng",
            "Bing Qin",
            "Yunbo Cao",
            "Shuming Shi"
        ],
        "published": "2022-02-24T15:15:48Z",
        "summary": "The standard BERT adopts subword-based tokenization, which may break a word\ninto two or more wordpieces (e.g., converting \"lossless\" to \"loss\" and \"less\").\nThis will bring inconvenience in following situations: (1) what is the best way\nto obtain the contextual vector of a word that is divided into multiple\nwordpieces? (2) how to predict a word via cloze test without knowing the number\nof wordpieces in advance? In this work, we explore the possibility of\ndeveloping BERT-style pretrained model over a vocabulary of words instead of\nwordpieces. We call such word-level BERT model as WordBERT. We train models\nwith different vocabulary sizes, initialization configurations and languages.\nResults show that, compared to standard wordpiece-based BERT, WordBERT makes\nsignificant improvements on cloze test and machine reading comprehension. On\nmany other natural language understanding tasks, including POS tagging,\nchunking and NER, WordBERT consistently performs better than BERT. Model\nanalysis indicates that the major advantage of WordBERT over BERT lies in the\nunderstanding for low-frequency words and rare words. Furthermore, since the\npipeline is language-independent, we train WordBERT for Chinese language and\nobtain significant gains on five natural language understanding datasets.\nLastly, the analyse on inference speed illustrates WordBERT has comparable time\ncost to BERT in natural language understanding tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.12142v1.pdf"
    },
    {
        "title": "Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition",
        "authors": [
            "Xichen Pan",
            "Peiyu Chen",
            "Yichen Gong",
            "Helong Zhou",
            "Xinbing Wang",
            "Zhouhan Lin"
        ],
        "published": "2022-02-24T15:12:17Z",
        "summary": "Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.",
        "pdf_link": "https://arxiv.org/pdf/2203.07996v2.pdf"
    },
    {
        "title": "From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems",
        "authors": [
            "Ilya Jackson",
            "Maria Jesus Saenz"
        ],
        "published": "2022-02-24T14:01:50Z",
        "summary": "Our work is the first attempt to apply Natural Language Processing to\nautomate the development of simulation models of systems vitally important for\nlogistics. We demonstrated that the framework built on top of the fine-tuned\nGPT-3 Codex, a Transformer-based language model, could produce functionally\nvalid simulations of queuing and inventory control systems given the verbal\ndescription. In conducted experiments, GPT-3 Codex demonstrated convincing\nexpertise in Python as well as an understanding of the domain-specific\nvocabulary. As a result, the language model could produce simulations of a\nsingle-product inventory-control system and single-server queuing system given\nthe domain-specific context, a detailed description of the process, and a list\nof variables with the corresponding values. The demonstrated results, along\nwith the rapid improvement of language models, open the door for significant\nsimplification of the workflow behind the simulation model development, which\nwill allow experts to focus on the high-level consideration of the problem and\nholistic thinking.",
        "pdf_link": "https://arxiv.org/pdf/2202.12107v3.pdf"
    },
    {
        "title": "Using calibrator to improve robustness in Machine Reading Comprehension",
        "authors": [
            "Jing Jin",
            "Houfeng Wang"
        ],
        "published": "2022-02-24T02:16:42Z",
        "summary": "Machine Reading Comprehension(MRC) has achieved a remarkable result since\nsome powerful models, such as BERT, are proposed. However, these models are not\nrobust enough and vulnerable to adversarial input perturbation and\ngeneralization examples. Some works tried to improve the performance on\nspecific types of data by adding some related examples into training data while\nit leads to degradation on the original dataset, because the shift of data\ndistribution makes the answer ranking based on the softmax probability of model\nunreliable. In this paper, we propose a method to improve the robustness by\nusing a calibrator as the post-hoc reranker, which is implemented based on\nXGBoost model. The calibrator combines both manual features and representation\nlearning features to rerank candidate results. Experimental results on\nadversarial datasets show that our model can achieve performance improvement by\nmore than 10\\% and also make improvement on the original and generalization\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2202.11865v1.pdf"
    },
    {
        "title": "Sky Computing: Accelerating Geo-distributed Computing in Federated Learning",
        "authors": [
            "Jie Zhu",
            "Shenggui Li",
            "Yang You"
        ],
        "published": "2022-02-24T00:14:38Z",
        "summary": "Federated learning is proposed by Google to safeguard data privacy through\ntraining models locally on users' devices. However, with deep learning models\ngrowing in size to achieve better results, it becomes increasingly difficult to\naccommodate the whole model on one single device. Thus, model parallelism is\nthen used to divide the model weights among several devices. With this logic,\nthe approach currently used evenly allocates weights among devices. However, in\nreality, a computation bottleneck may occur resulting from variant computing\npower of different users' devices. To address this problem, load balancing is\nneeded to allocate the model weights based on the computational capability of\nthe device. In this paper, we proposed Sky Computing, a load-balanced model\nparallelism framework to adaptively allocate the weights to devices. Sky\nComputing outperforms the baseline method by 55% in training time when training\n160-layer BERT with 64 nodes. The source code can be found at\nhttps://github.com/hpcaitech/SkyComputing.",
        "pdf_link": "https://arxiv.org/pdf/2202.11836v1.pdf"
    },
    {
        "title": "Consistent Dropout for Policy Gradient Reinforcement Learning",
        "authors": [
            "Matthew Hausknecht",
            "Nolan Wagener"
        ],
        "published": "2022-02-23T23:00:40Z",
        "summary": "Dropout has long been a staple of supervised learning, but is rarely used in\nreinforcement learning. We analyze why naive application of dropout is\nproblematic for policy-gradient learning algorithms and introduce consistent\ndropout, a simple technique to address this instability. We demonstrate\nconsistent dropout enables stable training with A2C and PPO in both continuous\nand discrete action environments across a wide range of dropout probabilities.\nFinally, we show that consistent dropout enables the online training of complex\narchitectures such as GPT without needing to disable the model's native\ndropout.",
        "pdf_link": "https://arxiv.org/pdf/2202.11818v1.pdf"
    },
    {
        "title": "Knowledge Augmented BERT Mutual Network in Multi-turn Spoken Dialogues",
        "authors": [
            "Ting-Wei Wu",
            "Biing-Hwang Juang"
        ],
        "published": "2022-02-23T04:03:35Z",
        "summary": "Modern spoken language understanding (SLU) systems rely on sophisticated\nsemantic notions revealed in single utterances to detect intents and slots.\nHowever, they lack the capability of modeling multi-turn dynamics within a\ndialogue particularly in long-term slot contexts. Without external knowledge,\ndepending on limited linguistic legitimacy within a word sequence may overlook\ndeep semantic information across dialogue turns. In this paper, we propose to\nequip a BERT-based joint model with a knowledge attention module to mutually\nleverage dialogue contexts between two SLU tasks. A gating mechanism is further\nutilized to filter out irrelevant knowledge triples and to circumvent\ndistracting comprehension. Experimental results in two complicated multi-turn\ndialogue datasets have demonstrate by mutually modeling two SLU tasks with\nfiltered knowledge and dialogue contexts, our approach has considerable\nimprovements compared with several competitive baselines.",
        "pdf_link": "https://arxiv.org/pdf/2202.11299v1.pdf"
    },
    {
        "title": "Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals",
        "authors": [
            "Thilo Hagendorff",
            "Leonie Bossert",
            "Tse Yip Fai",
            "Peter Singer"
        ],
        "published": "2022-02-22T12:23:21Z",
        "summary": "Massive efforts are made to reduce biases in both data and algorithms in\norder to render AI applications fair. These efforts are propelled by various\nhigh-profile cases where biased algorithmic decision-making caused harm to\nwomen, people of color, minorities, etc. However, the AI fairness field still\nsuccumbs to a blind spot, namely its insensitivity to discrimination against\nanimals. This paper is the first to describe the 'speciesist bias' and\ninvestigate it in several different AI systems. Speciesist biases are learned\nand solidified by AI applications when they are trained on datasets in which\nspeciesist patterns prevail. These patterns can be found in image recognition\nsystems, large language models, and recommender systems. Therefore, AI\ntechnologies currently play a significant role in perpetuating and normalizing\nviolence against animals. This can only be changed when AI fairness frameworks\nwiden their scope and include mitigation measures for speciesist biases. This\npaper addresses the AI community in this regard and stresses the influence AI\nsystems can have on either increasing or reducing the violence that is\ninflicted on animals, and especially on farmed animals.",
        "pdf_link": "https://arxiv.org/pdf/2202.10848v1.pdf"
    },
    {
        "title": "Improving CTC-based speech recognition via knowledge transferring from pre-trained language models",
        "authors": [
            "Keqi Deng",
            "Songjun Cao",
            "Yike Zhang",
            "Long Ma",
            "Gaofeng Cheng",
            "Ji Xu",
            "Pengyuan Zhang"
        ],
        "published": "2022-02-22T11:30:55Z",
        "summary": "Recently, end-to-end automatic speech recognition models based on\nconnectionist temporal classification (CTC) have achieved impressive results,\nespecially when fine-tuned from wav2vec2.0 models. Due to the conditional\nindependence assumption, CTC-based models are always weaker than\nattention-based encoder-decoder models and require the assistance of external\nlanguage models (LMs). To solve this issue, we propose two knowledge\ntransferring methods that leverage pre-trained LMs, such as BERT and GPT2, to\nimprove CTC-based models. The first method is based on representation learning,\nin which the CTC-based models use the representation produced by BERT as an\nauxiliary learning target. The second method is based on joint classification\nlearning, which combines GPT2 for text modeling with a hybrid CTC/attention\narchitecture. Experiment on AISHELL-1 corpus yields a character error rate\n(CER) of 4.2% on the test set. When compared to the vanilla CTC-based models\nfine-tuned from the wav2vec2.0 models, our knowledge transferring method\nreduces CER by 16.1% relatively without external LMs.",
        "pdf_link": "https://arxiv.org/pdf/2203.03582v1.pdf"
    },
    {
        "title": "Korean Tokenization for Beam Search Rescoring in Speech Recognition",
        "authors": [
            "Kyuhong Shim",
            "Hyewon Bae",
            "Wonyong Sung"
        ],
        "published": "2022-02-22T11:25:01Z",
        "summary": "The performance of automatic speech recognition (ASR) models can be greatly\nimproved by proper beam-search decoding with external language model (LM).\nThere has been an increasing interest in Korean speech recognition, but not\nmany studies have been focused on the decoding procedure. In this paper, we\npropose a Korean tokenization method for neural network-based LM used for\nKorean ASR. Although the common approach is to use the same tokenization method\nfor external LM as the ASR model, we show that it may not be the best choice\nfor Korean. We propose a new tokenization method that inserts a special token,\nSkipTC, when there is no trailing consonant in a Korean syllable. By utilizing\nthe proposed SkipTC token, the input sequence for LM becomes very regularly\npatterned so that the LM can better learn the linguistic characteristics. Our\nexperiments show that the proposed approach achieves a lower word error rate\ncompared to the same LM model without SkipTC. In addition, we are the first to\nreport the ASR performance for the recently introduced large-scale 7,600h\nKorean speech dataset.",
        "pdf_link": "https://arxiv.org/pdf/2203.03583v2.pdf"
    },
    {
        "title": "VU-BERT: A Unified framework for Visual Dialog",
        "authors": [
            "Tong Ye",
            "Shijing Si",
            "Jianzong Wang",
            "Rui Wang",
            "Ning Cheng",
            "Jing Xiao"
        ],
        "published": "2022-02-22T10:20:14Z",
        "summary": "The visual dialog task attempts to train an agent to answer multi-turn\nquestions given an image, which requires the deep understanding of interactions\nbetween the image and dialog history. Existing researches tend to employ the\nmodality-specific modules to model the interactions, which might be troublesome\nto use. To fill in this gap, we propose a unified framework for image-text\njoint embedding, named VU-BERT, and apply patch projection to obtain vision\nembedding firstly in visual dialog tasks to simplify the model. The model is\ntrained over two tasks: masked language modeling and next utterance retrieval.\nThese tasks help in learning visual concepts, utterances dependence, and the\nrelationships between these two modalities. Finally, our VU-BERT achieves\ncompetitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.",
        "pdf_link": "https://arxiv.org/pdf/2202.10787v1.pdf"
    },
    {
        "title": "Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users",
        "authors": [
            "Anne Kreuter",
            "Kai Sassenberg",
            "Roman Klinger"
        ],
        "published": "2022-02-21T18:24:59Z",
        "summary": "Machine-learned models for author profiling in social media often rely on\ndata acquired via self-reporting-based psychometric tests (questionnaires)\nfilled out by social media users. This is an expensive but accurate data\ncollection strategy. Another, less costly alternative, which leads to\npotentially more noisy and biased data, is to rely on labels inferred from\npublicly available information in the profiles of the users, for instance\nself-reported diagnoses or test results. In this paper, we explore a third\nstrategy, namely to directly use a corpus of items from validated psychometric\ntests as training data. Items from psychometric tests often consist of\nsentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora\nof test items constitute 'small data', but their availability for many concepts\nis a rich resource. We investigate this approach for personality profiling, and\nevaluate BERT classifiers fine-tuned on such psychometric test items for the\nbig five personality traits (openness, conscientiousness, extraversion,\nagreeableness, neuroticism) and analyze various augmentation strategies\nregarding their potential to address the challenges coming with such a small\ncorpus. Our evaluation on a publicly available Twitter corpus shows a\ncomparable performance to in-domain training for 4/5 personality traits with\nT5-based data augmentation.",
        "pdf_link": "https://arxiv.org/pdf/2202.10415v3.pdf"
    },
    {
        "title": "BERT WEAVER: Using WEight AVERaging to enable lifelong learning for transformer-based models in biomedical semantic search engines",
        "authors": [
            "Lisa K\u00fchnel",
            "Alexander Schulz",
            "Barbara Hammer",
            "Juliane Fluck"
        ],
        "published": "2022-02-21T10:34:41Z",
        "summary": "Recent developments in transfer learning have boosted the advancements in\nnatural language processing tasks. The performance is, however, dependent on\nhigh-quality, manually annotated training data. Especially in the biomedical\ndomain, it has been shown that one training corpus is not enough to learn\ngeneric models that are able to efficiently predict on new data. Therefore, in\norder to be used in real world applications state-of-the-art models need the\nability of lifelong learning to improve performance as soon as new data are\navailable - without the need of re-training the whole model from scratch. We\npresent WEAVER, a simple, yet efficient post-processing method that infuses old\nknowledge into the new model, thereby reducing catastrophic forgetting. We show\nthat applying WEAVER in a sequential manner results in similar word embedding\ndistributions as doing a combined training on all data at once, while being\ncomputationally more efficient. Because there is no need of data sharing, the\npresented method is also easily applicable to federated learning settings and\ncan for example be beneficial for the mining of electronic health records from\ndifferent clinics.",
        "pdf_link": "https://arxiv.org/pdf/2202.10101v3.pdf"
    },
    {
        "title": "Adaptive Discounting of Implicit Language Models in RNN-Transducers",
        "authors": [
            "Vinit Unni",
            "Shreya Khare",
            "Ashish Mittal",
            "Preethi Jyothi",
            "Sunita Sarawagi",
            "Samarth Bharadwaj"
        ],
        "published": "2022-02-21T08:44:56Z",
        "summary": "RNN-Transducer (RNN-T) models have become synonymous with streaming\nend-to-end ASR systems. While they perform competitively on a number of\nevaluation categories, rare words pose a serious challenge to RNN-T models. One\nmain reason for the degradation in performance on rare words is that the\nlanguage model (LM) internal to RNN-Ts can become overconfident and lead to\nhallucinated predictions that are acoustically inconsistent with the underlying\nspeech. To address this issue, we propose a lightweight adaptive LM discounting\ntechnique AdaptLMD, that can be used with any RNN-T architecture without\nrequiring any external resources or additional parameters. AdaptLMD uses a\ntwo-pronged approach: 1) Randomly mask the prediction network output to\nencourage the RNN-T to not be overly reliant on it's outputs. 2) Dynamically\nchoose when to discount the implicit LM (ILM) based on rarity of recently\npredicted tokens and divergence between ILM and implicit acoustic model (IAM)\nscores. Comparing AdaptLMD to a competitive RNN-T baseline, we obtain up to 4%\nand 14% relative reductions in overall WER and rare word PER, respectively, on\na conversational, code-mixed Hindi-English ASR task.",
        "pdf_link": "https://arxiv.org/pdf/2203.02317v1.pdf"
    },
    {
        "title": "Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning",
        "authors": [
            "Zhecheng Yuan",
            "Guozheng Ma",
            "Yao Mu",
            "Bo Xia",
            "Bo Yuan",
            "Xueqian Wang",
            "Ping Luo",
            "Huazhe Xu"
        ],
        "published": "2022-02-21T04:22:07Z",
        "summary": "One of the key challenges in visual Reinforcement Learning (RL) is to learn\npolicies that can generalize to unseen environments. Recently, data\naugmentation techniques aiming at enhancing data diversity have demonstrated\nproven performance in improving the generalization ability of learned policies.\nHowever, due to the sensitivity of RL training, naively applying data\naugmentation, which transforms each pixel in a task-agnostic manner, may suffer\nfrom instability and damage the sample efficiency, thus further exacerbating\nthe generalization performance. At the heart of this phenomenon is the diverged\naction distribution and high-variance value estimation in the face of augmented\nimages. To alleviate this issue, we propose Task-aware Lipschitz Data\nAugmentation (TLDA) for visual RL, which explicitly identifies the\ntask-correlated pixels with large Lipschitz constants, and only augments the\ntask-irrelevant pixels. To verify the effectiveness of TLDA, we conduct\nextensive experiments on DeepMind Control suite, CARLA and DeepMind\nManipulation tasks, showing that TLDA improves both sample efficiency in\ntraining time and generalization in test time. It outperforms previous\nstate-of-the-art methods across the 3 different visual control benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2202.09982v2.pdf"
    },
    {
        "title": "StyleBERT: Chinese pretraining by font style information",
        "authors": [
            "Chao Lv",
            "Han Zhang",
            "XinKai Du",
            "Yunhao Zhang",
            "Ying Huang",
            "Wenhao Li",
            "Jia Han",
            "Shanshan Gu"
        ],
        "published": "2022-02-21T02:45:12Z",
        "summary": "With the success of down streaming task using English pre-trained language\nmodel, the pre-trained Chinese language model is also necessary to get a better\nperformance of Chinese NLP task. Unlike the English language, Chinese has its\nspecial characters such as glyph information. So in this article, we propose\nthe Chinese pre-trained language model StyleBERT which incorporate the\nfollowing embedding information to enhance the savvy of language model, such as\nword, pinyin, five stroke and chaizi. The experiments show that the model\nachieves well performances on a wide range of Chinese NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.09955v2.pdf"
    },
    {
        "title": "GPT-based Open-Ended Knowledge Tracing",
        "authors": [
            "Naiming Liu",
            "Zichao Wang",
            "Richard G. Baraniuk",
            "Andrew Lan"
        ],
        "published": "2022-02-21T02:33:34Z",
        "summary": "In education applications, knowledge tracing refers to the problem of\nestimating students' time-varying concept/skill mastery level from their past\nresponses to questions and predicting their future performance. One key\nlimitation of most existing knowledge tracing methods is that they treat\nstudent responses to questions as binary-valued, i.e., whether they are correct\nor incorrect. Response correctness analysis/prediction ignores important\ninformation on student knowledge contained in the exact content of the\nresponses, especially for open-ended questions. In this paper, we conduct the\nfirst exploration into open-ended knowledge tracing (OKT) by studying the new\ntask of predicting students' exact open-ended responses to questions. Our work\nis grounded in the domain of computer science education with programming\nquestions. We develop an initial solution to the OKT problem, a student\nknowledge-guided code generation approach, that combines program synthesis\nmethods using language models with student knowledge tracing methods. We also\nconduct a series of quantitative and qualitative experiments on a real-world\nstudent code dataset to validate OKT and demonstrate its promise in educational\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2203.03716v4.pdf"
    },
    {
        "title": "Contextual Semantic Embeddings for Ontology Subsumption Prediction",
        "authors": [
            "Jiaoyan Chen",
            "Yuan He",
            "Yuxia Geng",
            "Ernesto Jimenez-Ruiz",
            "Hang Dong",
            "Ian Horrocks"
        ],
        "published": "2022-02-20T11:14:04Z",
        "summary": "Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is able to predict multiple kinds of subsumers including\nnamed classes from the same ontology or another ontology, and existential\nrestrictions from the same ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2202.09791v4.pdf"
    },
    {
        "title": "Reward Modeling for Mitigating Toxicity in Transformer-based Language Models",
        "authors": [
            "Farshid Faal",
            "Ketra Schmitt",
            "Jia Yuan Yu"
        ],
        "published": "2022-02-19T19:26:22Z",
        "summary": "Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.",
        "pdf_link": "https://arxiv.org/pdf/2202.09662v6.pdf"
    },
    {
        "title": "Do Transformers know symbolic rules, and would we know if they did?",
        "authors": [
            "Tommi Gr\u00f6ndahl",
            "Yujia Guo",
            "N. Asokan"
        ],
        "published": "2022-02-19T09:56:38Z",
        "summary": "To improve the explainability of leading Transformer networks used in NLP, it\nis important to tease apart genuine symbolic rules from merely associative\ninput-output patterns. However, we identify several inconsistencies in how\n``symbolicity'' has been construed in recent NLP literature. To mitigate this\nproblem, we propose two criteria to be the most relevant, one pertaining to a\nsystem's internal architecture and the other to the dissociation between\nabstract rules and specific input identities. From this perspective, we\ncritically examine prior work on the symbolic capacities of Transformers, and\ndeem the results to be fundamentally inconclusive for reasons inherent in\nexperiment design. We further maintain that there is no simple fix to this\nproblem, since it arises -- to an extent -- in all end-to-end settings.\nNonetheless, we emphasize the need for more robust evaluation of whether\nnon-symbolic explanations exist for success in seemingly symbolic tasks. To\nfacilitate this, we experiment on four sequence modelling tasks on the T5\nTransformer in two experiment settings: zero-shot generalization, and\ngeneralization across class-specific vocabularies flipped between the training\nand test set. We observe that T5's generalization is markedly stronger in\nsequence-to-sequence tasks than in comparable classification tasks. Based on\nthis, we propose a thus far overlooked analysis, where the Transformer itself\ndoes not need to be symbolic to be part of a symbolic architecture as the\nprocessor, operating on the input and output as external memory components.",
        "pdf_link": "https://arxiv.org/pdf/2203.00162v3.pdf"
    },
    {
        "title": "From FreEM to D'AlemBERT: a Large Corpus and a Language Model for Early Modern French",
        "authors": [
            "Simon Gabay",
            "Pedro Ortiz Suarez",
            "Alexandre Bartz",
            "Alix Chagu\u00e9",
            "Rachel Bawden",
            "Philippe Gambette",
            "Beno\u00eet Sagot"
        ],
        "published": "2022-02-18T22:17:22Z",
        "summary": "Language models for historical states of language are becoming increasingly\nimportant to allow the optimal digitisation and analysis of old textual\nsources. Because these historical states are at the same time more complex to\nprocess and more scarce in the corpora available, specific efforts are\nnecessary to train natural language processing (NLP) tools adapted to the data.\nIn this paper, we present our efforts to develop NLP tools for Early Modern\nFrench (historical French from the 16$^\\text{th}$ to the 18$^\\text{th}$\ncenturies). We present the $\\text{FreEM}_{\\text{max}}$ corpus of Early Modern\nFrench and D'AlemBERT, a RoBERTa-based language model trained on\n$\\text{FreEM}_{\\text{max}}$. We evaluate the usefulness of D'AlemBERT by\nfine-tuning it on a part-of-speech tagging task, outperforming previous work on\nthe test set. Importantly, we find evidence for the transfer learning capacity\nof the language model, since its performance on lesser-resourced time periods\nappears to have been boosted by the more resourced ones. We release D'AlemBERT\nand the open-sourced subpart of the $\\text{FreEM}_{\\text{max}}$ corpus.",
        "pdf_link": "https://arxiv.org/pdf/2202.09452v1.pdf"
    },
    {
        "title": "Mixture-of-Experts with Expert Choice Routing",
        "authors": [
            "Yanqi Zhou",
            "Tao Lei",
            "Hanxiao Liu",
            "Nan Du",
            "Yanping Huang",
            "Vincent Zhao",
            "Andrew Dai",
            "Zhifeng Chen",
            "Quoc Le",
            "James Laudon"
        ],
        "published": "2022-02-18T17:46:11Z",
        "summary": "Sparsely-activated Mixture-of-experts (MoE) models allow the number of\nparameters to greatly increase while keeping the amount of computation for a\ngiven token or a given sample unchanged. However, a poor expert routing\nstrategy (e.g. one resulting in load imbalance) can cause certain experts to be\nunder-trained, leading to an expert being under or over-specialized. Prior work\nallocates a fixed number of experts to each token using a top-k function\nregardless of the relative importance of different tokens. To address this, we\npropose a heterogeneous mixture-of-experts employing an expert choice method.\nInstead of letting tokens select the top-k experts, we have experts selecting\nthe top-k tokens. As a result, each token can be routed to a variable number of\nexperts and each expert can have a fixed bucket size. We systematically study\npre-training speedups using the same computational resources of the Switch\nTransformer top-1 and GShard top-2 gating of prior work and find that our\nmethod improves training convergence time by more than 2x. For the same\ncomputational cost, our method demonstrates higher performance in fine-tuning\n11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller\nactivation cost, our method outperforms the T5 dense model in 7 out of the 11\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.09368v2.pdf"
    },
    {
        "title": "Evaluating the Construct Validity of Text Embeddings with Application to Survey Questions",
        "authors": [
            "Qixiang Fang",
            "Dong Nguyen",
            "Daniel L Oberski"
        ],
        "published": "2022-02-18T12:35:46Z",
        "summary": "Text embedding models from Natural Language Processing can map text data\n(e.g. words, sentences, documents) to supposedly meaningful numerical\nrepresentations (a.k.a. text embeddings). While such models are increasingly\napplied in social science research, one important issue is often not addressed:\nthe extent to which these embeddings are valid representations of constructs\nrelevant for social science research. We therefore propose the use of the\nclassic construct validity framework to evaluate the validity of text\nembeddings. We show how this framework can be adapted to the opaque and\nhigh-dimensional nature of text embeddings, with application to survey\nquestions. We include several popular text embedding methods (e.g. fastText,\nGloVe, BERT, Sentence-BERT, Universal Sentence Encoder) in our construct\nvalidity analyses. We find evidence of convergent and discriminant validity in\nsome cases. We also show that embeddings can be used to predict respondent's\nanswers to completely new survey questions. Furthermore, BERT-based embedding\ntechniques and the Universal Sentence Encoder provide more valid\nrepresentations of survey questions than do others. Our results thus highlight\nthe necessity to examine the construct validity of text embeddings before\ndeploying them in social science research.",
        "pdf_link": "https://arxiv.org/pdf/2202.09166v1.pdf"
    },
    {
        "title": "AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification",
        "authors": [
            "Da Li",
            "Ming Yi",
            "Yukai He"
        ],
        "published": "2022-02-18T09:41:37Z",
        "summary": "Women are influential online, especially in image-based social media such as\nTwitter and Instagram. However, many in the network environment contain gender\ndiscrimination and aggressive information, which magnify gender stereotypes and\ngender inequality. Therefore, the filtering of illegal content such as gender\ndiscrimination is essential to maintain a healthy social network environment.\nIn this paper, we describe the system developed by our team for SemEval-2022\nTask 5: Multimedia Automatic Misogyny Identification. More specifically, we\nintroduce two novel system to analyze these posts: a multimodal multi-task\nlearning architecture that combines Bertweet for text encoding with ResNet-18\nfor image representation, and a single-flow transformer structure which\ncombines text embeddings from BERT-Embeddings and image embeddings from several\ndifferent modules such as EfficientNet and ResNet. In this manner, we show that\nthe information behind them can be properly revealed. Our approach achieves\ngood performance on each of the two subtasks of the current competition,\nranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706\nmacro F1-score) while exceeding the official baseline results by high margins.",
        "pdf_link": "https://arxiv.org/pdf/2202.09099v2.pdf"
    },
    {
        "title": "SGPT: GPT Sentence Embeddings for Semantic Search",
        "authors": [
            "Niklas Muennighoff"
        ],
        "published": "2022-02-17T21:35:56Z",
        "summary": "Decoder transformers have continued increasing in scale reaching hundreds of\nbillions of parameters. Due to their scale the same decoder sets\nstate-of-the-art results on various language tasks via prompting or\nfine-tuning. Yet, these large foundation models remain unusable for the related\nfields of semantic search and sentence embeddings. This prevents possibly new\nstate-of-the-art results and forces organizations to train and maintain\nseparate models. To this end, we propose SGPT to use decoders for sentence\nembeddings and semantic search via prompting or fine-tuning. At 5.8 billion\nparameters SGPT improves on the previously best sentence embeddings by a margin\nof 7% and outperforms a concurrent method with 175 billion parameters as\nmeasured on the BEIR search benchmark. Code, models and result files are freely\navailable at https://github.com/Muennighoff/sgpt.",
        "pdf_link": "https://arxiv.org/pdf/2202.08904v5.pdf"
    },
    {
        "title": "An alternative approach to train neural networks using monotone variational inequality",
        "authors": [
            "Chen Xu",
            "Xiuyuan Cheng",
            "Yao Xie"
        ],
        "published": "2022-02-17T19:24:20Z",
        "summary": "We propose an alternative approach to neural network training using the\nmonotone vector field, an idea inspired by the seminal work of Juditsky and\nNemirovski [Juditsky & Nemirovsky, 2019] developed originally to solve\nparameter estimation problems for generalized linear models (GLM) by reducing\nthe original non-convex problem to a convex problem of solving a monotone\nvariational inequality (VI). Our approach leads to computationally efficient\nprocedures that converge fast and offer guarantee in some special cases, such\nas training a single-layer neural network or fine-tuning the last layer of the\npre-trained model. Our approach can be used for more efficient fine-tuning of a\npre-trained model while freezing the bottom layers, an essential step for\ndeploying many machine learning models such as large language models (LLM). We\ndemonstrate its applicability in training fully-connected (FC) neural networks,\ngraph neural networks (GNN), and convolutional neural networks (CNN) and show\nthe competitive or better performance of our approach compared to stochastic\ngradient descent methods on both synthetic and real network data prediction\ntasks regarding various performance metrics.",
        "pdf_link": "https://arxiv.org/pdf/2202.08876v4.pdf"
    },
    {
        "title": "LAMP: Extracting Text from Gradients with Language Model Priors",
        "authors": [
            "Mislav Balunovi\u0107",
            "Dimitar I. Dimitrov",
            "Nikola Jovanovi\u0107",
            "Martin Vechev"
        ],
        "published": "2022-02-17T18:49:25Z",
        "summary": "Recent work shows that sensitive user data can be reconstructed from gradient\nupdates, breaking the key privacy promise of federated learning. While success\nwas demonstrated primarily on image data, these methods do not directly\ntransfer to other domains such as text. In this work, we propose LAMP, a novel\nattack tailored to textual data, that successfully reconstructs original text\nfrom gradients. Our attack is based on two key insights: (i) modeling prior\ntext probability with an auxiliary language model, guiding the search towards\nmore natural text, and (ii) alternating continuous and discrete optimization,\nwhich minimizes reconstruction loss on embeddings, while avoiding local minima\nby applying discrete text transformations. Our experiments demonstrate that\nLAMP is significantly more effective than prior work: it reconstructs 5x more\nbigrams and 23% longer subsequences on average. Moreover, we are the first to\nrecover inputs from batch sizes larger than 1 for textual models. These\nfindings indicate that gradient updates of models operating on textual data\nleak more information than previously thought.",
        "pdf_link": "https://arxiv.org/pdf/2202.08827v2.pdf"
    },
    {
        "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models",
        "authors": [
            "Da Yin",
            "Li Dong",
            "Hao Cheng",
            "Xiaodong Liu",
            "Kai-Wei Chang",
            "Furu Wei",
            "Jianfeng Gao"
        ],
        "published": "2022-02-17T17:17:43Z",
        "summary": "With the increasing of model capacity brought by pre-trained language models,\nthere emerges boosting needs for more knowledgeable natural language processing\n(NLP) models with advanced functionalities including providing and making\nflexible use of encyclopedic and commonsense knowledge. The mere pre-trained\nlanguage models, however, lack the capacity of handling such\nknowledge-intensive NLP tasks alone. To address this challenge, large numbers\nof pre-trained language models augmented with external knowledge sources are\nproposed and in rapid development. In this paper, we aim to summarize the\ncurrent progress of pre-trained language model-based knowledge-enhanced models\n(PLMKEs) by dissecting their three vital elements: knowledge sources,\nknowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we\npresent the challenges of PLMKEs based on the discussion regarding the three\nelements and attempt to provide NLP practitioners with potential directions for\nfurther research.",
        "pdf_link": "https://arxiv.org/pdf/2202.08772v1.pdf"
    },
    {
        "title": "Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing",
        "authors": [
            "Yi Nian",
            "Xinyue Hu",
            "Rui Zhang",
            "Jingna Feng",
            "Jingcheng Du",
            "Fang Li",
            "Yong Chen",
            "Cui Tao"
        ],
        "published": "2022-02-17T15:33:27Z",
        "summary": "To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.",
        "pdf_link": "https://arxiv.org/pdf/2202.08712v4.pdf"
    },
    {
        "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph",
        "authors": [
            "Han Shi",
            "Jiahui Gao",
            "Hang Xu",
            "Xiaodan Liang",
            "Zhenguo Li",
            "Lingpeng Kong",
            "Stephen M. S. Lee",
            "James T. Kwok"
        ],
        "published": "2022-02-17T12:20:52Z",
        "summary": "Recently over-smoothing phenomenon of Transformer-based models is observed in\nboth vision and language fields. However, no existing work has delved deeper to\nfurther investigate the main cause of this phenomenon. In this work, we make\nthe attempt to analyze the over-smoothing problem from the perspective of\ngraph, where such problem was first discovered and explored. Intuitively, the\nself-attention matrix can be seen as a normalized adjacent matrix of a\ncorresponding graph. Based on the above connection, we provide some theoretical\nanalysis and find that layer normalization plays a key role in the\nover-smoothing issue of Transformer-based models. Specifically, if the standard\ndeviation of layer normalization is sufficiently large, the output of\nTransformer stacks will converge to a specific low-rank subspace and result in\nover-smoothing. To alleviate the over-smoothing problem, we consider\nhierarchical fusion strategies, which combine the representations from\ndifferent layers adaptively to make the output more diverse. Extensive\nexperiment results on various data sets illustrate the effect of our fusion\nmethod.",
        "pdf_link": "https://arxiv.org/pdf/2202.08625v1.pdf"
    },
    {
        "title": "When BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing",
        "authors": [
            "Chao-Han Huck Yang",
            "Jun Qi",
            "Samuel Yen-Chi Chen",
            "Yu Tsao",
            "Pin-Yu Chen"
        ],
        "published": "2022-02-17T09:55:21Z",
        "summary": "The rapid development of quantum computing has demonstrated many unique\ncharacteristics of quantum advantages, such as richer feature representation\nand more secured protection on model parameters. This work proposes a vertical\nfederated learning architecture based on variational quantum circuits to\ndemonstrate the competitive performance of a quantum-enhanced pre-trained BERT\nmodel for text classification. In particular, our proposed hybrid\nclassical-quantum model consists of a novel random quantum temporal convolution\n(QTC) learning framework replacing some layers in the BERT-based decoder. Our\nexperiments on intent classification show that our proposed BERT-QTC model\nattains competitive experimental results in the Snips and ATIS spoken language\ndatasets. Particularly, the BERT-QTC boosts the performance of the existing\nquantum circuit-based language model in two text classification datasets by\n1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasibly\ndeployed on both existing commercial-accessible quantum computation hardware\nand CPU-based interface for ensuring data isolation.",
        "pdf_link": "https://arxiv.org/pdf/2203.03550v1.pdf"
    },
    {
        "title": "Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer",
        "authors": [
            "Yin Fang",
            "Zhuo Chen",
            "Xiaohui Fan",
            "Ningyu Zhang"
        ],
        "published": "2022-02-17T06:18:02Z",
        "summary": "Machine learning, notably deep learning, has significantly propelled\nmolecular investigations within the biochemical sphere. Traditionally, modeling\nfor such research has centered around a handful of paradigms. For instance, the\nprediction paradigm is frequently deployed for tasks such as molecular property\nprediction. To enhance the generation and decipherability of purely data-driven\nmodels, scholars have integrated biochemical domain knowledge into these\nmolecular study models. This integration has sparked a surge in paradigm\ntransfer, which is solving one molecular learning task by reformulating it as\nanother one. With the emergence of Large Language Models, these paradigms have\ndemonstrated an escalating trend towards harmonized unification. In this work,\nwe delineate a literature survey focused on knowledge-informed molecular\nlearning from the perspective of paradigm transfer. We classify the paradigms,\nscrutinize their methodologies, and dissect the contribution of domain\nknowledge. Moreover, we encapsulate prevailing trends and identify intriguing\navenues for future exploration in molecular learning.",
        "pdf_link": "https://arxiv.org/pdf/2202.10587v2.pdf"
    },
    {
        "title": "FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction",
        "authors": [
            "Minh Van Nguyen",
            "Nghia Trung Ngo",
            "Bonan Min",
            "Thien Huu Nguyen"
        ],
        "published": "2022-02-16T20:11:31Z",
        "summary": "This paper presents FAMIE, a comprehensive and efficient active learning (AL)\ntoolkit for multilingual information extraction. FAMIE is designed to address a\nfundamental problem in existing AL frameworks where annotators need to wait for\na long time between annotation batches due to the time-consuming nature of\nmodel training and data selection at each AL iteration. This hinders the\nengagement, productivity, and efficiency of annotators. Based on the idea of\nusing a small proxy network for fast data selection, we introduce a novel\nknowledge distillation mechanism to synchronize the proxy network with the main\nlarge model (i.e., BERT-based) to ensure the appropriateness of the selected\nannotation examples for the main model. Our AL framework can support multiple\nlanguages. The experiments demonstrate the advantages of FAMIE in terms of\ncompetitive performance and time efficiency for sequence labeling with AL. We\npublicly release our code (\\url{https://github.com/nlp-uoregon/famie}) and demo\nwebsite (\\url{http://nlp.uoregon.edu:9000/}). A demo video for FAMIE is\nprovided at: \\url{https://youtu.be/I2i8n_jAyrY}.",
        "pdf_link": "https://arxiv.org/pdf/2202.08316v2.pdf"
    },
    {
        "title": "GraphNLI: A Graph-based Natural Language Inference Model for Polarity Prediction in Online Debates",
        "authors": [
            "Vibhor Agarwal",
            "Sagar Joglekar",
            "Anthony P. Young",
            "Nishanth Sastry"
        ],
        "published": "2022-02-16T16:26:21Z",
        "summary": "Online forums that allow participatory engagement between users have been\ntransformative for public discussion of important issues. However, debates on\nsuch forums can sometimes escalate into full blown exchanges of hate or\nmisinformation. An important tool in understanding and tackling such problems\nis to be able to infer the argumentative relation of whether a reply is\nsupporting or attacking the post it is replying to. This so called polarity\nprediction task is difficult because replies may be based on external context\nbeyond a post and the reply whose polarity is being predicted. We propose\nGraphNLI, a novel graph-based deep learning architecture that uses graph walk\ntechniques to capture the wider context of a discussion thread in a principled\nfashion. Specifically, we propose methods to perform root-seeking graph walks\nthat start from a post and captures its surrounding context to generate\nadditional embeddings for the post. We then use these embeddings to predict the\npolarity relation between a reply and the post it is replying to. We evaluate\nthe performance of our models on a curated debate dataset from Kialo, an online\ndebating platform. Our model outperforms relevant baselines, including S-BERT,\nwith an overall accuracy of 83%.",
        "pdf_link": "https://arxiv.org/pdf/2202.08175v1.pdf"
    },
    {
        "title": "Capitalization Normalization for Language Modeling with an Accurate and Efficient Hierarchical RNN Model",
        "authors": [
            "Hao Zhang",
            "You-Chi Cheng",
            "Shankar Kumar",
            "W. Ronny Huang",
            "Mingqing Chen",
            "Rajiv Mathews"
        ],
        "published": "2022-02-16T16:21:53Z",
        "summary": "Capitalization normalization (truecasing) is the task of restoring the\ncorrect case (uppercase or lowercase) of noisy text. We propose a fast,\naccurate and compact two-level hierarchical word-and-character-based recurrent\nneural network model. We use the truecaser to normalize user-generated text in\na Federated Learning framework for language modeling. A case-aware language\nmodel trained on this normalized text achieves the same perplexity as a model\ntrained on text with gold capitalization. In a real user A/B experiment, we\ndemonstrate that the improvement translates to reduced prediction error rates\nin a virtual keyboard application. Similarly, in an ASR language model fusion\nexperiment, we show reduction in uppercase character error rate and word error\nrate.",
        "pdf_link": "https://arxiv.org/pdf/2202.08171v1.pdf"
    },
    {
        "title": "XFBoost: Improving Text Generation with Controllable Decoders",
        "authors": [
            "Xiangyu Peng",
            "Michael Sollami"
        ],
        "published": "2022-02-16T15:00:25Z",
        "summary": "Multimodal conditionality in transformer-based natural language models has\ndemonstrated state-of-the-art performance in the task of product description\ngeneration. Recent approaches condition a language model on one or more images\nand other textual metadata to achieve near-human performance for describing\nproducts from e-commerce stores. However, generated descriptions may exhibit\ndegrees of inaccuracy or even contradictory claims relative to the inputs of a\ngiven product. In this paper, we propose a controllable language generation\nframework called Extract-Finetune-Boost (XFBoost), which addresses the problem\nof inaccurate low-quality inference. By using visual semantic attributes as\nconstraints at the decoding stage of the generation process and finetuning the\nlanguage model with policy gradient techniques, the XFBoost framework is found\nto produce significantly more descriptive text with higher image relevancy,\noutperforming baselines and lowering the frequency of factually inaccurate\ndescriptions. We further demonstrate the application of XFBoost to online\nlearning wherein human-in-the-loop critics improve language models with active\nfeedback.",
        "pdf_link": "https://arxiv.org/pdf/2202.08124v1.pdf"
    },
    {
        "title": "Information Extraction in Low-Resource Scenarios: Survey and Perspective",
        "authors": [
            "Shumin Deng",
            "Yubo Ma",
            "Ningyu Zhang",
            "Yixin Cao",
            "Bryan Hooi"
        ],
        "published": "2022-02-16T13:44:00Z",
        "summary": "Information Extraction (IE) seeks to derive structured information from\nunstructured texts, often facing challenges in low-resource scenarios due to\ndata scarcity and unseen classes. This paper presents a review of neural\napproaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based}\nperspectives, systematically categorizing them into a fine-grained taxonomy.\nThen we conduct empirical study on LLM-based methods compared with previous\nstate-of-the-art models, and discover that (1) well-tuned LMs are still\npredominant; (2) tuning open-resource LLMs and ICL with GPT family is promising\nin general; (3) the optimal LLM-based technical solution for low-resource IE\ncan be task-dependent. In addition, we discuss low-resource IE with LLMs,\nhighlight promising applications, and outline potential research directions.\nThis survey aims to foster understanding of this field, inspire new ideas, and\nencourage widespread applications in both academia and industry.",
        "pdf_link": "https://arxiv.org/pdf/2202.08063v5.pdf"
    },
    {
        "title": "No One Left Behind: Inclusive Federated Learning over Heterogeneous Devices",
        "authors": [
            "Ruixuan Liu",
            "Fangzhao Wu",
            "Chuhan Wu",
            "Yanlin Wang",
            "Lingjuan Lyu",
            "Hong Chen",
            "Xing Xie"
        ],
        "published": "2022-02-16T13:03:27Z",
        "summary": "Federated learning (FL) is an important paradigm for training global models\nfrom decentralized data in a privacy-preserving way. Existing FL methods\nusually assume the global model can be trained on any participating client.\nHowever, in real applications, the devices of clients are usually\nheterogeneous, and have different computing power. Although big models like\nBERT have achieved huge success in AI, it is difficult to apply them to\nheterogeneous FL with weak clients. The straightforward solutions like removing\nthe weak clients or using a small model to fit all clients will lead to some\nproblems, such as under-representation of dropped clients and inferior accuracy\ndue to data loss or limited model representation ability. In this work, we\npropose InclusiveFL, a client-inclusive federated learning method to handle\nthis problem. The core idea of InclusiveFL is to assign models of different\nsizes to clients with different computing capabilities, bigger models for\npowerful clients and smaller ones for weak clients. We also propose an\neffective method to share the knowledge among multiple local models with\ndifferent sizes. In this way, all the clients can participate in the model\nlearning in FL, and the final model can be big and powerful enough. Besides, we\npropose a momentum knowledge distillation method to better transfer knowledge\nin big models on powerful clients to the small models on weak clients.\nExtensive experiments on many real-world benchmark datasets demonstrate the\neffectiveness of the proposed method in learning accurate models from clients\nwith heterogeneous devices under the FL framework.",
        "pdf_link": "https://arxiv.org/pdf/2202.08036v2.pdf"
    },
    {
        "title": "Should You Mask 15% in Masked Language Modeling?",
        "authors": [
            "Alexander Wettig",
            "Tianyu Gao",
            "Zexuan Zhong",
            "Danqi Chen"
        ],
        "published": "2022-02-16T11:42:34Z",
        "summary": "Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT's 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2202.08005v3.pdf"
    },
    {
        "title": "Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers",
        "authors": [
            "Yotaro Kubo",
            "Shigeki Karita",
            "Michiel Bacchiani"
        ],
        "published": "2022-02-16T07:02:24Z",
        "summary": "End-to-end speech recognition is a promising technology for enabling compact\nautomatic speech recognition (ASR) systems since it can unify the acoustic and\nlanguage model into a single neural network. However, as a drawback, training\nof end-to-end speech recognizers always requires transcribed utterances. Since\nend-to-end models are also known to be severely data hungry, this constraint is\ncrucial especially because obtaining transcribed utterances is costly and can\npossibly be impractical or impossible. This paper proposes a method for\nalleviating this issue by transferring knowledge from a language model neural\nnetwork that can be pretrained with text-only data. Specifically, this paper\nattempts to transfer semantic knowledge acquired in embedding vectors of\nlarge-scale language models. Since embedding vectors can be assumed as implicit\nrepresentations of linguistic information such as part-of-speech, intent, and\nso on, those are also expected to be useful modeling cues for ASR decoders.\nThis paper extends two types of ASR decoders, attention-based decoders and\nneural transducers, by modifying training loss functions to include embedding\nprediction terms. The proposed systems were shown to be effective for error\nrate reduction without incurring extra computational costs in the decoding\nphase.",
        "pdf_link": "https://arxiv.org/pdf/2202.07894v1.pdf"
    },
    {
        "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications",
        "authors": [
            "Jun Xia",
            "Yanqiao Zhu",
            "Yuanqi Du",
            "Stan Z. Li"
        ],
        "published": "2022-02-16T07:00:52Z",
        "summary": "Pretrained Language Models (PLMs) such as BERT have revolutionized the\nlandscape of Natural Language Processing (NLP). Inspired by their\nproliferation, tremendous efforts have been devoted to Pretrained Graph Models\n(PGMs). Owing to the powerful model architectures of PGMs, abundant knowledge\nfrom massive labeled and unlabeled graph data can be captured. The knowledge\nimplicitly encoded in model parameters can benefit various downstream tasks and\nhelp to alleviate several fundamental issues of learning on graphs. In this\npaper, we provide the first comprehensive survey for PGMs. We firstly present\nthe limitations of graph representation learning and thus introduce the\nmotivation for graph pre-training. Then, we systematically categorize existing\nPGMs based on a taxonomy from four different perspectives. Next, we present the\napplications of PGMs in social recommendation and drug discovery. Finally, we\noutline several promising research directions that can serve as a guideline for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2202.07893v2.pdf"
    },
    {
        "title": "Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models",
        "authors": [
            "Sarala Padi",
            "Seyed Omid Sadjadi",
            "Dinesh Manocha",
            "Ram D. Sriram"
        ],
        "published": "2022-02-16T00:23:42Z",
        "summary": "Automatic emotion recognition plays a key role in computer-human interaction\nas it has the potential to enrich the next-generation artificial intelligence\nwith emotional intelligence. It finds applications in customer and/or\nrepresentative behavior analysis in call centers, gaming, personal assistants,\nand social robots, to mention a few. Therefore, there has been an increasing\ndemand to develop robust automatic methods to analyze and recognize the various\nemotions. In this paper, we propose a neural network-based emotion recognition\nframework that uses a late fusion of transfer-learned and fine-tuned models\nfrom speech and text modalities. More specifically, we i) adapt a residual\nnetwork (ResNet) based model trained on a large-scale speaker recognition task\nusing transfer learning along with a spectrogram augmentation approach to\nrecognize emotions from speech, and ii) use a fine-tuned bidirectional encoder\nrepresentations from transformers (BERT) based model to represent and recognize\nemotions from the text. The proposed system then combines the ResNet and\nBERT-based model scores using a late fusion strategy to further improve the\nemotion recognition performance. The proposed multimodal solution addresses the\ndata scarcity limitation in emotion recognition using transfer learning, data\naugmentation, and fine-tuning, thereby improving the generalization performance\nof the emotion recognition models. We evaluate the effectiveness of our\nproposed multimodal approach on the interactive emotional dyadic motion capture\n(IEMOCAP) dataset. Experimental results indicate that both audio and text-based\nmodels improve the emotion recognition performance and that the proposed\nmultimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2202.08974v1.pdf"
    },
    {
        "title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation",
        "authors": [
            "Jannis Bulian",
            "Christian Buck",
            "Wojciech Gajewski",
            "Benjamin Boerschinger",
            "Tal Schuster"
        ],
        "published": "2022-02-15T18:53:58Z",
        "summary": "The predictions of question answering (QA)systems are typically evaluated\nagainst manually annotated finite sets of one or more answers. This leads to a\ncoverage limitation that results in underestimating the true performance of\nsystems, and is typically addressed by extending over exact match (EM) with\npre-defined rules or with the token-level F1 measure. In this paper, we present\nthe first systematic conceptual and data-driven analysis to examine the\nshortcomings of token-level equivalence measures.\n  To this end, we define the asymmetric notion of answer equivalence (AE),\naccepting answers that are equivalent to or improve over the reference, and\npublish over 23k human judgments for candidates produced by multiple QA systems\non SQuAD. Through a careful analysis of this data, we reveal and quantify\nseveral concrete limitations of the F1 measure, such as a false impression of\ngraduality, or missing dependence on the question.\n  Since collecting AE annotations for each evaluated model is expensive, we\nlearn a BERT matching (BEM) measure to approximate this task. Being a simpler\ntask than QA, we find BEM to provide significantly better AE approximations\nthan F1, and to more accurately reflect the performance of systems.\n  Finally, we demonstrate the practical utility of AE and BEM on the concrete\napplication of minimal accurate prediction sets, reducing the number of\nrequired answers by up to x2.6.",
        "pdf_link": "https://arxiv.org/pdf/2202.07654v2.pdf"
    },
    {
        "title": "Predicting on the Edge: Identifying Where a Larger Model Does Better",
        "authors": [
            "Taman Narayan",
            "Heinrich Jiang",
            "Sen Zhao",
            "Sanjiv Kumar"
        ],
        "published": "2022-02-15T18:53:14Z",
        "summary": "Much effort has been devoted to making large and more accurate models, but\nrelatively little has been put into understanding which examples are benefiting\nfrom the added complexity. In this paper, we demonstrate and analyze the\nsurprisingly tight link between a model's predictive uncertainty on individual\nexamples and the likelihood that larger models will improve prediction on them.\nThrough extensive numerical studies on the T5 encoder-decoder architecture, we\nshow that large models have the largest improvement on examples where the small\nmodel is most uncertain. On more certain examples, even those where the small\nmodel is not particularly accurate, large models are often unable to improve at\nall, and can even perform worse than the smaller model. Based on these\nfindings, we show that a switcher model which defers examples to a larger model\nwhen a small model is uncertain can achieve striking improvements in\nperformance and resource usage. We also explore committee-based uncertainty\nmetrics that can be more effective but less practical.",
        "pdf_link": "https://arxiv.org/pdf/2202.07652v1.pdf"
    },
    {
        "title": "Quantifying Memorization Across Neural Language Models",
        "authors": [
            "Nicholas Carlini",
            "Daphne Ippolito",
            "Matthew Jagielski",
            "Katherine Lee",
            "Florian Tramer",
            "Chiyuan Zhang"
        ],
        "published": "2022-02-15T18:48:31Z",
        "summary": "Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.",
        "pdf_link": "https://arxiv.org/pdf/2202.07646v3.pdf"
    },
    {
        "title": "Defending against Reconstruction Attacks with R\u00e9nyi Differential Privacy",
        "authors": [
            "Pierre Stock",
            "Igor Shilov",
            "Ilya Mironov",
            "Alexandre Sablayrolles"
        ],
        "published": "2022-02-15T18:09:30Z",
        "summary": "Reconstruction attacks allow an adversary to regenerate data samples of the\ntraining set using access to only a trained model. It has been recently shown\nthat simple heuristics can reconstruct data samples from language models,\nmaking this threat scenario an important aspect of model release. Differential\nprivacy is a known solution to such attacks, but is often used with a\nrelatively large privacy budget (epsilon > 8) which does not translate to\nmeaningful guarantees. In this paper we show that, for a same mechanism, we can\nderive privacy guarantees for reconstruction attacks that are better than the\ntraditional ones from the literature. In particular, we show that larger\nprivacy budgets do not protect against membership inference, but can still\nprotect extraction of rare secrets. We show experimentally that our guarantees\nhold against various language models, including GPT-2 finetuned on\nWikitext-103.",
        "pdf_link": "https://arxiv.org/pdf/2202.07623v1.pdf"
    },
    {
        "title": "BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer",
        "authors": [
            "Ana-Maria Bucur",
            "Adrian Cosma",
            "Ioan-Bogdan Iordache"
        ],
        "published": "2022-02-15T16:25:02Z",
        "summary": "Memes are prevalent on the internet and continue to grow and evolve alongside\nour culture. An automatic understanding of memes propagating on the internet\ncan shed light on the general sentiment and cultural attitudes of people. In\nthis work, we present team BLUE's solution for the second edition of the\nMEMOTION shared task. We showcase two approaches for meme classification (i.e.\nsentiment, humour, offensive, sarcasm and motivation levels) using a text-only\nmethod using BERT, and a Multi-Modal-Multi-Task transformer network that\noperates on both the meme image and its caption to output the final scores. In\nboth approaches, we leverage state-of-the-art pretrained models for text (BERT,\nSentence Transformer) and image processing (EfficientNetV4, CLIP). Through our\nefforts, we obtain first place in task A, second place in task B and third\nplace in task C. In addition, our team obtained the highest average score for\nall three tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.07543v3.pdf"
    },
    {
        "title": "Toxic Comments Hunter : Score Severity of Toxic Comments",
        "authors": [
            "Zhichang Wang",
            "Qipeng Zhu"
        ],
        "published": "2022-02-15T07:35:52Z",
        "summary": "The detection and identification of toxic comments are conducive to creating\na civilized and harmonious Internet environment. In this experiment, we\ncollected various data sets related to toxic comments. Because of the\ncharacteristics of comment data, we perform data cleaning and feature\nextraction operations on it from different angles to obtain different toxic\ncomment training sets. In terms of model construction, we used the training set\nto train the models based on TFIDF and finetuned the Bert model separately.\nFinally, we encapsulated the code into software to score toxic comments in\nreal-time.",
        "pdf_link": "https://arxiv.org/pdf/2203.03548v1.pdf"
    },
    {
        "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning",
        "authors": [
            "Yasaman Razeghi",
            "Robert L. Logan IV",
            "Matt Gardner",
            "Sameer Singh"
        ],
        "published": "2022-02-15T05:43:54Z",
        "summary": "Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.",
        "pdf_link": "https://arxiv.org/pdf/2202.07206v2.pdf"
    },
    {
        "title": "Text-Based Action-Model Acquisition for Planning",
        "authors": [
            "Kebing Jin",
            "Huaixun Chen",
            "Hankz Hankui Zhuo"
        ],
        "published": "2022-02-15T02:23:31Z",
        "summary": "Although there have been approaches that are capable of learning action\nmodels from plan traces, there is no work on learning action models from\ntextual observations, which is pervasive and much easier to collect from\nreal-world applications compared to plan traces. In this paper we propose a\nnovel approach to learning action models from natural language texts by\nintegrating Constraint Satisfaction and Natural Language Processing techniques.\nSpecifically, we first build a novel language model to extract plan traces from\ntexts, and then build a set of constraints to generate action models based on\nthe extracted plan traces. After that, we iteratively improve the language\nmodel and constraints until we achieve the convergent language model and action\nmodels. We empirically exhibit that our approach is both effective and\nefficient.",
        "pdf_link": "https://arxiv.org/pdf/2202.08373v2.pdf"
    },
    {
        "title": "QuadSim: A Quadcopter Rotational Dynamics Simulation Framework For Reinforcement Learning Algorithms",
        "authors": [
            "Burak Han Demirbilek"
        ],
        "published": "2022-02-14T20:34:08Z",
        "summary": "This study focuses on designing and developing a mathematically based\nquadcopter rotational dynamics simulation framework for testing reinforcement\nlearning (RL) algorithms in many flexible configurations. The design of the\nsimulation framework aims to simulate both linear and nonlinear representations\nof a quadcopter by solving initial value problems for ordinary differential\nequation (ODE) systems. In addition, the simulation environment is capable of\nmaking the simulation deterministic/stochastic by adding random Gaussian noise\nin the forms of process and measurement noises. In order to ensure that the\nscope of this simulation environment is not limited only with our own RL\nalgorithms, the simulation environment has been expanded to be compatible with\nthe OpenAI Gym toolkit. The framework also supports multiprocessing\ncapabilities to run simulation environments simultaneously in parallel. To test\nthese capabilities, many state-of-the-art deep RL algorithms were trained in\nthis simulation framework and the results were compared in detail.",
        "pdf_link": "https://arxiv.org/pdf/2202.07021v1.pdf"
    },
    {
        "title": "Punctuation restoration in Swedish through fine-tuned KB-BERT",
        "authors": [
            "John Bj\u00f6rkman Nilsson"
        ],
        "published": "2022-02-14T14:39:40Z",
        "summary": "Presented here is a method for automatic punctuation restoration in Swedish\nusing a BERT model. The method is based on KB-BERT, a publicly available,\nneural network language model pre-trained on a Swedish corpus by National\nLibrary of Sweden. This model has then been fine-tuned for this specific task\nusing a corpus of government texts. With a lower-case and unpunctuated Swedish\ntext as input, the model is supposed to return a grammatically correct\npunctuated copy of the text as output. A successful solution to this problem\nbrings benefits for an array of NLP domains, such as speech-to-text and\nautomated text. Only the punctuation marks period, comma and question marks\nwere considered for the project, due to a lack of data for more rare marks such\nas semicolon. Additionally, some marks are somewhat interchangeable with the\nmore common, such as exclamation points and periods. Thus, the data set had all\nexclamation points replaced with periods. The fine-tuned Swedish BERT model,\ndubbed prestoBERT, achieved an overall F1-score of 78.9. The proposed model\nscored similarly to international counterparts, with Hungarian and Chinese\nmodels obtaining F1-scores of 82.2 and 75.6 respectively. As further\ncomparison, a human evaluation case study was carried out. The human test group\nachieved an overall F1-score of 81.7, but scored substantially worse than\nprestoBERT on both period and comma. Inspecting output sentences from the model\nand humans show satisfactory results, despite the difference in F1-score. The\ndisconnect seems to stem from an unnecessary focus on replicating the exact\nsame punctuation used in the test set, rather than providing any of the number\nof correct interpretations. If the loss function could be rewritten to reward\nall grammatically correct outputs, rather than only the one original example,\nthe performance could improve significantly for both prestoBERT and the human\ngroup.",
        "pdf_link": "https://arxiv.org/pdf/2202.06769v1.pdf"
    },
    {
        "title": "CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences",
        "authors": [
            "Maliheh Izadi",
            "Roberta Gismondi",
            "Georgios Gousios"
        ],
        "published": "2022-02-14T13:26:54Z",
        "summary": "Code completion is an essential feature of IDEs, yet current autocompleters\nare restricted to either grammar-based or NLP-based single token completions.\nBoth approaches have significant drawbacks: grammar-based autocompletion is\nrestricted in dynamically-typed language environments, whereas NLP-based\nautocompleters struggle to understand the semantics of the programming language\nand the developer's code context.\n  In this work, we present CodeFill, a language model for autocompletion that\ncombines learned structure and naming information. Using a parallel Transformer\narchitecture and multi-task learning, CodeFill consumes sequences of source\ncode token names and their equivalent AST token types. Uniquely, CodeFill is\ntrained both for single-token and multi-token (statement) prediction, which\nenables it to learn long-range dependencies among grammatical and naming\nelements. We train CodeFill on two datasets, consisting of 29M and 425M lines\nof code, respectively. To make the evaluation more realistic, we develop a\nmethod to automatically infer points in the source code at which completion\nmatters. We compare CodeFill against four baselines and two state-of-the-art\nmodels, GPT-C and TravTrans+.CodeFill surpasses all baselines in single token\nprediction (MRR: 70.9% vs. 66.2% and 67.8%) and outperforms the state of the\nart for multi-token prediction (ROUGE-L: 63.7% vs. 52.4% and 59.2%, for n=4\ntokens). We publicly release our source code and datasets.",
        "pdf_link": "https://arxiv.org/pdf/2202.06689v1.pdf"
    },
    {
        "title": "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
        "authors": [
            "Patrick Schramowski",
            "Christopher Tauchmann",
            "Kristian Kersting"
        ],
        "published": "2022-02-14T13:00:31Z",
        "summary": "Large datasets underlying much of current machine learning raise serious\nissues concerning inappropriate content such as offensive, insulting,\nthreatening, or might otherwise cause anxiety. This calls for increased dataset\ndocumentation, e.g., using datasheets. They, among other topics, encourage to\nreflect on the composition of the datasets. So far, this documentation,\nhowever, is done manually and therefore can be tedious and error-prone,\nespecially for large image datasets. Here we ask the arguably \"circular\"\nquestion of whether a machine can help us reflect on inappropriate content,\nanswering Question 16 in Datasheets. To this end, we propose to use the\ninformation stored in pre-trained transformer models to assist us in the\ndocumentation process. Specifically, prompt-tuning based on a dataset of\nsocio-moral values steers CLIP to identify potentially inappropriate content,\ntherefore reducing human labor. We then document the inappropriate images found\nusing word clouds, based on captions generated using a vision-language model.\nThe documentations of two popular, large-scale computer vision datasets --\nImageNet and OpenImages -- produced this way suggest that machines can indeed\nhelp dataset creators to answer Question 16 on inappropriate image content.",
        "pdf_link": "https://arxiv.org/pdf/2202.06675v2.pdf"
    },
    {
        "title": "Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings",
        "authors": [
            "Malte Ostendorff",
            "Nils Rethmeier",
            "Isabelle Augenstein",
            "Bela Gipp",
            "Georg Rehm"
        ],
        "published": "2022-02-14T12:57:37Z",
        "summary": "Learning scientific document representations can be substantially improved\nthrough contrastive learning objectives, where the challenge lies in creating\npositive and negative training samples that encode the desired similarity\nsemantics. Prior work relies on discrete citation relations to generate\ncontrast samples. However, discrete citations enforce a hard cut-off to\nsimilarity. This is counter-intuitive to similarity-based learning, and ignores\nthat scientific papers can be very similar despite lacking a direct citation -\na core problem of finding related research. Instead, we use controlled nearest\nneighbor sampling over citation graph embeddings for contrastive learning. This\ncontrol allows us to learn continuous similarity, to sample hard-to-learn\nnegatives and positives, and also to avoid collisions between negative and\npositive samples by controlling the sampling margin between them. The resulting\nmethod SciNCL outperforms the state-of-the-art on the SciDocs benchmark.\nFurthermore, we demonstrate that it can train (or tune) models\nsample-efficiently, and that it can be combined with recent training-efficient\nmethods. Perhaps surprisingly, even training a general-domain language model\nthis way outperforms baselines pretrained in-domain.",
        "pdf_link": "https://arxiv.org/pdf/2202.06671v2.pdf"
    },
    {
        "title": "UserBERT: Modeling Long- and Short-Term User Preferences via Self-Supervision",
        "authors": [
            "Tianyu Li",
            "Ali Cevahir",
            "Derek Cho",
            "Hao Gong",
            "DuyKhuong Nguyen",
            "Bjorn Stenger"
        ],
        "published": "2022-02-14T08:31:36Z",
        "summary": "E-commerce platforms generate vast amounts of customer behavior data, such as\nclicks and purchases, from millions of unique users every day. However,\neffectively using this data for behavior understanding tasks is challenging\nbecause there are usually not enough labels to learn from all users in a\nsupervised manner. This paper extends the BERT model to e-commerce user data\nfor pre-training representations in a self-supervised manner. By viewing user\nactions in sequences as analogous to words in sentences, we extend the existing\nBERT model to user behavior data. Further, our model adopts a unified structure\nto simultaneously learn from long-term and short-term user behavior, as well as\nuser attributes. We propose methods for the tokenization of different types of\nuser behavior sequences, the generation of input representation vectors, and a\nnovel pretext task to enable the pre-trained model to learn from its own input,\neliminating the need for labeled training data. Extensive experiments\ndemonstrate that the learned representations result in significant improvements\nwhen transferred to three different real-world tasks, particularly compared to\ntask-specific modeling and multi-task representation learning",
        "pdf_link": "https://arxiv.org/pdf/2202.07605v1.pdf"
    },
    {
        "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models",
        "authors": [
            "Nikhil Kandpal",
            "Eric Wallace",
            "Colin Raffel"
        ],
        "published": "2022-02-14T08:20:15Z",
        "summary": "Past work has shown that large language models are susceptible to privacy\nattacks, where adversaries generate sequences from a trained model and detect\nwhich sequences are memorized from the training set. In this work, we show that\nthe success of these attacks is largely due to duplication in commonly used\nweb-scraped training sets. We first show that the rate at which language models\nregenerate training sequences is superlinearly related to a sequence's count in\nthe training set. For instance, a sequence that is present 10 times in the\ntraining data is on average generated ~1000 times more often than a sequence\nthat is present only once. We next show that existing methods for detecting\nmemorized sequences have near-chance accuracy on non-duplicated training\nsequences. Finally, we find that after applying methods to deduplicate training\ndata, language models are considerably more secure against these types of\nprivacy attacks. Taken together, our results motivate an increased focus on\ndeduplication in privacy-sensitive applications and a reevaluation of the\npracticality of existing privacy attacks.",
        "pdf_link": "https://arxiv.org/pdf/2202.06539v3.pdf"
    },
    {
        "title": "Modeling Intention, Emotion and External World in Dialogue Systems",
        "authors": [
            "Wei Peng",
            "Yue Hu",
            "Luxi Xing",
            "Yuqiang Xie",
            "Xingsheng Zhang",
            "Yajing Sun"
        ],
        "published": "2022-02-14T04:10:34Z",
        "summary": "Intention, emotion and action are important elements in human activities.\nModeling the interaction process between individuals by analyzing the\nrelationships between these elements is a challenging task. However, previous\nwork mainly focused on modeling intention and emotion independently, and\nneglected of exploring the mutual relationships between intention and emotion.\nIn this paper, we propose a RelAtion Interaction Network (RAIN), consisting of\nIntention Relation Module and Emotion Relation Module, to jointly model mutual\nrelationships and explicitly integrate historical intention information. The\nexperiments on the dataset show that our model can take full advantage of the\nintention, emotion and action between individuals and achieve a remarkable\nimprovement over BERT-style baselines. Qualitative analysis verifies the\nimportance of the mutual interaction between the intention and emotion.",
        "pdf_link": "https://arxiv.org/pdf/2202.06476v1.pdf"
    },
    {
        "title": "Assessment of contextualised representations in detecting outcome phrases in clinical trials",
        "authors": [
            "Micheal Abaho",
            "Danushka Bollegala",
            "Paula R Williamson",
            "Susanna Dodd"
        ],
        "published": "2022-02-13T15:08:00Z",
        "summary": "Automating the recognition of outcomes reported in clinical trials using\nmachine learning has a huge potential of speeding up access to evidence\nnecessary in healthcare decision-making. Prior research has however\nacknowledged inadequate training corpora as a challenge for the Outcome\ndetection (OD) task. Additionally, several contextualized representations like\nBERT and ELMO have achieved unparalleled success in detecting various diseases,\ngenes, proteins, and chemicals, however, the same cannot be emphatically stated\nfor outcomes, because these models have been relatively under-tested and\nstudied for the OD task. We introduce \"EBM-COMET\", a dataset in which 300\nPubMed abstracts are expertly annotated for clinical outcomes. Unlike prior\nrelated datasets that use arbitrary outcome classifications, we use labels from\na taxonomy recently published to standardize outcome classifications. To\nextract outcomes, we fine-tune a variety of pre-trained contextualized\nrepresentations, additionally, we use frozen contextualized and\ncontext-independent representations in our custom neural model augmented with\nclinically informed Part-Of-Speech embeddings and a cost-sensitive loss\nfunction. We adopt strict evaluation for the trained models by rewarding them\nfor correctly identifying full outcome phrases rather than words within the\nentities i.e. given an outcome \"systolic blood pressure\", the models are\nrewarded a classification score only when they predict all 3 words in sequence,\notherwise, they are not rewarded. We observe our best model (BioBERT) achieve\n81.5\\% F1, 81.3\\% sensitivity and 98.0\\% specificity. We reach a consensus on\nwhich contextualized representations are best suited for detecting outcomes\nfrom clinical-trial abstracts. Furthermore, our best model outperforms scores\npublished on the original EBM-NLP dataset leader-board scores.",
        "pdf_link": "https://arxiv.org/pdf/2203.03547v2.pdf"
    },
    {
        "title": "ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification",
        "authors": [
            "Xinjie Lin",
            "Gang Xiong",
            "Gaopeng Gou",
            "Zhen Li",
            "Junzheng Shi",
            "Jing Yu"
        ],
        "published": "2022-02-13T14:54:48Z",
        "summary": "Encrypted traffic classification requires discriminative and robust traffic\nrepresentation captured from content-invisible and imbalanced traffic data for\naccurate classification, which is challenging but indispensable to achieve\nnetwork security and network management. The major limitation of existing\nsolutions is that they highly rely on the deep features, which are overly\ndependent on data size and hard to generalize on unseen data. How to leverage\nthe open-domain unlabeled traffic data to learn representation with strong\ngeneralization ability remains a key challenge. In this paper,we propose a new\ntraffic representation model called Encrypted Traffic Bidirectional Encoder\nRepresentations from Transformer (ET-BERT), which pre-trains deep\ncontextualized datagram-level representation from large-scale unlabeled data.\nThe pre-trained model can be fine-tuned on a small number of task-specific\nlabeled data and achieves state-of-the-art performance across five encrypted\ntraffic classification tasks, remarkably pushing the F1 of ISCX-Tor to 99.2%\n(4.4% absolute improvement), ISCX-VPN-Service to 98.9% (5.2% absolute\nimprovement), Cross-Platform (Android) to 92.5% (5.4% absolute improvement),\nCSTNET-TLS 1.3 to 97.4% (10.0% absolute improvement). Notably, we provide\nexplanation of the empirically powerful pre-training model by analyzing the\nrandomness of ciphers. It gives us insights in understanding the boundary of\nclassification ability over encrypted traffic. The code is available at:\nhttps://github.com/linwhitehat/ET-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2202.06335v2.pdf"
    },
    {
        "title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models",
        "authors": [
            "Yanchen Liu",
            "Timo Schick",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2022-02-12T19:50:59Z",
        "summary": "Due to the high costs associated with finetuning large language models,\nvarious recent works propose to adapt them to specific tasks without any\nparameter updates through in-context learning. Unfortunately, for in-context\nlearning there is currently no way to leverage unlabeled data, which is often\nmuch easier to obtain in large quantities than labeled examples. In this work,\nwe therefore investigate ways to make use of unlabeled examples to improve the\nzero-shot performance of pretrained language models without any finetuning: We\nintroduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies\nexamples by retrieving semantically similar unlabeled examples, assigning\nlabels to them in a zero-shot fashion, and then using them for in-context\nlearning. We also propose bag-of-contexts priming, a new priming strategy that\nis more suitable for our setting and enables the usage of more examples than\nfit into the context window.",
        "pdf_link": "https://arxiv.org/pdf/2202.06133v1.pdf"
    },
    {
        "title": "Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers",
        "authors": [
            "Grzegorz Jacenk\u00f3w",
            "Alison Q. O'Neil",
            "Sotirios A. Tsaftaris"
        ],
        "published": "2022-02-12T14:23:30Z",
        "summary": "When a clinician refers a patient for an imaging exam, they include the\nreason (e.g. relevant patient history, suspected disease) in the scan request;\nthis appears as the indication field in the radiology report. The\ninterpretation and reporting of the image are substantially influenced by this\nrequest text, steering the radiologist to focus on particular aspects of the\nimage. We use the indication field to drive better image classification, by\ntaking a transformer network which is unimodally pre-trained on text (BERT) and\nfine-tuning it for multimodal classification of a dual image-text input. We\nevaluate the method on the MIMIC-CXR dataset, and present ablation studies to\ninvestigate the effect of the indication field on the classification\nperformance. The experimental results show our approach achieves 87.8 average\nmicro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and\nmultimodal (86.0) classification. Our code is available at\nhttps://github.com/jacenkow/mmbt.",
        "pdf_link": "https://arxiv.org/pdf/2202.06076v1.pdf"
    },
    {
        "title": "USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder",
        "authors": [
            "Bolaji Yusuf",
            "Ankur Gandhe",
            "Alex Sokolov"
        ],
        "published": "2022-02-12T11:35:59Z",
        "summary": "Improving end-to-end speech recognition by incorporating external text data\nhas been a longstanding research topic. There has been a recent focus on\ntraining E2E ASR models that get the performance benefits of external text data\nwithout incurring the extra cost of evaluating an external language model at\ninference time. In this work, we propose training ASR model jointly with a set\nof text-to-text auxiliary tasks with which it shares a decoder and parts of the\nencoder. When we jointly train ASR and masked language model with the 960-hour\nLibrispeech and Opensubtitles data respectively, we observe WER reductions of\n16% and 20% on test-other and test-clean respectively over an ASR-only baseline\nwithout any extra cost at inference time, and reductions of 6% and 8% compared\nto a stronger MUTE-L baseline which trains the decoder with the same text data\nas our model. We achieve further improvements when we train masked language\nmodel on Librispeech data or when we use machine translation as the auxiliary\ntask, without significantly sacrificing performance on the task itself.",
        "pdf_link": "https://arxiv.org/pdf/2202.06045v1.pdf"
    },
    {
        "title": "A multi-task semi-supervised framework for Text2Graph & Graph2Text",
        "authors": [
            "Oriol Domingo",
            "Marta R. Costa-juss\u00e0",
            "Carlos Escolano"
        ],
        "published": "2022-02-12T11:02:17Z",
        "summary": "The Artificial Intelligence industry regularly develops applications that\nmostly rely on Knowledge Bases, a data repository about specific, or general,\ndomains, usually represented in a graph shape. Similar to other databases, they\nface two main challenges: information ingestion and information retrieval. We\napproach these challenges by jointly learning graph extraction from text and\ntext generation from graphs. The proposed solution, a T5 architecture, is\ntrained in a multi-task semi-supervised environment, with our collected\nnon-parallel data, following a cycle training regime. Experiments on WebNLG\ndataset show that our approach surpasses unsupervised state-of-the-art results\nin text-to-graph and graph-to-text. More relevantly, our framework is more\nconsistent across seen and unseen domains than supervised models. The resulting\nmodel can be easily trained in any new domain with non-parallel data, by simply\nadding text and graphs about it, in our cycle framework.",
        "pdf_link": "https://arxiv.org/pdf/2202.06041v2.pdf"
    },
    {
        "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
        "authors": [
            "Yucheng Lu",
            "Conglong Li",
            "Minjia Zhang",
            "Christopher De Sa",
            "Yuxiong He"
        ],
        "published": "2022-02-12T08:02:23Z",
        "summary": "1-bit gradient compression and local steps are two representative techniques\nthat enable drastic communication reduction in distributed SGD. Their benefits,\nhowever, remain an open question on Adam-based large model pre-training (e.g.\nBERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes\nslow convergence even when 1-bit compression or local steps are individually\napplied. To alleviate this limitation, we propose 0/1 Adam that linearizes each\nAdam step via approximating its optimizer states using their stale estimates\nand linear correlation. 0/1 Adam performs an Adam-like step to preserve the\nadaptivity, while its linearity allows utilizing 1-bit compression and local\nsteps simultaneously for wall-clock time speed up. We provide convergence\nguarantee for 0/1 Adam on smooth non-convex objectives. On various large-scale\nbenchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we\ndemonstrate on up to 128 GPUs that 0/1 Adam is able to reduce up to 87% of data\nvolume, 54% of communication rounds, and achieve up to 2$\\times$ higher\ntraining throughput and end-to-end training time reduction compared to the\nstate-of-the-art baseline 1-bit Adam; while enjoying the same statistical\nconvergence speed and end task model accuracy on GLUE dataset and ImageNet\nvalidation set.",
        "pdf_link": "https://arxiv.org/pdf/2202.06009v3.pdf"
    },
    {
        "title": "White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense",
        "authors": [
            "Shahrukh Khan",
            "Mahnoor Shahid",
            "Navdeeppal Singh"
        ],
        "published": "2022-02-11T17:20:50Z",
        "summary": "In this work, we evaluate the adversarial robustness of BERT models trained\non German Hate Speech datasets. We also complement our evaluation with two\nnovel white-box character and word level attacks thereby contributing to the\nrange of attacks available. Furthermore, we also perform a comparison of two\nnovel character-level defense strategies and evaluate their robustness with one\nanother.",
        "pdf_link": "https://arxiv.org/pdf/2202.05778v2.pdf"
    },
    {
        "title": "HaT5: Hate Language Identification using Text-to-Text Transfer Transformer",
        "authors": [
            "Sana Sabah Sabry",
            "Tosin Adewumi",
            "Nosheen Abid",
            "Gy\u00f6rgy Kovacs",
            "Foteini Liwicki",
            "Marcus Liwicki"
        ],
        "published": "2022-02-11T15:21:27Z",
        "summary": "We investigate the performance of a state-of-the art (SoTA) architecture T5\n(available on the SuperGLUE) and compare with it 3 other previous SoTA\narchitectures across 5 different tasks from 2 relatively diverse datasets. The\ndatasets are diverse in terms of the number and types of tasks they have. To\nimprove performance, we augment the training data by using an autoregressive\nmodel. We achieve near-SoTA results on a couple of the tasks - macro F1 scores\nof 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate\nspeech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and\n83.05%, respectively. We perform error analysis and explain why one of the\nmodels (Bi-LSTM) makes the predictions it does by using a publicly available\nalgorithm: Integrated Gradient (IG). This is because explainable artificial\nintelligence (XAI) is essential for earning the trust of users. The main\ncontributions of this work are the implementation method of T5, which is\ndiscussed; the data augmentation using a new conversational AI model\ncheckpoint, which brought performance improvements; and the revelation on the\nshortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data\nannotation by using a small set of examples where the T5 model made the correct\npredictions, even when the ground truth of the test set were incorrect (in our\nopinion). We also provide our model checkpoints on the HuggingFace hub1 to\nfoster transparency.",
        "pdf_link": "https://arxiv.org/pdf/2202.05690v1.pdf"
    },
    {
        "title": "What Does it Mean for a Language Model to Preserve Privacy?",
        "authors": [
            "Hannah Brown",
            "Katherine Lee",
            "Fatemehsadat Mireshghallah",
            "Reza Shokri",
            "Florian Tram\u00e8r"
        ],
        "published": "2022-02-11T09:18:27Z",
        "summary": "Natural language reflects our private lives and identities, making its\nprivacy concerns as broad as those of real life. Language models lack the\nability to understand the context and sensitivity of text, and tend to memorize\nphrases present in their training sets. An adversary can exploit this tendency\nto extract training data. Depending on the nature of the content and the\ncontext in which this data was collected, this could violate expectations of\nprivacy. Thus there is a growing interest in techniques for training language\nmodels that preserve privacy. In this paper, we discuss the mismatch between\nthe narrow assumptions made by popular data protection techniques (data\nsanitization and differential privacy), and the broadness of natural language\nand of privacy as a social norm. We argue that existing protection methods\ncannot guarantee a generic and meaningful notion of privacy for language\nmodels. We conclude that language models should be trained on text data which\nwas explicitly produced for public use.",
        "pdf_link": "https://arxiv.org/pdf/2202.05520v2.pdf"
    },
    {
        "title": "Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs",
        "authors": [
            "Daniel Louzada Fernandes",
            "Marcos Henrique Fonseca Ribeiro",
            "Fabio Ribeiro Cerqueira",
            "Michel Melo Silva"
        ],
        "published": "2022-02-10T21:20:53Z",
        "summary": "Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.",
        "pdf_link": "https://arxiv.org/pdf/2202.05331v2.pdf"
    },
    {
        "title": "Locating and Editing Factual Associations in GPT",
        "authors": [
            "Kevin Meng",
            "David Bau",
            "Alex Andonian",
            "Yonatan Belinkov"
        ],
        "published": "2022-02-10T18:59:54Z",
        "summary": "We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel's factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/",
        "pdf_link": "https://arxiv.org/pdf/2202.05262v5.pdf"
    },
    {
        "title": "Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding",
        "authors": [
            "Peter Sullivan",
            "Toshiko Shibano",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2022-02-10T18:13:32Z",
        "summary": "ASR systems designed for native English (L1) usually underperform on\nnon-native English (L2). To address this performance gap, \\textbf{(i)} we\nextend our previous work to investigate fine-tuning of a pre-trained wav2vec\n2.0 model \\cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2\ntraining conditions. We further \\textbf{(ii)} incorporate language model\ndecoding in the ASR system, along with the fine-tuning method. Quantifying\ngains acquired from each of these two approaches separately and an error\nanalysis allows us to identify different sources of improvement within our\nmodels. We find that while the large self-trained wav2vec 2.0 may be\ninternalizing sufficient decoding knowledge for clean L1 speech\n\\cite{xu2021self}, this does not hold for L2 speech and accounts for the\nutility of employing language model decoding on L2 data.",
        "pdf_link": "https://arxiv.org/pdf/2202.05209v1.pdf"
    },
    {
        "title": "Zero Shot Learning for Predicting Energy Usage of Buildings in Sustainable Design",
        "authors": [
            "Arun Zachariah",
            "Praveen Rao",
            "Brian Corn",
            "Dominique Davison"
        ],
        "published": "2022-02-10T18:08:58Z",
        "summary": "The 2030 Challenge is aimed at making all new buildings and major renovations\ncarbon neutral by 2030. One of the potential solutions to meet this challenge\nis through innovative sustainable design strategies. For developing such\nstrategies it is important to understand how the various building factors\ncontribute to energy usage of a building, right at design time. The growth of\nartificial intelligence (AI) in recent years provides an unprecedented\nopportunity to advance sustainable design by learning complex relationships\nbetween building factors from available data. However, rich training datasets\nare needed for AI-based solutions to achieve good prediction accuracy.\nUnfortunately, obtaining training datasets are time consuming and expensive in\nmany real-world applications. Motivated by these reasons, we address the\nproblem of accurately predicting the energy usage of new or unknown building\ntypes, i.e., those building types that do not have any training data. We\npropose a novel approach based on zero-shot learning (ZSL) to solve this\nproblem. Our approach uses side information from building energy modeling\nexperts to predict the closest building types for a given new/unknown building\ntype. We then obtain the predicted energy usage for the k-closest building\ntypes using the models learned during training and combine the predicted values\nusing a weighted averaging function. We evaluated our approach on a dataset\ncontaining five building types generated using BuildSimHub, a popular platform\nfor building energy modeling. Our approach achieved better average accuracy\nthan a regression model (based on XGBoost) trained on the entire dataset of\nknown building types.",
        "pdf_link": "https://arxiv.org/pdf/2202.05206v1.pdf"
    },
    {
        "title": "InPars: Data Augmentation for Information Retrieval using Large Language Models",
        "authors": [
            "Luiz Bonifacio",
            "Hugo Abonizio",
            "Marzieh Fadaee",
            "Rodrigo Nogueira"
        ],
        "published": "2022-02-10T16:52:45Z",
        "summary": "The information retrieval community has recently witnessed a revolution due\nto large pretrained transformer models. Another key ingredient for this\nrevolution was the MS MARCO dataset, whose scale and diversity has enabled\nzero-shot transfer learning to various tasks. However, not all IR tasks and\ndomains can benefit from one single dataset equally. Extensive research in\nvarious NLP tasks has shown that using domain-specific training data, as\nopposed to a general-purpose one, improves the performance of neural models. In\nthis work, we harness the few-shot capabilities of large pretrained language\nmodels as synthetic data generators for IR tasks. We show that models finetuned\nsolely on our unsupervised dataset outperform strong baselines such as BM25 as\nwell as recently proposed self-supervised dense retrieval methods. Furthermore,\nretrievers finetuned on both supervised and our synthetic data achieve better\nzero-shot transfer than models finetuned only on supervised data. Code, models,\nand data are available at https://github.com/zetaalphavector/inpars .",
        "pdf_link": "https://arxiv.org/pdf/2202.05144v1.pdf"
    },
    {
        "title": "Slovene SuperGLUE Benchmark: Translation and Evaluation",
        "authors": [
            "Ale\u0161 \u017dagar",
            "Marko Robnik-\u0160ikonja"
        ],
        "published": "2022-02-10T12:46:06Z",
        "summary": "We present a Slovene combined machine-human translated SuperGLUE benchmark.\nWe describe the translation process and problems arising due to differences in\nmorphology and grammar. We evaluate the translated datasets in several modes:\nmonolingual, cross-lingual, and multilingual, taking into account differences\nbetween machine and human translated training sets. The results show that the\nmonolingual Slovene SloBERTa model is superior to massively multilingual and\ntrilingual BERT models, but these also show a good cross-lingual performance on\ncertain tasks. The performance of Slovene models still lags behind the best\nEnglish models.",
        "pdf_link": "https://arxiv.org/pdf/2202.04994v1.pdf"
    },
    {
        "title": "Predicting Human Similarity Judgments Using Large Language Models",
        "authors": [
            "Raja Marjieh",
            "Ilia Sucholutsky",
            "Theodore R. Sumers",
            "Nori Jacoby",
            "Thomas L. Griffiths"
        ],
        "published": "2022-02-09T21:09:25Z",
        "summary": "Similarity judgments provide a well-established method for accessing mental\nrepresentations, with applications in psychology, neuroscience and machine\nlearning. However, collecting similarity judgments can be prohibitively\nexpensive for naturalistic datasets as the number of comparisons grows\nquadratically in the number of stimuli. One way to tackle this problem is to\nconstruct approximation procedures that rely on more accessible proxies for\npredicting similarity. Here we leverage recent advances in language models and\nonline recruitment, proposing an efficient domain-general procedure for\npredicting human similarity judgments based on text descriptions. Intuitively,\nsimilar stimuli are likely to evoke similar descriptions, allowing us to use\ndescription similarity to predict pairwise similarity judgments. Crucially, the\nnumber of descriptions required grows only linearly with the number of stimuli,\ndrastically reducing the amount of data required. We test this procedure on six\ndatasets of naturalistic images and show that our models outperform previous\napproaches based on visual information.",
        "pdf_link": "https://arxiv.org/pdf/2202.04728v1.pdf"
    },
    {
        "title": "Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations",
        "authors": [
            "Yu Meng",
            "Yunyi Zhang",
            "Jiaxin Huang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "published": "2022-02-09T17:26:08Z",
        "summary": "Topic models have been the prominent tools for automatic topic discovery from\ntext corpora. Despite their effectiveness, topic models suffer from several\nlimitations including the inability of modeling word ordering information in\ndocuments, the difficulty of incorporating external linguistic knowledge, and\nthe lack of both accurate and efficient inference methods for approximating the\nintractable posterior. Recently, pretrained language models (PLMs) have brought\nastonishing performance improvements to a wide variety of tasks due to their\nsuperior representations of text. Interestingly, there have not been standard\napproaches to deploy PLMs for topic discovery as better alternatives to topic\nmodels. In this paper, we begin by analyzing the challenges of using PLM\nrepresentations for topic discovery, and then propose a joint latent space\nlearning and clustering framework built upon PLM embeddings. In the latent\nspace, topic-word and document-topic distributions are jointly modeled so that\nthe discovered topics can be interpreted by coherent and distinctive terms and\nmeanwhile serve as meaningful summaries of the documents. Our model effectively\nleverages the strong representation power and superb linguistic features\nbrought by PLMs for topic discovery, and is conceptually simpler than topic\nmodels. On two benchmark datasets in different domains, our model generates\nsignificantly more coherent and diverse topics than strong topic models, and\noffers better topic-wise document representations, based on both automatic and\nhuman evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2202.04582v1.pdf"
    },
    {
        "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
        "authors": [
            "Yu Meng",
            "Jiaxin Huang",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "published": "2022-02-09T16:02:18Z",
        "summary": "Pretrained language models (PLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) are\nwell known for their superior text generation capabilities; bidirectional PLMs\n(e.g., BERT) have been the prominent choice for natural language understanding\n(NLU) tasks. While both types of models have achieved promising few-shot\nlearning performance, their potential for zero-shot learning has been\nunderexplored. In this paper, we present a simple approach that uses both types\nof PLMs for fully zero-shot learning of NLU tasks without requiring any\ntask-specific data: A unidirectional PLM generates class-conditioned texts\nguided by prompts, which are used as the training data for fine-tuning a\nbidirectional PLM. With quality training data selected based on the generation\nprobability and regularization techniques (label smoothing and temporal\nensembling) applied to the fine-tuning stage for better generalization and\nstability, our approach demonstrates strong performance across seven\nclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and\n92.8 on SST-2), significantly outperforming zero-shot prompting methods and\nachieving even comparable results to strong few-shot approaches using 32\ntraining samples per class.",
        "pdf_link": "https://arxiv.org/pdf/2202.04538v2.pdf"
    },
    {
        "title": "Social Media as an Instant Source of Feedback on Water Quality",
        "authors": [
            "Khubaib Ahmad",
            "Muhammad Asif Ayub",
            "Kashif Ahmad",
            "Jebran Khan",
            "Nasir Ahmad",
            "Ala Al-Fuqaha"
        ],
        "published": "2022-02-09T13:47:33Z",
        "summary": "This paper focuses on an important environmental challenge; namely, water\nquality by analyzing the potential of social media as an immediate source of\nfeedback. The main goal of the work is to automatically analyze and retrieve\nsocial media posts relevant to water quality with particular attention to posts\ndescribing different aspects of water quality, such as watercolor, smell,\ntaste, and related illnesses. To this aim, we propose a novel framework\nincorporating different preprocessing, data augmentation, and classification\ntechniques. In total, three different Neural Networks (NNs) architectures,\nnamely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom\nLong short-term memory (LSTM) model, are employed in a merit-based fusion\nscheme. For merit-based weight assignment to the models, several optimization\nand search techniques are compared including a Particle Swarm Optimization\n(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's\noptimization methods. We also provide an evaluation of the individual models\nwhere the highest F1-score of 0.81 is obtained with the BERT model. In\nmerit-based fusion, overall better results are obtained with BF achieving an\nF1-score score of 0.852.\n  We also provide comparison against existing methods, where a significant\nimprovement for our proposed solutions is obtained. We believe such rigorous\nanalysis of this relatively new topic will provide a baseline for future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2202.04462v2.pdf"
    },
    {
        "title": "Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?",
        "authors": [
            "Jiawen Zhang",
            "Abhijit Mishra",
            "Avinesh P. V. S",
            "Siddharth Patwardhan",
            "Sachin Agarwal"
        ],
        "published": "2022-02-09T06:47:40Z",
        "summary": "The task of Outside Knowledge Visual Question Answering (OKVQA) requires an\nautomatic system to answer natural language questions about pictures and images\nusing external knowledge. We observe that many visual questions, which contain\ndeictic referential phrases referring to entities in the image, can be\nrewritten as \"non-grounded\" questions and can be answered by existing\ntext-based question answering systems. This allows for the reuse of existing\ntext-based Open Domain Question Answering (QA) Systems for visual question\nanswering. In this work, we propose a potentially data-efficient approach that\nreuses existing systems for (a) image analysis, (b) question rewriting, and (c)\ntext-based question answering to answer such visual questions. Given an image\nand a question pertaining to that image (a visual question), we first extract\nthe entities present in the image using pre-trained object and scene\nclassifiers. Using these detected entities, the visual questions can be\nrewritten so as to be answerable by open domain QA systems. We explore two\nrewriting strategies: (1) an unsupervised method using BERT for masking and\nrewriting, and (2) a weakly supervised approach that combines adaptive\nrewriting and reinforcement learning techniques to use the implicit feedback\nfrom the QA system. We test our strategies on the publicly available OKVQA\ndataset and obtain a competitive performance with state-of-the-art models while\nusing only 10% of the training data.",
        "pdf_link": "https://arxiv.org/pdf/2202.04306v1.pdf"
    },
    {
        "title": "The Volcspeech system for the ICASSP 2022 multi-channel multi-party meeting transcription challenge",
        "authors": [
            "Chen Shen",
            "Yi Liu",
            "Wenzhi Fan",
            "Bin Wang",
            "Shixue Wen",
            "Yao Tian",
            "Jun Zhang",
            "Jingsheng Yang",
            "Zejun Ma"
        ],
        "published": "2022-02-09T03:38:39Z",
        "summary": "This paper describes our submission to ICASSP 2022 Multi-channel Multi-party\nMeeting Transcription (M2MeT) Challenge. For Track 1, we propose several\napproaches to empower the clustering-based speaker diarization system to handle\noverlapped speech. Front-end dereverberation and the direction-of-arrival (DOA)\nestimation are used to improve the accuracy of speaker diarization.\nMulti-channel combination and overlap detection are applied to reduce the\nmissed speaker error. A modified DOVER-Lap is also proposed to fuse the results\nof different systems. We achieve the final DER of 5.79% on the Eval set and\n7.23% on the Test set. For Track 2, we develop our system using the Conformer\nmodel in a joint CTC-attention architecture. Serialized output training is\nadopted to multi-speaker overlapped speech recognition. We propose a neural\nfront-end module to model multi-channel audio and train the model end-to-end.\nVarious data augmentation methods are utilized to mitigate over-fitting in the\nmulti-channel multi-speaker E2E system. Transformer language model fusion is\ndeveloped to achieve better performance. The final CER is 19.2% on the Eval set\nand 20.8% on the Test set.",
        "pdf_link": "https://arxiv.org/pdf/2202.04261v2.pdf"
    },
    {
        "title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models",
        "authors": [
            "Boxin Wang",
            "Wei Ping",
            "Chaowei Xiao",
            "Peng Xu",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bo Li",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "published": "2022-02-08T22:10:40Z",
        "summary": "Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2202.04173v3.pdf"
    },
    {
        "title": "Logical Reasoning for Task Oriented Dialogue Systems",
        "authors": [
            "Sajjad Beygi",
            "Maryam Fazel-Zarandi",
            "Alessandra Cervone",
            "Prakash Krishnan",
            "Siddhartha Reddy Jonnalagadda"
        ],
        "published": "2022-02-08T21:46:27Z",
        "summary": "In recent years, large pretrained models have been used in dialogue systems\nto improve successful task completion rates. However, lack of reasoning\ncapabilities of dialogue platforms make it difficult to provide relevant and\nfluent responses, unless the designers of a conversational experience spend a\nconsiderable amount of time implementing these capabilities in external rule\nbased modules. In this work, we propose a novel method to fine-tune pretrained\ntransformer models such as Roberta and T5. to reason over a set of facts in a\ngiven dialogue context. Our method includes a synthetic data generation\nmechanism which helps the model learn logical relations, such as comparison\nbetween list of numerical values, inverse relations (and negation), inclusion\nand exclusion for categorical attributes, and application of a combination of\nattributes over both numerical and categorical values, and spoken form for\nnumerical values, without need for additional training dataset. We show that\nthe transformer based model can perform logical reasoning to answer questions\nwhen the dialogue context contains all the required information, otherwise it\nis able to extract appropriate constraints to pass to downstream components\n(e.g. a knowledge base) when partial information is available. We observe that\ntransformer based models such as UnifiedQA-T5 can be fine-tuned to perform\nlogical reasoning (such as numerical and categorical attributes' comparison)\nover attributes that been seen in training time (e.g., accuracy of 90\\%+ for\ncomparison of smaller than $k_{\\max}$=5 values over heldout test dataset).",
        "pdf_link": "https://arxiv.org/pdf/2202.04161v1.pdf"
    },
    {
        "title": "Using a Language Model in a Kiosk Recommender System at Fast-Food Restaurants",
        "authors": [
            "Eduard Zubchuk",
            "Dmitry Menshikov",
            "Nikolay Mikhaylovskiy"
        ],
        "published": "2022-02-08T21:09:36Z",
        "summary": "Kiosks are a popular self-service option in many fast-food restaurants, they\nsave time for the visitors and save labor for the fast-food chains. In this\npaper, we propose an effective design of a kiosk shopping cart recommender\nsystem that combines a language model as a vectorizer and a neural\nnetwork-based classifier. The model performs better than other models in\noffline tests and exhibits performance comparable to the best models in A/B/C\ntests.",
        "pdf_link": "https://arxiv.org/pdf/2202.04145v1.pdf"
    },
    {
        "title": "PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX",
        "authors": [
            "Guangyao Zhou",
            "Antoine Dedieu",
            "Nishanth Kumar",
            "Wolfgang Lehrach",
            "Miguel L\u00e1zaro-Gredilla",
            "Shrinu Kushagra",
            "Dileep George"
        ],
        "published": "2022-02-08T19:27:48Z",
        "summary": "PGMax is an open-source Python package for (a) easily specifying discrete\nProbabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically\nrunning efficient and scalable loopy belief propagation (LBP) in JAX. PGMax\nsupports general factor graphs with tractable factors, and leverages modern\naccelerators like GPUs for inference. Compared with existing alternatives,\nPGMax obtains higher-quality inference results with up to three\norders-of-magnitude inference time speedups. PGMax additionally interacts\nseamlessly with the rapidly growing JAX ecosystem, opening up new research\npossibilities. Our source code, examples and documentation are available at\nhttps://github.com/deepmind/PGMax.",
        "pdf_link": "https://arxiv.org/pdf/2202.04110v4.pdf"
    },
    {
        "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models",
        "authors": [
            "Jaemin Cho",
            "Abhay Zala",
            "Mohit Bansal"
        ],
        "published": "2022-02-08T18:36:52Z",
        "summary": "Recently, DALL-E, a multimodal transformer language model, and its variants,\nincluding diffusion models, have shown high-quality text-to-image generation\ncapabilities. However, despite the realistic image generation results, there\nhas not been a detailed analysis of how to evaluate such models. In this work,\nwe investigate the visual reasoning capabilities and social biases of different\ntext-to-image models, covering both multimodal transformer language models and\ndiffusion models. First, we measure three visual reasoning skills: object\nrecognition, object counting, and spatial relation understanding. For this, we\npropose PaintSkills, a compositional diagnostic evaluation dataset that\nmeasures these skills. Despite the high-fidelity image generation capability, a\nlarge gap exists between the performance of recent models and the upper bound\naccuracy in object counting and spatial relation understanding skills. Second,\nwe assess the gender and skin tone biases by measuring the gender/skin tone\ndistribution of generated images across various professions and attributes. We\ndemonstrate that recent text-to-image generation models learn specific biases\nabout gender and skin tone from web image-text pairs. We hope our work will\nhelp guide future progress in improving text-to-image generation models on\nvisual reasoning skills and learning socially unbiased representations. Code\nand data: https://github.com/j-min/DallEval",
        "pdf_link": "https://arxiv.org/pdf/2202.04053v3.pdf"
    },
    {
        "title": "Differentiable N-gram Objective on Abstractive Summarization",
        "authors": [
            "Yunqi Zhu",
            "Xuebing Yang",
            "Yuanyuan Wu",
            "Mingjin Zhu",
            "Wensheng Zhang"
        ],
        "published": "2022-02-08T17:19:23Z",
        "summary": "ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.",
        "pdf_link": "https://arxiv.org/pdf/2202.04003v6.pdf"
    },
    {
        "title": "TimeLMs: Diachronic Language Models from Twitter",
        "authors": [
            "Daniel Loureiro",
            "Francesco Barbieri",
            "Leonardo Neves",
            "Luis Espinosa Anke",
            "Jose Camacho-Collados"
        ],
        "published": "2022-02-08T12:47:38Z",
        "summary": "Despite its importance, the time variable has been largely neglected in the\nNLP and language model literature. In this paper, we present TimeLMs, a set of\nlanguage models specialized on diachronic Twitter data. We show that a\ncontinual learning strategy contributes to enhancing Twitter-based language\nmodels' capacity to deal with future and out-of-distribution tweets, while\nmaking them competitive with standardized and more monolithic benchmarks. We\nalso perform a number of qualitative analyses showing how they cope with trends\nand peaks in activity involving specific named entities or concept drift.",
        "pdf_link": "https://arxiv.org/pdf/2202.03829v2.pdf"
    },
    {
        "title": "skrl: Modular and Flexible Library for Reinforcement Learning",
        "authors": [
            "Antonio Serrano-Mu\u00f1oz",
            "Dimitris Chrysostomou",
            "Simon B\u00f8gh",
            "Nestor Arana-Arexolaleiba"
        ],
        "published": "2022-02-08T12:43:31Z",
        "summary": "skrl is an open-source modular library for reinforcement learning written in\nPython and designed with a focus on readability, simplicity, and transparency\nof algorithm implementations. In addition to supporting environments that use\nthe traditional interfaces from OpenAI Gym and DeepMind, it provides the\nfacility to load, configure, and operate NVIDIA Isaac Gym and NVIDIA Omniverse\nIsaac Gym environments. Furthermore, it enables the simultaneous training of\nseveral agents with customizable scopes (subsets of environments among all\navailable ones), which may or may not share resources, in the same run. The\nlibrary's documentation can be found at https://skrl.readthedocs.io and its\nsource code is available on GitHub at https://github.com/Toni-SM/skrl.",
        "pdf_link": "https://arxiv.org/pdf/2202.03825v2.pdf"
    },
    {
        "title": "What are the best systems? New perspectives on NLP Benchmarking",
        "authors": [
            "Pierre Colombo",
            "Nathan Noiry",
            "Ekhine Irurozki",
            "Stephan Clemencon"
        ],
        "published": "2022-02-08T11:44:20Z",
        "summary": "In Machine Learning, a benchmark refers to an ensemble of datasets associated\nwith one or multiple metrics together with a way to aggregate different systems\nperformances. They are instrumental in (i) assessing the progress of new\nmethods along different axes and (ii) selecting the best systems for practical\nuse. This is particularly the case for NLP with the development of large\npre-trained models (e.g. GPT, BERT) that are expected to generalize well on a\nvariety of tasks. While the community mainly focused on developing new datasets\nand metrics, there has been little interest in the aggregation procedure, which\nis often reduced to a simple average over various performance measures.\nHowever, this procedure can be problematic when the metrics are on a different\nscale, which may lead to spurious conclusions. This paper proposes a new\nprocedure to rank systems based on their performance across different tasks.\nMotivated by the social choice theory, the final system ordering is obtained\nthrough aggregating the rankings induced by each task and is theoretically\ngrounded. We conduct extensive numerical experiments (on over 270k scores) to\nassess the soundness of our approach both on synthetic and real scores (e.g.\nGLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method\nyields different conclusions on state-of-the-art systems than the\nmean-aggregation procedure while being both more reliable and robust.",
        "pdf_link": "https://arxiv.org/pdf/2202.03799v4.pdf"
    },
    {
        "title": "Semantic features of object concepts generated with GPT-3",
        "authors": [
            "Hannes Hansen",
            "Martin N. Hebart"
        ],
        "published": "2022-02-08T09:51:48Z",
        "summary": "Semantic features have been playing a central role in investigating the\nnature of our conceptual representations. Yet the enormous time and effort\nrequired to empirically sample and norm features from human raters has\nrestricted their use to a limited set of manually curated concepts. Given\nrecent promising developments with transformer-based language models, here we\nasked whether it was possible to use such models to automatically generate\nmeaningful lists of properties for arbitrary object concepts and whether these\nmodels would produce features similar to those found in humans. To this end, we\nprobed a GPT-3 model to generate semantic features for 1,854 objects and\ncompared automatically-generated features to existing human feature norms.\nGPT-3 generated many more features than humans, yet showed a similar\ndistribution in the types of generated features. Generated feature norms\nrivaled human norms in predicting similarity, relatedness, and category\nmembership, while variance partitioning demonstrated that these predictions\nwere driven by similar variance in humans and GPT-3. Together, these results\nhighlight the potential of large language models to capture important facets of\nhuman knowledge and yield a new approach for automatically generating\ninterpretable feature sets, thus drastically expanding the potential use of\nsemantic features in psychological and linguistic studies.",
        "pdf_link": "https://arxiv.org/pdf/2202.03753v2.pdf"
    },
    {
        "title": "How to Understand Masked Autoencoders",
        "authors": [
            "Shuhao Cao",
            "Peng Xu",
            "David A. Clifton"
        ],
        "published": "2022-02-08T06:15:07Z",
        "summary": "\"Masked Autoencoders (MAE) Are Scalable Vision Learners\" revolutionizes the\nself-supervised learning method in that it not only achieves the\nstate-of-the-art for image pre-training, but is also a milestone that bridges\nthe gap between visual and linguistic masked autoencoding (BERT-style)\npre-trainings. However, to our knowledge, to date there are no theoretical\nperspectives to explain the powerful expressivity of MAE. In this paper, we,\nfor the first time, propose a unified theoretical framework that provides a\nmathematical understanding for MAE. Specifically, we explain the patch-based\nattention approaches of MAE using an integral kernel under a non-overlapping\ndomain decomposition setting. To help the research community to further\ncomprehend the main reasons of the great success of MAE, based on our\nframework, we pose five questions and answer them with mathematical rigor using\ninsights from operator theory.",
        "pdf_link": "https://arxiv.org/pdf/2202.03670v2.pdf"
    },
    {
        "title": "Causal Scene BERT: Improving object detection by searching for challenging groups of data",
        "authors": [
            "Cinjon Resnick",
            "Or Litany",
            "Amlan Kar",
            "Karsten Kreis",
            "James Lucas",
            "Kyunghyun Cho",
            "Sanja Fidler"
        ],
        "published": "2022-02-08T05:14:16Z",
        "summary": "Modern computer vision applications rely on learning-based perception modules\nparameterized with neural networks for tasks like object detection. These\nmodules frequently have low expected error overall but high error on atypical\ngroups of data due to biases inherent in the training process. In building\nautonomous vehicles (AV), this problem is an especially important challenge\nbecause their perception modules are crucial to the overall system performance.\nAfter identifying failures in AV, a human team will comb through the associated\ndata to group perception failures that share common causes. More data from\nthese groups is then collected and annotated before retraining the model to fix\nthe issue. In other words, error groups are found and addressed in hindsight.\nOur main contribution is a pseudo-automatic method to discover such groups in\nforesight by performing causal interventions on simulated scenes. To keep our\ninterventions on the data manifold, we utilize masked language models. We\nverify that the prioritized groups found via intervention are challenging for\nthe object detector and show that retraining with data collected from these\ngroups helps inordinately compared to adding more IID data. We also plan to\nrelease software to run interventions in simulated scenes, which we hope will\nbenefit the causality community.",
        "pdf_link": "https://arxiv.org/pdf/2202.03651v2.pdf"
    },
    {
        "title": "ECRECer: Enzyme Commission Number Recommendation and Benchmarking based on Multiagent Dual-core Learning",
        "authors": [
            "Zhenkun Shi",
            "Qianqian Yuan",
            "Ruoyu Wang",
            "Hoaran Li",
            "Xiaoping Liao",
            "Hongwu Ma"
        ],
        "published": "2022-02-08T04:00:49Z",
        "summary": "Enzyme Commission (EC) numbers, which associate a protein sequence with the\nbiochemical reactions it catalyzes, are essential for the accurate\nunderstanding of enzyme functions and cellular metabolism. Many ab-initio\ncomputational approaches were proposed to predict EC numbers for given input\nsequences directly. However, the prediction performance (accuracy, recall,\nprecision), usability, and efficiency of existing methods still have much room\nto be improved. Here, we report ECRECer, a cloud platform for accurately\npredicting EC numbers based on novel deep learning techniques. To build\nECRECer, we evaluate different protein representation methods and adopt a\nprotein language model for protein sequence embedding. After embedding, we\npropose a multi-agent hierarchy deep learning-based framework to learn the\nproposed tasks in a multi-task manner. Specifically, we used an extreme\nmulti-label classifier to perform the EC prediction and employed a greedy\nstrategy to integrate and fine-tune the final model. Comparative analyses\nagainst four representative methods demonstrate that ECRECer delivers the\nhighest performance, which improves accuracy and F1 score by 70% and 20% over\nthe state-of-the-the-art, respectively. With ECRECer, we can annotate numerous\nenzymes in the Swiss-Prot database with incomplete EC numbers to their full\nfourth level. Take UniPort protein \"A0A0U5GJ41\" as an example (1.14.-.-),\nECRECer annotated it with \"1.14.11.38\", which supported by further protein\nstructure analysis based on AlphaFold2. Finally, we established a webserver\n(https://ecrecer.biodesign.ac.cn) and provided an offline bundle to improve\nusability.",
        "pdf_link": "https://arxiv.org/pdf/2202.03632v1.pdf"
    },
    {
        "title": "Survey of Hallucination in Natural Language Generation",
        "authors": [
            "Ziwei Ji",
            "Nayeon Lee",
            "Rita Frieske",
            "Tiezheng Yu",
            "Dan Su",
            "Yan Xu",
            "Etsuko Ishii",
            "Yejin Bang",
            "Delong Chen",
            "Ho Shu Chan",
            "Wenliang Dai",
            "Andrea Madotto",
            "Pascale Fung"
        ],
        "published": "2022-02-08T03:55:01Z",
        "summary": "Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.",
        "pdf_link": "https://arxiv.org/pdf/2202.03629v6.pdf"
    },
    {
        "title": "HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic Analysis",
        "authors": [
            "Wenjun Qiu",
            "Yang Xu"
        ],
        "published": "2022-02-08T02:53:48Z",
        "summary": "Contextualized word embeddings have demonstrated state-of-the-art performance\nin various natural language processing tasks including those that concern\nhistorical semantic change. However, language models such as BERT was trained\nprimarily on contemporary corpus data. To investigate whether training on\nhistorical corpus data improves diachronic semantic analysis, we present a\npre-trained BERT-based language model, HistBERT, trained on the balanced Corpus\nof Historical American English. We examine the effectiveness of our approach by\ncomparing the performance of the original BERT and that of HistBERT, and we\nreport promising results in word similarity and semantic shift analysis. Our\nwork suggests that the effectiveness of contextual embeddings in diachronic\nsemantic analysis is dependent on the temporal profile of the input text and\ncare should be taken in applying this methodology to study historical semantic\nchange.",
        "pdf_link": "https://arxiv.org/pdf/2202.03612v1.pdf"
    },
    {
        "title": "Do Language Models Learn Position-Role Mappings?",
        "authors": [
            "Jackson Petty",
            "Michael Wilson",
            "Robert Frank"
        ],
        "published": "2022-02-08T02:50:53Z",
        "summary": "How is knowledge of position-role mappings in natural language learned? We\nexplore this question in a computational setting, testing whether a variety of\nwell-performing pertained language models (BERT, RoBERTa, and DistilBERT)\nexhibit knowledge of these mappings, and whether this knowledge persists across\nalternations in syntactic, structural, and lexical alternations. In Experiment\n1, we show that these neural models do indeed recognize distinctions between\ntheme and recipient roles in ditransitive constructions, and that these\ndistinct patterns are shared across construction type. We strengthen this\nfinding in Experiment 2 by showing that fine-tuning these language models on\nnovel theme- and recipient-like tokens in one paradigm allows the models to\nmake correct predictions about their placement in other paradigms, suggesting\nthat the knowledge of these mappings is shared rather than independently\nlearned. We do, however, observe some limitations of this generalization when\ntasks involve constructions with novel ditransitive verbs, hinting at a degree\nof lexical specificity which underlies model performance.",
        "pdf_link": "https://arxiv.org/pdf/2202.03611v1.pdf"
    },
    {
        "title": "Universal Spam Detection using Transfer Learning of BERT Model",
        "authors": [
            "Vijay Srinivas Tida",
            "Sonya Hsu"
        ],
        "published": "2022-02-07T19:37:39Z",
        "summary": "Deep learning transformer models become important by training on text data\nbased on self-attention mechanisms. This manuscript demonstrated a novel\nuniversal spam detection model using pre-trained Google's Bidirectional Encoder\nRepresentations from Transformers (BERT) base uncased models with four datasets\nby efficiently classifying ham or spam emails in real-time scenarios. Different\nmethods for Enron, Spamassain, Lingspam, and Spamtext message classification\ndatasets, were used to train models individually in which a single model was\nobtained with acceptable performance on four datasets. The Universal Spam\nDetection Model (USDM) was trained with four datasets and leveraged\nhyperparameters from each model. The combined model was finetuned with the same\nhyperparameters from these four models separately. When each model using its\ncorresponding dataset, an F1-score is at and above 0.9 in individual models. An\noverall accuracy reached 97%, with an F1 score of 0.96. Research results and\nimplications were discussed.",
        "pdf_link": "https://arxiv.org/pdf/2202.03480v1.pdf"
    },
    {
        "title": "Cedille: A large autoregressive French language model",
        "authors": [
            "Martin M\u00fcller",
            "Florian Laurent"
        ],
        "published": "2022-02-07T17:40:43Z",
        "summary": "Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.",
        "pdf_link": "https://arxiv.org/pdf/2202.03371v1.pdf"
    },
    {
        "title": "Fine-Tuning Approach for Arabic Offensive Language Detection System: BERT-Based Model",
        "authors": [
            "Fatemah Husain",
            "Ozlem Uzuner"
        ],
        "published": "2022-02-07T17:26:35Z",
        "summary": "The problem of online offensive language limits the health and security of\nonline users. It is essential to apply the latest state-of-the-art techniques\nin developing a system to detect online offensive language and to ensure social\njustice to the online communities. Our study investigates the effects of\nfine-tuning across several Arabic offensive language datasets. We develop\nmultiple classifiers that use four datasets individually and in combination in\norder to gain knowledge about online Arabic offensive content and classify\nusers comments accordingly. Our results demonstrate the limited effects of\ntransfer learning on the classifiers performance, particularly for highly\ndialectal comments.",
        "pdf_link": "https://arxiv.org/pdf/2203.03542v1.pdf"
    },
    {
        "title": "Soft Actor-Critic with Inhibitory Networks for Faster Retraining",
        "authors": [
            "Jaime S. Ide",
            "Daria Mi\u0107ovi\u0107",
            "Michael J. Guarino",
            "Kevin Alcedo",
            "David Rosenbluth",
            "Adrian P. Pope"
        ],
        "published": "2022-02-07T03:10:34Z",
        "summary": "Reusing previously trained models is critical in deep reinforcement learning\nto speed up training of new agents. However, it is unclear how to acquire new\nskills when objectives and constraints are in conflict with previously learned\nskills. Moreover, when retraining, there is an intrinsic conflict between\nexploiting what has already been learned and exploring new skills. In soft\nactor-critic (SAC) methods, a temperature parameter can be dynamically adjusted\nto weight the action entropy and balance the explore $\\times$ exploit\ntrade-off. However, controlling a single coefficient can be challenging within\nthe context of retraining, even more so when goals are contradictory. In this\nwork, inspired by neuroscience research, we propose a novel approach using\ninhibitory networks to allow separate and adaptive state value evaluations, as\nwell as distinct automatic entropy tuning. Ultimately, our approach allows for\ncontrolling inhibition to handle conflict between exploiting less risky,\nacquired behaviors and exploring novel ones to overcome more challenging tasks.\nWe validate our method through experiments in OpenAI Gym environments.",
        "pdf_link": "https://arxiv.org/pdf/2202.02918v2.pdf"
    },
    {
        "title": "Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data",
        "authors": [
            "Yaoqing Yang",
            "Ryan Theisen",
            "Liam Hodgkinson",
            "Joseph E. Gonzalez",
            "Kannan Ramchandran",
            "Charles H. Martin",
            "Michael W. Mahoney"
        ],
        "published": "2022-02-06T20:07:35Z",
        "summary": "Selecting suitable architecture parameters and training hyperparameters is\nessential for enhancing machine learning (ML) model performance. Several recent\nempirical studies conduct large-scale correlational analysis on neural networks\n(NNs) to search for effective \\emph{generalization metrics} that can guide this\ntype of model selection. Effective metrics are typically expected to correlate\nstrongly with test performance. In this paper, we expand on prior analyses by\nexamining generalization-metric-based model selection with the following\nobjectives: (i) focusing on natural language processing (NLP) tasks, as prior\nwork primarily concentrates on computer vision (CV) tasks; (ii) considering\nmetrics that directly predict \\emph{test error} instead of the\n\\emph{generalization gap}; (iii) exploring metrics that do not need access to\ndata to compute. From these objectives, we are able to provide the first model\nselection results on large pretrained Transformers from Huggingface using\ngeneralization metrics. Our analyses consider (I) hundreds of Transformers\ntrained in different settings, in which we systematically vary the amount of\ndata, the model size and the optimization hyperparameters, (II) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding GPT2, BERT, etc., and (III) a total of 28 existing and novel\ngeneralization metrics. Despite their niche status, we find that metrics\nderived from the heavy-tail (HT) perspective are particularly useful in NLP\ntasks, exhibiting stronger correlations than other, more popular metrics. To\nfurther examine these metrics, we extend prior formulations relying on power\nlaw (PL) spectral distributions to exponential (EXP) and\nexponentially-truncated power law (E-TPL) families.",
        "pdf_link": "https://arxiv.org/pdf/2202.02842v3.pdf"
    },
    {
        "title": "Ethics, Rules of Engagement, and AI: Neural Narrative Mapping Using Large Transformer Language Models",
        "authors": [
            "Philip Feldman",
            "Aaron Dant",
            "David Rosenbluth"
        ],
        "published": "2022-02-05T22:08:21Z",
        "summary": "The problem of determining if a military unit has correctly understood an\norder and is properly executing on it is one that has bedeviled military\nplanners throughout history. The advent of advanced language models such as\nOpenAI's GPT-series offers new possibilities for addressing this problem. This\npaper presents a mechanism to harness the narrative output of large language\nmodels and produce diagrams or \"maps\" of the relationships that are latent in\nthe weights of such models as the GPT-3. The resulting \"Neural Narrative Maps\"\n(NNMs), are intended to provide insight into the organization of information,\nopinion, and belief in the model, which in turn provide means to understand\nintent and response in the context of physical distance. This paper discusses\nthe problem of mapping information spaces in general, and then presents a\nconcrete implementation of this concept in the context of OpenAI's GPT-3\nlanguage model for determining if a subordinate is following a commander's\nintent in a high-risk situation. The subordinate's locations within the NNM\nallow a novel capability to evaluate the intent of the subordinate with respect\nto the commander. We show that is is possible not only to determine if they are\nnearby in narrative space, but also how they are oriented, and what\n\"trajectory\" they are on. Our results show that our method is able to produce\nhigh-quality maps, and demonstrate new ways of evaluating intent more\ngenerally.",
        "pdf_link": "https://arxiv.org/pdf/2202.02647v1.pdf"
    },
    {
        "title": "Classification on Sentence Embeddings for Legal Assistance",
        "authors": [
            "Arka Mitra"
        ],
        "published": "2022-02-05T20:57:05Z",
        "summary": "Legal proceedings take plenty of time and also cost a lot. The lawyers have\nto do a lot of work in order to identify the different sections of prior cases\nand statutes. The paper tries to solve the first tasks in AILA2021 (Artificial\nIntelligence for Legal Assistance) that will be held in FIRE2021 (Forum for\nInformation Retrieval Evaluation). The task is to semantically segment the\ndocument into different assigned one of the 7 predefined labels or \"rhetorical\nroles.\" The paper uses BERT to obtain the sentence embeddings from a sentence,\nand then a linear classifier is used to output the final prediction. The\nexperiments show that when more weightage is assigned to the class with the\nhighest frequency, the results are better than those when more weightage is\ngiven to the class with a lower frequency. In task 1, the team legalNLP\nobtained a F1 score of 0.22.",
        "pdf_link": "https://arxiv.org/pdf/2202.02639v1.pdf"
    },
    {
        "title": "Multilingual Hate Speech and Offensive Content Detection using Modified Cross-entropy Loss",
        "authors": [
            "Arka Mitra",
            "Priyanshu Sankhala"
        ],
        "published": "2022-02-05T20:31:40Z",
        "summary": "The number of increased social media users has led to a lot of people\nmisusing these platforms to spread offensive content and use hate speech.\nManual tracking the vast amount of posts is impractical so it is necessary to\ndevise automated methods to identify them quickly. Large language models are\ntrained on a lot of data and they also make use of contextual embeddings. We\nfine-tune the large language models to help in our task. The data is also quite\nunbalanced; so we used a modified cross-entropy loss to tackle the issue. We\nobserved that using a model which is fine-tuned in hindi corpora performs\nbetter. Our team (HNLP) achieved the macro F1-scores of 0.808, 0.639 in English\nSubtask A and English Subtask B respectively. For Hindi Subtask A, Hindi\nSubtask B our team achieved macro F1-scores of 0.737, 0.443 respectively in\nHASOC 2021.",
        "pdf_link": "https://arxiv.org/pdf/2202.02635v1.pdf"
    },
    {
        "title": "JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One-Shot Learning",
        "authors": [
            "Yash Jakhotiya",
            "Vaibhav Kumar",
            "Ashwin Pathak",
            "Raj Shah"
        ],
        "published": "2022-02-04T21:17:41Z",
        "summary": "Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.",
        "pdf_link": "https://arxiv.org/pdf/2202.02394v6.pdf"
    },
    {
        "title": "Webly Supervised Concept Expansion for General Purpose Vision Models",
        "authors": [
            "Amita Kamath",
            "Christopher Clark",
            "Tanmay Gupta",
            "Eric Kolve",
            "Derek Hoiem",
            "Aniruddha Kembhavi"
        ],
        "published": "2022-02-04T18:58:36Z",
        "summary": "General Purpose Vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from supervised datasets,\nlearn concepts from web image search, and leverage a key characteristic of\nGPVs: the ability to transfer visual knowledge across skills. We use a dataset\nof 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised\nconcept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5\nCOCO-based datasets (80 primary concepts), a newly curated series of 5 datasets\nbased on the OpenImages and VisualGenome repositories (~500 concepts), and the\nWeb-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2\nthat supports a variety of tasks -- from vision tasks like classification and\nlocalization to vision+language tasks like QA and captioning, to more niche\nones like human-object interaction detection. GPV-2 benefits hugely from web\ndata and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,\nand web demo are available at https://prior.allenai.org/projects/gpv2.",
        "pdf_link": "https://arxiv.org/pdf/2202.02317v2.pdf"
    },
    {
        "title": "StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?",
        "authors": [
            "Stefan Pasch",
            "Daniel Ehnes"
        ],
        "published": "2022-02-04T17:50:53Z",
        "summary": "To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.",
        "pdf_link": "https://arxiv.org/pdf/2202.02268v1.pdf"
    },
    {
        "title": "From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer",
        "authors": [
            "Xin Xie",
            "Ningyu Zhang",
            "Zhoubo Li",
            "Shumin Deng",
            "Hui Chen",
            "Feiyu Xiong",
            "Mosha Chen",
            "Huajun Chen"
        ],
        "published": "2022-02-04T12:52:32Z",
        "summary": "Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.",
        "pdf_link": "https://arxiv.org/pdf/2202.02113v7.pdf"
    },
    {
        "title": "Temporal Attention for Language Models",
        "authors": [
            "Guy D. Rosin",
            "Kira Radinsky"
        ],
        "published": "2022-02-04T11:55:34Z",
        "summary": "Pretrained language models based on the transformer architecture have shown\ngreat success in NLP. Textual training data often comes from the web and is\nthus tagged with time-specific information, but most language models ignore\nthis information. They are trained on the textual data alone, limiting their\nability to generalize temporally. In this work, we extend the key component of\nthe transformer architecture, i.e., the self-attention mechanism, and propose\ntemporal attention - a time-aware self-attention mechanism. Temporal attention\ncan be applied to any transformer model and requires the input texts to be\naccompanied with their relevant time points. It allows the transformer to\ncapture this temporal information and create time-specific contextualized word\nrepresentations. We leverage these representations for the task of semantic\nchange detection; we apply our proposed mechanism to BERT and experiment on\nthree datasets in different languages (English, German, and Latin) that also\nvary in time, size, and genre. Our proposed model achieves state-of-the-art\nresults on all the datasets.",
        "pdf_link": "https://arxiv.org/pdf/2202.02093v2.pdf"
    },
    {
        "title": "A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications",
        "authors": [
            "Vijini Liyanage",
            "Davide Buscaldi",
            "Adeline Nazarenko"
        ],
        "published": "2022-02-04T08:16:56Z",
        "summary": "Automatic text generation based on neural language models has achieved\nperformance levels that make the generated text almost indistinguishable from\nthose written by humans. Despite the value that text generation can have in\nvarious applications, it can also be employed for malicious tasks. The\ndiffusion of such practices represent a threat to the quality of academic\npublishing. To address these problems, we propose in this paper two datasets\ncomprised of artificially generated research content: a completely synthetic\ndataset and a partial text substitution dataset. In the first case, the content\nis completely generated by the GPT-2 model after a short prompt extracted from\noriginal papers. The partial or hybrid dataset is created by replacing several\nsentences of abstracts with sentences that are generated by the Arxiv-NLP\nmodel. We evaluate the quality of the datasets comparing the generated texts to\naligned original texts using fluency metrics such as BLEU and ROUGE. The more\nnatural the artificial texts seem, the more difficult they are to detect and\nthe better is the benchmark. We also evaluate the difficulty of the task of\ndistinguishing original from generated text by using state-of-the-art\nclassification models.",
        "pdf_link": "https://arxiv.org/pdf/2202.02013v2.pdf"
    },
    {
        "title": "A Unified Training Process for Fake News Detection based on Fine-Tuned BERT Model",
        "authors": [
            "Vijay Srinivas Tida",
            "Dr. Sonya Hsu",
            "Dr. Xiali Hei"
        ],
        "published": "2022-02-03T23:23:26Z",
        "summary": "An efficient fake news detector becomes essential as the accessibility of\nsocial media platforms increases rapidly.",
        "pdf_link": "https://arxiv.org/pdf/2202.01907v2.pdf"
    },
    {
        "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition",
        "authors": [
            "Chung-Cheng Chiu",
            "James Qin",
            "Yu Zhang",
            "Jiahui Yu",
            "Yonghui Wu"
        ],
        "published": "2022-02-03T21:29:04Z",
        "summary": "We present a simple and effective self-supervised learning approach for\nspeech recognition. The approach learns a model to predict the masked speech\nsignals, in the form of discrete labels generated with a random-projection\nquantizer. In particular the quantizer projects speech inputs with a randomly\ninitialized matrix, and does a nearest-neighbor lookup in a\nrandomly-initialized codebook. Neither the matrix nor the codebook is updated\nduring self-supervised learning. Since the random-projection quantizer is not\ntrained and is separated from the speech recognition model, the design makes\nthe approach flexible and is compatible with universal speech recognition\narchitecture. On LibriSpeech our approach achieves similar word-error-rates as\nprevious work using self-supervised learning with non-streaming models, and\nprovides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with\nstreaming models. On multilingual tasks the approach also provides significant\nimprovement over wav2vec 2.0 and w2v-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2202.01855v2.pdf"
    },
    {
        "title": "Pre-Trained Language Models for Interactive Decision-Making",
        "authors": [
            "Shuang Li",
            "Xavier Puig",
            "Chris Paxton",
            "Yilun Du",
            "Clinton Wang",
            "Linxi Fan",
            "Tao Chen",
            "De-An Huang",
            "Ekin Aky\u00fcrek",
            "Anima Anandkumar",
            "Jacob Andreas",
            "Igor Mordatch",
            "Antonio Torralba",
            "Yuke Zhu"
        ],
        "published": "2022-02-03T18:55:52Z",
        "summary": "Language model (LM) pre-training is useful in many language processing tasks.\nBut can pre-trained LMs be further leveraged for more general machine learning\nproblems? We propose an approach for using LMs to scaffold learning and\ngeneralization in general sequential decision-making problems. In this\napproach, goals and observations are represented as a sequence of embeddings,\nand a policy network initialized with a pre-trained LM predicts the next\naction. We demonstrate that this framework enables effective combinatorial\ngeneralization across different environments and supervisory modalities. We\nbegin by assuming access to a set of expert demonstrations, and show that\ninitializing policies with LMs and fine-tuning them via behavior cloning\nimproves task completion rates by 43.6% in the VirtualHome environment. Next,\nwe integrate an active data gathering procedure in which agents iteratively\ninteract with the environment, relabel past \"failed\" experiences with new\ngoals, and update their policies in a self-supervised loop. Active data\ngathering further improves combinatorial generalization, outperforming the best\nbaseline by 25.1%. Finally, we explain these results by investigating three\npossible factors underlying the effectiveness of the LM-based policy. We find\nthat sequential input representations (vs. fixed-dimensional feature vectors)\nand LM-based weight initialization are both important for generalization.\nSurprisingly, however, the format of the policy inputs encoding (e.g. as a\nnatural language string vs. an arbitrary sequential encoding) has little\ninfluence. Together, these results suggest that language modeling induces\nrepresentations that are useful for modeling not just language, but also goals\nand plans; these representations can aid learning and generalization even\noutside of language processing.",
        "pdf_link": "https://arxiv.org/pdf/2202.01771v4.pdf"
    },
    {
        "title": "mSLAM: Massively multilingual joint pre-training for speech and text",
        "authors": [
            "Ankur Bapna",
            "Colin Cherry",
            "Yu Zhang",
            "Ye Jia",
            "Melvin Johnson",
            "Yong Cheng",
            "Simran Khanuja",
            "Jason Riesa",
            "Alexis Conneau"
        ],
        "published": "2022-02-03T02:26:40Z",
        "summary": "We present mSLAM, a multilingual Speech and LAnguage Model that learns\ncross-lingual cross-modal representations of speech and text by pre-training\njointly on large amounts of unlabeled speech and text in multiple languages.\nmSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on\ncharacter-level text, along with Connectionist Temporal Classification (CTC)\nlosses on paired speech and transcript data, to learn a single model capable of\nlearning from and representing both speech and text signals in a shared\nrepresentation space. We evaluate mSLAM on several downstream speech\nunderstanding tasks and find that joint pre-training with text improves quality\non speech translation, speech intent classification and speech language-ID\nwhile being competitive on multilingual ASR, when compared against speech-only\npre-training. Our speech translation model demonstrates zero-shot text\ntranslation without seeing any text translation data, providing evidence for\ncross-modal alignment of representations. mSLAM also benefits from multi-modal\nfine-tuning, further improving the quality of speech translation by directly\nleveraging text translation data during the fine-tuning process. Our empirical\nanalysis highlights several opportunities and challenges arising from\nlarge-scale multimodal pre-training, suggesting directions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2202.01374v1.pdf"
    },
    {
        "title": "ASR-Aware End-to-end Neural Diarization",
        "authors": [
            "Aparna Khare",
            "Eunjung Han",
            "Yuguang Yang",
            "Andreas Stolcke"
        ],
        "published": "2022-02-02T21:17:14Z",
        "summary": "We present a Conformer-based end-to-end neural diarization (EEND) model that\nuses both acoustic input and features derived from an automatic speech\nrecognition (ASR) model. Two categories of features are explored: features\nderived directly from ASR output (phones, position-in-word and word boundaries)\nand features derived from a lexical speaker change detection model, trained by\nfine-tuning a pretrained BERT model on the ASR output. Three modifications to\nthe Conformer-based EEND architecture are proposed to incorporate the features.\nFirst, ASR features are concatenated with acoustic features. Second, we propose\na new attention mechanism called contextualized self-attention that utilizes\nASR features to build robust speaker representations. Finally, multi-task\nlearning is used to train the model to minimize classification loss for the ASR\nfeatures along with diarization loss. Experiments on the two-speaker English\nconversations of Switchboard+SRE data sets show that multi-task learning with\nposition-in-word information is the most effective way of utilizing ASR\nfeatures, reducing the diarization error rate (DER) by 20% relative to the\nbaseline.",
        "pdf_link": "https://arxiv.org/pdf/2202.01286v1.pdf"
    },
    {
        "title": "Unified Scaling Laws for Routed Language Models",
        "authors": [
            "Aidan Clark",
            "Diego de las Casas",
            "Aurelia Guy",
            "Arthur Mensch",
            "Michela Paganini",
            "Jordan Hoffmann",
            "Bogdan Damoc",
            "Blake Hechtman",
            "Trevor Cai",
            "Sebastian Borgeaud",
            "George van den Driessche",
            "Eliza Rutherford",
            "Tom Hennigan",
            "Matthew Johnson",
            "Katie Millican",
            "Albin Cassirer",
            "Chris Jones",
            "Elena Buchatskaya",
            "David Budden",
            "Laurent Sifre",
            "Simon Osindero",
            "Oriol Vinyals",
            "Jack Rae",
            "Erich Elsen",
            "Koray Kavukcuoglu",
            "Karen Simonyan"
        ],
        "published": "2022-02-02T17:58:52Z",
        "summary": "The performance of a language model has been shown to be effectively modeled\nas a power-law in its parameter count. Here we study the scaling behaviors of\nRouting Networks: architectures that conditionally use only a subset of their\nparameters while processing an input. For these models, parameter count and\ncomputational requirement form two independent axes along which an increase\nleads to better performance. In this work we derive and justify scaling laws\ndefined on these two variables which generalize those known for standard\nlanguage models and describe the performance of a wide range of routing\narchitectures trained via three different techniques. Afterwards we provide two\napplications of these laws: first deriving an Effective Parameter Count along\nwhich all models scale at the same rate, and then using the scaling\ncoefficients to give a quantitative comparison of the three routing techniques\nconsidered. Our analysis derives from an extensive evaluation of Routing\nNetworks across five orders of magnitude of size, including models with\nhundreds of experts and hundreds of billions of parameters.",
        "pdf_link": "https://arxiv.org/pdf/2202.01169v2.pdf"
    },
    {
        "title": "L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources",
        "authors": [
            "Raviraj Joshi"
        ],
        "published": "2022-02-02T17:35:52Z",
        "summary": "We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from\ndifferent internet sources. We expand the existing Marathi monolingual corpus\nwith 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,\nand MahaRoBerta all BERT-based masked language models, and MahaFT, the fast\ntext word embeddings both trained on full Marathi corpus with 752M tokens. We\nshow the effectiveness of these resources on downstream Marathi sentiment\nanalysis, text classification, and named entity recognition (NER) tasks. We\nalso release MahaGPT, a generative Marathi GPT model trained on Marathi corpus.\nMarathi is a popular language in India but still lacks these resources. This\nwork is a step forward in building open resources for the Marathi language. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2202.01159v2.pdf"
    },
    {
        "title": "Pop Quiz! Can a Large Language Model Help With Reverse Engineering?",
        "authors": [
            "Hammond Pearce",
            "Benjamin Tan",
            "Prashanth Krishnamurthy",
            "Farshad Khorrami",
            "Ramesh Karri",
            "Brendan Dolan-Gavitt"
        ],
        "published": "2022-02-02T17:09:15Z",
        "summary": "Large language models (such as OpenAI's Codex) have demonstrated impressive\nzero-shot multi-task capabilities in the software domain, including code\nexplanation. In this work, we examine if this ability can be used to help with\nreverse engineering. Specifically, we investigate prompting Codex to identify\nthe purpose, capabilities, and important variable names or values from code,\neven when the code is produced through decompilation. Alongside an examination\nof the model's responses in answering open-ended questions, we devise a\ntrue/false quiz framework to characterize the performance of the language\nmodel. We present an extensive quantitative analysis of the measured\nperformance of the language model on a set of program purpose identification\nand information extraction tasks: of the 136,260 questions we posed, it\nanswered 72,754 correctly. A key takeaway is that while promising, LLMs are not\nyet ready for zero-shot reverse engineering.",
        "pdf_link": "https://arxiv.org/pdf/2202.01142v1.pdf"
    },
    {
        "title": "RescoreBERT: Discriminative Speech Recognition Rescoring with BERT",
        "authors": [
            "Liyan Xu",
            "Yile Gu",
            "Jari Kolehmainen",
            "Haidar Khan",
            "Ankur Gandhe",
            "Ariya Rastrow",
            "Andreas Stolcke",
            "Ivan Bulyko"
        ],
        "published": "2022-02-02T15:45:26Z",
        "summary": "Second-pass rescoring is an important component in automatic speech\nrecognition (ASR) systems that is used to improve the outputs from a first-pass\ndecoder by implementing a lattice rescoring or $n$-best re-ranking. While\npretraining with a masked language model (MLM) objective has received great\nsuccess in various natural language understanding (NLU) tasks, it has not\ngained traction as a rescoring model for ASR. Specifically, training a\nbidirectional model like BERT on a discriminative objective such as minimum WER\n(MWER) has not been explored. Here we show how to train a BERT-based rescoring\nmodel with MWER loss, to incorporate the improvements of a discriminative loss\ninto fine-tuning of deep bidirectional pretrained models for ASR. Specifically,\nwe propose a fusion strategy that incorporates the MLM into the discriminative\ntraining process to effectively distill knowledge from a pretrained model. We\nfurther propose an alternative discriminative loss. This approach, which we\ncall RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech\nclean/other test sets over a BERT baseline without discriminative objective. We\nalso evaluate our method on an internal dataset from a conversational agent and\nfind that it reduces both latency and WER (by 3 to 8% relative) over an LSTM\nrescoring model.",
        "pdf_link": "https://arxiv.org/pdf/2202.01094v3.pdf"
    },
    {
        "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records",
        "authors": [
            "Xi Yang",
            "Aokun Chen",
            "Nima PourNejatian",
            "Hoo Chang Shin",
            "Kaleb E Smith",
            "Christopher Parisien",
            "Colin Compas",
            "Cheryl Martin",
            "Mona G Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Christopher A Harle",
            "Gloria Lipori",
            "Duane A Mitchell",
            "William R Hogan",
            "Elizabeth A Shenkman",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2022-02-02T14:28:51Z",
        "summary": "There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using >90 billion words of text (including >82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.",
        "pdf_link": "https://arxiv.org/pdf/2203.03540v3.pdf"
    },
    {
        "title": "Robust Training of Neural Networks Using Scale Invariant Architectures",
        "authors": [
            "Zhiyuan Li",
            "Srinadh Bhojanapalli",
            "Manzil Zaheer",
            "Sashank J. Reddi",
            "Sanjiv Kumar"
        ],
        "published": "2022-02-02T11:58:56Z",
        "summary": "In contrast to SGD, adaptive gradient methods like Adam allow robust training\nof modern deep networks, especially large language models. However, the use of\nadaptivity not only comes at the cost of extra memory but also raises the\nfundamental question: can non-adaptive methods like SGD enjoy similar benefits?\nIn this paper, we provide an affirmative answer to this question by proposing\nto achieve both robust and memory-efficient training via the following general\nrecipe: (1) modify the architecture and make it scale invariant, i.e. the scale\nof parameter doesn't affect the output of the network, (2) train with SGD and\nweight decay, and optionally (3) clip the global gradient norm proportional to\nweight norm multiplied by $\\sqrt{\\tfrac{2\\lambda}{\\eta}}$, where $\\eta$ is\nlearning rate and $\\lambda$ is weight decay. We show that this general approach\nis robust to rescaling of parameter and loss by proving that its convergence\nonly depends logarithmically on the scale of initialization and loss, whereas\nthe standard SGD might not even converge for many initializations. Following\nour recipe, we design a scale invariant version of BERT, called SIBERT, which\nwhen trained simply by vanilla SGD achieves performance comparable to BERT\ntrained by adaptive methods like Adam on downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2202.00980v2.pdf"
    },
    {
        "title": "What Has Been Enhanced in my Knowledge-Enhanced Language Model?",
        "authors": [
            "Yifan Hou",
            "Guoji Fu",
            "Mrinmaya Sachan"
        ],
        "published": "2022-02-02T11:23:36Z",
        "summary": "Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.",
        "pdf_link": "https://arxiv.org/pdf/2202.00964v7.pdf"
    },
    {
        "title": "Co-training Improves Prompt-based Learning for Large Language Models",
        "authors": [
            "Hunter Lang",
            "Monica Agrawal",
            "Yoon Kim",
            "David Sontag"
        ],
        "published": "2022-02-02T00:48:26Z",
        "summary": "We demonstrate that co-training (Blum & Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.",
        "pdf_link": "https://arxiv.org/pdf/2202.00828v1.pdf"
    },
    {
        "title": "A Semi-Supervised Deep Clustering Pipeline for Mining Intentions From Texts",
        "authors": [
            "Xinyu Chen",
            "Ian Beaver"
        ],
        "published": "2022-02-01T23:01:05Z",
        "summary": "Mining the latent intentions from large volumes of natural language inputs is\na key step to help data analysts design and refine Intelligent Virtual\nAssistants (IVAs) for customer service. To aid data analysts in this task we\npresent Verint Intent Manager (VIM), an analysis platform that combines\nunsupervised and semi-supervised approaches to help analysts quickly surface\nand organize relevant user intentions from conversational texts. For the\ninitial exploration of data we make use of a novel unsupervised and\nsemi-supervised pipeline that integrates the fine-tuning of high performing\nlanguage models, a distributed k-NN graph building method and community\ndetection techniques for mining the intentions and topics from texts. The\nfine-tuning step is necessary because pre-trained language models cannot encode\ntexts to efficiently surface particular clustering structures when the target\ntexts are from an unseen domain or the clustering task is not topic detection.\nFor flexibility we deploy two clustering approaches: where the number of\nclusters must be specified and where the number of clusters is detected\nautomatically with comparable clustering quality but at the expense of\nadditional computation time. We describe the application and deployment and\ndemonstrate its performance using BERT on three text mining tasks. Our\nexperiments show that BERT begins to produce better task-aware representations\nusing a labeled subset as small as 0.5% of the task data. The clustering\nquality exceeds the state-of-the-art results when BERT is fine-tuned with\nlabeled subsets of only 2.5% of the task data. As deployed in the VIM\napplication, this flexible clustering pipeline produces high quality results,\nimproving the performance of data analysts and reducing the time it takes to\nsurface intentions from customer service data, thereby reducing the time it\ntakes to build and deploy IVAs in new domains.",
        "pdf_link": "https://arxiv.org/pdf/2202.00802v1.pdf"
    },
    {
        "title": "BEA-Base: A Benchmark for ASR of Spontaneous Hungarian",
        "authors": [
            "P. Mihajlik",
            "A. Balog",
            "T. E. Gr\u00e1czi",
            "A. Koh\u00e1ri",
            "B. Tarj\u00e1n",
            "K. M\u00e1dy"
        ],
        "published": "2022-02-01T17:45:22Z",
        "summary": "Hungarian is spoken by 15 million people, still, easily accessible Automatic\nSpeech Recognition (ASR) benchmark datasets - especially for spontaneous speech\n- have been practically unavailable. In this paper, we introduce BEA-Base, a\nsubset of the BEA spoken Hungarian database comprising mostly spontaneous\nspeech of 140 speakers. It is built specifically to assess ASR, primarily for\nconversational AI applications. After defining the speech recognition subsets\nand task, several baselines - including classic HMM-DNN hybrid and end-to-end\napproaches augmented by cross-language transfer learning - are developed using\nopen-source toolkits. The best results obtained are based on multilingual\nself-supervised pretraining, achieving a 45% recognition error rate reduction\nas compared to the classical approach - without the application of an external\nlanguage model or additional supervised data. The results show the feasibility\nof using BEA-Base for training and evaluation of Hungarian speech recognition\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2202.00601v1.pdf"
    },
    {
        "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation",
        "authors": [
            "Biao Zhang",
            "Behrooz Ghorbani",
            "Ankur Bapna",
            "Yong Cheng",
            "Xavier Garcia",
            "Jonathan Shen",
            "Orhan Firat"
        ],
        "published": "2022-02-01T16:20:15Z",
        "summary": "Natural language understanding and generation models follow one of the two\ndominant architectural paradigms: language models (LMs) that process\nconcatenated sequences in a single stack of layers, and encoder-decoder models\n(EncDec) that utilize separate layer stacks for input and output processing. In\nmachine translation, EncDec has long been the favoured approach, but with few\nstudies investigating the performance of LMs. In this work, we thoroughly\nexamine the role of several architectural design choices on the performance of\nLMs on bilingual, (massively) multilingual and zero-shot translation tasks,\nunder systematic variations of data conditions and model sizes. Our results\nshow that: (i) Different LMs have different scaling properties, where\narchitectural differences often have a significant impact on model performance\nat small scales, but the performance gap narrows as the number of parameters\nincreases, (ii) Several design choices, including causal masking and\nlanguage-modeling objectives for the source sequence, have detrimental effects\non translation quality, and (iii) When paired with full-visible masking for\nsource sequences, LMs could perform on par with EncDec on supervised bilingual\nand multilingual translation tasks, and improve greatly on zero-shot directions\nby facilitating the reduction of off-target translations.",
        "pdf_link": "https://arxiv.org/pdf/2202.00528v3.pdf"
    },
    {
        "title": "AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB",
        "authors": [
            "Zhangyang Gao",
            "Cheng Tan",
            "Stan Z. Li"
        ],
        "published": "2022-02-01T08:28:24Z",
        "summary": "While DeepMind has tentatively solved protein folding, its inverse problem --\nprotein design which predicts protein sequences from their 3D structures --\nstill faces significant challenges. Particularly, the lack of large-scale\nstandardized benchmark and poor accuray hinder the research progress. In order\nto standardize comparisons and draw more research interest, we use AlphaFold\nDB, one of the world's largest protein structure databases, to establish a new\ngraph-based benchmark -- AlphaDesign. Based on AlphaDesign, we propose a new\nmethod called ADesign to improve accuracy by introducing protein angles as new\nfeatures, using a simplified graph transformer encoder (SGT), and proposing a\nconfidence-aware protein decoder (CPD). Meanwhile, SGT and CPD also improve\nmodel efficiency by simplifying the training and testing procedures.\nExperiments show that ADesign significantly outperforms previous graph models,\ne.g., the average accuracy is improved by 8\\%, and the inference speed is 40+\ntimes faster than before.",
        "pdf_link": "https://arxiv.org/pdf/2202.01079v2.pdf"
    },
    {
        "title": "Transformer-based Models of Text Normalization for Speech Applications",
        "authors": [
            "Jae Hun Ro",
            "Felix Stahlberg",
            "Ke Wu",
            "Shankar Kumar"
        ],
        "published": "2022-02-01T00:03:41Z",
        "summary": "Text normalization, or the process of transforming text into a consistent,\ncanonical form, is crucial for speech applications such as text-to-speech\nsynthesis (TTS). In TTS, the system must decide whether to verbalize \"1995\" as\n\"nineteen ninety five\" in \"born in 1995\" or as \"one thousand nine hundred\nninety five\" in \"page 1995\". We present an experimental comparison of various\nTransformer-based sequence-to-sequence (seq2seq) models of text normalization\nfor speech and evaluate them on a variety of datasets of written text aligned\nto its normalized spoken form. These models include variants of the 2-stage\nRNN-based tagging/seq2seq architecture introduced by Zhang et al. (2019), where\nwe replace the RNN with a Transformer in one or more stages, as well as vanilla\nTransformers that output string representations of edit sequences. Of our\napproaches, using Transformers for sentence context encoding within the 2-stage\nmodel proved most effective, with the fine-tuned BERT encoder yielding the best\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2202.00153v1.pdf"
    },
    {
        "title": "Learning affective meanings that derives the social behavior using Bidirectional Encoder Representations from Transformers",
        "authors": [
            "Moeen Mostafavi",
            "Michael D. Porter",
            "Dawn T. Robinson"
        ],
        "published": "2022-01-31T19:58:28Z",
        "summary": "Predicting the outcome of a process requires modeling the system dynamic and\nobserving the states. In the context of social behaviors, sentiments\ncharacterize the states of the system. Affect Control Theory (ACT) uses\nsentiments to manifest potential interaction. ACT is a generative theory of\nculture and behavior based on a three-dimensional sentiment lexicon.\nTraditionally, the sentiments are quantified using survey data which is fed\ninto a regression model to explain social behavior. The lexicons used in the\nsurvey are limited due to prohibitive cost. This paper uses a fine-tuned\nBidirectional Encoder Representations from Transformers (BERT) model to develop\na replacement for these surveys. This model achieves state-of-the-art accuracy\nin estimating affective meanings, expanding the affective lexicon, and allowing\nmore behaviors to be explained.",
        "pdf_link": "https://arxiv.org/pdf/2202.00065v2.pdf"
    },
    {
        "title": "Score vs. Winrate in Score-Based Games: which Reward for Reinforcement Learning?",
        "authors": [
            "Luca Pasqualini",
            "Gianluca Amato",
            "Marco Fantozzi",
            "Rosa Gini",
            "Alessandro Marchetti",
            "Carlo Metta",
            "Francesco Morandin",
            "Maurizio Parton"
        ],
        "published": "2022-01-31T12:38:02Z",
        "summary": "In the last years, the DeepMind algorithm AlphaZero has become the state of\nthe art to efficiently tackle perfect information two-player zero-sum games\nwith a win/lose outcome. However, when the win/lose outcome is decided by a\nfinal score difference, AlphaZero may play score-suboptimal moves because all\nwinning final positions are equivalent from the win/lose outcome perspective.\nThis can be an issue, for instance when used for teaching, or when trying to\nunderstand whether there is a better move. Moreover, there is the theoretical\nquest for the perfect game. A naive approach would be training an\nAlphaZero-like agent to predict score differences instead of win/lose outcomes.\nSince the game of Go is deterministic, this should as well produce an\noutcome-optimal play. However, it is a folklore belief that \"this does not\nwork\".\n  In this paper, we first provide empirical evidence for this belief. We then\ngive a theoretical interpretation of this suboptimality in general perfect\ninformation two-player zero-sum game where the complexity of a game like Go is\nreplaced by the randomness of the environment. We show that an outcome-optimal\npolicy has a different preference for uncertainty when it is winning or losing.\nIn particular, when in a losing state, an outcome-optimal agent chooses actions\nleading to a higher score variance. We then posit that when approximation is\ninvolved, a deterministic game behaves like a nondeterministic game, where the\nscore variance is modeled by how uncertain the position is. We validate this\nhypothesis in AlphaZero-like software with a human expert.",
        "pdf_link": "https://arxiv.org/pdf/2201.13176v2.pdf"
    },
    {
        "title": "Assessment of DeepONet for reliability analysis of stochastic nonlinear dynamical systems",
        "authors": [
            "Shailesh Garg",
            "Harshit Gupta",
            "Souvik Chakraborty"
        ],
        "published": "2022-01-31T11:41:08Z",
        "summary": "Time dependent reliability analysis and uncertainty quantification of\nstructural system subjected to stochastic forcing function is a challenging\nendeavour as it necessitates considerable computational time. We investigate\nthe efficacy of recently proposed DeepONet in solving time dependent\nreliability analysis and uncertainty quantification of systems subjected to\nstochastic loading. Unlike conventional machine learning and deep learning\nalgorithms, DeepONet learns is a operator network and learns a function to\nfunction mapping and hence, is ideally suited to propagate the uncertainty from\nthe stochastic forcing function to the output responses. We use DeepONet to\nbuild a surrogate model for the dynamical system under consideration. Multiple\ncase studies, involving both toy and benchmark problems, have been conducted to\nexamine the efficacy of DeepONet in time dependent reliability analysis and\nuncertainty quantification of linear and nonlinear dynamical systems. Results\nobtained indicate that the DeepONet architecture is accurate as well as\nefficient. Moreover, DeepONet posses zero shot learning capabilities and hence,\na trained model easily generalizes to unseen and new environment with no\nfurther training.",
        "pdf_link": "https://arxiv.org/pdf/2201.13145v1.pdf"
    },
    {
        "title": "Disaster Tweets Classification using BERT-Based Language Model",
        "authors": [
            "Anh Duc Le"
        ],
        "published": "2022-01-31T10:25:29Z",
        "summary": "Social networking services have became an important communication channel in\ntime of emergency. The aim of this study is to create a machine learning\nlanguage model that is able to investigate if a person or area was in danger or\nnot. The ubiquitousness of smartphones enables people to announce an emergency\nthey are observing in real-time. Because of this, more agencies are interested\nin programmatically monitoring Twitter (i.e. disaster relief organizations and\nnews agencies). Design a language model that is able to understand and\nacknowledge when a disaster is happening based on the social network posts will\nbecome more and more necessary over time.",
        "pdf_link": "https://arxiv.org/pdf/2202.00795v1.pdf"
    },
    {
        "title": "Similarity Learning based Few Shot Learning for ECG Time Series Classification",
        "authors": [
            "Priyanka Gupta",
            "Sathvik Bhaskarpandit",
            "Manik Gupta"
        ],
        "published": "2022-01-31T09:47:15Z",
        "summary": "Using deep learning models to classify time series data generated from the\nInternet of Things (IoT) devices requires a large amount of labeled data.\nHowever, due to constrained resources available in IoT devices, it is often\ndifficult to accommodate training using large data sets. This paper proposes\nand demonstrates a Similarity Learning-based Few Shot Learning for ECG\narrhythmia classification using Siamese Convolutional Neural Networks. Few shot\nlearning resolves the data scarcity issue by identifying novel classes from\nvery few labeled examples. Few Shot Learning relies first on pretraining the\nmodel on a related relatively large database, and then the learning is used for\nfurther adaptation towards few examples available per class. Our experiments\nevaluate the performance accuracy with respect to K (number of instances per\nclass) for ECG time series data classification. The accuracy with 5- shot\nlearning is 92.25% which marginally improves with further increase in K. We\nalso compare the performance of our method against other well-established\nsimilarity learning techniques such as Dynamic Time Warping (DTW), Euclidean\nDistance (ED), and a deep learning model - Long Short Term Memory Fully\nConvolutional Network (LSTM-FCN) with the same amount of data and conclude that\nour method outperforms them for a limited dataset size. For K=5, the accuracies\nobtained are 57%, 54%, 33%, and 92% approximately for ED, DTW, LSTM-FCN, and\nSCNN, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2202.00612v1.pdf"
    },
    {
        "title": "A Frustratingly Simple Approach for End-to-End Image Captioning",
        "authors": [
            "Ziyang Luo",
            "Yadong Xi",
            "Rongsheng Zhang",
            "Jing Ma"
        ],
        "published": "2022-01-30T04:44:54Z",
        "summary": "Image Captioning is a fundamental task to join vision and language,\nconcerning about cross-modal understanding and text generation. Recent years\nwitness the emerging attention on image captioning. Most of existing works\nfollow a traditional two-stage training paradigm. Before training the\ncaptioning models, an extra object detector is utilized to recognize the\nobjects in the image at first. However, they require sizeable datasets with\nfine-grained object annotation for training the object detector, which is a\ndaunting task. In addition, the errors of the object detectors are easy to\npropagate to the following captioning models, degenerating models' performance.\nTo alleviate such defects, we propose a frustratingly simple but highly\neffective end-to-end image captioning framework, Visual Conditioned GPT\n(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language\ndecoder (GPT2). Different from the vanilla connection method that directly\ninserts the cross-attention modules into GPT2, we come up with a self-ensemble\ncross-modal fusion mechanism that comprehensively considers both the single-\nand cross-modal knowledge. As a result, we do not need extra object detectors\nfor model training. Experimental results conducted on three popular image\ncaptioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our\nVC-GPT achieves either the best or the second-best performance across all\nevaluation metrics over extensive baseline systems.",
        "pdf_link": "https://arxiv.org/pdf/2201.12723v3.pdf"
    },
    {
        "title": "MVPTR: Multi-Level Semantic Alignment for Vision-Language Pre-Training via Multi-Stage Learning",
        "authors": [
            "Zejun Li",
            "Zhihao Fan",
            "Huaixiao Tou",
            "Jingjing Chen",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "published": "2022-01-29T14:30:59Z",
        "summary": "Previous vision-language pre-training models mainly construct multi-modal\ninputs with tokens and objects (pixels) followed by performing cross-modality\ninteraction between them. We argue that the input of only tokens and object\nfeatures limits high-level semantic alignment like phrase-to-region grounding.\nMeanwhile, multi-level alignments are inherently consistent and able to\nfacilitate the representation learning synergistically. Therefore, in this\npaper, we propose to learn Multi-level semantic alignment for Vision-language\nPre-TRaining (MVPTR). In MVPTR, we follow the nested structure of both\nmodalities to introduce concepts as high-level semantics. To ease the learning\nfrom multi-modal multi-level inputs, our framework is split into two stages,\nthe first stage focuses on intra-modality multi-level representation learning,\nthe second enforces interactions across modalities via both coarse-grained and\nfine-grained semantic alignment tasks. In addition to the commonly used\nimage-text matching and masked language model tasks, we introduce a masked\nconcept recovering task in the first stage to enhance the concept\nrepresentation learning, and two more tasks in the second stage to explicitly\nencourage multi-level alignments across modalities. Our code is available at\nhttps://github.com/Junction4Nako/mvp_pytorch.",
        "pdf_link": "https://arxiv.org/pdf/2201.12596v3.pdf"
    },
    {
        "title": "AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models",
        "authors": [
            "Dongkuan Xu",
            "Subhabrata Mukherjee",
            "Xiaodong Liu",
            "Debadeepta Dey",
            "Wenhui Wang",
            "Xiang Zhang",
            "Ahmed Hassan Awadallah",
            "Jianfeng Gao"
        ],
        "published": "2022-01-29T06:13:04Z",
        "summary": "Knowledge distillation (KD) methods compress large models into smaller\nstudents with manually-designed student architectures given pre-specified\ncomputational cost. This requires several trials to find a viable student, and\nfurther repeating the process for each student or computational budget change.\nWe use Neural Architecture Search (NAS) to automatically distill several\ncompressed students with variable cost from a large model. Current works train\na single SuperLM consisting of millions of subnetworks with weight-sharing,\nresulting in interference between subnetworks of different sizes. Our framework\nAutoDistil addresses above challenges with the following steps: (a)\nIncorporates inductive bias and heuristics to partition Transformer search\nspace into K compact sub-spaces (K=3 for typical student sizes of base, small\nand tiny); (b) Trains one SuperLM for each sub-space using task-agnostic\nobjective (e.g., self-attention distillation) with weight-sharing of students;\n(c) Lightweight search for the optimal student without re-training. Fully\ntask-agnostic training and search allow students to be reused for fine-tuning\non any downstream task. Experiments on GLUE benchmark against state-of-the-art\nKD and NAS methods demonstrate AutoDistil to outperform leading compression\ntechniques with upto 2.7x reduction in computational cost and negligible loss\nin task performance.",
        "pdf_link": "https://arxiv.org/pdf/2201.12507v2.pdf"
    },
    {
        "title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise",
        "authors": [
            "Minjia Zhang",
            "Niranjan Uma Naresh",
            "Yuxiong He"
        ],
        "published": "2022-01-29T01:47:01Z",
        "summary": "In recent years, large pre-trained Transformer-based language models have led\nto dramatic improvements in many natural language understanding tasks. To train\nthese models with increasing sizes, many neural network practitioners attempt\nto increase the batch sizes in order to leverage multiple GPUs to improve\ntraining speed. However, increasing the batch size often makes the optimization\nmore difficult, leading to slow convergence or poor generalization that can\nrequire orders of magnitude more training time to achieve the same model\nquality. In this paper, we explore the steepness of the loss landscape of\nlarge-batch optimization for adapting pre-trained Transformer-based language\nmodels to domain-specific tasks and find that it tends to be highly complex and\nirregular, posing challenges to generalization on downstream tasks.\n  To tackle this challenge, we propose ScaLA, a novel and efficient method to\naccelerate the adaptation speed of pre-trained transformer networks. Different\nfrom prior methods, we take a sequential game-theoretic approach by adding\nlightweight adversarial noise into large-batch optimization, which\nsignificantly improves adaptation speed while preserving model generalization.\nExperiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups\nover the baseline for GLUE on BERT-base and RoBERTa-large, while achieving\ncomparable and sometimes higher accuracy than the state-of-the-art large-batch\noptimization methods. Finally, we also address the theoretical aspect of\nlarge-batch optimization with adversarial noise and provide a theoretical\nconvergence rate analysis for ScaLA using techniques for analyzing non-convex\nsaddle-point problems.",
        "pdf_link": "https://arxiv.org/pdf/2201.12469v1.pdf"
    },
    {
        "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
        "authors": [
            "Uri Alon",
            "Frank F. Xu",
            "Junxian He",
            "Sudipta Sengupta",
            "Dan Roth",
            "Graham Neubig"
        ],
        "published": "2022-01-28T21:38:56Z",
        "summary": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .",
        "pdf_link": "https://arxiv.org/pdf/2201.12431v2.pdf"
    },
    {
        "title": "Schema-Free Dependency Parsing via Sequence Generation",
        "authors": [
            "Boda Lin",
            "Zijun Yao",
            "Jiaxin Shi",
            "Shulin Cao",
            "Binghao Tang",
            "Si Li",
            "Yong Luo",
            "Juanzi Li",
            "Lei Hou"
        ],
        "published": "2022-01-28T20:32:04Z",
        "summary": "Dependency parsing aims to extract syntactic dependency structure or semantic\ndependency structure for sentences. Existing methods suffer the drawbacks of\nlacking universality or highly relying on the auxiliary decoder. To remedy\nthese drawbacks, we propose to achieve universal and schema-free Dependency\nParsing (DP) via Sequence Generation (SG) DPSG by utilizing only the\npre-trained language model (PLM) without any auxiliary structures or parsing\nalgorithms. We first explore different serialization designing strategies for\nconverting parsing structures into sequences. Then we design dependency units\nand concatenate these units into the sequence for DPSG. Thanks to the high\nflexibility of the sequence generation, our DPSG can achieve both syntactic DP\nand semantic DP using a single model. By concatenating the prefix to indicate\nthe specific schema with the sequence, our DPSG can even accomplish\nmulti-schemata parsing. The effectiveness of our DPSG is demonstrated by the\nexperiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and\nSemEval16. DPSG achieves comparable results with the first-tier methods on all\nthe benchmarks and even the state-of-the-art (SOTA) performance in CODT and\nSemEval16. This paper demonstrates our DPSG has the potential to be a new\nparsing paradigm. We will release our codes upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2201.12407v1.pdf"
    },
    {
        "title": "Describing Differences between Text Distributions with Natural Language",
        "authors": [
            "Ruiqi Zhong",
            "Charlie Snell",
            "Dan Klein",
            "Jacob Steinhardt"
        ],
        "published": "2022-01-28T18:38:13Z",
        "summary": "How do two distributions of texts differ? Humans are slow at answering this,\nsince discovering patterns might require tediously reading through hundreds of\nsamples. We propose to automatically summarize the differences by \"learning a\nnatural language hypothesis\": given two distributions $D_{0}$ and $D_{1}$, we\nsearch for a description that is more often true for $D_{1}$, e.g., \"is\nmilitary-related.\" To tackle this problem, we fine-tune GPT-3 to propose\ndescriptions with the prompt: \"[samples of $D_{0}$] + [samples of $D_{1}$] +\nthe difference between them is_____.\" We then re-rank the descriptions by\nchecking how often they hold on a larger set of samples with a learned\nverifier. On a benchmark of 54 real-world binary classification tasks, while\nGPT-3 Curie (13B) only generates a description similar to human annotation 7%\nof the time, the performance reaches 61% with fine-tuning and re-ranking, and\nour best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to\ndescribe distribution shifts, debug dataset shortcuts, summarize unknown tasks,\nand label text clusters, and present analyses based on automatically generated\ndescriptions.",
        "pdf_link": "https://arxiv.org/pdf/2201.12323v2.pdf"
    },
    {
        "title": "A Post-Quantum Associative Memory",
        "authors": [
            "Ludovico Lami",
            "Daniel Goldwater",
            "Gerardo Adesso"
        ],
        "published": "2022-01-28T18:10:19Z",
        "summary": "Associative memories are devices storing information that can be fully\nretrieved given partial disclosure of it. We examine a toy model of associative\nmemory and the ultimate limitations it is subjected to within the framework of\ngeneral probabilistic theories (GPTs), which represent the most general class\nof physical theories satisfying some basic operational axioms. We ask ourselves\nhow large the dimension of a GPT should be so that it can accommodate $2^m$\nstates with the property that any $N$ of them are perfectly distinguishable.\nCall $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and\nGr\\\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the\nGPT is required to be either classical or quantum. This yields an example of a\ntask where GPTs outperform both classical and quantum theory exponentially.\nMore generally, we resolve the case of fixed $N$ and asymptotically large $m$,\nproving that $d(N,m) \\leq m^{1+o_N(1)}$ (as $m\\to\\infty$) for every $N\\geq 2$,\nwhich yields again an exponential improvement over classical and quantum\ntheories. Finally, we develop a numerical approach to the general problem of\nfinding the largest $N$-wise mutually distinguishable set for a given GPT,\nwhich can be seen as an instance of the maximum clique problem on $N$-regular\nhypergraphs.",
        "pdf_link": "https://arxiv.org/pdf/2201.12305v3.pdf"
    },
    {
        "title": "From data to functa: Your data point is a function and you can treat it like one",
        "authors": [
            "Emilien Dupont",
            "Hyunjik Kim",
            "S. M. Ali Eslami",
            "Danilo Rezende",
            "Dan Rosenbaum"
        ],
        "published": "2022-01-28T15:59:58Z",
        "summary": "It is common practice in deep learning to represent a measurement of the\nworld on a discrete grid, e.g. a 2D grid of pixels. However, the underlying\nsignal represented by these measurements is often continuous, e.g. the scene\ndepicted in an image. A powerful continuous alternative is then to represent\nthese measurements using an implicit neural representation, a neural function\ntrained to output the appropriate measurement value for any input spatial\nlocation. In this paper, we take this idea to its next level: what would it\ntake to perform deep learning on these functions instead, treating them as\ndata? In this context we refer to the data as functa, and propose a framework\nfor deep learning on functa. This view presents a number of challenges around\nefficient conversion from data to functa, compact representation of functa, and\neffectively solving downstream tasks on functa. We outline a recipe to overcome\nthese challenges and apply it to a wide range of data modalities including\nimages, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We\ndemonstrate that this approach has various compelling properties across data\nmodalities, in particular on the canonical tasks of generative modeling, data\nimputation, novel view synthesis and classification. Code:\nhttps://github.com/deepmind/functa",
        "pdf_link": "https://arxiv.org/pdf/2201.12204v3.pdf"
    },
    {
        "title": "Protum: A New Method For Prompt Tuning Based on \"[MASK]\"",
        "authors": [
            "Pan He",
            "Yuxi Chen",
            "Yan Wang",
            "Yanru Zhang"
        ],
        "published": "2022-01-28T13:34:30Z",
        "summary": "Recently, prompt tuning \\cite{lester2021power} has gradually become a new\nparadigm for NLP, which only depends on the representation of the words by\nfreezing the parameters of pre-trained language models (PLMs) to obtain\nremarkable performance on downstream tasks. It maintains the consistency of\nMasked Language Model (MLM) \\cite{devlin2018bert} task in the process of\npre-training, and avoids some issues that may happened during fine-tuning.\nNaturally, we consider that the \"[MASK]\" tokens carry more useful information\nthan other tokens because the model combines with context to predict the masked\ntokens. Among the current prompt tuning methods, there will be a serious\nproblem of random composition of the answer tokens in prediction when they\npredict multiple words so that they have to map tokens to labels with the help\nverbalizer. In response to the above issue, we propose a new \\textbf{Pro}mpt\n\\textbf{Tu}ning based on \"[\\textbf{M}ASK]\" (\\textbf{Protum}) method in this\npaper, which constructs a classification task through the information carried\nby the hidden layer of \"[MASK]\" tokens and then predicts the labels directly\nrather than the answer tokens. At the same time, we explore how different\nhidden layers under \"[MASK]\" impact on our classification model on many\ndifferent data sets. Finally, we find that our \\textbf{Protum} can achieve much\nbetter performance than fine-tuning after continuous pre-training with less\ntime consumption. Our model facilitates the practical application of large\nmodels in NLP.",
        "pdf_link": "https://arxiv.org/pdf/2201.12109v1.pdf"
    },
    {
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
        "authors": [
            "Shaden Smith",
            "Mostofa Patwary",
            "Brandon Norick",
            "Patrick LeGresley",
            "Samyam Rajbhandari",
            "Jared Casper",
            "Zhun Liu",
            "Shrimai Prabhumoye",
            "George Zerveas",
            "Vijay Korthikanti",
            "Elton Zhang",
            "Rewon Child",
            "Reza Yazdani Aminabadi",
            "Julie Bernauer",
            "Xia Song",
            "Mohammad Shoeybi",
            "Yuxiong He",
            "Michael Houston",
            "Saurabh Tiwary",
            "Bryan Catanzaro"
        ],
        "published": "2022-01-28T08:59:57Z",
        "summary": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.",
        "pdf_link": "https://arxiv.org/pdf/2201.11990v3.pdf"
    },
    {
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "authors": [
            "Jason Wei",
            "Xuezhi Wang",
            "Dale Schuurmans",
            "Maarten Bosma",
            "Brian Ichter",
            "Fei Xia",
            "Ed Chi",
            "Quoc Le",
            "Denny Zhou"
        ],
        "published": "2022-01-28T02:33:07Z",
        "summary": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.",
        "pdf_link": "https://arxiv.org/pdf/2201.11903v6.pdf"
    },
    {
        "title": "\"That's so cute!\": The CARE Dataset for Affective Response Detection",
        "authors": [
            "Jane Dwivedi-Yu",
            "Alon Y. Halevy"
        ],
        "published": "2022-01-28T02:17:50Z",
        "summary": "Social media plays an increasing role in our communication with friends and\nfamily, and our consumption of information and entertainment. Hence, to design\neffective ranking functions for posts on social media, it would be useful to\npredict the affective response to a post (e.g., whether the user is likely to\nbe humored, inspired, angered, informed). Similar to work on emotion\nrecognition (which focuses on the affect of the publisher of the post), the\ntraditional approach to recognizing affective response would involve an\nexpensive investment in human annotation of training data.\n  We introduce CARE$_{db}$, a dataset of 230k social media posts annotated\naccording to 7 affective responses using the Common Affective Response\nExpression (CARE) method. The CARE method is a means of leveraging the signal\nthat is present in comments that are posted in response to a post, providing\nhigh-precision evidence about the affective response of the readers to the post\nwithout human annotation. Unlike human annotation, the annotation process we\ndescribe here can be iterated upon to expand the coverage of the method,\nparticularly for new affective responses. We present experiments that\ndemonstrate that the CARE annotations compare favorably with crowd-sourced\nannotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models\nfor predicting affective response as well as emotion detection, demonstrating\nthe utility of the dataset for related tasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.11895v2.pdf"
    },
    {
        "title": "Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers",
        "authors": [
            "Payam Karisani"
        ],
        "published": "2022-01-28T00:50:01Z",
        "summary": "We present a novel multiple-source unsupervised model for text classification\nunder domain shift. Our model exploits the update rates in document\nrepresentations to dynamically integrate domain encoders. It also employs a\nprobabilistic heuristic to infer the error rate in the target domain in order\nto pair source classifiers. Our heuristic exploits data transformation cost and\nthe classifier accuracy in the target feature space. We have used real world\nscenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We\nalso used pretrained multi-layer transformers as the document encoder in the\nexperiments to demonstrate whether the improvement achieved by domain\nadaptation models can be delivered by out-of-the-box language model\npretraining. The experiments testify that our model is the top performing\napproach in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2201.11870v2.pdf"
    },
    {
        "title": "Neural-FST Class Language Model for End-to-End Speech Recognition",
        "authors": [
            "Antoine Bruguier",
            "Duc Le",
            "Rohit Prabhavalkar",
            "Dangna Li",
            "Zhe Liu",
            "Bo Wang",
            "Eun Chang",
            "Fuchun Peng",
            "Ozlem Kalinli",
            "Michael L. Seltzer"
        ],
        "published": "2022-01-28T00:20:57Z",
        "summary": "We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech\nrecognition, a novel method that combines neural network language models\n(NNLMs) and finite state transducers (FSTs) in a mathematically consistent\nframework. Our method utilizes a background NNLM which models generic\nbackground text together with a collection of domain-specific entities modeled\nas individual FSTs. Each output token is generated by a mixture of these\ncomponents; the mixture weights are estimated with a separately trained neural\ndecider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in\nterms of Word Error Rate. NFCLM achieves similar performance as traditional\nNNLM and FST shallow fusion while being less prone to overbiasing and 12 times\nmore compact, making it more suitable for on-device usage.",
        "pdf_link": "https://arxiv.org/pdf/2201.11867v2.pdf"
    },
    {
        "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences",
        "authors": [
            "Yikuan Li",
            "Ramsey M. Wehbe",
            "Faraz S. Ahmad",
            "Hanyin Wang",
            "Yuan Luo"
        ],
        "published": "2022-01-27T22:51:58Z",
        "summary": "Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].",
        "pdf_link": "https://arxiv.org/pdf/2201.11838v3.pdf"
    },
    {
        "title": "Going Extreme: Comparative Analysis of Hate Speech in Parler and Gab",
        "authors": [
            "Abraham Israeli",
            "Oren Tsur"
        ],
        "published": "2022-01-27T19:29:17Z",
        "summary": "Social platforms such as Gab and Parler, branded as `free-speech' networks,\nhave seen a significant growth of their user base in recent years. This\npopularity is mainly attributed to the stricter moderation enforced by\nmainstream platforms such as Twitter, Facebook, and Reddit. In this work we\nprovide the first large scale analysis of hate-speech on Parler.\n  We experiment with an array of algorithms for hate-speech detection,\ndemonstrating limitations of transfer learning in that domain, given the\nillusive and ever changing nature of the ways hate-speech is delivered. In\norder to improve classification accuracy we annotated 10K Parler posts, which\nwe use to fine-tune a BERT classifier. Classification of individual posts is\nthen leveraged for the classification of millions of users via label\npropagation over the social network. Classifying users by their propensity to\ndisseminate hate, we find that hate mongers make 16.1\\% of Parler active users,\nand that they have distinct characteristics comparing to other user groups. We\nfind that hate mongers are more active, more central and express distinct\nlevels of sentiment and convey a distinct array of emotions like anger and\nsadness. We further complement our analysis by comparing the trends discovered\nin Parler and those found in Gab.\n  To the best of our knowledge, this is among the first works to analyze hate\nspeech in Parler in a quantitative manner and on the user level, and the first\nannotated dataset to be made available to the community.",
        "pdf_link": "https://arxiv.org/pdf/2201.11770v1.pdf"
    },
    {
        "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation",
        "authors": [
            "Jixuan Wang",
            "Kuan-Chieh Wang",
            "Frank Rudzicz",
            "Michael Brudno"
        ],
        "published": "2022-01-27T15:29:30Z",
        "summary": "Large pretrained language models (LMs) like BERT have improved performance in\nmany disparate natural language processing (NLP) tasks. However, fine tuning\nsuch models requires a large number of training examples for each target task.\nSimultaneously, many realistic NLP problems are \"few shot\", without a\nsufficiently large training set. In this work, we propose a novel conditional\nneural process-based approach for few-shot text classification that learns to\ntransfer from other diverse tasks with rich annotation. Our key idea is to\nrepresent each task using gradient information from a base model and to train\nan adaptation network that modulates a text classifier conditioned on the task\nrepresentation. While previous task-aware few-shot learners represent tasks by\ninput encoding, our novel task representation is more powerful, as the gradient\ncaptures input-output relationships of a task. Experimental results show that\nour approach outperforms traditional fine-tuning, sequential transfer learning,\nand state-of-the-art meta learning approaches on a collection of diverse\nfew-shot tasks. We further conducted analysis and ablations to justify our\ndesign choices.",
        "pdf_link": "https://arxiv.org/pdf/2201.11576v1.pdf"
    },
    {
        "title": "Synchromesh: Reliable code generation from pre-trained language models",
        "authors": [
            "Gabriel Poesia",
            "Oleksandr Polozov",
            "Vu Le",
            "Ashish Tiwari",
            "Gustavo Soares",
            "Christopher Meek",
            "Sumit Gulwani"
        ],
        "published": "2022-01-26T22:57:44Z",
        "summary": "Large pre-trained language models have been used to generate code,providing a\nflexible interface for synthesizing programs from natural language\nspecifications. However, they often violate syntactic and semantic rules of\ntheir output language, limiting their practical usability. In this paper, we\npropose Synchromesh: a framework for substantially improving the reliability of\npre-trained models for code generation. Synchromesh comprises two components.\nFirst, it retrieves few-shot examples from a training bank using Target\nSimilarity Tuning (TST), a novel method for semantic example selection. TST\nlearns to recognize utterances that describe similar target programs despite\ndifferences in surface natural language features. Then, Synchromesh feeds the\nexamples to a pre-trained language model and samples programs using Constrained\nSemantic Decoding (CSD): a general framework for constraining the output to a\nset of valid programs in the target language. CSD leverages constraints on\npartial outputs to sample complete correct programs, and needs neither\nre-training nor fine-tuning of the language model. We evaluate our methods by\nsynthesizing code from natural language descriptions using GPT-3 and Codex in\nthree real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow\nprograms. These domains showcase rich constraints that CSD is able to enforce,\nincluding syntax, scope, typing rules, and contextual logic. We observe\nsubstantial complementary gains from CSD and TST in prediction accuracy and in\neffectively preventing run-time errors.",
        "pdf_link": "https://arxiv.org/pdf/2201.11227v1.pdf"
    },
    {
        "title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence",
        "authors": [
            "Wei Zhao",
            "Michael Strube",
            "Steffen Eger"
        ],
        "published": "2022-01-26T20:28:26Z",
        "summary": "Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\nare weak in recognizing coherence, and thus are not reliable in a way to spot\nthe discourse-level improvements of those text generation systems. In this\nwork, we introduce DiscoScore, a parametrized discourse metric, which uses BERT\nto model discourse coherence from different perspectives, driven by Centering\ntheory. Our experiments encompass 16 non-discourse and discourse metrics,\nincluding DiscoScore and popular coherence models, evaluated on summarization\nand document-level machine translation (MT). We find that (i) the majority of\nBERT-based metrics correlate much worse with human rated coherence than early\ndiscourse metrics, invented a decade ago; (ii) the recent state-of-the-art\nBARTScore is weak when operated at system level -- which is particularly\nproblematic as systems are typically compared in this manner. DiscoScore, in\ncontrast, achieves strong system-level correlation with human ratings, not only\nin coherence but also in factual consistency and other aspects, and surpasses\nBARTScore by over 10 correlation points on average. Further, aiming to\nunderstand DiscoScore, we provide justifications to the importance of discourse\ncoherence for evaluation metrics, and explain the superiority of one variant\nover another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.",
        "pdf_link": "https://arxiv.org/pdf/2201.11176v4.pdf"
    },
    {
        "title": "Language-biased image classification: evaluation based on semantic representations",
        "authors": [
            "Yoann Lemesle",
            "Masataka Sawayama",
            "Guillermo Valle-Perez",
            "Maxime Adolphe",
            "H\u00e9l\u00e8ne Sauz\u00e9on",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2022-01-26T15:46:36Z",
        "summary": "Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interference similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.",
        "pdf_link": "https://arxiv.org/pdf/2201.11014v2.pdf"
    },
    {
        "title": "Learning To Recognize Procedural Activities with Distant Supervision",
        "authors": [
            "Xudong Lin",
            "Fabio Petroni",
            "Gedas Bertasius",
            "Marcus Rohrbach",
            "Shih-Fu Chang",
            "Lorenzo Torresani"
        ],
        "published": "2022-01-26T15:06:28Z",
        "summary": "In this paper we consider the problem of classifying fine-grained, multi-step\nactivities (e.g., cooking different recipes, making disparate home\nimprovements, creating various forms of arts and crafts) from long videos\nspanning up to several minutes. Accurately categorizing these activities\nrequires not only recognizing the individual steps that compose the task but\nalso capturing their temporal dependencies. This problem is dramatically\ndifferent from traditional action classification, where models are typically\noptimized on videos that span only a few seconds and that are manually trimmed\nto contain simple atomic actions. While step annotations could enable the\ntraining of models to recognize the individual steps of procedural activities,\nexisting large-scale datasets in this area do not include such segment labels\ndue to the prohibitive cost of manually annotating temporal boundaries in long\nvideos. To address this issue, we propose to automatically identify steps in\ninstructional videos by leveraging the distant supervision of a textual\nknowledge base (wikiHow) that includes detailed descriptions of the steps\nneeded for the execution of a wide variety of complex activities. Our method\nuses a language model to match noisy, automatically-transcribed speech from the\nvideo to step descriptions in the knowledge base. We demonstrate that video\nmodels trained to recognize these automatically-labeled steps (without manual\nsupervision) yield a representation that achieves superior generalization\nperformance on four downstream tasks: recognition of procedural activities,\nstep classification, step forecasting and egocentric video classification.",
        "pdf_link": "https://arxiv.org/pdf/2201.10990v3.pdf"
    },
    {
        "title": "FiNCAT: Financial Numeral Claim Analysis Tool",
        "authors": [
            "Sohom Ghosh",
            "Sudip Kumar Naskar"
        ],
        "published": "2022-01-26T11:53:34Z",
        "summary": "While making investment decisions by reading financial documents, investors\nneed to differentiate between in-claim and outof-claim numerals. In this paper,\nwe present a tool which does it automatically. It extracts context embeddings\nof the numerals using one of the transformer based pre-trained language model\ncalled BERT. After this, it uses a Logistic Regression based model to detect\nwhether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)\ndataset to train our model. After conducting rigorous experiments we achieve a\nMacro F1 score of 0.8223 on the validation set. We have open-sourced this tool\nand it can be accessed from\nhttps://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool",
        "pdf_link": "https://arxiv.org/pdf/2202.00631v1.pdf"
    },
    {
        "title": "On the Effectiveness of Pinyin-Character Dual-Decoding for End-to-End Mandarin Chinese ASR",
        "authors": [
            "Zhao Yang",
            "Dianwen Ng",
            "Xiao Fu",
            "Liping Han",
            "Wei Xi",
            "Rui Wang",
            "Rui Jiang",
            "Jizhong Zhao"
        ],
        "published": "2022-01-26T07:59:03Z",
        "summary": "End-to-end automatic speech recognition (ASR) has achieved promising results.\nHowever, most existing end-to-end ASR methods neglect the use of specific\nlanguage characteristics. For Mandarin Chinese ASR tasks, there exist mutual\npromotion relationship between Pinyin and Character where Chinese characters\ncan be romanized by Pinyin. Based on the above intuition, we first investigate\ntypes of end-to-end encoder-decoder based models in the single-input\ndual-output (SIDO) multi-task framework, after which a novel asynchronous\ndecoding with fuzzy Pinyin sampling method is proposed according to the\none-to-one correspondence characteristics between Pinyin and Character.\nFurthermore, we proposed a two-stage training strategy to make training more\nstable and converge faster. The results on the test sets of AISHELL-1 dataset\nshow that the proposed enhanced dual-decoder model without a language model is\nimproved by a big margin compared to strong baseline models.",
        "pdf_link": "https://arxiv.org/pdf/2201.10792v2.pdf"
    },
    {
        "title": "Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR",
        "authors": [
            "Yufei Liu",
            "Rao Ma",
            "Haihua Xu",
            "Yi He",
            "Zejun Ma",
            "Weibin Zhang"
        ],
        "published": "2022-01-26T07:47:27Z",
        "summary": "An end-to-end (E2E) ASR model implicitly learns a prior Internal Language\nModel (ILM) from the training transcripts. To fuse an external LM using Bayes\nposterior theory, the log likelihood produced by the ILM has to be accurately\nestimated and subtracted. In this paper we propose two novel approaches to\nestimate the ILM based on Listen-Attend-Spell (LAS) framework. The first method\nis to replace the context vector of the LAS decoder at every time step with a\nvector that is learned with training transcripts. Furthermore, we propose\nanother method that uses a lightweight feed-forward network to directly map\nquery vector to context vector in a dynamic sense. Since the context vectors\nare learned by minimizing the perplexities on training transcripts, and their\nestimation is independent of encoder output, hence the ILMs are accurately\nlearned for both methods. Experiments show that the ILMs achieve the lowest\nperplexity, indicating the efficacy of the proposed methods. In addition, they\nalso significantly outperform the shallow fusion method, as well as two\npreviously proposed ILM Estimation (ILME) approaches on several datasets.",
        "pdf_link": "https://arxiv.org/pdf/2201.11627v3.pdf"
    },
    {
        "title": "Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation",
        "authors": [
            "Sinan Tan",
            "Mengmeng Ge",
            "Di Guo",
            "Huaping Liu",
            "Fuchun Sun"
        ],
        "published": "2022-01-26T07:43:47Z",
        "summary": "In the Vision-and-Language Navigation task, the embodied agent follows\nlinguistic instructions and navigates to a specific goal. It is important in\nmany practical scenarios and has attracted extensive attention from both\ncomputer vision and robotics communities. However, most existing works only use\nRGB images but neglect the 3D semantic information of the scene. To this end,\nwe develop a novel self-supervised training framework to encode the voxel-level\n3D semantic reconstruction into a 3D semantic representation. Specifically, a\nregion query task is designed as the pretext task, which predicts the presence\nor absence of objects of a particular class in a specific 3D region. Then, we\nconstruct an LSTM-based navigation model and train it with the proposed 3D\nsemantic representations and BERT language features on vision-language pairs.\nExperiments show that the proposed approach achieves success rates of 68% and\n66% on the validation unseen and test unseen splits of the R2R dataset\nrespectively, which are superior to most of RGB-based methods utilizing\nvision-language transformers.",
        "pdf_link": "https://arxiv.org/pdf/2201.10788v1.pdf"
    },
    {
        "title": "Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models",
        "authors": [
            "Lu Dong",
            "Zhi-Qiang Guo",
            "Chao-Hong Tan",
            "Ya-Jun Hu",
            "Yuan Jiang",
            "Zhen-Hua Ling"
        ],
        "published": "2022-01-26T02:49:56Z",
        "summary": "Neural network models have achieved state-of-the-art performance on\ngrapheme-to-phoneme (G2P) conversion. However, their performance relies on\nlarge-scale pronunciation dictionaries, which may not be available for a lot of\nlanguages. Inspired by the success of the pre-trained language model BERT, this\npaper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which\nis built by self-supervised training on a large, language-specific word list\nwith only grapheme information. Furthermore, two approaches are developed to\nincorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,\nfine-tuning GBERT or fusing GBERT into the Transformer model by attention.\nExperimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean\ndatasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our\nGBERT-based G2P models under both medium-resource and low-resource data\nconditions.",
        "pdf_link": "https://arxiv.org/pdf/2201.10716v1.pdf"
    },
    {
        "title": "A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model",
        "authors": [
            "Xin Sun",
            "Tao Ge",
            "Shuming Ma",
            "Jingjing Li",
            "Furu Wei",
            "Houfeng Wang"
        ],
        "published": "2022-01-26T02:10:32Z",
        "summary": "Synthetic data construction of Grammatical Error Correction (GEC) for\nnon-English languages relies heavily on human-designed and language-specific\nrules, which produce limited error-corrected patterns. In this paper, we\npropose a generic and language-independent strategy for multilingual GEC, which\ncan train a GEC system effectively for a new non-English language with only two\neasy-to-access resources: 1) a pretrained cross-lingual language model (PXLM)\nand 2) parallel translation data between English and the language. Our approach\ncreates diverse parallel GEC data without any language-specific operations by\ntaking the non-autoregressive translation generated by PXLM and the gold\ntranslation as error-corrected sentence pairs. Then, we reuse PXLM to\ninitialize the GEC model and pretrain it with the synthetic data generated by\nitself, which yields further improvement. We evaluate our approach on three\npublic benchmarks of GEC in different languages. It achieves the\nstate-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains\ncompetitive performance on Falko-Merlin (German) and RULEC-GEC (Russian).\nFurther analysis demonstrates that our data construction method is\ncomplementary to rule-based approaches.",
        "pdf_link": "https://arxiv.org/pdf/2201.10707v1.pdf"
    },
    {
        "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection",
        "authors": [
            "Suchin Gururangan",
            "Dallas Card",
            "Sarah K. Dreier",
            "Emily K. Gade",
            "Leroy Z. Wang",
            "Zeyu Wang",
            "Luke Zettlemoyer",
            "Noah A. Smith"
        ],
        "published": "2022-01-25T17:20:04Z",
        "summary": "Language models increasingly rely on massive web dumps for diverse text data.\nHowever, these sources are rife with undesirable content. As such, resources\nlike Wikipedia, books, and newswire often serve as anchors for automatically\nselecting web text most suitable for language modeling, a process typically\nreferred to as quality filtering. Using a new dataset of U.S. high school\nnewspaper articles -- written by students from across the country -- we\ninvestigate whose language is preferred by the quality filter used for GPT-3.\nWe find that newspapers from larger schools, located in wealthier, educated,\nand urban ZIP codes are more likely to be classified as high quality. We then\ndemonstrate that the filter's measurement of quality is unaligned with other\nsensible metrics, such as factuality or literary acclaim. We argue that\nprivileging any corpus as high quality entails a language ideology, and more\ncare is needed to construct training corpora for language models, with better\ntransparency and justification for the inclusion or exclusion of various texts.",
        "pdf_link": "https://arxiv.org/pdf/2201.10474v2.pdf"
    },
    {
        "title": "Differentially Private Temporal Difference Learning with Stochastic Nonconvex-Strongly-Concave Optimization",
        "authors": [
            "Canzhe Zhao",
            "Yanjie Ze",
            "Jing Dong",
            "Baoxiang Wang",
            "Shuai Li"
        ],
        "published": "2022-01-25T16:48:29Z",
        "summary": "Temporal difference (TD) learning is a widely used method to evaluate\npolicies in reinforcement learning. While many TD learning methods have been\ndeveloped in recent years, little attention has been paid to preserving privacy\nand most of the existing approaches might face the concerns of data privacy\nfrom users. To enable complex representative abilities of policies, in this\npaper, we consider preserving privacy in TD learning with nonlinear value\nfunction approximation. This is challenging because such a nonlinear problem is\nusually studied in the formulation of stochastic nonconvex-strongly-concave\noptimization to gain finite-sample analysis, which would require simultaneously\npreserving the privacy on primal and dual sides. To this end, we employ a\nmomentum-based stochastic gradient descent ascent to achieve a single-timescale\nalgorithm, and achieve a good trade-off between meaningful privacy and utility\nguarantees of both the primal and dual sides by perturbing the gradients on\nboth sides using well-calibrated Gaussian noises. As a result, our DPTD\nalgorithm could provide $(\\epsilon,\\delta)$-differential privacy (DP) guarantee\nfor the sensitive information encoded in transitions and retain the original\npower of TD learning, with the utility upper bounded by\n$\\widetilde{\\mathcal{O}}(\\frac{(d\\log(1/\\delta))^{1/8}}{(n\\epsilon)^{1/4}})$\n(The tilde in this paper hides the log factor.), where $n$ is the trajectory\nlength and $d$ is the dimension. Extensive experiments conducted in OpenAI Gym\nshow the advantages of our proposed algorithm.",
        "pdf_link": "https://arxiv.org/pdf/2201.10447v1.pdf"
    },
    {
        "title": "BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment",
        "authors": [
            "Luis Lebron",
            "Yvette Graham",
            "Kevin McGuinness",
            "Konstantinos Kouramas",
            "Noel E. O'Connor"
        ],
        "published": "2022-01-25T11:29:58Z",
        "summary": "Evaluating video captioning systems is a challenging task as there are\nmultiple factors to consider; for instance: the fluency of the caption,\nmultiple actions happening in a single scene, and the human bias of what is\nconsidered important. Most metrics try to measure how similar the system\ngenerated captions are to a single or a set of human-annotated captions. This\npaper presents a new method based on a deep learning model to evaluate these\nsystems. The model is based on BERT, which is a language model that has been\nshown to work well in multiple NLP tasks. The aim is for the model to learn to\nperform an evaluation similar to that of a human. To do so, we use a dataset\nthat contains human evaluations of system generated captions. The dataset\nconsists of the human judgments of the captions produce by the system\nparticipating in various years of the TRECVid video to text task. These\nannotations will be made publicly available. BERTHA obtain favourable results,\noutperforming the commonly used metrics in some setups.",
        "pdf_link": "https://arxiv.org/pdf/2201.10243v3.pdf"
    },
    {
        "title": "Pre-Trained Language Transformers are Universal Image Classifiers",
        "authors": [
            "Rahul Goel",
            "Modar Sulaiman",
            "Kimia Noorbakhsh",
            "Mahdi Sharifi",
            "Rajesh Sharma",
            "Pooyan Jamshidi",
            "Kallol Roy"
        ],
        "published": "2022-01-25T08:56:14Z",
        "summary": "Facial images disclose many hidden personal traits such as age, gender, race,\nhealth, emotion, and psychology. Understanding these traits will help to\nclassify the people in different attributes. In this paper, we have presented a\nnovel method for classifying images using a pretrained transformer model. We\napply the pretrained transformer for the binary classification of facial images\nin criminal and non-criminal classes. The pretrained transformer of GPT-2 is\ntrained to generate text and then fine-tuned to classify facial images. During\nthe finetuning process with images, most of the layers of GT-2 are frozen\nduring backpropagation and the model is frozen pretrained transformer (FPT).\nThe FPT acts as a universal image classifier, and this paper shows the\napplication of FPT on facial images. We also use our FPT on encrypted images\nfor classification. Our FPT shows high accuracy on both raw facial images and\nencrypted images. We hypothesize the meta-learning capacity FPT gained because\nof its large size and trained on a large size with theory and experiments. The\nGPT-2 trained to generate a single word token at a time, through the\nautoregressive process, forced to heavy-tail distribution. Then the FPT uses\nthe heavy-tail property as its meta-learning capacity for classifying images.\nOur work shows one way to avoid bias during the machine classification of\nimages.The FPT encodes worldly knowledge because of the pretraining of one\ntext, which it uses during the classification. The statistical error of\nclassification is reduced because of the added context gained from the text.Our\npaper shows the ethical dimension of using encrypted data for\nclassification.Criminal images are sensitive to share across the boundary but\nencrypted largely evades ethical concern.FPT showing good classification\naccuracy on encrypted images shows promise for further research on\nprivacy-preserving machine learning.",
        "pdf_link": "https://arxiv.org/pdf/2201.10182v1.pdf"
    },
    {
        "title": "Multimodal data matters: language model pre-training over structured and unstructured electronic health records",
        "authors": [
            "Sicen Liu",
            "Xiaolong Wang",
            "Yongshuai Hou",
            "Ge Li",
            "Hui Wang",
            "Hui Xu",
            "Yang Xiang",
            "Buzhou Tang"
        ],
        "published": "2022-01-25T06:14:49Z",
        "summary": "As two important textual modalities in electronic health records (EHR), both\nstructured data (clinical codes) and unstructured data (clinical narratives)\nhave recently been increasingly applied to the healthcare domain. Most existing\nEHR-oriented studies, however, either focus on a particular modality or\nintegrate data from different modalities in a straightforward manner, which\nusually treats structured and unstructured data as two independent sources of\ninformation about patient admission and ignore the intrinsic interactions\nbetween them. In fact, the two modalities are documented during the same\nencounter where structured data inform the documentation of unstructured data\nand vice versa. In this paper, we proposed a Medical Multimodal Pre-trained\nLanguage Model, named MedM-PLM, to learn enhanced EHR representations over\nstructured and unstructured data and explore the interaction of two modalities.\nIn MedM-PLM, two Transformer-based neural network components are firstly\nadopted to learn representative characteristics from each modality. A\ncross-modal module is then introduced to model their interactions. We\npre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of\nthe model on three downstream clinical tasks, i.e., medication recommendation,\n30-day readmission prediction and ICD coding. Extensive experiments demonstrate\nthe power of MedM-PLM compared with state-of-the-art methods. Further analyses\nand visualizations show the robustness of our model, which could potentially\nprovide more comprehensive interpretations for clinical decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2201.10113v7.pdf"
    },
    {
        "title": "Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources",
        "authors": [
            "Angelina McMillan-Major",
            "Zaid Alyafeai",
            "Stella Biderman",
            "Kimbo Chen",
            "Francesco De Toni",
            "G\u00e9rard Dupont",
            "Hady Elsahar",
            "Chris Emezue",
            "Alham Fikri Aji",
            "Suzana Ili\u0107",
            "Nurulaqilla Khamis",
            "Colin Leong",
            "Maraim Masoud",
            "Aitor Soroa",
            "Pedro Ortiz Suarez",
            "Zeerak Talat",
            "Daniel van Strien",
            "Yacine Jernite"
        ],
        "published": "2022-01-25T03:05:23Z",
        "summary": "In recent years, large-scale data collection efforts have prioritized the\namount of data collected in order to improve the modeling capabilities of large\nlanguage models. This prioritization, however, has resulted in concerns with\nrespect to the rights of data subjects represented in data collections,\nparticularly when considering the difficulty in interrogating these collections\ndue to insufficient documentation and tools for analysis. Mindful of these\npitfalls, we present our methodology for a documentation-first, human-centered\ndata collection project as part of the BigScience initiative. We identified a\ngeographically diverse set of target language groups (Arabic, Basque, Chinese,\nCatalan, English, French, Indic languages, Indonesian, Niger-Congo languages,\nPortuguese, Spanish, and Vietnamese, as well as programming languages) for\nwhich to collect metadata on potential data sources. To structure this effort,\nwe developed our online catalogue as a supporting tool for gathering metadata\nthrough organized public hackathons. We present our development process;\nanalyses of the resulting resource metadata, including distributions over\nlanguages, regions, and resource types; and our lessons learned in this\nendeavor.",
        "pdf_link": "https://arxiv.org/pdf/2201.10066v1.pdf"
    },
    {
        "title": "Relational Memory Augmented Language Models",
        "authors": [
            "Qi Liu",
            "Dani Yogatama",
            "Phil Blunsom"
        ],
        "published": "2022-01-24T13:25:41Z",
        "summary": "We present a memory-augmented approach to condition an autoregressive\nlanguage model on a knowledge graph. We represent the graph as a collection of\nrelation triples and retrieve relevant relations for a given context to improve\ntext generation. Experiments on WikiText-103, WMT19, and enwik8 English\ndatasets demonstrate that our approach produces a better language model in\nterms of perplexity and bits per character. We also show that relational memory\nimproves coherence, is complementary to token-based memory, and enables causal\ninterventions. Our model provides a simple yet effective way to combine an\nautoregressive language model with a knowledge graph for a more coherent and\nlogical generation.",
        "pdf_link": "https://arxiv.org/pdf/2201.09680v1.pdf"
    },
    {
        "title": "Unified Multimodal Punctuation Restoration Framework for Mixed-Modality Corpus",
        "authors": [
            "Yaoming Zhu",
            "Liwei Wu",
            "Shanbo Cheng",
            "Mingxuan Wang"
        ],
        "published": "2022-01-24T10:15:53Z",
        "summary": "The punctuation restoration task aims to correctly punctuate the output\ntranscriptions of automatic speech recognition systems. Previous punctuation\nmodels, either using text only or demanding the corresponding audio, tend to be\nconstrained by real scenes, where unpunctuated sentences are a mixture of those\nwith and without audio. This paper proposes a unified multimodal punctuation\nrestoration framework, named UniPunc, to punctuate the mixed sentences with a\nsingle model. UniPunc jointly represents audio and non-audio samples in a\nshared latent space, based on which the model learns a hybrid representation\nand punctuates both kinds of samples. We validate the effectiveness of the\nUniPunc on real-world datasets, which outperforms various strong baselines\n(e.g. BERT, MuSe) by at least 0.8 overall F1 scores, making a new\nstate-of-the-art. Extensive experiments show that UniPunc's design is a\npervasive solution: by grafting onto previous models, UniPunc enables them to\npunctuate on the mixed corpus. Our code is available at\ngithub.com/Yaoming95/UniPunc",
        "pdf_link": "https://arxiv.org/pdf/2202.00468v1.pdf"
    },
    {
        "title": "Synthetic Books",
        "authors": [
            "Varvara Guljajeva"
        ],
        "published": "2022-01-24T08:26:28Z",
        "summary": "The article explores new ways of written language aided by AI technologies,\nlike GPT-2 and GPT-3. The question that is stated in the paper is not about\nwhether these novel technologies will eventually replace authored books, but\nhow to relate to and contextualize such publications and what kind of new\ntools, processes, and ideas are behind them. For that purpose, a new concept of\nsynthetic books is introduced in the article. It stands for the publications\ncreated by deploying AI technology, more precisely autoregressive language\nmodels that are able to generate human-like text. Supported by the case\nstudies, the value and reasoning of the synthetic books are discussed. The\npaper emphasizes that artistic quality is an issue when it comes to\nAI-generated content. The article introduces projects that demonstrate an\ninteractive input by an artist and/or audience combined with the\ndeep-learning-based language models. In the end, the paper focuses on\nunderstanding the neural aesthetics of written language in the art context.",
        "pdf_link": "https://arxiv.org/pdf/2201.09518v1.pdf"
    },
    {
        "title": "Emotion-based Modeling of Mental Disorders on Social Media",
        "authors": [
            "Xiaobo Guo",
            "Yaojia Sun",
            "Soroush Vosoughi"
        ],
        "published": "2022-01-24T04:41:02Z",
        "summary": "According to the World Health Organization (WHO), one in four people will be\naffected by mental disorders at some point in their lives. However, in many\nparts of the world, patients do not actively seek professional diagnosis\nbecause of stigma attached to mental illness, ignorance of mental health and\nits associated symptoms. In this paper, we propose a model for passively\ndetecting mental disorders using conversations on Reddit. Specifically, we\nfocus on a subset of mental disorders that are characterized by distinct\nemotional patterns (henceforth called emotional disorders): major depressive,\nanxiety, and bipolar disorders. Through passive (i.e., unprompted) detection,\nwe can encourage patients to seek diagnosis and treatment for mental disorders.\nOur proposed model is different from other work in this area in that our model\nis based entirely on the emotional states, and the transition between these\nstates of users on Reddit, whereas prior work is typically based on\ncontent-based representations (e.g., n-grams, language model embeddings, etc).\nWe show that content-based representation is affected by domain and topic bias\nand thus does not generalize, while our model, on the other hand, suppresses\ntopic-specific information and thus generalizes well across different topics\nand times. We conduct experiments on our model's ability to detect different\nemotional disorders and on the generalizability of our model. Our experiments\nshow that while our model performs comparably to content-based models, such as\nBERT, it generalizes much better across time and topic.",
        "pdf_link": "https://arxiv.org/pdf/2201.09451v1.pdf"
    },
    {
        "title": "An Application of Pseudo-Log-Likelihoods to Natural Language Scoring",
        "authors": [
            "Darren Abramson",
            "Ali Emami"
        ],
        "published": "2022-01-23T22:00:54Z",
        "summary": "Language models built using semi-supervised machine learning on large corpora\nof natural language have very quickly enveloped the fields of natural language\ngeneration and understanding. In this paper we apply a zero-shot approach\nindependently developed by a number of researchers now gaining recognition as a\nsignificant alternative to fine-tuning for evaluation on common sense tasks. A\nlanguage model with relatively few parameters and training steps compared to a\nmore recent language model (T5) can outperform it on a recent large data set\n(TimeDial), while displaying robustness in its performance across a similar\nclass of language tasks. Surprisingly, this result is achieved by using a\nhyperparameter-free zero-shot method with the smaller model, compared to\nfine-tuning to the larger model. We argue that robustness of the smaller model\nought to be understood in terms of compositionality, in a sense that we draw\nfrom recent literature on a class of similar models. We identify a practical\ncost for our method and model: high GPU-time for natural language evaluation.\nThe zero-shot measurement technique that produces remarkable stability, both\nfor ALBERT and other BERT variants, is an application of pseudo-log-likelihoods\nto masked language models for the relative measurement of probability for\nsubstitution alternatives in forced choice language tasks such as the Winograd\nSchema Challenge, Winogrande, and others. One contribution of this paper is to\nbring together a number of similar, but independent strands of research. We\nproduce some absolute state-of-the-art results for common sense reasoning in\nbinary choice tasks, performing better than any published result in the\nliterature, including fine-tuned efforts. We show a remarkable consistency of\nthe model's performance under adversarial settings, which we argue is best\nexplained by the model's compositionality of representations.",
        "pdf_link": "https://arxiv.org/pdf/2201.09377v1.pdf"
    },
    {
        "title": "A Large and Diverse Arabic Corpus for Language Modeling",
        "authors": [
            "Abbas Raza Ali",
            "Muhammad Ajmal Siddiqui",
            "Rema Algunaibet",
            "Hasan Raza Ali"
        ],
        "published": "2022-01-23T11:17:53Z",
        "summary": "Language models (LMs) have introduced a major paradigm shift in Natural\nLanguage Processing (NLP) modeling where large pre-trained LMs became integral\nto most of the NLP tasks. The LMs are intelligent enough to find useful and\nrelevant representations of the language without any supervision. Perhaps,\nthese models are used to fine-tune typical NLP tasks with significantly high\naccuracy as compared to the traditional approaches. Conversely, the training of\nthese models requires a massively large corpus that is a good representation of\nthe language. English LMs generally perform better than their other language\ncounterparts, due to the availability of massive English corpora. This work\nelaborates on the design and development of a large Arabic corpus. It consists\nof over 500 GB of Arabic cleaned text targeted at improving cross-domain\nknowledge and downstream generalization capability of large-scale language\nmodels. Moreover, the corpus is utilized in the training of a large Arabic LM.\nIn order to evaluate the effectiveness of the LM, a number of typical NLP tasks\nare fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when\ncompared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my\nknowledge, this is currently the largest clean and diverse Arabic corpus ever\ncollected.",
        "pdf_link": "https://arxiv.org/pdf/2201.09227v3.pdf"
    },
    {
        "title": "Chinese Word Segmentation with Heterogeneous Graph Neural Network",
        "authors": [
            "Xuemei Tang",
            "Jun Wang",
            "Qi Su"
        ],
        "published": "2022-01-22T06:25:56Z",
        "summary": "In recent years, deep learning has achieved significant success in the\nChinese word segmentation (CWS) task. Most of these methods improve the\nperformance of CWS by leveraging external information, e.g., words, sub-words,\nsyntax. However, existing approaches fail to effectively integrate the\nmulti-level linguistic information and also ignore the structural feature of\nthe external information. Therefore, in this paper, we proposed a framework to\nimprove CWS, named HGNSeg. It exploits multi-level external information\nsufficiently with the pre-trained language model and heterogeneous graph neural\nnetwork. The experimental results on six benchmark datasets (e.g., Bakeoff\n2005, Bakeoff 2008) validate that our approach can effectively improve the\nperformance of Chinese word segmentation. Importantly, in cross-domain\nscenarios, our method also shows a strong ability to alleviate the\nout-of-vocabulary (OOV) problem.",
        "pdf_link": "https://arxiv.org/pdf/2201.08975v1.pdf"
    },
    {
        "title": "Nearest Class-Center Simplification through Intermediate Layers",
        "authors": [
            "Ido Ben-Shaul",
            "Shai Dekel"
        ],
        "published": "2022-01-21T23:21:26Z",
        "summary": "Recent advances in theoretical Deep Learning have introduced geometric\nproperties that occur during training, past the Interpolation Threshold --\nwhere the training error reaches zero. We inquire into the phenomena coined\nNeural Collapse in the intermediate layers of the networks, and emphasize the\ninnerworkings of Nearest Class-Center Mismatch inside the deepnet. We further\nshow that these processes occur both in vision and language model\narchitectures. Lastly, we propose a Stochastic Variability-Simplification Loss\n(SVSL) that encourages better geometrical features in intermediate layers, and\nimproves both train metrics and generalization.",
        "pdf_link": "https://arxiv.org/pdf/2201.08924v2.pdf"
    },
    {
        "title": "Recurrent Neural Networks with Mixed Hierarchical Structures and EM Algorithm for Natural Language Processing",
        "authors": [
            "Zhaoxin Luo",
            "Michael Zhu"
        ],
        "published": "2022-01-21T23:08:33Z",
        "summary": "How to obtain hierarchical representations with an increasing level of\nabstraction becomes one of the key issues of learning with deep neural\nnetworks. A variety of RNN models have recently been proposed to incorporate\nboth explicit and implicit hierarchical information in modeling languages in\nthe literature. In this paper, we propose a novel approach called the latent\nindicator layer to identify and learn implicit hierarchical information (e.g.,\nphrases), and further develop an EM algorithm to handle the latent indicator\nlayer in training. The latent indicator layer further simplifies a text's\nhierarchical structure, which allows us to seamlessly integrate different\nlevels of attention mechanisms into the structure. We called the resulting\narchitecture as the EM-HRNN model. Furthermore, we develop two bootstrap\nstrategies to effectively and efficiently train the EM-HRNN model on long text\ndocuments. Simulation studies and real data applications demonstrate that the\nEM-HRNN model with bootstrap training outperforms other RNN-based models in\ndocument classification tasks. The performance of the EM-HRNN model is\ncomparable to a Transformer-based method called Bert-base, though the former is\nmuch smaller model and does not require pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2201.08919v1.pdf"
    },
    {
        "title": "Less is Less: When Are Snippets Insufficient for Human vs Machine Relevance Estimation?",
        "authors": [
            "Gabriella Kazai",
            "Bhaskar Mitra",
            "Anlei Dong",
            "Nick Craswell",
            "Linjun Yang"
        ],
        "published": "2022-01-21T14:41:16Z",
        "summary": "Traditional information retrieval (IR) ranking models process the full text\nof documents. Newer models based on Transformers, however, would incur a high\ncomputational cost when processing long texts, so typically use only snippets\nfrom the document instead. The model's input based on a document's URL, title,\nand snippet (UTS) is akin to the summaries that appear on a search engine\nresults page (SERP) to help searchers decide which result to click. This raises\nquestions about when such summaries are sufficient for relevance estimation by\nthe ranking model or the human assessor, and whether humans and machines\nbenefit from the document's full text in similar ways. To answer these\nquestions, we study human and neural model based relevance assessments on 12k\nquery-documents sampled from Bing's search logs. We compare changes in the\nrelevance assessments when only the document summaries and when the full text\nis also exposed to assessors, studying a range of query and document\nproperties, e.g., query type, snippet length. Our findings show that the full\ntext is beneficial for humans and a BERT model for similar query and document\ntypes, e.g., tail, long queries. A closer look, however, reveals that humans\nand machines respond to the additional input in very different ways. Adding the\nfull text can also hurt the ranker's performance, e.g., for navigational\nqueries.",
        "pdf_link": "https://arxiv.org/pdf/2201.08721v1.pdf"
    },
    {
        "title": "A Comparative Study on Language Models for Task-Oriented Dialogue Systems",
        "authors": [
            "Vinsen Marselino Andreas",
            "Genta Indra Winata",
            "Ayu Purwarianti"
        ],
        "published": "2022-01-21T13:24:25Z",
        "summary": "The recent development of language models has shown promising results by\nachieving state-of-the-art performance on various natural language tasks by\nfine-tuning pretrained models. In task-oriented dialogue (ToD) systems,\nlanguage models can be used for end-to-end training without relying on dialogue\nstate tracking to track the dialogue history but allowing the language models\nto generate responses according to the context given as input. This paper\nconducts a comparative study to show the effectiveness and strength of using\nrecent pretrained models for fine-tuning, such as BART and T5, on endto-end ToD\nsystems. The experimental results show substantial performance improvements\nafter language model fine-tuning. The models produce more fluent responses\nafter adding knowledge to the context that guides the model to avoid\nhallucination and generate accurate entities in the generated responses.\nFurthermore, we found that BART and T5 outperform GPT-based models in BLEU and\nF1 scores and achieve state-of-the-art performance in a ToD system.",
        "pdf_link": "https://arxiv.org/pdf/2201.08687v1.pdf"
    },
    {
        "title": "Deep Q-learning: a robust control approach",
        "authors": [
            "Balazs Varga",
            "Balazs Kulcsar",
            "Morteza Haghir Chehreghani"
        ],
        "published": "2022-01-21T09:47:34Z",
        "summary": "In this paper, we place deep Q-learning into a control-oriented perspective\nand study its learning dynamics with well-established techniques from robust\ncontrol. We formulate an uncertain linear time-invariant model by means of the\nneural tangent kernel to describe learning. We show the instability of learning\nand analyze the agent's behavior in frequency-domain. Then, we ensure\nconvergence via robust controllers acting as dynamical rewards in the loss\nfunction. We synthesize three controllers: state-feedback gain scheduling H2,\ndynamic Hinf, and constant gain Hinf controllers. Setting up the learning agent\nwith a control-oriented tuning methodology is more transparent and has\nwell-established literature compared to the heuristics in reinforcement\nlearning. In addition, our approach does not use a target network and\nrandomized replay memory. The role of the target network is overtaken by the\ncontrol input, which also exploits the temporal dependency of samples (opposed\nto a randomized memory buffer). Numerical simulations in different OpenAI Gym\nenvironments suggest that the Hinf controlled learning performs slightly better\nthan Double deep Q-learning.",
        "pdf_link": "https://arxiv.org/pdf/2201.08610v2.pdf"
    },
    {
        "title": "Identifying Adversarial Attacks on Text Classifiers",
        "authors": [
            "Zhouhang Xie",
            "Jonathan Brophy",
            "Adam Noack",
            "Wencong You",
            "Kalyani Asthana",
            "Carter Perkins",
            "Sabrina Reis",
            "Sameer Singh",
            "Daniel Lowd"
        ],
        "published": "2022-01-21T06:16:04Z",
        "summary": "The landscape of adversarial attacks against text classifiers continues to\ngrow, with new attacks developed every year and many of them available in\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\ngrowing body of work on robust learning, which reduces vulnerability to these\nattacks, though sometimes at a high cost in compute time or accuracy. In this\npaper, we take an alternate approach -- we attempt to understand the attacker\nby analyzing adversarial text to determine which methods were used to create\nit. Our first contribution is an extensive dataset for attack detection and\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\ntargeting three classifiers trained on six source datasets for sentiment\nanalysis and abuse detection in English. As our second contribution, we use\nthis dataset to develop and benchmark a number of classifiers for attack\nidentification -- determining if a given text has been adversarially\nmanipulated and by which attack. As a third contribution, we demonstrate the\neffectiveness of three classes of features for these tasks: text properties,\ncapturing content and presentation of text; language model properties,\ndetermining which tokens are more or less probable throughout the input; and\ntarget model properties, representing how the text classifier is influenced by\nthe attack, including internal node activations. Overall, this represents a\nfirst step towards forensics for adversarial attacks against text classifiers.",
        "pdf_link": "https://arxiv.org/pdf/2201.08555v1.pdf"
    },
    {
        "title": "AutoDistill: an End-to-End Framework to Explore and Distill Hardware-Efficient Language Models",
        "authors": [
            "Xiaofan Zhang",
            "Zongwei Zhou",
            "Deming Chen",
            "Yu Emma Wang"
        ],
        "published": "2022-01-21T04:32:19Z",
        "summary": "Recently, large pre-trained models have significantly improved the\nperformance of various Natural LanguageProcessing (NLP) tasks but they are\nexpensive to serve due to long serving latency and large memory usage. To\ncompress these models, knowledge distillation has attracted an increasing\namount of interest as one of the most effective methods for model compression.\nHowever, existing distillation methods have not yet addressed the unique\nchallenges of model serving in datacenters, such as handling fast evolving\nmodels, considering serving performance, and optimizing for multiple\nobjectives. To solve these problems, we propose AutoDistill, an end-to-end\nmodel distillation framework integrating model architecture exploration and\nmulti-objective optimization for building hardware-efficient NLP pre-trained\nmodels. We use Bayesian Optimization to conduct multi-objective Neural\nArchitecture Search for selecting student model architectures. The proposed\nsearch comprehensively considers both prediction accuracy and serving latency\non target hardware. The experiments on TPUv4i show the finding of seven model\narchitectures with better pre-trained accuracy (up to 3.2% higher) and lower\ninference latency (up to 1.44x faster) than MobileBERT. By running downstream\nNLP tasks in the GLUE benchmark, the model distilled for pre-training by\nAutoDistill with 28.5M parameters achieves an 81.69 average score, which is\nhigher than BERT_BASE, DistillBERT, TinyBERT, NAS-BERT, and MobileBERT. The\nmost compact model found by AutoDistill contains only 20.6M parameters but\nstill outperform BERT_BASE(109M), DistillBERT(67M), TinyBERT(67M), and\nMobileBERT(25.3M) regarding the average GLUE score. By evaluating on SQuAD, a\nmodel found by AutoDistill achieves an 88.4% F1 score with 22.8M parameters,\nwhich reduces parameters by more than 62% while maintaining higher accuracy\nthan DistillBERT, TinyBERT, and NAS-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2201.08539v1.pdf"
    },
    {
        "title": "Black-box Prompt Learning for Pre-trained Language Models",
        "authors": [
            "Shizhe Diao",
            "Zhichao Huang",
            "Ruijia Xu",
            "Xuechun Li",
            "Yong Lin",
            "Xiao Zhou",
            "Tong Zhang"
        ],
        "published": "2022-01-21T03:53:19Z",
        "summary": "The increasing scale of general-purpose Pre-trained Language Models (PLMs)\nnecessitates the study of more efficient adaptation across different downstream\ntasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)\nto resonate with pragmatic interactions between the cloud infrastructure and\nedge devices. Particularly, instead of fine-tuning the model in the cloud, we\nadapt PLMs by prompt learning, which efficiently optimizes only a few\nparameters of the discrete prompts. Moreover, we consider the scenario that we\ndo not have access to the parameters and gradients of the pre-trained models,\nexcept for its outputs given inputs. This black-box setting secures the cloud\ninfrastructure from potential attack and misuse to cause a single-point\nfailure, which is preferable to the white-box counterpart by current\ninfrastructures. Under this black-box constraint, we apply a variance-reduced\npolicy gradient algorithm to estimate the gradients of parameters in the\ncategorical distribution of each discrete prompt. In light of our method, the\nuser devices can efficiently tune their tasks by querying the PLMs bounded by a\nrange of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the\nproposed algorithm achieves significant improvement on eight benchmarks in a\ncloud-device collaboration manner. Finally, we conduct in-depth case studies to\ncomprehensively analyze our method in terms of various data sizes, prompt\nlengths, training budgets, optimization objectives, prompt transferability, and\nexplanations of the learned prompts. Our code will be available at\nhttps://github.com/shizhediao/Black-Box-Prompt-Learning.",
        "pdf_link": "https://arxiv.org/pdf/2201.08531v3.pdf"
    },
    {
        "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
        "authors": [
            "Suraj Nair",
            "Eugene Yang",
            "Dawn Lawrie",
            "Kevin Duh",
            "Paul McNamee",
            "Kenton Murray",
            "James Mayfield",
            "Douglas W. Oard"
        ],
        "published": "2022-01-20T22:11:38Z",
        "summary": "The advent of transformer-based models such as BERT has led to the rise of\nneural ranking models. These models have improved the effectiveness of\nretrieval systems well beyond that of lexical term matching models such as\nBM25. While monolingual retrieval tasks have benefited from large-scale\ntraining collections such as MS MARCO and advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. This\npaper introduces ColBERT-X, a generalization of the ColBERT\nmulti-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)\nencoder to support cross-language information retrieval (CLIR). ColBERT-X can\nbe trained in two ways. In zero-shot training, the system is trained on the\nEnglish MS MARCO collection, relying on the XLM-R encoder for cross-language\nmappings. In translate-train, the system is trained on the MS MARCO English\nqueries coupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages demonstrate\nsubstantial and statistically significant improvements of these trained dense\nretrieval models over traditional lexical CLIR baselines.",
        "pdf_link": "https://arxiv.org/pdf/2201.08471v1.pdf"
    },
    {
        "title": "Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs",
        "authors": [
            "Anna Filighera",
            "Sebastian Ochs",
            "Tim Steuer",
            "Thomas Tregel"
        ],
        "published": "2022-01-20T17:34:33Z",
        "summary": "Automatic grading models are valued for the time and effort saved during the\ninstruction of large student bodies. Especially with the increasing\ndigitization of education and interest in large-scale standardized testing, the\npopularity of automatic grading has risen to the point where commercial\nsolutions are widely available and used. However, for short answer formats,\nautomatic grading is challenging due to natural language ambiguity and\nversatility. While automatic short answer grading models are beginning to\ncompare to human performance on some datasets, their robustness, especially to\nadversarially manipulated data, is questionable. Exploitable vulnerabilities in\ngrading models can have far-reaching consequences ranging from cheating\nstudents receiving undeserved credit to undermining automatic grading\naltogether - even when most predictions are valid. In this paper, we devise a\nblack-box adversarial attack tailored to the educational short answer grading\nscenario to investigate the grading models' robustness. In our attack, we\ninsert adjectives and adverbs into natural places of incorrect student answers,\nfooling the model into predicting them as correct. We observed a loss of\nprediction accuracy between 10 and 22 percentage points using the\nstate-of-the-art models BERT and T5. While our attack made answers appear less\nnatural to humans in our experiments, it did not significantly increase the\ngraders' suspicions of cheating. Based on our experiments, we provide\nrecommendations for utilizing automatic grading systems more safely in\npractice.",
        "pdf_link": "https://arxiv.org/pdf/2201.08318v2.pdf"
    },
    {
        "title": "End-to-end Generative Pretraining for Multimodal Video Captioning",
        "authors": [
            "Paul Hongsuck Seo",
            "Arsha Nagrani",
            "Anurag Arnab",
            "Cordelia Schmid"
        ],
        "published": "2022-01-20T16:16:21Z",
        "summary": "Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.",
        "pdf_link": "https://arxiv.org/pdf/2201.08264v2.pdf"
    },
    {
        "title": "LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training",
        "authors": [
            "Qi Shi",
            "Qian Liu",
            "Bei Chen",
            "Yu Zhang",
            "Ting Liu",
            "Jian-Guang Lou"
        ],
        "published": "2022-01-20T09:29:34Z",
        "summary": "Language-based environment manipulation requires agents to manipulate the\nenvironment following natural language instructions, which is challenging due\nto the huge space of the environments. To address this challenge, various\napproaches have been proposed in recent work. Although these approaches work\nwell for their intended environments, they are difficult to generalize across\nenvironments. In this work, we propose LEMON, a general framework for\nlanguage-based environment manipulation tasks. Specifically, we first specify a\ntask-agnostic approach for language-based environment manipulation tasks, which\ncan deal with various environments using the same generative language model.\nThen we propose an execution-guided pre-training strategy to inject prior\nknowledge of environments to the language model with a pure synthetic\npre-training corpus. Experimental results on tasks including Alchemy, Scene,\nTangrams, ProPara and Recipes demonstrate the effectiveness of LEMON: it\nachieves new state-of-the-art results on four of the tasks, and the\nexecution-guided pre-training strategy brings remarkable improvements on all\nexperimental tasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.08081v3.pdf"
    },
    {
        "title": "Sentiment Analysis: Predicting Yelp Scores",
        "authors": [
            "Bhanu Prakash Reddy Guda",
            "Mashrin Srivastava",
            "Deep Karkhanis"
        ],
        "published": "2022-01-20T04:47:12Z",
        "summary": "In this work, we predict the sentiment of restaurant reviews based on a\nsubset of the Yelp Open Dataset. We utilize the meta features and text\navailable in the dataset and evaluate several machine learning and\nstate-of-the-art deep learning approaches for the prediction task. Through\nseveral qualitative experiments, we show the success of the deep models with\nattention mechanism in learning a balanced model for reviews across different\nrestaurants. Finally, we propose a novel Multi-tasked joint BERT model that\nimproves the overall classification performance.",
        "pdf_link": "https://arxiv.org/pdf/2201.07999v1.pdf"
    },
    {
        "title": "AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees",
        "authors": [
            "Rong Liang",
            "Tiehua Zhang",
            "Yujie Lu",
            "Yuze Liu",
            "Zhen Huang",
            "Xin Chen"
        ],
        "published": "2022-01-20T03:27:26Z",
        "summary": "Using the pre-trained language models to understand source codes has\nattracted increasing attention from financial institutions owing to the great\npotential to uncover financial risks. However, there are several challenges in\napplying these language models to solve programming language-related problems\ndirectly. For instance, the shift of domain knowledge between natural language\n(NL) and programming language (PL) requires understanding the semantic and\nsyntactic information from the data from different perspectives. To this end,\nwe propose the AstBERT model, a pre-trained PL model aiming to better\nunderstand the financial codes using the abstract syntax tree (AST).\nSpecifically, we collect a sheer number of source codes (both Java and Python)\nfrom the Alipay code repository and incorporate both syntactic and semantic\ncode knowledge into our model through the help of code parsers, in which AST\ninformation of the source codes can be interpreted and integrated. We evaluate\nthe performance of the proposed model on three tasks, including code question\nanswering, code clone detection and code refinement. Experiment results show\nthat our AstBERT achieves promising performance on three different downstream\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.07984v4.pdf"
    },
    {
        "title": "Near-Optimal Sparse Allreduce for Distributed Deep Learning",
        "authors": [
            "Shigang Li",
            "Torsten Hoefler"
        ],
        "published": "2022-01-19T13:56:57Z",
        "summary": "Communication overhead is one of the major obstacles to train large deep\nlearning models at scale. Gradient sparsification is a promising technique to\nreduce the communication volume. However, it is very challenging to obtain real\nperformance improvement because of (1) the difficulty of achieving an scalable\nand efficient sparse allreduce algorithm and (2) the sparsification overhead.\nThis paper proposes O$k$-Top$k$, a scheme for distributed training with sparse\ngradients. O$k$-Top$k$ integrates a novel sparse allreduce algorithm (less than\n6$k$ communication volume which is asymptotically optimal) with the\ndecentralized parallel Stochastic Gradient Descent (SGD) optimizer, and its\nconvergence is proved. To reduce the sparsification overhead, O$k$-Top$k$\nefficiently selects the top-$k$ gradient values according to an estimated\nthreshold. Evaluations are conducted on the Piz Daint supercomputer with neural\nnetwork models from different deep learning domains. Empirical results show\nthat O$k$-Top$k$ achieves similar model accuracy to dense allreduce. Compared\nwith the optimized dense and the state-of-the-art sparse allreduces,\nO$k$-Top$k$ is more scalable and significantly improves training throughput\n(e.g., 3.29x-12.95x improvement for BERT on 256 GPUs).",
        "pdf_link": "https://arxiv.org/pdf/2201.07598v2.pdf"
    },
    {
        "title": "TourBERT: A pretrained language model for the tourism industry",
        "authors": [
            "Veronika Arefieva",
            "Roman Egger"
        ],
        "published": "2022-01-19T07:24:30Z",
        "summary": "The Bidirectional Encoder Representations from Transformers (BERT) is\ncurrently one of the most important and state-of-the-art models for natural\nlanguage. However, it has also been shown that for domain-specific tasks it is\nhelpful to pretrain BERT on a domain-specific corpus. In this paper, we present\nTourBERT, a pretrained language model for tourism. We describe how TourBERT was\ndeveloped and evaluated. The evaluations show that TourBERT is outperforming\nBERT in all tourism-specific tasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.07449v3.pdf"
    },
    {
        "title": "GAP-Gen: Guided Automatic Python Code Generation",
        "authors": [
            "Junchen Zhao",
            "Yurun Song",
            "Junlin Wang",
            "Ian G. Harris"
        ],
        "published": "2022-01-19T06:32:47Z",
        "summary": "Automatic code generation from natural language descriptions can be highly\nbeneficial during the process of software development. In this work, we propose\nGAP-Gen, a Guided Automatic Python Code Generation method based on Python\nsyntactic constraints and semantic constraints. We first introduce Python\nsyntactic constraints in the form of Syntax-Flow, which is a simplified version\nof Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract\nSyntax Tree but maintaining crucial syntactic information of Python code. In\naddition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\nand function names consistently through out the code. In our work, rather than\npretraining, we focus on modifying the finetuning process which reduces\ncomputational requirements but retains high generation performance on automatic\nPython code generation task. GAP-Gen fine-tunes the transformer based language\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\nCodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our\nexperiments show that GAP-Gen achieves better results on automatic Python code\ngeneration task than previous works.",
        "pdf_link": "https://arxiv.org/pdf/2201.08810v2.pdf"
    },
    {
        "title": "Many Ways to Be Lonely: Fine-Grained Characterization of Loneliness and Its Potential Changes in COVID-19",
        "authors": [
            "Yueyi Jiang",
            "Yunfan Jiang",
            "Liu Leqi",
            "Piotr Winkielman"
        ],
        "published": "2022-01-19T05:22:55Z",
        "summary": "Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provided\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification achieved an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adults related\nforums were distinct from other forums. Those in young adult-focused forums\nwere more likely to express concerns pertaining to peer relationship, and were\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we showed that different forms of loneliness have\ndifferential use in coping strategies.",
        "pdf_link": "https://arxiv.org/pdf/2201.07423v5.pdf"
    },
    {
        "title": "Fooling MOSS Detection with Pretrained Language Models",
        "authors": [
            "Stella Biderman",
            "Edward Raff"
        ],
        "published": "2022-01-19T04:00:46Z",
        "summary": "As artificial intelligence (AI) technologies become increasingly powerful and\nprominent in society, their misuse is a growing concern. In educational\nsettings, AI technologies could be used by students to cheat on assignments and\nexams. In this paper we explore whether transformers can be used to solve\nintroductory level programming assignments while bypassing commonly used AI\ntools to detect similarities between pieces of software. We find that a student\nusing GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level\nprogramming assignments without triggering suspicion from MOSS [Aiken, 2000], a\nwidely used software similarity and plagiarism detection tool. This holds\ndespite the fact that GPT-J was not trained on the problems in question and is\nnot provided with any examples to work from. We further find that the code\nwritten by GPT-J is diverse in structure, lacking any particular tells that\nfuture plagiarism detection techniques may use to try to identify\nalgorithmically generated code. We conclude with a discussion of the ethical\nand educational implications of large language models and directions for future\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2201.07406v2.pdf"
    },
    {
        "title": "Unveiling Project-Specific Bias in Neural Code Models",
        "authors": [
            "Zhiming Li",
            "Yanzhou Li",
            "Tianlin Li",
            "Mengnan Du",
            "Bozhi Wu",
            "Yushi Cao",
            "Junzhe Jiang",
            "Yang Liu"
        ],
        "published": "2022-01-19T02:09:48Z",
        "summary": "Deep learning has introduced significant improvements in many software\nanalysis tasks. Although the Large Language Models (LLMs) based neural code\nmodels demonstrate commendable performance when trained and tested within the\nintra-project independent and identically distributed (IID) setting, they often\nstruggle to generalize effectively to real-world inter-project\nout-of-distribution (OOD) data. In this work, we show that this phenomenon is\ncaused by the heavy reliance on project-specific shortcuts for prediction\ninstead of ground-truth evidence. We propose a Cond-Idf measurement to\ninterpret this behavior, which quantifies the relatedness of a token with a\nlabel and its project-specificness. The strong correlation between model\nbehavior and the proposed measurement indicates that without proper\nregularization, models tend to leverage spurious statistical cues for\nprediction. Equipped with these observations, we propose a novel bias\nmitigation mechanism that regularizes the model's learning behavior by\nleveraging latent logic relations among samples. Experimental results on two\nrepresentative program analysis tasks indicate that our mitigation framework\ncan improve both inter-project OOD generalization and adversarial robustness,\nwhile not sacrificing accuracy on intra-project IID data.",
        "pdf_link": "https://arxiv.org/pdf/2201.07381v2.pdf"
    },
    {
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "authors": [
            "Wenlong Huang",
            "Pieter Abbeel",
            "Deepak Pathak",
            "Igor Mordatch"
        ],
        "published": "2022-01-18T18:59:45Z",
        "summary": "Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner",
        "pdf_link": "https://arxiv.org/pdf/2201.07207v2.pdf"
    },
    {
        "title": "Accelerating Representation Learning with View-Consistent Dynamics in Data-Efficient Reinforcement Learning",
        "authors": [
            "Tao Huang",
            "Jiachen Wang",
            "Xiao Chen"
        ],
        "published": "2022-01-18T14:28:30Z",
        "summary": "Learning informative representations from image-based observations is of\nfundamental concern in deep Reinforcement Learning (RL). However,\ndata-inefficiency remains a significant barrier to this objective. To overcome\nthis obstacle, we propose to accelerate state representation learning by\nenforcing view-consistency on the dynamics. Firstly, we introduce a formalism\nof Multi-view Markov Decision Process (MMDP) that incorporates multiple views\nof the state. Following the structure of MMDP, our method, View-Consistent\nDynamics (VCD), learns state representations by training a view-consistent\ndynamics model in the latent space, where views are generated by applying data\naugmentation to states. Empirical evaluation on DeepMind Control Suite and\nAtari-100k demonstrates VCD to be the SoTA data-efficient algorithm on visual\ncontrol tasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.07016v1.pdf"
    },
    {
        "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities",
        "authors": [
            "Mina Lee",
            "Percy Liang",
            "Qian Yang"
        ],
        "published": "2022-01-18T07:51:57Z",
        "summary": "Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs' generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3's\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3's language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing \"collaborator\" under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs' promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.",
        "pdf_link": "https://arxiv.org/pdf/2201.06796v2.pdf"
    },
    {
        "title": "Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records",
        "authors": [
            "Shuai Niu",
            "Qing Yin",
            "Yunya Song",
            "Yike Guo",
            "Xian Yang"
        ],
        "published": "2022-01-18T07:21:20Z",
        "summary": "Disease risk prediction has attracted increasing attention in the field of\nmodern healthcare, especially with the latest advances in artificial\nintelligence (AI). Electronic health records (EHRs), which contain\nheterogeneous patient information, are widely used in disease risk prediction\ntasks. One challenge of applying AI models for risk prediction lies in\ngenerating interpretable evidence to support the prediction results while\nretaining the prediction ability. In order to address this problem, we propose\nthe method of jointly embedding words and labels whereby attention modules\nlearn the weights of words from medical notes according to their relevance to\nthe names of risk prediction labels. This approach boosts interpretability by\nemploying an attention mechanism and including the names of prediction tasks in\nthe model. However, its application is only limited to the handling of textual\ninputs such as medical notes. In this paper, we propose a label dependent\nattention model LDAM to 1) improve the interpretability by exploiting\nClinical-BERT (a biomedical language model pre-trained on a large clinical\ncorpus) to encode biomedically meaningful features and labels jointly; 2)\nextend the idea of joint embedding to the processing of time-series data, and\ndevelop a multi-modal learning framework for integrating heterogeneous\ninformation from medical notes and time-series health status indicators. To\ndemonstrate our method, we apply LDAM to the MIMIC-III dataset to predict\ndifferent disease risks. We evaluate our method both quantitatively and\nqualitatively. Specifically, the predictive power of LDAM will be shown, and\ncase studies will be carried out to illustrate its interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2201.06779v1.pdf"
    },
    {
        "title": "Hierarchical Neural Network Approaches for Long Document Classification",
        "authors": [
            "Snehal Khandve",
            "Vedangi Wagh",
            "Apurva Wani",
            "Isha Joshi",
            "Raviraj Joshi"
        ],
        "published": "2022-01-18T07:17:40Z",
        "summary": "Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.",
        "pdf_link": "https://arxiv.org/pdf/2201.06774v1.pdf"
    },
    {
        "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus",
        "authors": [
            "Julien Abadji",
            "Pedro Ortiz Suarez",
            "Laurent Romary",
            "Beno\u00eet Sagot"
        ],
        "published": "2022-01-17T22:12:59Z",
        "summary": "The need for raw large raw corpora has dramatically increased in recent years\nwith the introduction of transfer learning and semi-supervised learning methods\nto Natural Language Processing. And while there have been some recent attempts\nto manually curate the amount of data necessary to train large language models,\nthe main way to obtain this data is still through automatic web crawling. In\nthis paper we take the existing multilingual web corpus OSCAR and its pipeline\nUngoliant that extracts and classifies data from Common Crawl at the line\nlevel, and propose a set of improvements and automatic annotations in order to\nproduce a new document-oriented version of OSCAR that could prove more suitable\nto pre-train large generative language models as well as hopefully other\napplications in Natural Language Processing and Digital Humanities.",
        "pdf_link": "https://arxiv.org/pdf/2201.06642v1.pdf"
    },
    {
        "title": "Language Model-Based Paired Variational Autoencoders for Robotic Language Learning",
        "authors": [
            "Ozan \u00d6zdemir",
            "Matthias Kerzel",
            "Cornelius Weber",
            "Jae Hee Lee",
            "Stefan Wermter"
        ],
        "published": "2022-01-17T10:05:26Z",
        "summary": "Human infants learn language while interacting with their environment in\nwhich their caregivers may describe the objects and actions they perform.\nSimilar to human infants, artificial agents can learn language while\ninteracting with their environment. In this work, first, we present a neural\nmodel that bidirectionally binds robot actions and their language descriptions\nin a simple object manipulation scenario. Building on our previous Paired\nVariational Autoencoders (PVAE) model, we demonstrate the superiority of the\nvariational autoencoder over standard autoencoders by experimenting with cubes\nof different colours, and by enabling the production of alternative\nvocabularies. Additional experiments show that the model's channel-separated\nvisual feature extraction module can cope with objects of different shapes.\nNext, we introduce PVAE-BERT, which equips the model with a pretrained\nlarge-scale language model, i.e., Bidirectional Encoder Representations from\nTransformers (BERT), enabling the model to go beyond comprehending only the\npredefined descriptions that the network has been trained on; the recognition\nof action descriptions generalises to unconstrained natural language as the\nmodel becomes capable of understanding unlimited variations of the same\ndescriptions. Our experiments suggest that using a pretrained language model as\nthe language encoder allows our approach to scale up for real-world scenarios\nwith instructions from human users.",
        "pdf_link": "https://arxiv.org/pdf/2201.06317v1.pdf"
    },
    {
        "title": "MuLVE, A Multi-Language Vocabulary Evaluation Data Set",
        "authors": [
            "Anik Jacobsen",
            "Salar Mohtaj",
            "Sebastian M\u00f6ller"
        ],
        "published": "2022-01-17T09:02:59Z",
        "summary": "Vocabulary learning is vital to foreign language learning. Correct and\nadequate feedback is essential to successful and satisfying vocabulary\ntraining. However, many vocabulary and language evaluation systems perform on\nsimple rules and do not account for real-life user learning data. This work\nintroduces Multi-Language Vocabulary Evaluation Data Set (MuLVE), a data set\nconsisting of vocabulary cards and real-life user answers, labeled indicating\nwhether the user answer is correct or incorrect. The data source is user\nlearning data from the Phase6 vocabulary trainer. The data set contains\nvocabulary questions in German and English, Spanish, and French as target\nlanguage and is available in four different variations regarding pre-processing\nand deduplication. We experiment to fine-tune pre-trained BERT language models\non the downstream task of vocabulary evaluation with the proposed MuLVE data\nset. The results provide outstanding results of > 95.5 accuracy and F2-score.\nThe data set is available on the European Language Grid.",
        "pdf_link": "https://arxiv.org/pdf/2201.06286v1.pdf"
    },
    {
        "title": "Unintended Bias in Language Model-driven Conversational Recommendation",
        "authors": [
            "Tianshu Shen",
            "Jiaru Li",
            "Mohamed Reda Bouadjenek",
            "Zheda Mai",
            "Scott Sanner"
        ],
        "published": "2022-01-17T05:50:14Z",
        "summary": "Conversational Recommendation Systems (CRSs) have recently started to\nleverage pretrained language models (LM) such as BERT for their ability to\nsemantically interpret a wide range of preference statement variations.\nHowever, pretrained LMs are well-known to be prone to intrinsic biases in their\ntraining data, which may be exacerbated by biases embedded in domain-specific\nlanguage data(e.g., user reviews) used to fine-tune LMs for CRSs. We study a\nrecently introduced LM-driven recommendation backbone (termed LMRec) of a CRS\nto investigate how unintended bias i.e., language variations such as name\nreferences or indirect indicators of sexual orientation or location that should\nnot affect recommendations manifests in significantly shifted price and\ncategory distributions of restaurant recommendations. The alarming results we\nobserve strongly indicate that LMRec has learned to reinforce harmful\nstereotypes through its recommendations. For example, offhand mention of names\nassociated with the black community significantly lowers the price distribution\nof recommended restaurants, while offhand mentions of common male-associated\nnames lead to an increase in recommended alcohol-serving establishments. These\nand many related results presented in this work raise a red flag that advances\nin the language handling capability of LM-drivenCRSs do not come without\nsignificant challenges related to mitigating unintended bias in future deployed\nCRS assistants with a potential reach of hundreds of millions of end-users.",
        "pdf_link": "https://arxiv.org/pdf/2201.06224v2.pdf"
    },
    {
        "title": "Korean-Specific Dataset for Table Question Answering",
        "authors": [
            "Changwook Jun",
            "Jooyoung Choi",
            "Myoseop Sim",
            "Hyun Kim",
            "Hansol Jang",
            "Kyungkoo Min"
        ],
        "published": "2022-01-17T05:47:44Z",
        "summary": "Existing question answering systems mainly focus on dealing with text data.\nHowever, much of the data produced daily is stored in the form of tables that\ncan be found in documents and relational databases, or on the web. To solve the\ntask of question answering over tables, there exist many datasets for table\nquestion answering written in English, but few Korean datasets. In this paper,\nwe demonstrate how we construct Korean-specific datasets for table question\nanswering: Korean tabular dataset is a collection of 1.4M tables with\ncorresponding descriptions for unsupervised pre-training language models.\nKorean table question answering corpus consists of 70k pairs of questions and\nanswers created by crowd-sourced workers. Subsequently, we then build a\npre-trained language model based on Transformer and fine-tune the model for\ntable question answering with these datasets. We then report the evaluation\nresults of our model. We make our datasets publicly available via our GitHub\nrepository and hope that those datasets will help further studies for question\nanswering over tables, and for the transformation of table formats.",
        "pdf_link": "https://arxiv.org/pdf/2201.06223v2.pdf"
    },
    {
        "title": "Natural Language Deduction through Search over Statement Compositions",
        "authors": [
            "Kaj Bostrom",
            "Zayne Sprague",
            "Swarat Chaudhuri",
            "Greg Durrett"
        ],
        "published": "2022-01-16T12:05:48Z",
        "summary": "In settings from fact-checking to question answering, we frequently want to\nknow whether a collection of evidence (premises) entails a hypothesis. Existing\nmethods primarily focus on the end-to-end discriminative version of this task,\nbut less work has treated the generative version in which a model searches over\nthe space of statements entailed by the premises to constructively derive the\nhypothesis. We propose a system for doing this kind of deductive reasoning in\nnatural language by decomposing the task into separate steps coordinated by a\nsearch procedure, producing a tree of intermediate conclusions that faithfully\nreflects the system's reasoning process. Our experiments on the EntailmentBank\ndataset (Dalvi et al., 2021) demonstrate that the proposed system can\nsuccessfully prove true statements while rejecting false ones. Moreover, it\nproduces natural language explanations with a 17% absolute higher step validity\nthan those produced by an end-to-end T5 model.",
        "pdf_link": "https://arxiv.org/pdf/2201.06028v2.pdf"
    },
    {
        "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Peter Clark",
            "Yiming Yang"
        ],
        "published": "2022-01-16T10:11:37Z",
        "summary": "Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homophone, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. Code, data, and\ninstructions to implement MEMPROMPT for a new task at\nhttps://www.memprompt.com/.",
        "pdf_link": "https://arxiv.org/pdf/2201.06009v7.pdf"
    },
    {
        "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
        "authors": [
            "Tianbao Xie",
            "Chen Henry Wu",
            "Peng Shi",
            "Ruiqi Zhong",
            "Torsten Scholak",
            "Michihiro Yasunaga",
            "Chien-Sheng Wu",
            "Ming Zhong",
            "Pengcheng Yin",
            "Sida I. Wang",
            "Victor Zhong",
            "Bailin Wang",
            "Chengzu Li",
            "Connor Boyle",
            "Ansong Ni",
            "Ziyu Yao",
            "Dragomir Radev",
            "Caiming Xiong",
            "Lingpeng Kong",
            "Rui Zhang",
            "Noah A. Smith",
            "Luke Zettlemoyer",
            "Tao Yu"
        ],
        "published": "2022-01-16T04:36:18Z",
        "summary": "Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.",
        "pdf_link": "https://arxiv.org/pdf/2201.05966v3.pdf"
    },
    {
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
        "authors": [
            "Alisa Liu",
            "Swabha Swayamdipta",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "published": "2022-01-16T03:13:49Z",
        "summary": "A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI improves performance on eight out-of-domain test sets we\nconsider, including by 11% on HANS and 9% on Adversarial NLI, compared to\ntraining on the 4x larger MultiNLI. Moreover, it continues to be more effective\nthan MultiNLI augmented with other NLI datasets. Our results demonstrate the\npromise of leveraging natural language generation techniques and re-imagining\nthe role of humans in the dataset creation process.",
        "pdf_link": "https://arxiv.org/pdf/2201.05955v5.pdf"
    },
    {
        "title": "Automatic Correction of Syntactic Dependency Annotation Differences",
        "authors": [
            "Andrew Zupon",
            "Andrew Carnie",
            "Michael Hammond",
            "Mihai Surdeanu"
        ],
        "published": "2022-01-15T17:17:55Z",
        "summary": "Annotation inconsistencies between data sets can cause problems for\nlow-resource NLP, where noisy or inconsistent data cannot be as easily replaced\ncompared with resource-rich languages. In this paper, we propose a method for\nautomatically detecting annotation mismatches between dependency parsing\ncorpora, as well as three related methods for automatically converting the\nmismatches. All three methods rely on comparing an unseen example in a new\ncorpus with similar examples in an existing corpus. These three methods include\na simple lexical replacement using the most frequent tag of the example in the\nexisting corpus, a GloVe embedding-based replacement that considers a wider\npool of examples, and a BERT embedding-based replacement that uses\ncontextualized embeddings to provide examples fine-tuned to our specific data.\nWe then evaluate these conversions by retraining two dependency parsers --\nStanza (Qi et al. 2020) and Parsing as Tagging (PaT) (Vacareanu et al. 2020) --\non the converted and unconverted data. We find that applying our conversions\nyields significantly better performance in many cases. Some differences\nobserved between the two parsers are observed. Stanza has a more complex\narchitecture with a quadratic algorithm, so it takes longer to train, but it\ncan generalize better with less data. The PaT parser has a simpler architecture\nwith a linear algorithm, speeding up training time but requiring more training\ndata to reach comparable or better performance.",
        "pdf_link": "https://arxiv.org/pdf/2201.05891v1.pdf"
    },
    {
        "title": "StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning",
        "authors": [
            "Yupei Liu",
            "Jinyuan Jia",
            "Hongbin Liu",
            "Neil Zhenqiang Gong"
        ],
        "published": "2022-01-15T17:04:38Z",
        "summary": "Pre-trained encoders are general-purpose feature extractors that can be used\nfor many downstream tasks. Recent progress in self-supervised learning can\npre-train highly effective encoders using a large volume of unlabeled data,\nleading to the emerging encoder as a service (EaaS). A pre-trained encoder may\nbe deemed confidential because its training requires lots of data and\ncomputation resources as well as its public release may facilitate misuse of\nAI, e.g., for deepfakes generation. In this paper, we propose the first attack\ncalled StolenEncoder to steal pre-trained image encoders. We evaluate\nStolenEncoder on multiple target encoders pre-trained by ourselves and three\nreal-world target encoders including the ImageNet encoder pre-trained by\nGoogle, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding\nencoder deployed as a paid EaaS. Our results show that our stolen encoders have\nsimilar functionality with the target encoders. In particular, the downstream\nclassifiers built upon a target encoder and a stolen one have similar accuracy.\nMoreover, stealing a target encoder using StolenEncoder requires much less data\nand computation resources than pre-training it from scratch. We also explore\nthree defenses that perturb feature vectors produced by a target encoder. Our\nresults show these defenses are not enough to mitigate StolenEncoder.",
        "pdf_link": "https://arxiv.org/pdf/2201.05889v2.pdf"
    },
    {
        "title": "Automatic Lexical Simplification for Turkish",
        "authors": [
            "Ahmet Yavuz Uluslu"
        ],
        "published": "2022-01-15T15:58:44Z",
        "summary": "In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.",
        "pdf_link": "https://arxiv.org/pdf/2201.05878v3.pdf"
    },
    {
        "title": "Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition",
        "authors": [
            "Yi Zhang",
            "Mingyuan Chen",
            "Jundong Shen",
            "Chongjun Wang"
        ],
        "published": "2022-01-15T12:02:28Z",
        "summary": "Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various\nhuman emotions from heterogeneous visual, audio and text modalities. Previous\nmethods mainly focus on projecting multiple modalities into a common latent\nspace and learning an identical representation for all labels, which neglects\nthe diversity of each modality and fails to capture richer semantic information\nfor each label from different perspectives. Besides, associated relationships\nof modalities and labels have not been fully exploited. In this paper, we\npropose versaTile multi-modAl learning for multI-labeL emOtion Recognition\n(TAILOR), aiming to refine multi-modal representations and enhance\ndiscriminative capacity of each label. Specifically, we design an adversarial\nmulti-modal refinement module to sufficiently explore the commonality among\ndifferent modalities and strengthen the diversity of each modality. To further\nexploit label-modal dependence, we devise a BERT-like cross-modal encoder to\ngradually fuse private and common modality representations in a granularity\ndescent way, as well as a label-guided decoder to adaptively generate a\ntailored representation for each label with the guidance of label semantics. In\naddition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in\nboth aligned and unaligned settings, which demonstrate the superiority of\nTAILOR over the state-of-the-arts. Code is available at\nhttps://github.com/kniter1/TAILOR.",
        "pdf_link": "https://arxiv.org/pdf/2201.05834v1.pdf"
    },
    {
        "title": "A Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition",
        "authors": [
            "Jibao Qiu",
            "C. L. Philip Chen",
            "Tong Zhang"
        ],
        "published": "2022-01-15T07:45:10Z",
        "summary": "Symbolic Music Emotion Recognition(SMER) is to predict music emotion from\nsymbolic data, such as MIDI and MusicXML. Previous work mainly focused on\nlearning better representation via (mask) language model pre-training but\nignored the intrinsic structure of the music, which is extremely important to\nthe emotional expression of music. In this paper, we present a simple\nmulti-task framework for SMER, which incorporates the emotion recognition task\nwith other emotion-related auxiliary tasks derived from the intrinsic structure\nof the music. The results show that our multi-task framework can be adapted to\ndifferent models. Moreover, the labels of auxiliary tasks are easy to be\nobtained, which means our multi-task methods do not require manually annotated\nlabels other than emotion. Conducting on two publicly available datasets\n(EMOPIA and VGMIDI), the experiments show that our methods perform better in\nSMER task. Specifically, accuracy has been increased by 4.17 absolute point to\n67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset.\nAblation studies also show the effectiveness of multi-task methods designed in\nthis paper.",
        "pdf_link": "https://arxiv.org/pdf/2201.05782v1.pdf"
    },
    {
        "title": "Machine Learning for Food Review and Recommendation",
        "authors": [
            "Tan Khang Le",
            "Siu Cheung Hui"
        ],
        "published": "2022-01-15T02:33:59Z",
        "summary": "Food reviews and recommendations have always been important for online food\nservice websites. However, reviewing and recommending food is not simple as it\nis likely to be overwhelmed by disparate contexts and meanings. In this paper,\nwe use different deep learning approaches to address the problems of sentiment\nanalysis, automatic review tag generation, and retrieval of food reviews. We\npropose to develop a web-based food review system at Nanyang Technological\nUniversity (NTU) named NTU Food Hunter, which incorporates different deep\nlearning approaches that help users with food selection. First, we implement\nthe BERT and LSTM deep learning models into the system for sentiment analysis\nof food reviews. Then, we develop a Part-of-Speech (POS) algorithm to\nautomatically identify and extract adjective-noun pairs from the review content\nfor review tag generation based on POS tagging and dependency parsing. Finally,\nwe also train a RankNet model for the re-ranking of the retrieval results to\nimprove the accuracy in our Solr-based food reviews search system. The\nexperimental results show that our proposed deep learning approaches are\npromising for the applications of real-world problems.",
        "pdf_link": "https://arxiv.org/pdf/2201.10978v1.pdf"
    },
    {
        "title": "The Dark Side of the Language: Pre-trained Transformers in the DarkNet",
        "authors": [
            "Leonardo Ranaldi",
            "Aria Nourbakhsh",
            "Arianna Patrizi",
            "Elena Sofia Ruzzetti",
            "Dario Onorati",
            "Francesca Fallucchi",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2022-01-14T16:04:09Z",
        "summary": "Pre-trained Transformers are challenging human performances in many NLP\ntasks. The massive datasets used for pre-training seem to be the key to their\nsuccess on existing tasks. In this paper, we explore how a range of pre-trained\nNatural Language Understanding models perform on definitely unseen sentences\nprovided by classification tasks over a DarkNet corpus. Surprisingly, results\nshow that syntactic and lexical neural networks perform on par with pre-trained\nTransformers even after fine-tuning. Only after what we call extreme domain\nadaptation, that is, retraining with the masked language model task on all the\nnovel corpus, pre-trained Transformers reach their standard high results. This\nsuggests that huge pre-training corpora may give Transformers unexpected help\nsince they are exposed to many of the possible sentences.",
        "pdf_link": "https://arxiv.org/pdf/2201.05613v3.pdf"
    },
    {
        "title": "Polarity and Subjectivity Detection with Multitask Learning and BERT Embedding",
        "authors": [
            "Ranjan Satapathy",
            "Shweta Pardeshi",
            "Erik Cambria"
        ],
        "published": "2022-01-14T09:52:15Z",
        "summary": "Multitask learning often helps improve the performance of related tasks as\nthese often have inter-dependence on each other and perform better when solved\nin a joint framework. In this paper, we present a deep multitask learning\nframework that jointly performs polarity and subjective detection. We propose\nan attention-based multitask model for predicting polarity and subjectivity.\nThe input sentences are transformed into vectors using pre-trained BERT and\nGlove embeddings, and the results depict that BERT embedding based model works\nbetter than the Glove based model. We compare our approach with\nstate-of-the-art models in both subjective and polarity classification\nsingle-task and multitask frameworks. The proposed approach reports baseline\nperformances for both polarity detection and subjectivity detection.",
        "pdf_link": "https://arxiv.org/pdf/2201.05363v1.pdf"
    },
    {
        "title": "A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models",
        "authors": [
            "Hanqing Zhang",
            "Haolin Song",
            "Shaoyu Li",
            "Ming Zhou",
            "Dawei Song"
        ],
        "published": "2022-01-14T08:32:20Z",
        "summary": "Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that better meet the specific constraints\nin practical applications. In recent years, methods using large-scale\npre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the limited level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks that require different types of controlled\nconstraints. In this paper, we present a systematic critical review on the\ncommon tasks, main approaches, and evaluation methods in this area. Finally, we\ndiscuss the challenges that the field is facing, and put forward various\npromising future directions. To the best of our knowledge, this is the first\nsurvey paper to summarize the state-of-the-art CTG techniques from the\nperspective of Transformer-based PLMs. We hope it can help researchers and\npractitioners in the related fields to quickly track the academic and\ntechnological frontier, providing them with a landscape of the area and a\nroadmap for future research.",
        "pdf_link": "https://arxiv.org/pdf/2201.05337v5.pdf"
    },
    {
        "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification",
        "authors": [
            "Alon Talmor",
            "Ori Yoran",
            "Ronan Le Bras",
            "Chandra Bhagavatula",
            "Yoav Goldberg",
            "Yejin Choi",
            "Jonathan Berant"
        ],
        "published": "2022-01-14T06:49:15Z",
        "summary": "Constructing benchmarks that test the abilities of modern natural language\nunderstanding models is difficult - pre-trained language models exploit\nartifacts in benchmarks to achieve human parity, but still fail on adversarial\nexamples and make errors that demonstrate a lack of common sense. In this work,\nwe propose gamification as a framework for data construction. The goal of\nplayers in the game is to compose questions that mislead a rival AI while using\nspecific phrases for extra points. The game environment leads to enhanced user\nengagement and simultaneously gives the game designer control over the\ncollected data, allowing us to collect high-quality data at scale. Using our\nmethod we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and\ndemonstrate its difficulty for models that are orders-of-magnitude larger than\nthe AI used in the game itself. Our best baseline, the T5-based Unicorn with\n11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3\n(52.9%) in a few-shot inference setup. Both score well below human performance\nwhich is at 94.1%.",
        "pdf_link": "https://arxiv.org/pdf/2201.05320v1.pdf"
    },
    {
        "title": "Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation",
        "authors": [
            "Md Faisal Mahbub Chowdhury",
            "Gaetano Rossiello",
            "Michael Glass",
            "Nandana Mihindukulasooriya",
            "Alfio Gliozzo"
        ],
        "published": "2022-01-14T04:50:28Z",
        "summary": "In recent years, a number of keyphrase generation (KPG) approaches were\nproposed consisting of complex model architectures, dedicated training\nparadigms and decoding strategies. In this work, we opt for simplicity and show\nhow a commonly used seq2seq language model, BART, can be easily adapted to\ngenerate keyphrases from the text in a single batch computation using a simple\ntraining procedure. Empirical results on five benchmarks show that our approach\nis as good as the existing state-of-the-art KPG systems, but using a much\nsimpler and easy to deploy framework.",
        "pdf_link": "https://arxiv.org/pdf/2201.05302v1.pdf"
    },
    {
        "title": "Assemble Foundation Models for Automatic Code Summarization",
        "authors": [
            "Jian Gu",
            "Pasquale Salza",
            "Harald C. Gall"
        ],
        "published": "2022-01-13T21:38:33Z",
        "summary": "Automatic code summarization is beneficial to daily software development\nsince it could help reduce the requirement of manual writing. Currently,\nartificial intelligence is undergoing a paradigm shift. The foundation models\npretrained on massive data and finetuned to downstream tasks surpass specially\ncustomized models. This trend inspired us to consider reusing foundation models\ninstead of learning from scratch. Thereby, we propose a flexible and robust\napproach for automatic code summarization, based on neural models. We assemble\navailable foundation models, such as CodeBERT and GPT-2, into a single neural\nmodel named AdaMo. Moreover, we utilize Gaussian noise as the simulation of\ncontextual information to optimize the latent representation. Furthermore, we\nintroduce two adaptive schemes from the perspective of knowledge transfer,\nnamely continuous pretraining and intermediate finetuning, and design\nintermediate stage tasks for general sequence-to-sequence learning. Finally, we\nevaluate AdaMo against a benchmark dataset for code summarization, by comparing\nit with state-of-the-art models.",
        "pdf_link": "https://arxiv.org/pdf/2201.05222v2.pdf"
    },
    {
        "title": "Multi-task Pre-training Language Model for Semantic Network Completion",
        "authors": [
            "Da Li",
            "Sen Yang",
            "Kele Xu",
            "Ming Yi",
            "Yukai He",
            "Huaimin Wang"
        ],
        "published": "2022-01-13T09:18:30Z",
        "summary": "Semantic networks, such as the knowledge graph, can represent the knowledge\nleveraging the graph structure. Although the knowledge graph shows promising\nvalues in natural language processing, it suffers from incompleteness. This\npaper focuses on knowledge graph completion by predicting linkage between\nentities, which is a fundamental yet critical task. Semantic matching is a\npotential solution as it can deal with unseen entities, which the translational\ndistance based methods struggle with. However, to achieve competitive\nperformance as translational distance based methods, semantic matching based\nmethods require large-scale datasets for the training purpose, which are\ntypically unavailable in practical settings. Therefore, we employ the language\nmodel and introduce a novel knowledge graph architecture named LP-BERT, which\ncontains two main stages: multi-task pre-training and knowledge graph\nfine-tuning. In the pre-training phase, three tasks are taken to drive the\nmodel to learn the relationship from triples by predicting either entities or\nrelations. While in the fine-tuning phase, inspired by contrastive learning, we\ndesign a triple-style negative sampling in a batch, which greatly increases the\nproportion of negative sampling while keeping the training time almost\nunchanged. Furthermore, we propose a new data augmentation method utilizing the\ninverse relationship of triples to improve the performance and robustness of\nthe model. To demonstrate the effectiveness of our method, we conduct extensive\nexperiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The\nexperimental results demonstrate the superiority of our methods, and our\napproach achieves state-of-the-art results on WN18RR and FB15k-237 datasets.\nSignificantly, Hits@10 indicator is improved by 5% from previous\nstate-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2201.04843v2.pdf"
    },
    {
        "title": "Direct Mutation and Crossover in Genetic Algorithms Applied to Reinforcement Learning Tasks",
        "authors": [
            "Tarek Faycal",
            "Claudio Zito"
        ],
        "published": "2022-01-13T07:19:28Z",
        "summary": "Neuroevolution has recently been shown to be quite competitive in\nreinforcement learning (RL) settings, and is able to alleviate some of the\ndrawbacks of gradient-based approaches. This paper will focus on applying\nneuroevolution using a simple genetic algorithm (GA) to find the weights of a\nneural network that produce optimally behaving agents. In addition, we present\ntwo novel modifications that improve the data efficiency and speed of\nconvergence when compared to the initial implementation. The modifications are\nevaluated on the FrozenLake environment provided by OpenAI gym and prove to be\nsignificantly better than the baseline approach.",
        "pdf_link": "https://arxiv.org/pdf/2201.04815v2.pdf"
    },
    {
        "title": "Detection of Increased Time Intervals of Anti-Vaccine Tweets for COVID-19 Vaccine with BERT Model",
        "authors": [
            "\u00dclk\u00fc Tuncer K\u00fc\u00e7\u00fckta\u015f",
            "Fatih Uysal",
            "F\u0131rat Hardala\u00e7",
            "\u0130smail Biri"
        ],
        "published": "2022-01-12T18:30:23Z",
        "summary": "The most effective of the solutions against Covid-19 is the various vaccines\ndeveloped. Distrust of vaccines can hinder the rapid and effective use of this\nremedy. One of the means of expressing the thoughts of society is social media.\nDetermining the time intervals during which anti-vaccination increases in\nsocial media can help institutions determine the strategy to be used in\ncombating anti-vaccination. Recording and tracking every tweet entered with\nhuman labor would be inefficient, so various automation solutions are needed.\nIn this study, The Bidirectional Encoder Representations from Transformers\n(BERT) model, which is a deep learning-based natural language processing (NLP)\nmodel, was used. In a dataset of 1506 tweets divided into four different\ncategories as news, irrelevant, anti-vaccine, and vaccine supporters, the model\nwas trained with a learning rate of 5e-6 for 25 epochs. To determine the\nintervals in which anti-vaccine tweets are concentrated, the categories to\nwhich 652840 tweets belong were determined by using the trained model. The\nchange of the determined categories overtime was visualized and the events that\ncould cause the change were determined. As a result of model training, in the\ntest dataset, the f-score of 0.81 and AUC values for different classes were\nobtained as 0.99,0.91, 0.92, 0.92, respectively. In this model, unlike the\nstudies in the literature, an auxiliary system is designed that provides data\nthat institutions can use when determining their strategy by measuring and\nvisualizing the frequency of anti-vaccine tweets in a time interval, different\nfrom detecting and censoring such tweets.",
        "pdf_link": "https://arxiv.org/pdf/2202.00477v1.pdf"
    },
    {
        "title": "Diagnosing BERT with Retrieval Heuristics",
        "authors": [
            "Arthur C\u00e2mara",
            "Claudia Hauff"
        ],
        "published": "2022-01-12T13:11:17Z",
        "summary": "Word embeddings, made widely popular in 2013 with the release of word2vec,\nhave become a mainstay of NLP engineering pipelines. Recently, with the release\nof BERT, word embeddings have moved from the term-based embedding space to the\ncontextual embedding space -- each term is no longer represented by a single\nlow-dimensional vector but instead each term and \\emph{its context} determine\nthe vector weights. BERT's setup and architecture have been shown to be general\nenough to be applicable to many natural language tasks. Importantly for\nInformation Retrieval (IR), in contrast to prior deep learning solutions to IR\nproblems which required significant tuning of neural net architectures and\ntraining regimes, \"vanilla BERT\" has been shown to outperform existing\nretrieval algorithms by a wide margin, including on tasks and corpora that have\nlong resisted retrieval effectiveness gains over traditional IR baselines (such\nas Robust04). In this paper, we employ the recently proposed axiomatic dataset\nanalysis technique -- that is, we create diagnostic datasets that each fulfil a\nretrieval heuristic (both term matching and semantic-based) -- to explore what\nBERT is able to learn. In contrast to our expectations, we find BERT, when\napplied to a recently released large-scale web corpus with ad-hoc topics, to\n\\emph{not} adhere to any of the explored axioms. At the same time, BERT\noutperforms the traditional query likelihood retrieval model by 40\\%. This\nmeans that the axiomatic approach to IR (and its extension of diagnostic\ndatasets created for retrieval heuristics) may in its current form not be\napplicable to large-scale corpora. Additional -- different -- axioms are\nneeded.",
        "pdf_link": "https://arxiv.org/pdf/2201.04458v1.pdf"
    },
    {
        "title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts",
        "authors": [
            "Ting Jiang",
            "Jian Jiao",
            "Shaohan Huang",
            "Zihan Zhang",
            "Deqing Wang",
            "Fuzhen Zhuang",
            "Furu Wei",
            "Haizhen Huang",
            "Denvy Deng",
            "Qi Zhang"
        ],
        "published": "2022-01-12T06:54:21Z",
        "summary": "We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.",
        "pdf_link": "https://arxiv.org/pdf/2201.04337v2.pdf"
    },
    {
        "title": "A Feature Extraction based Model for Hate Speech Identification",
        "authors": [
            "Salar Mohtaj",
            "Vera Schmitt",
            "Sebastian M\u00f6ller"
        ],
        "published": "2022-01-11T22:53:28Z",
        "summary": "The detection of hate speech online has become an important task, as\noffensive language such as hurtful, obscene and insulting content can harm\nmarginalized people or groups. This paper presents TU Berlin team experiments\nand results on the task 1A and 1B of the shared task on hate speech and\noffensive content identification in Indo-European languages 2021. The success\nof different Natural Language Processing models is evaluated for the respective\nsubtasks throughout the competition. We tested different models based on\nrecurrent neural networks in word and character levels and transfer learning\napproaches based on Bert on the provided dataset by the competition. Among the\ntested models that have been used for the experiments, the transfer\nlearning-based models achieved the best results in both subtasks.",
        "pdf_link": "https://arxiv.org/pdf/2201.04227v1.pdf"
    },
    {
        "title": "Quantifying Robustness to Adversarial Word Substitutions",
        "authors": [
            "Yuting Yang",
            "Pei Huang",
            "FeiFei Ma",
            "Juan Cao",
            "Meishan Zhang",
            "Jian Zhang",
            "Jintao Li"
        ],
        "published": "2022-01-11T08:18:39Z",
        "summary": "Deep-learning-based NLP models are found to be vulnerable to word\nsubstitution perturbations. Before they are widely adopted, the fundamental\nissues of robustness need to be addressed. Along this line, we propose a formal\nframework to evaluate word-level robustness. First, to study safe regions for a\nmodel, we introduce robustness radius which is the boundary where the model can\nresist any perturbation. As calculating the maximum robustness radius is\ncomputationally hard, we estimate its upper and lower bound. We repurpose\nattack methods as ways of seeking upper bound and design a pseudo-dynamic\nprogramming algorithm for a tighter upper bound. Then verification method is\nutilized for a lower bound. Further, for evaluating the robustness of regions\noutside a safe radius, we reexamine robustness from another view:\nquantification. A robustness metric with a rigorous statistical guarantee is\nintroduced to measure the quantification of adversarial examples, which\nindicates the model's susceptibility to perturbations outside the safe radius.\nThe metric helps us figure out why state-of-the-art models like BERT can be\neasily fooled by a few word substitutions, but generalize well in the presence\nof real-world noises.",
        "pdf_link": "https://arxiv.org/pdf/2201.03829v1.pdf"
    },
    {
        "title": "Polish Natural Language Inference and Factivity -- an Expert-based Dataset and Benchmarks",
        "authors": [
            "Daniel Ziembicki",
            "Anna Wr\u00f3blewska",
            "Karolina Seweryn"
        ],
        "published": "2022-01-10T18:32:55Z",
        "summary": "Despite recent breakthroughs in Machine Learning for Natural Language\nProcessing, the Natural Language Inference (NLI) problems still constitute a\nchallenge. To this purpose we contribute a new dataset that focuses exclusively\non the factivity phenomenon; however, our task remains the same as other NLI\ntasks, i.e. prediction of entailment, contradiction or neutral (ECN). The\ndataset contains entirely natural language utterances in Polish and gathers\n2,432 verb-complement pairs and 309 unique verbs. The dataset is based on the\nNational Corpus of Polish (NKJP) and is a representative sample in regards to\nfrequency of main verbs and other linguistic features (e.g. occurrence of\ninternal negation). We found that transformer BERT-based models working on\nsentences obtained relatively good results ($\\approx89\\%$ F1 score). Even\nthough better results were achieved using linguistic features ($\\approx91\\%$ F1\nscore), this model requires more human labour (humans in the loop) because\nfeatures were prepared manually by expert linguists. BERT-based models\nconsuming only the input sentences show that they capture most of the\ncomplexity of NLI/factivity. Complex cases in the phenomenon - e.g. cases with\nentitlement (E) and non-factive verbs - remain an open issue for further\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2201.03521v1.pdf"
    },
    {
        "title": "Black-Box Tuning for Language-Model-as-a-Service",
        "authors": [
            "Tianxiang Sun",
            "Yunfan Shao",
            "Hong Qian",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2022-01-10T18:17:05Z",
        "summary": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.",
        "pdf_link": "https://arxiv.org/pdf/2201.03514v4.pdf"
    },
    {
        "title": "BERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives",
        "authors": [
            "Frederico Souza",
            "Jo\u00e3o Filho"
        ],
        "published": "2022-01-10T15:05:05Z",
        "summary": "BERT has revolutionized the NLP field by enabling transfer learning with\nlarge language models that can capture complex textual patterns, reaching the\nstate-of-the-art for an expressive number of NLP applications. For text\nclassification tasks, BERT has already been extensively explored. However,\naspects like how to better cope with the different embeddings provided by the\nBERT output layer and the usage of language-specific instead of multilingual\nmodels are not well studied in the literature, especially for the Brazilian\nPortuguese language. The purpose of this article is to conduct an extensive\nexperimental study regarding different strategies for aggregating the features\nproduced in the BERT output layer, with a focus on the sentiment analysis task.\nThe experiments include BERT models trained with Brazilian Portuguese corpora\nand the multilingual version, contemplating multiple aggregation strategies and\nopen-source datasets with predefined training, validation, and test partitions\nto facilitate the reproducibility of the results. BERT achieved the highest\nROC-AUC values for the majority of cases as compared to TF-IDF. Nonetheless,\nTF-IDF represents a good trade-off between the predictive performance and\ncomputational cost.",
        "pdf_link": "https://arxiv.org/pdf/2201.03382v1.pdf"
    },
    {
        "title": "Latency Adjustable Transformer Encoder for Language Understanding",
        "authors": [
            "Sajjad Kachuee",
            "Mohammad Sharifkhani"
        ],
        "published": "2022-01-10T13:04:39Z",
        "summary": "Adjusting the latency, power, and accuracy of natural language understanding\nmodels is a desirable objective of an efficient architecture. This paper\nproposes an efficient Transformer architecture that adjusts the inference\ncomputational cost adaptively with a desired inference latency speedup. In\nfine-tuning phase, the proposed method detects less important hidden sequence\nelements (word-vectors) and eliminates them in each encoder layer using a\nproposed Attention Context Contribution (ACC) metric. After the fine-tuning\nphase, with the novel offline-tuning property, the inference latency of the\nmodel can be adjusted in a wide range of inference speedup selections without\nany further training. The proposed method is applied to the BERT-base and GPT-2\nmodels for evaluation. Extensive experiments show that most of the word-vectors\nin higher Transformer layers have less contribution to the subsequent layers;\nhence, they can be eliminated to improve the inference latency. Experimental\nresults on extensive sentiment analysis, classification, text generation tasks\nand regression benchmarks like GLUE showed that the method is effective in\nvarious datasets with minimal impact on global context. The proposed method\nmathematically and experimentally improves the inference latency of BERT-base\nand GPT-2 by up to 4.8 and 3.72 times with less than 0.75% accuracy drop and\npassable perplexity on average. The suggested approach posits that in Large\nLanguage Models (LLMs), although the complete network is necessary for\ntraining, it can be truncated during the fine-tuning phase.",
        "pdf_link": "https://arxiv.org/pdf/2201.03327v7.pdf"
    },
    {
        "title": "Handwriting recognition and automatic scoring for descriptive answers in Japanese language tests",
        "authors": [
            "Hung Tuan Nguyen",
            "Cuong Tuan Nguyen",
            "Haruki Oka",
            "Tsunenori Ishioka",
            "Masaki Nakagawa"
        ],
        "published": "2022-01-10T08:47:52Z",
        "summary": "This paper presents an experiment of automatically scoring handwritten\ndescriptive answers in the trial tests for the new Japanese university entrance\nexamination, which were made for about 120,000 examinees in 2017 and 2018.\nThere are about 400,000 answers with more than 20 million characters. Although\nall answers have been scored by human examiners, handwritten characters are not\nlabeled. We present our attempt to adapt deep neural network-based handwriting\nrecognizers trained on a labeled handwriting dataset into this unlabeled answer\nset. Our proposed method combines different training strategies, ensembles\nmultiple recognizers, and uses a language model built from a large general\ncorpus to avoid overfitting into specific data. In our experiment, the proposed\nmethod records character accuracy of over 97% using about 2,000 verified\nlabeled answers that account for less than 0.5% of the dataset. Then, the\nrecognized answers are fed into a pre-trained automatic scoring system based on\nthe BERT model without correcting misrecognized characters and providing rubric\nannotations. The automatic scoring system achieves from 0.84 to 0.98 of\nQuadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents an acceptable\nsimilarity of scoring between the automatic scoring system and the human\nexaminers. These results are promising for further research on end-to-end\nautomatic scoring of descriptive answers.",
        "pdf_link": "https://arxiv.org/pdf/2201.03215v2.pdf"
    },
    {
        "title": "Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework",
        "authors": [
            "Rohitash Chandra",
            "Venkatesh Kulkarni"
        ],
        "published": "2022-01-09T23:59:11Z",
        "summary": "It is well known that translations of songs and poems not only break rhythm\nand rhyming patterns, but can also result in loss of semantic information. The\nBhagavad Gita is an ancient Hindu philosophical text originally written in\nSanskrit that features a conversation between Lord Krishna and Arjuna prior to\nthe Mahabharata war. The Bhagavad Gita is also one of the key sacred texts in\nHinduism and is known as the forefront of the Vedic corpus of Hinduism. In the\nlast two centuries, there has been a lot of interest in Hindu philosophy from\nwestern scholars; hence, the Bhagavad Gita has been translated in a number of\nlanguages. However, there is not much work that validates the quality of the\nEnglish translations. Recent progress of language models powered by deep\nlearning has enabled not only translations but a better understanding of\nlanguage and texts with semantic and sentiment analysis. Our work is motivated\nby the recent progress of language models powered by deep learning methods. In\nthis paper, we present a framework that compares selected translations (from\nSanskrit to English) of the Bhagavad Gita using semantic and sentiment\nanalyses. We use hand-labelled sentiment dataset for tuning state-of-art deep\nlearning-based language model known as bidirectional encoder representations\nfrom transformers (BERT). We provide sentiment and semantic analysis for\nselected chapters and verses across translations. Our results show that\nalthough the style and vocabulary in the respective translations vary widely,\nthe sentiment analysis and semantic similarity shows that the message conveyed\nare mostly similar.",
        "pdf_link": "https://arxiv.org/pdf/2201.03115v2.pdf"
    },
    {
        "title": "Medication Error Detection Using Contextual Language Models",
        "authors": [
            "Yu Jiang",
            "Christian Poellabauer"
        ],
        "published": "2022-01-09T15:21:54Z",
        "summary": "Medication errors most commonly occur at the ordering or prescribing stage,\npotentially leading to medical complications and poor health outcomes. While it\nis possible to catch these errors using different techniques; the focus of this\nwork is on textual and contextual analysis of prescription information to\ndetect and prevent potential medication errors. In this paper, we demonstrate\nhow to use BERT-based contextual language models to detect anomalies in written\nor spoken text based on a data set extracted from real-world medical data of\nthousands of patient records. The proposed models are able to learn patterns of\ntext dependency and predict erroneous output based on contextual information\nsuch as patient data. The experimental results yield accuracy up to 96.63% for\ntext input and up to 79.55% for speech input, which is satisfactory for most\nreal-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2201.03035v1.pdf"
    },
    {
        "title": "An Ensemble Approach to Acronym Extraction using Transformers",
        "authors": [
            "Prashant Sharma",
            "Hadeel Saadany",
            "Leonardo Zilio",
            "Diptesh Kanojia",
            "Constantin Or\u0103san"
        ],
        "published": "2022-01-09T14:49:46Z",
        "summary": "Acronyms are abbreviated units of a phrase constructed by using initial\ncomponents of the phrase in a text. Automatic extraction of acronyms from a\ntext can help various Natural Language Processing tasks like machine\ntranslation, information retrieval, and text summarisation. This paper\ndiscusses an ensemble approach for the task of Acronym Extraction, which\nutilises two different methods to extract acronyms and their corresponding long\nforms. The first method utilises a multilingual contextual language model and\nfine-tunes the model to perform the task. The second method relies on a\nconvolutional neural network architecture to extract acronyms and append them\nto the output of the previous method. We also augment the official training\ndataset with additional training samples extracted from several open-access\njournals to help improve the task performance. Our dataset analysis also\nhighlights the noise within the current task dataset. Our approach achieves the\nfollowing macro-F1 scores on test data released with the task: Danish (0.74),\nEnglish-Legal (0.72), English-Scientific (0.73), French (0.63), Persian (0.57),\nSpanish (0.65), Vietnamese (0.65). We release our code and models publicly.",
        "pdf_link": "https://arxiv.org/pdf/2201.03026v1.pdf"
    },
    {
        "title": "Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow",
        "authors": [
            "Maarten Sap",
            "Anna Jafarpour",
            "Yejin Choi",
            "Noah A. Smith",
            "James W. Pennebaker",
            "Eric Horvitz"
        ],
        "published": "2022-01-07T20:10:47Z",
        "summary": "Lifelong experiences and learned knowledge lead to shared expectations about\nhow common situations tend to unfold. Such knowledge of narrative event flow\nenables people to weave together a story. However, comparable computational\ntools to evaluate the flow of events in narratives are limited. We quantify the\ndifferences between autobiographical and imagined stories by introducing\nsequentiality, a measure of narrative flow of events, drawing probabilistic\ninferences from a cutting-edge large language model (GPT-3). Sequentiality\ncaptures the flow of a narrative by comparing the probability of a sentence\nwith and without its preceding story context. We applied our measure to study\nthousands of diary-like stories, collected from crowdworkers about either a\nrecent remembered experience or an imagined story on the same topic. The\nresults show that imagined stories have higher sequentiality than\nautobiographical stories and that the sequentiality of autobiographical stories\nincreases when the memories are retold several months later. In pursuit of\ndeeper understandings of how sequentiality measures the flow of narratives, we\nexplore proportions of major and minor events in story sentences, as annotated\nby crowdworkers. We find that lower sequentiality is associated with higher\nproportions of major events. The methods and results highlight opportunities to\nuse cutting-edge computational analyses, such as sequentiality, on large\ncorpora of matched imagined and autobiographical stories to investigate the\ninfluences of memory and reasoning on language generation processes.",
        "pdf_link": "https://arxiv.org/pdf/2201.02662v2.pdf"
    },
    {
        "title": "Textual Data Augmentation for Arabic-English Code-Switching Speech Recognition",
        "authors": [
            "Amir Hussein",
            "Shammur Absar Chowdhury",
            "Ahmed Abdelali",
            "Najim Dehak",
            "Ahmed Ali",
            "Sanjeev Khudanpur"
        ],
        "published": "2022-01-07T17:14:19Z",
        "summary": "The pervasiveness of intra-utterance code-switching (CS) in spoken content\nrequires that speech recognition (ASR) systems handle mixed language. Designing\na CS-ASR system has many challenges, mainly due to data scarcity, grammatical\nstructure complexity, and domain mismatch. The most common method for\naddressing CS is to train an ASR system with the available transcribed CS\nspeech, along with monolingual data. In this work, we propose a zero-shot\nlearning methodology for CS-ASR by augmenting the monolingual data with\nartificially generating CS text. We based our approach on random lexical\nreplacements and Equivalence Constraint (EC) while exploiting aligned\ntranslation pairs to generate random and grammatically valid CS content. Our\nempirical results show a 65.5% relative reduction in language model perplexity,\nand 7.7% in ASR WER on two ecologically valid CS test sets. The human\nevaluation of the generated text using EC suggests that more than 80% is of\nadequate quality.",
        "pdf_link": "https://arxiv.org/pdf/2201.02550v2.pdf"
    },
    {
        "title": "An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN",
        "authors": [
            "Md. Mahadi Hasan Sany",
            "Mumenunnesa Keya",
            "Sharun Akter Khushbu",
            "Akm Shahariar Azad Rabby",
            "Abu Kaisar Mohammad Masum"
        ],
        "published": "2022-01-06T15:59:20Z",
        "summary": "The global world is crossing a pandemic situation where this is a\ncatastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a\nglobal threat all over the 212 countries that people every day meet with mighty\nsituations. On the contrary, thousands of infected people live rich in\nmountains. Mental health is also affected by this worldwide coronavirus\nsituation. Due to this situation online sources made a communicative place that\ncommon people shares their opinion in any agenda. Such as affected news related\npositive and negative, financial issues, country and family crisis, lack of\nimport and export earning system etc. different kinds of circumstances are\nrecent trendy news in anywhere. Thus, vast amounts of text are produced within\nmoments therefore, in subcontinent areas the same as situation in other\ncountries and peoples opinion of text and situation also same but the language\nis different. This article has proposed some specific inputs along with Bangla\ntext comments from individual sources which can assure the goal of illustration\nthat machine learning outcome capable of building an assistive system. Opinion\nmining assistive system can be impactful in all language preferences possible.\nTo the best of our knowledge, the article predicted the Bangla input text on\nCOVID-19 issues proposed ML algorithms and deep learning models analysis also\ncheck the future reachability with a comparative analysis. Comparative analysis\nstates a report on text prediction accuracy is 91% along with ML algorithms and\n79% along with Deep Learning Models.",
        "pdf_link": "https://arxiv.org/pdf/2201.02119v1.pdf"
    },
    {
        "title": "Self-Training Vision Language BERTs with a Unified Conditional Model",
        "authors": [
            "Xiaofeng Yang",
            "Fengmao Lv",
            "Fayao Liu",
            "Guosheng Lin"
        ],
        "published": "2022-01-06T11:00:52Z",
        "summary": "Natural language BERTs are trained with language corpus in a self-supervised\nmanner. Unlike natural language BERTs, vision language BERTs need paired data\nto train, which restricts the scale of VL-BERT pretraining. We propose a\nself-training approach that allows training VL-BERTs from unlabeled image data.\nThe proposed method starts with our unified conditional model -- a vision\nlanguage BERT model that can perform zero-shot conditional generation. Given\ndifferent conditions, the unified conditional model can generate captions,\ndense captions, and even questions. We use the labeled image data to train a\nteacher model and use the trained model to generate pseudo captions on\nunlabeled image data. We then combine the labeled data and pseudo labeled data\nto train a student model. The process is iterated by putting the student model\nas a new teacher. By using the proposed self-training approach and only 300k\nunlabeled extra data, we are able to get competitive or even better\nperformances compared to the models of similar model size trained with 3\nmillion extra image data.",
        "pdf_link": "https://arxiv.org/pdf/2201.02010v2.pdf"
    },
    {
        "title": "Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model",
        "authors": [
            "Jinchuan Tian",
            "Jianwei Yu",
            "Chao Weng",
            "Yuexian Zou",
            "Dong Yu"
        ],
        "published": "2022-01-06T10:04:56Z",
        "summary": "Despite the rapid progress of end-to-end (E2E) automatic speech recognition\n(ASR), it has been shown that incorporating external language models (LMs) into\nthe decoding can further improve the recognition performance of E2E ASR\nsystems. To align with the modeling units adopted in E2E ASR systems,\nsubword-level (e.g., characters, BPE) LMs are usually used to cooperate with\ncurrent E2E ASR systems. However, the use of subword-level LMs will ignore the\nword-level information, which may limit the strength of the external LMs in E2E\nASR. Although several methods have been proposed to incorporate word-level\nexternal LMs in E2E ASR, these methods are mainly designed for languages with\nclear word boundaries such as English and cannot be directly applied to\nlanguages like Mandarin, in which each character sequence can have multiple\ncorresponding word sequences. To this end, we propose a novel decoding\nalgorithm where a word-level lattice is constructed on-the-fly to consider all\npossible word sequences for each partial hypothesis. Then, the LM score of the\nhypothesis is obtained by intersecting the generated lattice with an external\nword N-gram LM. The proposed method is examined on both Attention-based\nEncoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments\nsuggest that our method consistently outperforms subword-level LMs, including\nN-gram LM and neural network LM. We achieve state-of-the-art results on both\nAishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by\n14.8% relatively on a 21K-hour Mandarin dataset.",
        "pdf_link": "https://arxiv.org/pdf/2201.01995v1.pdf"
    },
    {
        "title": "Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models",
        "authors": [
            "Diana Kim",
            "Ahmed Elgammal",
            "Marian Mazzone"
        ],
        "published": "2022-01-05T21:03:29Z",
        "summary": "We present a machine learning system that can quantify fine art paintings\nwith a set of visual elements and principles of art. This formal analysis is\nfundamental for understanding art, but developing such a system is challenging.\nPaintings have high visual complexities, but it is also difficult to collect\nenough training data with direct labels. To resolve these practical\nlimitations, we introduce a novel mechanism, called proxy learning, which\nlearns visual concepts in paintings though their general relation to styles.\nThis framework does not require any visual annotation, but only uses style\nlabels and a general relationship between visual concepts and style. In this\npaper, we propose a novel proxy model and reformulate four pre-existing methods\nin the context of proxy learning. Through quantitative and qualitative\ncomparison, we evaluate these methods and compare their effectiveness in\nquantifying the artistic visual concepts, where the general relationship is\nestimated by language models; GloVe or BERT. The language modeling is a\npractical and scalable solution requiring no labeling, but it is inevitably\nimperfect. We demonstrate how the new proxy model is robust to the\nimperfection, while the other models are sensitively affected by it.",
        "pdf_link": "https://arxiv.org/pdf/2201.01819v1.pdf"
    },
    {
        "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
        "authors": [
            "Bowen Shi",
            "Wei-Ning Hsu",
            "Kushal Lakhotia",
            "Abdelrahman Mohamed"
        ],
        "published": "2022-01-05T17:40:45Z",
        "summary": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert",
        "pdf_link": "https://arxiv.org/pdf/2201.02184v2.pdf"
    },
    {
        "title": "Comparison of biomedical relationship extraction methods and models for knowledge graph creation",
        "authors": [
            "Nikola Milosevic",
            "Wolfgang Thielemann"
        ],
        "published": "2022-01-05T15:09:33Z",
        "summary": "Biomedical research is growing at such an exponential pace that scientists,\nresearchers, and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a way that claims and hypotheses can be easily\nfound, accessed, and validated. Knowledge graphs can provide such a framework\nfor semantic knowledge representation from literature. However, in order to\nbuild a knowledge graph, it is necessary to extract knowledge as relationships\nbetween biomedical entities and normalize both entities and relationship types.\nIn this paper, we present and compare few rule-based and machine learning-based\n(Naive Bayes, Random Forests as examples of traditional machine learning\nmethods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of\nmodern deep learning transformers) methods for scalable relationship extraction\nfrom biomedical literature, and for the integration into the knowledge graphs.\nWe examine how resilient are these various methods to unbalanced and fairly\nsmall datasets. Our experiments show that transformer-based models handle well\nboth small (due to pre-training on a large dataset) and unbalanced datasets.\nThe best performing model was the PubMedBERT-based model fine-tuned on balanced\ndata, with a reported F1-score of 0.92. DistilBERT-based model followed with\nF1-score of 0.89, performing faster and with lower resource requirements.\nBERT-based models performed better then T5-based generative models.",
        "pdf_link": "https://arxiv.org/pdf/2201.01647v4.pdf"
    },
    {
        "title": "Submix: Practical Private Prediction for Large-Scale Language Models",
        "authors": [
            "Antonio Ginart",
            "Laurens van der Maaten",
            "James Zou",
            "Chuan Guo"
        ],
        "published": "2022-01-04T04:23:38Z",
        "summary": "Recent data-extraction attacks have exposed that language models can memorize\nsome training samples verbatim. This is a vulnerability that can compromise the\nprivacy of the model's training data. In this work, we introduce SubMix: a\npractical protocol for private next-token prediction designed to prevent\nprivacy violations by language models that were fine-tuned on a private corpus\nafter pre-training on a public corpus. We show that SubMix limits the leakage\nof information that is unique to any individual user in the private corpus via\na relaxation of group differentially private prediction. Importantly, SubMix\nadmits a tight, data-dependent privacy accounting mechanism, which allows it to\nthwart existing data-extraction attacks while maintaining the utility of the\nlanguage model. SubMix is the first protocol that maintains privacy even when\npublicly releasing tens of thousands of next-token predictions made by large\ntransformer-based models such as GPT-2.",
        "pdf_link": "https://arxiv.org/pdf/2201.00971v1.pdf"
    },
    {
        "title": "Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety",
        "authors": [
            "Rajagopal A",
            "Nirmala V",
            "Arun Muthuraj Vedamanickam"
        ],
        "published": "2022-01-04T04:21:07Z",
        "summary": "There is amazing progress in Deep Learning based models for Image captioning\nand Low Light image enhancement. For the first time in literature, this paper\ndevelops a Deep Learning model that translates night scenes to sentences,\nopening new possibilities for AI applications in the safety of visually\nimpaired women. Inspired by Image Captioning and Visual Question Answering, a\nnovel Interactive Image Captioning is developed. A user can make the AI focus\non any chosen person of interest by influencing the attention scoring.\nAttention context vectors are computed from CNN feature vectors and\nuser-provided start word. The Encoder-Attention-Decoder neural network learns\nto produce captions from low brightness images. This paper demonstrates how\nwomen safety can be enabled by researching a novel AI capability in the\nInteractive Vision-Language model for perception of the environment in the\nnight.",
        "pdf_link": "https://arxiv.org/pdf/2201.00969v1.pdf"
    },
    {
        "title": "An Adversarial Benchmark for Fake News Detection Models",
        "authors": [
            "Lorenzo Jaime Yu Flores",
            "Yiding Hao"
        ],
        "published": "2022-01-03T23:51:55Z",
        "summary": "With the proliferation of online misinformation, fake news detection has\ngained importance in the artificial intelligence community. In this paper, we\npropose an adversarial benchmark that tests the ability of fake news detectors\nto reason about real-world facts. We formulate adversarial attacks that target\nthree aspects of \"understanding\": compositional semantics, lexical relations,\nand sensitivity to modifiers. We test our benchmark using BERT classifiers\nfine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets,\nand show that both models fail to respond to changes in compositional and\nlexical meaning. Our results strengthen the need for such models to be used in\nconjunction with other fact checking methods.",
        "pdf_link": "https://arxiv.org/pdf/2201.00912v1.pdf"
    },
    {
        "title": "Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models",
        "authors": [
            "Made Nindyatama Nityasya",
            "Haryo Akbarianto Wibowo",
            "Rendi Chevi",
            "Radityo Eko Prasojo",
            "Alham Fikri Aji"
        ],
        "published": "2022-01-03T10:07:13Z",
        "summary": "We perform knowledge distillation (KD) benchmark from task-specific BERT-base\nteacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,\nand BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text\nclassification and sequence labeling in the Indonesian language. We also\ncompare various aspects of distillations including the usage of word embeddings\nand unlabeled data augmentation. Our experiments show that, despite the rising\npopularity of Transformer-based models, using BiLSTM and CNN student models\nprovide the best trade-off between performance and computational resource (CPU,\nRAM, and storage) compared to pruned BERT models. We further propose some quick\nwins on performing KD to produce small NLP models via efficient KD training\nmechanisms involving simple choices of loss functions, word embeddings, and\nunlabeled data preparation.",
        "pdf_link": "https://arxiv.org/pdf/2201.00558v1.pdf"
    },
    {
        "title": "On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations",
        "authors": [
            "Aamir Miyajiwala",
            "Arnav Ladkat",
            "Samiksha Jagadale",
            "Raviraj Joshi"
        ],
        "published": "2022-01-02T08:33:49Z",
        "summary": "Text classification is a fundamental Natural Language Processing task that\nhas a wide variety of applications, where deep learning approaches have\nproduced state-of-the-art results. While these models have been heavily\ncriticized for their black-box nature, their robustness to slight perturbations\nin input text has been a matter of concern. In this work, we carry out a\ndata-focused study evaluating the impact of systematic practical perturbations\non the performance of the deep learning based text classification models like\nCNN, LSTM, and BERT-based algorithms. The perturbations are induced by the\naddition and removal of unwanted tokens like punctuation and stop-words that\nare minimally associated with the final performance of the model. We show that\nthese deep learning approaches including BERT are sensitive to such legitimate\ninput perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,\nand tweet_eval. We observe that BERT is more susceptible to the removal of\ntokens as compared to the addition of tokens. Moreover, LSTM is slightly more\nsensitive to input perturbations as compared to CNN based model. The work also\nserves as a practical guide to assessing the impact of discrepancies in\ntrain-test conditions on the final performance of models.",
        "pdf_link": "https://arxiv.org/pdf/2201.00318v2.pdf"
    },
    {
        "title": "Semantic Search for Large Scale Clinical Ontologies",
        "authors": [
            "Duy-Hoa Ngo",
            "Madonna Kemp",
            "Donna Truran",
            "Bevan Koopman",
            "Alejandro Metke-Jimenez"
        ],
        "published": "2022-01-01T05:15:42Z",
        "summary": "Finding concepts in large clinical ontologies can be challenging when queries\nuse different vocabularies. A search algorithm that overcomes this problem is\nuseful in applications such as concept normalisation and ontology matching,\nwhere concepts can be referred to in different ways, using different synonyms.\nIn this paper, we present a deep learning based approach to build a semantic\nsearch system for large clinical ontologies. We propose a Triplet-BERT model\nand a method that generates training data directly from the ontologies. The\nmodel is evaluated using five real benchmark data sets and the results show\nthat our approach achieves high results on both free text to concept and\nconcept to concept searching tasks, and outperforms all baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2201.00118v1.pdf"
    }
]